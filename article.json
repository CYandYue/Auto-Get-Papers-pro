[
  {
    "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop",
    "url": "http://arxiv.org/abs/2506.10968v1",
    "arxiv_id": "2506.10968v1",
    "authors": [
      "Justin Kerr",
      "Kush Hari",
      "Ethan Weber",
      "Chung Min Kim",
      "Brent Yi",
      "Tyler Bonnen",
      "Ken Goldberg",
      "Angjoo Kanazawa"
    ],
    "published": "2025-06-12T17:59:11+00:00",
    "summary": "Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/"
  },
  {
    "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation",
    "url": "http://arxiv.org/abs/2506.10966v1",
    "arxiv_id": "2506.10966v1",
    "authors": [
      "Ning Gao",
      "Yilun Chen",
      "Shuai Yang",
      "Xinyi Chen",
      "Yang Tian",
      "Hao Li",
      "Haifeng Huang",
      "Hanqing Wang",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "published": "2025-06-12T17:59:04+00:00",
    "summary": "Robotic manipulation in real-world settings remains challenging, especially regarding robust generalization. Existing simulation platforms lack sufficient support for exploring how policies adapt to varied instructions and scenarios. Thus, they lag behind the growing interest in instruction-following foundation models like LLMs, whose adaptability is crucial yet remains underexplored in fair comparisons. To bridge this gap, we introduce GenManip, a realistic tabletop simulation platform tailored for policy generalization studies. It features an automatic pipeline via LLM-driven task-oriented scene graph to synthesize large-scale, diverse tasks using 10K annotated 3D object assets. To systematically assess generalization, we present GenManip-Bench, a benchmark of 200 scenarios refined via human-in-the-loop corrections. We evaluate two policy types: (1) modular manipulation systems integrating foundation models for perception, reasoning, and planning, and (2) end-to-end policies trained through scalable data collection. Results show that while data scaling benefits end-to-end methods, modular systems enhanced with foundation models generalize more effectively across diverse scenarios. We anticipate this platform to facilitate critical insights for advancing policy generalization in realistic conditions. Project Page: https://genmanip.axi404.top/."
  },
  {
    "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
    "url": "http://arxiv.org/abs/2506.10947v1",
    "arxiv_id": "2506.10947v1",
    "authors": [
      "Rulin Shao",
      "Shuyue Stella Li",
      "Rui Xin",
      "Scott Geng",
      "Yiping Wang",
      "Sewoong Oh",
      "Simon Shaolei Du",
      "Nathan Lambert",
      "Sewon Min",
      "Ranjay Krishna",
      "Yulia Tsvetkov",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh",
      "Luke Zettlemoyer"
    ],
    "published": "2025-06-12T17:49:55+00:00",
    "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the 29.1% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, we find code reasoning -- thinking in code without actual code execution -- to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 65% to over 90%, even with spurious rewards. Overall, we hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. We suggest that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as we show that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals."
  },
  {
    "title": "Self-Adapting Language Models",
    "url": "http://arxiv.org/abs/2506.10943v1",
    "arxiv_id": "2506.10943v1",
    "authors": [
      "Adam Zweiger",
      "Jyothish Pari",
      "Han Guo",
      "Ekin Aky\u00fcrek",
      "Yoon Kim",
      "Pulkit Agrawal"
    ],
    "published": "2025-06-12T17:48:13+00:00",
    "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal."
  },
  {
    "title": "Magistral",
    "url": "http://arxiv.org/abs/2506.10910v1",
    "arxiv_id": "2506.10910v1",
    "authors": [
      "Mistral-AI",
      ":",
      "Abhinav Rastogi",
      "Albert Q. Jiang",
      "Andy Lo",
      "Gabrielle Berrada",
      "Guillaume Lample",
      "Jason Rute",
      "Joep Barmentlo",
      "Karmesh Yadav",
      "Kartik Khandelwal",
      "Khyathi Raghavi Chandu",
      "L\u00e9onard Blier",
      "Lucile Saulnier",
      "Matthieu Dinot",
      "Maxime Darrin",
      "Neha Gupta",
      "Roman Soletskyi",
      "Sagar Vaze",
      "Teven Le Scao",
      "Yihan Wang",
      "Adam Yang",
      "Alexander H. Liu",
      "Alexandre Sablayrolles",
      "Am\u00e9lie H\u00e9liou",
      "Am\u00e9lie Martin",
      "Andy Ehrenberg",
      "Anmol Agarwal",
      "Antoine Roux",
      "Arthur Darcet",
      "Arthur Mensch",
      "Baptiste Bout",
      "Baptiste Rozi\u00e8re",
      "Baudouin De Monicault",
      "Chris Bamford",
      "Christian Wallenwein",
      "Christophe Renaudin",
      "Cl\u00e9mence Lanfranchi",
      "Darius Dabert",
      "Devon Mizelle",
      "Diego de las Casas",
      "Elliot Chane-Sane",
      "Emilien Fugier",
      "Emma Bou Hanna",
      "Gauthier Delerce",
      "Gauthier Guinet",
      "Georgii Novikov",
      "Guillaume Martin",
      "Himanshu Jaju",
      "Jan Ludziejewski",
      "Jean-Hadrien Chabran",
      "Jean-Malo Delignon",
      "Joachim Studnia",
      "Jonas Amar",
      "Josselin Somerville Roberts",
      "Julien Denize",
      "Karan Saxena",
      "Kush Jain",
      "Lingxiao Zhao",
      "Louis Martin",
      "Luyu Gao",
      "L\u00e9lio Renard Lavaud",
      "Marie Pellat",
      "Mathilde Guillaumin",
      "Mathis Felardos",
      "Maximilian Augustin",
      "Micka\u00ebl Seznec",
      "Nikhil Raghuraman",
      "Olivier Duchenne",
      "Patricia Wang",
      "Patrick von Platen",
      "Patryk Saffer",
      "Paul Jacob",
      "Paul Wambergue",
      "Paula Kurylowicz",
      "Pavankumar Reddy Muddireddy",
      "Philom\u00e8ne Chagniot",
      "Pierre Stock",
      "Pravesh Agrawal",
      "Romain Sauvestre",
      "R\u00e9mi Delacourt",
      "Sanchit Gandhi",
      "Sandeep Subramanian",
      "Shashwat Dalal",
      "Siddharth Gandhi",
      "Soham Ghosh",
      "Srijan Mishra",
      "Sumukh Aithal",
      "Szymon Antoniak",
      "Thibault Schueller",
      "Thibaut Lavril",
      "Thomas Robert",
      "Thomas Wang",
      "Timoth\u00e9e Lacroix",
      "Valeriia Nemychnikova",
      "Victor Paltz",
      "Virgile Richard",
      "Wen-Ding Li",
      "William Marshall",
      "Xuanyu Zhang",
      "Yunhao Tang"
    ],
    "published": "2025-06-12T17:22:37+00:00",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium."
  },
  {
    "title": "Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.10889v1",
    "arxiv_id": "2506.10889v1",
    "authors": [
      "Waylon Luo",
      "Jiapeng Zhao",
      "Tong Zhan",
      "Qiang Guan"
    ],
    "published": "2025-06-12T16:54:19+00:00",
    "summary": "Present-day quantum systems face critical bottlenecks, including limited qubit counts, brief coherence intervals, and high susceptibility to errors-all of which obstruct the execution of large and complex circuits. The advancement of quantum algorithms has outpaced the capabilities of existing quantum hardware, making it difficult to scale computations effectively. Additionally, inconsistencies in hardware performance and pervasive quantum noise undermine system stability and computational accuracy. To optimize quantum workloads under these constraints, strategic approaches to task scheduling and resource coordination are essential. These methods must aim to accelerate processing, retain operational fidelity, and reduce the communication burden inherent to distributed setups. One of the persistent challenges in this domain is how to efficiently divide and execute large circuits across multiple quantum processors (QPUs), especially in error-prone environments. In response, we introduce a simulation-based tool that supports distributed scheduling and concurrent execution of quantum jobs on networked QPUs connected via real-time classical channels. The tool models circuit decomposition for workloads that surpass individual QPU limits, allowing for parallel execution through inter-processor communication. Using this simulation environment, we compare four distinct scheduling techniques-among them, a model informed by reinforcement learning. These strategies are evaluated across multiple metrics, including runtime efficiency, fidelity preservation, and communication costs. Our analysis underscores the trade-offs inherent in each approach and highlights how parallelized, noise-aware scheduling can meaningfully improve computational throughput in distributed quantum infrastructures."
  },
  {
    "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization",
    "url": "http://arxiv.org/abs/2506.10871v1",
    "arxiv_id": "2506.10871v1",
    "authors": [
      "Pierre-Fran\u00e7ois Massiani",
      "Alexander von Rohr",
      "Lukas Haverbeck",
      "Sebastian Trimpe"
    ],
    "published": "2025-06-12T16:34:19+00:00",
    "summary": "Despite the many recent advances in reinforcement learning (RL), the question of learning policies that robustly satisfy state constraints under unknown disturbances remains open. In this paper, we offer a new perspective on achieving robust safety by analyzing the interplay between two well-established techniques in model-free RL: entropy regularization, and constraints penalization. We reveal empirically that entropy regularization in constrained RL inherently biases learning toward maximizing the number of future viable actions, thereby promoting constraints satisfaction robust to action noise. Furthermore, we show that by relaxing strict safety constraints through penalties, the constrained RL problem can be approximated arbitrarily closely by an unconstrained one and thus solved using standard model-free RL. This reformulation preserves both safety and optimality while empirically improving resilience to disturbances. Our results indicate that the connection between entropy regularization and robustness is a promising avenue for further empirical and theoretical investigation, as it enables robust safety in RL through simple reward shaping."
  },
  {
    "title": "Joint Beamforming with Extremely Large Scale RIS: A Sequential Multi-Agent A2C Approach",
    "url": "http://arxiv.org/abs/2506.10815v1",
    "arxiv_id": "2506.10815v1",
    "authors": [
      "Zhi Chai",
      "Jiajie Xu",
      "Justin P Coon",
      "Mohamed-Slim Alouini"
    ],
    "published": "2025-06-12T15:29:57+00:00",
    "summary": "It is a challenging problem to jointly optimize the base station (BS) precoding matrix and the reconfigurable intelligent surface (RIS) phases simultaneously in a RIS-assisted multiple-user multiple-input-multiple-output (MU-MIMO) scenario when the size of the RIS becomes extremely large. In this paper, we propose a deep reinforcement learning algorithm called sequential multi-agent advantage actor-critic (A2C) to solve this problem. In addition, the discrete phase of RISs, imperfect channel state information (CSI), and channel correlations between users are taken into consideration. The computational complexity is also analyzed, and the performance of the proposed algorithm is compared with the zero-forcing (ZF) beamformer in terms of the sum spectral efficiency (SE). It is noted that the computational complexity of the proposed algorithm is lower than the benchmark, while the performance is better than the benchmark. Throughout simulations, it is also found that the proposed algorithm is robust to medium channel estimation error."
  },
  {
    "title": "Human-Robot Navigation using Event-based Cameras and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.10790v1",
    "arxiv_id": "2506.10790v1",
    "authors": [
      "Ignacio Bugueno-Cordova",
      "Javier Ruiz-del-Solar",
      "Rodrigo Verschae"
    ],
    "published": "2025-06-12T15:03:08+00:00",
    "summary": "This work introduces a robot navigation controller that combines event cameras and other sensors with reinforcement learning to enable real-time human-centered navigation and obstacle avoidance. Unlike conventional image-based controllers, which operate at fixed rates and suffer from motion blur and latency, this approach leverages the asynchronous nature of event cameras to process visual information over flexible time intervals, enabling adaptive inference and control. The framework integrates event-based perception, additional range sensing, and policy optimization via Deep Deterministic Policy Gradient, with an initial imitation learning phase to improve sample efficiency. Promising results are achieved in simulated environments, demonstrating robust navigation, pedestrian following, and obstacle avoidance. A demo video is available at the project website."
  },
  {
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework",
    "url": "http://arxiv.org/abs/2506.10741v1",
    "arxiv_id": "2506.10741v1",
    "authors": [
      "SiXiang Chen",
      "Jianyu Lai",
      "Jialin Gao",
      "Tian Ye",
      "Haoyu Chen",
      "Hengyu Shi",
      "Shitong Shao",
      "Yunlong Lin",
      "Song Fei",
      "Zhaohu Xing",
      "Yeying Jin",
      "Junfeng Luo",
      "Xiaoming Wei",
      "Lei Zhu"
    ],
    "published": "2025-06-12T14:28:12+00:00",
    "summary": "Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft"
  },
  {
    "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs",
    "url": "http://arxiv.org/abs/2506.10630v1",
    "arxiv_id": "2506.10630v1",
    "authors": [
      "Yucong Luo",
      "Yitong Zhou",
      "Mingyue Cheng",
      "Jiahao Wang",
      "Daoyu Wang",
      "Tingyue Pan",
      "Jintao Zhang"
    ],
    "published": "2025-06-12T12:15:50+00:00",
    "summary": "To advance time series forecasting (TSF), various methods have been proposed to improve prediction accuracy, evolving from statistical techniques to data-driven deep learning architectures. Despite their effectiveness, most existing methods still adhere to a fast thinking paradigm-relying on extracting historical patterns and mapping them to future values as their core modeling philosophy, lacking an explicit thinking process that incorporates intermediate time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1) have shown remarkable multi-step reasoning capabilities, offering an alternative way to overcome these issues. However, prompt engineering alone presents several limitations - including high computational cost, privacy risks, and limited capacity for in-depth domain-specific time series reasoning. To address these limitations, a more promising approach is to train LLMs to develop slow thinking capabilities and acquire strong time series reasoning skills. For this purpose, we propose Time-R1, a two-stage reinforcement fine-tuning framework designed to enhance multi-step reasoning ability of LLMs for time series forecasting. Specifically, the first stage conducts supervised fine-tuning for warmup adaptation, while the second stage employs reinforcement learning to improve the model's generalization ability. Particularly, we design a fine-grained multi-objective reward specifically for time series forecasting, and then introduce GRIP (group-based relative importance for policy optimization), which leverages non-uniform sampling to further encourage and optimize the model's exploration of effective reasoning paths. Experiments demonstrate that Time-R1 significantly improves forecast performance across diverse datasets."
  },
  {
    "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.10629v1",
    "arxiv_id": "2506.10629v1",
    "authors": [
      "Yucheng Yang",
      "Tianyi Zhou",
      "Qiang He",
      "Lei Han",
      "Mykola Pechenizkiy",
      "Meng Fang"
    ],
    "published": "2025-06-12T12:13:58+00:00",
    "summary": "Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can initialize a downstream task's policy. Our new theoretical analysis in this paper shows that the diversity and separability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee these properties. To complement MISL, we propose a novel disentanglement metric LSEPIN. Moreover, we build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. We extend the geometric analysis to it, which leads to a novel skill-learning objective WSEP. It is theoretically justified to be helpful to downstream task adaptation and it is capable of discovering more initial policies for downstream tasks than MISL. We finally propose another Wasserstein distance-based algorithm PWSEP that can theoretically discover all optimal initial policies."
  },
  {
    "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence",
    "url": "http://arxiv.org/abs/2506.10600v1",
    "arxiv_id": "2506.10600v1",
    "authors": [
      "Wang Xinjie",
      "Liu Liu",
      "Cao Yu",
      "Wu Ruiqi",
      "Qin Wenkang",
      "Wang Dehui",
      "Sui Wei",
      "Su Zhizhong"
    ],
    "published": "2025-06-12T11:43:50+00:00",
    "summary": "Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html."
  },
  {
    "title": "Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty",
    "url": "http://arxiv.org/abs/2506.10446v1",
    "arxiv_id": "2506.10446v1",
    "authors": [
      "Zehui Ling",
      "Deshu Chen",
      "Hongwei Zhang",
      "Yifeng Jiao",
      "Xin Guo",
      "Yuan Cheng"
    ],
    "published": "2025-06-12T07:49:24+00:00",
    "summary": "Large language models (LLMs) have demonstrated significant advancements in reasoning capabilities, performing well on various challenging benchmarks. Techniques like Chain-of-Thought prompting have been introduced to further improve reasoning. However, these approaches frequently generate longer outputs, which in turn increase computational latency. Although some methods use reinforcement learning to shorten reasoning, they often apply uniform penalties without considering the problem's complexity, leading to suboptimal outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by promoting conciseness for simpler problems while preserving sufficient reasoning for more complex ones for accuracy, thus improving the model's overall performance. Specifically, we manage the model's reasoning efficiency by dividing the reward function and including a novel penalty for output length. Our approach has yielded impressive outcomes in benchmark evaluations across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively simpler datasets GSM8K and MATH500, our method has effectively shortened output lengths while preserving or enhancing accuracy. On the more demanding AIME2024 dataset, our approach has resulted in improved accuracy."
  },
  {
    "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier",
    "url": "http://arxiv.org/abs/2506.10406v1",
    "arxiv_id": "2506.10406v1",
    "authors": [
      "Yuhua Jiang",
      "Yuwen Xiong",
      "Yufeng Yuan",
      "Chao Xin",
      "Wenyuan Xu",
      "Yu Yue",
      "Qianchuan Zhao",
      "Lin Yan"
    ],
    "published": "2025-06-12T06:59:35+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency."
  },
  {
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts",
    "url": "http://arxiv.org/abs/2506.10357v1",
    "arxiv_id": "2506.10357v1",
    "authors": [
      "Zaijing Li",
      "Yuquan Xie",
      "Rui Shao",
      "Gongwei Chen",
      "Weili Guan",
      "Dongmei Jiang",
      "Liqiang Nie"
    ],
    "published": "2025-06-12T05:29:40+00:00",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/"
  },
  {
    "title": "Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation",
    "url": "http://arxiv.org/abs/2506.10353v1",
    "arxiv_id": "2506.10353v1",
    "authors": [
      "Runqi Ouyang",
      "Haoyun Li",
      "Zhenyuan Zhang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Guan Huang",
      "Xingang Wang"
    ],
    "published": "2025-06-12T05:21:43+00:00",
    "summary": "Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the model's ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available."
  },
  {
    "title": "A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pok\u00e9mon",
    "url": "http://arxiv.org/abs/2506.10326v1",
    "arxiv_id": "2506.10326v1",
    "authors": [
      "Cameron Angliss",
      "Jiaxun Cui",
      "Jiaheng Hu",
      "Arrasy Rahman",
      "Peter Stone"
    ],
    "published": "2025-06-12T03:19:39+00:00",
    "summary": "Developing AI agents that can robustly adapt to dramatically different strategic landscapes without retraining is a central challenge for multi-agent learning. Pok\\'emon Video Game Championships (VGC) is a domain with an extraordinarily large space of possible team configurations of approximately $10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete, combinatorial nature of team building in Pok\\'emon VGC causes optimal strategies to shift dramatically depending on both the team being piloted and the opponent's team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies human-play datasets and a range of baselines - from large-language-model agents and behavior cloning to reinforcement learning and empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated on a single-team configuration, our methods are able to win against a professional VGC competitor. We extensively evaluated all baseline methods over progressively larger team sets and find that even the best-performing algorithm in the single-team setting struggles at scaling up as team size grows. Thus, policy generalization across diverse team strategies remains an open challenge for the community. Our code is open sourced at https://github.com/cameronangliss/VGC-Bench."
  },
  {
    "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks",
    "url": "http://arxiv.org/abs/2506.10287v1",
    "arxiv_id": "2506.10287v1",
    "authors": [
      "Rohit Sonker",
      "Alexandre Capone",
      "Andrew Rothstein",
      "Hiro Josep Farre Kaga",
      "Egemen Kolemen",
      "Jeff Schneider"
    ],
    "published": "2025-06-12T01:52:22+00:00",
    "summary": "Machine learning algorithms often struggle to control complex real-world systems. In the case of nuclear fusion, these challenges are exacerbated, as the dynamics are notoriously complex, data is poor, hardware is subject to failures, and experiments often affect dynamics beyond the experiment's duration. Existing tools like reinforcement learning, supervised learning, and Bayesian optimization address some of these challenges but fail to provide a comprehensive solution. To overcome these limitations, we present a multi-scale Bayesian optimization approach that integrates a high-frequency data-driven dynamics model with a low-frequency Gaussian process. By updating the Gaussian process between experiments, the method rapidly adapts to new data, refining the predictions of the less reliable dynamical model. We validate our approach by controlling tearing instabilities in the DIII-D nuclear fusion plant. Offline testing on historical data shows that our method significantly outperforms several baselines. Results on live experiments on the DIII-D tokamak, conducted under high-performance plasma scenarios prone to instabilities, shows a 50% success rate, marking a 117% improvement over historical outcomes."
  },
  {
    "title": "Wasserstein Barycenter Soft Actor-Critic",
    "url": "http://arxiv.org/abs/2506.10167v1",
    "arxiv_id": "2506.10167v1",
    "authors": [
      "Zahra Shahrooei",
      "Ali Baheri"
    ],
    "published": "2025-06-11T20:39:50+00:00",
    "summary": "Deep off-policy actor-critic algorithms have emerged as the leading framework for reinforcement learning in continuous control domains. However, most of these algorithms suffer from poor sample efficiency, especially in environments with sparse rewards. In this paper, we take a step towards addressing this issue by providing a principled directed exploration strategy. We propose Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from a pessimistic actor for temporal difference learning and an optimistic actor to promote exploration. This is achieved by using the Wasserstein barycenter of the pessimistic and optimistic policies as the exploration policy and adjusting the degree of exploration throughout the learning process. We compare WBSAC with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is more sample-efficient on MuJoCo continuous control tasks."
  },
  {
    "title": "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective",
    "url": "http://arxiv.org/abs/2506.10161v1",
    "arxiv_id": "2506.10161v1",
    "authors": [
      "Yi Wang",
      "Max Kreminski"
    ],
    "published": "2025-06-11T20:27:08+00:00",
    "summary": "Story generation has been a prominent application of Large Language Models (LLMs). However, understanding LLMs' ability to produce high-quality stories remains limited due to challenges in automatic evaluation methods and the high cost and subjectivity of manual evaluation. Computational narratology offers valuable insights into what constitutes a good story, which has been applied in the symbolic narrative planning approach to story generation. This work aims to deepen the understanding of LLMs' story generation capabilities by using them to solve narrative planning problems. We present a benchmark for evaluating LLMs on narrative planning based on literature examples, focusing on causal soundness, character intentionality, and dramatic conflict. Our experiments show that GPT-4 tier LLMs can generate causally sound stories at small scales, but planning with character intentionality and dramatic conflict remains challenging, requiring LLMs trained with reinforcement learning for complex reasoning. The results offer insights on the scale of stories that LLMs can generate while maintaining quality from different aspects. Our findings also highlight interesting problem solving behaviors and shed lights on challenges and considerations for applying LLM narrative planning in game environments."
  },
  {
    "title": "Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows",
    "url": "http://arxiv.org/abs/2506.10153v1",
    "arxiv_id": "2506.10153v1",
    "authors": [
      "Zhecheng Liu",
      "Jeff D. Eldredge"
    ],
    "published": "2025-06-11T20:14:28+00:00",
    "summary": "A linear flow control strategy designed for weak disturbances may not remain effective in sequences of strong disturbances due to nonlinear interactions, but it is sensible to leverage it for developing a better strategy. In the present study, we propose a transformer-based reinforcement learning (RL) framework to learn an effective control strategy for regulating aerodynamic lift in gust sequences via pitch control. The transformer addresses the challenge of partial observability from limited surface pressure sensors. We demonstrate that the training can be accelerated with two techniques -- pretraining with an expert policy (here, linear control) and task-level transfer learning (here, extending a policy trained on isolated gusts to multiple gusts). We show that the learned strategy outperforms the best proportional control, with the performance gap widening as the number of gusts increases. The control strategy learned in an environment with a small number of successive gusts is shown to effectively generalize to an environment with an arbitrarily long sequence of gusts. We investigate the pivot configuration and show that quarter-chord pitching control can achieve superior lift regulation with substantially less control effort compared to mid-chord pitching control. Through a decomposition of the lift, we attribute this advantage to the dominant added-mass contribution accessible via quarter-chord pitching. The success on multiple configurations shows the generalizability of the proposed transformer-based RL framework, which offers a promising approach to solve more computationally demanding flow control problems when combined with the proposed acceleration techniques."
  },
  {
    "title": "Unsupervised Elicitation of Language Models",
    "url": "http://arxiv.org/abs/2506.10139v1",
    "arxiv_id": "2506.10139v1",
    "authors": [
      "Jiaxin Wen",
      "Zachary Ankner",
      "Arushi Somani",
      "Peter Hase",
      "Samuel Marks",
      "Jacob Goldman-Wetzler",
      "Linda Petrini",
      "Henry Sleight",
      "Collin Burns",
      "He He",
      "Shi Feng",
      "Ethan Perez",
      "Jan Leike"
    ],
    "published": "2025-06-11T19:40:08+00:00",
    "summary": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \\emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts."
  },
  {
    "title": "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban",
    "url": "http://arxiv.org/abs/2506.10138v1",
    "arxiv_id": "2506.10138v1",
    "authors": [
      "Mohammad Taufeeque",
      "Aaron David Tucker",
      "Adam Gleave",
      "Adri\u00e0 Garriga-Alonso"
    ],
    "published": "2025-06-11T19:36:17+00:00",
    "summary": "We partially reverse-engineer a convolutional recurrent neural network (RNN) trained to play the puzzle game Sokoban with model-free reinforcement learning. Prior work found that this network solves more levels with more test-time compute. Our analysis reveals several mechanisms analogous to components of classic bidirectional search. For each square, the RNN represents its plan in the activations of channels associated with specific directions. These state-action activations are analogous to a value function - their magnitudes determine when to backtrack and which plan branch survives pruning. Specialized kernels extend these activations (containing plan and value) forward and backward to create paths, forming a transition model. The algorithm is also unlike classical search in some ways. State representation is not unified; instead, the network considers each box separately. Each layer has its own plan representation and value function, increasing search depth. Far from being inscrutable, the mechanisms leveraging test-time compute learned in this network by model-free training can be understood in familiar terms."
  },
  {
    "title": "Provable Sim-to-Real Transfer via Offline Domain Randomization",
    "url": "http://arxiv.org/abs/2506.10133v1",
    "arxiv_id": "2506.10133v1",
    "authors": [
      "Arnaud Fickinger",
      "Abderrahim Bendahi",
      "Stuart Russell"
    ],
    "published": "2025-06-11T19:22:33+00:00",
    "summary": "Reinforcement-learning agents often struggle when deployed from simulation to the real-world. A dominant strategy for reducing the sim-to-real gap is domain randomization (DR) which trains the policy across many simulators produced by sampling dynamics parameters, but standard DR ignores offline data already available from the real system. We study offline domain randomization (ODR), which first fits a distribution over simulator parameters to an offline dataset. While a growing body of empirical work reports substantial gains with algorithms such as DROPO, the theoretical foundations of ODR remain largely unexplored. In this work, we (i) formalize ODR as a maximum-likelihood estimation over a parametric simulator family, (ii) prove consistency of this estimator under mild regularity and identifiability conditions, showing it converges to the true dynamics as the dataset grows, (iii) derive gap bounds demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than uniform DR in the finite-simulator case (and analogous gains in the continuous setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an entropy bonus to prevent variance collapse, yielding broader randomization and more robust zero-shot transfer in practice."
  },
  {
    "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs",
    "url": "http://arxiv.org/abs/2506.10128v1",
    "arxiv_id": "2506.10128v1",
    "authors": [
      "Xiyao Wang",
      "Zhengyuan Yang",
      "Chao Feng",
      "Yongyuan Liang",
      "Yuhang Zhou",
      "Xiaoyu Liu",
      "Ziyi Zang",
      "Ming Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Linjie Li",
      "Furong Huang",
      "Lijuan Wang"
    ],
    "published": "2025-06-11T19:16:54+00:00",
    "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs."
  },
  {
    "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning",
    "url": "http://arxiv.org/abs/2506.10125v1",
    "arxiv_id": "2506.10125v1",
    "authors": [
      "Muqi Zou",
      "Hongyu Cai",
      "Hongwei Wu",
      "Zion Leonahenahe Basque",
      "Arslan Khan",
      "Berkay Celik",
      "Dave",
      "Tian",
      "Antonio Bianchi",
      "Ruoyu",
      "Wang",
      "Dongyan Xu"
    ],
    "published": "2025-06-11T19:09:08+00:00",
    "summary": "Decompilers, which reconstruct human-readable source code from binary executables, are vital to many security tasks. Yet, despite recent advances, their output often suffers from syntactic and semantic errors and remains difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals significant limitations, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LiFT, an automated decompiler backend that harnesses and further trains LLMs to improve the quality of decompiled code via reinforcement learning (RL). Unlike prior work that overlooks preserving accuracy, D-LiFT adheres to a key principle for enhancing the quality of decompiled code: \\textit{preserving accuracy while improving readability}. Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system to score the decompiled code from multiple aspects. In line with our principle, D-SCORE assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Specifically, D-SCORE first verifies the syntactic and semantic correctness via the compiler and symbolic execution; only if a candidate is deemed accurate, it then evaluates readability using established metrics to compare the LLM output with the original decompiled code. The score will then be fed back to the LLM for fine-tuning. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled functions, as measured by D-SCORE."
  },
  {
    "title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering",
    "url": "http://arxiv.org/abs/2506.10116v1",
    "arxiv_id": "2506.10116v1",
    "authors": [
      "Caijun Jia",
      "Nan Xu",
      "Jingxuan Wei",
      "Qingli Wang",
      "Lei Wang",
      "Bihui Yu",
      "Junnan Zhu"
    ],
    "published": "2025-06-11T18:55:36+00:00",
    "summary": "Recently, large language models have shown remarkable reasoning capabilities through long-chain reasoning before responding. However, how to extend this capability to visual reasoning tasks remains an open challenge. Existing multimodal reasoning approaches transfer such visual reasoning task into textual reasoning task via several image-to-text conversions, which often lose critical structural and semantic information embedded in visualizations, especially for tasks like chart question answering that require a large amount of visual details. To bridge this gap, we propose ChartReasoner, a code-driven novel two-stage framework designed to enable precise, interpretable reasoning over charts. We first train a high-fidelity model to convert diverse chart images into structured ECharts codes, preserving both layout and data semantics as lossless as possible. Then, we design a general chart reasoning data synthesis pipeline, which leverages this pretrained transport model to automatically and scalably generate chart reasoning trajectories and utilizes a code validator to filter out low-quality samples. Finally, we train the final multimodal model using a combination of supervised fine-tuning and reinforcement learning on our synthesized chart reasoning dataset and experimental results on four public benchmarks clearly demonstrate the effectiveness of our proposed ChartReasoner. It can preserve the original details of the charts as much as possible and perform comparably with state-of-the-art open-source models while using fewer parameters, approaching the performance of proprietary systems like GPT-4o in out-of-domain settings."
  },
  {
    "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2506.10100v1",
    "arxiv_id": "2506.10100v1",
    "authors": [
      "Yantai Yang",
      "Yuhao Wang",
      "Zichen Wen",
      "Luo Zhongwei",
      "Chang Zou",
      "Zhipeng Zhang",
      "Chuan Wen",
      "Linfeng Zhang"
    ],
    "published": "2025-06-11T18:34:57+00:00",
    "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark."
  },
  {
    "title": "Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy",
    "url": "http://arxiv.org/abs/2506.10073v1",
    "arxiv_id": "2506.10073v1",
    "authors": [
      "Malvern Madondo",
      "Yuan Shao",
      "Yingzi Liu",
      "Jun Zhou",
      "Xiaofeng Yang",
      "Zhen Tian"
    ],
    "published": "2025-06-11T18:00:06+00:00",
    "summary": "Anatomical changes during intensity-modulated proton therapy (IMPT) for head-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and organ-at-risk overdosing. As a result, treatment replanning is often required to maintain clinically acceptable treatment quality. However, current manual replanning processes are resource-intensive and time-consuming. We propose a patient-specific deep reinforcement learning (DRL) framework for automated IMPT replanning, with a reward-shaping mechanism based on a $150$-point plan quality score addressing competing clinical objectives. We formulate the planning process as an RL problem where agents learn control policies to adjust optimization priorities, maximizing plan quality. Unlike population-based approaches, our framework trains personalized agents for each patient using their planning CT (Computed Tomography) and augmented anatomies simulating anatomical changes (tumor progression and regression). This patient-specific approach leverages anatomical similarities throughout treatment, enabling effective plan adaptation. We implemented two DRL algorithms, Deep Q-Network and Proximal Policy Optimization, using dose-volume histograms (DVHs) as state representations and a $22$-dimensional action space of priority adjustments. Evaluation on five HNC patients using actual replanning CT data showed both DRL agents improved initial plan scores from $120.63 \\pm 21.40$ to $139.78 \\pm 6.84$ (DQN) and $142.74 \\pm 5.16$ (PPO), surpassing manual replans generated by a human planner ($137.20 \\pm 5.58$). Clinical validation confirms that improvements translate to better tumor coverage and OAR sparing across diverse anatomical changes. This work demonstrates DRL's potential in addressing geometric and dosimetric complexities of adaptive proton therapy, offering efficient offline adaptation solutions and advancing online adaptive proton therapy."
  },
  {
    "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
    "url": "http://arxiv.org/abs/2506.09996v1",
    "arxiv_id": "2506.09996v1",
    "authors": [
      "Yang Li",
      "Qiang Sheng",
      "Yehan Yang",
      "Xueyao Zhang",
      "Juan Cao"
    ],
    "published": "2025-06-11T17:59:58+00:00",
    "summary": "Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Then, we propose the streaming content monitor, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO."
  },
  {
    "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs",
    "url": "http://arxiv.org/abs/2506.10054v1",
    "arxiv_id": "2506.10054v1",
    "authors": [
      "Shangpin Peng",
      "Weinong Wang",
      "Zhuotao Tian",
      "Senqiao Yang",
      "Xing Wu",
      "Haotian Xu",
      "Chengquan Zhang",
      "Takashi Isobe",
      "Baotian Hu",
      "Min Zhang"
    ],
    "published": "2025-06-11T17:58:05+00:00",
    "summary": "Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO."
  },
  {
    "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
    "url": "http://arxiv.org/abs/2506.09965v1",
    "arxiv_id": "2506.09965v1",
    "authors": [
      "Junfei Wu",
      "Jian Guan",
      "Kaituo Feng",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang",
      "Wei Wu",
      "Tieniu Tan"
    ],
    "published": "2025-06-11T17:41:50+00:00",
    "summary": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%."
  },
  {
    "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following",
    "url": "http://arxiv.org/abs/2506.09942v1",
    "arxiv_id": "2506.09942v1",
    "authors": [
      "Hao Peng",
      "Yunjia Qi",
      "Xiaozhi Wang",
      "Bin Xu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "published": "2025-06-11T17:10:36+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF."
  },
  {
    "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability",
    "url": "http://arxiv.org/abs/2506.09940v1",
    "arxiv_id": "2506.09940v1",
    "authors": [
      "Jiachen Hu",
      "Rui Ai",
      "Han Zhong",
      "Xiaoyu Chen",
      "Liwei Wang",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ],
    "published": "2025-06-11T17:06:57+00:00",
    "summary": "Information asymmetry is a pervasive feature of multi-agent systems, especially evident in economics and social sciences. In these settings, agents tailor their actions based on private information to maximize their rewards. These strategic behaviors often introduce complexities due to confounding variables. Simultaneously, knowledge transportability poses another significant challenge, arising from the difficulties of conducting experiments in target environments. It requires transferring knowledge from environments where empirical data is more readily available. Against these backdrops, this paper explores a fundamental question in online learning: Can we employ non-i.i.d. actions to learn about confounders even when requiring knowledge transfer? We present a sample-efficient algorithm designed to accurately identify system dynamics under information asymmetry and to navigate the challenges of knowledge transfer effectively in reinforcement learning, framed within an online strategic interaction model. Our method provably achieves learning of an $\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$."
  },
  {
    "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)",
    "url": "http://arxiv.org/abs/2506.09901v1",
    "arxiv_id": "2506.09901v1",
    "authors": [
      "Noel Brindise",
      "Vijeth Hebbar",
      "Riya Shah",
      "Cedric Langbort"
    ],
    "published": "2025-06-11T16:15:56+00:00",
    "summary": "In this work, we provide an extended discussion of a new approach to explainable Reinforcement Learning called Diverse Near-Optimal Alternatives (DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for trajectory-planning agents, optimizing policies to produce qualitatively diverse trajectories in Euclidean space. In the spirit of explainability, these distinct policies are used to \"explain\" an agent's options in terms of available trajectory shapes from which a human user may choose. In particular, DNA applies to value function-based policies on Markov decision processes where agents are limited to continuous trajectories. Here, we describe DNA, which uses reward shaping in local, modified Q-learning problems to solve for distinct policies with guaranteed epsilon-optimality. We show that it successfully returns qualitatively different policies that constitute meaningfully different \"options\" in simulation, including a brief comparison to related approaches in the stochastic optimization field of Quality Diversity. Beyond the explanatory motivation, this work opens new possibilities for exploration and adaptive planning in RL."
  },
  {
    "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints",
    "url": "http://arxiv.org/abs/2506.09859v1",
    "arxiv_id": "2506.09859v1",
    "authors": [
      "Huajian Liu",
      "Yixuan Feng",
      "Wei Dong",
      "Kunpeng Fan",
      "Chao Wang",
      "Yongzhuo Gao"
    ],
    "published": "2025-06-11T15:31:25+00:00",
    "summary": "In this paper, we propose a novel hierarchical framework for robot navigation in dynamic environments with heterogeneous constraints. Our approach leverages a graph neural network trained via reinforcement learning (RL) to efficiently estimate the robot's cost-to-go, formulated as local goal recommendations. A spatio-temporal path-searching module, which accounts for kinematic constraints, is then employed to generate a reference trajectory to facilitate solving the non-convex optimization problem used for explicit constraint enforcement. More importantly, we introduce an incremental action-masking mechanism and a privileged learning strategy, enabling end-to-end training of the proposed planner. Both simulation and real-world experiments demonstrate that the proposed method effectively addresses local planning in complex dynamic environments, achieving state-of-the-art (SOTA) performance. Compared with existing learning-optimization hybrid methods, our approach eliminates the dependency on high-fidelity simulation environments, offering significant advantages in computational efficiency and training scalability. The code will be released as open-source upon acceptance of the paper."
  },
  {
    "title": "Foundation Model-Aided Deep Reinforcement Learning for RIS-Assisted Wireless Communication",
    "url": "http://arxiv.org/abs/2506.09855v1",
    "arxiv_id": "2506.09855v1",
    "authors": [
      "Mohammad Ghassemi",
      "Sara Farrag Mobarak",
      "Han Zhang",
      "Ali Afana",
      "Akram Bin Sediq",
      "Melike Erol-Kantarci"
    ],
    "published": "2025-06-11T15:27:03+00:00",
    "summary": "Reconfigurable intelligent surfaces (RIS) have emerged as a promising technology for enhancing wireless communication by dynamically controlling signal propagation in the environment. However, their efficient deployment relies on accurate channel state information (CSI), which leads to high channel estimation overhead due to their passive nature and the large number of reflective elements. In this work, we solve this challenge by proposing a novel framework that leverages a pre-trained open-source foundation model (FM) named large wireless model (LWM) to process wireless channels and generate versatile and contextualized channel embeddings. These embeddings are then used for the joint optimization of the BS beamforming and RIS configurations. To be more specific, for joint optimization, we design a deep reinforcement learning (DRL) model to automatically select the BS beamforming vector and RIS phase-shift matrix, aiming to maximize the spectral efficiency (SE). This work shows that a pre-trained FM for radio signal understanding can be fine-tuned and integrated with DRL for effective decision-making in wireless networks. It highlights the potential of modality-specific FMs in real-world network optimization. According to the simulation results, the proposed method outperforms the DRL-based approach and beam sweeping-based approach, achieving 9.89% and 43.66% higher SE, respectively."
  },
  {
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "url": "http://arxiv.org/abs/2506.09839v1",
    "arxiv_id": "2506.09839v1",
    "authors": [
      "Chen Gao",
      "Liankai Jin",
      "Xingyu Peng",
      "Jiazhao Zhang",
      "Yue Deng",
      "Annan Li",
      "He Wang",
      "Si Liu"
    ],
    "published": "2025-06-11T15:15:17+00:00",
    "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods."
  },
  {
    "title": "CoRT: Code-integrated Reasoning within Thinking",
    "url": "http://arxiv.org/abs/2506.09820v1",
    "arxiv_id": "2506.09820v1",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-06-11T14:59:02+00:00",
    "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\\% and 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT."
  },
  {
    "title": "CoRT: Code-integrated Reasoning within Thinking",
    "url": "http://arxiv.org/abs/2506.09820v2",
    "arxiv_id": "2506.09820v2",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "published": "2025-06-11T14:59:02+00:00",
    "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\\% and 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT."
  },
  {
    "title": "Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy",
    "url": "http://arxiv.org/abs/2506.09805v1",
    "arxiv_id": "2506.09805v1",
    "authors": [
      "Tonghe Wang",
      "Yining Feng",
      "Xiaofeng Yang"
    ],
    "published": "2025-06-11T14:46:42+00:00",
    "summary": "Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the pattern of needle placement solely relies on physician experience. We investigated the feasibility of using reinforcement learning (RL) to provide needle positions and dwell times based on patient anatomy during pre-planning stage. This approach would reduce procedure time and ensure consistent plan quality. Materials and Methods: We train a RL agent to adjust the position of one selected needle and all the dwell times on it to maximize a pre-defined reward function after observing the environment. After adjusting, the RL agent then moves on to the next needle, until all needles are adjusted. Multiple rounds are played by the agent until the maximum number of rounds is reached. Plan data from 11 prostate HDR boost patients (1 for training, and 10 for testing) treated in our clinic were included in this study. The dosimetric metrics and the number of used needles of RL plan were compared to those of the clinical results (ground truth). Results: On average, RL plans and clinical plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no statistical significance), while RL plans have less prostate hotspot (Prostate V150) and Urethra D20% plans with statistical significance. Moreover, RL plans use 2 less needles than clinical plan on average. Conclusion: We present the first study demonstrating the feasibility of using reinforcement learning to autonomously generate clinically practical HDR prostate brachytherapy plans. This RL-based method achieved equal or improved plan quality compared to conventional clinical approaches while requiring fewer needles. With minimal data requirements and strong generalizability, this approach has substantial potential to standardize brachytherapy planning, reduce clinical variability, and enhance patient outcomes."
  },
  {
    "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.09800v1",
    "arxiv_id": "2506.09800v1",
    "authors": [
      "Haochen Liu",
      "Tianyu Li",
      "Haohan Yang",
      "Li Chen",
      "Caojun Wang",
      "Ke Guo",
      "Haochen Tian",
      "Hongchen Li",
      "Hongyang Li",
      "Chen Lv"
    ],
    "published": "2025-06-11T14:42:11+00:00",
    "summary": "End-to-end autonomous driving has emerged as a promising paradigm for directly mapping sensor inputs to planning maneuvers using learning-based modular integrations. However, existing imitation learning (IL)-based models suffer from generalization to hard cases, and a lack of corrective feedback loop under post-deployment. While reinforcement learning (RL) offers a potential solution to tackle hard cases with optimality, it is often hindered by overfitting to specific driving cases, resulting in catastrophic forgetting of generalizable knowledge and sample inefficiency. To overcome these challenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE), a novel learning pipeline that constantly refines hard domain while keeping generalizable driving policy for model-agnostic end-to-end driving systems. Through reinforcement fine-tuning and policy expansion that facilitates continuous improvement, R2SE features three key components: 1) Generalist Pretraining with hard-case allocation trains a generalist imitation learning (IL) driving system while dynamically identifying failure-prone cases for targeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes residual corrections using reinforcement learning (RL) to improve performance in hard case domain while preserving global driving knowledge; 3) Self-aware Adapter Expansion dynamically integrates specialist policies back into the generalist model, enhancing continuous performance improvement. Experimental results in closed-loop simulation and real-world datasets demonstrate improvements in generalization, safety, and long-horizon policy robustness over state-of-the-art E2E systems, highlighting the effectiveness of reinforce refinement for scalable autonomous driving."
  },
  {
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "url": "http://arxiv.org/abs/2506.09790v1",
    "arxiv_id": "2506.09790v1",
    "authors": [
      "Zhenran Xu",
      "Yiyu Wang",
      "Xue Yang",
      "Longyue Wang",
      "Weihua Luo",
      "Kaifu Zhang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "published": "2025-06-11T14:35:15+00:00",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation."
  },
  {
    "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning",
    "url": "http://arxiv.org/abs/2506.09736v1",
    "arxiv_id": "2506.09736v1",
    "authors": [
      "Yuting Li",
      "Lai Wei",
      "Kaipeng Zheng",
      "Jingyuan Huang",
      "Linghe Kong",
      "Lichao Sun",
      "Weiran Huang"
    ],
    "published": "2025-06-11T13:39:46+00:00",
    "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters."
  },
  {
    "title": "Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error",
    "url": "http://arxiv.org/abs/2506.09685v1",
    "arxiv_id": "2506.09685v1",
    "authors": [
      "Armin Gie\u00dfler",
      "Albertus Johannes Malan",
      "S\u00f6ren Hohmann"
    ],
    "published": "2025-06-11T13:02:21+00:00",
    "summary": "In this paper, we present a novel method for computing the optimal feedback gain of the infinite-horizon Linear Quadratic Regulator (LQR) problem via an ordinary differential equation. We introduce a novel continuous-time Bellman error, derived from the Hamilton-Jacobi-Bellman (HJB) equation, which quantifies the suboptimality of stabilizing policies and is parametrized in terms of the feedback gain. We analyze its properties, including its effective domain, smoothness, coerciveness and show the existence of a unique stationary point within the stability region. Furthermore, we derive a closed-form gradient expression of the Bellman error that induces a gradient flow. This converges to the optimal feedback and generates a unique trajectory which exclusively comprises stabilizing feedback policies. Additionally, this work advances interesting connections between LQR theory and Reinforcement Learning (RL) by redefining suboptimality of the Algebraic Riccati Equation (ARE) as a Bellman error, adapting a state-independent formulation, and leveraging Lyapunov equations to overcome the infinite-horizon challenge. We validate our method in a simulation and compare it to the state of the art."
  },
  {
    "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model",
    "url": "http://arxiv.org/abs/2506.09659v1",
    "arxiv_id": "2506.09659v1",
    "authors": [
      "Eltayeb Ahmed",
      "Uljad Berdica",
      "Martha Elliott",
      "Danijela Horak",
      "Jakob N. Foerster"
    ],
    "published": "2025-06-11T12:26:45+00:00",
    "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications."
  },
  {
    "title": "Attention-Based Map Encoding for Learning Generalized Legged Locomotion",
    "url": "http://arxiv.org/abs/2506.09588v1",
    "arxiv_id": "2506.09588v1",
    "authors": [
      "Junzhe He",
      "Chong Zhang",
      "Fabian Jenelten",
      "Ruben Grandia",
      "Moritz B\u00c4cher",
      "Marco Hutter"
    ],
    "published": "2025-06-11T10:38:59+00:00",
    "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in expanding the operational range of mobile robots. It requires precise planning when possible footholds are sparse, robustness against uncertainties and disturbances, and generalizability across diverse terrains. While traditional model-based controllers excel at planning on complex terrains, they struggle with real-world uncertainties. Learning-based controllers offer robustness to such uncertainties but often lack precision on terrains with sparse steppable areas. Hybrid methods achieve enhanced robustness on sparse terrains by combining both methods but are computationally demanding and constrained by the inherent limitations of model-based planners. To achieve generalized legged locomotion on diverse terrains while preserving the robustness of learning-based controllers, this paper proposes to learn an attention-based map encoding conditioned on robot proprioception, which is trained as part of the end-to-end controller using reinforcement learning. We show that the network learns to focus on steppable areas for future footholds when the robot dynamically navigates diverse and challenging terrains. We synthesize behaviors that exhibit robustness against uncertainties while enabling precise and agile traversal of sparse terrains. Additionally, our method offers a way to interpret the topographical perception of a neural network. We have trained two controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot respectively and tested the resulting controllers in the real world under various challenging indoor and outdoor scenarios, including ones unseen during training."
  },
  {
    "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09574v1",
    "arxiv_id": "2506.09574v1",
    "authors": [
      "Gaurav Chaudhary",
      "Wassim Uddin Mondal",
      "Laxmidhar Behera"
    ],
    "published": "2025-06-11T10:12:50+00:00",
    "summary": "Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design components and added computational complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without added complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios."
  },
  {
    "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09562v1",
    "arxiv_id": "2506.09562v1",
    "authors": [
      "Songze Li",
      "Mingxuan Zhang",
      "Oubo Ma",
      "Kang Wei",
      "Shouling Ji"
    ],
    "published": "2025-06-11T09:50:17+00:00",
    "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide range of sequential decision-making domains, including robotics, healthcare, smart grids, and finance. Recent research demonstrates that attackers can efficiently exploit system vulnerabilities during the training phase to execute backdoor attacks, producing malicious actions when specific trigger patterns are present in the state observations. However, most existing backdoor attacks rely primarily on simplistic and heuristic trigger configurations, overlooking the potential efficacy of trigger optimization. To address this gap, we introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor Attacks on DRL), the first framework to systematically optimize DRL backdoor triggers along three critical axes, i.e., temporal, spatial, and magnitude. Specifically, we first introduce a performance-aware adaptive freezing mechanism for injection timing. Then, we formulate dimension selection as a cooperative game, utilizing Shapley value analysis to identify the most influential state variable for the injection dimension. Furthermore, we propose a gradient-based adversarial procedure to optimize the injection magnitude under environment constraints. Evaluations on three mainstream DRL algorithms and nine benchmark tasks show that TooBadRL significantly improves attack success rates, while ensuring minimal degradation of normal task performance. These results highlight the previously underappreciated importance of principled trigger optimization in DRL backdoor attacks. The source code of TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL."
  },
  {
    "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09562v2",
    "arxiv_id": "2506.09562v2",
    "authors": [
      "Songze Li",
      "Mingxuan Zhang",
      "Kang Wei",
      "Shouling Ji"
    ],
    "published": "2025-06-11T09:50:17+00:00",
    "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide range of sequential decision-making domains, including robotics, healthcare, smart grids, and finance. Recent research demonstrates that attackers can efficiently exploit system vulnerabilities during the training phase to execute backdoor attacks, producing malicious actions when specific trigger patterns are present in the state observations. However, most existing backdoor attacks rely primarily on simplistic and heuristic trigger configurations, overlooking the potential efficacy of trigger optimization. To address this gap, we introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor Attacks on DRL), the first framework to systematically optimize DRL backdoor triggers along three critical axes, i.e., temporal, spatial, and magnitude. Specifically, we first introduce a performance-aware adaptive freezing mechanism for injection timing. Then, we formulate dimension selection as a cooperative game, utilizing Shapley value analysis to identify the most influential state variable for the injection dimension. Furthermore, we propose a gradient-based adversarial procedure to optimize the injection magnitude under environment constraints. Evaluations on three mainstream DRL algorithms and nine benchmark tasks show that TooBadRL significantly improves attack success rates, while ensuring minimal degradation of normal task performance. These results highlight the previously underappreciated importance of principled trigger optimization in DRL backdoor attacks. The source code of TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL."
  },
  {
    "title": "GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models",
    "url": "http://arxiv.org/abs/2506.10047v1",
    "arxiv_id": "2506.10047v1",
    "authors": [
      "Zilong Wang",
      "Xiang Zheng",
      "Xiaosen Wang",
      "Bo Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "published": "2025-06-11T09:09:12+00:00",
    "summary": "Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and are now widely used in content creation. However, these models can be misused to generate harmful content, including nudity or violence, posing significant safety risks. While most platforms employ content moderation systems, underlying vulnerabilities can still be exploited by determined adversaries. Recent research on red-teaming and adversarial attacks against T2I models has notable limitations: some studies successfully generate highly toxic images but use adversarial prompts that are easily detected and blocked by safety filters, while others focus on bypassing safety mechanisms but fail to produce genuinely harmful outputs, neglecting the discovery of truly high-risk prompts. Consequently, there remains a lack of reliable tools for evaluating the safety of defended T2I models. To address this gap, we propose GenBreak, a framework that fine-tunes a red-team large language model (LLM) to systematically explore underlying vulnerabilities in T2I generators. Our approach combines supervised fine-tuning on curated datasets with reinforcement learning via interaction with a surrogate T2I model. By integrating multiple reward signals, we guide the LLM to craft adversarial prompts that enhance both evasion capability and image toxicity, while maintaining semantic coherence and diversity. These prompts demonstrate strong effectiveness in black-box attacks against commercial T2I generators, revealing practical and concerning safety weaknesses."
  },
  {
    "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications",
    "url": "http://arxiv.org/abs/2506.09512v1",
    "arxiv_id": "2506.09512v1",
    "authors": [
      "Donglin Wang",
      "Anjie Qiu",
      "Qiuheng Zhou",
      "Hans D. Schotten"
    ],
    "published": "2025-06-11T08:36:18+00:00",
    "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is transforming Intelligent Transportation Systems (ITS), with 6G networks expected to provide ultra-reliable, low-latency, and high-capacity connectivity for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and Machine Learning (ML) have emerged as key enablers in optimizing V2X communication by enhancing network management, predictive analytics, security, and cooperative driving due to their outstanding performance across various domains, such as natural language processing and computer vision. This survey comprehensively reviews recent advances in AI and ML models applied to 6G-V2X communication. It focuses on state-of-the-art techniques, including Deep Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and Federated Learning (FL), with particular emphasis on developments from the past two years. Notably, AI, especially GL, has shown remarkable progress and emerging potential in enhancing the performance, adaptability, and intelligence of 6G-V2X systems. Despite these advances, a systematic summary of recent research efforts in this area remains lacking, which this survey aims to address. We analyze their roles in 6G-V2X applications, such as intelligent resource allocation, beamforming, intelligent traffic management, and security management. Furthermore, we explore the technical challenges, including computational complexity, data privacy, and real-time decision-making constraints, while identifying future research directions for AI-driven 6G-V2X development. This study aims to provide valuable insights for researchers, engineers, and policymakers working towards realizing intelligent, AI-powered V2X ecosystems in 6G communication."
  },
  {
    "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design",
    "url": "http://arxiv.org/abs/2506.09508v1",
    "arxiv_id": "2506.09508v1",
    "authors": [
      "Andreas Schlaginhaufen",
      "Reda Ouhamma",
      "Maryam Kamgarpour"
    ],
    "published": "2025-06-11T08:27:16+00:00",
    "summary": "We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries."
  },
  {
    "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes",
    "url": "http://arxiv.org/abs/2506.09499v1",
    "arxiv_id": "2506.09499v1",
    "authors": [
      "Thomas J. Ringstrom",
      "Paul R. Schrater"
    ],
    "published": "2025-06-11T08:21:22+00:00",
    "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free Markov Decision Process. Rather than a value function, OKBEs directly construct and optimize a predictive map called a state-time option kernel (STOK) to maximize the probability of completing a goal while avoiding constraint violations. STOKs are compositional, modular, and interpretable initiation-to-termination transition kernels for policies in the Options Framework of Reinforcement Learning. This means: 1) STOKs can be composed using Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple policies over long horizons, 2) high-dimensional STOKs can be represented and computed efficiently in a factorized and reconfigurable form, and 3) STOKs record the probabilities of semantically interpretable goal-success and constraint-violation events, needed for formal verification. Given a high-dimensional state-transition model for an intractable planning problem, we can decompose it with local STOKs and goal-conditioned policies that are aggregated into a factorized goal kernel, making it possible to forward-plan at the level of goals in high-dimensions to solve the problem. These properties lead to highly flexible agents that can rapidly synthesize meta-policies, reuse planning representations across many tasks, and justify goals using empowerment, an intrinsic motivation function. We argue that reward-maximization is in conflict with the properties of compositionality, modularity, and interpretability. Alternatively, OKBEs facilitate these properties to support verifiable long-horizon planning and intrinsic motivation that scales to dynamic high-dimensional world-models."
  },
  {
    "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2506.09496v1",
    "arxiv_id": "2506.09496v1",
    "authors": [
      "Dingyi Rong",
      "Haotian Lu",
      "Wenzhuo Zheng",
      "Fan Zhang",
      "Shuangjia Zheng",
      "Ning Liu"
    ],
    "published": "2025-06-11T08:12:26+00:00",
    "summary": "Designing protein sequences with optimal energetic stability is a key challenge in protein inverse folding, as current deep learning methods are primarily trained by maximizing sequence recovery rates, often neglecting the energy of the generated sequences. This work aims to overcome this limitation by developing a model that directly generates low-energy, stable protein sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused on generating low-energy, high-stability protein sequences. Our core innovation lies in: First, integrating Markov Bridges with Direct Preference Optimization (DPO), where energy-based preferences are used to fine-tune the Markov Bridge model. The Markov Bridge initiates optimization from an information-rich prior sequence, providing DPO with a pool of structurally plausible sequence candidates. Second, an explicit energy constraint loss is introduced, which enhances the energy-driven nature of DPO based on prior sequences, enabling the model to effectively learn energy representations from a wealth of prior knowledge and directly predict sequence energy values, thereby capturing quantitative features of the energy landscape. Our evaluations demonstrate that EnerBridge-DPO can design protein complex sequences with lower energy while maintaining sequence recovery rates comparable to state-of-the-art models, and accurately predicts $\\Delta \\Delta G$ values between various sequences."
  },
  {
    "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning",
    "url": "http://arxiv.org/abs/2506.09473v1",
    "arxiv_id": "2506.09473v1",
    "authors": [
      "Cheng Chen",
      "Yunpeng Zhai",
      "Yifan Zhao",
      "Jinyang Gao",
      "Bolin Ding",
      "Jia Li"
    ],
    "published": "2025-06-11T07:38:12+00:00",
    "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims at enhancing the performance of large language models by providing clear task guidance and examples, improving their capability in task understanding and execution. This paper investigates ICL on Large Vision-Language Models (LVLMs) and explores the policies of multi-modal demonstration selection. Existing research efforts in ICL face significant challenges: First, they rely on pre-defined demonstrations or heuristic selecting strategies based on human intuition, which are usually inadequate for covering diverse task requirements, leading to sub-optimal solutions; Second, individually selecting each demonstration fails in modeling the interactions between them, resulting in information redundancy. Unlike these prevailing efforts, we propose a new exploration-exploitation reinforcement learning framework, which explores policies to fuse multi-modal information and adaptively select adequate demonstrations as an integrated whole. The framework allows LVLMs to optimize themselves by continually refining their demonstrations through self-exploration, enabling the ability to autonomously identify and generate the most effective selection policies for in-context learning. Experimental results verify the superior performance of our approach on four Visual Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing the generalization capability of few-shot LVLMs."
  },
  {
    "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms",
    "url": "http://arxiv.org/abs/2506.09457v1",
    "arxiv_id": "2506.09457v1",
    "authors": [
      "Zeguan Xiao",
      "Yun Chen",
      "Guanhua Chen"
    ],
    "published": "2025-06-11T07:02:18+00:00",
    "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the \"reward-generation gap\" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with POET, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all positions, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs."
  },
  {
    "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?",
    "url": "http://arxiv.org/abs/2506.09434v1",
    "arxiv_id": "2506.09434v1",
    "authors": [
      "Michael Amir",
      "Matteo Bettini",
      "Amanda Prorok"
    ],
    "published": "2025-06-11T06:33:55+00:00",
    "summary": "The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, our goal is to study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneous Environment Design (HED), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Experiments in matrix games and an embodied Multi-Goal-Capture environment show that, despite the difference in settings, HED rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HED and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit."
  },
  {
    "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization",
    "url": "http://arxiv.org/abs/2506.09404v1",
    "arxiv_id": "2506.09404v1",
    "authors": [
      "Shengda Gu",
      "Kai Li",
      "Junliang Xing",
      "Yifan Zhang",
      "Jian Cheng"
    ],
    "published": "2025-06-11T05:17:30+00:00",
    "summary": "Combinatorial optimization problems are notoriously challenging due to their discrete structure and exponentially large solution space. Recent advances in deep reinforcement learning (DRL) have enabled the learning heuristics directly from data. However, DRL methods often suffer from limited exploration and susceptibility to local optima. On the other hand, evolutionary algorithms such as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but are typically sample inefficient and computationally intensive. In this work, we propose the Evolutionary Augmentation Mechanism (EAM), a general and plug-and-play framework that synergizes the learning efficiency of DRL with the global search power of GAs. EAM operates by generating solutions from a learned policy and refining them through domain-specific genetic operations such as crossover and mutation. These evolved solutions are then selectively reinjected into the policy training loop, thereby enhancing exploration and accelerating convergence. We further provide a theoretical analysis that establishes an upper bound on the KL divergence between the evolved solution distribution and the policy distribution, ensuring stable and effective policy updates. EAM is model-agnostic and can be seamlessly integrated with state-of-the-art DRL solvers such as the Attention Model, POMO, and SymNCO. Extensive results on benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM significantly improves both solution quality and training efficiency over competitive baselines."
  },
  {
    "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization",
    "url": "http://arxiv.org/abs/2506.09373v1",
    "arxiv_id": "2506.09373v1",
    "authors": [
      "Jiaqi Tang",
      "Yu Xia",
      "Yi-Feng Wu",
      "Yuwei Hu",
      "Yuhui Chen",
      "Qing-Guo Chen",
      "Xiaogang Xu",
      "Xiangyu Wu",
      "Hao Lu",
      "Yanqing Ma",
      "Shiyin Lu",
      "Qifeng Chen"
    ],
    "published": "2025-06-11T03:43:30+00:00",
    "summary": "The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, it further introduces a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPO's superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon, at https://github.com/AIDC-AI/LPO."
  },
  {
    "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending",
    "url": "http://arxiv.org/abs/2506.09366v1",
    "arxiv_id": "2506.09366v1",
    "authors": [
      "Yuxuan Kuang",
      "Haoran Geng",
      "Amine Elhafsi",
      "Tan-Dzung Do",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Marco Pavone",
      "Yue Wang"
    ],
    "published": "2025-06-11T03:24:26+00:00",
    "summary": "Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/."
  },
  {
    "title": "ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)",
    "url": "http://arxiv.org/abs/2506.09365v1",
    "arxiv_id": "2506.09365v1",
    "authors": [
      "Ronal Singh",
      "Mohan Baruwal Chhetri",
      "Surya Nepal",
      "Cecile Paris"
    ],
    "published": "2025-06-11T03:24:18+00:00",
    "summary": "Modern Security Operations Centres (SOCs) integrate diverse tools, such as SIEM, IDS, and XDR systems, offering rich contextual data, including alert enrichments, flow features, and similar case histories. Yet, analysts must still manually determine which of these contextual cues are most relevant when validating specific alerts. We introduce ContextBuddy, an AI assistant that learns from analysts' prior investigations to help them identify the most relevant context for new alerts. Rather than providing enrichments, ContextBuddy models how analysts have previously selected context and suggests tailored cues based on the characteristics of each alert. We formulate context selection as a sequential decision-making problem and apply imitation learning (IL) to capture analysts' strategies, evaluating multiple IL approaches. Through staged evaluation, we validate ContextBuddy using two intrusion detection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments, ContextBuddy helped simulated reinforcement learning analysts improve classification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9% for UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and keeping false positives below 1%. Decision confidence among agents also improved by 2-3% (p < 0.001). In a within-subject user study (N=13; power = 0.8), non-experts using ContextBuddy improved classification accuracy by 21.1% (p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results demonstrate that by learning context-selection patterns from analysts, ContextBuddy can yield notable improvements in investigation effectiveness and efficiency."
  },
  {
    "title": "RePO: Replay-Enhanced Policy Optimization",
    "url": "http://arxiv.org/abs/2506.09340v1",
    "arxiv_id": "2506.09340v1",
    "authors": [
      "Siheng Li",
      "Zhanhui Zhou",
      "Wai Lam",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "published": "2025-06-11T02:44:10+00:00",
    "summary": "Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15\\%$ while raising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at https://github.com/SihengLi99/RePO."
  },
  {
    "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation",
    "url": "http://arxiv.org/abs/2506.09331v1",
    "arxiv_id": "2506.09331v1",
    "authors": [
      "Arjun Vaithilingam Sudhakar"
    ],
    "published": "2025-06-11T02:12:34+00:00",
    "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction."
  },
  {
    "title": "Towards Efficient and Effective Alignment of Large Language Models",
    "url": "http://arxiv.org/abs/2506.09329v1",
    "arxiv_id": "2506.09329v1",
    "authors": [
      "Yuxin Jiang"
    ],
    "published": "2025-06-11T02:08:52+00:00",
    "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current models' constraint adherence, offering insights for future improvements."
  },
  {
    "title": "Toward Scalable Quantum Compilation for Modular Architecture: Qubit Mapping and Reuse via Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09323v1",
    "arxiv_id": "2506.09323v1",
    "authors": [
      "Sokea Sang",
      "Leanghok Hour",
      "Youngsun Han"
    ],
    "published": "2025-06-11T01:52:17+00:00",
    "summary": "Modular quantum architectures have emerged as a promising approach for scaling quantum computing systems by connecting multiple Quantum Processing Units (QPUs). However, this approach introduces significant challenges due to costly inter-core operations between chips and quantum state transfers, which contribute to noise and quantum decoherence. This paper presents QARMA, a novel Qubit mapping using Attention-based deep Reinforcement learning (DRL) for Modular quantum Architectures, along with its extension QARMA-R that incorporates dynamic qubit reuse capabilities. Our approach combines an attention-based mechanism with Graph Neural Networks (GNN) to learn optimal qubit allocation, routing, and reuse strategies that minimize inter-core communications. We introduce two key innovations: (1) a transformer-based encoder that captures both the global circuit structure and local qubit interactions and (2) a dynamic qubit reuse compilation mechanism that leverages mid-circuit measurement and reset operations to reduce inter-operation and qubit requirements. Our experimental results show significant improvements over state-of-the-art approaches. Compared to highly-optimized Qiskit with modular architecture configuration, QARMA-R reduces inter-core communications by up to 100% (on average 85%), while QARMA maintains 15-40% improvement for larger circuits without reuse. Against traditional modular qubit mapping, our approach achieves 96.4-100% reduction in inter-core operation. The proposed methods advance quantum circuit compilation techniques and enable the execution of more extensive quantum algorithms on resource-constrained modular quantum systems, contributing to the growing body of research on scalable quantum computing architectures."
  },
  {
    "title": "Learning The Minimum Action Distance",
    "url": "http://arxiv.org/abs/2506.09276v1",
    "arxiv_id": "2506.09276v1",
    "authors": [
      "Lorenzo Steccanella",
      "Joshua B. Evans",
      "\u00d6zg\u00fcr \u015eim\u015fek",
      "Anders Jonsson"
    ],
    "published": "2025-06-10T22:27:11+00:00",
    "summary": "This paper presents a state representation framework for Markov decision processes (MDPs) that can be learned solely from state trajectories, requiring neither reward signals nor the actions executed by the agent. We propose learning the minimum action distance (MAD), defined as the minimum number of actions required to transition between states, as a fundamental metric that captures the underlying structure of an environment. MAD naturally enables critical downstream tasks such as goal-conditioned reinforcement learning and reward shaping by providing a dense, geometrically meaningful measure of progress. Our self-supervised learning approach constructs an embedding space where the distances between embedded state pairs correspond to their MAD, accommodating both symmetric and asymmetric approximations. We evaluate the framework on a comprehensive suite of environments with known MAD values, encompassing both deterministic and stochastic dynamics, as well as discrete and continuous state spaces, and environments with noisy observations. Empirical results demonstrate that the proposed approach not only efficiently learns accurate MAD representations across these diverse settings but also significantly outperforms existing state representation methods in terms of representation quality."
  },
  {
    "title": "Uncertainty Prioritized Experience Replay",
    "url": "http://arxiv.org/abs/2506.09270v1",
    "arxiv_id": "2506.09270v1",
    "authors": [
      "Rodrigo Carrasco-Davis",
      "Sebastian Lee",
      "Claudia Clopath",
      "Will Dabney"
    ],
    "published": "2025-06-10T22:07:13+00:00",
    "summary": "Prioritized experience replay, which improves sample efficiency by selecting relevant transitions to update parameter estimates, is a crucial component of contemporary value-based deep reinforcement learning models. Typically, transitions are prioritized based on their temporal difference error. However, this approach is prone to favoring noisy transitions, even when the value estimation closely approximates the target mean. This phenomenon resembles the noisy TV problem postulated in the exploration literature, in which exploration-guided agents get stuck by mistaking noise for novelty. To mitigate the disruptive effects of noise in value estimation, we propose using epistemic uncertainty estimation to guide the prioritization of transitions from the replay buffer. Epistemic uncertainty quantifies the uncertainty that can be reduced by learning, hence reducing transitions sampled from the buffer generated by unpredictable random processes. We first illustrate the benefits of epistemic uncertainty prioritized replay in two tabular toy models: a simple multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our prioritization scheme on the Atari suite, outperforming quantile regression deep Q-learning benchmarks; thus forging a path for the use of uncertainty prioritized replay in reinforcement learning agents."
  },
  {
    "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs",
    "url": "http://arxiv.org/abs/2506.09215v1",
    "arxiv_id": "2506.09215v1",
    "authors": [
      "Greyson Brothers"
    ],
    "published": "2025-06-10T20:18:32+00:00",
    "summary": "We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based adaptive pooling method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks."
  },
  {
    "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09202v1",
    "arxiv_id": "2506.09202v1",
    "authors": [
      "Hao Hu",
      "Xinqi Wang",
      "Simon Shaolei Du"
    ],
    "published": "2025-06-10T19:44:48+00:00",
    "summary": "We introduce a novel task of clustering trajectories from offline reinforcement learning (RL) datasets, where each cluster center represents the policy that generated its trajectories. By leveraging the connection between the KL-divergence of offline trajectory distributions and a mixture of policy-induced distributions, we formulate a natural clustering objective. To solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies and assigns trajectories based on policy generation probabilities, while CAAE resembles the VQ-VAE framework by guiding the latent representations of trajectories toward the vicinity of specific codebook entries to achieve clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans and identify a key challenge in offline trajectory clustering: the inherent ambiguity of optimal solutions due to policy-induced conflicts, which can result in multiple equally valid but structurally distinct clusterings. Experimentally, we validate our methods on the widely used D4RL dataset and custom GridWorld environments. Our results show that both PG-Kmeans and CAAE effectively partition trajectories into meaningful clusters. They offer a promising framework for policy-based trajectory clustering, with broad applications in offline RL and beyond."
  },
  {
    "title": "Multi-Task Reward Learning from Human Ratings",
    "url": "http://arxiv.org/abs/2506.09183v1",
    "arxiv_id": "2506.09183v1",
    "authors": [
      "Mingkang Wu",
      "Devin White",
      "Evelyn Rose",
      "Vernon Lawhern",
      "Nicholas R Waytowich",
      "Yongcan Cao"
    ],
    "published": "2025-06-10T19:00:19+00:00",
    "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in aligning model behavior with users' goals. However, while humans integrate multiple strategies when making decisions, current RLHF approaches often simplify this process by modeling human reasoning through isolated tasks such as classification or regression. In this paper, we propose a novel reinforcement learning (RL) method that mimics human decision-making by jointly considering multiple tasks. Specifically, we leverage human ratings in reward-free environments to infer a reward function, introducing learnable weights that balance the contributions of both classification and regression models. This design captures the inherent uncertainty in human decision-making and allows the model to adaptively emphasize different strategies. We conduct several experiments using synthetic human ratings to validate the effectiveness of the proposed approach. Results show that our method consistently outperforms existing rating-based RL methods, and in some cases, even surpasses traditional RL approaches."
  },
  {
    "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09049v1",
    "arxiv_id": "2506.09049v1",
    "authors": [
      "Li Kang",
      "Xiufeng Song",
      "Heng Zhou",
      "Yiran Qin",
      "Jie Yang",
      "Xiaohong Liu",
      "Philip Torr",
      "Lei Bai",
      "Zhenfei Yin"
    ],
    "published": "2025-06-10T17:59:44+00:00",
    "summary": "Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."
  },
  {
    "title": "Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks",
    "url": "http://arxiv.org/abs/2506.09039v1",
    "arxiv_id": "2506.09039v1",
    "authors": [
      "Abderrahime Filali",
      "Diala Naboulsi",
      "Georges Kaddoum"
    ],
    "published": "2025-06-10T17:57:33+00:00",
    "summary": "The next generation of tactical networks (TNs) is poised to further leverage the key enablers of 5G and beyond 5G (B5G) technology, such as radio access network (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple architectural options and opportunities for a wide range of innovative applications. RAN slicing and the O-RAN paradigm are considered game changers in TNs, where the former makes it possible to tailor user services to users requirements, and the latter brings openness and intelligence to the management of the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing strategy. Although this type of strategy ensures efficient bandwidth utilization, it compromises RAN slicing isolation in terms of quality of service (QoS) performance. To deal with this challenge, we propose a deep reinforcement learning (DRL)-based RAN slicing mechanism that achieves a trade-off between efficient RAN bandwidth sharing and appropriate inter- and intra-slice isolation. The proposed mechanism performs bandwidth allocation in two stages. In the first stage, the bandwidth is allocated to the RAN slices. In the second stage, each slice partitions its bandwidth among its associated users. In both stages, the slicing operation is constrained by several considerations related to improving the QoS of slices and users that in turn foster inter- and intra-slice isolation. The proposed RAN slicing mechanism is based on DRL algorithms to perform the bandwidth sharing operation in each stage. We propose to deploy the mechanism in an O-RAN architecture and describe the O-RAN functional blocks and the main DRL model lifecycle management phases involved. We also develop three different implementations of the proposed mechanism, each based on a different DRL algorithm, and evaluate their performance against multiple baselines across various parameters."
  },
  {
    "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.09033v1",
    "arxiv_id": "2506.09033v1",
    "authors": [
      "Haozhen Zhang",
      "Tao Feng",
      "Jiaxuan You"
    ],
    "published": "2025-06-10T17:56:45+00:00",
    "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1."
  },
  {
    "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning",
    "url": "http://arxiv.org/abs/2506.09016v1",
    "arxiv_id": "2506.09016v1",
    "authors": [
      "Ruiqi Zhang",
      "Daman Arora",
      "Song Mei",
      "Andrea Zanette"
    ],
    "published": "2025-06-10T17:42:42+00:00",
    "summary": "Training large language models with reinforcement learning (RL) against verifiable rewards significantly enhances their reasoning abilities, yet remains computationally expensive due to inefficient uniform prompt sampling. We introduce Selective Prompting with Efficient Estimation of Difficulty (SPEED), an adaptive online RL curriculum that selectively chooses training examples of intermediate difficulty to maximize learning efficiency. Theoretically, we establish that intermediate-difficulty prompts improve the gradient estimator's signal-to-noise ratio, accelerating convergence. Empirically, our efficient implementation leads to 2x to 6x faster training without degrading accuracy, requires no manual tuning, and integrates seamlessly into standard RL algorithms."
  },
  {
    "title": "Learning to Reason Across Parallel Samples for LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.09014v1",
    "arxiv_id": "2506.09014v1",
    "authors": [
      "Jianing Qi",
      "Xi Ye",
      "Hao Tang",
      "Zhigang Zhu",
      "Eunsol Choi"
    ],
    "published": "2025-06-10T17:42:35+00:00",
    "summary": "Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently."
  },
  {
    "title": "On Monotonicity in AI Alignment",
    "url": "http://arxiv.org/abs/2506.08998v1",
    "arxiv_id": "2506.08998v1",
    "authors": [
      "Gilles Bareilles",
      "Julien Fageot",
      "L\u00ea-Nguy\u00ean Hoang",
      "Peva Blanchard",
      "Wassim Bouaziz",
      "S\u00e9bastien Rouault",
      "El-Mahdi El-Mhamdi"
    ],
    "published": "2025-06-10T17:17:48+00:00",
    "summary": "Comparison-based preference learning has become central to the alignment of AI models with human preferences. However, these methods may behave counterintuitively. After empirically observing that, when accounting for a preference for response $y$ over $z$, the model may actually decrease the probability (and reward) of generating $y$ (an observation also made by others), this paper investigates the root causes of (non) monotonicity, for a general comparison-based preference learning framework that subsumes Direct Preference Optimization (DPO), Generalized Preference Optimization (GPO) and Generalized Bradley-Terry (GBT). Under mild assumptions, we prove that such methods still satisfy what we call local pairwise monotonicity. We also provide a bouquet of formalizations of monotonicity, and identify sufficient conditions for their guarantee, thereby providing a toolbox to evaluate how prone learning models are to monotonicity violations. These results clarify the limitations of current methods and provide guidance for developing more trustworthy preference learning algorithms."
  },
  {
    "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.08989v1",
    "arxiv_id": "2506.08989v1",
    "authors": [
      "Xiao Liang",
      "Zhong-Zhi Li",
      "Yeyun Gong",
      "Yang Wang",
      "Hengyuan Zhang",
      "Yelong Shen",
      "Ying Nian Wu",
      "Weizhu Chen"
    ],
    "published": "2025-06-10T17:02:00+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks."
  },
  {
    "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model",
    "url": "http://arxiv.org/abs/2506.08967v1",
    "arxiv_id": "2506.08967v1",
    "authors": [
      "Ailin Huang",
      "Bingxin Li",
      "Bruce Wang",
      "Boyong Wu",
      "Chao Yan",
      "Chengli Feng",
      "Heng Wang",
      "Hongyu Zhou",
      "Hongyuan Wang",
      "Jingbei Li",
      "Jianjian Sun",
      "Joanna Wang",
      "Mingrui Chen",
      "Peng Liu",
      "Ruihang Miao",
      "Shilei Jiang",
      "Tian Fei",
      "Wang You",
      "Xi Chen",
      "Xuerui Yang",
      "Yechang Huang",
      "Yuxiang Zhang",
      "Zheng Ge",
      "Zheng Gong",
      "Zhewei Huang",
      "Zixin Zhang",
      "Bin Wang",
      "Bo Li",
      "Buyun Ma",
      "Changxin Miao",
      "Changyi Wan",
      "Chen Xu",
      "Dapeng Shi",
      "Dingyuan Hu",
      "Enle Liu",
      "Guanzhe Huang",
      "Gulin Yan",
      "Hanpeng Hu",
      "Haonan Jia",
      "Jiahao Gong",
      "Jiaoren Wu",
      "Jie Wu",
      "Jie Yang",
      "Junzhe Lin",
      "Kaixiang Li",
      "Lei Xia",
      "Longlong Gu",
      "Ming Li",
      "Nie Hao",
      "Ranchen Ming",
      "Shaoliang Pang",
      "Siqi Liu",
      "Song Yuan",
      "Tiancheng Cao",
      "Wen Li",
      "Wenqing He",
      "Xu Zhao",
      "Xuelin Zhang",
      "Yanbo Yu",
      "Yinmin Zhong",
      "Yu Zhou",
      "Yuanwei Liang",
      "Yuanwei Lu",
      "Yuxiang Yang",
      "Zidong Yang",
      "Zili Zhang",
      "Binxing Jiao",
      "Heung-Yeung Shum",
      "Jiansheng Chen",
      "Jing Li",
      "Xiangyu Zhang",
      "Xinhao Zhang",
      "Yibo Zhu",
      "Daxin Jiang",
      "Shuchang Zhou",
      "Chen Hu"
    ],
    "published": "2025-06-10T16:37:39+00:00",
    "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent human-computer interaction, yet their reliance on text-based outputs limits their ability to generate natural speech responses directly, hindering seamless audio interactions. To address this, we introduce Step-Audio-AQAA, a fully end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model integrates a dual-codebook audio tokenizer for linguistic and semantic feature extraction, a 130-billion-parameter backbone LLM and a neural vocoder for high-fidelity speech synthesis. Our post-training approach employs interleaved token-output of text and audio to enhance semantic coherence and combines Direct Preference Optimization (DPO) with model merge to improve performance. Evaluations on the StepEval-Audio-360 benchmark demonstrate that Step-Audio-AQAA excels especially in speech control, outperforming the state-of-art LALMs in key areas. This work contributes a promising solution for end-to-end LALMs and highlights the critical role of token-based vocoder in enhancing overall performance for AQAA tasks."
  },
  {
    "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO",
    "url": "http://arxiv.org/abs/2506.08965v1",
    "arxiv_id": "2506.08965v1",
    "authors": [
      "Yiyang Zhao",
      "Huiyu Bai",
      "Xuejiao Zhao"
    ],
    "published": "2025-06-10T16:37:13+00:00",
    "summary": "The ability to train high-performing reward models with few-shot data is critical for enhancing the efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF). We propose a data augmentation and expansion framework that enables generative reward models trained on small datasets to achieve comparable performance to those trained on large-scale datasets. Traditional methods to train a generative reward model, such as Direct Preference Optimization (DPO), are constrained by inefficiencies in sample pairing and limited data diversity. This work introduces preference refinement, which employs Chain-of-Thought (CoT) sampling to uncover diverse and high-quality preference relationships. It also incorporates a perplexity-based scoring mechanism to assign nuanced preference levels and utilizes Multi-level Direct Preference Optimization (M-DPO) to enable the model to capture finer-grained preference differences between samples. Experimental results demonstrate that the proposed method significantly enhances data efficiency and model performance, enabling reward models trained in a few-shot setting to achieve results on par with those trained on large-scale datasets. This study underscores the potential of data-efficient strategies in advancing reward model optimization, offering a robust solution for low-resource RLHF applications."
  },
  {
    "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation",
    "url": "http://arxiv.org/abs/2506.08961v1",
    "arxiv_id": "2506.08961v1",
    "authors": [
      "Chenxu Wang",
      "Huaping Liu"
    ],
    "published": "2025-06-10T16:32:31+00:00",
    "summary": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have been widely studied in various threat models; however, few consider environmental state perturbations, which are natural in embodied scenarios. To improve the robustness of DRL agents, we formulate the problem of environmental state perturbation, introducing a preliminary non-targeted attack method as a calibration adversary, and then propose a defense framework, named Boosted Adversarial Training (BAT), which first tunes the agents via supervised learning to avoid catastrophic failure and subsequently adversarially trains the agent with reinforcement learning. Extensive experimental results substantiate the vulnerability of mainstream agents under environmental state perturbations and the effectiveness of our proposed attack. The defense results demonstrate that while existing robust reinforcement learning algorithms may not be suitable, our BAT framework can significantly enhance the robustness of agents against environmental state perturbations across various situations."
  },
  {
    "title": "Can A Gamer Train A Mathematical Reasoning Model?",
    "url": "http://arxiv.org/abs/2506.08935v1",
    "arxiv_id": "2506.08935v1",
    "authors": [
      "Andrew Shin"
    ],
    "published": "2025-06-10T16:00:12+00:00",
    "summary": "While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath."
  },
  {
    "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions",
    "url": "http://arxiv.org/abs/2506.08927v1",
    "arxiv_id": "2506.08927v1",
    "authors": [
      "David Acuna",
      "Ximing Lu",
      "Jaehun Jung",
      "Hyunwoo Kim",
      "Amlan Kar",
      "Sanja Fidler",
      "Yejin Choi"
    ],
    "published": "2025-06-10T15:51:16+00:00",
    "summary": "Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning -- akin to the success observed in language models -- via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces -- without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer pairs into the model's output stream. We show that framing reasoning as a search process -- where subquestions act as latent decisions within a broader inference trajectory -- helps the model \"connect the dots\" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts."
  },
  {
    "title": "Intention-Conditioned Flow Occupancy Models",
    "url": "http://arxiv.org/abs/2506.08902v1",
    "arxiv_id": "2506.08902v1",
    "authors": [
      "Chongyi Zheng",
      "Seohong Park",
      "Sergey Levine",
      "Benjamin Eysenbach"
    ],
    "published": "2025-06-10T15:27:46+00:00",
    "summary": "Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \\times$ median improvement in returns and increases success rates by $36\\%$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom"
  },
  {
    "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation",
    "url": "http://arxiv.org/abs/2506.08898v1",
    "arxiv_id": "2506.08898v1",
    "authors": [
      "Mingfeng Fan",
      "Jianan Zhou",
      "Yifeng Zhang",
      "Yaoxin Wu",
      "Jinbiao Chen",
      "Guillaume Adrien Sartoretti"
    ],
    "published": "2025-06-10T15:25:06+00:00",
    "summary": "Recent deep reinforcement learning methods have achieved remarkable success in solving multi-objective combinatorial optimization problems (MOCOPs) by decomposing them into multiple subproblems, each associated with a specific weight vector. However, these methods typically treat all subproblems equally and solve them using a single model, hindering the effective exploration of the solution space and thus leading to suboptimal performance. To overcome the limitation, we propose POCCO, a novel plug-and-play framework that enables adaptive selection of model structures for subproblems, which are subsequently optimized based on preference signals rather than explicit reward values. Specifically, we design a conditional computation block that routes subproblems to specialized neural architectures. Moreover, we propose a preference-driven optimization algorithm that learns pairwise preferences between winning and losing solutions. We evaluate the efficacy and versatility of POCCO by applying it to two state-of-the-art neural methods for MOCOPs. Experimental results across four classic MOCOP benchmarks demonstrate its significant superiority and strong generalization."
  },
  {
    "title": "Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08893v1",
    "arxiv_id": "2506.08893v1",
    "authors": [
      "Kai Zhou",
      "Youbiao He",
      "Chong Zhong",
      "Yifu Wu"
    ],
    "published": "2025-06-10T15:20:39+00:00",
    "summary": "Despite high reliability, modern power systems with growing renewable penetration face an increasing risk of cascading outages. Real-time cascade mitigation requires fast, complex operational decisions under uncertainty. In this work, we extend the influence graph into a Markov decision process model (MDP) for real-time mitigation of cascading outages in power transmission systems, accounting for uncertainties in generation, load, and initial contingencies. The MDP includes a do-nothing action to allow for conservative decision-making and is solved using reinforcement learning. We present a policy gradient learning algorithm initialized with a policy corresponding to the unmitigated case and designed to handle invalid actions. The proposed learning method converges faster than the conventional algorithm. Through careful reward design, we learn a policy that takes conservative actions without deteriorating system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus systems. The results show that proactive line disconnections can effectively reduce cascading risk, and certain lines consistently emerge as critical in mitigating cascade propagation."
  },
  {
    "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
    "url": "http://arxiv.org/abs/2506.08885v1",
    "arxiv_id": "2506.08885v1",
    "authors": [
      "Danush Khanna",
      "Krishna Kumar",
      "Basab Ghosh",
      "Vinija Jain",
      "Vasu Sharma",
      "Aman Chadha",
      "Amitava Das"
    ],
    "published": "2025-06-10T15:14:17+00:00",
    "summary": "Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md."
  },
  {
    "title": "Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing",
    "url": "http://arxiv.org/abs/2506.08850v1",
    "arxiv_id": "2506.08850v1",
    "authors": [
      "Amin Avan",
      "Akramul Azim",
      "Qusay Mahmoud"
    ],
    "published": "2025-06-10T14:38:07+00:00",
    "summary": "Soft real-time applications are becoming increasingly complex, posing significant challenges for scheduling offloaded tasks in edge computing environments while meeting task timing constraints. Moreover, the exponential growth of the search space, presence of multiple objectives and parameters, and highly dynamic nature of edge computing environments further exacerbate the complexity of task scheduling. As a result, schedulers based on heuristic and metaheuristic algorithms frequently encounter difficulties in generating optimal or near-optimal task schedules due to their constrained ability to adapt to the dynamic conditions and complex environmental characteristics of edge computing. Accordingly, reinforcement learning algorithms have been incorporated into schedulers to address the complexity and dynamic conditions inherent in task scheduling in edge computing. However, a significant limitation of reinforcement learning algorithms is the prolonged learning time required to adapt to new environments and to address medium- and large-scale problems. This challenge arises from the extensive global action space and frequent random exploration of irrelevant actions. Therefore, this study proposes Agile Reinforcement learning (aRL), in which the RL-agent performs informed exploration and executes only relevant actions. Consequently, the predictability of the RL-agent is enhanced, leading to rapid adaptation and convergence, which positions aRL as a suitable candidate for scheduling the tasks of soft real-time applications in edge computing. The experiments demonstrate that the combination of informed exploration and action-masking methods enables aRL to achieve a higher hit-ratio and converge faster than the baseline approaches."
  },
  {
    "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains",
    "url": "http://arxiv.org/abs/2506.08840v1",
    "arxiv_id": "2506.08840v1",
    "authors": [
      "Dewei Wang",
      "Xinmiao Wang",
      "Xinzhe Liu",
      "Jiyuan Shi",
      "Yingnan Zhao",
      "Chenjia Bai",
      "Xuelong Li"
    ],
    "published": "2025-06-10T14:25:58+00:00",
    "summary": "Humanoid robots have demonstrated robust locomotion capabilities using Reinforcement Learning (RL)-based approaches. Further, to obtain human-like behaviors, existing methods integrate human motion-tracking or motion prior in the RL framework. However, these methods are limited in flat terrains with proprioception only, restricting their abilities to traverse challenging terrains with human-like gaits. In this work, we propose a novel framework using a mixture of latent residual experts with multi-discriminators to train an RL policy, which is capable of traversing complex terrains in controllable lifelike gaits with exteroception. Our two-stage training pipeline first teaches the policy to traverse complex terrains using a depth camera, and then enables gait-commanded switching between human-like gait patterns. We also design gait rewards to adjust human-like behaviors like robot base height. Simulation and real-world experiments demonstrate that our framework exhibits exceptional performance in traversing complex terrains, and achieves seamless transitions between multiple human-like gait patterns."
  },
  {
    "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning",
    "url": "http://arxiv.org/abs/2506.08745v1",
    "arxiv_id": "2506.08745v1",
    "authors": [
      "Kongcheng Zhang",
      "Qi Yao",
      "Shunyu Liu",
      "Yingjie Wang",
      "Baisheng Lai",
      "Jieping Ye",
      "Mingli Song",
      "Dacheng Tao"
    ],
    "published": "2025-06-10T12:40:39+00:00",
    "summary": "Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at https://github.com/sastpg/CoVo."
  },
  {
    "title": "Exploration by Random Reward Perturbation",
    "url": "http://arxiv.org/abs/2506.08737v1",
    "arxiv_id": "2506.08737v1",
    "authors": [
      "Haozhe Ma",
      "Guoji Fu",
      "Zhengding Luo",
      "Jiele Wu",
      "Tze-Yun Leong"
    ],
    "published": "2025-06-10T12:34:00+00:00",
    "summary": "We introduce Random Reward Perturbation (RRP), a novel exploration strategy for reinforcement learning (RL). Our theoretical analyses demonstrate that adding zero-mean noise to environmental rewards effectively enhances policy diversity during training, thereby expanding the range of exploration. RRP is fully compatible with the action-perturbation-based exploration strategies, such as $\\epsilon$-greedy, stochastic policies, and entropy regularization, providing additive improvements to exploration effects. It is general, lightweight, and can be integrated into existing RL algorithms with minimal implementation effort and negligible computational overhead. RRP establishes a theoretical connection between reward shaping and noise-driven exploration, highlighting their complementary potential. Experiments show that RRP significantly boosts the performance of Proximal Policy Optimization and Soft Actor-Critic, achieving higher sample efficiency and escaping local optima across various tasks, under both sparse and dense reward scenarios."
  },
  {
    "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization",
    "url": "http://arxiv.org/abs/2506.08712v1",
    "arxiv_id": "2506.08712v1",
    "authors": [
      "Hee Suk Yoon",
      "Eunseop Yoon",
      "Mark A. Hasegawa-Johnson",
      "Sungwoong Kim",
      "Chang D. Yoo"
    ],
    "published": "2025-06-10T11:54:22+00:00",
    "summary": "We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead."
  },
  {
    "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling",
    "url": "http://arxiv.org/abs/2506.08681v1",
    "arxiv_id": "2506.08681v1",
    "authors": [
      "Phuc Minh Nguyen",
      "Ngoc-Hieu Nguyen",
      "Duy H. M. Nguyen",
      "Anji Liu",
      "An Mai",
      "Binh T. Nguyen",
      "Daniel Sonntag",
      "Khoa D. Doan"
    ],
    "published": "2025-06-10T10:45:26+00:00",
    "summary": "Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. However, these methods are more susceptible to over-optimization, in which the model drifts away from the reference policy, leading to degraded performance as training progresses. This paper proposes a novel importance-sampling approach to mitigate the over-optimization problem of offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem. Our implementations are provided publicly at this link."
  },
  {
    "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling",
    "url": "http://arxiv.org/abs/2506.08672v1",
    "arxiv_id": "2506.08672v1",
    "authors": [
      "Yang Liu",
      "Jiaqi Li",
      "Zilong Zheng"
    ],
    "published": "2025-06-10T10:31:21+00:00",
    "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1% average points on eight ID tasks and $\\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL."
  },
  {
    "title": "Semi-gradient DICE for Offline Constrained Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08644v1",
    "arxiv_id": "2506.08644v1",
    "authors": [
      "Woosung Kim",
      "JunHo Seo",
      "Jongmin Lee",
      "Byung-Jun Lee"
    ],
    "published": "2025-06-10T09:57:25+00:00",
    "summary": "Stationary Distribution Correction Estimation (DICE) addresses the mismatch between the stationary distribution induced by a policy and the target distribution required for reliable off-policy evaluation (OPE) and policy optimization. DICE-based offline constrained RL particularly benefits from the flexibility of DICE, as it simultaneously maximizes return while estimating costs in offline settings. However, we have observed that recent approaches designed to enhance the offline RL performance of the DICE framework inadvertently undermine its ability to perform OPE, making them unsuitable for constrained RL scenarios. In this paper, we identify the root cause of this limitation: their reliance on a semi-gradient optimization, which solves a fundamentally different optimization problem and results in failures in cost estimation. Building on these insights, we propose a novel method to enable OPE and constrained RL through semi-gradient DICE. Our method ensures accurate cost estimation and achieves state-of-the-art performance on the offline constrained RL benchmark, DSRL."
  },
  {
    "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators",
    "url": "http://arxiv.org/abs/2506.08639v1",
    "arxiv_id": "2506.08639v1",
    "authors": [
      "Amir Hossein Barjini",
      "Seyed Adel Alizadeh Kolagar",
      "Sadeq Yaqubi",
      "Jouni Mattila"
    ],
    "published": "2025-06-10T09:53:57+00:00",
    "summary": "This article presents a motion planning and control framework for flexible robotic manipulators, integrating deep reinforcement learning (DRL) with a nonlinear partial differential equation (PDE) controller. Unlike conventional approaches that focus solely on control, we demonstrate that the desired trajectory significantly influences endpoint vibrations. To address this, a DRL motion planner, trained using the soft actor-critic (SAC) algorithm, generates optimized trajectories that inherently minimize vibrations. The PDE nonlinear controller then computes the required torques to track the planned trajectory while ensuring closed-loop stability using Lyapunov analysis. The proposed methodology is validated through both simulations and real-world experiments, demonstrating superior vibration suppression and tracking accuracy compared to traditional methods. The results underscore the potential of combining learning-based motion planning with model-based control for enhancing the precision and stability of flexible robotic manipulators."
  },
  {
    "title": "Modular Recurrence in Contextual MDPs for Universal Morphology Control",
    "url": "http://arxiv.org/abs/2506.08630v1",
    "arxiv_id": "2506.08630v1",
    "authors": [
      "Laurens Engwegen",
      "Daan Brinks",
      "Wendelin B\u00f6hmer"
    ],
    "published": "2025-06-10T09:44:30+00:00",
    "summary": "A universal controller for any robot morphology would greatly improve computational and data efficiency. By utilizing contextual information about the properties of individual robots and exploiting their modular structure in the architecture of deep reinforcement learning agents, steps have been made towards multi-robot control. Generalization to new, unseen robots, however, remains a challenge. In this paper we hypothesize that the relevant contextual information is partially observable, but that it can be inferred through interactions for better generalization to contexts that are not seen during training. To this extent, we implement a modular recurrent architecture and evaluate its generalization performance on a large set of MuJoCo robots. The results show a substantial improved performance on robots with unseen dynamics, kinematics, and topologies, in four different environments."
  },
  {
    "title": "HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08580v1",
    "arxiv_id": "2506.08580v1",
    "authors": [
      "Yang Lv",
      "Jinlong Lei",
      "Peng Yi"
    ],
    "published": "2025-06-10T08:51:18+00:00",
    "summary": "Two-stage Colonel Blotto game represents a typical adversarial resource allocation problem, in which two opposing agents sequentially allocate resources in a network topology across two phases: an initial resource deployment followed by multiple rounds of dynamic reallocation adjustments. The sequential dependency between game stages and the complex constraints imposed by the graph topology make it difficult for traditional approaches to attain a globally optimal strategy. To address these challenges, we propose a hierarchical graph Transformer framework called HGformer. By incorporating an enhanced graph Transformer encoder with structural biases and a two-agent hierarchical decision model, our approach enables efficient policy generation in large-scale adversarial environments. Moreover, we design a layer-by-layer feedback reinforcement learning algorithm that feeds the long-term returns from lower-level decisions back into the optimization of the higher-level strategy, thus bridging the coordination gap between the two decision-making stages. Experimental results demonstrate that, compared to existing hierarchical decision-making or graph neural network methods, HGformer significantly improves resource allocation efficiency and adversarial payoff, achieving superior overall performance in complex dynamic game scenarios."
  },
  {
    "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge",
    "url": "http://arxiv.org/abs/2506.08553v1",
    "arxiv_id": "2506.08553v1",
    "authors": [
      "Agnese Taluzzi",
      "Davide Gesualdi",
      "Riccardo Santambrogio",
      "Chiara Plizzari",
      "Francesca Palermo",
      "Simone Mentasti",
      "Matteo Matteucci"
    ],
    "published": "2025-06-10T08:21:38+00:00",
    "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with a multi-modal large language model (MLLM) to capture fine-grained object interactions, spatial relationships, and temporally grounded events. In parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge to introduce high-level semantic connections between entities, enabling reasoning beyond directly observable visual evidence. Each method demonstrates distinct strengths across the seven categories of the HD-EPIC benchmark, and their combination within our framework results in an overall accuracy of 44.21% on the challenge, highlighting its effectiveness for complex egocentric VQA tasks."
  },
  {
    "title": "DeepForm: Reasoning Large Language Model for Communication System Formulation",
    "url": "http://arxiv.org/abs/2506.08551v1",
    "arxiv_id": "2506.08551v1",
    "authors": [
      "Panlong Wu",
      "Ting Wang",
      "Yifei Zhong",
      "Haoqi Zhang",
      "Zitong Wang",
      "Fangxin Wang"
    ],
    "published": "2025-06-10T08:16:24+00:00",
    "summary": "Communication system formulation is critical for advancing 6G and future wireless technologies, yet it remains a complex, expertise-intensive task. While Large Language Models (LLMs) offer potential, existing general-purpose models often lack the specialized domain knowledge, nuanced reasoning capabilities, and access to high-quality, domain-specific training data required for adapting a general LLM into an LLM specially for communication system formulation. To bridge this gap, we introduce DeepForm, the first reasoning LLM specially for automated communication system formulation. We propose the world-first large-scale, open-source dataset meticulously curated for this domain called Communication System Formulation Reasoning Corpus (CSFRC). Our framework employs a two-stage training strategy: first, Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge; second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated reasoning patterns like self-correction and verification. Extensive experiments demonstrate that our model achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs on diverse senerios. We will release related resources to foster further research in this area after the paper is accepted."
  },
  {
    "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)",
    "url": "http://arxiv.org/abs/2506.08533v1",
    "arxiv_id": "2506.08533v1",
    "authors": [
      "Nihal Acharya Adde",
      "Alexandra Gianzina",
      "Hanno Gottschalk",
      "Andreas Ebert"
    ],
    "published": "2025-06-10T07:52:35+00:00",
    "summary": "This paper introduces Evolutionary Multi-Objective Network Architecture Search (EMNAS) for the first time to optimize neural network architectures in large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses genetic algorithms to automate network design, tailored to enhance rewards and reduce model size without compromising performance. Additionally, parallelization techniques are employed to accelerate the search, and teacher-student methodologies are implemented to ensure scalable optimization. This research underscores the potential of transfer learning as a robust framework for optimizing performance across iterative learning processes by effectively leveraging knowledge from earlier generations to enhance learning efficiency and stability in subsequent generations. Experimental results demonstrate that tailored EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters. The findings of these strategies contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios."
  },
  {
    "title": "Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness",
    "url": "http://arxiv.org/abs/2506.08532v1",
    "arxiv_id": "2506.08532v1",
    "authors": [
      "Yanwei Gong",
      "Xiaolin Chang"
    ],
    "published": "2025-06-10T07:51:29+00:00",
    "summary": "The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking."
  },
  {
    "title": "Predictive reinforcement learning based adaptive PID controller",
    "url": "http://arxiv.org/abs/2506.08509v1",
    "arxiv_id": "2506.08509v1",
    "authors": [
      "Chaoqun Ma",
      "Zhiyong Zhang"
    ],
    "published": "2025-06-10T07:09:45+00:00",
    "summary": "Purpose: This study aims to address the challenges of controlling unstable and nonlinear systems by proposing an adaptive PID controller based on predictive reinforcement learning (PRL-PID), where the PRL-PID combines the advantages of both data-driven and model-driven approaches. Design/methodology/approach: A predictive reinforcement learning framework is introduced, incorporating action smooth strategy to suppress overshoot and oscillations, and a hierarchical reward function to support training. Findings: Experimental results show that the PRL-PID controller achieves superior stability and tracking accuracy in nonlinear, unstable, and strongly coupled systems, consistently outperforming existing RL-tuned PID methods while maintaining excellent robustness and adaptability across diverse operating conditions. Originality/Value: By adopting predictive learning, the proposed PRL-PID integrates system model priors into data-driven control, enhancing both the control framework's training efficiency and the controller's stability. As a result, PRL-PID provides a balanced blend of model-based and data-driven approaches, delivering robust, high-performance control."
  },
  {
    "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08507v1",
    "arxiv_id": "2506.08507v1",
    "authors": [
      "Kuo Yang",
      "Xingjie Yang",
      "Linhui Yu",
      "Qing Xu",
      "Yan Fang",
      "Xu Wang",
      "Zhengyang Zhou",
      "Yang Wang"
    ],
    "published": "2025-06-10T07:04:25+00:00",
    "summary": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality."
  },
  {
    "title": "How to Provably Improve Return Conditioned Supervised Learning?",
    "url": "http://arxiv.org/abs/2506.08463v1",
    "arxiv_id": "2506.08463v1",
    "authors": [
      "Zhishuai Liu",
      "Yu Yang",
      "Ruhan Wang",
      "Pan Xu",
      "Dongruo Zhou"
    ],
    "published": "2025-06-10T05:37:51+00:00",
    "summary": "In sequential decision-making problems, Return-Conditioned Supervised Learning (RCSL) has gained increasing recognition for its simplicity and stability in modern decision-making tasks. Unlike traditional offline reinforcement learning (RL) algorithms, RCSL frames policy learning as a supervised learning problem by taking both the state and return as input. This approach eliminates the instability often associated with temporal difference (TD) learning in offline RL. However, RCSL has been criticized for lacking the stitching property, meaning its performance is inherently limited by the quality of the policy used to generate the offline dataset. To address this limitation, we propose a principled and simple framework called Reinforced RCSL. The key innovation of our framework is the introduction of a concept we call the in-distribution optimal return-to-go. This mechanism leverages our policy to identify the best achievable in-dataset future return based on the current state, avoiding the need for complex return augmentation techniques. Our theoretical analysis demonstrates that Reinforced RCSL can consistently outperform the standard RCSL approach. Empirical results further validate our claims, showing significant performance improvements across a range of benchmarks."
  },
  {
    "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08460v1",
    "arxiv_id": "2506.08460v1",
    "authors": [
      "Yihong Guo",
      "Yu Yang",
      "Pan Xu",
      "Anqi Liu"
    ],
    "published": "2025-06-10T05:36:54+00:00",
    "summary": "We study the off-dynamics offline reinforcement learning problem, where the goal is to learn a policy from offline datasets collected from source and target domains with mismatched transition. Existing off-dynamics offline RL methods typically either filter source transitions that resemble those of the target domain or apply reward augmentation to source data, both constrained by the limited transitions available from the target domain. As a result, the learned policy is unable to explore target domain beyond the offline datasets. We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that addresses this limitation by enabling exploration of the target domain via learned dynamics. MOBODY generates new synthetic transitions in the target domain through model rollouts, which are used as data augmentation during offline policy learning. Unlike existing model-based methods that learn dynamics from a single domain, MOBODY tackles the challenge of mismatched dynamics by leveraging both source and target datasets. Directly merging these datasets can bias the learned model toward source dynamics. Instead, MOBODY learns target dynamics by discovering a shared latent representation of states and transitions across domains through representation learning. To stabilize training, MOBODY incorporates a behavior cloning loss that regularizes the policy. Specifically, we introduce a Q-weighted behavior cloning loss that regularizes the policy toward actions with high target-domain Q-values, rather than uniformly imitating all actions in the dataset. These Q-values are learned from an enhanced target dataset composed of offline target data, augmented source data, and rollout data from the learned target dynamics. We evaluate MOBODY on MuJoCo benchmarks and show that it significantly outperforms state-of-the-art baselines, with especially pronounced improvements in challenging scenarios."
  },
  {
    "title": "A Survey on Large Language Models for Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2506.08446v1",
    "arxiv_id": "2506.08446v1",
    "authors": [
      "Peng-Yuan Wang",
      "Tian-Shuo Liu",
      "Chenyang Wang",
      "Yi-Di Wang",
      "Shu Yan",
      "Cheng-Xing Jia",
      "Xu-Hui Liu",
      "Xin-Wei Chen",
      "Jia-Cheng Xu",
      "Ziniu Li",
      "Yang Yu"
    ],
    "published": "2025-06-10T04:44:28+00:00",
    "summary": "Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and \"test-time scaling\". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains."
  },
  {
    "title": "GPS Spoofing Attacks on AI-based Navigation Systems with Obstacle Avoidance in UAV",
    "url": "http://arxiv.org/abs/2506.08445v1",
    "arxiv_id": "2506.08445v1",
    "authors": [
      "Ji Hyuk Jung",
      "Mi Yeon Hong",
      "Ji Won Yoon"
    ],
    "published": "2025-06-10T04:42:55+00:00",
    "summary": "Recently, approaches using Deep Reinforcement Learning (DRL) have been proposed to solve UAV navigation systems in complex and unknown environments. However, despite extensive research and attention, systematic studies on various security aspects have not yet been conducted. Therefore, in this paper, we conduct research on security vulnerabilities in DRL-based navigation systems, particularly focusing on GPS spoofing attacks against the system. Many recent basic DRL-based navigation systems fundamentally share an efficient structure. This paper presents an attack model that operates through GPS spoofing attacks briefly modeling the range of spoofing attack against EKF sensor fusion of PX4 autopilot, and combine this with the DRL-based system to design attack scenarios that are closer to reality. Finally, this paper experimentally demonstrated that attacks are possible both in the basic DRL system and in attack models combining the DRL system with PX4 autopilot system."
  },
  {
    "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2506.08440v1",
    "arxiv_id": "2506.08440v1",
    "authors": [
      "Zengjue Chen",
      "Runliang Niu",
      "He Kong",
      "Qi Wang"
    ],
    "published": "2025-06-10T04:27:49+00:00",
    "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated strong generalization capabilities across diverse scenes, tasks, and robotic platforms when pretrained at large-scale datasets. However, these models still require task-specific fine-tuning in novel environments, a process that relies almost exclusively on supervised fine-tuning (SFT) using static trajectory datasets. Such approaches neither allow robot to interact with environment nor do they leverage feedback from live execution. Also, their success is critically dependent on the size and quality of the collected trajectories. Reinforcement learning (RL) offers a promising alternative by enabling closed-loop interaction and aligning learned policies directly with task objectives. In this work, we draw inspiration from the ideas of GRPO and propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method. By fusing step-level and trajectory-level advantage signals, this method improves GRPO's group-level advantage estimation, thereby making the algorithm more suitable for online reinforcement learning training of VLA. Experimental results on ten manipulation tasks from the libero-object benchmark demonstrate that TGRPO consistently outperforms various baseline methods, capable of generating more robust and efficient policies across multiple tested scenarios. Our source codes are available at: https://github.com/hahans/TGRPO"
  },
  {
    "title": "Attention-based Learning for 3D Informative Path Planning",
    "url": "http://arxiv.org/abs/2506.08434v1",
    "arxiv_id": "2506.08434v1",
    "authors": [
      "Rui Zhao",
      "Xingjian Zhang",
      "Yuhong Cao",
      "Yizhuo Wang",
      "Guillaume Sartoretti"
    ],
    "published": "2025-06-10T04:16:10+00:00",
    "summary": "In this work, we propose an attention-based deep reinforcement learning approach to address the adaptive informative path planning (IPP) problem in 3D space, where an aerial robot equipped with a downward-facing sensor must dynamically adjust its 3D position to balance sensing footprint and accuracy, and finally obtain a high-quality belief of an underlying field of interest over a given domain (e.g., presence of specific plants, hazardous gas, geological structures, etc.). In adaptive IPP tasks, the agent is tasked with maximizing information collected under time/distance constraints, continuously adapting its path based on newly acquired sensor data. To this end, we leverage attention mechanisms for their strong ability to capture global spatial dependencies across large action spaces, allowing the agent to learn an implicit estimation of environmental transitions. Our model builds a contextual belief representation over the entire domain, guiding sequential movement decisions that optimize both short- and long-term search objectives. Comparative evaluations against state-of-the-art planners demonstrate that our approach significantly reduces environmental uncertainty within constrained budgets, thus allowing the agent to effectively balance exploration and exploitation. We further show our model generalizes well to environments of varying sizes, highlighting its potential for many real-world applications."
  },
  {
    "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood",
    "url": "http://arxiv.org/abs/2506.08417v1",
    "arxiv_id": "2506.08417v1",
    "authors": [
      "Qingmao Yao",
      "Zhichao Lei",
      "Tianyuan Chen",
      "Ziyue Yuan",
      "Xuefan Chen",
      "Jianxiang Liu",
      "Faguo Wu",
      "Xiao Zhang"
    ],
    "published": "2025-06-10T03:43:22+00:00",
    "summary": "Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which constrains the $Q$-function generalization. This over-constraint issue results in poor $Q$-value estimation and hinders policy improvement. In this paper, we introduce a novel approach to achieve better $Q$-value estimation by enhancing $Q$-function generalization in OOD regions within Convex Hull and its Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by smoothing them with neighboring in-sample $Q$-values. We theoretically show that SBO approximates true $Q$-values for both in-sample and OOD actions within the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG), empirically alleviates the over-constraint issue, achieving near-accurate $Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing state-of-the-art methods in both performance and computational efficiency."
  },
  {
    "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots",
    "url": "http://arxiv.org/abs/2506.08416v1",
    "arxiv_id": "2506.08416v1",
    "authors": [
      "Bolin Li",
      "Linwei Sun",
      "Xuecong Huang",
      "Yuzhi Jiang",
      "Lijun Zhu"
    ],
    "published": "2025-06-10T03:42:04+00:00",
    "summary": "This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot's learning environment. Second, based on this gait planner, we design three effective reward functions within a reinforcement learning framework, forming a reward composition to achieve periodic bipedal gait. This reward composition reduces the robot's learning time and enhances locomotion performance. Finally, a gait design example and performance comparison are presented to demonstrate the effectiveness of the proposed method."
  },
  {
    "title": "Reinforcement Learning Teachers of Test Time Scaling",
    "url": "http://arxiv.org/abs/2506.08388v1",
    "arxiv_id": "2506.08388v1",
    "authors": [
      "Edoardo Cetin",
      "Tianyu Zhao",
      "Yujin Tang"
    ],
    "published": "2025-06-10T02:53:24+00:00",
    "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply \"connect-the-dots\" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework."
  },
  {
    "title": "Reinforce LLM Reasoning through Multi-Agent Reflection",
    "url": "http://arxiv.org/abs/2506.08379v1",
    "arxiv_id": "2506.08379v1",
    "authors": [
      "Yurun Yuan",
      "Tengyang Xie"
    ],
    "published": "2025-06-10T02:43:47+00:00",
    "summary": "Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization."
  },
  {
    "title": "Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models",
    "url": "http://arxiv.org/abs/2506.08352v1",
    "arxiv_id": "2506.08352v1",
    "authors": [
      "Wentao Shi",
      "Yiqing Shen"
    ],
    "published": "2025-06-10T02:09:57+00:00",
    "summary": "Large language models (LLMs) can face factual limitations when responding to time-sensitive queries about recent events that arise after their knowledge thresholds in the training corpus. Existing search-augmented approaches fall into two categories, each with distinct limitations: multi-agent search frameworks incur substantial computational overhead by separating search planning and response synthesis across multiple LLMs, while single-LLM tool-calling methods restrict themselves to sequential planned, single-query searches from sole search sources. We present Reasoning-Search (R-Search), a single-LLM search framework that unifies multi-step planning, multi-source search execution, and answer synthesis within one coherent inference process. Innovatively, it structure the output into four explicitly defined components, including reasoning steps that guide the search process (<think>), a natural-language directed acyclic graph that represents the search plans with respect to diverse sources (<search>), retrieved results from executing the search plans (<result>), and synthesized final answers (<answer>). To enable effective generation of these structured outputs, we propose a specialized Reinforcement Fine-Tuning (ReFT) method based on GRPO, together with a multi-component reward function that optimizes LLM's answer correctness, structural validity of the generated DAG, and adherence to the defined output format. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and seven Q and A benchmarks demonstrates that R-Search outperforms state-of-the-art methods, while achieving substantial efficiency gains through 70% reduction in context token usage and approximately 50% decrease in execution latency. Code is available at https://github.com/wentao0429/Reasoning-search."
  },
  {
    "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.08344v1",
    "arxiv_id": "2506.08344v1",
    "authors": [
      "Ne\u015fet \u00dcnver Akmandor",
      "Sarvesh Prajapati",
      "Mark Zolotas",
      "Ta\u015fk\u0131n Pad\u0131r"
    ],
    "published": "2025-06-10T01:58:32+00:00",
    "summary": "Traditional motion planning methods for robots with many degrees-of-freedom, such as mobile manipulators, are often computationally prohibitive for real-world settings. In this paper, we propose a novel multi-model motion planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear Model Predictive Control (NMPC). Re4MPC generates trajectories in a computationally efficient manner by reactively selecting the model, cost, and constraints of the NMPC problem depending on the complexity of the task and robot state. The policy for this reactive decision-making is learned via a Deep Reinforcement Learning (DRL) framework. We introduce a mathematical formulation to integrate NMPC into this DRL framework. To validate our methodology and design choices, we evaluate DRL training and test outcomes in a physics-based simulation involving a mobile manipulator. Experimental results demonstrate that Re4MPC is more computationally efficient and achieves higher success rates in reaching end-effector goals than the NMPC baseline, which computes whole-body trajectories without our learning mechanism."
  },
  {
    "title": "Dynamical System Optimization",
    "url": "http://arxiv.org/abs/2506.08340v1",
    "arxiv_id": "2506.08340v1",
    "authors": [
      "Emo Todorov"
    ],
    "published": "2025-06-10T01:50:38+00:00",
    "summary": "We develop an optimization framework centered around a core idea: once a (parametric) policy is specified, control authority is transferred to the policy, resulting in an autonomous dynamical system. Thus we should be able to optimize policy parameters without further reference to controls or actions, and without directly using the machinery of approximate Dynamic Programming and Reinforcement Learning. Here we derive simpler algorithms at the autonomous system level, and show that they compute the same quantities as policy gradients and Hessians, natural gradients, proximal methods. Analogs to approximate policy iteration and off-policy learning are also available. Since policy parameters and other system parameters are treated uniformly, the same algorithms apply to behavioral cloning, mechanism design, system identification, learning of state estimators. Tuning of generative AI models is not only possible, but is conceptually closer to the present framework than to Reinforcement Learning."
  },
  {
    "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium",
    "url": "http://arxiv.org/abs/2506.08292v1",
    "arxiv_id": "2506.08292v1",
    "authors": [
      "Xie Yi",
      "Zhanke Zhou",
      "Chentao Cao",
      "Qiyu Niu",
      "Tongliang Liu",
      "Bo Han"
    ],
    "published": "2025-06-09T23:49:14+00:00",
    "summary": "Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges. We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON's ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON."
  },
  {
    "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints",
    "url": "http://arxiv.org/abs/2506.08266v1",
    "arxiv_id": "2506.08266v1",
    "authors": [
      "Yaswanth Chittepu",
      "Blossom Metevier",
      "Will Schwarzer",
      "Austin Hoag",
      "Scott Niekum",
      "Philip S. Thomas"
    ],
    "published": "2025-06-09T22:03:56+00:00",
    "summary": "Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods."
  },
  {
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions",
    "url": "http://arxiv.org/abs/2506.08234v1",
    "arxiv_id": "2506.08234v1",
    "authors": [
      "Yu-Ang Lee",
      "Guan-Ting Yi",
      "Mei-Yi Liu",
      "Jui-Chao Lu",
      "Guan-Bo Yang",
      "Yun-Nung Chen"
    ],
    "published": "2025-06-09T21:04:14+00:00",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey."
  },
  {
    "title": "Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms",
    "url": "http://arxiv.org/abs/2506.08192v1",
    "arxiv_id": "2506.08192v1",
    "authors": [
      "Jared Claypoole",
      "Steven Cheung",
      "Ashish Gehani",
      "Vinod Yegneswaran",
      "Ahmad Ridley"
    ],
    "published": "2025-06-09T20:07:26+00:00",
    "summary": "We analyze two open source deep reinforcement learning agents submitted to the CAGE Challenge 2 cyber defense challenge, where each competitor submitted an agent to defend a simulated network against each of several provided rules-based attack agents. We demonstrate that one can gain interpretability of agent successes and failures by simplifying the complex state and action spaces and by tracking important events, shedding light on the fine-grained behavior of both the defense and attack agents in each experimental scenario. By analyzing important events within an evaluation episode, we identify patterns in infiltration and clearing events that tell us how well the attacker and defender played their respective roles; for example, defenders were generally able to clear infiltrations within one or two timesteps of a host being exploited. By examining transitions in the environment's state caused by the various possible actions, we determine which actions tended to be effective and which did not, showing that certain important actions are between 40% and 99% ineffective. We examine how decoy services affect exploit success, concluding for instance that decoys block up to 94% of exploits that would directly grant privileged access to a host. Finally, we discuss the realism of the challenge and ways that the CAGE Challenge 4 has addressed some of our concerns."
  },
  {
    "title": "Play to Generalize: Learning to Reason Through Game Play",
    "url": "http://arxiv.org/abs/2506.08011v1",
    "arxiv_id": "2506.08011v1",
    "authors": [
      "Yunfei Xie",
      "Yinsong Ma",
      "Shiyi Lan",
      "Alan Yuille",
      "Junfei Xiao",
      "Chen Wei"
    ],
    "published": "2025-06-09T17:59:57+00:00",
    "summary": "Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs."
  },
  {
    "title": "Reinforcement Pre-Training",
    "url": "http://arxiv.org/abs/2506.08007v1",
    "arxiv_id": "2506.08007v1",
    "authors": [
      "Qingxiu Dong",
      "Li Dong",
      "Yao Tang",
      "Tianzhu Ye",
      "Yutao Sun",
      "Zhifang Sui",
      "Furu Wei"
    ],
    "published": "2025-06-09T17:59:53+00:00",
    "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training."
  },
  {
    "title": "Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator",
    "url": "http://arxiv.org/abs/2506.07980v1",
    "arxiv_id": "2506.07980v1",
    "authors": [
      "Alberto Baz\u00e1n-Guill\u00e9n",
      "Carlos Beis-Penedo",
      "Diego Cajaraville-Aboy",
      "Pablo Barbecho-Bautista",
      "Rebeca P. D\u00edaz-Redondo",
      "Luis J. de la Cruz Llopis",
      "Ana Fern\u00e1ndez-Vilas",
      "M\u00f3nica Aguilar Igartua",
      "Manuel Fern\u00e1ndez-Veiga"
    ],
    "published": "2025-06-09T17:51:45+00:00",
    "summary": "Realistic urban traffic simulation is essential for sustainable urban planning and the development of intelligent transportation systems. However, generating high-fidelity, time-varying traffic profiles that accurately reflect real-world conditions, especially in large-scale scenarios, remains a major challenge. Existing methods often suffer from limitations in accuracy, scalability, or raise privacy concerns due to centralized data processing. This work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a novel framework that integrates Deep Reinforcement Learning (DRL) agents with the SUMO simulator to generate realistic 24-hour traffic patterns. A key innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL), wherein each traffic detector and its corresponding urban zone function as an independent learning node. These nodes train local DRL models using minimal historical data and collaboratively refine their performance by exchanging model parameters with selected peers (e.g., geographically adjacent zones), without requiring a central coordinator. Evaluated using real-world data from the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as RouteSampler, as well as other centralized learning approaches, by delivering more accurate and privacy-preserving traffic pattern generation."
  },
  {
    "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "url": "http://arxiv.org/abs/2506.07976v1",
    "arxiv_id": "2506.07976v1",
    "authors": [
      "Junhong Shen",
      "Hao Bai",
      "Lunjun Zhang",
      "Yifei Zhou",
      "Amrith Setlur",
      "Shengbang Tong",
      "Diego Caples",
      "Nan Jiang",
      "Tong Zhang",
      "Ameet Talwalkar",
      "Aviral Kumar"
    ],
    "published": "2025-06-09T17:50:02+00:00",
    "summary": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents."
  },
  {
    "title": "A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle",
    "url": "http://arxiv.org/abs/2506.07929v1",
    "arxiv_id": "2506.07929v1",
    "authors": [
      "Amirreza Yasami",
      "Mohammadali Tofigh",
      "Mahdi Shahbakhti",
      "Charles Robert Koch"
    ],
    "published": "2025-06-09T16:44:42+00:00",
    "summary": "Accurate driving cycle construction is crucial for vehicle design, fuel economy analysis, and environmental impact assessments. A generative Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs representative driving cycles by capturing transient dynamics, acceleration, deceleration, idling, and road grade transitions while ensuring model fidelity is introduced. Leveraging a physics-informed reinforcement learning framework with Monte Carlo sampling, PIESMC delivers efficient cycle construction with reduced computational cost. Experimental evaluations on two real-world datasets demonstrate that PIESMC replicates key kinematic and energy metrics, achieving up to a 57.3% reduction in cumulative kinematic fragment errors compared to the Micro-trip-based (MTB) method and a 10.5% reduction relative to the Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude faster than conventional techniques. Analyses of vehicle-specific power distributions and wavelet-transformed frequency content further confirm its ability to reproduce experimental central tendencies and variability."
  },
  {
    "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement",
    "url": "http://arxiv.org/abs/2506.07915v1",
    "arxiv_id": "2506.07915v1",
    "authors": [
      "Dimitris Panagopoulos",
      "Adolfo Perrusquia",
      "Weisi Guo"
    ],
    "published": "2025-06-09T16:30:05+00:00",
    "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success."
  },
  {
    "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07905v1",
    "arxiv_id": "2506.07905v1",
    "authors": [
      "Jie Yang",
      "Feipeng Ma",
      "Zitian Wang",
      "Dacheng Yin",
      "Kang Rong",
      "Fengyun Rao",
      "Ruimao Zhang"
    ],
    "published": "2025-06-09T16:20:54+00:00",
    "summary": "Building on the success of text-based reasoning models like DeepSeek-R1, extending these capabilities to multimodal reasoning holds great promise. While recent works have attempted to adapt DeepSeek-R1-style reinforcement learning (RL) training paradigms to multimodal large language models (MLLM), focusing on domain-specific tasks like math and visual perception, a critical question remains: How can we achieve the general-purpose visual-language reasoning through RL? To address this challenge, we make three key efforts: (1) A novel Scalable Multimodal QA Synthesis pipeline that autonomously generates context-aware, reasoning-centric question-answer (QA) pairs directly from the given images. (2) The open-source WeThink dataset containing over 120K multimodal QA pairs with annotated reasoning paths, curated from 18 diverse dataset sources and covering various question domains. (3) A comprehensive exploration of RL on our dataset, incorporating a hybrid reward mechanism that combines rule-based verification with model-based assessment to optimize RL training efficiency across various task domains. Across 14 diverse MLLM benchmarks, we demonstrate that our WeThink dataset significantly enhances performance, from mathematical reasoning to diverse general multimodal tasks. Moreover, we show that our automated data pipeline can continuously increase data diversity to further improve model performance."
  },
  {
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "url": "http://arxiv.org/abs/2506.07900v1",
    "arxiv_id": "2506.07900v1",
    "authors": [
      "MiniCPM Team",
      "Chaojun Xiao",
      "Yuxuan Li",
      "Xu Han",
      "Yuzhuo Bai",
      "Jie Cai",
      "Haotian Chen",
      "Wentong Chen",
      "Xin Cong",
      "Ganqu Cui",
      "Ning Ding",
      "Shengdan Fan",
      "Yewei Fang",
      "Zixuan Fu",
      "Wenyu Guan",
      "Yitong Guan",
      "Junshao Guo",
      "Yufeng Han",
      "Bingxiang He",
      "Yuxiang Huang",
      "Cunliang Kong",
      "Qiuzuo Li",
      "Siyuan Li",
      "Wenhao Li",
      "Yanghao Li",
      "Yishan Li",
      "Zhen Li",
      "Dan Liu",
      "Biyuan Lin",
      "Yankai Lin",
      "Xiang Long",
      "Quanyu Lu",
      "Yaxi Lu",
      "Peiyan Luo",
      "Hongya Lyu",
      "Litu Ou",
      "Yinxu Pan",
      "Zekai Qu",
      "Qundong Shi",
      "Zijun Song",
      "Jiayuan Su",
      "Zhou Su",
      "Ao Sun",
      "Xianghui Sun",
      "Peijun Tang",
      "Fangzheng Wang",
      "Feng Wang",
      "Shuo Wang",
      "Yudong Wang",
      "Yesai Wu",
      "Zhenyu Xiao",
      "Jie Xie",
      "Zihao Xie",
      "Yukun Yan",
      "Jiarui Yuan",
      "Kaihuo Zhang",
      "Lei Zhang",
      "Linyue Zhang",
      "Xueren Zhang",
      "Yudi Zhang",
      "Hengyu Zhao",
      "Weilin Zhao",
      "Weilun Zhao",
      "Yuanqian Zhao",
      "Zhi Zheng",
      "Ge Zhou",
      "Jie Zhou",
      "Wei Zhou",
      "Zihan Zhou",
      "Zixuan Zhou",
      "Zhiyuan Liu",
      "Guoyang Zeng",
      "Chao Jia",
      "Dahai Li",
      "Maosong Sun"
    ],
    "published": "2025-06-09T16:16:50+00:00",
    "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability."
  },
  {
    "title": "Diffusion-RL for Scalable Resource Allocation for 6G Networks",
    "url": "http://arxiv.org/abs/2506.07880v1",
    "arxiv_id": "2506.07880v1",
    "authors": [
      "Salar Nouri",
      "Mojdeh Karbalaee Motalleb",
      "Vahid Shah-Mansouri"
    ],
    "published": "2025-06-09T15:52:18+00:00",
    "summary": "This paper presents a novel approach to resource allocation in Open Radio Access Networks (O-RAN), leveraging a Generative AI technique with network slicing to address the diverse demands of 5G and 6G service types such as Enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communications (URLLC), and Massive Machine-Type Communications (mMTC). Additionally, we provide a comprehensive analysis and comparison of machine learning (ML) techniques for resource allocation within O-RAN, evaluating their effectiveness in optimizing network performance. We introduce a diffusion-based reinforcement learning (Diffusion-RL) algorithm designed to optimize the allocation of physical resource blocks (PRBs) and power consumption, thereby maximizing weighted throughput and minimizing the delay for user equipment (UE). The Diffusion-RL model incorporates controlled noise and perturbations to explore optimal resource distribution while meeting each service type's Quality of Service (QoS) requirements. We evaluate the performance of our proposed method against several benchmarks, including an exhaustive search algorithm, deep Q-networks (DQN), and the Semi-Supervised Variational Autoencoder (SS-VAE). Comprehensive metrics, such as throughput and latency, are presented for each service type. Experimental results demonstrate that the Diffusion-based RL approach outperforms existing methods in efficiency, scalability, and robustness, offering a promising solution for resource allocation in dynamic and heterogeneous O-RAN environments with significant implications for future 6G networks."
  },
  {
    "title": "Versatile Loco-Manipulation through Flexible Interlimb Coordination",
    "url": "http://arxiv.org/abs/2506.07876v1",
    "arxiv_id": "2506.07876v1",
    "authors": [
      "Xinghao Zhu",
      "Yuxin Chen",
      "Lingfeng Sun",
      "Farzad Niroui",
      "Simon Le CleacH",
      "Jiuguang Wang",
      "Kuan Fang"
    ],
    "published": "2025-06-09T15:49:29+00:00",
    "summary": "The ability to flexibly leverage limbs for loco-manipulation is essential for enabling autonomous robots to operate in unstructured environments. Yet, prior work on loco-manipulation is often constrained to specific tasks or predetermined limb configurations. In this work, we present Reinforcement Learning for Interlimb Coordination (ReLIC), an approach that enables versatile loco-manipulation through flexible interlimb coordination. The key to our approach is an adaptive controller that seamlessly bridges the execution of manipulation motions and the generation of stable gaits based on task demands. Through the interplay between two controller modules, ReLIC dynamically assigns each limb for manipulation or locomotion and robustly coordinates them to achieve the task success. Using efficient reinforcement learning in simulation, ReLIC learns to perform stable gaits in accordance with the manipulation goals in the real world. To solve diverse and complex tasks, we further propose to interface the learned controller with different types of task specifications, including target trajectories, contact points, and natural language instructions. Evaluated on 12 real-world tasks that require diverse and complex coordination patterns, ReLIC demonstrates its versatility and robustness by achieving a success rate of 78.9% on average. Videos and code can be found at https://relic-locoman.github.io/."
  },
  {
    "title": "Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing",
    "url": "http://arxiv.org/abs/2506.07859v1",
    "arxiv_id": "2506.07859v1",
    "authors": [
      "Amanuel Anteneh L\u00e9andre Brunel",
      "Carlos Gonz\u00e1lez-Arciniegas",
      "Olivier Pfister"
    ],
    "published": "2025-06-09T15:22:54+00:00",
    "summary": "Cubic-phase states are a sufficient resource for universal quantum computing over continuous variables. We present results from numerical experiments in which deep neural networks are trained via reinforcement learning to control a quantum optical circuit for generating cubic-phase states, with an average success rate of 96%. The only non-Gaussian resource required is photon-number-resolving measurements. We also show that the exact same resources enable the direct generation of a quartic-phase gate, with no need for a cubic gate decomposition."
  },
  {
    "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information",
    "url": "http://arxiv.org/abs/2506.07829v1",
    "arxiv_id": "2506.07829v1",
    "authors": [
      "Jan Corazza",
      "Hadi Partovi Aria",
      "Hyohun Kim",
      "Daniel Neider",
      "Zhe Xu"
    ],
    "published": "2025-06-09T14:53:03+00:00",
    "summary": "Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL."
  },
  {
    "title": "Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation",
    "url": "http://arxiv.org/abs/2506.07822v1",
    "arxiv_id": "2506.07822v1",
    "authors": [
      "Xintong Duan",
      "Yutong He",
      "Fahim Tajwar",
      "Ruslan Salakhutdinov",
      "J. Zico Kolter",
      "Jeff Schneider"
    ],
    "published": "2025-06-09T14:48:19+00:00",
    "summary": "Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While the consistency model offers a potential solution, its applications to decision-making often struggle with suboptimal demonstrations or rely on complex concurrent training of multiple networks. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method enables single-step generation while maintaining higher performance and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and long horizon planning demonstrate that our approach can achieve an 8.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time."
  },
  {
    "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
    "url": "http://arxiv.org/abs/2506.07751v1",
    "arxiv_id": "2506.07751v1",
    "authors": [
      "Silin Gao",
      "Antoine Bosselut",
      "Samy Bengio",
      "Emmanuel Abbe"
    ],
    "published": "2025-06-09T13:34:50+00:00",
    "summary": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in their reasoning. I.e., they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In contrast, our approach focuses on \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. We find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstraL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks."
  },
  {
    "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07744v1",
    "arxiv_id": "2506.07744v1",
    "authors": [
      "Seungho Baek",
      "Taegeon Park",
      "Jongchan Park",
      "Seungjun Oh",
      "Yusung Kim"
    ],
    "published": "2025-06-09T13:26:23+00:00",
    "summary": "Existing offline hierarchical reinforcement learning methods rely on high-level policy learning to generate subgoal sequences. However, their efficiency degrades as task horizons increase, and they lack effective strategies for stitching useful state transitions across different trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that formulates subgoal selection as a graph search problem rather than learning an explicit high-level policy. By embedding states into a Temporal Distance Representation (TDR) space, GAS clusters semantically similar states from different trajectories into unified graph nodes, enabling efficient transition stitching. A shortest-path algorithm is then applied to select subgoal sequences within the graph, while a low-level policy learns to reach the subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE) metric, which filters out noisy or inefficient transition states, significantly enhancing task performance. GAS outperforms prior offline HRL methods across locomotion, navigation, and manipulation tasks. Notably, in the most stitching-critical task, it achieves a score of 88.3, dramatically surpassing the previous state-of-the-art score of 1.0. Our source code is available at: https://github.com/qortmdgh4141/GAS."
  },
  {
    "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models",
    "url": "http://arxiv.org/abs/2506.07712v1",
    "arxiv_id": "2506.07712v1",
    "authors": [
      "Renjie Luo",
      "Jiaxi Li",
      "Chen Huang",
      "Wei Lu"
    ],
    "published": "2025-06-09T12:56:41+00:00",
    "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models."
  },
  {
    "title": "Synthetic Visual Genome",
    "url": "http://arxiv.org/abs/2506.07643v1",
    "arxiv_id": "2506.07643v1",
    "authors": [
      "Jae Sung Park",
      "Zixian Ma",
      "Linjie Li",
      "Chenhao Zheng",
      "Cheng-Yu Hsieh",
      "Ximing Lu",
      "Khyathi Chandu",
      "Quan Kong",
      "Norimasa Kobori",
      "Ali Farhadi",
      "Yejin Choi",
      "Ranjay Krishna"
    ],
    "published": "2025-06-09T11:09:10+00:00",
    "summary": "Reasoning over visual relationships-spatial, functional, interactional, social, etc.-is considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models (MLMs), precise reasoning over relationships and their generations remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by completing the missing relations of selected objects in existing scene graphs using a teacher MLM and a carefully designed filtering process to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further refines ROBIN's predicted scene graphs by removing unlikely relations and/or suggesting relevant ones. In total, our dataset contains 146K images and 5.6M relationships for 2.6M objects. Results show that our ROBIN-3B model, despite being trained on less than 3 million instances, outperforms similar-size models trained on over 300 million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of 88.9, surpassing the previous best of 87.4. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning task."
  },
  {
    "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
    "url": "http://arxiv.org/abs/2506.07570v1",
    "arxiv_id": "2506.07570v1",
    "authors": [
      "Yixuan Yang",
      "Zhen Luo",
      "Tongsheng Ding",
      "Junru Lu",
      "Mingqi Gao",
      "Jinyu Yang",
      "Victor Sanchez",
      "Feng Zheng"
    ],
    "published": "2025-06-09T09:13:06+00:00",
    "summary": "Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation."
  },
  {
    "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07548v1",
    "arxiv_id": "2506.07548v1",
    "authors": [
      "Weiqiang Jin",
      "Hongyang Du",
      "Guizhong Liu",
      "Dong In Kim"
    ],
    "published": "2025-06-09T08:38:18+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC."
  },
  {
    "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification",
    "url": "http://arxiv.org/abs/2506.07528v1",
    "arxiv_id": "2506.07528v1",
    "authors": [
      "Qisheng Hu",
      "Quanyu Long",
      "Wenya Wang"
    ],
    "published": "2025-06-09T08:11:43+00:00",
    "summary": "Multi-hop claim verification is inherently challenging, requiring multi-step reasoning to construct verification chains while iteratively searching for information to uncover hidden bridging facts. This process is fundamentally interleaved, as effective reasoning relies on dynamically retrieved evidence, while effective search demands reasoning to refine queries based on partial information. To achieve this, we propose Hierarchical Agent Reasoning and Information Search (HARIS), explicitly modeling the coordinated process of reasoning-driven searching and search-informed reasoning. HARIS consists of a high-level reasoning agent that focuses on constructing the main verification chain, generating factual questions when more information is needed, and a low-level search agent that iteratively retrieves more information, refining its search based on intermediate findings. This design allows each agent to specialize in its respective task, enhancing verification accuracy and interpretability. HARIS is trained using reinforcement learning with outcome-based rewards. Experimental results on the EX-FEVER and HOVER benchmarks demonstrate that HARIS achieves strong performance, greatly advancing multi-hop claim verification."
  },
  {
    "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions",
    "url": "http://arxiv.org/abs/2506.07527v1",
    "arxiv_id": "2506.07527v1",
    "authors": [
      "Lu Ma",
      "Hao Liang",
      "Meiyi Qiang",
      "Lexiang Tang",
      "Xiaochen Ma",
      "Zhen Hao Wong",
      "Junbo Niu",
      "Chengyu Shen",
      "Runming He",
      "Bin Cui",
      "Wentao Zhang"
    ],
    "published": "2025-06-09T08:11:20+00:00",
    "summary": "Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential."
  },
  {
    "title": "Towards Large Language Models with Self-Consistent Natural Language Explanations",
    "url": "http://arxiv.org/abs/2506.07523v1",
    "arxiv_id": "2506.07523v1",
    "authors": [
      "Sahar Admoni",
      "Ofra Amir",
      "Assaf Hallak",
      "Yftah Ziser"
    ],
    "published": "2025-06-09T08:06:33+00:00",
    "summary": "Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs."
  },
  {
    "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment",
    "url": "http://arxiv.org/abs/2506.07520v1",
    "arxiv_id": "2506.07520v1",
    "authors": [
      "Shun Lei",
      "Yaoxun Xu",
      "Zhiwei Lin",
      "Huaicheng Zhang",
      "Wei Tan",
      "Hangting Chen",
      "Jianwei Yu",
      "Yixuan Zhang",
      "Chenyu Yang",
      "Haina Zhu",
      "Shuai Wang",
      "Zhiyong Wu",
      "Dong Yu"
    ],
    "published": "2025-06-09T07:57:24+00:00",
    "summary": "Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation. However, existing approaches still struggle with the complex composition of songs and the scarcity of high-quality data, leading to limitations in sound quality, musicality, instruction following, and vocal-instrument harmony. To address these challenges, we introduce LeVo, an LM-based framework consisting of LeLM and a music codec. LeLM is capable of parallelly modeling two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. It employs two decoder-only transformers and a modular extension training strategy to prevent interference between different token types. To further enhance musicality and instruction following, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO). This method handles diverse human preferences through a semi-automatic data construction process and DPO post-training. Experimental results demonstrate that LeVo consistently outperforms existing methods on both objective and subjective metrics. Ablation studies further justify the effectiveness of our designs. Audio examples are available at https://levo-demo.github.io/."
  },
  {
    "title": "Reinforcement Learning via Implicit Imitation Guidance",
    "url": "http://arxiv.org/abs/2506.07505v1",
    "arxiv_id": "2506.07505v1",
    "authors": [
      "Perry Dong",
      "Alec M. Lessing",
      "Annie S. Chen",
      "Chelsea Finn"
    ],
    "published": "2025-06-09T07:32:52+00:00",
    "summary": "We study the problem of sample efficient reinforcement learning, where prior data such as demonstrations are provided for initialization in lieu of a dense reward signal. A natural approach is to incorporate an imitation learning objective, either as regularization during training or to acquire a reference policy. However, imitation learning objectives can ultimately degrade long-term performance, as it does not directly align with reward maximization. In this work, we propose to use prior data solely for guiding exploration via noise added to the policy, sidestepping the need for explicit behavior cloning constraints. The key insight in our framework, Data-Guided Noise (DGN), is that demonstrations are most useful for identifying which actions should be explored, rather than forcing the policy to take certain actions. Our approach achieves up to 2-3x improvement over prior reinforcement learning from offline data methods across seven simulated continuous control tasks."
  },
  {
    "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model",
    "url": "http://arxiv.org/abs/2506.07492v1",
    "arxiv_id": "2506.07492v1",
    "authors": [
      "Xiangkun Hu",
      "Lemin Kong",
      "Tong He",
      "David Wipf"
    ],
    "published": "2025-06-09T07:11:01+00:00",
    "summary": "The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step. Instead, through the judicious use of a reparameterization trick that induces an \\textit{implicit} reward, DPO and related methods consolidate learning to the minimization of a single loss function. And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated artifacts of the reparameterizations upon which they are based. To this end, we introduce an \\textit{explicit} preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward. Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not. Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO."
  },
  {
    "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models",
    "url": "http://arxiv.org/abs/2506.07468v1",
    "arxiv_id": "2506.07468v1",
    "authors": [
      "Mickel Liu",
      "Liwei Jiang",
      "Yancheng Liang",
      "Simon Shaolei Du",
      "Yejin Choi",
      "Tim Althoff",
      "Natasha Jaques"
    ],
    "published": "2025-06-09T06:35:12+00:00",
    "summary": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL)."
  },
  {
    "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
    "url": "http://arxiv.org/abs/2506.07464v1",
    "arxiv_id": "2506.07464v1",
    "authors": [
      "Jinyoung Park",
      "Jeehye Na",
      "Jinyoung Kim",
      "Hyunwoo J. Kim"
    ],
    "published": "2025-06-09T06:15:54+00:00",
    "summary": "Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks."
  },
  {
    "title": "ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07459v1",
    "arxiv_id": "2506.07459v1",
    "authors": [
      "Ziwen Wang",
      "Jiajun Fan",
      "Ruihan Guo",
      "Thao Nguyen",
      "Heng Ji",
      "Ge Liu"
    ],
    "published": "2025-06-09T06:08:59+00:00",
    "summary": "Protein generative models have shown remarkable promise in protein design but still face limitations in success rate, due to the scarcity of high-quality protein datasets for supervised pretraining. We present ProteinZero, a novel framework that enables scalable, automated, and continuous self-improvement of the inverse folding model through online reinforcement learning. To achieve computationally tractable online feedback, we introduce efficient proxy reward models based on ESM-fold and a novel rapid ddG predictor that significantly accelerates evaluation speed. ProteinZero employs a general RL framework balancing multi-reward maximization, KL-divergence from a reference model, and a novel protein-embedding level diversity regularization that prevents mode collapse while promoting higher sequence diversity. Through extensive experiments, we demonstrate that ProteinZero substantially outperforms existing methods across every key metric in protein design, achieving significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity. Most impressively, ProteinZero reduces design failure rates by approximately 36% - 48% compared to widely-used methods like ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates exceeding 90% across diverse and complex protein folds. Notably, the entire RL run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days, including reward computation. Our work establishes a new paradigm for protein design where models evolve continuously from their own generated outputs, opening new possibilities for exploring the vast protein design space."
  },
  {
    "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs",
    "url": "http://arxiv.org/abs/2506.07454v1",
    "arxiv_id": "2506.07454v1",
    "authors": [
      "Jared Strader",
      "Aaron Ray",
      "Jacob Arkin",
      "Mason B. Peterson",
      "Yun Chang",
      "Nathan Hughes",
      "Christopher Bradley",
      "Yi Xuan Jia",
      "Carlos Nieto-Granda",
      "Rajat Talak",
      "Chuchu Fan",
      "Luca Carlone",
      "Jonathan P. How",
      "Nicholas Roy"
    ],
    "published": "2025-06-09T06:02:34+00:00",
    "summary": "In this paper, we introduce a multi-robot system that integrates mapping, localization, and task and motion planning (TAMP) enabled by 3D scene graphs to execute complex instructions expressed in natural language. Our system builds a shared 3D scene graph incorporating an open-set object-based map, which is leveraged for multi-robot 3D scene graph fusion. This representation supports real-time, view-invariant relocalization (via the object-based map) and planning (via the 3D scene graph), allowing a team of robots to reason about their surroundings and execute complex tasks. Additionally, we introduce a planning approach that translates operator intent into Planning Domain Definition Language (PDDL) goals using a Large Language Model (LLM) by leveraging context from the shared 3D scene graph and robot capabilities. We provide an experimental assessment of the performance of our system on real-world tasks in large-scale, outdoor environments."
  },
  {
    "title": "An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07411v1",
    "arxiv_id": "2506.07411v1",
    "authors": [
      "Ze Yang",
      "Yihong Jin",
      "Juntian Liu",
      "Xinhe Xu"
    ],
    "published": "2025-06-09T04:14:05+00:00",
    "summary": "As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios."
  },
  {
    "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks",
    "url": "http://arxiv.org/abs/2506.07392v1",
    "arxiv_id": "2506.07392v1",
    "authors": [
      "Yuyang Zhou",
      "Guang Cheng",
      "Kang Du",
      "Zihan Chen",
      "Tian Qin",
      "Yuyu Zhao"
    ],
    "published": "2025-06-09T03:33:04+00:00",
    "summary": "The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide range of mission-critical applications, but also exposes UAV networks to severe Denial-of-Service (DoS) threats due to their open wireless environment, dynamic topology, and resource constraints. Traditional static or centralized defense mechanisms are often inadequate for such dynamic and distributed scenarios. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically, we design three lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, that leverage the inherent flexibility of UAV swarms to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process (POMDP), capturing the distributed, resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV is equipped with a local policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense policies via reward-weighted aggregation, enabling distributed learning without sharing raw data and thus reducing communication overhead. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, while maintaining robust mission continuity under various DoS attack strategies."
  },
  {
    "title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition",
    "url": "http://arxiv.org/abs/2506.07259v1",
    "arxiv_id": "2506.07259v1",
    "authors": [
      "Daolang Huang",
      "Xinyi Wen",
      "Ayush Bharti",
      "Samuel Kaski",
      "Luigi Acerbi"
    ],
    "published": "2025-06-08T19:15:34+00:00",
    "summary": "Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points."
  },
  {
    "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification",
    "url": "http://arxiv.org/abs/2506.07235v1",
    "arxiv_id": "2506.07235v1",
    "authors": [
      "Tianyi Bai",
      "Zengjie Hu",
      "Fupeng Sun",
      "Jiantao Qiu",
      "Yizhen Jiang",
      "Guangxin He",
      "Bohan Zeng",
      "Conghui He",
      "Binhang Yuan",
      "Wentao Zhang"
    ],
    "published": "2025-06-08T17:38:49+00:00",
    "summary": "Multi-modal large language models (MLLMs) have achieved remarkable capabilities by integrating visual perception with language understanding, enabling applications such as image-grounded dialogue, visual question answering, and scientific analysis. However, most MLLMs adopt a static inference paradigm, encoding the entire image into fixed visual tokens upfront, which limits their ability to iteratively refine understanding or adapt to context during inference. This contrasts sharply with human perception, which is dynamic, selective, and feedback-driven. In this work, we introduce a novel framework for inference-time visual token scaling that enables MLLMs to perform iterative, verifier-guided reasoning over visual content. We formulate the problem as a Markov Decision Process, involving a reasoner that proposes visual actions and a verifier, which is trained via multi-step Direct Preference Optimization (DPO), that evaluates these actions and determines when reasoning should terminate. To support this, we present a new dataset, VTS, comprising supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning comparisons (VTS-DPO). Our method significantly outperforms existing approaches across diverse visual reasoning benchmarks, offering not only improved accuracy but also more interpretable and grounded reasoning processes. These results demonstrate the promise of dynamic inference mechanisms for enabling fine-grained, context-aware visual reasoning in next-generation MLLMs."
  },
  {
    "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments",
    "url": "http://arxiv.org/abs/2506.07232v1",
    "arxiv_id": "2506.07232v1",
    "authors": [
      "Xinran Li",
      "Chenjia Bai",
      "Zijian Li",
      "Jiakun Zheng",
      "Ting Xiao",
      "Jun Zhang"
    ],
    "published": "2025-06-08T17:32:03+00:00",
    "summary": "Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities."
  },
  {
    "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments",
    "url": "http://arxiv.org/abs/2506.07223v1",
    "arxiv_id": "2506.07223v1",
    "authors": [
      "Yangqing Zheng",
      "Shunqi Mao",
      "Dingxin Zhang",
      "Weidong Cai"
    ],
    "published": "2025-06-08T17:09:26+00:00",
    "summary": "In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios."
  },
  {
    "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward",
    "url": "http://arxiv.org/abs/2506.07218v1",
    "arxiv_id": "2506.07218v1",
    "authors": [
      "Tong Xiao",
      "Xin Xu",
      "Zhenya Huang",
      "Hongyu Gao",
      "Quan Liu",
      "Qi Liu",
      "Enhong Chen"
    ],
    "published": "2025-06-08T16:48:42+00:00",
    "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data."
  },
  {
    "title": "Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation",
    "url": "http://arxiv.org/abs/2506.07202v1",
    "arxiv_id": "2506.07202v1",
    "authors": [
      "Ming Liu",
      "Wensheng Zhang"
    ],
    "published": "2025-06-08T15:52:38+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task \"ability vector.\" We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting."
  },
  {
    "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless Agent Actions",
    "url": "http://arxiv.org/abs/2506.07200v1",
    "arxiv_id": "2506.07200v1",
    "authors": [
      "Kanato Nakanishi",
      "Soramichi Akiyama"
    ],
    "published": "2025-06-08T15:48:16+00:00",
    "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak sensitive data, posing a severe threat to modern systems. Despite its severity, analyzing the vulnerability of a given cache structure against cache-timing attacks is challenging. To this end, a method based on Reinforcement Learning (RL) has been proposed to automatically explore vulnerabilities for a given cache structure. However, a naive RL-based approach suffers from inefficiencies due to the agent performing actions that do not contribute to the exploration. In this paper, we propose a method to identify these useless actions during training and penalize them so that the agent avoids them and the exploration efficiency is improved. Experiments on 17 cache structures show that our training mechanism reduces the number of useless actions by up to 43.08%. This resulted in the reduction of training time by 28\\% in the base case and 4.84\\% in the geomean compared to a naive RL-based approach."
  },
  {
    "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization",
    "url": "http://arxiv.org/abs/2506.07160v1",
    "arxiv_id": "2506.07160v1",
    "authors": [
      "Yikun Wang",
      "Yibin Wang",
      "Dianyi Wang",
      "Zimian Peng",
      "Qipeng Guo",
      "Dacheng Tao",
      "Jiaqi Wang"
    ],
    "published": "2025-06-08T14:18:15+00:00",
    "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks."
  },
  {
    "title": "Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07134v1",
    "arxiv_id": "2506.07134v1",
    "authors": [
      "Eshwar S. R.",
      "Gugan Thoppe",
      "Aditya Gopalan",
      "Gal Dalal"
    ],
    "published": "2025-06-08T13:27:11+00:00",
    "summary": "Despite decades of research, it remains challenging to correctly use Reinforcement Learning (RL) algorithms with function approximation. A prime example is policy iteration, whose fundamental guarantee of monotonic improvement collapses even under linear function approximation. To address this issue, we introduce Reliable Policy Iteration (RPI). It replaces the common projection or Bellman-error minimization during policy evaluation with a Bellman-based constrained optimization. We prove that not only does RPI confer textbook monotonicity on its value estimates but these estimates also lower bound the true return. Also, their limit partially satisfies the unprojected Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first algorithm with such monotonicity and convergence guarantees under function approximation. For practical use, we provide a model-free variant of RPI that amounts to a novel critic. It can be readily integrated into primary model-free PI implementations such as DQN and DDPG. In classical control tasks, such RPI-enhanced variants consistently maintain their lower-bound guarantee while matching or surpassing the performance of all baseline methods."
  },
  {
    "title": "How Far Are We from Optimal Reasoning Efficiency?",
    "url": "http://arxiv.org/abs/2506.07104v1",
    "arxiv_id": "2506.07104v1",
    "authors": [
      "Jiaxuan Gao",
      "Shu Yan",
      "Qixin Tan",
      "Lu Yang",
      "Shusheng Xu",
      "Wei Fu",
      "Zhiyu Mei",
      "Kaifeng Lyu",
      "Yi Wu"
    ],
    "published": "2025-06-08T12:18:50+00:00",
    "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference costs and limits practical deployment. While existing fine-tuning methods aim to improve reasoning efficiency, assessing their efficiency gains remains challenging due to inconsistent evaluations. In this work, we introduce the reasoning efficiency frontiers, empirical upper bounds derived from fine-tuning base LRMs across diverse approaches and training configurations. Based on these frontiers, we propose the Reasoning Efficiency Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from these frontiers. Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods: they either sacrifice accuracy for short length or still remain inefficient under tight token budgets. To reduce the efficiency gap, we propose REO-RL, a class of Reinforcement Learning algorithms that minimizes REG by targeting a sparse set of token budgets. Leveraging numerical integration over strategically selected budgets, REO-RL approximates the full efficiency objective with low error using a small set of token budgets. Through systematic benchmarking, we demonstrate that our efficiency metric, REG, effectively captures the accuracy-length trade-off, with low-REG methods reducing length while maintaining accuracy. Our approach, REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss. Ablation studies confirm the effectiveness of our exponential token budget strategy. Finally, our findings highlight that fine-tuning LRMs to perfectly align with the efficiency frontiers remains an open challenge."
  },
  {
    "title": "State Entropy Regularization for Robust Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07085v1",
    "arxiv_id": "2506.07085v1",
    "authors": [
      "Uri Koren",
      "Yonatan Ashlag",
      "Mirco Mutti",
      "Esther Derman",
      "Pierre-Luc Bacon",
      "Shie Mannor"
    ],
    "published": "2025-06-08T11:15:31+00:00",
    "summary": "State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation."
  },
  {
    "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)",
    "url": "http://arxiv.org/abs/2506.07079v1",
    "arxiv_id": "2506.07079v1",
    "authors": [
      "Mostafa Eslami",
      "Maryam Babazadeh"
    ],
    "published": "2025-06-08T10:44:01+00:00",
    "summary": "This paper introduces a hypothetical hybrid control framework for port-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition based on Data-Assisted Control (DAC). The system's evolution is split into two parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable $\\Pi$ serves as the interface between these two components. A nonlinear controller manages the intrinsic Hamiltonian flow, determining a desired port control value $\\Pi_c$. Concurrently, Reinforcement Learning (RL) is applied to the dissipative/input flow to learn an agent for providing optimal policy in mapping $\\Pi_c$ to the actual system input. This hybrid approach effectively manages RHS uncertainties while preserving the system's inherent structure. Key advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability through the port variable $\\Pi$, the ability to guarantee safety and state attainability with hard/soft constraints, reduced complexity in learning hypothesis classes compared to end-to-end solutions, and improved state/parameter estimation using LHS prior knowledge and system Hamiltonian to address partial observability. The paper details the p$\\mathcal{H}$ formulation, derives the decomposition, and presents the modular controller architecture. Beyond design, crucial aspects of stability and robustness analysis and synthesis are investigated, paving the way for deeper theoretical investigations. An application example, a pendulum with nonlinear dynamics, is simulated to demonstrate the approach's empirical and phenomenological benefits for future research."
  },
  {
    "title": "Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead",
    "url": "http://arxiv.org/abs/2506.07054v1",
    "arxiv_id": "2506.07054v1",
    "authors": [
      "Uri Koren",
      "Navdeep Kumar",
      "Uri Gadot",
      "Giorgia Ramponi",
      "Kfir Yehuda Levy",
      "Shie Mannor"
    ],
    "published": "2025-06-08T09:28:11+00:00",
    "summary": "Classical policy gradient (PG) methods in reinforcement learning frequently converge to suboptimal local optima, a challenge exacerbated in large or complex environments. This work investigates Policy Gradient with Tree Search (PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance policy optimization. We provide theoretical analysis demonstrating that increasing the tree search depth $m$-monotonically reduces the set of undesirable stationary points and, consequently, improves the worst-case performance of any resulting stationary policy. Critically, our analysis accommodates practical scenarios where policy updates are restricted to states visited by the current policy, rather than requiring updates across the entire state space. Empirical evaluations on diverse MDP structures, including Ladder, Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit \"farsightedness,\" navigate challenging reward landscapes, escape local traps where standard PG fails, and achieve superior solutions."
  },
  {
    "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
    "url": "http://arxiv.org/abs/2506.07047v1",
    "arxiv_id": "2506.07047v1",
    "authors": [
      "Yu Xuejun",
      "Jianyuan Zhong",
      "Zijin Feng",
      "Pengyi Zhai",
      "Roozbeh Yousefzadeh",
      "Wei Chong Ng",
      "Haoxiong Liu",
      "Ziyi Shou",
      "Jing Xiong",
      "Yudong Zhou",
      "Claudia Beth Ong",
      "Austen Jeremy Sugiarto",
      "Yaoxi Zhang",
      "Wai Ming Tai",
      "Huan Cao",
      "Dongcai Lu",
      "Jiacheng Sun",
      "Qiang Xu",
      "Shen Xin",
      "Zhenguo Li"
    ],
    "published": "2025-06-08T09:04:14+00:00",
    "summary": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal."
  },
  {
    "title": "QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine",
    "url": "http://arxiv.org/abs/2506.07046v1",
    "arxiv_id": "2506.07046v1",
    "authors": [
      "Anushka Jha",
      "Tanushree Dewangan",
      "Mukul Lokhande",
      "Santosh Kumar Vishvakarma"
    ],
    "published": "2025-06-08T08:55:49+00:00",
    "summary": "Reinforcement Learning (RL) has outperformed other counterparts in sequential decision-making and dynamic environment control. However, FPGA deployment is significantly resource-expensive, as associated with large number of computations in training agents with high-quality images and possess new challenges. In this work, we propose QForce-RL takes benefits of quantization to enhance throughput and reduce energy footprint with light-weight RL architecture, without significant performance degradation. QForce-RL takes advantages from E2HRL to reduce overall RL actions to learn desired policy and QuaRL for quantization based SIMD for hardware acceleration. We have also provided detailed analysis for different RL environments, with emphasis on model size, parameters, and accelerated compute ops. The architecture is scalable for resource-constrained devices and provide parametrized efficient deployment with flexibility in latency, throughput, power, and energy efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x and better FPS - 2.6x compared to SoTA works."
  },
  {
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning",
    "url": "http://arxiv.org/abs/2506.07044v1",
    "arxiv_id": "2506.07044v1",
    "authors": [
      "LASA Team",
      "Weiwen Xu",
      "Hou Pong Chan",
      "Long Li",
      "Mahani Aljunied",
      "Ruifeng Yuan",
      "Jianyu Wang",
      "Chenghao Xiao",
      "Guizhen Chen",
      "Chaoqun Liu",
      "Zhaodonghui Li",
      "Yu Sun",
      "Junao Shen",
      "Chaojun Wang",
      "Jie Tan",
      "Deli Zhao",
      "Tingyang Xu",
      "Hao Zhang",
      "Yu Rong"
    ],
    "published": "2025-06-08T08:47:30+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ..."
  },
  {
    "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.07040v1",
    "arxiv_id": "2506.07040v1",
    "authors": [
      "Yang Xu",
      "Swetha Ganesh",
      "Vaneet Aggarwal"
    ],
    "published": "2025-06-08T08:26:27+00:00",
    "summary": "We present the first $Q$-learning and actor-critic algorithms for robust average reward Markov Decision Processes (MDPs) with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets. We show that the robust $Q$ Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm with constant functions being quotiented out. This property supports a stochastic approximation update, that learns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples. We also show that the same idea can be used for robust $Q$ function estimation, which can be further used for critic estimation. Coupling it with theories in robust policy mirror descent update, we present a natural actor-critic algorithm that attains an $\\epsilon$-optimal robust policy in $\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of distributionally robust reinforcement learning in the average reward setting."
  },
  {
    "title": "AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2506.07035v1",
    "arxiv_id": "2506.07035v1",
    "authors": [
      "Zixuan Jiang",
      "Renjing Xu"
    ],
    "published": "2025-06-08T07:59:09+00:00",
    "summary": "Deciphering protein function remains a fundamental challenge in protein representation learning. The task presents significant difficulties for protein language models (PLMs) due to the sheer volume of functional annotation categories and the highly imbalanced distribution of annotated instances across biological ontologies. Inspired by the remarkable success of reinforcement learning from human feedback (RLHF) in large language model (LLM) alignment, we propose AnnoDPO, a novel multi-modal framework for protein function prediction that leverages Direct Preference Optimization (DPO) to enhance annotation learning. Our methodology addresses the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives, establishing a new paradigm for biological knowledge integration in protein representation learning."
  },
  {
    "title": "CARoL: Context-aware Adaptation for Robot Learning",
    "url": "http://arxiv.org/abs/2506.07006v1",
    "arxiv_id": "2506.07006v1",
    "authors": [
      "Zechen Hu",
      "Tong Xu",
      "Xuesu Xiao",
      "Xuan Wang"
    ],
    "published": "2025-06-08T06:05:32+00:00",
    "summary": "Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is often inefficient. Leveraging prior knowledge has the potential to significantly enhance learning efficiency, which, however, raises two critical challenges: how to determine the relevancy of existing knowledge and how to adaptively integrate them into learning a new task. In this paper, we propose Context-aware Adaptation for Robot Learning (CARoL), a novel framework to efficiently learn a similar but distinct new task from prior knowledge. CARoL incorporates context awareness by analyzing state transitions in system dynamics to identify similarities between the new task and prior knowledge. It then utilizes these identified similarities to prioritize and adapt specific knowledge pieces for the new task. Additionally, CARoL has a broad applicability spanning policy-based, value-based, and actor-critic RL algorithms. We validate the efficiency and generalizability of CARoL on both simulated robotic platforms and physical ground vehicles. The simulations include CarRacing and LunarLander environments, where CARoL demonstrates faster convergence and higher rewards when learning policies for new tasks. In real-world experiments, we show that CARoL enables a ground vehicle to quickly and efficiently adapt policies learned in simulation to smoothly traverse real-world off-road terrain."
  },
  {
    "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
    "url": "http://arxiv.org/abs/2506.06261v1",
    "arxiv_id": "2506.06261v1",
    "authors": [
      "Jihwan Jeong",
      "Xiaoyu Wang",
      "Jingmin Wang",
      "Scott Sanner",
      "Pascal Poupart"
    ],
    "published": "2025-06-06T17:40:12+00:00",
    "summary": "Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies."
  },
  {
    "title": "How to craft a deep reinforcement learning policy for wind farm flow control",
    "url": "http://arxiv.org/abs/2506.06204v1",
    "arxiv_id": "2506.06204v1",
    "authors": [
      "Elie Kadoche",
      "Pascal Bianchi",
      "Florence Carton",
      "Philippe Ciblat",
      "Damien Ernst"
    ],
    "published": "2025-06-06T16:07:05+00:00",
    "summary": "Within wind farms, wake effects between turbines can significantly reduce overall energy production. Wind farm flow control encompasses methods designed to mitigate these effects through coordinated turbine control. Wake steering, for example, consists in intentionally misaligning certain turbines with the wind to optimize airflow and increase power output. However, designing a robust wake steering controller remains challenging, and existing machine learning approaches are limited to quasi-static wind conditions or small wind farms. This work presents a new deep reinforcement learning methodology to develop a wake steering policy that overcomes these limitations. Our approach introduces a novel architecture that combines graph attention networks and multi-head self-attention blocks, alongside a novel reward function and training strategy. The resulting model computes the yaw angles of each turbine, optimizing energy production in time-varying wind conditions. An empirical study conducted on steady-state, low-fidelity simulation, shows that our model requires approximately 10 times fewer training steps than a fully connected neural network and achieves more robust performance compared to a strong optimization baseline, increasing energy production by up to 14 %. To the best of our knowledge, this is the first deep reinforcement learning-based wake steering controller to generalize effectively across any time-varying wind conditions in a low-fidelity, steady-state numerical simulation setting."
  },
  {
    "title": "A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization",
    "url": "http://arxiv.org/abs/2506.06179v1",
    "arxiv_id": "2506.06179v1",
    "authors": [
      "Muhammed Ustaomeroglu",
      "Guannan Qu"
    ],
    "published": "2025-06-06T15:44:10+00:00",
    "summary": "Self-attention has emerged as a core component of modern neural architectures, yet its theoretical underpinnings remain elusive. In this paper, we study self-attention through the lens of interacting entities, ranging from agents in multi-agent reinforcement learning to alleles in genetic sequences, and show that a single layer linear self-attention can efficiently represent, learn, and generalize functions capturing pairwise interactions, including out-of-distribution scenarios. Our analysis reveals that self-attention acts as a mutual interaction learner under minimal assumptions on the diversity of interaction patterns observed during training, thereby encompassing a wide variety of real-world domains. In addition, we validate our theoretical insights through experiments demonstrating that self-attention learns interaction functions and generalizes across both population distributions and out-of-distribution scenarios. Building on our theories, we introduce HyperFeatureAttention, a novel neural network module designed to learn couplings of different feature-level interactions between entities. Furthermore, we propose HyperAttention, a new module that extends beyond pairwise interactions to capture multi-entity dependencies, such as three-way, four-way, or general n-way interactions."
  },
  {
    "title": "Reusing Trajectories in Policy Gradients Enables Fast Convergence",
    "url": "http://arxiv.org/abs/2506.06178v1",
    "arxiv_id": "2506.06178v1",
    "authors": [
      "Alessandro Montenegro",
      "Federico Mansutti",
      "Marco Mussi",
      "Matteo Papini",
      "Alberto Maria Metelli"
    ],
    "published": "2025-06-06T15:42:15+00:00",
    "summary": "Policy gradient (PG) methods are a class of effective reinforcement learning algorithms, particularly when dealing with continuous control problems. These methods learn the parameters of parametric policies via stochastic gradient ascent, typically using on-policy trajectory data to estimate the policy gradient. However, such reliance on fresh data makes them sample-inefficient. Indeed, vanilla PG methods require $O(\\epsilon^{-2})$ trajectories to reach an $\\epsilon$-approximate stationary point. A common strategy to improve efficiency is to reuse off-policy information from past iterations, such as previous gradients or trajectories. While gradient reuse has received substantial theoretical attention, leading to improved rates of $O(\\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored from a theoretical perspective. In this work, we provide the first rigorous theoretical evidence that extensive reuse of past off-policy trajectories can significantly accelerate convergence in PG methods. We introduce a power mean correction to the multiple importance weighting estimator and propose RPG (Retrospective Policy Gradient), a PG algorithm that combines old and new trajectories for policy updates. Through a novel analysis, we show that, under established assumptions, RPG achieves a sample complexity of $\\widetilde{O}(\\epsilon^{-1})$, the best known rate in the literature. We further validate empirically our approach against PG methods with state-of-the-art rates."
  },
  {
    "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach",
    "url": "http://arxiv.org/abs/2506.06175v1",
    "arxiv_id": "2506.06175v1",
    "authors": [
      "James Ford",
      "Anthony Rios"
    ],
    "published": "2025-06-06T15:39:17+00:00",
    "summary": "Large language models can translate natural-language chart descriptions into runnable code, yet approximately 15\\% of the generated scripts still fail to execute, even after supervised fine-tuning and reinforcement learning. We investigate whether this persistent error rate stems from model limitations or from reliance on a single-prompt design. To explore this, we propose a lightweight multi-agent pipeline that separates drafting, execution, repair, and judgment, using only an off-the-shelf GPT-4o-mini model. On the \\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\\% within three repair iterations, outperforming the strongest fine-tuned baseline by nearly 5 percentage points while requiring significantly less compute. Similar performance is observed on the \\textsc{ChartX} benchmark, with an error rate of 4.6\\%, demonstrating strong generalization. Under current benchmarks, execution success appears largely solved. However, manual review reveals that 6 out of 100 sampled charts contain hallucinations, and an LLM-based accessibility audit shows that only 33.3\\% (\\textsc{Text2Chart31}) and 7.2\\% (\\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines. These findings suggest that future work should shift focus from execution reliability toward improving chart aesthetics, semantic fidelity, and accessibility."
  },
  {
    "title": "Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models",
    "url": "http://arxiv.org/abs/2506.06137v1",
    "arxiv_id": "2506.06137v1",
    "authors": [
      "Rihui Jin",
      "Zheyu Xin",
      "Xing Xie",
      "Zuoyi Li",
      "Guilin Qi",
      "Yongrui Chen",
      "Xinbang Dai",
      "Tongtong Wu",
      "Gholamreza Haffari"
    ],
    "published": "2025-06-06T14:52:19+00:00",
    "summary": "Table reasoning (TR) requires structured reasoning over semi-structured tabular data and remains challenging, particularly for small language models (SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs (LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR), which circumvents key limitations of text-based TR (T-TR), notably in numerical reasoning, by generating executable programs. However, applying P-TR to SLMs introduces two challenges: (i) vulnerability to heterogeneity in table layouts, and (ii) inconsistency in reasoning due to limited code generation capability. We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1 introduces an innovative self-supervised learning task, Layout Transformation Inference, to improve tabular layout generalization from a programmatic view. Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization, enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed. Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all SLM-based methods, achieving at least a 15% accuracy improvement over the base model (LLaMA-8B) across all datasets and reaching performance competitive with LLMs."
  },
  {
    "title": "Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library",
    "url": "http://arxiv.org/abs/2506.06122v1",
    "arxiv_id": "2506.06122v1",
    "authors": [
      "Weixun Wang",
      "Shaopan Xiong",
      "Gengru Chen",
      "Wei Gao",
      "Sheng Guo",
      "Yancheng He",
      "Ju Huang",
      "Jiaheng Liu",
      "Zhendong Li",
      "Xiaoyang Li",
      "Zichen Liu",
      "Haizhou Zhao",
      "Dakai An",
      "Lunxi Cao",
      "Qiyang Cao",
      "Wanxi Deng",
      "Feilei Du",
      "Yiliang Gu",
      "Jiahe Li",
      "Xiang Li",
      "Mingjie Liu",
      "Yijia Luo",
      "Zihe Liu",
      "Yadao Wang",
      "Pei Wang",
      "Tianyuan Wu",
      "Yanan Wu",
      "Yuheng Zhao",
      "Shuaibing Zhao",
      "Jin Yang",
      "Siran Yang",
      "Yingshui Tan",
      "Huimin Yi",
      "Yuchi Xu",
      "Yujin Yuan",
      "Xingyao Zhang",
      "Lin Qu",
      "Wenbo Su",
      "Wei Wang",
      "Jiamang Wang",
      "Bo Zheng"
    ],
    "published": "2025-06-06T14:33:56+00:00",
    "summary": "We introduce ROLL, an efficient, scalable, and user-friendly library designed for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters to three primary user groups: tech pioneers aiming for cost-effective, fault-tolerant large-scale training, developers requiring flexible control over training workflows, and researchers seeking agile experimentation. ROLL is built upon several key modules to serve these user groups effectively. First, a single-controller architecture combined with an abstraction of the parallel worker simplifies the development of the training pipeline. Second, the parallel strategy and data transfer modules enable efficient and scalable training. Third, the rollout scheduler offers fine-grained management of each sample's lifecycle during the rollout stage. Fourth, the environment worker and reward worker support rapid and flexible experimentation with agentic RL algorithms and reward designs. Finally, AutoDeviceMapping allows users to assign resources to different models flexibly across various stages."
  },
  {
    "title": "On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems",
    "url": "http://arxiv.org/abs/2506.06094v1",
    "arxiv_id": "2506.06094v1",
    "authors": [
      "Elim Kwan",
      "Rehman Qureshi",
      "Liam Fletcher",
      "Colin Laganier",
      "Victoria Nockles",
      "Richard Walters"
    ],
    "published": "2025-06-06T13:54:19+00:00",
    "summary": "Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems."
  },
  {
    "title": "Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning",
    "url": "http://arxiv.org/abs/2506.06093v1",
    "arxiv_id": "2506.06093v1",
    "authors": [
      "Atharv Kulkarni",
      "Vivek Srikumar"
    ],
    "published": "2025-06-06T13:52:41+00:00",
    "summary": "In this work, we study the problem of code generation with a large language model (LLM), with a focus on generating SQL queries from natural language questions. We ask: Instead of using supervised fine tuning with text-code pairs, can we tune a model by having it interact with a database engine? We frame this problem as a reinforcement learning problem where the model receives execution-based feedback from the environment in the form of scalar rewards. These rewards penalize execution failures and assign positive values when a query returns a correct answer. We use the rewards within the Group Relative Policy Optimization (GRPO) framework. We use a tabular reasoning benchmark to test and evaluate our findings. We find that with only weak supervision in the form of question-answer pairs, RL-tuning improves the accuracy of model generated SQL code from 31.49 to 49.83 while reducing error percentage from 25.43% to 14.71%. This improvement allowed the model nearly match the performance performance to the larger SQLCoder-70B model. Our work demonstrates the potential of using execution-based feedback to improve symbolic reasoning capabilities of LLMs."
  },
  {
    "title": "Self driving algorithm for an active four wheel drive racecar",
    "url": "http://arxiv.org/abs/2506.06077v1",
    "arxiv_id": "2506.06077v1",
    "authors": [
      "Gergely Bari",
      "Laszlo Palkovics"
    ],
    "published": "2025-06-06T13:33:15+00:00",
    "summary": "Controlling autonomous vehicles at their handling limits is a significant challenge, particularly for electric vehicles with active four wheel drive (A4WD) systems offering independent wheel torque control. While traditional Vehicle Dynamics Control (VDC) methods use complex physics-based models, this study explores Deep Reinforcement Learning (DRL) to develop a unified, high-performance controller. We employ the Proximal Policy Optimization (PPO) algorithm to train an agent for optimal lap times in a simulated racecar (TORCS) at the tire grip limit. Critically, the agent learns an end-to-end policy that directly maps vehicle states, like velocities, accelerations, and yaw rate, to a steering angle command and independent torque commands for each of the four wheels. This formulation bypasses conventional pedal inputs and explicit torque vectoring algorithms, allowing the agent to implicitly learn the A4WD control logic needed for maximizing performance and stability. Simulation results demonstrate the RL agent learns sophisticated strategies, dynamically optimizing wheel torque distribution corner-by-corner to enhance handling and mitigate the vehicle's inherent understeer. The learned behaviors mimic and, in aspects of grip utilization, potentially surpass traditional physics-based A4WD controllers while achieving competitive lap times. This research underscores DRL's potential to create adaptive control systems for complex vehicle dynamics, suggesting RL is a potent alternative for advancing autonomous driving in demanding, grip-limited scenarios for racing and road safety."
  },
  {
    "title": "Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game",
    "url": "http://arxiv.org/abs/2506.06032v1",
    "arxiv_id": "2506.06032v1",
    "authors": [
      "Edward Hughes",
      "Tina O. Zhu",
      "Martin J. Chadwick",
      "Raphael Koster",
      "Antonio Garc\u00eda Casta\u00f1eda",
      "Charles Beattie",
      "Thore Graepel",
      "Matthew M. Botvinick",
      "Joel Z. Leibo"
    ],
    "published": "2025-06-06T12:26:33+00:00",
    "summary": "Multi-agent reinforcement learning algorithms are useful for simulating social behavior in settings that are too complex for other theoretical approaches like game theory. However, they have not yet been empirically supported by laboratory experiments with real human participants. In this work we demonstrate how multi-agent reinforcement learning can model group behavior in a spatially and temporally complex public good provision game called Clean Up. We show that human groups succeed in Clean Up when they can see who is who and track reputations over time but fail under conditions of anonymity. A new multi-agent reinforcement learning model of reputation-based cooperation demonstrates the same difference between identifiable and anonymous conditions. Furthermore, both human groups and artificial agent groups solve the problem via turn-taking despite other options being available. Our results highlight the benefits of using multi-agent reinforcement learning to model human social behavior in complex environments."
  },
  {
    "title": "Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory via End-to-End Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05997v1",
    "arxiv_id": "2506.05997v1",
    "authors": [
      "Fan Yang",
      "Per Frivik",
      "David Hoeller",
      "Chen Wang",
      "Cesar Cadena",
      "Marco Hutter"
    ],
    "published": "2025-06-06T11:35:48+00:00",
    "summary": "Recent advancements in robot navigation, especially with end-to-end learning approaches like reinforcement learning (RL), have shown remarkable efficiency and effectiveness. Yet, successful navigation still relies on two key capabilities: mapping and planning, whether explicit or implicit. Classical approaches use explicit mapping pipelines to register ego-centric observations into a coherent map frame for the planner. In contrast, end-to-end learning achieves this implicitly, often through recurrent neural networks (RNNs) that fuse current and past observations into a latent space for planning. While architectures such as LSTM and GRU capture temporal dependencies, our findings reveal a key limitation: their inability to perform effective spatial memorization. This skill is essential for transforming and integrating sequential observations from varying perspectives to build spatial representations that support downstream planning. To address this, we propose Spatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification to existing RNNs, designed to enhance spatial memorization capabilities. We introduce an attention-based architecture with SRUs, enabling long-range navigation using a single forward-facing stereo camera. Regularization techniques are employed to ensure robust end-to-end recurrent training via RL. Experimental results show our approach improves long-range navigation by 23.5% compared to existing RNNs. Furthermore, with SRU memory, our method outperforms the RL baseline with explicit mapping and memory modules, achieving a 29.6% improvement in diverse environments requiring long-horizon mapping and memorization. Finally, we address the sim-to-real gap by leveraging large-scale pretraining on synthetic depth data, enabling zero-shot transfer to diverse and complex real-world environments."
  },
  {
    "title": "AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification",
    "url": "http://arxiv.org/abs/2506.05980v1",
    "arxiv_id": "2506.05980v1",
    "authors": [
      "Geonwoo Cho",
      "Jaemoon Lee",
      "Jaegyun Im",
      "Subi Lee",
      "Jihwan Lee",
      "Sundong Kim"
    ],
    "published": "2025-06-06T10:59:39+00:00",
    "summary": "Skill-based reinforcement learning (SBRL) enables rapid adaptation in environments with sparse rewards by pretraining a skill-conditioned policy. Effective skill learning requires jointly maximizing both exploration and skill diversity. However, existing methods often face challenges in simultaneously optimizing for these two conflicting objectives. In this work, we propose a new method, Adaptive Multi-objective Projection for balancing Exploration and skill Diversification (AMPED), which explicitly addresses both exploration and skill diversification. We begin by conducting extensive ablation studies to identify and define a set of objectives that effectively capture the aspects of exploration and skill diversity, respectively. During the skill pretraining phase, AMPED introduces a gradient surgery technique to balance the objectives of exploration and skill diversity, mitigating conflicts and reducing reliance on heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a skill selector module that dynamically selects suitable skills for downstream tasks, based on task-specific performance signals. Our approach achieves performance that surpasses SBRL baselines across various benchmarks. These results highlight the importance of explicitly harmonizing exploration and diversity and demonstrate the effectiveness of AMPED in enabling robust and generalizable skill learning. Project Page: https://geonwoo.me/amped/"
  },
  {
    "title": "Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05968v1",
    "arxiv_id": "2506.05968v1",
    "authors": [
      "Motoki Omura",
      "Kazuki Ota",
      "Takayuki Osa",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ],
    "published": "2025-06-06T10:46:20+00:00",
    "summary": "For continuous action spaces, actor-critic methods are widely used in online reinforcement learning (RL). However, unlike RL algorithms for discrete actions, which generally model the optimal value function using the Bellman optimality operator, RL algorithms for continuous actions typically model Q-values for the current policy using the Bellman operator. These algorithms for continuous actions rely exclusively on policy updates for improvement, which often results in low sample efficiency. This study examines the effectiveness of incorporating the Bellman optimality operator into actor-critic frameworks. Experiments in a simple environment show that modeling optimal values accelerates learning but leads to overestimation bias. To address this, we propose an annealing approach that gradually transitions from the Bellman optimality operator to the Bellman operator, thereby accelerating learning while mitigating bias. Our method, combined with TD3 and SAC, significantly outperforms existing approaches across various locomotion and manipulation tasks, demonstrating improved performance and robustness to hyperparameters related to optimality."
  },
  {
    "title": "Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes",
    "url": "http://arxiv.org/abs/2506.05953v1",
    "arxiv_id": "2506.05953v1",
    "authors": [
      "Alessandro Montenegro",
      "Leonardo Cesani",
      "Marco Mussi",
      "Matteo Papini",
      "Alberto Maria Metelli"
    ],
    "published": "2025-06-06T10:29:05+00:00",
    "summary": "Constrained Reinforcement Learning (CRL) addresses sequential decision-making problems where agents are required to achieve goals by maximizing the expected return while meeting domain-specific constraints. In this setting, policy-based methods are widely used thanks to their advantages when dealing with continuous-control problems. These methods search in the policy space with an action-based or a parameter-based exploration strategy, depending on whether they learn the parameters of a stochastic policy or those of a stochastic hyperpolicy. We introduce an exploration-agnostic algorithm, called C-PG, which enjoys global last-iterate convergence guarantees under gradient domination assumptions. Furthermore, under specific noise models where the (hyper)policy is expressed as a stochastic perturbation of the actions or of the parameters of an underlying deterministic policy, we additionally establish global last-iterate convergence guarantees of C-PG to the optimal deterministic policy. This holds when learning a stochastic (hyper)policy and subsequently switching off the stochasticity at the end of training, thereby deploying a deterministic policy. Finally, we empirically validate both the action-based (C-PGAE) and parameter-based (C-PGPE) variants of C-PG on constrained control tasks, and compare them against state-of-the-art baselines, demonstrating their effectiveness, in particular when deploying deterministic policies after training."
  },
  {
    "title": "Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router",
    "url": "http://arxiv.org/abs/2506.05901v1",
    "arxiv_id": "2506.05901v1",
    "authors": [
      "Chenyang Shao",
      "Xinyang Liu",
      "Yutang Lin",
      "Fengli Xu",
      "Yong Li"
    ],
    "published": "2025-06-06T09:18:56+00:00",
    "summary": "Multi-step reasoning has proven essential for enhancing the problem-solving capabilities of Large Language Models (LLMs) by decomposing complex tasks into intermediate steps, either explicitly or implicitly. Extending the reasoning chain at test time through deeper thought processes or broader exploration, can furthur improve performance, but often incurs substantial costs due to the explosion in token usage. Yet, many reasoning steps are relatively simple and can be handled by more efficient smaller-scale language models (SLMs). This motivates hybrid approaches that allocate subtasks across models of varying capacities. However, realizing such collaboration requires accurate task decomposition and difficulty-aware subtask allocation, which is challenging. To address this, we propose R2-Reasoner, a novel framework that enables collaborative reasoning across heterogeneous LLMs by dynamically routing sub-tasks based on estimated complexity. At the core of our framework is a Reinforced Model Router, composed of a task decomposer and a subtask allocator. The task decomposer segments complex input queries into logically ordered subtasks, while the subtask allocator assigns each subtask to the most appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing accuracy and efficiency. To train this router, we introduce a staged pipeline that combines supervised fine-tuning on task-specific datasets with Group Relative Policy Optimization algorithm, enabling self-supervised refinement through iterative reinforcement learning. Extensive experiments across four challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85% while maintaining or surpassing baseline accuracy. Our framework paves the way for more cost-effective and adaptive LLM reasoning. The code is open-source at https://anonymous.4open.science/r/R2_Reasoner ."
  },
  {
    "title": "Policy Optimization for Continuous-time Linear-Quadratic Graphon Mean Field Games",
    "url": "http://arxiv.org/abs/2506.05894v1",
    "arxiv_id": "2506.05894v1",
    "authors": [
      "Philipp Plank",
      "Yufei Zhang"
    ],
    "published": "2025-06-06T09:06:06+00:00",
    "summary": "Multi-agent reinforcement learning, despite its popularity and empirical success, faces significant scalability challenges in large-population dynamic games. Graphon mean field games (GMFGs) offer a principled framework for approximating such games while capturing heterogeneity among players. In this paper, we propose and analyze a policy optimization framework for continuous-time, finite-horizon linear-quadratic GMFGs. Exploiting the structural properties of GMFGs, we design an efficient policy parameterization in which each player's policy is represented as an affine function of their private state, with a shared slope function and player-specific intercepts. We develop a bilevel optimization algorithm that alternates between policy gradient updates for best-response computation under a fixed population distribution, and distribution updates using the resulting policies. We prove linear convergence of the policy gradient steps to best-response policies and establish global convergence of the overall algorithm to the Nash equilibrium. The analysis relies on novel landscape characterizations over infinite-dimensional policy spaces. Numerical experiments demonstrate the convergence and robustness of the proposed algorithm under varying graphon structures, noise levels, and action frequencies."
  },
  {
    "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2506.05850v1",
    "arxiv_id": "2506.05850v1",
    "authors": [
      "Cheonbok Park",
      "Jeonghoon Kim",
      "Joosung Lee",
      "Sanghwan Bae",
      "Jaegul Choo",
      "Kangmin Yoo"
    ],
    "published": "2025-06-06T08:08:48+00:00",
    "summary": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the chain-of-thought (CoT) of a multilingual language model reverts to its dominant pre-training language even when the prompt is expressed in a different language. Recent large language models (LLMs) with reinforcement learning with verifiable reward (RLVR) have achieved strong logical reasoning performances by exposing their intermediate reasoning traces, giving rise to large reasoning models (LRMs). However, the mechanism behind multilingual reasoning in LRMs is not yet fully explored. To investigate the issue, we fine-tune multilingual LRMs with Group-Relative Policy Optimization (GRPO) on translated versions of the GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese, Korean, and Ukrainian. During training, we monitor both task accuracy and language consistency of the reasoning chains. Our experiments reveal three key findings: (i) GRPO rapidly amplifies pre-training language imbalances, leading to the erosion of low-resource languages within just a few hundred updates; (ii) language consistency reward mitigates this drift but does so at the expense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting language collapse is severely damaging and largely irreversible, as subsequent fine-tuning struggles to steer the model back toward its original target-language reasoning capabilities. Together, these findings point to a remarkable conclusion: \\textit{not all languages are trained equally for reasoning}. Furthermore, our paper sheds light on the roles of reward shaping, data difficulty, and pre-training priors in eliciting multilingual reasoning."
  },
  {
    "title": "CodeContests+: High-Quality Test Case Generation for Competitive Programming",
    "url": "http://arxiv.org/abs/2506.05817v1",
    "arxiv_id": "2506.05817v1",
    "authors": [
      "Zihan Wang",
      "Siyao Liu",
      "Yang Sun",
      "Hongyan Li",
      "Kai Shen"
    ],
    "published": "2025-06-06T07:29:01+00:00",
    "summary": "Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL."
  },
  {
    "title": "Stochastic modeling of deterministic laser chaos using generator extended dynamic mode decomposition",
    "url": "http://arxiv.org/abs/2506.05798v1",
    "arxiv_id": "2506.05798v1",
    "authors": [
      "Kakutaro Fukushi",
      "Jun Ohkubo"
    ],
    "published": "2025-06-06T06:53:12+00:00",
    "summary": "Recently, chaotic phenomena in laser dynamics have attracted much attention to its applied aspects, and a synchronization phenomenon, leader-laggard relationship, in time-delay coupled lasers has been used in reinforcement learning. In the present paper, we discuss the possibility of capturing the essential stochasticity of the leader-laggard relationship; in nonlinear science, it is known that coarse-graining allows one to derive stochastic models from deterministic systems. We derive stochastic models with the aid of the Koopman operator approach, and we clarify that the low-pass filtered data is enough to recover the essential features of the original deterministic chaos, such as peak shifts in the distribution of being the leader and a power-law behavior in the distribution of switching-time intervals. We also confirm that the derived stochastic model works well in reinforcement learning tasks, i.e., multi-armed bandit problems, as with the original laser chaos system."
  },
  {
    "title": "EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs",
    "url": "http://arxiv.org/abs/2506.05787v1",
    "arxiv_id": "2506.05787v1",
    "authors": [
      "Ivan Rodin",
      "Tz-Ying Wu",
      "Kyle Min",
      "Sharath Nittur Sridhar",
      "Antonino Furnari",
      "Subarna Tripathi",
      "Giovanni Maria Farinella"
    ],
    "published": "2025-06-06T06:33:16+00:00",
    "summary": "We introduce EASG-Bench, a question-answering benchmark for egocentric videos where the question-answering pairs are created from spatio-temporally grounded dynamic scene graphs capturing intricate relationships among actors, actions, and objects. We propose a systematic evaluation framework and evaluate several language-only and video large language models (video-LLMs) on this benchmark. We observe a performance gap in language-only and video-LLMs, especially on questions focusing on temporal ordering, thus identifying a research gap in the area of long-context video understanding. To promote the reproducibility of our findings and facilitate further research, the benchmark and accompanying code are available at the following GitHub page: https://github.com/fpv-iplab/EASG-bench."
  },
  {
    "title": "BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05762v1",
    "arxiv_id": "2506.05762v1",
    "authors": [
      "Yunpeng Qing",
      "Shuo Chen",
      "Yixiao Chi",
      "Shunyu Liu",
      "Sixu Lin",
      "Changqing Zou"
    ],
    "published": "2025-06-06T05:41:33+00:00",
    "summary": "Recent advances in offline Reinforcement Learning (RL) have proven that effective policy learning can benefit from imposing conservative constraints on pre-collected datasets. However, such static datasets often exhibit distribution bias, resulting in limited generalizability. To address this limitation, a straightforward solution is data augmentation (DA), which leverages generative models to enrich data distribution. Despite the promising results, current DA techniques focus solely on reconstructing future trajectories from given states, while ignoring the exploration of history transitions that reach them. This single-direction paradigm inevitably hinders the discovery of diverse behavior patterns, especially those leading to critical states that may have yielded high-reward outcomes. In this work, we introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework for offline RL that models both future and history trajectories from any intermediate states. Specifically, we decompose the trajectory generation task into two independent yet complementary diffusion processes: one generating forward trajectories to predict future dynamics, and the other generating backward trajectories to trace essential history transitions.BiTrajDiff can efficiently leverage critical states as anchors to expand into potentially valuable yet underexplored regions of the state space, thereby facilitating dataset diversity. Extensive experiments on the D4RL benchmark suite demonstrate that BiTrajDiff achieves superior performance compared to other advanced DA methods across various offline RL backbones."
  },
  {
    "title": "Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05760v1",
    "arxiv_id": "2506.05760v1",
    "authors": [
      "Xuanyu Lei",
      "Chenliang Li",
      "Yuning Wu",
      "Kaiming Liu",
      "Weizhou Shen",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Yang Liu"
    ],
    "published": "2025-06-06T05:40:39+00:00",
    "summary": "Recent advances in Large Language Models (LLMs) have enabled strong performance in long-form writing, yet existing supervised fine-tuning (SFT) approaches suffer from limitations such as data saturation and restricted learning capacity bounded by teacher signals. In this work, we present Writing-RL: an Adaptive Curriculum Reinforcement Learning framework to advance long-form writing capabilities beyond SFT. The framework consists of three key components: Margin-aware Data Selection strategy that prioritizes samples with high learning potential, Pairwise Comparison Reward mechanism that provides discriminative learning signals in the absence of verifiable rewards, and Dynamic Reference Scheduling approach, which plays a particularly critical role by adaptively adjusting task difficulty based on evolving model performance. Experiments on 7B-scale writer models show that our RL framework largely improves long-form writing performance over strong SFT baselines. Furthermore, we observe that models trained with long-output RL generalize surprisingly well to long-input reasoning tasks, potentially offering a promising perspective for rethinking long-context training."
  },
  {
    "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance",
    "url": "http://arxiv.org/abs/2506.05748v1",
    "arxiv_id": "2506.05748v1",
    "authors": [
      "Rudransh Agnihotri",
      "Ananya Pandey"
    ],
    "published": "2025-06-06T05:18:54+00:00",
    "summary": "Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately = 9/10 in similarity to human explanations, while zero-shot judges score around =5/10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF."
  },
  {
    "title": "Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning",
    "url": "http://arxiv.org/abs/2506.05716v1",
    "arxiv_id": "2506.05716v1",
    "authors": [
      "Adrian Ly",
      "Richard Dazeley",
      "Peter Vamplew",
      "Francisco Cruz",
      "Sunil Aryal"
    ],
    "published": "2025-06-06T03:36:19+00:00",
    "summary": "While many algorithmic extensions to Deep Q-Networks (DQN) have been proposed, there remains limited understanding of how different improvements interact. In particular, multi-step and ensemble style extensions have shown promise in reducing overestimation bias, thereby improving sample efficiency and algorithmic stability. In this paper, we introduce a novel algorithm called Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step updates to stabilise algorithmic performance. EEDQN is designed to address two major challenges in deep reinforcement learning: overestimation bias and sample efficiency. We evaluated EEDQN against standard and ensemble DQN variants across the MinAtar benchmark, a set of environments that emphasise behavioral learning while reducing representational complexity. Our results show that EEDQN achieves consistently robust performance across all tested environments, outperforming baseline DQN methods and matching or exceeding state-of-the-art ensemble DQNs in final returns on most of the MinAtar environments. These findings highlight the potential of systematically combining algorithmic improvements and provide evidence that ensemble and multi-step methods, when carefully integrated, can yield substantial gains."
  },
  {
    "title": "Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection",
    "url": "http://arxiv.org/abs/2506.05651v1",
    "arxiv_id": "2506.05651v1",
    "authors": [
      "Shanmukha Vellamcheti",
      "Sanjoy Kundu",
      "Sathyanarayanan N. Aakur"
    ],
    "published": "2025-06-06T00:43:15+00:00",
    "summary": "Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding."
  },
  {
    "title": "AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization",
    "url": "http://arxiv.org/abs/2506.05634v1",
    "arxiv_id": "2506.05634v1",
    "authors": [
      "Saeed Hedayatian",
      "Stefanos Nikolaidis"
    ],
    "published": "2025-06-05T23:34:53+00:00",
    "summary": "Quality-Diversity (QD) algorithms have shown remarkable success in discovering diverse, high-performing solutions, but rely heavily on hand-crafted behavioral descriptors that constrain exploration to predefined notions of diversity. Leveraging the equivalence between policies and occupancy measures, we present a theoretically grounded approach to automatically generate behavioral descriptors by embedding the occupancy measures of policies in Markov Decision Processes. Our method, AutoQD, leverages random Fourier features to approximate the Maximum Mean Discrepancy (MMD) between policy occupancy measures, creating embeddings whose distances reflect meaningful behavioral differences. A low-dimensional projection of these embeddings that captures the most behaviorally significant dimensions is then used as behavioral descriptors for off-the-shelf QD methods. We prove that our embeddings converge to true MMD distances between occupancy measures as the number of sampled trajectories and embedding dimensions increase. Through experiments in multiple continuous control tasks we demonstrate AutoQD's ability in discovering diverse policies without predefined behavioral descriptors, presenting a well-motivated alternative to prior methods in unsupervised Reinforcement Learning and QD optimization. Our approach opens new possibilities for open-ended learning and automated behavior discovery in sequential decision making settings without requiring domain-specific knowledge."
  },
  {
    "title": "When Maximum Entropy Misleads Policy Optimization",
    "url": "http://arxiv.org/abs/2506.05615v1",
    "arxiv_id": "2506.05615v1",
    "authors": [
      "Ruipeng Zhang",
      "Ya-Chien Chang",
      "Sicun Gao"
    ],
    "published": "2025-06-05T22:03:38+00:00",
    "summary": "The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading approach for achieving efficient learning and robust performance across many RL tasks. However, MaxEnt methods have also been shown to struggle with performance-critical control problems in practice, where non-MaxEnt algorithms can successfully learn. In this work, we analyze how the trade-off between robustness and optimality affects the performance of MaxEnt algorithms in complex control tasks: while entropy maximization enhances exploration and robustness, it can also mislead policy optimization, leading to failure in tasks that require precise, low-entropy policies. Through experiments on a variety of control problems, we concretely demonstrate this misleading effect. Our analysis leads to better understanding of how to balance reward design and entropy maximization in challenging control problems."
  },
  {
    "title": "Quantum circuits as a game: A reinforcement learning agent for quantum compilation and its application to reconfigurable neutral atom arrays",
    "url": "http://arxiv.org/abs/2506.05536v1",
    "arxiv_id": "2506.05536v1",
    "authors": [
      "Kouhei Nakaji",
      "Jonathan Wurtz",
      "Haozhe Huang",
      "Luis Mantilla Calder\u00f3n",
      "Karthik Panicker",
      "Elica Kyoseva",
      "Al\u00e1n Aspuru-Guzik"
    ],
    "published": "2025-06-05T19:34:01+00:00",
    "summary": "We introduce the \"quantum circuit daemon\" (QC-Daemon), a reinforcement learning agent for compiling quantum device operations aimed at efficient quantum hardware execution. We apply QC-Daemon to the move synthesis problem called the Atom Game, which involves orchestrating parallel circuits on reconfigurable neutral atom arrays. In our numerical simulation, the QC-Daemon is implemented by two different types of transformers with a physically motivated architecture and trained by a reinforcement learning algorithm. We observe a reduction of the logarithmic infidelity for various benchmark problems up to 100 qubits by intelligently changing the layout of atoms. Additionally, we demonstrate the transferability of our approach: a Transformer-based QC-Daemon trained on a diverse set of circuits successfully generalizes its learned strategy to previously unseen circuits."
  },
  {
    "title": "Avoiding Death through Fear Intrinsic Conditioning",
    "url": "http://arxiv.org/abs/2506.05529v1",
    "arxiv_id": "2506.05529v1",
    "authors": [
      "Rodney Sanchez",
      "Ferat Sahin",
      "Alexander Ororbia",
      "Jamison Heard"
    ],
    "published": "2025-06-05T19:24:51+00:00",
    "summary": "Biological and psychological concepts have inspired reinforcement learning algorithms to create new complex behaviors that expand agents' capacity. These behaviors can be seen in the rise of techniques like goal decomposition, curriculum, and intrinsic rewards, which have paved the way for these complex behaviors. One limitation in evaluating these methods is the requirement for engineered extrinsic for realistic environments. A central challenge in engineering the necessary reward function(s) comes from these environments containing states that carry high negative rewards, but provide no feedback to the agent. Death is one such stimuli that fails to provide direct feedback to the agent. In this work, we introduce an intrinsic reward function inspired by early amygdala development and produce this intrinsic reward through a novel memory-augmented neural network (MANN) architecture. We show how this intrinsic motivation serves to deter exploration of terminal states and results in avoidance behavior similar to fear conditioning observed in animals. Furthermore, we demonstrate how modifying a threshold where the fear response is active produces a range of behaviors that are described under the paradigm of general anxiety disorders (GADs). We demonstrate this behavior in the Miniworld Sidewalk environment, which provides a partially observable Markov decision process (POMDP) and a sparse reward with a non-descriptive terminal condition, i.e., death. In effect, this study results in a biologically-inspired neural architecture and framework for fear conditioning paradigms; we empirically demonstrate avoidance behavior in a constructed agent that is able to solve environments with non-descriptive terminal conditions."
  },
  {
    "title": "Sequence Modeling for N-Agent Ad Hoc Teamwork",
    "url": "http://arxiv.org/abs/2506.05527v1",
    "arxiv_id": "2506.05527v1",
    "authors": [
      "Caroline Wang",
      "Di Yang Shi",
      "Elad Liebman",
      "Ishan Durugkar",
      "Arrasy Rahman",
      "Peter Stone"
    ],
    "published": "2025-06-05T19:20:12+00:00",
    "summary": "N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent reinforcement learning, where controlled subteams of varying sizes must dynamically collaborate with varying numbers and types of unknown teammates without pre-coordination. The existing learning algorithm (POAM) considers only independent learning for its flexibility in dealing with a changing number of agents. However, independent learning fails to fully capture the inter-agent dynamics essential for effective collaboration. Based on our observation that transformers deal effectively with sequences with varying lengths and have been shown to be highly effective for a variety of machine learning problems, this work introduces a centralized, transformer-based method for N-agent ad hoc teamwork. Our proposed approach incorporates historical observations and actions of all controlled agents, enabling optimal responses to diverse and unseen teammates in partially observable environments. Empirical evaluation on a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving superior sample efficiency and generalization, without auxiliary agent-modeling objectives."
  },
  {
    "title": "FocusDiff: Advancing Fine-Grained Text-Image Alignment for Autoregressive Visual Generation through RL",
    "url": "http://arxiv.org/abs/2506.05501v1",
    "arxiv_id": "2506.05501v1",
    "authors": [
      "Kaihang Pan",
      "Wendong Bu",
      "Yuruo Wu",
      "Yang Wu",
      "Kai Shen",
      "Yunfei Li",
      "Hang Zhao",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "published": "2025-06-05T18:36:33+00:00",
    "summary": "Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark -- featuring test cases of paired prompts with similar syntax but different fine-grained semantics -- reveals that existing models struggle with fine-grained text-image alignment thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing a novel reinforcement learning algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves state-of-the-art performance on existing text-to-image benchmarks and significantly outperforms prior methods on PairComp."
  },
  {
    "title": "ContentV: Efficient Training of Video Generation Models with Limited Compute",
    "url": "http://arxiv.org/abs/2506.05343v1",
    "arxiv_id": "2506.05343v1",
    "authors": [
      "Wenfeng Lin",
      "Renjie Chen",
      "Boyuan Liu",
      "Shiyue Yan",
      "Ruoyu Feng",
      "Jiangchuan Wei",
      "Yichen Zhang",
      "Yimeng Zhou",
      "Chao Feng",
      "Jiao Ran",
      "Qi Wu",
      "Zuotao Liu",
      "Mingyu Guo"
    ],
    "published": "2025-06-05T17:59:54+00:00",
    "summary": "Recent advances in video generation demand increasingly efficient training recipes to mitigate escalating computational costs. In this report, we present ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art performance (85.14 on VBench) after training on 256 x 64GB Neural Processing Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality videos across multiple resolutions and durations from text prompts, enabled by three key innovations: (1) A minimalist architecture that maximizes reuse of pre-trained image generation models for video generation; (2) A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency; and (3) A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations. All the code and models are available at: https://contentv.github.io."
  },
  {
    "title": "Zeroth-Order Optimization Finds Flat Minima",
    "url": "http://arxiv.org/abs/2506.05454v1",
    "arxiv_id": "2506.05454v1",
    "authors": [
      "Liang Zhang",
      "Bingcong Li",
      "Kiran Koshy Thekumparampil",
      "Sewoong Oh",
      "Michael Muehlebach",
      "Niao He"
    ],
    "published": "2025-06-05T17:59:09+00:00",
    "summary": "Zeroth-order methods are extensively used in machine learning applications where gradients are infeasible or expensive to compute, such as black-box attacks, reinforcement learning, and language model fine-tuning. Existing optimization theory focuses on convergence to an arbitrary stationary point, but less is known on the implicit regularization that provides a fine-grained characterization on which particular solutions are finally reached. We show that zeroth-order optimization with the standard two-point estimator favors solutions with small trace of Hessian, which is widely used in previous work to distinguish between sharp and flat minima. We further provide convergence rates of zeroth-order optimization to approximate flat minima for convex and sufficiently smooth functions, where flat minima are defined as the minimizers that achieve the smallest trace of Hessian among all optimal solutions. Experiments on binary classification tasks with convex losses and language model fine-tuning support our theoretical findings."
  },
  {
    "title": "AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs",
    "url": "http://arxiv.org/abs/2506.05328v1",
    "arxiv_id": "2506.05328v1",
    "authors": [
      "Lidong Lu",
      "Guo Chen",
      "Zhiqi Li",
      "Yicheng Liu",
      "Tong Lu"
    ],
    "published": "2025-06-05T17:58:33+00:00",
    "summary": "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io."
  },
  {
    "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
    "url": "http://arxiv.org/abs/2506.05316v1",
    "arxiv_id": "2506.05316v1",
    "authors": [
      "Yifan Sun",
      "Jingyan Shen",
      "Yibin Wang",
      "Tianyu Chen",
      "Zhendong Wang",
      "Mingyuan Zhou",
      "Huan Zhang"
    ],
    "published": "2025-06-05T17:55:43+00:00",
    "summary": "Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism that reuses recent rollouts, lowering per-step computation while maintaining stable updates. Extensive experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 25% to 65% to reach the same level of performance as the original GRPO algorithm."
  },
  {
    "title": "Estimation of Treatment Effects Under Nonstationarity via Truncated Difference-in-Q's",
    "url": "http://arxiv.org/abs/2506.05308v1",
    "arxiv_id": "2506.05308v1",
    "authors": [
      "Ramesh Johari",
      "Tianyi Peng",
      "Wenqian Xing"
    ],
    "published": "2025-06-05T17:53:26+00:00",
    "summary": "Randomized controlled experiments (''A/B testing'') are fundamental for assessing interventions in dynamic technology-driven environments, such as recommendation systems, online marketplaces, and digital health interventions. In these systems, interventions typically impact not only the current state of the system, but also future states; therefore, accurate estimation of the global average treatment effect (or GATE) from experiments requires accounting for the dynamic temporal behavior of the system. To address this, recent literature has analyzed a range of estimators applied to Bernoulli randomized experiments in stationary environments, ranging from the standard difference-in-means (DM) estimator to methods building on reinforcement learning techniques, such as off-policy evaluation and the recently proposed difference-in-Q's (DQ) estimator. However, all these estimators exhibit high bias and variance when the environment is nonstationary. This paper addresses the challenge of estimation under nonstationarity. We show that a simple extension of the DM estimator using differences in truncated outcome trajectories yields favorable bias and variance in nonstationary Markovian settings. Our theoretical analysis establishes this result by first showing that the truncated estimator is in fact estimating an appropriate policy gradient that can be expressed as a difference in Q-values; thus we refer to our estimator as the truncated DQ estimator (by analogy to the DQ estimator). We then show that the corresponding policy gradient is a first-order approximation to the GATE. Combining these insights yields our bias and variance bounds. We validate our results through synthetic and realistic simulations-including hospital and ride-sharing settings-and show that a well-calibrated truncated DQ estimator achieves low bias and variance even in nonstationary environments."
  },
  {
    "title": "LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs",
    "url": "http://arxiv.org/abs/2506.05260v1",
    "arxiv_id": "2506.05260v1",
    "authors": [
      "Xiaodong Wang",
      "Jinfa Huang",
      "Li Yuan",
      "Peixi Peng"
    ],
    "published": "2025-06-05T17:21:16+00:00",
    "summary": "Most Video Large Language Models (Video-LLMs) adopt preference alignment techniques, e.g., DPO~\\citep{rafailov2024dpo}, to optimize the reward margin between a winning response ($y_w$) and a losing response ($y_l$). However, the likelihood displacement observed in DPO indicates that both $\\log \\pi_\\theta (y_w\\mid x)$ and $\\log \\pi_\\theta (y_l\\mid x) $ often decrease during training, inadvertently boosting the probabilities of non-target responses. In this paper, we systematically revisit this phenomenon from LLMs to Video-LLMs, showing that it intensifies when dealing with the redundant complexity of video content. To alleviate the impact of this phenomenon, we propose \\emph{Lean Preference Optimization} (LeanPO), a reference-free approach that reformulates the implicit reward as the average likelihood of the response with respect to the policy model. A key component of LeanPO is the reward-trustworthiness correlated self-generated preference data pipeline, which carefully infuses relevant prior knowledge into the model while continuously refining the preference data via self-reflection. This allows the policy model to obtain high-quality paired data and accurately estimate the newly defined reward, thus mitigating the unintended drop. In addition, we introduce a dynamic label smoothing strategy that mitigates the impact of noise in responses from diverse video content, preventing the model from overfitting to spurious details. Extensive experiments demonstrate that LeanPO significantly enhances the performance of state-of-the-art Video-LLMs, consistently boosting baselines of varying capacities with minimal additional training overhead. Moreover, LeanPO offers a simple yet effective solution for aligning Video-LLM preferences with human trustworthiness, paving the way toward the reliable and efficient Video-LLMs."
  },
  {
    "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05256v1",
    "arxiv_id": "2506.05256v1",
    "authors": [
      "Violet Xiang",
      "Chase Blagden",
      "Rafael Rafailov",
      "Nathan Lile",
      "Sang Truong",
      "Chelsea Finn",
      "Nick Haber"
    ],
    "published": "2025-06-05T17:17:05+00:00",
    "summary": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost."
  },
  {
    "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05256v2",
    "arxiv_id": "2506.05256v2",
    "authors": [
      "Violet Xiang",
      "Chase Blagden",
      "Rafael Rafailov",
      "Nathan Lile",
      "Sang Truong",
      "Chelsea Finn",
      "Nick Haber"
    ],
    "published": "2025-06-05T17:17:05+00:00",
    "summary": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost."
  },
  {
    "title": "Towards Language-Augmented Multi-Agent Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05236v1",
    "arxiv_id": "2506.05236v1",
    "authors": [
      "Maxime Toquebiau",
      "Jae-Yun Jun",
      "Fa\u00efz Benamar",
      "Nicolas Bredeche"
    ],
    "published": "2025-06-05T16:55:52+00:00",
    "summary": "Communication is a fundamental aspect of coordinated behavior in multi-agent reinforcement learning. Yet, most prior works in this field have focused on emergent communication protocols developed from scratch, often resulting in inefficient or non-interpretable systems. Inspired by the role of language in natural intelligence, we investigate how grounding agents in a human-defined language can improve learning and coordination of multiple embodied agents. We propose a framework in which agents are trained not only to act but also to produce and interpret natural language descriptions of their observations. This language-augmented learning serves a dual role: enabling explicit communication between agents and guiding representation learning. We demonstrate that agents trained with our method outperform traditional emergent communication baselines across various tasks. Our analysis reveals that language grounding leads to more informative internal representations, better generalization to new partners, and improved capability for human-agent interaction. These findings demonstrate the effectiveness of integrating structured language into multi-agent learning and open avenues for more interpretable and capable multi-agent systems."
  },
  {
    "title": "Optimal-PhiBE: A PDE-based Model-free framework for Continuous-time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05208v1",
    "arxiv_id": "2506.05208v1",
    "authors": [
      "Yuhua Zhu",
      "Yuming Zhang",
      "Haoyu Zhang"
    ],
    "published": "2025-06-05T16:20:15+00:00",
    "summary": "This paper addresses continuous-time reinforcement learning (CTRL) where the system dynamics are governed by a stochastic differential equation but are unknown, and only discrete-time observations are available. Existing approaches face limitations: model-based PDE methods suffer from non-identifiability, while model-free methods based on the optimal Bellman equation (Optimal-BE) are prone to large discretization errors sensitive to both the dynamics and reward structure. To overcome these challenges, we introduce Optimal-PhiBE, a formulation that integrates discrete-time information into a continuous-time PDE, combining the strength of both existing frameworks while mitigating their limitations. Optimal-PhiBE avoids explicit dynamics estimation, exhibits smaller discretization errors when the uncontrolled system evolves slowly, and demonstrates reduced sensitivity to oscillatory reward structures. In the linear-quadratic regulator (LQR) setting, sharp error bounds are established for both Optimal-PhiBE and Optimal-BE. The results show that Optimal-PhiBE exactly recovers the optimal policy in the undiscounted case and substantially outperforms Optimal-BE when the problem is weakly discounted or control-dominant. Furthermore, we extend Optimal-PhiBE to higher orders, providing increasingly accurate approximations. A model-free policy iteration algorithm is proposed to solve the Optimal-PhiBE directly from trajectory data. Numerical experiments are conducted to verify the theoretical findings."
  },
  {
    "title": "TreeRPO: Tree Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2506.05183v1",
    "arxiv_id": "2506.05183v1",
    "authors": [
      "Zhicheng Yang",
      "Zhijiang Guo",
      "Yinya Huang",
      "Xiaodan Liang",
      "Yiwei Wang",
      "Jing Tang"
    ],
    "published": "2025-06-05T15:56:38+00:00",
    "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \\textbf{\\name}, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \\name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \\name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \\name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \\name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name significantly outperforms GRPO by 2.9\\% in performance while simultaneously reducing the average response length by 18.1\\%, showcasing its effectiveness and efficiency. Our code will be available at \\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}."
  },
  {
    "title": "Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning",
    "url": "http://arxiv.org/abs/2506.05168v1",
    "arxiv_id": "2506.05168v1",
    "authors": [
      "Yunsheng Tian",
      "Joshua Jacob",
      "Yijiang Huang",
      "Jialiang Zhao",
      "Edward Gu",
      "Pingchuan Ma",
      "Annan Zhang",
      "Farhad Javid",
      "Branden Romero",
      "Sachin Chitta",
      "Shinjiro Sueda",
      "Hui Li",
      "Wojciech Matusik"
    ],
    "published": "2025-06-05T15:43:52+00:00",
    "summary": "Multi-part assembly poses significant challenges for robots to execute long-horizon, contact-rich manipulation with generalization across complex geometries. We present Fabrica, a dual-arm robotic system capable of end-to-end planning and control for autonomous assembly of general multi-part objects. For planning over long horizons, we develop hierarchies of precedence, sequence, grasp, and motion planning with automated fixture generation, enabling general multi-step assembly on any dual-arm robots. The planner is made efficient through a parallelizable design and is optimized for downstream control stability. For contact-rich assembly steps, we propose a lightweight reinforcement learning framework that trains generalist policies across object geometries, assembly directions, and grasp poses, guided by equivariance and residual actions obtained from the plan. These policies transfer zero-shot to the real world and achieve 80% successful steps. For systematic evaluation, we propose a benchmark suite of multi-part assemblies resembling industrial and daily objects across diverse categories and geometries. By integrating efficient global planning and robust local control, we showcase the first system to achieve complete and generalizable real-world multi-part assembly without domain knowledge or human demonstrations. Project website: http://fabrica.csail.mit.edu/"
  },
  {
    "title": "Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline",
    "url": "http://arxiv.org/abs/2506.05117v1",
    "arxiv_id": "2506.05117v1",
    "authors": [
      "Zihan Xu",
      "Mengxian Hu",
      "Kaiyan Xiao",
      "Qin Fang",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "published": "2025-06-05T15:02:43+00:00",
    "summary": "Human motion retargeting for humanoid robots, transferring human motion data to robots for imitation, presents significant challenges but offers considerable potential for real-world applications. Traditionally, this process relies on human demonstrations captured through pose estimation or motion capture systems. In this paper, we explore a text-driven approach to mapping human motion to humanoids. To address the inherent discrepancies between the generated motion representations and the kinematic constraints of humanoid robots, we propose an angle signal network based on norm-position and rotation loss (NPR Loss). It generates joint angles, which serve as inputs to a reinforcement learning-based whole-body joint motion control policy. The policy ensures tracking of the generated motions while maintaining the robot's stability during execution. Our experimental results demonstrate the efficacy of this approach, successfully transferring text-driven human motion to a real humanoid robot NAO."
  },
  {
    "title": "Whole-Body Constrained Learning for Legged Locomotion via Hierarchical Optimization",
    "url": "http://arxiv.org/abs/2506.05115v1",
    "arxiv_id": "2506.05115v1",
    "authors": [
      "Haoyu Wang",
      "Ruyi Zhou",
      "Liang Ding",
      "Tie Liu",
      "Zhelin Zhang",
      "Peng Xu",
      "Haibo Gao",
      "Zongquan Deng"
    ],
    "published": "2025-06-05T15:00:27+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated impressive performance in legged locomotion over various challenging environments. However, due to the sim-to-real gap and lack of explainability, unconstrained RL policies deployed in the real world still suffer from inevitable safety issues, such as joint collisions, excessive torque, or foot slippage in low-friction environments. These problems limit its usage in missions with strict safety requirements, such as planetary exploration, nuclear facility inspection, and deep-sea operations. In this paper, we design a hierarchical optimization-based whole-body follower, which integrates both hard and soft constraints into RL framework to make the robot move with better safety guarantees. Leveraging the advantages of model-based control, our approach allows for the definition of various types of hard and soft constraints during training or deployment, which allows for policy fine-tuning and mitigates the challenges of sim-to-real transfer. Meanwhile, it preserves the robustness of RL when dealing with locomotion in complex unstructured environments. The trained policy with introduced constraints was deployed in a hexapod robot and tested in various outdoor environments, including snow-covered slopes and stairs, demonstrating the great traversability and safety of our approach."
  },
  {
    "title": "RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation",
    "url": "http://arxiv.org/abs/2506.05070v1",
    "arxiv_id": "2506.05070v1",
    "authors": [
      "Tianjiao Li",
      "Mengran Yu",
      "Chenyu Shi",
      "Yanjun Zhao",
      "Xiaojing Liu",
      "Qiang Zhang",
      "Qi Zhang",
      "Xuanjing Huang",
      "Jiayin Wang"
    ],
    "published": "2025-06-05T14:18:21+00:00",
    "summary": "Large language models (LLMs) possess strong multilingual capabilities, and combining Reinforcement Learning from Human Feedback (RLHF) with translation tasks has shown great potential. However, we observe that this paradigm performs unexpectedly poorly when applied to colloquial subtitle translation tasks. In this work, we investigate this issue and find that the offline reward model (RM) gradually diverges from the online LLM due to distributional shift, ultimately leading to undesirable training outcomes. To address this, we propose RIVAL, an adversarial training framework that formulates the process as a min-max game between the RM and the LLM. RIVAL iteratively updates the both models, with the RM trained to distinguish strong from weak translations (qualitative preference reward), and the LLM trained to enhance its translation for closing this gap. To stabilize training and improve generalizability, we also incorporate quantitative preference reward (e.g., BLEU) into the RM, enabling reference-free quality modeling aligned with human evaluation. Through extensive experiments, we demonstrate that the proposed adversarial training framework significantly improves upon translation baselines."
  },
  {
    "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation",
    "url": "http://arxiv.org/abs/2506.05069v1",
    "arxiv_id": "2506.05069v1",
    "authors": [
      "Keyu Zhao",
      "Fengli Xu",
      "Yong Li"
    ],
    "published": "2025-06-05T14:16:44+00:00",
    "summary": "Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement in HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: https://anonymous.4open.science/r/R2Rec-7C5D."
  },
  {
    "title": "PulseRide: A Robotic Wheelchair for Personalized Exertion Control with Human-in-the-Loop Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.05056v1",
    "arxiv_id": "2506.05056v1",
    "authors": [
      "Azizul Zahid",
      "Bibek Poudel",
      "Danny Scott",
      "Jason Scott",
      "Scott Crouter",
      "Weizi Li",
      "Sai Swaminathan"
    ],
    "published": "2025-06-05T14:00:59+00:00",
    "summary": "Maintaining an active lifestyle is vital for quality of life, yet challenging for wheelchair users. For instance, powered wheelchairs face increasing risks of obesity and deconditioning due to inactivity. Conversely, manual wheelchair users, who propel the wheelchair by pushing the wheelchair's handrims, often face upper extremity injuries from repetitive motions. These challenges underscore the need for a mobility system that promotes activity while minimizing injury risk. Maintaining optimal exertion during wheelchair use enhances health benefits and engagement, yet the variations in individual physiological responses complicate exertion optimization. To address this, we introduce PulseRide, a novel wheelchair system that provides personalized assistance based on each user's physiological responses, helping them maintain their physical exertion goals. Unlike conventional assistive systems focused on obstacle avoidance and navigation, PulseRide integrates real-time physiological data-such as heart rate and ECG-with wheelchair speed to deliver adaptive assistance. Using a human-in-the-loop reinforcement learning approach with Deep Q-Network algorithm (DQN), the system adjusts push assistance to keep users within a moderate activity range without under- or over-exertion. We conducted preliminary tests with 10 users on various terrains, including carpet and slate, to assess PulseRide's effectiveness. Our findings show that, for individual users, PulseRide maintains heart rates within the moderate activity zone as much as 71.7 percent longer than manual wheelchairs. Among all users, we observed an average reduction in muscle contractions of 41.86 percent, delaying fatigue onset and enhancing overall comfort and engagement. These results indicate that PulseRide offers a healthier, adaptive mobility solution, bridging the gap between passive and physically taxing mobility options."
  },
  {
    "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic",
    "url": "http://arxiv.org/abs/2506.05445v1",
    "arxiv_id": "2506.05445v1",
    "authors": [
      "Thanh Vinh Vo",
      "Young Lee",
      "Haozhe Ma",
      "Chien Lu",
      "Tze-Yun Leong"
    ],
    "published": "2025-06-05T13:52:38+00:00",
    "summary": "Hidden confounders that influence both states and actions can bias policy learning in reinforcement learning (RL), leading to suboptimal or non-generalizable behavior. Most RL algorithms ignore this issue, learning policies from observational trajectories based solely on statistical associations rather than causal effects. We propose DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment), a principled extension of the SAC algorithm that corrects for hidden confounding via causal intervention estimation. DoSAC estimates the interventional policy $\\pi(a | \\mathrm{do}(s))$ using the backdoor criterion, without requiring access to true confounders or causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor that infers pseudo-past variables (previous state and action) from the current state to enable backdoor adjustment from observational data. This module is integrated into a soft actor-critic framework to compute both the interventional policy and its entropy. Empirical results on continuous control benchmarks show that DoSAC outperforms baselines under confounded settings, with improved robustness, generalization, and policy reliability."
  },
  {
    "title": "Multi-scale Image Super Resolution with a Single Auto-Regressive Model",
    "url": "http://arxiv.org/abs/2506.04990v1",
    "arxiv_id": "2506.04990v1",
    "authors": [
      "Enrique Sanchez",
      "Isma Hadji",
      "Adrian Bulat",
      "Christos Tzelepis",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "published": "2025-06-05T13:02:23+00:00",
    "summary": "In this paper we tackle Image Super Resolution (ISR), using recent advances in Visual Auto-Regressive (VAR) modeling. VAR iteratively estimates the residual in latent space between gradually increasing image scales, a process referred to as next-scale prediction. Thus, the strong priors learned during pre-training align well with the downstream task (ISR). To our knowledge, only VARSR has exploited this synergy so far, showing promising results. However, due to the limitations of existing residual quantizers, VARSR works only at a fixed resolution, i.e. it fails to map intermediate outputs to the corresponding image scales. Additionally, it relies on a 1B transformer architecture (VAR-d24), and leverages a large-scale private dataset to achieve state-of-the-art results. We address these limitations through two novel components: a) a Hierarchical Image Tokenization approach with a multi-scale image tokenizer that progressively represents images at different scales while simultaneously enforcing token overlap across scales, and b) a Direct Preference Optimization (DPO) regularization term that, relying solely on the LR and HR tokenizations, encourages the transformer to produce the latter over the former. To the best of our knowledge, this is the first time a quantizer is trained to force semantically consistent residuals at different scales, and the first time that preference-based optimization is used to train a VAR. Using these two components, our model can denoise the LR image and super-resolve at half and full target upscale factors in a single forward pass. Additionally, we achieve \\textit{state-of-the-art results on ISR}, while using a small model (300M params vs ~1B params of VARSR), and without using external training data."
  },
  {
    "title": "ArtVIP: Articulated Digital Assets of Visual Realism, Modular Interaction, and Physical Fidelity for Robot Learning",
    "url": "http://arxiv.org/abs/2506.04941v1",
    "arxiv_id": "2506.04941v1",
    "authors": [
      "Zhao Jin",
      "Zhengping Che",
      "Zhen Zhao",
      "Kun Wu",
      "Yuheng Zhang",
      "Yinuo Zhao",
      "Zehui Liu",
      "Qiang Zhang",
      "Xiaozhu Ju",
      "Jing Tian",
      "Yousong Xue",
      "Jian Tang"
    ],
    "published": "2025-06-05T12:16:27+00:00",
    "summary": "Robot learning increasingly relies on simulation to advance complex ability such as dexterous manipulations and precise interactions, necessitating high-quality digital assets to bridge the sim-to-real gap. However, existing open-source articulated-object datasets for simulation are limited by insufficient visual realism and low physical fidelity, which hinder their utility for training models mastering robotic tasks in real world. To address these challenges, we introduce ArtVIP, a comprehensive open-source dataset comprising high-quality digital-twin articulated objects, accompanied by indoor-scene assets. Crafted by professional 3D modelers adhering to unified standards, ArtVIP ensures visual realism through precise geometric meshes and high-resolution textures, while physical fidelity is achieved via fine-tuned dynamic parameters. Meanwhile, the dataset pioneers embedded modular interaction behaviors within assets and pixel-level affordance annotations. Feature-map visualization and optical motion capture are employed to quantitatively demonstrate ArtVIP 's visual and physical fidelity, with its applicability validated across imitation learning and reinforcement learning experiments. Provided in USD format with detailed production guidelines, \\ours is fully open-source, benefiting the research community and advancing robot learning research. Our project is at https://x-humanoid-artvip.github.io/"
  },
  {
    "title": "Dissecting Long Reasoning Models: An Empirical Study",
    "url": "http://arxiv.org/abs/2506.04913v1",
    "arxiv_id": "2506.04913v1",
    "authors": [
      "Yongyu Mu",
      "Jiali Zeng",
      "Bei Li",
      "Xinyan Guan",
      "Fandong Meng",
      "Jie Zhou",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "published": "2025-06-05T11:47:10+00:00",
    "summary": "Despite recent progress in training long-context reasoning models via reinforcement learning (RL), several open questions and counterintuitive behaviors remain. This work focuses on three key aspects: (1) We systematically analyze the roles of positive and negative samples in RL, revealing that positive samples mainly facilitate data fitting, whereas negative samples significantly enhance generalization and robustness. Interestingly, training solely on negative samples can rival standard RL training performance. (2) We identify substantial data inefficiency in group relative policy optimization, where over half of the samples yield zero advantage. To address this, we explore two straightforward strategies, including relative length rewards and offline sample injection, to better leverage these data and enhance reasoning efficiency and capability. (3) We investigate unstable performance across various reasoning models and benchmarks, attributing instability to uncertain problems with ambiguous outcomes, and demonstrate that multiple evaluation runs mitigate this issue."
  },
  {
    "title": "Safe Planning and Policy Optimization via World Model Learning",
    "url": "http://arxiv.org/abs/2506.04828v1",
    "arxiv_id": "2506.04828v1",
    "authors": [
      "Artem Latyshev",
      "Gregory Gorbov",
      "Aleksandr I. Panov"
    ],
    "published": "2025-06-05T09:50:02+00:00",
    "summary": "Reinforcement Learning (RL) applications in real-world scenarios must prioritize safety and reliability, which impose strict constraints on agent behavior. Model-based RL leverages predictive world models for action planning and policy optimization, but inherent model inaccuracies can lead to catastrophic failures in safety-critical settings. We propose a novel model-based RL framework that jointly optimizes task performance and safety. To address world model errors, our method incorporates an adaptive mechanism that dynamically switches between model-based planning and direct policy execution. We resolve the objective mismatch problem of traditional model-based approaches using an implicit world model. Furthermore, our framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, consistently selecting actions that surpass safe policy suggestions in both performance and safety. Experiments demonstrate significant improvements over non-adaptive methods, showing that our approach optimizes safety and performance simultaneously rather than merely meeting minimum safety requirements. The proposed framework achieves robust performance on diverse safety-critical continuous control tasks, outperforming existing methods."
  },
  {
    "title": "LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04821v1",
    "arxiv_id": "2506.04821v1",
    "authors": [
      "Zhen Hao Wong",
      "Jingwen Deng",
      "Runming He",
      "Zirong Chen",
      "Qijie You",
      "Hejun Dong",
      "Hao Liang",
      "Chengyu Shen",
      "Bin Cui",
      "Wentao Zhang"
    ],
    "published": "2025-06-05T09:40:47+00:00",
    "summary": "Large language models (LLMs) excel at many supervised tasks but often struggle with structured reasoning in unfamiliar settings. This discrepancy suggests that standard fine-tuning pipelines may instill narrow, domain-specific heuristics rather than fostering general-purpose thinking strategies. In this work, we propose a \"play to learn\" framework that fine-tunes LLMs through reinforcement learning on a suite of seven custom logic puzzles, each designed to cultivate distinct reasoning skills such as constraint propagation, spatial consistency, and symbolic deduction. Using a reinforcement learning setup with verifiable rewards, models receive binary feedback based on puzzle correctness, encouraging iterative, hypothesis-driven problem solving. We demonstrate that this training approach significantly improves out-of-distribution performance on a range of mathematical benchmarks, especially for mid-difficulty problems that require multi-step reasoning. Analyses across problem categories and difficulty levels reveal that puzzle training promotes transferable reasoning routines, strengthening algebraic manipulation, geometric inference, and combinatorial logic, while offering limited gains on rote or highly specialized tasks. These findings show that reinforcement learning over logic puzzles reshapes the internal reasoning of LLMs, enabling more robust and compositional generalization without relying on task-specific symbolic tools."
  },
  {
    "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning",
    "url": "http://arxiv.org/abs/2506.04755v1",
    "arxiv_id": "2506.04755v1",
    "authors": [
      "Shenshen Li",
      "Kaiyuan Deng",
      "Lei Wang",
      "Hao Yang",
      "Chong Peng",
      "Peng Yan",
      "Fumin Shen",
      "Heng Tao Shen",
      "Xing Xu"
    ],
    "published": "2025-06-05T08:40:24+00:00",
    "summary": "While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP."
  },
  {
    "title": "Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models",
    "url": "http://arxiv.org/abs/2506.04746v1",
    "arxiv_id": "2506.04746v1",
    "authors": [
      "Fei Ding",
      "Baiqiao Wang",
      "Zijian Zeng",
      "Youwei Wang"
    ],
    "published": "2025-06-05T08:27:34+00:00",
    "summary": "The Group Relative Policy Optimization (GRPO) algorithm has demonstrated considerable success in enhancing the reasoning capabilities of large language models (LLMs), as evidenced by DeepSeek-R1. However, the absence of intermediate supervision in GRPO frequently leads to inefficient exploration dynamics. A single error in a complex reasoning chain can invalidate the entire solution, resulting in abrupt reward vanishing and compromising training stability.To address these challenges, we propose MGRPO (Multi-layer GRPO). MGRPO operates in two layers: the first layer employs standard GRPO to generate an initial response. This response, along with the original query, is then fed into a second-layer GRPO process. This second layer is specifically trained to identify and correct errors in the initial response, effectively creating a self-correction loop. This mechanism provides implicit process-level supervision by rewarding successful error correction, without requiring an explicit, densely-annotated reward model. Experimental results on several mathematical reasoning benchmarks demonstrate that MGRPO significantly outperforms standard GRPO, achieving superior performance by fostering both reasoning and self-correction abilities."
  },
  {
    "title": "SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs",
    "url": "http://arxiv.org/abs/2506.04743v1",
    "arxiv_id": "2506.04743v1",
    "authors": [
      "Shuhan Xu",
      "Siyuan Liang",
      "Hongling Zheng",
      "Yong Luo",
      "Aishan Liu",
      "Dacheng Tao"
    ],
    "published": "2025-06-05T08:22:24+00:00",
    "summary": "Vision-Language Models (VLMs) have achieved remarkable performance in image captioning, but recent studies show they are vulnerable to backdoor attacks. Attackers can inject imperceptible perturbations-such as local pixel triggers or global semantic phrases-into the training data, causing the model to generate malicious, attacker-controlled captions for specific inputs. These attacks are hard to detect and defend due to their stealthiness and cross-modal nature. By analyzing attack samples, we identify two key vulnerabilities: (1) abnormal attention concentration on specific image regions, and (2) semantic drift and incoherence in generated captions. To counter this, we propose Semantic Reward Defense (SRD), a reinforcement learning framework that mitigates backdoor behavior without prior knowledge of triggers. SRD uses a Deep Q-Network to learn policies for applying discrete perturbations (e.g., occlusion, color masking) to sensitive image regions, aiming to disrupt the activation of malicious pathways. We design a semantic fidelity score as the reward signal, which jointly evaluates semantic consistency and linguistic fluency of the output, guiding the agent toward generating robust yet faithful captions. Experiments across mainstream VLMs and datasets show SRD reduces attack success rates to 5.6%, while preserving caption quality on clean inputs with less than 10% performance drop. SRD offers a trigger-agnostic, interpretable defense paradigm against stealthy backdoor threats in multimodal generative models."
  },
  {
    "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04723v1",
    "arxiv_id": "2506.04723v1",
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Zixuan Ke",
      "Caiming Xiong",
      "Shafiq Joty",
      "Aws Albarghouthi",
      "Frederic Sala"
    ],
    "published": "2025-06-05T07:53:59+00:00",
    "summary": "Reinforcement learning (RL) has become the dominant paradigm for endowing language models with advanced reasoning capabilities. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of their advantages is still lacking. To address this gap, we introduce a fine-grained analytic framework to dissect the impact of RL on reasoning. Our framework specifically investigates key elements that have been hypothesized to benefit from RL training: (1) plan-following and execution, (2) problem decomposition, and (3) improved reasoning and knowledge utilization. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit step-by-step plans surprisingly degrades performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than their base counterparts. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances the model's capacity to integrate provided knowledge into its reasoning process, leading to performance improvements across diverse tasks. We also study difficulty, showing improved training by developing new ways to exploit hard problems. Our findings lay a foundation for more principled training and evaluation of reasoning models."
  },
  {
    "title": "Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling",
    "url": "http://arxiv.org/abs/2506.04699v1",
    "arxiv_id": "2506.04699v1",
    "authors": [
      "Bihan Xu",
      "Shiwei Zhao",
      "Runze Wu",
      "Zhenya Huang",
      "Jiawei Wang",
      "Zhipeng Hu",
      "Kai Wang",
      "Haoyu Liu",
      "Tangjie Lv",
      "Le Li",
      "Changjie Fan",
      "Xin Tong",
      "Jiangze Han"
    ],
    "published": "2025-06-05T07:21:13+00:00",
    "summary": "Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules."
  },
  {
    "title": "On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models",
    "url": "http://arxiv.org/abs/2506.04695v1",
    "arxiv_id": "2506.04695v1",
    "authors": [
      "Xingwu Chen",
      "Tianle Li",
      "Difan Zou"
    ],
    "published": "2025-06-05T07:17:04+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated remarkable success in enhancing model capabilities, including instruction-following, preference learning, and reasoning. Yet despite its empirical successes, the mechanisms by which RL improves reasoning abilities remain poorly understood. We present a systematic study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that its primary benefit comes from optimizing the selection of existing reasoning patterns. Through extensive experiments, we demonstrate that RLVR-trained models preferentially adopt high-success-rate reasoning patterns while mostly maintaining stable performance on individual patterns. We further develop theoretical analyses on the convergence and training dynamics of RLVR based on a simplified question-reason-answer model. We study the gradient flow and show that RLVR can indeed find the solution that selects the reason pattern with the highest success rate. Besides, our theoretical results   reveal two distinct regimes regarding the convergence of RLVR training: (1) rapid convergence for models with relatively strong initial reasoning capabilities versus (2) slower optimization dynamics for weaker models. Furthermore, we show that the slower optimization for weaker models can be mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using a feasibly high-quality SFT dataset. We validate the theoretical findings through extensive experiments. This work advances our theoretical understanding of RL's role in LLM fine-tuning and offers insights for further enhancing reasoning capabilities."
  },
  {
    "title": "Can Artificial Intelligence Trade the Stock Market?",
    "url": "http://arxiv.org/abs/2506.04658v1",
    "arxiv_id": "2506.04658v1",
    "authors": [
      "J\u0119drzej Maskiewicz",
      "Pawe\u0142 Sakowski"
    ],
    "published": "2025-06-05T05:59:10+00:00",
    "summary": "The paper explores the use of Deep Reinforcement Learning (DRL) in stock market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and Proximal Policy Optimization (PPO) and compares them with Buy and Hold benchmark. It evaluates these algorithms across three currency pairs, the S&P 500 index and Bitcoin, on the daily data in the period of 2019-2023. The results demonstrate DRL's effectiveness in trading and its ability to manage risk by strategically avoiding trades in unfavorable conditions, providing a substantial edge over classical approaches, based on supervised learning in terms of risk-adjusted returns."
  },
  {
    "title": "Composing Agents to Minimize Worst-case Risk",
    "url": "http://arxiv.org/abs/2506.04632v1",
    "arxiv_id": "2506.04632v1",
    "authors": [
      "Guruprerana Shabadi",
      "Rajeev Alur"
    ],
    "published": "2025-06-05T05:04:44+00:00",
    "summary": "From software development to robot control, modern agentic systems decompose complex objectives into a sequence of subtasks and choose a set of specialized AI agents to complete them. We formalize an agentic workflow as a directed acyclic graph, called an agent graph, where edges represent AI agents and paths correspond to feasible compositions of agents. When deploying these systems in the real world, we need to choose compositions of agents that not only maximize the task success, but also minimize risk where the risk captures requirements like safety, fairness, and privacy. This additionally requires carefully analyzing the low-probability (tail) behaviors of compositions of agents. In this work, we consider worst-case risk minimization over the set of feasible agent compositions. We define worst-case risk as the tail quantile -- also known as value-at-risk -- of the loss distribution of the agent composition where the loss quantifies the risk associated with agent behaviors. We introduce an efficient algorithm that traverses the agent graph and finds a near-optimal composition of agents by approximating the value-at-risk via a union bound and dynamic programming. Furthermore, we prove that the approximation is near-optimal asymptotically for a broad class of practical loss functions. To evaluate our framework, we consider a suite of video game-like control benchmarks that require composing several agents trained with reinforcement learning and demonstrate our algorithm's effectiveness in approximating the value-at-risk and identifying the optimal agent composition."
  },
  {
    "title": "Enhancing Efficiency and Propulsion in Bio-mimetic Robotic Fish through End-to-End Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04627v1",
    "arxiv_id": "2506.04627v1",
    "authors": [
      "Xinyu Cui",
      "Boai Sun",
      "Yi Zhu",
      "Ning Yang",
      "Haifeng Zhang",
      "Weicheng Cui",
      "Dixia Fan",
      "Jun Wang"
    ],
    "published": "2025-06-05T04:36:46+00:00",
    "summary": "Aquatic organisms are known for their ability to generate efficient propulsion with low energy expenditure. While existing research has sought to leverage bio-inspired structures to reduce energy costs in underwater robotics, the crucial role of control policies in enhancing efficiency has often been overlooked. In this study, we optimize the motion of a bio-mimetic robotic fish using deep reinforcement learning (DRL) to maximize propulsion efficiency and minimize energy consumption. Our novel DRL approach incorporates extended pressure perception, a transformer model processing sequences of observations, and a policy transfer scheme. Notably, significantly improved training stability and speed within our approach allow for end-to-end training of the robotic fish. This enables agiler responses to hydrodynamic environments and possesses greater optimization potential compared to pre-defined motion pattern controls. Our experiments are conducted on a serially connected rigid robotic fish in a free stream with a Reynolds number of 6000 using computational fluid dynamics (CFD) simulations. The DRL-trained policies yield impressive results, demonstrating both high efficiency and propulsion. The policies also showcase the agent's embodiment, skillfully utilizing its body structure and engaging with surrounding fluid dynamics, as revealed through flow analysis. This study provides valuable insights into the bio-mimetic underwater robots optimization through DRL training, capitalizing on their structural advantages, and ultimately contributing to more efficient underwater propulsion systems."
  },
  {
    "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04626v1",
    "arxiv_id": "2506.04626v1",
    "authors": [
      "Haochen Zhang",
      "Zhong Zheng",
      "Lingzhou Xue"
    ],
    "published": "2025-06-05T04:36:38+00:00",
    "summary": "Motivated by real-world settings where data collection and policy deployment -- whether for a single agent or across multiple agents -- are costly, we study the problem of on-policy single-agent reinforcement learning (RL) and federated RL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to reach near-optimal regret) and policy switching or communication costs. In parallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$ states and $A$ actions, existing methods either require superlinear burn-in costs in $S$ and $A$ or fail to achieve logarithmic switching or communication costs. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost -- that are the first in the literature to simultaneously achieve: (i) the best near-optimal regret among all known model-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly with $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent RL or communication cost for FRL. Additionally, we establish gap-dependent theoretical guarantees for both regret and switching/communication costs, improving or matching the best-known gap-dependent bounds."
  },
  {
    "title": "Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning",
    "url": "http://arxiv.org/abs/2506.04595v1",
    "arxiv_id": "2506.04595v1",
    "authors": [
      "Ziqi Jia",
      "Anmin Wang",
      "Xiaoyang Qu",
      "Xiaowen Yang",
      "Jianzong Wang"
    ],
    "published": "2025-06-05T03:20:47+00:00",
    "summary": "Previous continual learning setups for embodied intelligence focused on executing low-level actions based on human commands, neglecting the ability to learn high-level planning and multi-level knowledge. To address these issues, we propose the Hierarchical Embodied Continual Learning Setups (HEC) that divide the agent's continual learning process into two layers: high-level instructions and low-level actions, and define five embodied continual learning sub-setups. Building on these setups, we introduce the Task-aware Mixture of Incremental LoRA Experts (Task-aware MoILE) method. This approach achieves task recognition by clustering visual-text embeddings and uses both a task-level router and a token-level router to select the appropriate LoRA experts. To effectively address the issue of catastrophic forgetting, we apply Singular Value Decomposition (SVD) to the LoRA parameters obtained from prior tasks, preserving key components while orthogonally training the remaining parts. The experimental results show that our method stands out in reducing the forgetting of old tasks compared to other methods, effectively supporting agents in retaining prior knowledge while continuously learning new tasks."
  },
  {
    "title": "The future of gravitational wave science unlocking LIGO potential: AI-driven data analysis and exploration",
    "url": "http://arxiv.org/abs/2506.04584v1",
    "arxiv_id": "2506.04584v1",
    "authors": [
      "Yong Xiao",
      "Li",
      "Zin Nandar Win",
      "He Wang",
      "Hla Myo Tun",
      "Win Thu Zar"
    ],
    "published": "2025-06-05T02:58:42+00:00",
    "summary": "The advent of gravitational wave astronomy (GW) has revolutionized the observation of cataclysmic cosmic events, such as black hole mergers and neutron star collisions. The Laser Interferometer Gravitational-Wave Observatory (LIGO) has been at the forefront of these discoveries. However, the immense volume and complexity of gravitational wave data present significant challenges for traditional analysis methods. This paper investigates the growing synergy between artificial intelligence (AI) and GW science, emphasizing how AI enhances signal detection, noise reduction, and data interpretation. It begins with an overview of GW fundamentals and the role of machine learning in increasing detector sensitivity. Notable GW events observed by LIGO are discussed alongside persistent analytical challenges such as data quality, generalization, and computational constraints. A comprehensive performance review of AI techniques, including supervised learning, unsupervised learning, deep learning, and reinforcement learning, is presented based on data spanning 2021 to 2024. Evaluation metrics include accuracy, precision, true positive rate (TPR), false positive rate (FPR), and computational efficiency. Findings indicate that deep learning and supervised learning outperform other approaches, particularly in enhancing TPR and minimizing FPR. While unsupervised and reinforcement learning models offer less precision, they demonstrate high efficiency and potential for real-time applications. The study also explores AI integration into next-generation detectors and waveform reconstruction techniques. Overall, the integration of AI into GW research significantly improves the reliability and speed of event detection, unlocking new possibilities for exploring the dynamic universe. This paper provides a comprehensive outlook on the transformative role of AI in shaping the future of GW astronomy."
  },
  {
    "title": "Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning",
    "url": "http://arxiv.org/abs/2506.04559v1",
    "arxiv_id": "2506.04559v1",
    "authors": [
      "Yunhao Gou",
      "Kai Chen",
      "Zhili Liu",
      "Lanqing Hong",
      "Xin Jin",
      "Zhenguo Li",
      "James T. Kwok",
      "Yu Zhang"
    ],
    "published": "2025-06-05T02:28:07+00:00",
    "summary": "Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment."
  },
  {
    "title": "Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04549v1",
    "arxiv_id": "2506.04549v1",
    "authors": [
      "Vardhan Palod",
      "Pranav Mahajan",
      "Veeky Baths",
      "Boris S. Gutkin"
    ],
    "published": "2025-06-05T01:54:09+00:00",
    "summary": "Despite a strong desire to quit, individuals with long-term substance use disorder (SUD) often struggle to resist drug use, even when aware of its harmful consequences. This disconnect between knowledge and compulsive behavior reflects a fundamental cognitive-behavioral conflict in addiction. Neurobiologically, differential cue-induced activity within striatal subregions, along with dopamine-mediated connectivity from the ventral to the dorsal striatum, contributes to compulsive drug-seeking. However, the functional mechanism linking these findings to behavioral conflict remains unclear. Another hallmark of addiction is temporal discounting: individuals with drug dependence exhibit steeper discount rates than non-users. Assuming the ventral-dorsal striatal organization reflects a gradient from cognitive to motor representations, addiction can be modeled within a hierarchical reinforcement learning (HRL) framework. However, integrating discounting into biologically grounded HRL remains an open challenge. In this work, we build on a model showing how action choices reinforced with drug rewards become insensitive to the negative consequences that follow. We address the integration of discounting by ensuring natural reward values converge across all levels in the HRL hierarchy, while drug rewards diverge due to their dopaminergic effects. Our results show that high discounting amplifies drug-seeking across the hierarchy, linking faster discounting with increased addiction severity and impulsivity. We demonstrate alignment with empirical findings on temporal discounting and propose testable predictions, establishing addiction as a disorder of hierarchical decision-making."
  },
  {
    "title": "SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04505v1",
    "arxiv_id": "2506.04505v1",
    "authors": [
      "Nikita Oskolkov",
      "Huzhenyu Zhang",
      "Dmitry Makarov",
      "Dmitry Yudin",
      "Aleksandr Panov"
    ],
    "published": "2025-06-04T23:09:45+00:00",
    "summary": "The 3D scene graph models spatial relationships between objects, enabling the agent to efficiently navigate in a partially observable environment and predict the location of the target object.This paper proposes an original framework named SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for mapless reinforcement learning-based robot navigation with learnable representation of open-vocabulary 3D scene graph. To accelerate and stabilize the training of reinforcement learning-based algorithms, the framework also employs imitation learning and curriculum learning. The first one enables the agent to learn from demonstrations, while the second one structures the training process by gradually increasing task complexity from simple to more advanced scenarios. Numerical experiments conducted in the Isaac Sim environment showed that using a 3D scene graph for reinforcement learning significantly increased the success rate in difficult navigation cases. The code is open-sourced and available at: https://github.com/Xisonik/Aloha\\_graph."
  },
  {
    "title": "Leveraging Reward Models for Guiding Code Review Comment Generation",
    "url": "http://arxiv.org/abs/2506.04464v1",
    "arxiv_id": "2506.04464v1",
    "authors": [
      "Oussama Ben Sghaier",
      "Rosalia Tufano",
      "Gabriele Bavota",
      "Houari Sahraoui"
    ],
    "published": "2025-06-04T21:31:38+00:00",
    "summary": "Code review is a crucial component of modern software development, involving the evaluation of code quality, providing feedback on potential issues, and refining the code to address identified problems. Despite these benefits, code review can be rather time consuming, and influenced by subjectivity and human factors. For these reasons, techniques to (partially) automate the code review process have been proposed in the literature. Among those, the ones exploiting deep learning (DL) are able to tackle the generative aspect of code review, by commenting on a given code as a human reviewer would do (i.e., comment generation task) or by automatically implementing code changes required to address a reviewer's comment (i.e., code refinement task). In this paper, we introduce CoRAL, a deep learning framework automating review comment generation by exploiting reinforcement learning with a reward mechanism considering both the semantics of the generated comments as well as their usefulness as input for other models automating the code refinement task. The core idea is that if the DL model generates comments that are semantically similar to the expected ones or can be successfully implemented by a second model specialized in code refinement, these comments are likely to be meaningful and useful, thus deserving a high reward in the reinforcement learning framework. We present both quantitative and qualitative comparisons between the comments generated by CoRAL and those produced by the latest baseline techniques, highlighting the effectiveness and superiority of our approach."
  },
  {
    "title": "Aligning Large Language Models with Implicit Preferences from User-Generated Content",
    "url": "http://arxiv.org/abs/2506.04463v1",
    "arxiv_id": "2506.04463v1",
    "authors": [
      "Zhaoxuan Tan",
      "Zheng Li",
      "Tianyi Liu",
      "Haodong Wang",
      "Hyokun Yun",
      "Ming Zeng",
      "Pei Chen",
      "Zhihan Zhang",
      "Yifan Gao",
      "Ruijie Wang",
      "Priyanka Nigam",
      "Bing Yin",
      "Meng Jiang"
    ],
    "published": "2025-06-04T21:29:11+00:00",
    "summary": "Learning from preference feedback is essential for aligning large language models (LLMs) with human values and improving the quality of generated responses. However, existing preference learning methods rely heavily on curated data from humans or advanced LLMs, which is costly and difficult to scale. In this work, we present PUGC, a novel framework that leverages implicit human Preferences in unlabeled User-Generated Content (UGC) to generate preference data. Although UGC is not explicitly created to guide LLMs in generating human-preferred responses, it often reflects valuable insights and implicit preferences from its creators that has the potential to address readers' questions. PUGC transforms UGC into user queries and generates responses from the policy model. The UGC is then leveraged as a reference text for response scoring, aligning the model with these implicit preferences. This approach improves the quality of preference data while enabling scalable, domain-specific alignment. Experimental results on Alpaca Eval 2 show that models trained with DPO and PUGC achieve a 9.37% performance improvement over traditional methods, setting a 35.93% state-of-the-art length-controlled win rate using Mistral-7B-Instruct. Further studies highlight gains in reward quality, domain-specific alignment effectiveness, robustness against UGC quality, and theory of mind capabilities. Our code and dataset are available at https://zhaoxuan.info/PUGC.github.io/"
  },
  {
    "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale",
    "url": "http://arxiv.org/abs/2506.04405v1",
    "arxiv_id": "2506.04405v1",
    "authors": [
      "Ran Xu",
      "Yuchen Zhuang",
      "Yishan Zhong",
      "Yue Yu",
      "Xiangru Tang",
      "Hang Wu",
      "May D. Wang",
      "Peifeng Ruan",
      "Donghan Yang",
      "Tao Wang",
      "Guanghua Xiao",
      "Carl Yang",
      "Yang Xie",
      "Wenqi Shi"
    ],
    "published": "2025-06-04T19:38:55+00:00",
    "summary": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice."
  },
  {
    "title": "Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04399v1",
    "arxiv_id": "2506.04399v1",
    "authors": [
      "Suzan Ece Ada",
      "Emre Ugur"
    ],
    "published": "2025-06-04T19:27:47+00:00",
    "summary": "We introduce Unsupervised Meta-Testing with Conditional Neural Processes (UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method that uniquely combines, yet distinctly separates, parameterized policy gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for settings where the reward signal is missing during meta-testing, our method increases sample efficiency without requiring additional samples in meta-training. UMCNP leverages the efficiency and scalability of Conditional Neural Processes (CNPs) to reduce the number of online interactions required in meta-testing. During meta-training, samples previously collected through PPG meta-RL are efficiently reused for learning task inference in an offline manner. UMCNP infers the latent representation of the transition dynamics model from a single test task rollout with unknown parameters. This approach allows us to generate rollouts for self-adaptation by interacting with the learned dynamics model. We demonstrate our method can adapt to an unseen test task using significantly fewer samples during meta-testing than the baselines in 2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with unknown angle sensor bias, walker agent with randomized dynamics parameters."
  },
  {
    "title": "Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning",
    "url": "http://arxiv.org/abs/2506.04398v1",
    "arxiv_id": "2506.04398v1",
    "authors": [
      "Th\u00e9o Vincent",
      "Yogesh Tripathi",
      "Tim Faust",
      "Yaniv Oren",
      "Jan Peters",
      "Carlo D'Eramo"
    ],
    "published": "2025-06-04T19:27:29+00:00",
    "summary": "In value-based reinforcement learning, removing the target network is tempting as the boostrapped target would be built from up-to-date estimates, and the spared memory occupied by the target network could be reallocated to expand the capacity of the online network. However, eliminating the target network introduces instability, leading to a decline in performance. Removing the target network also means we cannot leverage the literature developed around target networks. In this work, we propose to use a copy of the last linear layer of the online network as a target network, while sharing the remaining parameters with the up-to-date online network, hence stepping out of the binary choice between target-based and target-free methods. It enables us to leverage the concept of iterated Q-learning, which consists of learning consecutive Bellman iterations in parallel, to reduce the performance gap between target-free and target-based approaches. Our findings demonstrate that this novel method, termed iterated Shared Q-Learning (iS-QL), improves the sample efficiency of target-free approaches across various settings. Importantly, iS-QL requires a smaller memory footprint and comparable training time to classical target-based algorithms, highlighting its potential to scale reinforcement learning research."
  },
  {
    "title": "A Risk-Aware Reinforcement Learning Reward for Financial Trading",
    "url": "http://arxiv.org/abs/2506.04358v1",
    "arxiv_id": "2506.04358v1",
    "authors": [
      "Uditansh Srivastava",
      "Shivam Aryan",
      "Shaurya Singh"
    ],
    "published": "2025-06-04T18:19:48+00:00",
    "summary": "We propose a novel composite reward function for reinforcement learning in financial trading that balances return and risk using four differentiable terms: annualized return downside risk differential return and the Treynor ratio   Unlike single metric objectives for example the Sharpe ratio our formulation is modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners to encode diverse investor preferences   We tune these weights via grid search to target specific risk return profiles   We derive closed form gradients for each term to facilitate gradient based training and analyze key theoretical properties including monotonicity boundedness and modularity   This framework offers a general blueprint for building robust multi objective reward functions in complex trading environments and can be extended with additional risk measures or adaptive weighting"
  },
  {
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04207v1",
    "arxiv_id": "2506.04207v1",
    "authors": [
      "Shuang Chen",
      "Yue Guo",
      "Zhaochen Su",
      "Yafu Li",
      "Yulun Wu",
      "Jiacheng Chen",
      "Jiayu Chen",
      "Weijie Wang",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "published": "2025-06-04T17:51:08+00:00",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."
  },
  {
    "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures",
    "url": "http://arxiv.org/abs/2506.04195v1",
    "arxiv_id": "2506.04195v1",
    "authors": [
      "Elena Zamaraeva",
      "Christopher M. Collins",
      "George R. Darling",
      "Matthew S. Dyer",
      "Bei Peng",
      "Rahul Savani",
      "Dmytro Antypov",
      "Vladimir V. Gusev",
      "Judith Clymo",
      "Paul G. Spirakis",
      "Matthew J. Rosseinsky"
    ],
    "published": "2025-06-04T17:40:57+00:00",
    "summary": "Geometry optimization of atomic structures is a common and crucial task in computational chemistry and materials design. Following the learning to optimize paradigm, we propose a new multi-agent reinforcement learning method called Multi-Agent Crystal Structure optimization (MACS) to address periodic crystal structure optimization. MACS treats geometry optimization as a partially observable Markov game in which atoms are agents that adjust their positions to collectively discover a stable configuration. We train MACS across various compositions of reported crystalline materials to obtain a policy that successfully optimizes structures from the training compositions as well as structures of larger sizes and unseen compositions, confirming its excellent scalability and zero-shot transferability. We benchmark our approach against a broad range of state-of-the-art optimization methods and demonstrate that MACS optimizes periodic crystal structures significantly faster, with fewer energy calculations, and the lowest failure rate."
  },
  {
    "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04185v1",
    "arxiv_id": "2506.04185v1",
    "authors": [
      "Qingfei Zhao",
      "Ruobing Wang",
      "Dingling Xu",
      "Daren Zha",
      "Limin Liu"
    ],
    "published": "2025-06-04T17:29:22+00:00",
    "summary": "Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at https://github.com/QingFei1/R-Search."
  },
  {
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models",
    "url": "http://arxiv.org/abs/2506.04180v1",
    "arxiv_id": "2506.04180v1",
    "authors": [
      "Yuhao Wu",
      "Yushi Bai",
      "Zhiqiang Hu",
      "Juanzi Li",
      "Roy Ka-Wei Lee"
    ],
    "published": "2025-06-04T17:27:42+00:00",
    "summary": "Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation."
  },
  {
    "title": "Horizon Reduction Makes RL Scalable",
    "url": "http://arxiv.org/abs/2506.04168v1",
    "arxiv_id": "2506.04168v1",
    "authors": [
      "Seohong Park",
      "Kevin Frans",
      "Deepinder Mann",
      "Benjamin Eysenbach",
      "Aviral Kumar",
      "Sergey Levine"
    ],
    "published": "2025-06-04T17:06:54+00:00",
    "summary": "In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000x larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL. Code: https://github.com/seohongpark/horizon-reduction"
  },
  {
    "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL",
    "url": "http://arxiv.org/abs/2506.04147v1",
    "arxiv_id": "2506.04147v1",
    "authors": [
      "Jiaheng Hu",
      "Peter Stone",
      "Roberto Mart\u00edn-Mart\u00edn"
    ],
    "published": "2025-06-04T16:41:55+00:00",
    "summary": "Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors. More information, code, and videos at robo-rl.github.io"
  },
  {
    "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward",
    "url": "http://arxiv.org/abs/2506.04070v1",
    "arxiv_id": "2506.04070v1",
    "authors": [
      "Yi Zhao",
      "Siqi Wang",
      "Jing Li"
    ],
    "published": "2025-06-04T15:34:33+00:00",
    "summary": "Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}."
  },
  {
    "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2506.04065v1",
    "arxiv_id": "2506.04065v1",
    "authors": [
      "Muling Wu",
      "Qi Qian",
      "Wenhao Liu",
      "Xiaohua Wang",
      "Zisu Huang",
      "Di Liang",
      "LI Miao",
      "Shihan Dou",
      "Changze Lv",
      "Zhenghua Wang",
      "Zhibo Xu",
      "Lina Chen",
      "Tianlong Li",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "published": "2025-06-04T15:31:46+00:00",
    "summary": "Large Language Models (LLMs) have achieved remarkable performance across various reasoning tasks, yet post-training is constrained by inefficient sample utilization and inflexible difficulty samples processing. To address these limitations, we propose Customized Curriculum Learning (CCL), a novel framework with two key innovations. First, we introduce model-adaptive difficulty definition that customizes curriculum datasets based on each model's individual capabilities rather than using predefined difficulty metrics. Second, we develop \"Guided Prompting,\" which dynamically reduces sample difficulty through strategic hints, enabling effective utilization of challenging samples that would otherwise degrade performance. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, confirming its effectiveness across both paradigms in enhancing sample utilization and model performance."
  },
  {
    "title": "Crowd-SFT: Crowdsourcing for LLM Alignment",
    "url": "http://arxiv.org/abs/2506.04063v1",
    "arxiv_id": "2506.04063v1",
    "authors": [
      "Alex Sotiropoulos",
      "Sulyab Thottungal Valapu",
      "Linus Lei",
      "Jared Coleman",
      "Bhaskar Krishnamachari"
    ],
    "published": "2025-06-04T15:26:38+00:00",
    "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model responses with human preferences. While RLHF employs a reinforcement learning approach with a separate reward model, SFT uses human-curated datasets for supervised learning. Both approaches traditionally depend on small, vetted groups of annotators, making them costly, prone to bias, and limited in scalability. We propose an open, crowd-sourced fine-tuning framework that addresses these limitations by enabling broader feedback collection for SFT without extensive annotator training. Our framework promotes incentive fairness via a point-based reward system correlated with Shapley values and guides model convergence through iterative model updates. Our multi-model selection framework demonstrates up to a 55% reduction in target distance over single-model selection, enabling subsequent experiments that validate our point-based reward mechanism's close alignment with Shapley values (a well-established method for attributing individual contributions) thereby supporting fair and scalable participation."
  },
  {
    "title": "Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration",
    "url": "http://arxiv.org/abs/2506.04040v1",
    "arxiv_id": "2506.04040v1",
    "authors": [
      "Chengdong Wu",
      "Sven Kirchner",
      "Nils Purschke",
      "Alois C. Knoll"
    ],
    "published": "2025-06-04T15:05:06+00:00",
    "summary": "The controller is one of the most important modules in the autonomous driving pipeline, ensuring the vehicle reaches its desired position. In this work, a reinforcement learning based lateral control approach, despite the imperfections in the vehicle models due to measurement errors and simplifications, is presented. Our approach ensures comfortable, efficient, and robust control performance considering the interface between controlling and other modules. The controller consists of the conventional Model Predictive Control (MPC)-PID part as the basis and the demonstrator, and the Deep Reinforcement Learning (DRL) part which leverages the online information from the MPC-PID part. The controller's performance is evaluated in CARLA using the ground truth of the waypoints as inputs. Experimental results demonstrate the effectiveness of the controller when vehicle information is incomplete, and the training of DRL can be stabilized with the demonstration part. These findings highlight the potential to reduce development and integration efforts for autonomous driving pipelines in the future."
  },
  {
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "url": "http://arxiv.org/abs/2506.04034v1",
    "arxiv_id": "2506.04034v1",
    "authors": [
      "Qing Jiang",
      "Xingyu Chen",
      "Zhaoyang Zeng",
      "Junzhi Yu",
      "Lei Zhang"
    ],
    "published": "2025-06-04T14:56:57+00:00",
    "summary": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings."
  },
  {
    "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.04022v1",
    "arxiv_id": "2506.04022v1",
    "authors": [
      "Qiyue Xia",
      "J. Michael Herrmann"
    ],
    "published": "2025-06-04T14:52:18+00:00",
    "summary": "Multi-objective reinforcement learning (MORL) aims at optimising several, often conflicting goals in order to improve flexibility and reliability of RL in practical tasks. This can be achieved by finding diverse policies that are optimal for some objective preferences and non-dominated by optimal policies for other preferences so that they form a Pareto front in the multi-objective performance space. The relation between the multi-objective performance space and the parameter space that represents the policies is generally non-unique. Using a training scheme that is based on a locally linear map between the parameter space and the performance space, we show that an approximate Pareto front can provide an interpretation of the current parameter vectors in terms of the objectives which enables an effective search within contiguous solution domains. Experiments are conducted with and without retraining across different domains, and the comparison with previous methods demonstrates the efficiency of our approach."
  },
  {
    "title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.03921v1",
    "arxiv_id": "2506.03921v1",
    "authors": [
      "Xunzhu Tang",
      "Jacques Klein",
      "Tegawend\u00e9 F. Bissyand\u00e9"
    ],
    "published": "2025-06-04T13:13:58+00:00",
    "summary": "Several closed-source LLMs have consistently outperformed open-source alternatives in program repair tasks, primarily due to their superior reasoning capabilities and extensive pre-training. This paper introduces Repairity, a novel three-stage methodology that significantly narrows this performance gap through reasoning extraction and reinforcement learning. Our approach: (1) systematically filters high-quality reasoning traces from closed-source models using correctness verification, (2) transfers this reasoning knowledge to open-source models via supervised fine-tuning, and (3) develops reinforcement learning with LLM-based feedback to further optimize performance. Empirical evaluation across multiple program repair benchmarks demonstrates that Repairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open source LLM, by 8.68\\% on average, reducing the capability gap with Claude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%. Ablation studies confirm that both reasoning extraction and LLM-guided reinforcement learning contribute significantly to these improvements. Our methodology generalizes effectively to additional code-related tasks, enabling organizations to leverage high-quality program repair capabilities while maintaining the customizability, transparency, and deployment flexibility inherent to open-source models."
  },
  {
    "title": "Scaling CrossQ with Weight Normalization",
    "url": "http://arxiv.org/abs/2506.03758v1",
    "arxiv_id": "2506.03758v1",
    "authors": [
      "Daniel Palenicek",
      "Florian Vogt",
      "Jan Peters"
    ],
    "published": "2025-06-04T09:24:17+00:00",
    "summary": "Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics which are emphasized by higher UTDs, particularly Q-bias explosion and the growing magnitude of critic network weights. To address this, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, prevents potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive or superior performance across a range of challenging tasks on the DeepMind control benchmark, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a robust pathway for improving sample efficiency and scalability in model-free reinforcement learning."
  },
  {
    "title": "PPO in the Fisher-Rao geometry",
    "url": "http://arxiv.org/abs/2506.03757v1",
    "arxiv_id": "2506.03757v1",
    "authors": [
      "Razvan-Andrei Lascu",
      "David \u0160i\u0161ka",
      "\u0141ukasz Szpruch"
    ],
    "published": "2025-06-04T09:23:27+00:00",
    "summary": "Proximal Policy Optimization (PPO) has become a widely adopted algorithm for reinforcement learning, offering a practical policy gradient method with strong empirical performance. Despite its popularity, PPO lacks formal theoretical guarantees for policy improvement and convergence. PPO is motivated by Trust Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL divergence penalty, which arises from linearizing the value function within a flat geometric space. In this paper, we derive a tighter surrogate in the Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO). Our proposed scheme provides strong theoretical guarantees, including monotonic policy improvement. Furthermore, in the tabular setting, we demonstrate that FR-PPO achieves sub-linear convergence without any dependence on the dimensionality of the action or state spaces, marking a significant step toward establishing formal convergence results for PPO-based algorithms."
  },
  {
    "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision",
    "url": "http://arxiv.org/abs/2506.03723v1",
    "arxiv_id": "2506.03723v1",
    "authors": [
      "Chaeyun Jang",
      "Moonseok Choi",
      "Yegon Kim",
      "Hyungi Lee",
      "Juho Lee"
    ],
    "published": "2025-06-04T08:56:24+00:00",
    "summary": "Uncertainty calibration is essential for the safe deployment of large language models (LLMs), particularly when users rely on verbalized confidence estimates. While prior work has focused on classifiers or short-form generation, confidence calibration for chain-of-thought (CoT) reasoning remains largely unexplored. Surprisingly, we find that supervised fine-tuning with scalar confidence labels alone suffices to elicit self-verification behavior of language models, without any explicit reasoning supervision or reinforcement learning-based rewards. Despite being trained only to produce a verbalized confidence score without any self-verifying examples, the model learns to generate longer and self-checking responses for low-confidence queries while providing more concise answers for high-confidence ones. We further propose a simple rethinking method that boosts performance via test-time scaling based on calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence."
  },
  {
    "title": "Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond",
    "url": "http://arxiv.org/abs/2506.03703v1",
    "arxiv_id": "2506.03703v1",
    "authors": [
      "Xiansheng Cai",
      "Sihan Hu",
      "Tao Wang",
      "Yuan Huang",
      "Pan Zhang",
      "Youjin Deng",
      "Kun Chen"
    ],
    "published": "2025-06-04T08:35:05+00:00",
    "summary": "Fundamental physics often confronts complex symbolic problems with few guiding exemplars or established principles. While artificial intelligence (AI) offers promise, its typical need for vast datasets to learn from hinders its use in these information-scarce frontiers. We introduce learning at criticality (LaC), a reinforcement learning (RL) scheme that tunes Large Language Models (LLMs) to a sharp learning transition, addressing this information scarcity. At this transition, LLMs achieve peak generalization from minimal data, exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic reasoning. To elucidate this peak, we analyze a minimal concept-network model (CoNet) designed to capture the essence of how LLMs might link tokens. Trained on a single exemplar, this model also undergoes a sharp learning transition. This transition exhibits hallmarks of a second-order phase transition, notably power-law distributed solution path lengths. At this critical point, the system maximizes a ``critical thinking pattern\" crucial for generalization, enabled by the underlying scale-free exploration. This suggests LLMs reach peak performance by operating at criticality, where such explorative dynamics enable the extraction of underlying operational rules. We demonstrate LaC in quantum field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems, significantly outperforming far larger models. LaC thus leverages critical phenomena, a physical principle, to empower AI for complex, data-sparse challenges in fundamental physics."
  },
  {
    "title": "Robust Preference Optimization via Dynamic Target Margins",
    "url": "http://arxiv.org/abs/2506.03690v1",
    "arxiv_id": "2506.03690v1",
    "authors": [
      "Jie Sun",
      "Junkang Wu",
      "Jiancan Wu",
      "Zhibo Zhu",
      "Xingyu Lu",
      "Jun Zhou",
      "Lintao Ma",
      "Xiang Wang"
    ],
    "published": "2025-06-04T08:19:37+00:00",
    "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}."
  },
  {
    "title": "Latent Guided Sampling for Combinatorial Optimization",
    "url": "http://arxiv.org/abs/2506.03672v1",
    "arxiv_id": "2506.03672v1",
    "authors": [
      "Sobihan Surendran",
      "Adeline Fermanian",
      "Sylvain Le Corff"
    ],
    "published": "2025-06-04T08:02:59+00:00",
    "summary": "Combinatorial Optimization problems are widespread in domains such as logistics, manufacturing, and drug discovery, yet their NP-hard nature makes them computationally challenging. Recent Neural Combinatorial Optimization methods leverage deep learning to learn solution strategies, trained via Supervised or Reinforcement Learning (RL). While promising, these approaches often rely on task-specific augmentations, perform poorly on out-of-distribution instances, and lack robust inference mechanisms. Moreover, existing latent space models either require labeled data or rely on pre-trained policies. In this work, we propose LGS-Net, a novel latent space model that conditions on problem instances, and introduce an efficient inference method, Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic Approximation. We show that the iterations of our method form a time-inhomogeneous Markov Chain and provide rigorous theoretical convergence guarantees. Empirical results on benchmark routing tasks show that our method achieves state-of-the-art performance among RL-based approaches."
  },
  {
    "title": "Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations",
    "url": "http://arxiv.org/abs/2506.03613v1",
    "arxiv_id": "2506.03613v1",
    "authors": [
      "Shaoshan Liu",
      "Fan Wang",
      "Hongjun Zhou",
      "Yuanfeng Wang"
    ],
    "published": "2025-06-04T06:44:49+00:00",
    "summary": "While theory and practice are often seen as separate domains, this article shows that theoretical insight is essential for overcoming real-world engineering barriers. We begin with a practical challenge: training a cross-morphology embodied AI policy that generalizes across diverse robot morphologies. We formalize this as the Heterogeneous Embodied Agent Training (HEAT) problem and prove it reduces to a structured Partially Observable Markov Decision Process (POMDP) that is PSPACE-complete. This result explains why current reinforcement learning pipelines break down under morphological diversity, due to sequential training constraints, memory-policy coupling, and data incompatibility. We further explore Collective Adaptation, a distributed learning alternative inspired by biological systems. Though NEXP-complete in theory, it offers meaningful scalability and deployment benefits in practice. This work illustrates how computational theory can illuminate system design trade-offs and guide the development of more robust, scalable embodied AI. For practitioners and researchers to explore this problem, the implementation code of this work has been made publicly available at https://github.com/airs-admin/HEAT"
  },
  {
    "title": "Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach",
    "url": "http://arxiv.org/abs/2506.03586v1",
    "arxiv_id": "2506.03586v1",
    "authors": [
      "Yu Ma",
      "Chongtao Guo",
      "Le Liang",
      "Xiao Li",
      "Shi Jin"
    ],
    "published": "2025-06-04T05:33:33+00:00",
    "summary": "This paper investigates a joint phase design and resource allocation problem in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal frequency division multiplexing (OFDM) systems to optimize average delay, where data packets for each user arrive at the base station stochastically. The sequential optimization problem is inherently a Markov decision process (MDP), making it fall within the scope of reinforcement learning. To effectively handle the mixed action space and reduce the state space dimensionality, a hybrid deep reinforcement learning (DRL) approach is proposed. Specifically, proximal policy optimization (PPO)-$\\Theta$ is employed to optimize RIS phase shift design, while PPO-N is responsible for subcarrier allocation decisions. To further mitigate the curse of dimensionality associated with subcarrier allocation, a multi-agent strategy is introduced to optimize subcarrier allocation indicater more efficiently. Moreover, to achieve more adaptive resource allocation and accurately capture network dynamics, key factors closely related to average delay, including the number of backlogged packets in buffers and the current packet arrivals, are incorporated into the state space. Furthermore, a transfer learning framework is introduced to enhance training efficiency and accelerate convergence. Simulation results demonstrate that the proposed algorithm significantly reduces average delay, enhances resource allocation efficiency, and achieves superior system robustness and fairness compared to baseline methods."
  },
  {
    "title": "MiMo-VL Technical Report",
    "url": "http://arxiv.org/abs/2506.03569v1",
    "arxiv_id": "2506.03569v1",
    "authors": [
      "Xiaomi LLM-Core Team",
      ":",
      "Zihao Yue",
      "Zhenru Lin",
      "Yifan Song",
      "Weikun Wang",
      "Shuhuai Ren",
      "Shuhao Gu",
      "Shicheng Li",
      "Peidian Li",
      "Liang Zhao",
      "Lei Li",
      "Kainan Bao",
      "Hao Tian",
      "Hailin Zhang",
      "Gang Wang",
      "Dawei Zhu",
      "Cici",
      "Chenhong He",
      "Bowen Ye",
      "Bowen Shen",
      "Zihan Zhang",
      "Zihan Jiang",
      "Zhixian Zheng",
      "Zhichao Song",
      "Zhenbo Luo",
      "Yue Yu",
      "Yudong Wang",
      "Yuanyuan Tian",
      "Yu Tu",
      "Yihan Yan",
      "Yi Huang",
      "Xu Wang",
      "Xinzhe Xu",
      "Xingchen Song",
      "Xing Zhang",
      "Xing Yong",
      "Xin Zhang",
      "Xiangwei Deng",
      "Wenyu Yang",
      "Wenhan Ma",
      "Weiwei Lv",
      "Weiji Zhuang",
      "Wei Liu",
      "Sirui Deng",
      "Shuo Liu",
      "Shimao Chen",
      "Shihua Yu",
      "Shaohui Liu",
      "Shande Wang",
      "Rui Ma",
      "Qiantong Wang",
      "Peng Wang",
      "Nuo Chen",
      "Menghang Zhu",
      "Kangyang Zhou",
      "Kang Zhou",
      "Kai Fang",
      "Jun Shi",
      "Jinhao Dong",
      "Jiebao Xiao",
      "Jiaming Xu",
      "Huaqiu Liu",
      "Hongshen Xu",
      "Heng Qu",
      "Haochen Zhao",
      "Hanglong Lv",
      "Guoan Wang",
      "Duo Zhang",
      "Dong Zhang",
      "Di Zhang",
      "Chong Ma",
      "Chang Liu",
      "Can Cai",
      "Bingquan Xia"
    ],
    "published": "2025-06-04T04:32:54+00:00",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL."
  },
  {
    "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.03568v1",
    "arxiv_id": "2506.03568v1",
    "authors": [
      "Li Zeqiao",
      "Wang Yijing",
      "Wang Haoyu",
      "Li Zheng",
      "Li Peng",
      "Zuo zhiqiang",
      "Hu Chuan"
    ],
    "published": "2025-06-04T04:31:10+00:00",
    "summary": "Autonomous driving promises significant advancements in mobility, road safety and traffic efficiency, yet reinforcement learning and imitation learning face safe-exploration and distribution-shift challenges. Although human-AI collaboration alleviates these issues, it often relies heavily on extensive human intervention, which increases costs and reduces efficiency. This paper develops a confidence-guided human-AI collaboration (C-HAC) strategy to overcome these limitations. First, C-HAC employs a distributional proxy value propagation method within the distributional soft actor-critic (DSAC) framework. By leveraging return distributions to represent human intentions C-HAC achieves rapid and stable learning of human-guided policies with minimal human interaction. Subsequently, a shared control mechanism is activated to integrate the learned human-guided policy with a self-learning policy that maximizes cumulative rewards. This enables the agent to explore independently and continuously enhance its performance beyond human guidance. Finally, a policy confidence evaluation algorithm capitalizes on DSAC's return distribution networks to facilitate dynamic switching between human-guided and self-learning policies via a confidence-based intervention function. This ensures the agent can pursue optimal policies while maintaining safety and performance guarantees. Extensive experiments across diverse driving scenarios reveal that C-HAC significantly outperforms conventional methods in terms of safety, efficiency, and overall performance, achieving state-of-the-art results. The effectiveness of the proposed method is further validated through real-world road tests in complex traffic conditions. The videos and code are available at: https://github.com/lzqw/C-HAC."
  },
  {
    "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving",
    "url": "http://arxiv.org/abs/2506.03568v2",
    "arxiv_id": "2506.03568v2",
    "authors": [
      "Li Zeqiao",
      "Wang Yijing",
      "Wang Haoyu",
      "Li Zheng",
      "Li Peng",
      "Zuo zhiqiang",
      "Hu Chuan"
    ],
    "published": "2025-06-04T04:31:10+00:00",
    "summary": "Autonomous driving promises significant advancements in mobility, road safety and traffic efficiency, yet reinforcement learning and imitation learning face safe-exploration and distribution-shift challenges. Although human-AI collaboration alleviates these issues, it often relies heavily on extensive human intervention, which increases costs and reduces efficiency. This paper develops a confidence-guided human-AI collaboration (C-HAC) strategy to overcome these limitations. First, C-HAC employs a distributional proxy value propagation method within the distributional soft actor-critic (DSAC) framework. By leveraging return distributions to represent human intentions C-HAC achieves rapid and stable learning of human-guided policies with minimal human interaction. Subsequently, a shared control mechanism is activated to integrate the learned human-guided policy with a self-learning policy that maximizes cumulative rewards. This enables the agent to explore independently and continuously enhance its performance beyond human guidance. Finally, a policy confidence evaluation algorithm capitalizes on DSAC's return distribution networks to facilitate dynamic switching between human-guided and self-learning policies via a confidence-based intervention function. This ensures the agent can pursue optimal policies while maintaining safety and performance guarantees. Extensive experiments across diverse driving scenarios reveal that C-HAC significantly outperforms conventional methods in terms of safety, efficiency, and overall performance, achieving state-of-the-art results. The effectiveness of the proposed method is further validated through real-world road tests in complex traffic conditions. The videos and code are available at: https://github.com/lzqw/C-HAC."
  },
  {
    "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2506.03557v1",
    "arxiv_id": "2506.03557v1",
    "authors": [
      "Lin Sun",
      "Chuang Liu",
      "Peng Liu",
      "Bingyang Li",
      "Weijia Lu",
      "Ning Wu"
    ],
    "published": "2025-06-04T04:21:01+00:00",
    "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for aligning Large Language Models (LLMs) with human preferences. While DPO effectively preserves the relative ordering between chosen and rejected responses through pairwise ranking losses, it often neglects absolute reward magnitudes. This oversight can decrease the likelihood of chosen responses and increase the risk of generating out-of-distribution responses, leading to poor performance. We term this issue Degraded Chosen Responses (DCR).To address this issue, we propose Balanced Preference Optimization (BPO), a novel framework that dynamically balances the optimization of chosen and rejected responses through two key components: balanced reward margin and gap adaptor. Unlike previous methods, BPO can fundamentally resolve DPO's DCR issue, without introducing additional constraints to the loss function. Experimental results on multiple mathematical reasoning tasks show that BPO significantly outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8% to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a single line of code modification, making it simple to implement and fully compatible with existing DPO-based frameworks."
  },
  {
    "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
    "url": "http://arxiv.org/abs/2506.03541v1",
    "arxiv_id": "2506.03541v1",
    "authors": [
      "Xiaofeng Zhou",
      "Heyan Huang",
      "Lizi Liao"
    ],
    "published": "2025-06-04T03:52:20+00:00",
    "summary": "Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin."
  },
  {
    "title": "Seed-Coder: Let the Code Model Curate Data for Itself",
    "url": "http://arxiv.org/abs/2506.03524v1",
    "arxiv_id": "2506.03524v1",
    "authors": [
      "Yuyu Zhang",
      "Jing Su",
      "Yifan Sun",
      "Chenguang Xi",
      "Xia Xiao",
      "Shen Zheng",
      "Anxiang Zhang",
      "Kaibo Liu",
      "Daoguang Zan",
      "Tao Sun",
      "Jinhua Zhu",
      "Shulin Xin",
      "Dong Huang",
      "Yetao Bai",
      "Lixin Dong",
      "Chao Li",
      "Jianchong Chen",
      "Hanzhi Zhou",
      "Yifan Huang",
      "Guanghan Ning",
      "Xierui Song",
      "Jiaze Chen",
      "Siyao Liu",
      "Kai Shen",
      "Liang Xiang",
      "Yonghui Wu"
    ],
    "published": "2025-06-04T03:17:19+00:00",
    "summary": "Code data in large language model (LLM) pretraining is recognized crucial not only for code-related tasks but also for enhancing general intelligence of LLMs. Current open-source LLMs often heavily rely on human effort to produce their code pretraining data, such as employing hand-crafted filtering rules tailored to individual programming languages, or using human-annotated data to train quality filters. However, these approaches are inherently limited in scalability, prone to subjective biases, and costly to extend and maintain across diverse programming languages. To address these challenges, we introduce Seed-Coder, a series of open-source LLMs comprising base, instruct and reasoning models of 8B size, minimizing human involvement in data construction. Our code pretraining data is produced by a model-centric data pipeline, which predominantly leverages LLMs for scoring and filtering code data. The instruct model is further trained via supervised fine-tuning and preference optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT) reinforcement learning to improve multi-step code reasoning. Seed-Coder achieves state-of-the-art results among open-source models of similar size and even surpasses some much larger models, demonstrating superior performance in code generation, code completion, code editing, code reasoning, and software engineering tasks."
  },
  {
    "title": "Seed-Coder: Let the Code Model Curate Data for Itself",
    "url": "http://arxiv.org/abs/2506.03524v2",
    "arxiv_id": "2506.03524v2",
    "authors": [
      "ByteDance Seed",
      "Yuyu Zhang",
      "Jing Su",
      "Yifan Sun",
      "Chenguang Xi",
      "Xia Xiao",
      "Shen Zheng",
      "Anxiang Zhang",
      "Kaibo Liu",
      "Daoguang Zan",
      "Tao Sun",
      "Jinhua Zhu",
      "Shulin Xin",
      "Dong Huang",
      "Yetao Bai",
      "Lixin Dong",
      "Chao Li",
      "Jianchong Chen",
      "Hanzhi Zhou",
      "Yifan Huang",
      "Guanghan Ning",
      "Xierui Song",
      "Jiaze Chen",
      "Siyao Liu",
      "Kai Shen",
      "Liang Xiang",
      "Yonghui Wu"
    ],
    "published": "2025-06-04T03:17:19+00:00",
    "summary": "Code data in large language model (LLM) pretraining is recognized crucial not only for code-related tasks but also for enhancing general intelligence of LLMs. Current open-source LLMs often heavily rely on human effort to produce their code pretraining data, such as employing hand-crafted filtering rules tailored to individual programming languages, or using human-annotated data to train quality filters. However, these approaches are inherently limited in scalability, prone to subjective biases, and costly to extend and maintain across diverse programming languages. To address these challenges, we introduce Seed-Coder, a series of open-source LLMs comprising base, instruct and reasoning models of 8B size, minimizing human involvement in data construction. Our code pretraining data is produced by a model-centric data pipeline, which predominantly leverages LLMs for scoring and filtering code data. The instruct model is further trained via supervised fine-tuning and preference optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT) reinforcement learning to improve multi-step code reasoning. Seed-Coder achieves state-of-the-art results among open-source models of similar size and even surpasses some much larger models, demonstrating superior performance in code generation, code completion, code editing, code reasoning, and software engineering tasks."
  },
  {
    "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals",
    "url": "http://arxiv.org/abs/2506.03519v1",
    "arxiv_id": "2506.03519v1",
    "authors": [
      "Yangyang Zhao",
      "Ben Niu",
      "Libo Qin",
      "Shihan Wang"
    ],
    "published": "2025-06-04T03:07:55+00:00",
    "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA's search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks."
  },
  {
    "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals",
    "url": "http://arxiv.org/abs/2506.03519v2",
    "arxiv_id": "2506.03519v2",
    "authors": [
      "Yangyang Zhao",
      "Ben Niu",
      "Libo Qin",
      "Shihan Wang"
    ],
    "published": "2025-06-04T03:07:55+00:00",
    "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA's search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks."
  },
  {
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models",
    "url": "http://arxiv.org/abs/2506.03517v1",
    "arxiv_id": "2506.03517v1",
    "authors": [
      "Ziyi Wu",
      "Anil Kag",
      "Ivan Skorokhodov",
      "Willi Menapace",
      "Ashkan Mirzaei",
      "Igor Gilitschenski",
      "Sergey Tulyakov",
      "Aliaksandr Siarohin"
    ],
    "published": "2025-06-04T03:06:08+00:00",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels."
  },
  {
    "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models",
    "url": "http://arxiv.org/abs/2506.03516v1",
    "arxiv_id": "2506.03516v1",
    "authors": [
      "Arnab Debnath",
      "Gregory J. Stein",
      "Jana Kosecka"
    ],
    "published": "2025-06-04T03:04:54+00:00",
    "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent is instructed to locate a target object in an unexplored environment. Traditional learning-based methods rely heavily on large-scale annotated data or require extensive interaction with the environment in a reinforcement learning setting, often failing to generalize to novel environments and limiting scalability. To overcome these challenges, we explore a zero-shot setting where the agent operates without task-specific training, enabling more scalable and adaptable solution. Recent advances in Vision Foundation Models (VFMs) offer powerful capabilities for visual understanding and reasoning, making them ideal for agents to comprehend scenes, identify relevant regions, and infer the likely locations of objects. In this work, we present a zero-shot object goal navigation framework that integrates the perceptual strength of VFMs with a model-based planner that is capable of long-horizon decision making through frontier exploration. We evaluate our approach on the HM3D dataset using the Habitat simulator and demonstrate that our method achieves state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation."
  },
  {
    "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking",
    "url": "http://arxiv.org/abs/2506.03487v1",
    "arxiv_id": "2506.03487v1",
    "authors": [
      "Xianming Li",
      "Aamir Shakir",
      "Rui Huang",
      "Julius Lipp",
      "Jing Li"
    ],
    "published": "2025-06-04T02:00:44+00:00",
    "summary": "Reranking is fundamental to information retrieval and retrieval-augmented generation, with recent Large Language Models (LLMs) significantly advancing reranking quality. While recent advances with LLMs have significantly improved document reranking quality, current approaches primarily rely on large-scale LLMs (>7B parameters) through zero-shot prompting, presenting high computational costs. Small Language Models (SLMs) offer a promising alternative because of their efficiency, but our preliminary quantitative analysis reveals they struggle with understanding task prompts without fine-tuning. This limits their effectiveness for document reranking tasks. To address this issue, we introduce a novel two-stage training approach, ProRank, for SLM-based document reranking. First, we propose a prompt warmup stage using reinforcement learning GRPO to steer SLMs to understand task prompts and generate more accurate coarse-grained binary relevance scores for document reranking. Then, we continuously fine-tune the SLMs with a fine-grained score learning stage without introducing additional layers to further improve the reranking quality. Comprehensive experimental results demonstrate that the proposed ProRank consistently outperforms both the most advanced open-source and proprietary reranking models. Notably, our lightweight ProRank-0.5B model even surpasses the powerful 32B LLM reranking model on the BEIR benchmark, establishing that properly trained SLMs can achieve superior document reranking performance while maintaining computational efficiency."
  },
  {
    "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design",
    "url": "http://arxiv.org/abs/2506.03474v1",
    "arxiv_id": "2506.03474v1",
    "authors": [
      "Yifeng Xiao",
      "Yurong Xu",
      "Ning Yan",
      "Masood Mortazavi",
      "Pierluigi Nuzzo"
    ],
    "published": "2025-06-04T01:08:34+00:00",
    "summary": "Simulation-based design space exploration (DSE) aims to efficiently optimize high-dimensional structured designs under complex constraints and expensive evaluation costs. Existing approaches, including heuristic and multi-step reinforcement learning (RL) methods, struggle to balance sampling efficiency and constraint satisfaction due to sparse, delayed feedback, and large hybrid action spaces. In this paper, we introduce CORE, a constraint-aware, one-step RL method for simulationguided DSE. In CORE, the policy agent learns to sample design configurations by defining a structured distribution over them, incorporating dependencies via a scaling-graph-based decoder, and by reward shaping to penalize invalid designs based on the feedback obtained from simulation. CORE updates the policy using a surrogate objective that compares the rewards of designs within a sampled batch, without learning a value function. This critic-free formulation enables efficient learning by encouraging the selection of higher-reward designs. We instantiate CORE for hardware-mapping co-design of neural network accelerators, demonstrating that it significantly improves sample efficiency and achieves better accelerator configurations compared to state-of-the-art baselines. Our approach is general and applicable to a broad class of discrete-continuous constrained design problems."
  },
  {
    "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration",
    "url": "http://arxiv.org/abs/2506.03469v1",
    "arxiv_id": "2506.03469v1",
    "authors": [
      "Tuan Le",
      "Risal Shefin",
      "Debashis Gupta",
      "Thai Le",
      "Sarra Alqahtani"
    ],
    "published": "2025-06-04T00:54:01+00:00",
    "summary": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes environments requires not only formal verification but also interpretability and targeted falsification. While model checking provides formal guarantees, its effectiveness is limited by abstraction quality and the completeness of the underlying trajectory dataset. We propose a hybrid framework that integrates (1) explainability, (2) model checking, and (3) risk-guided falsification to achieve both rigor and coverage. Our approach begins by constructing a human-interpretable abstraction of the RL policy using Comprehensible Abstract Policy Summarization (CAPS). This abstract graph, derived from offline trajectories, is both verifier-friendly, semantically meaningful, and can be used as input to Storm probabilistic model checker to verify satisfaction of temporal safety specifications. If the model checker identifies a violation, it will return an interpretable counterexample trace by which the policy fails the safety requirement. However, if no violation is detected, we cannot conclude satisfaction due to potential limitation in the abstraction and coverage of the offline dataset. In such cases, we estimate associated risk during model checking to guide a falsification strategy that prioritizes searching in high-risk states and regions underrepresented in the trajectory dataset. We further provide PAC-style guarantees on the likelihood of uncovering undetected violations. Finally, we incorporate a lightweight safety shield that switches to a fallback policy at runtime when such a risk exceeds a threshold, facilitating failure mitigation without retraining."
  },
  {
    "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks",
    "url": "http://arxiv.org/abs/2506.03404v1",
    "arxiv_id": "2506.03404v1",
    "authors": [
      "Walter Mayor",
      "Johan Obando-Ceron",
      "Aaron Courville",
      "Pablo Samuel Castro"
    ],
    "published": "2025-06-03T21:27:17+00:00",
    "summary": "The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance between sample efficiency and overfitting. We conduct an empirical analysis of these trade-offs on PPO, one of the most popular RL algorithms that uses parallel actors, and establish connections to network plasticity and, more generally, optimization stability. We examine its impact on network architectures, as well as the hyper-parameter sensitivity when scaling data. Our analyses indicate that larger dataset sizes can increase final performance across a variety of settings, and that scaling parallel environments is more effective than increasing rollout lengths. These findings highlight the critical role of data collection strategies in improving agent performance."
  },
  {
    "title": "Reinforcement Learning Enhanced Greedy Decoding for Quantum Stabilizer Codes over $\\mathbb{F}_q$",
    "url": "http://arxiv.org/abs/2506.03397v1",
    "arxiv_id": "2506.03397v1",
    "authors": [
      "Vahid Nourozi"
    ],
    "published": "2025-06-03T21:08:36+00:00",
    "summary": "We construct new classical Goppa codes and corresponding quantum stabilizer codes from plane curves defined by separated polynomials. In particular, over $\\mathbb{F}_3$ with the Hermitian curve $y^3 + y = x^4$, we obtain a ternary code of length 27, dimension 13, distance 4, which yields a [[27, 13, 4]]$_3$ quantum code. To decode, we introduce an RL-on-Greedy algorithm: first apply a standard greedy syndrome decoder, then use a trained Deep Q-Network to correct any residual syndrome. Simulation under a depolarizing noise model shows that RL-on-Greedy dramatically reduces logical failure compared to greedy alone. Our work thus broadens the class of Goppa- and quantum-stabilizer codes from separated-polynomial curves and delivers a learned decoder with near-optimal performance."
  },
  {
    "title": "Seeing the Arrow of Time in Large Multimodal Models",
    "url": "http://arxiv.org/abs/2506.03340v1",
    "arxiv_id": "2506.03340v1",
    "authors": [
      "Zihui Xue",
      "Mi Luo",
      "Kristen Grauman"
    ],
    "published": "2025-06-03T19:32:07+00:00",
    "summary": "The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is fundamental to video comprehension, yet remains a significant challenge for modern large multimodal models (LMMs). Current LMMs struggle to perceive and utilize temporal directionality in video when responding to language queries, obstructing deeper temporal understanding. We tackle this deficiency by first providing a critical analysis of existing benchmarks and models. We then introduce ArrowRL, a reinforcement learning (RL)-based training strategy with an innovative reverse reward that instills AoT awareness by encouraging divergent video interpretations between forward and reversed visual frames. For rigorous evaluation, we additionally develop AoTBench, a new multi-faceted benchmark probing temporally challenging questions. Experiments show ArrowRL greatly advances temporal perception: it not only achieves substantial improvements on our challenging AoTBench but also demonstrably boosts performance on standard video question answering (VQA) benchmarks (with peak accuracy gains reaching over 20% and 10% respectively). This validates ArrowRL's effectiveness and highlights the critical need for dedicated AoT understanding in LMMs."
  },
  {
    "title": "A Differential Perspective on Distributional Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.03333v1",
    "arxiv_id": "2506.03333v1",
    "authors": [
      "Juan Sebastian Rojas",
      "Chi-Guhn Lee"
    ],
    "published": "2025-06-03T19:26:25+00:00",
    "summary": "To date, distributional reinforcement learning (distributional RL) methods have exclusively focused on the discounted setting, where an agent aims to optimize a potentially-discounted sum of rewards over time. In this work, we extend distributional RL to the average-reward setting, where an agent aims to optimize the reward received per time-step. In particular, we utilize a quantile-based approach to develop the first set of algorithms that can successfully learn and/or optimize the long-run per-step reward distribution, as well as the differential return distribution of an average-reward MDP. We derive proven-convergent tabular algorithms for both prediction and control, as well as a broader family of algorithms that have appealing scaling properties. Empirically, we find that these algorithms consistently yield competitive performance when compared to their non-distributional equivalents, while also capturing rich information about the long-run reward and return distributions."
  },
  {
    "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions",
    "url": "http://arxiv.org/abs/2506.03320v1",
    "arxiv_id": "2506.03320v1",
    "authors": [
      "Jack Bell",
      "Luigi Quarantiello",
      "Eric Nuertey Coleman",
      "Lanpei Li",
      "Malio Li",
      "Mauro Madeddu",
      "Elia Piccoli",
      "Vincenzo Lomonaco"
    ],
    "published": "2025-06-03T19:06:41+00:00",
    "summary": "Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever."
  },
  {
    "title": "Dynamics and Control of Vision-Aided Multi-UAV-tethered Netted System Capturing Non-Cooperative Target",
    "url": "http://arxiv.org/abs/2506.03297v1",
    "arxiv_id": "2506.03297v1",
    "authors": [
      "Runhan Liu",
      "Hui Ren",
      "Wei Fan"
    ],
    "published": "2025-06-03T18:36:25+00:00",
    "summary": "As the number of Unmanned Aerial Vehicles (UAVs) operating in low-altitude airspace continues to increase, non-cooperative targets pose growing challenges to low-altitude operations. To address this issue, this paper proposes a multi-UAV-tethered netted system as a non-lethal solution for capturing non-cooperative targets. To validate the proposed system, we develop mySim, a multibody dynamics-based UAV simulation environment that integrates high-precision physics modeling, vision-based motion tracking, and reinforcement learning-driven control strategies. In mySim, the spring-damper model is employed to simulate the dynamic behavior of the tethered net, while the dynamics of the entire system is modeled using multibody dynamics (MBD) to achieve accurate representations of system interactions. The motion of the UAVs and the target are estimated using VINS-MONO and DETR, and the system autonomously executes the capture strategy through MAPPO. Simulation results demonstrate that mySim accurately simulates dynamics and control of the system, successfully enabling the multi-UAV-tethered netted system to capture both non-propelled and maneuvering non-cooperative targets. By providing a high-precision simulation platform that integrates dynamics modeling with perception and learning-based control, mySim enables efficient testing and optimization of UAV-based control policies before real-world deployment. This approach offers significant advantages for simulating complex UAVs coordination tasks and has the potential to be applied to the design of other UAV-based systems."
  },
  {
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem",
    "url": "http://arxiv.org/abs/2506.03295v1",
    "arxiv_id": "2506.03295v1",
    "authors": [
      "Yubo Wang",
      "Ping Nie",
      "Kai Zou",
      "Lijun Wu",
      "Wenhu Chen"
    ],
    "published": "2025-06-03T18:35:52+00:00",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs."
  },
  {
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem",
    "url": "http://arxiv.org/abs/2506.03295v2",
    "arxiv_id": "2506.03295v2",
    "authors": [
      "Yubo Wang",
      "Ping Nie",
      "Kai Zou",
      "Lijun Wu",
      "Wenhu Chen"
    ],
    "published": "2025-06-03T18:35:52+00:00",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs."
  },
  {
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.03136v1",
    "arxiv_id": "2506.03136v1",
    "authors": [
      "Yinjie Wang",
      "Ling Yang",
      "Ye Tian",
      "Ke Shen",
      "Mengdi Wang"
    ],
    "published": "2025-06-03T17:58:42+00:00",
    "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE"
  },
  {
    "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation",
    "url": "http://arxiv.org/abs/2506.03122v1",
    "arxiv_id": "2506.03122v1",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Luyao Shi",
      "Ehsan Degan",
      "Vandana Mukherjee",
      "Xin Zhang"
    ],
    "published": "2025-06-03T17:54:30+00:00",
    "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design."
  },
  {
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "url": "http://arxiv.org/abs/2506.03106v1",
    "arxiv_id": "2506.03106v1",
    "authors": [
      "Xiaoying Zhang",
      "Hao Sun",
      "Yipeng Zhang",
      "Kaituo Feng",
      "Chao Yang",
      "Helen Meng"
    ],
    "published": "2025-06-03T17:39:02+00:00",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration."
  },
  {
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "url": "http://arxiv.org/abs/2506.03106v2",
    "arxiv_id": "2506.03106v2",
    "authors": [
      "Xiaoying Zhang",
      "Hao Sun",
      "Yipeng Zhang",
      "Kaituo Feng",
      "Chaochao Lu",
      "Chao Yang",
      "Helen Meng"
    ],
    "published": "2025-06-03T17:39:02+00:00",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration."
  },
  {
    "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
    "url": "http://arxiv.org/abs/2506.03097v1",
    "arxiv_id": "2506.03097v1",
    "authors": [
      "Ashwin Vinod",
      "Shrey Pandit",
      "Aditya Vavre",
      "Linshen Liu"
    ],
    "published": "2025-06-03T17:28:00+00:00",
    "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning."
  },
  {
    "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
    "url": "http://arxiv.org/abs/2506.03095v1",
    "arxiv_id": "2506.03095v1",
    "authors": [
      "Man Luo",
      "David Cobbley",
      "Xin Su",
      "Shachar Rosenman",
      "Vasudev Lal",
      "Shao-Yen Tseng",
      "Phillip Howard"
    ],
    "published": "2025-06-03T17:27:04+00:00",
    "summary": "Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents."
  },
  {
    "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis",
    "url": "http://arxiv.org/abs/2506.03082v1",
    "arxiv_id": "2506.03082v1",
    "authors": [
      "Ssharvien Kumar Sivakumar",
      "Yannik Frisch",
      "Ghazal Ghazaei",
      "Anirban Mukhopadhyay"
    ],
    "published": "2025-06-03T17:02:38+00:00",
    "summary": "Surgical simulation plays a pivotal role in training novice surgeons, accelerating their learning curve and reducing intra-operative errors. However, conventional simulation tools fall short in providing the necessary photorealism and the variability of human anatomy. In response, current methods are shifting towards generative model-based simulators. Yet, these approaches primarily focus on using increasingly complex conditioning for precise synthesis while neglecting the fine-grained human control aspect. To address this gap, we introduce SG2VID, the first diffusion-based video model that leverages Scene Graphs for both precise video synthesis and fine-grained human control. We demonstrate SG2VID's capabilities across three public datasets featuring cataract and cholecystectomy surgery. While SG2VID outperforms previous methods both qualitatively and quantitatively, it also enables precise synthesis, providing accurate control over tool and anatomy's size and movement, entrance of new tools, as well as the overall scene layout. We qualitatively motivate how SG2VID can be used for generative augmentation and present an experiment demonstrating its ability to improve a downstream phase detection task when the training set is extended with our synthetic videos. Finally, to showcase SG2VID's ability to retain human control, we interact with the Scene Graphs to generate new video samples depicting major yet rare intra-operative irregularities."
  },
  {
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
    "url": "http://arxiv.org/abs/2506.03077v1",
    "arxiv_id": "2506.03077v1",
    "authors": [
      "Qijun Luo",
      "Mengqi Li",
      "Lei Zhao",
      "Xiao Li"
    ],
    "published": "2025-06-03T16:54:15+00:00",
    "summary": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP."
  },
  {
    "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link Function",
    "url": "http://arxiv.org/abs/2506.03066v1",
    "arxiv_id": "2506.03066v1",
    "authors": [
      "Qining Zhang",
      "Lei Ying"
    ],
    "published": "2025-06-03T16:42:39+00:00",
    "summary": "Link functions, which characterize how human preferences are generated from the value function of an RL problem, are a crucial component in designing RLHF algorithms. Almost all RLHF algorithms, including state-of-the-art ones in empirical studies such as DPO and PPO, assume the link function is known to the agent (e.g., a logistic function according to the Bradley-Terry model), which is arguably unrealistic considering the complex nature of human preferences. To avoid link function mis-specification, this paper studies general RLHF problems with unknown link functions. We propose a novel policy optimization algorithm called ZSPO based on a new zeroth-order policy optimization method, where the key is to use human preference to construct a parameter update direction that is positively correlated with the true policy gradient direction. ZSPO achieves it by estimating the sign of the value function difference instead of estimating the gradient from the value function difference, so it does not require knowing the link function. Under mild conditions, ZSPO converges to a stationary policy with a polynomial convergence rate depending on the number of policy iterations and trajectories per iteration. Numerical results also show the superiority of ZSPO under link function mismatch."
  },
  {
    "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment",
    "url": "http://arxiv.org/abs/2506.03046v1",
    "arxiv_id": "2506.03046v1",
    "authors": [
      "Mikolaj Walczak",
      "Romina Aalishah",
      "Wyatt Mackey",
      "Brittany Story",
      "David L. Boothe Jr.",
      "Nicholas Waytowich",
      "Xiaomin Lin",
      "Tinoosh Mohsenin"
    ],
    "published": "2025-06-03T16:28:33+00:00",
    "summary": "Deep reinforcement learning agents are often fragile while humans remain adaptive and flexible to varying scenarios. To bridge this gap, we present EDEN, a biologically inspired navigation framework that integrates learned entorhinal-like grid cell representations and reinforcement learning to enable autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system, EDEN allows agents to perform path integration and vector-based navigation using visual and motion sensor data. At the core of EDEN is a grid cell encoder that transforms egocentric motion into periodic spatial codes, producing low-dimensional, interpretable embeddings of position. To generate these activations from raw sensory input, we combine fiducial marker detections in the lightweight MiniWorld simulator and DINO-based visual features in the high-fidelity Gazebo simulator. These spatial representations serve as input to a policy trained with Proximal Policy Optimization (PPO), enabling dynamic, goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid prototyping, and Gazebo, which offers realistic physics and perception noise. Compared to baseline agents using raw state inputs (e.g., position, velocity) or standard convolutional image encoders, EDEN achieves a 99% success rate, within the simple scenarios, and >94% within complex floorplans with occluded paths with more efficient and reliable step-wise navigation. In addition, as a replacement of ground truth activations, we present a trainable Grid Cell encoder enabling the development of periodic grid-like patterns from vision and motion sensor data, emulating the development of such patterns within biological mammals. This work represents a step toward biologically grounded spatial intelligence in robotics, bridging neural navigation principles with reinforcement learning for scalable deployment."
  },
  {
    "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective",
    "url": "http://arxiv.org/abs/2506.03038v1",
    "arxiv_id": "2506.03038v1",
    "authors": [
      "Jintian Shao",
      "Yiming Cheng"
    ],
    "published": "2025-06-03T16:20:47+00:00",
    "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex, long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework, despite sophisticated mechanisms like Decoupled GAE, theoretically faces fundamental limitations in comprehensively modeling and leveraging deep, long-term value for fine-grained, step-by-step policy guidance in extended reasoning chains. We argue these limitations stem from inherent difficulties in credit assignment, value function representational capacity with temporally abstracted goals, and translating global value signals into local policy improvements, especially with sparse rewards. Our theoretical analysis examines these aspects to illuminate VAPO's boundaries in long-term value modeling, aiming to deepen understanding of current RL for advanced reasoning and suggest future research for more robust LLM agents."
  },
  {
    "title": "Protein Inverse Folding From Structure Feedback",
    "url": "http://arxiv.org/abs/2506.03028v1",
    "arxiv_id": "2506.03028v1",
    "authors": [
      "Junde Xu",
      "Zijun Gao",
      "Xinyi Zhou",
      "Jie Hu",
      "Xingyi Cheng",
      "Le Song",
      "Guangyong Chen",
      "Pheng-Ann Heng",
      "Jiezhong Qiu"
    ],
    "published": "2025-06-03T16:02:12+00:00",
    "summary": "The inverse folding problem, aiming to design amino acid sequences that fold into desired three-dimensional structures, is pivotal for various biotechnological applications. Here, we introduce a novel approach leveraging Direct Preference Optimization (DPO) to fine-tune an inverse folding model using feedback from a protein folding model. Given a target protein structure, we begin by sampling candidate sequences from the inverse-folding model, then predict the three-dimensional structure of each sequence with the folding model to generate pairwise structural-preference labels. These labels are used to fine-tune the inverse-folding model under the DPO objective. Our results on the CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence recovery of baseline models but also leads to a significant improvement in average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity. Furthermore, iterative application of our DPO-based method on challenging protein structures yields substantial gains, with an average TM-Score increase of 79.5\\% with regard to the baseline model. This work establishes a promising direction for enhancing protein sequence design ability from structure feedback by effectively utilizing preference optimization."
  },
  {
    "title": "A Preference-Driven Methodology for High-Quality Solidity Code Generation",
    "url": "http://arxiv.org/abs/2506.03006v1",
    "arxiv_id": "2506.03006v1",
    "authors": [
      "Zhiyuan Peng",
      "Xin Yin",
      "Chenhao Ying",
      "Chao Ni",
      "Yuan Luo"
    ],
    "published": "2025-06-03T15:45:31+00:00",
    "summary": "While Large Language Models (LLMs) have demonstrated remarkable progress in generating functionally correct Solidity code, they continue to face critical challenges in producing gas-efficient and secure code, which are critical requirements for real-world smart contract deployment. Although recent advances leverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) for code preference alignment, existing approaches treat functional correctness, gas optimization, and security as independent objectives, resulting in contracts that may achieve operational soundness but suffer from prohibitive execution costs or dangerous vulnerabilities. To address these limitations, we propose PrefGen, a novel framework that extends standard DPO beyond human preferences to incorporate quantifiable blockchain-specific metrics, enabling holistic multi-objective optimization specifically tailored for smart contract generation. Our framework introduces a comprehensive evaluation methodology with four complementary metrics: Pass@k (functional correctness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and Secure@k (security assessment), providing rigorous multi-dimensional contract evaluation. Through extensive experimentation, we demonstrate that PrefGen significantly outperforms existing approaches across all critical dimensions, achieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating production-ready smart contracts that are functionally correct, cost-efficient, and secure."
  },
  {
    "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver",
    "url": "http://arxiv.org/abs/2506.02935v1",
    "arxiv_id": "2506.02935v1",
    "authors": [
      "Yuepeng Zheng",
      "Fu Luo",
      "Zhenkun Wang",
      "Yaoxin Wu",
      "Yu Zhou"
    ],
    "published": "2025-06-03T14:35:36+00:00",
    "summary": "Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities."
  },
  {
    "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.02911v1",
    "arxiv_id": "2506.02911v1",
    "authors": [
      "Yin Fang",
      "Qiao Jin",
      "Guangzhi Xiong",
      "Bowen Jin",
      "Xianrui Zhong",
      "Siru Ouyang",
      "Aidong Zhang",
      "Jiawei Han",
      "Zhiyong Lu"
    ],
    "published": "2025-06-03T14:16:53+00:00",
    "summary": "Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1."
  },
  {
    "title": "A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks",
    "url": "http://arxiv.org/abs/2506.02883v1",
    "arxiv_id": "2506.02883v1",
    "authors": [
      "Anthony Kobanda",
      "Odalric-Ambrym Maillard",
      "R\u00e9my Portelas"
    ],
    "published": "2025-06-03T13:48:20+00:00",
    "summary": "Autonomous agents operating in domains such as robotics or video game simulations must adapt to changing tasks without forgetting about the previous ones. This process called Continual Reinforcement Learning poses non-trivial difficulties, from preventing catastrophic forgetting to ensuring the scalability of the approaches considered. Building on recent advances, we introduce a benchmark providing a suite of video-game navigation scenarios, thus filling a gap in the literature and capturing key challenges : catastrophic forgetting, task adaptation, and memory efficiency. We define a set of various tasks and datasets, evaluation protocols, and metrics to assess the performance of algorithms, including state-of-the-art baselines. Our benchmark is designed not only to foster reproducible research and to accelerate progress in continual reinforcement learning for gaming, but also to provide a reproducible framework for production pipelines -- helping practitioners to identify and to apply effective approaches."
  },
  {
    "title": "BNPO: Beta Normalization Policy Optimization",
    "url": "http://arxiv.org/abs/2506.02864v1",
    "arxiv_id": "2506.02864v1",
    "authors": [
      "Changyi Xiao",
      "Mengdi Zhang",
      "Yixin Cao"
    ],
    "published": "2025-06-03T13:28:57+00:00",
    "summary": "Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPO's variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPO's applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at https://github.com/changyi7231/BNPO."
  },
  {
    "title": "Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games",
    "url": "http://arxiv.org/abs/2506.02849v1",
    "arxiv_id": "2506.02849v1",
    "authors": [
      "Alejandro Sanchez Roncero",
      "Olov Andersson",
      "Petter Ogren"
    ],
    "published": "2025-06-03T13:19:23+00:00",
    "summary": "The increasing proliferation of small UAVs in civilian and military airspace has raised critical safety and security concerns, especially when unauthorized or malicious drones enter restricted zones. In this work, we present a reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion. We train neural network policies to command body rates and collective thrust, enabling high-speed pursuit and evasive maneuvers that fully exploit the quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic forgetting during adversarial co-training, we introduce an Asynchronous Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the pursuer or evader learns against a sampled opponent drawn from a growing population of past and current policies. This continual learning setup ensures monotonic performance improvement and retention of earlier strategies. Our results show that (i) rate-based policies achieve significantly higher capture rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields stable, monotonic gains against a suite of benchmark opponents."
  },
  {
    "title": "Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods",
    "url": "http://arxiv.org/abs/2506.02841v1",
    "arxiv_id": "2506.02841v1",
    "authors": [
      "Tom Danino",
      "Nahum Shimkin"
    ],
    "published": "2025-06-03T13:13:15+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($\\lambda$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps."
  },
  {
    "title": "Adaptive Configuration Selection for Multi-Model Inference Pipelines in Edge Computing",
    "url": "http://arxiv.org/abs/2506.02814v1",
    "arxiv_id": "2506.02814v1",
    "authors": [
      "Jinhao Sheng",
      "Zhiqing Tang",
      "Jianxiong Guo",
      "Tian Wang"
    ],
    "published": "2025-06-03T12:44:46+00:00",
    "summary": "The growing demand for real-time processing tasks is driving the need for multi-model inference pipelines on edge devices. However, cost-effectively deploying these pipelines while optimizing Quality of Service (QoS) and costs poses significant challenges. Existing solutions often neglect device resource constraints, focusing mainly on inference accuracy and cost efficiency. To address this, we develop a framework for configuring multi-model inference pipelines. Specifically: 1) We model the decision-making problem by considering the pipeline's QoS, costs, and device resource limitations. 2) We create a feature extraction module using residual networks and a load prediction model based on Long Short-Term Memory (LSTM) to gather comprehensive node and pipeline status information. Then, we implement a Reinforcement Learning (RL) algorithm based on policy gradients for online configuration decisions. 3) Experiments conducted in a real Kubernetes cluster show that our approach significantly improve QoS while reducing costs and shorten decision-making time for complex pipelines compared to baseline algorithms."
  },
  {
    "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts",
    "url": "http://arxiv.org/abs/2506.02781v1",
    "arxiv_id": "2506.02781v1",
    "authors": [
      "Tongyuan Bai",
      "Wangyuanfan Bai",
      "Dong Chen",
      "Tieru Wu",
      "Manyi Li",
      "Rui Ma"
    ],
    "published": "2025-06-03T12:01:41+00:00",
    "summary": "Controllability plays a crucial role in the practical applications of 3D indoor scene synthesis. Existing works either allow rough language-based control, that is convenient but lacks fine-grained scene customization, or employ graph based control, which offers better controllability but demands considerable knowledge for the cumbersome graph design process. To address these challenges, we present FreeScene, a user-friendly framework that enables both convenient and effective control for indoor scene synthesis.Specifically, FreeScene supports free-form user inputs including text description and/or reference images, allowing users to express versatile design intentions. The user inputs are adequately analyzed and integrated into a graph representation by a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion Transformer, which performs graph-aware denoising to enhance scene generation. Our MG-DiT not only excels at preserving graph structure but also offers broad applicability to various tasks, including, but not limited to, text-to-scene, graph-to-scene, and rearrangement, all within a single model. Extensive experiments demonstrate that FreeScene provides an efficient and user-friendly solution that unifies text-based and graph based scene synthesis, outperforming state-of-the-art methods in terms of both generation quality and controllability in a range of applications."
  },
  {
    "title": "Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization",
    "url": "http://arxiv.org/abs/2506.02767v1",
    "arxiv_id": "2506.02767v1",
    "authors": [
      "Marco Cal\u00ec",
      "Giulio Giacomuzzo",
      "Ruggero Carli",
      "Alberto Dalla Libera"
    ],
    "published": "2025-06-03T11:30:59+00:00",
    "summary": "This paper addresses the slow policy optimization convergence of Monte Carlo Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art model-based reinforcement learning (MBRL) algorithm, by integrating it with iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization method suitable for nonlinear systems. The proposed method, Exploration-Boosted MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory trajectories and initialize the policy, significantly reducing the number of required optimization steps. Experiments on the cart-pole task demonstrate that EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up to $\\bm{45.9\\%}$ reduction in execution time when both methods solve the task in four trials. EB-MC-PILCO also maintains a $\\bm{100\\%}$ success rate across trials while solving the task faster, even in cases where MC-PILCO converges in fewer iterations."
  },
  {
    "title": "Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search",
    "url": "http://arxiv.org/abs/2506.02746v1",
    "arxiv_id": "2506.02746v1",
    "authors": [
      "Lin Xie",
      "Hanyi Li"
    ],
    "published": "2025-06-03T11:07:41+00:00",
    "summary": "The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems (RMFS) involves selecting optimal storage locations for pods returning from pick stations. This work presents an improved solution method that integrates Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning (DRL). A DRL agent dynamically selects destroy and repair operators and adjusts key parameters such as destruction degree and acceptance thresholds during the search. Specialized heuristics for both operators are designed to reflect PRP-specific characteristics, including pod usage frequency and movement costs. Computational results show that this DRL-guided ALNS outperforms traditional approaches such as cheapest-place, fixed-place, binary integer programming, and static heuristics. The method demonstrates strong solution quality and illustrating the benefit of learning-driven control within combinatorial optimization for warehouse systems."
  },
  {
    "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models",
    "url": "http://arxiv.org/abs/2506.02726v1",
    "arxiv_id": "2506.02726v1",
    "authors": [
      "Qihang Yan",
      "Xinyu Zhang",
      "Luming Guo",
      "Qi Zhang",
      "Feifan Liu"
    ],
    "published": "2025-06-03T10:36:38+00:00",
    "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains."
  },
  {
    "title": "Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2506.02718v1",
    "arxiv_id": "2506.02718v1",
    "authors": [
      "Guanzhong Chen",
      "Shaoxiong Yang",
      "Chao Li",
      "Wei Liu",
      "Jian Luan",
      "Zenglin Xu"
    ],
    "published": "2025-06-03T10:17:19+00:00",
    "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS."
  },
  {
    "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences",
    "url": "http://arxiv.org/abs/2506.02698v1",
    "arxiv_id": "2506.02698v1",
    "authors": [
      "Yunhong Lu",
      "Qichao Wang",
      "Hengyuan Cao",
      "Xiaoyin Xu",
      "Min Zhang"
    ],
    "published": "2025-06-03T09:47:22+00:00",
    "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation models with human preferences using pairwise preference data. Although substantial resources are expended in collecting and labeling datasets, a critical aspect is often neglected: \\textit{preferences vary across individuals and should be represented with more granularity.} To address this, we propose SmPO-Diffusion, a novel method for modeling preference distributions to improve the DPO objective, along with a numerical upper bound estimation for the diffusion optimization objective. First, we introduce a smoothed preference distribution to replace the original binary distribution. We employ a reward model to simulate human preferences and apply preference likelihood averaging to improve the DPO loss, such that the loss function approaches zero when preferences are similar. Furthermore, we utilize an inversion technique to simulate the trajectory preference distribution of the diffusion model, enabling more accurate alignment with the optimization objective. Our approach effectively mitigates issues of excessive optimization and objective misalignment present in existing methods through straightforward modifications. Our SmPO-Diffusion achieves state-of-the-art performance in preference evaluation, outperforming baselines across metrics with lower training costs. The project page is https://jaydenlyh.github.io/SmPO-project-page/."
  },
  {
    "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
    "url": "http://arxiv.org/abs/2506.02678v1",
    "arxiv_id": "2506.02678v1",
    "authors": [
      "Zhong-Zhi Li",
      "Xiao Liang",
      "Zihao Tang",
      "Lei Ji",
      "Peijie Wang",
      "Haotian Xu",
      "Xing W",
      "Haizhen Huang",
      "Weiwei Deng",
      "Ying Nian Wu",
      "Yeyun Gong",
      "Zhijiang Guo",
      "Xiao Liu",
      "Fei Yin",
      "Cheng-Lin Liu"
    ],
    "published": "2025-06-03T09:23:41+00:00",
    "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon."
  },
  {
    "title": "FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems",
    "url": "http://arxiv.org/abs/2506.02668v1",
    "arxiv_id": "2506.02668v1",
    "authors": [
      "Frederico Metelo",
      "Alexandre Oliveira",
      "Stevo Rackovi\u0107",
      "Pedro \u00c1kos Costa",
      "Cl\u00e1udia Soares"
    ],
    "published": "2025-06-03T09:15:03+00:00",
    "summary": "Edge computing addresses the growing data demands of connected-device networks by placing computational resources closer to end users through decentralized infrastructures. This decentralization challenges traditional, fully centralized orchestration, which suffers from latency and resource bottlenecks. We present \\textbf{FAuNO} -- \\emph{Federated Asynchronous Network Orchestrator} -- a buffered, asynchronous \\emph{federated reinforcement-learning} (FRL) framework for decentralized task offloading in edge systems. FAuNO adopts an actor-critic architecture in which local actors learn node-specific dynamics and peer interactions, while a federated critic aggregates experience across agents to encourage efficient cooperation and improve overall system performance. Experiments in the \\emph{PeersimGym} environment show that FAuNO consistently matches or exceeds heuristic and federated multi-agent RL baselines in reducing task loss and latency, underscoring its adaptability to dynamic edge-computing scenarios."
  },
  {
    "title": "Computational Thinking Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2506.02658v1",
    "arxiv_id": "2506.02658v1",
    "authors": [
      "Kechi Zhang",
      "Ge Li",
      "Jia Li",
      "Huangzhao Zhang",
      "Jingjing Xu",
      "Hao Zhu",
      "Lecheng Wang",
      "Jia Li",
      "Yihong Dong",
      "Jing Mai",
      "Bin Gu",
      "Zhi Jin"
    ],
    "published": "2025-06-03T09:11:15+00:00",
    "summary": "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they often struggle with complex tasks that require specific thinking paradigms, such as divide-and-conquer and procedural deduction, \\etc Previous researches integrate external, reliable tools to alleviate logical inconsistencies and hallucinations in LLMs' problem-solving processes. However, we argue that the root challenge is more profound: LLMs lack the complex thinking paradigms (\\ie, computational thinking) during reasoning. In this paper, we propose Computational Thinking Model (CTM), a novel framework that incorporates computational thinking paradigms into LLMs. This framework enables LLMs to reformulate complex problems through decomposition, abstraction, reduction, and simulation, among other techniques. Specifically, live code execution is seamlessly integrated into the reasoning process, allowing CTM to think by computing. CTM directly instills computational thinking objectives into LLMs through tailored reinforcement learning rewards, which encourages problem simplification, modular planning, and iterative verification. We conduct extensive evaluations on multiple code generation and mathematical benchmarks. The results demonstrate that CTM outperforms conventional reasoning models and tool-augmented baselines in terms of accuracy, interpretability, and generalizability. We hope this study offers valuable insights for AI reasoning, where LLMs can transform problems into robust, verifiable, and scalable computational workflows, much like computer scientists do."
  },
  {
    "title": "Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.02657v1",
    "arxiv_id": "2506.02657v1",
    "authors": [
      "Tam Ninh Thi-Thanh",
      "Trinh Van Chien",
      "Hung Tran",
      "Nguyen Hoai Son",
      "Van Nhan Vo"
    ],
    "published": "2025-06-03T09:10:31+00:00",
    "summary": "Metaverse and Digital Twin (DT) have attracted much academic and industrial attraction to approach the future digital world. This paper introduces the advantages of deep reinforcement learning (DRL) in assisting Metaverse system-based Digital Twin. In this system, we assume that it includes several Metaverse User devices collecting data from the real world to transfer it into the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the processing of data, and an edge computing server that receives the offloading data from the MVAP. The proposed model works under a dynamic environment with various parameters changing over time. The experiment results show that our proposed DRL algorithm is suitable for offloading tasks to ensure the promptness of DT in a dynamic environment."
  },
  {
    "title": "From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV",
    "url": "http://arxiv.org/abs/2506.02649v1",
    "arxiv_id": "2506.02649v1",
    "authors": [
      "Yousef Emami",
      "Hao Zhou",
      "Miguel Gutierrez Gaitan",
      "Kai Li",
      "Luis Almeida",
      "Zhu Han"
    ],
    "published": "2025-06-03T09:01:33+00:00",
    "summary": "A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness in emergency response. Its agility and ability to optimize mobility and establish Line-of-Sight (LoS) communication make it increasingly vital for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. While Deep Reinforcement Learning (DRL) has been applied to optimize UAV navigation and control, its high training complexity, low sample efficiency, and simulation-to-reality gap limit its practicality in public safety. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation via natural language prompts and example-based guidance, without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, thereby making them suitable for real-time, mission-critical public safety UAVs. This paper proposes the integration of LLM-enabled ICL with public safety UAV to address the key functions, such as path planning and velocity control, in the context of emergency response. We present a case study on data collection scheduling where the LLM-enabled ICL framework can significantly reduce packet loss compared to conventional approaches, while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAV, thus offering a lightweight and efficient solution for enhancing UAV autonomy and responsiveness in emergencies."
  },
  {
    "title": "Compositional Learning for Modular Multi-Agent Self-Organizing Networks",
    "url": "http://arxiv.org/abs/2506.02616v1",
    "arxiv_id": "2506.02616v1",
    "authors": [
      "Qi Liao",
      "Parijat Bhattacharjee"
    ],
    "published": "2025-06-03T08:33:18+00:00",
    "summary": "Self-organizing networks face challenges from complex parameter interdependencies and conflicting objectives. This study introduces two compositional learning approaches-Compositional Deep Reinforcement Learning (CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their performance under training time and safety constraints in multi-agent systems. We propose a modular, two-tier framework with cell-level and cell-pair-level agents to manage heterogeneous agent granularities while reducing model complexity. Numerical simulations reveal a significant reduction in handover failures, along with improved throughput and latency, outperforming conventional multi-agent deep reinforcement learning approaches. The approach also demonstrates superior scalability, faster convergence, higher sample efficiency, and safer training in large-scale self-organizing networks."
  },
  {
    "title": "Learning-based primal-dual optimal control of discrete-time stochastic systems with multiplicative noise",
    "url": "http://arxiv.org/abs/2506.02613v1",
    "arxiv_id": "2506.02613v1",
    "authors": [
      "Xiushan Jiang",
      "Weihai Zhang"
    ],
    "published": "2025-06-03T08:30:13+00:00",
    "summary": "Reinforcement learning (RL) is an effective approach for solving optimal control problems without knowing the exact information of the system model. However, the classical Q-learning method, a model-free RL algorithm, has its limitations, such as lack of strict theoretical analysis and the need for artificial disturbances during implementation. This paper explores the partially model-free stochastic linear quadratic regulator (SLQR) problem for a system with multiplicative noise from the primal-dual perspective to address these challenges. This approach lays a strong theoretical foundation for understanding the intrinsic mechanisms of classical RL algorithms. We reformulate the SLQR into a non-convex primal-dual optimization problem and derive a strong duality result, which enables us to provide model-based and model-free algorithms for SLQR optimal policy design based on the Karush-Kuhn-Tucker (KKT) conditions. An illustrative example demonstrates the proposed model-free algorithm's validity, showcasing the central nervous system's learning mechanism in human arm movement."
  },
  {
    "title": "A Hybrid Approach to Indoor Social Navigation: Integrating Reactive Local Planning and Proactive Global Planning",
    "url": "http://arxiv.org/abs/2506.02593v1",
    "arxiv_id": "2506.02593v1",
    "authors": [
      "Arnab Debnath",
      "Gregory J. Stein",
      "Jana Kosecka"
    ],
    "published": "2025-06-03T08:12:55+00:00",
    "summary": "We consider the problem of indoor building-scale social navigation, where the robot must reach a point goal as quickly as possible without colliding with humans who are freely moving around. Factors such as varying crowd densities, unpredictable human behavior, and the constraints of indoor spaces add significant complexity to the navigation task, necessitating a more advanced approach. We propose a modular navigation framework that leverages the strengths of both classical methods and deep reinforcement learning (DRL). Our approach employs a global planner to generate waypoints, assigning soft costs around anticipated pedestrian locations, encouraging caution around potential future positions of humans. Simultaneously, the local planner, powered by DRL, follows these waypoints while avoiding collisions. The combination of these planners enables the agent to perform complex maneuvers and effectively navigate crowded and constrained environments while improving reliability. Many existing studies on social navigation are conducted in simplistic or open environments, limiting the ability of trained models to perform well in complex, real-world settings. To advance research in this area, we introduce a new 2D benchmark designed to facilitate development and testing of social navigation strategies in indoor environments. We benchmark our method against traditional and RL-based navigation strategies, demonstrating that our approach outperforms both."
  },
  {
    "title": "Reachability Weighted Offline Goal-conditioned Resampling",
    "url": "http://arxiv.org/abs/2506.02577v1",
    "arxiv_id": "2506.02577v1",
    "authors": [
      "Wenyan Yang",
      "Joni Pajarinen"
    ],
    "published": "2025-06-03T07:55:57+00:00",
    "summary": "Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets where many potential goals share the same state and action spaces. However, these potential goals are not explicitly represented in the collected trajectories. To learn a generalizable goal-conditioned policy, it is common to sample goals and state-action pairs uniformly using dynamic programming methods such as Q-learning. Uniform sampling, however, requires an intractably large dataset to cover all possible combinations and creates many unreachable state-goal-action pairs that degrade policy performance. Our key insight is that sampling should favor transitions that enable goal achievement. To this end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability classifier trained via positive-unlabeled (PU) learning on goal-conditioned state-action values. The classifier maps these values to a reachability score, which is then used as a sampling priority. RWS is a plug-and-play module that integrates seamlessly with standard offline RL algorithms. Experiments on six complex simulated robotic manipulation tasks, including those with a robot arm and a dexterous hand, show that RWS significantly improves performance. In one notable case, performance on the HandBlock-Z task improved by nearly 50 percent relative to the baseline. These results indicate the effectiveness of reachability-weighted sampling."
  },
  {
    "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective",
    "url": "http://arxiv.org/abs/2506.02553v1",
    "arxiv_id": "2506.02553v1",
    "authors": [
      "Shenghua He",
      "Tian Xia",
      "Xuan Zhou",
      "Hui Wei"
    ],
    "published": "2025-06-03T07:44:31+00:00",
    "summary": "We study a common challenge in reinforcement learning for large language models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e., intermediate token generations) receive zero task-specific immediate reward, while only the final token receives a reward for the entire response. This assumption arises frequently in practice, as precise token-level rewards are often difficult or infeasible to obtain in LLM applications. In this work, we provide a unifying theoretical perspective. We introduce the Trajectory Policy Gradient Theorem, which shows that the policy gradient based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model, regardless of whether the Zero-Reward Assumption holds or not, for algorithms in the REINFORCE and Actor-Critic families. This result reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals, offering a theoretical justification for response-level reward approaches. Our findings pave the way for more practical, efficient LLM fine-tuning, allowing developers to treat training algorithms as black boxes and focus on improving the response-level reward model with auxiliary sub-models. We also offer a detailed analysis of popular RL and non-RL methods, comparing their theoretical foundations and practical advantages across common LLM tasks. Finally, we propose a new algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically grounded method that is simpler than PPO, matches GRPO in memory efficiency, and holds promise for broad applicability."
  },
  {
    "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making",
    "url": "http://arxiv.org/abs/2506.02522v1",
    "arxiv_id": "2506.02522v1",
    "authors": [
      "Xu Wan",
      "Wenyue Xu",
      "Chao Yang",
      "Mingyang Sun"
    ],
    "published": "2025-06-03T06:52:37+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) and Reinforcement Learning (RL) have shown significant promise in decision-making tasks. Nevertheless, for large-scale industrial decision problems, both approaches face distinct challenges: LLMs lack real-time long-sequence decision-making capabilities, while RL struggles with sample efficiency in vast action spaces. To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic framework between LLMs and RL agents for large-scale decision-making scenarios. ACE introduces a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor and Value Critic during RL's training: the Actor refines suboptimal actions via multi-step reasoning and environment validation, while the Critic performs temporal credit assignment through trajectory-level reward shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making with high-quality fine-tuning datasets generated via prioritized experience replay. Through extensive experiments across multiple power grid operation challenges with action spaces exceeding 60K discrete actions, ACE demonstrates superior performance over existing RL methods and LLM-based methods."
  },
  {
    "title": "AURA: Agentic Upskilling via Reinforced Abstractions",
    "url": "http://arxiv.org/abs/2506.02507v1",
    "arxiv_id": "2506.02507v1",
    "authors": [
      "Alvin Zhu",
      "Yusuke Tanaka",
      "Dennis Hong"
    ],
    "published": "2025-06-03T06:37:34+00:00",
    "summary": "We study the combinatorial explosion involved in translating high-level task prompts into deployable control policies for agile robots through multi-stage reinforcement learning. We introduce AURA (Agentic Upskilling via Reinforced Abstractions), a schema-centric curriculum RL framework that leverages Large Language Models (LLMs) as autonomous designers of multi-stage curricula. AURA transforms user prompts into YAML workflows that encode full reward functions, domain randomization strategies, and training configurations. All files are statically validated against a schema before any GPU time is consumed, ensuring reliable and efficient execution without human intervention. A retrieval-augmented feedback loop allows specialized LLM agents to design, execute, and refine staged curricula based on prior training results stored in a vector database, supporting continual improvement over time. Ablation studies highlight the importance of retrieval for curriculum quality and convergence stability. Quantitative experiments show that AURA consistently outperforms LLM-guided baselines on GPU-accelerated training frameworks. In qualitative tests, AURA successfully trains end-to-end policies directly from user prompts and deploys them zero-shot on a custom humanoid robot across a range of environments. By abstracting away the complexity of curriculum design, AURA enables scalable and adaptive policy learning pipelines that would be prohibitively complex to construct by hand."
  },
  {
    "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text",
    "url": "http://arxiv.org/abs/2506.02494v1",
    "arxiv_id": "2506.02494v1",
    "authors": [
      "Junzhe Zhang",
      "Huixuan Zhang",
      "Xinyu Hu",
      "Li Lin",
      "Mingqi Gao",
      "Shi Qiu",
      "Xiaojun Wan"
    ],
    "published": "2025-06-03T06:17:16+00:00",
    "summary": "Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks."
  },
  {
    "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage",
    "url": "http://arxiv.org/abs/2506.02479v1",
    "arxiv_id": "2506.02479v1",
    "authors": [
      "Kalyan Nakka",
      "Nitesh Saxena"
    ],
    "published": "2025-06-03T05:51:18+00:00",
    "summary": "The inherent risk of generating harmful and unsafe content by Large Language Models (LLMs), has highlighted the need for their safety alignment. Various techniques like supervised fine-tuning, reinforcement learning from human feedback, and red-teaming were developed for ensuring the safety alignment of LLMs. However, the robustness of these aligned LLMs is always challenged by adversarial attacks that exploit unexplored and underlying vulnerabilities of the safety alignment. In this paper, we develop a novel black-box jailbreak attack, called BitBypass, that leverages hyphen-separated bitstream camouflage for jailbreaking aligned LLMs. This represents a new direction in jailbreaking by exploiting fundamental information representation of data as continuous bits, rather than leveraging prompt engineering or adversarial manipulations. Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude 3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the capabilities of BitBypass in bypassing their safety alignment and tricking them into generating harmful and unsafe content. Further, we observed that BitBypass outperforms several state-of-the-art jailbreak attacks in terms of stealthiness and attack success. Overall, these results highlights the effectiveness and efficiency of BitBypass in jailbreaking these state-of-the-art LLMs."
  },
  {
    "title": "A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization",
    "url": "http://arxiv.org/abs/2506.02458v1",
    "arxiv_id": "2506.02458v1",
    "authors": [
      "Nguyen Chi Long",
      "Trinh Van Chien",
      "Ta Hai Tung",
      "Van Son Nguyen",
      "Trong-Minh Hoang",
      "Nguyen Ngoc Hai Dang"
    ],
    "published": "2025-06-03T05:22:02+00:00",
    "summary": "Mobile edge computing (MEC) allows appliances to offload workloads to neighboring MEC servers that have the potential for computation-intensive tasks with limited computational capabilities. This paper studied how deep reinforcement learning (DRL) algorithms are used in an MEC system to find feasible decentralized dynamic computation offloading strategies, which leads to the construction of an extensible MEC system that operates effectively with finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG) algorithm, subject to their knowledge of the MEC system, can be used to allocate powers of both computation offloading and local execution, to learn a computation offloading policy for each user independently, we realized that this solution still has some inherent weaknesses. Hence, we introduced a new approach for this problem based on the Twin Delayed DDPG algorithm, which enables us to overcome this proneness and investigate cases where mobile users are portable. Numerical results showed that individual users can autonomously learn adequate policies through the proposed approach. Besides, the performance of the suggested solution exceeded the conventional DDPG-based power control strategy."
  },
  {
    "title": "Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments",
    "url": "http://arxiv.org/abs/2506.02394v1",
    "arxiv_id": "2506.02394v1",
    "authors": [
      "Yuan Bian",
      "Xingche Guo",
      "Yuanjia Wang"
    ],
    "published": "2025-06-03T03:21:10+00:00",
    "summary": "Major depressive disorder (MDD), a leading cause of disability and mortality, is associated with reward-processing abnormalities and concentration issues. Motivated by the probabilistic reward task from the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel framework that integrates the reinforcement learning (RL) model and drift-diffusion model (DDM) to jointly analyze reward-based decision-making with response times. To account for emerging evidence suggesting that decision-making may alternate between multiple interleaved strategies, we model latent state switching using a hidden Markov model (HMM). In the ''engaged'' state, decisions follow an RL-DDM, simultaneously capturing reward processing, decision dynamics, and temporal structure. In contrast, in the ''lapsed'' state, decision-making is modeled using a simplified DDM, where specific parameters are fixed to approximate random guessing with equal probability. The proposed method is implemented using a computationally efficient generalized expectation-maximization algorithm with forward-backward procedures. Through extensive numerical studies, we demonstrate that our proposed method outperforms competing approaches under various reward-generating distributions, both with and without strategy switching. When applied to the EMBARC study, our framework reveals that MDD patients exhibit lower overall engagement than healthy controls and experience longer decision times when they do engage. Additionally, we show that neuroimaging measures of brain activities are associated with decision-making characteristics in the ''engaged'' state but not in the ''lapsed'' state, providing evidence of brain-behavioral association specific to the ''engaged'' state."
  },
  {
    "title": "Multi-agent Markov Entanglement",
    "url": "http://arxiv.org/abs/2506.02385v1",
    "arxiv_id": "2506.02385v1",
    "authors": [
      "Shuze Chen",
      "Tianyi Peng"
    ],
    "published": "2025-06-03T02:54:25+00:00",
    "summary": "Value decomposition has long been a fundamental technique in multi-agent dynamic programming and reinforcement learning (RL). Specifically, the value function of a global state $(s_1,s_2,\\ldots,s_N)$ is often approximated as the sum of local functions: $V(s_1,s_2,\\ldots,s_N)\\approx\\sum_{i=1}^N V_i(s_i)$. This approach traces back to the index policy in restless multi-armed bandit problems and has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored.   In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a multi-agent Markov decision process (MDP) permits value decomposition if and only if its transition matrix is not \"entangled\" -- a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the \"Markov entanglement\" for multi-agent MDPs and show that this measure can be used to bound the decomposition error in general multi-agent MDPs.   Using the concept of Markov entanglement, we proved that a widely-used class of index policies is weakly entangled and enjoys a sublinear $\\mathcal O(\\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we show how Markov entanglement can be efficiently estimated in practice, providing practitioners with an empirical proxy for the quality of value decomposition."
  },
  {
    "title": "Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening",
    "url": "http://arxiv.org/abs/2506.02355v1",
    "arxiv_id": "2506.02355v1",
    "authors": [
      "Andre He",
      "Daniel Fried",
      "Sean Welleck"
    ],
    "published": "2025-06-03T01:15:15+00:00",
    "summary": "Reinforcement learning has emerged as an effective framework for training large language models on structured language-conditioned tasks. We identify a critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL algorithm in this setting. For tasks that require multi-sample performance, such as formal theorem proving, GRPO biasedly reinforces already probable solutions and neglects rare but correct proofs. This implicit bias impairs performance on pass@$N$ metrics at large sample sizes, limiting its practicality for training theorem provers. To address this, we introduce the unlikeliness reward, a straightforward method that explicitly encourages reinforcing rare correct solutions. Additionally, we find that increasing the number of PPO epochs further mitigates this bias. Our experiments confirm that incorporating the unlikeliness reward significantly improves pass@$N$ across a large range of N, outperforming standard GRPO and substantially increasing sample diversity. Applying our revised recipe to Lean, we achieve competitive performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation, providing a simple yet effective recipe for training formal theorem provers with RL."
  },
  {
    "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL",
    "url": "http://arxiv.org/abs/2506.02338v1",
    "arxiv_id": "2506.02338v1",
    "authors": [
      "Hyungjoo Chae",
      "Dongjin Kang",
      "Jihyuk Kim",
      "Beong-woo Kwak",
      "Sunghyun Park",
      "Haeju Park",
      "Jinyoung Yeo",
      "Moontae Lee",
      "Kyungjae Lee"
    ],
    "published": "2025-06-03T00:29:15+00:00",
    "summary": "With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR."
  },
  {
    "title": "Efficient Manipulation-Enhanced Semantic Mapping With Uncertainty-Informed Action Selection",
    "url": "http://arxiv.org/abs/2506.02286v1",
    "arxiv_id": "2506.02286v1",
    "authors": [
      "Nils Dengler",
      "Jesper M\u00fccke",
      "Rohit Menon",
      "Maren Bennewitz"
    ],
    "published": "2025-06-02T21:57:53+00:00",
    "summary": "Service robots operating in cluttered human environments such as homes, offices, and schools cannot rely on predefined object arrangements and must continuously update their semantic and spatial estimates while dealing with possible frequent rearrangements. Efficient and accurate mapping under such conditions demands selecting informative viewpoints and targeted manipulations to reduce occlusions and uncertainty. In this work, we present a manipulation-enhanced semantic mapping framework for occlusion-heavy shelf scenes that integrates evidential metric-semantic mapping with reinforcement-learning-based next-best view planning and targeted action selection. Our method thereby exploits uncertainty estimates from the Dirichlet and Beta distributions in the semantic and occupancy prediction networks to guide both active sensor placement and object manipulation, focusing on areas of limited knowledge and selecting actions with high expected information gain. For object manipulation, we introduce an uncertainty-informed push strategy that targets occlusion-critical objects and generates minimally invasive actions to reveal hidden regions. The experimental evaluation shows that our framework highly reduces object displacement and drops while achieving a 95% reduction in planning time compared to the state-of-the-art, thereby realizing real-world applicability."
  },
  {
    "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals",
    "url": "http://arxiv.org/abs/2506.02281v1",
    "arxiv_id": "2506.02281v1",
    "authors": [
      "Qinsi Wang",
      "Jinghan Ke",
      "Hancheng Ye",
      "Yueqian Lin",
      "Yuzhe Fu",
      "Jianyi Zhang",
      "Kurt Keutzer",
      "Chenfeng Xu",
      "Yiran Chen"
    ],
    "published": "2025-06-02T21:40:38+00:00",
    "summary": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main."
  },
  {
    "title": "SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems",
    "url": "http://arxiv.org/abs/2506.02255v1",
    "arxiv_id": "2506.02255v1",
    "authors": [
      "Asha Ramanujam",
      "Adam Elyoumi",
      "Hao Chen",
      "Sai Madhukiran Kompalli",
      "Akshdeep Singh Ahluwalia",
      "Shraman Pal",
      "Dimitri J. Papageorgiou",
      "Can Li"
    ],
    "published": "2025-06-02T20:59:45+00:00",
    "summary": "Most existing safe reinforcement learning (RL) benchmarks focus on robotics and control tasks, offering limited relevance to high-stakes domains that involve structured constraints, mixed-integer decisions, and industrial complexity. This gap hinders the advancement and deployment of safe RL in critical areas such as energy systems, manufacturing, and supply chains. To address this limitation, we present SafeOR-Gym, a benchmark suite of nine operations research (OR) environments tailored for safe RL under complex constraints. Each environment captures a realistic planning, scheduling, or control problems characterized by cost-based constraint violations, planning horizons, and hybrid discrete-continuous action spaces. The suite integrates seamlessly with the Constrained Markov Decision Process (CMDP) interface provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms across these environments, revealing a wide range of performance: while some tasks are tractable, others expose fundamental limitations in current approaches. SafeOR-Gym provides a challenging and practical testbed that aims to catalyze future research in safe RL for real-world decision-making problems. The SafeOR-Gym framework and all accompanying code are available at: https://github.com/li-group/SafeOR-Gym."
  },
  {
    "title": "Improving LLM-Generated Code Quality with GRPO",
    "url": "http://arxiv.org/abs/2506.02211v1",
    "arxiv_id": "2506.02211v1",
    "authors": [
      "Maxime Robeyns",
      "Laurence Aitchison"
    ],
    "published": "2025-06-02T19:50:16+00:00",
    "summary": "Large Language Models (LLMs) are gaining widespread use for code generation. Recent training procedures use execution feedback as a reward signal, typically focusing on the functional correctness of the code, using unit test pass rate as a reward signal. However, this reward signal fails to capture notions of maintainability, quality and safety of the code produced. We address this under-explored area and develop a comprehensive library to quantify various aspects of code quality, and use it as a reward in GRPO. We find GRPO increases code quality according to this measure, which is confirmed by expert, blinded human annotators."
  },
  {
    "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2506.02208v1",
    "arxiv_id": "2506.02208v1",
    "authors": [
      "Hongling Xu",
      "Qi Zhu",
      "Heyuan Deng",
      "Jinpeng Li",
      "Lu Hou",
      "Yasheng Wang",
      "Lifeng Shang",
      "Ruifeng Xu",
      "Fei Mi"
    ],
    "published": "2025-06-02T19:46:41+00:00",
    "summary": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs."
  },
  {
    "title": "Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation",
    "url": "http://arxiv.org/abs/2506.02206v1",
    "arxiv_id": "2506.02206v1",
    "authors": [
      "Chengyang Peng",
      "Zhihao Zhang",
      "Shiting Gong",
      "Sankalp Agrawal",
      "Keith A. Redmill",
      "Ayonga Hereid"
    ],
    "published": "2025-06-02T19:45:29+00:00",
    "summary": "Safe and real-time navigation is fundamental for humanoid robot applications. However, existing bipedal robot navigation frameworks often struggle to balance computational efficiency with the precision required for stable locomotion. We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments. Our method comprises a high-level reinforcement learning (RL) planner for subgoal selection in a robot-centric coordinate system and a low-level Model Predictive Control (MPC) based planner which produces robust walking gaits to reach these subgoals. To expedite and stabilize the training process, we incorporate a data bootstrapping technique that leverages a model-based navigation approach to generate a diverse, informative dataset. We validate our method in simulation using the Agility Robotics Digit humanoid across multiple scenarios with random obstacles. Results show that our framework significantly improves navigation success rates and adaptability compared to both the original model-based method and other learning-based methods."
  },
  {
    "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL",
    "url": "http://arxiv.org/abs/2505.24875v1",
    "arxiv_id": "2505.24875v1",
    "authors": [
      "Yu Zhang",
      "Yunqi Li",
      "Yifan Yang",
      "Rui Wang",
      "Yuqing Yang",
      "Dai Qi",
      "Jianmin Bao",
      "Dongdong Chen",
      "Chong Luo",
      "Lili Qiu"
    ],
    "published": "2025-05-30T17:59:48+00:00",
    "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based \"thinking\" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen."
  },
  {
    "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners",
    "url": "http://arxiv.org/abs/2505.24872v1",
    "arxiv_id": "2505.24872v1",
    "authors": [
      "Zilin Xiao",
      "Jaywon Koo",
      "Siru Ouyang",
      "Jefferson Hernandez",
      "Yu Meng",
      "Vicente Ordonez"
    ],
    "published": "2025-05-30T17:59:43+00:00",
    "summary": "Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at https://github.com/MrZilinXiao/ProxyThinker."
  },
  {
    "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24871v1",
    "arxiv_id": "2505.24871v1",
    "authors": [
      "Yiqing Liang",
      "Jielin Qiu",
      "Wenhao Ding",
      "Zuxin Liu",
      "James Tompkin",
      "Mengdi Xu",
      "Mengzhou Xia",
      "Zhengzhong Tu",
      "Laixi Shi",
      "Jiacheng Zhu"
    ],
    "published": "2025-05-30T17:59:38+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline."
  },
  {
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "url": "http://arxiv.org/abs/2505.24864v1",
    "arxiv_id": "2505.24864v1",
    "authors": [
      "Mingjie Liu",
      "Shizhe Diao",
      "Ximing Lu",
      "Jian Hu",
      "Xin Dong",
      "Yejin Choi",
      "Jan Kautz",
      "Yi Dong"
    ],
    "published": "2025-05-30T17:59:01+00:00",
    "summary": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"
  },
  {
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.24850v1",
    "arxiv_id": "2505.24850v1",
    "authors": [
      "Shuyao Xu",
      "Cheng Peng",
      "Jiangxuan Long",
      "Weidi Xu",
      "Wei Chu",
      "Yuan Qi"
    ],
    "published": "2025-05-30T17:47:17+00:00",
    "summary": "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data."
  },
  {
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning",
    "url": "http://arxiv.org/abs/2505.24846v1",
    "arxiv_id": "2505.24846v1",
    "authors": [
      "Jingyan Shen",
      "Jiarui Yao",
      "Rui Yang",
      "Yifan Sun",
      "Feng Luo",
      "Rui Pan",
      "Tong Zhang",
      "Han Zhao"
    ],
    "published": "2025-05-30T17:44:28+00:00",
    "summary": "Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization."
  },
  {
    "title": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models",
    "url": "http://arxiv.org/abs/2505.24784v1",
    "arxiv_id": "2505.24784v1",
    "authors": [
      "Conor Heins",
      "Toon Van de Maele",
      "Alexander Tschantz",
      "Hampus Linander",
      "Dimitrije Markovic",
      "Tommaso Salvatori",
      "Corrado Pezzato",
      "Ozan Catal",
      "Ran Wei",
      "Magnus Koudahl",
      "Marco Perin",
      "Karl Friston",
      "Tim Verbelen",
      "Christopher Buckley"
    ],
    "published": "2025-05-30T16:46:20+00:00",
    "summary": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization."
  },
  {
    "title": "Diffusion-Based Symbolic Regression",
    "url": "http://arxiv.org/abs/2505.24776v1",
    "arxiv_id": "2505.24776v1",
    "authors": [
      "Zachary Bastiani",
      "Robert M. Kirby",
      "Jacob Hochhalter",
      "Shandian Zhe"
    ],
    "published": "2025-05-30T16:39:29+00:00",
    "summary": "Diffusion has emerged as a powerful framework for generative modeling, achieving remarkable success in applications such as image and audio synthesis. Enlightened by this progress, we propose a novel diffusion-based approach for symbolic regression. We construct a random mask-based diffusion and denoising process to generate diverse and high-quality equations. We integrate this generative processes with a token-wise Group Relative Policy Optimization (GRPO) method to conduct efficient reinforcement learning on the given measurement dataset. In addition, we introduce a long short-term risk-seeking policy to expand the pool of top-performing candidates, further enhancing performance. Extensive experiments and ablation studies have demonstrated the effectiveness of our approach."
  },
  {
    "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards",
    "url": "http://arxiv.org/abs/2505.24760v1",
    "arxiv_id": "2505.24760v1",
    "authors": [
      "Zafir Stojanovski",
      "Oliver Stanley",
      "Joe Sharratt",
      "Richard Jones",
      "Abdulhakeem Adefioye",
      "Jean Kaddour",
      "Andreas K\u00f6pf"
    ],
    "published": "2025-05-30T16:20:18+00:00",
    "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models."
  },
  {
    "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24726v1",
    "arxiv_id": "2505.24726v1",
    "authors": [
      "Shelly Bensal",
      "Umar Jamil",
      "Christopher Bryant",
      "Melisa Russak",
      "Kiran Kamble",
      "Dmytro Mozolevskyi",
      "Muayad Ali",
      "Waseem AlShikh"
    ],
    "published": "2025-05-30T15:49:42+00:00",
    "summary": "We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback."
  },
  {
    "title": "Reinforcing Video Reasoning with Focused Thinking",
    "url": "http://arxiv.org/abs/2505.24718v1",
    "arxiv_id": "2505.24718v1",
    "authors": [
      "Jisheng Dang",
      "Jingze Wu",
      "Teng Wang",
      "Xuanhui Lin",
      "Nannan Zhu",
      "Hongbo Chen",
      "Wei-Shi Zheng",
      "Meng Wang",
      "Tat-Seng Chua"
    ],
    "published": "2025-05-30T15:42:19+00:00",
    "summary": "Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group variance), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over Video-R1) and 65.8\\% on MMVU. Our codes are available at \\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}."
  },
  {
    "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting",
    "url": "http://arxiv.org/abs/2505.24710v1",
    "arxiv_id": "2505.24710v1",
    "authors": [
      "Wei Chen",
      "Jiahao Zhang",
      "Haipeng Zhu",
      "Boyan Xu",
      "Zhifeng Hao",
      "Keli Zhang",
      "Junjian Ye",
      "Ruichu Cai"
    ],
    "published": "2025-05-30T15:30:44+00:00",
    "summary": "Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting\" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter\" validate the effectiveness of our proposed method."
  },
  {
    "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences",
    "url": "http://arxiv.org/abs/2505.24709v1",
    "arxiv_id": "2505.24709v1",
    "authors": [
      "Soichiro Nishimori",
      "Yu-Jie Zhang",
      "Thanawat Lodkaew",
      "Masashi Sugiyama"
    ],
    "published": "2025-05-30T15:30:43+00:00",
    "summary": "Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO."
  },
  {
    "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.24630v1",
    "arxiv_id": "2505.24630v1",
    "authors": [
      "Junyi Li",
      "Hwee Tou Ng"
    ],
    "published": "2025-05-30T14:23:32+00:00",
    "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance."
  },
  {
    "title": "Distributed Intelligence in the Computing Continuum with Active Inference",
    "url": "http://arxiv.org/abs/2505.24618v1",
    "arxiv_id": "2505.24618v1",
    "authors": [
      "Victor Casamayor Pujol",
      "Boris Sedlak",
      "Tommaso Salvatori",
      "Karl Friston",
      "Schahram Dustdar"
    ],
    "published": "2025-05-30T14:10:33+00:00",
    "summary": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm that spans from local Internet of Things sensors and constrained edge devices to large-scale cloud data centers. Its goal is to orchestrate a vast array of diverse and distributed computing resources to support the next generation of Internet-based applications. However, the distributed, heterogeneous, and dynamic nature of CC platforms demands distributed intelligence for adaptive and resilient service management. This article introduces a distributed stream processing pipeline as a CC use case, where each service is managed by an Active Inference (AIF) agent. These agents collaborate to fulfill service needs specified by SLOiDs, a term we introduce to denote Service Level Objectives that are aware of its deployed devices, meaning that non-functional requirements must consider the characteristics of the hosting device. We demonstrate how AIF agents can be modeled and deployed alongside distributed services to manage them autonomously. Our experiments show that AIF agents achieve over 90% SLOiD fulfillment when using tested transition models, and around 80% when learning the models during deployment. We compare their performance to a multi-agent reinforcement learning algorithm, finding that while both approaches yield similar results, MARL requires extensive training, whereas AIF agents can operate effectively from the start. Additionally, we evaluate the behavior of AIF agents in offloading scenarios, observing a strong capacity for adaptation. Finally, we outline key research directions to advance AIF integration in CC platforms."
  },
  {
    "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams",
    "url": "http://arxiv.org/abs/2505.24584v1",
    "arxiv_id": "2505.24584v1",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Shivam Gupta",
      "Venkataramana Runkana"
    ],
    "published": "2025-05-30T13:32:00+00:00",
    "summary": "Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment."
  },
  {
    "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams",
    "url": "http://arxiv.org/abs/2505.24584v2",
    "arxiv_id": "2505.24584v2",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Shivam Gupta",
      "Venkataramana Runkana"
    ],
    "published": "2025-05-30T13:32:00+00:00",
    "summary": "Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&D timelines from lab discovery to plant deployment."
  },
  {
    "title": "Fine-tuning for Data-enabled Predictive Control of Noisy Systems by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24572v1",
    "arxiv_id": "2505.24572v1",
    "authors": [
      "Jinbao Wang",
      "Shiliang Zhang",
      "Jun Liu",
      "Xuehui Ma",
      "Haolin Liu"
    ],
    "published": "2025-05-30T13:24:58+00:00",
    "summary": "Data-enabled predictive control (DeePC) leverages system measurements in characterizing system dynamics for optimal control. The performance of DeePC relies on optimizing its hyperparameters, especially in noisy systems where the optimal hyperparameters adapt over time. Existing hyperparameter tuning approaches for DeePC are more than often computationally inefficient or overly conservative. This paper proposes an adaptive DeePC where we guide its hyperparameters adaption through reinforcement learning. We start with establishing the relationship between the system I/O behavior and DeePC hyperparameters. Then we formulate the hyperparameter tuning as a sequential decision-making problem, and we address the decision-making through reinforcement learning. We implement offline training to gain a reinforcement learning model, and we integrate the trained model with DeePC to adjust its hyperparameters adaptively in real time. We conduct numerical simulations with diverse noisy conditions, and the results demonstrate the identification of near-optimal hyperparameters and the robustness of the proposed approach against noises in the control."
  },
  {
    "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors",
    "url": "http://arxiv.org/abs/2505.24523v1",
    "arxiv_id": "2505.24523v1",
    "authors": [
      "Andrea Pedrotti",
      "Michele Papucci",
      "Cristiano Ciaccio",
      "Alessio Miaschi",
      "Giovanni Puccetti",
      "Felice Dell'Orletta",
      "Andrea Esuli"
    ],
    "published": "2025-05-30T12:33:30+00:00",
    "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts."
  },
  {
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence",
    "url": "http://arxiv.org/abs/2505.24500v1",
    "arxiv_id": "2505.24500v1",
    "authors": [
      "Guiyang Hou",
      "Xing Gao",
      "Yuchuan Wu",
      "Xiang Huang",
      "Wenqi Zhang",
      "Zhe Zheng",
      "Yongliang Shen",
      "Jialu Du",
      "Fei Huang",
      "Yongbin Li",
      "Weiming Lu"
    ],
    "published": "2025-05-30T12:01:06+00:00",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights."
  },
  {
    "title": "Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation",
    "url": "http://arxiv.org/abs/2505.24499v1",
    "arxiv_id": "2505.24499v1",
    "authors": [
      "Ximing Xing",
      "Yandong Guan",
      "Jing Zhang",
      "Dong Xu",
      "Qian Yu"
    ],
    "published": "2025-05-30T11:57:58+00:00",
    "summary": "Generating high-quality Scalable Vector Graphics (SVGs) is challenging for Large Language Models (LLMs), as it requires advanced reasoning for structural validity, semantic faithfulness, and visual coherence -- capabilities in which current LLMs often fall short. In this work, we introduce Reason-SVG, a novel framework designed to enhance LLM reasoning for SVG generation. Reason-SVG pioneers the \"Drawing-with-Thought\" (DwT) paradigm, in which models generate both SVG code and explicit design rationales, mimicking the human creative process. Reason-SVG adopts a two-stage training strategy: First, Supervised Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group Relative Policy Optimization (GRPO), empowers the model to generate both DwT and SVGs rationales through refined, reward-driven reasoning. To facilitate reasoning-driven SVG generation, we design a Hybrid Reward function that evaluates the presence and utility of DwT reasoning, along with structural validity, semantic alignment, and visual quality. We also introduce the SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT, and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance in generating accurate and visually compelling SVGs, potentially fostering \"Aha moments\" in design."
  },
  {
    "title": "Towards Effective Code-Integrated Reasoning",
    "url": "http://arxiv.org/abs/2505.24480v1",
    "arxiv_id": "2505.24480v1",
    "authors": [
      "Fei Bai",
      "Yingqian Min",
      "Beichen Zhang",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Lei Fang",
      "Zheng Liu",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-30T11:30:18+00:00",
    "summary": "In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR."
  },
  {
    "title": "Reactive Aerobatic Flight via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24396v1",
    "arxiv_id": "2505.24396v1",
    "authors": [
      "Zhichao Han",
      "Xijie Huang",
      "Zhuxiu Xu",
      "Jiarui Zhang",
      "Yuze Wu",
      "Mingyang Wang",
      "Tianyue Wu",
      "Fei Gao"
    ],
    "published": "2025-05-30T09:24:30+00:00",
    "summary": "Quadrotors have demonstrated remarkable versatility, yet their full aerobatic potential remains largely untapped due to inherent underactuation and the complexity of aggressive maneuvers. Traditional approaches, separating trajectory optimization and tracking control, suffer from tracking inaccuracies, computational latency, and sensitivity to initial conditions, limiting their effectiveness in dynamic, high-agility scenarios. Inspired by recent breakthroughs in data-driven methods, we propose a reinforcement learning-based framework that directly maps drone states and aerobatic intentions to control commands, eliminating modular separation to enable quadrotors to perform end-to-end policy optimization for extreme aerobatic maneuvers. To ensure efficient and stable training, we introduce an automated curriculum learning strategy that dynamically adjusts aerobatic task difficulty. Enabled by domain randomization for robust zero-shot sim-to-real transfer, our approach is validated in demanding real-world experiments, including the first demonstration of a drone autonomously performing continuous inverted flight while reactively navigating a moving gate, showcasing unprecedented agility."
  },
  {
    "title": "MagicGripper: A Multimodal Sensor-Integrated Gripper for Contact-Rich Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.24382v1",
    "arxiv_id": "2505.24382v1",
    "authors": [
      "Wen Fan",
      "Haoran Li",
      "Dandan Zhang"
    ],
    "published": "2025-05-30T09:10:31+00:00",
    "summary": "Contact-rich manipulation in unstructured environments demands precise, multimodal perception to enable robust and adaptive control. Vision-based tactile sensors (VBTSs) have emerged as an effective solution; however, conventional VBTSs often face challenges in achieving compact, multi-modal functionality due to hardware constraints and algorithmic complexity. In this work, we present MagicGripper, a multimodal sensor-integrated gripper designed for contact-rich robotic manipulation. Building on our prior design, MagicTac, we develop a compact variant, mini-MagicTac, which features a three-dimensional, multi-layered grid embedded in a soft elastomer. MagicGripper integrates mini-MagicTac, enabling high-resolution tactile feedback alongside proximity and visual sensing within a compact, gripper-compatible form factor. We conduct a thorough evaluation of mini-MagicTac's performance, demonstrating its capabilities in spatial resolution, contact localization, and force regression. We also assess its robustness across manufacturing variability, mechanical deformation, and sensing performance under real-world conditions. Furthermore, we validate the effectiveness of MagicGripper through three representative robotic tasks: a teleoperated assembly task, a contact-based alignment task, and an autonomous robotic grasping task. Across these experiments, MagicGripper exhibits reliable multimodal perception, accurate force estimation, and high adaptability to challenging manipulation scenarios. Our results highlight the potential of MagicGripper as a practical and versatile tool for embodied intelligence in complex, contact-rich environments."
  },
  {
    "title": "Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer",
    "url": "http://arxiv.org/abs/2505.24378v1",
    "arxiv_id": "2505.24378v1",
    "authors": [
      "Yilun Kong",
      "Guozheng Ma",
      "Qi Zhao",
      "Haoyu Wang",
      "Li Shen",
      "Xueqian Wang",
      "Dacheng Tao"
    ],
    "published": "2025-05-30T09:08:52+00:00",
    "summary": "Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance."
  },
  {
    "title": "Adversarial Preference Learning for Robust LLM Alignment",
    "url": "http://arxiv.org/abs/2505.24369v1",
    "arxiv_id": "2505.24369v1",
    "authors": [
      "Yuanfu Wang",
      "Pengyu Wang",
      "Chenyang Xi",
      "Bo Tang",
      "Junyi Zhu",
      "Wenqiang Wei",
      "Chen Chen",
      "Chao Yang",
      "Jingfeng Zhang",
      "Chaochao Lu",
      "Yijun Niu",
      "Keming Mao",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Jie Hu",
      "Mingchuan Yang"
    ],
    "published": "2025-05-30T09:02:07+00:00",
    "summary": "Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model."
  },
  {
    "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion",
    "url": "http://arxiv.org/abs/2505.24362v1",
    "arxiv_id": "2505.24362v1",
    "authors": [
      "Anum Afzal",
      "Florian Matthes",
      "Gal Chechik",
      "Yftah Ziser"
    ],
    "published": "2025-05-30T08:54:28+00:00",
    "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \\emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.\\footnote{Code and data is available at \\href{https://github.com/anum94/CoTpred}{\\texttt{github.com/anum94/CoTpred}}."
  },
  {
    "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion",
    "url": "http://arxiv.org/abs/2505.24362v2",
    "arxiv_id": "2505.24362v2",
    "authors": [
      "Anum Afzal",
      "Florian Matthes",
      "Gal Chechik",
      "Yftah Ziser"
    ],
    "published": "2025-05-30T08:54:28+00:00",
    "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \\emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits."
  },
  {
    "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24332v1",
    "arxiv_id": "2505.24332v1",
    "authors": [
      "Wenxuan Shi",
      "Haochen Tan",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Xiaozhe Ren",
      "Chen Zhang",
      "Hanting Chen",
      "Yasheng Wang",
      "Lifeng Shang",
      "Fisher Yu",
      "Yunhe Wang"
    ],
    "published": "2025-05-30T08:15:39+00:00",
    "summary": "Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing methods rely on static prompting rules or training with Wikipedia-based corpora and retrieval environments, limiting adaptability to the real-world web environment where ambiguity, conflicting evidence, and noise are prevalent. These constrained training settings hinder LLMs from learning to dynamically decide when and where to search, and how to adjust search depth and frequency based on informational demands. We define this missing capacity as Search Intensity Scaling (SIS)--the emergent skill to intensify search efforts under ambiguous or conflicting conditions, rather than settling on overconfident, under-verification answers.   To study SIS, we introduce WebPuzzle, the first dataset designed to foster information-seeking behavior in open-world internet environments. WebPuzzle consists of 24K training instances and 275 test questions spanning both wiki-based and open-web queries. Building on this dataset, we propose DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by encouraging adaptive search policies through exploration under a real-world open-web environment. Experimental results show that Pangu-7B-Reasoner empowered by DeepDiver achieve performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from cold-start supervised fine-tuning to a carefully designed RL phase, and present that its capability of SIS generalizes from closed-form QA to open-ended tasks such as long-form writing. Our contributions advance adaptive information seeking in LLMs and provide a valuable benchmark and dataset for future research."
  },
  {
    "title": "ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.24317v1",
    "arxiv_id": "2505.24317v1",
    "authors": [
      "Yongming Chen",
      "Miner Chen",
      "Liewen Liao",
      "Mingyang Jiang",
      "Xiang Zuo",
      "Hengrui Zhang",
      "Yuchen Xi",
      "Songan Zhang"
    ],
    "published": "2025-05-30T08:00:51+00:00",
    "summary": "Reinforcement learning (RL) in autonomous driving employs a trial-and-error mechanism, enhancing robustness in unpredictable environments. However, crafting effective reward functions remains challenging, as conventional approaches rely heavily on manual design and demonstrate limited efficacy in complex scenarios. To address this issue, this study introduces a responsibility-oriented reward function that explicitly incorporates traffic regulations into the RL framework. Specifically, we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment. This integration guides agents to adhere strictly to traffic laws, thus minimizing rule violations and optimizing decision-making performance in diverse driving conditions. Experimental validations demonstrate that the proposed methodology significantly improves the accuracy of assigning accident responsibilities and effectively reduces the agent's liability in traffic incidents."
  },
  {
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "url": "http://arxiv.org/abs/2505.24298v1",
    "arxiv_id": "2505.24298v1",
    "authors": [
      "Wei Fu",
      "Jiaxuan Gao",
      "Xujie Shen",
      "Chen Zhu",
      "Zhiyu Mei",
      "Chuyi He",
      "Shusheng Xu",
      "Guo Wei",
      "Jun Mei",
      "Jiashu Wang",
      "Tongkai Yang",
      "Binhang Yuan",
      "Yi Wu"
    ],
    "published": "2025-05-30T07:18:25+00:00",
    "summary": "Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \\emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \\textbf{up to 2.57$\\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/."
  },
  {
    "title": "EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding",
    "url": "http://arxiv.org/abs/2505.24287v1",
    "arxiv_id": "2505.24287v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Arda Mamur",
      "Felix Tristram",
      "Chantal Pellegrini",
      "Magdalena Wysocki",
      "Benjamin Busam",
      "Nassir Navab"
    ],
    "published": "2025-05-30T07:02:00+00:00",
    "summary": "Operating rooms (ORs) demand precise coordination among surgeons, nurses, and equipment in a fast-paced, occlusion-heavy environment, necessitating advanced perception models to enhance safety and efficiency. Existing datasets either provide partial egocentric views or sparse exocentric multi-view context, but do not explore the comprehensive combination of both. We introduce EgoExOR, the first OR dataset and accompanying benchmark to fuse first-person and third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D cameras, and ultrasound imagery. Its detailed scene graph annotations, covering 36 entities and 22 relations (568,235 triplets), enable robust modeling of clinical interactions, supporting tasks like action recognition and human-centric perception. We evaluate the surgical scene graph generation performance of two adapted state-of-the-art models and offer a new baseline that explicitly leverages EgoExOR's multimodal and multi-perspective signals. This new dataset and benchmark set a new foundation for OR perception, offering a rich, multimodal resource for next-generation clinical perception."
  },
  {
    "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.24273v1",
    "arxiv_id": "2505.24273v1",
    "authors": [
      "Hongyi James Cai",
      "Junlin Wang",
      "Xiaoyin Chen",
      "Bhuwan Dhingra"
    ],
    "published": "2025-05-30T06:49:00+00:00",
    "summary": "Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs."
  },
  {
    "title": "R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24265v1",
    "arxiv_id": "2505.24265v1",
    "authors": [
      "Harsh Goel",
      "Mohammad Omama",
      "Behdad Chalaki",
      "Vaishnav Tadiparthi",
      "Ehsan Moradi Pari",
      "Sandeep Chinchali"
    ],
    "published": "2025-05-30T06:40:19+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agent's past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agent's role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agents' roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%."
  },
  {
    "title": "A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming",
    "url": "http://arxiv.org/abs/2505.24252v1",
    "arxiv_id": "2505.24252v1",
    "authors": [
      "Yizhong Ding"
    ],
    "published": "2025-05-30T06:16:42+00:00",
    "summary": "Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven \\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as \"chosen\" data and benign code as \"rejected\" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness."
  },
  {
    "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search",
    "url": "http://arxiv.org/abs/2505.24251v1",
    "arxiv_id": "2505.24251v1",
    "authors": [
      "Xiaoyu Li",
      "Xiao Li",
      "Li Gao",
      "Yiding Liu",
      "Xiaoyang Wang",
      "Shuaiqiang Wang",
      "Junfeng Wang",
      "Dawei Yin"
    ],
    "published": "2025-05-30T06:16:30+00:00",
    "summary": "The evolution of Large Language Models (LLMs) has significantly advanced multi-turn conversation systems, emphasizing the need for proactive guidance to enhance users' interactions. However, these systems face challenges in dynamically adapting to shifts in users' goals and maintaining low latency for real-time interactions. In the Baidu Search AI assistant, an industrial-scale multi-turn search system, we propose a novel two-phase framework to provide proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning (G-SFT), employs a goal adaptation agent that dynamically adapts to user goal shifts and provides goal-relevant contextual information. G-SFT also incorporates scalable knowledge transfer to distill insights from LLMs into a lightweight model for real-time interaction. The second phase, Click-oriented Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically constructs preference pairs from user click signals, and proactively improves click-through rates through more engaging guidance. This dual-phase architecture achieves complementary objectives: G-SFT ensures accurate goal tracking, while C-RL optimizes interaction quality through click signal-driven reinforcement learning. Extensive experiments demonstrate that our framework achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and 25.28% CTR in online deployment (149.06% relative improvement), while reducing inference latency by 69.55% through scalable knowledge distillation."
  },
  {
    "title": "ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction",
    "url": "http://arxiv.org/abs/2505.24230v1",
    "arxiv_id": "2505.24230v1",
    "authors": [
      "Murari Ambati"
    ],
    "published": "2025-05-30T05:44:34+00:00",
    "summary": "We propose ProofNet++, a neuro-symbolic framework that enhances automated theorem proving by combining large language models (LLMs) with formal proof verification and self-correction mechanisms. Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning. ProofNet++ mitigates these limitations by integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module. Our experiments on miniF2F, Lean's mathlib, and HOL Light show that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability over prior models. We provide theoretical analysis of the convergence and stability of the verifier-guided RL framework and release our datasets and codebase for future research."
  },
  {
    "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling",
    "url": "http://arxiv.org/abs/2505.24199v1",
    "arxiv_id": "2505.24199v1",
    "authors": [
      "Yimin Du"
    ],
    "published": "2025-05-30T04:20:00+00:00",
    "summary": "The quality of human preference data is crucial for training and evaluating large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional side-by-side (SBS) annotation approaches often struggle with inherent uncertainty, annotator disagreement, and the complexity of preference judgments. This paper introduces a novel framework based on intuitionistic fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data annotation tasks. Our approach captures not only the degree of preference but also the uncertainty and hesitation inherent in human judgment through membership, non-membership, and hesitation degrees. We propose an IFS-based annotation protocol that enables more nuanced preference modeling, develops aggregation methods for handling annotator disagreement, and introduces quality metrics for preference data assessment. Experimental validation on multiple datasets demonstrates that our IFS-based approach significantly improves annotation consistency, reduces annotator fatigue, and produces higher-quality preference data compared to traditional binary and Likert-scale methods. The resulting preference datasets lead to improved model performance in downstream tasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\% reduction in annotation time. Our framework provides a principled approach to handling uncertainty in human preference annotation and offers practical benefits for large-scale LLM training."
  },
  {
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "url": "http://arxiv.org/abs/2505.24183v1",
    "arxiv_id": "2505.24183v1",
    "authors": [
      "Yaoyu Zhu",
      "Di Huang",
      "Hanqi Lyu",
      "Xiaoyun Zhang",
      "Chongxiao Li",
      "Wenxuan Shi",
      "Yutong Wu",
      "Jianan Mu",
      "Jinghua Wang",
      "Yang Zhao",
      "Pengwei Jin",
      "Shuyao Cheng",
      "Shengwen Liang",
      "Xishan Zhang",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ],
    "published": "2025-05-30T03:51:06+00:00",
    "summary": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities."
  },
  {
    "title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2505.24164v1",
    "arxiv_id": "2505.24164v1",
    "authors": [
      "Shilin Xu",
      "Yanwei Li",
      "Rui Yang",
      "Tao Zhang",
      "Yueyi Sun",
      "Wei Chow",
      "Linfeng Li",
      "Hang Song",
      "Qi Xu",
      "Yunhai Tong",
      "Xiangtai Li",
      "Hao Fei"
    ],
    "published": "2025-05-30T03:11:46+00:00",
    "summary": "Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at https://github.com/xushilin1/mixed-r1."
  },
  {
    "title": "Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control",
    "url": "http://arxiv.org/abs/2505.24161v1",
    "arxiv_id": "2505.24161v1",
    "authors": [
      "Zijie Xu",
      "Tong Bu",
      "Zecheng Hao",
      "Jianhao Ding",
      "Zhaofei Yu"
    ],
    "published": "2025-05-30T03:08:03+00:00",
    "summary": "Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making through neuromorphic hardware, making them compelling for Reinforcement Learning (RL) in resource-constrained edge devices. Recent studies in this field directly replace Artificial Neural Networks (ANNs) by SNNs in existing RL frameworks, overlooking whether the RL algorithm is suitable for SNNs. However, most RL algorithms in continuous control are designed tailored to ANNs, including the target network soft updates mechanism, which conflict with the discrete, non-differentiable dynamics of SNN spikes. We identify that this mismatch destabilizes SNN training in continuous control tasks. To bridge this gap between discrete SNN and continuous control, we propose a novel proxy target framework. The continuous and differentiable dynamics of the proxy target enable smooth updates, bypassing the incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy network operates only during training, the SNN retains its energy efficiency during deployment without inference overhead. Extensive experiments on continuous control benchmarks demonstrate that compared to vanilla SNNs, the proxy target framework enables SNNs to achieve up to 32% higher performance across different spiking neurons. Notably, we are the first to surpass ANN performance in continuous control with simple Leaky-Integrate-and-Fire (LIF) neurons. This work motivates a new class of SNN-friendly RL algorithms tailored to SNN's characteristics, paving the way for neuromorphic agents that combine high performance with low power consumption."
  },
  {
    "title": "Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24155v1",
    "arxiv_id": "2505.24155v1",
    "authors": [
      "Ehtesamul Azim",
      "Dongjie Wang",
      "Tae Hyun Hwang",
      "Yanjie Fu",
      "Wei Zhang"
    ],
    "published": "2025-05-30T03:01:07+00:00",
    "summary": "Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods."
  },
  {
    "title": "Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24113v1",
    "arxiv_id": "2505.24113v1",
    "authors": [
      "Pengcheng Dai",
      "Yuanqiu Mo",
      "Wenwu Yu",
      "Wei Ren"
    ],
    "published": "2025-05-30T01:23:14+00:00",
    "summary": "This paper studies the networked multi-agent reinforcement learning (NMARL) problem, where the objective of agents is to collaboratively maximize the discounted average cumulative rewards. Different from the existing methods that suffer from poor expression due to linear function approximation, we propose a distributed neural policy gradient algorithm that features two innovatively designed neural networks, specifically for the approximate Q-functions and policy functions of agents. This distributed neural policy gradient algorithm consists of two key components: the distributed critic step and the decentralized actor step. In the distributed critic step, agents receive the approximate Q-function parameters from their neighboring agents via a time-varying communication networks to collaboratively evaluate the joint policy. In contrast, in the decentralized actor step, each agent updates its local policy parameter solely based on its own approximate Q-function. In the convergence analysis, we first establish the global convergence of agents for the joint policy evaluation in the distributed critic step. Subsequently, we rigorously demonstrate the global convergence of the overall distributed neural policy gradient algorithm with respect to the objective function. Finally, the effectiveness of the proposed algorithm is demonstrated by comparing it with a centralized algorithm through simulation in the robot path planning environment."
  },
  {
    "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.24105v1",
    "arxiv_id": "2505.24105v1",
    "authors": [
      "Jiacheng Lin",
      "Zhenbang Wu",
      "Jimeng Sun"
    ],
    "published": "2025-05-30T01:13:22+00:00",
    "summary": "We present EHRMIND, a practical recipe for adapting large language models (LLMs) to complex clinical reasoning tasks using reinforcement learning with verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding, its application to healthcare contexts presents unique challenges due to the specialized knowledge and reasoning required for electronic health record (EHR) interpretation. Our pilot study on the MEDCALC benchmark reveals two key failure modes: (1) misapplied knowledge, where models possess relevant medical knowledge but apply it incorrectly, and (2) missing knowledge, where models lack essential domain knowledge. To address these cases, EHRMIND applies a two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that injects missing domain knowledge, stabilizes subsequent training, and encourages structured, interpretable outputs; followed by RLVR, which reinforces outcome correctness and refines the model's decision-making. We demonstrate the effectiveness of our method across diverse clinical applications, including medical calculations (MEDCALC), patient-trial matching (TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers consistent gains in accuracy, interpretability, and cross-task generalization. These findings offer practical guidance for applying RLVR to enhance LLM capabilities in healthcare settings."
  },
  {
    "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "url": "http://arxiv.org/abs/2505.24098v1",
    "arxiv_id": "2505.24098v1",
    "authors": [
      "Zhongmou He",
      "Yee Man Choi",
      "Kexun Zhang",
      "Jiabao Ji",
      "Junting Zhou",
      "Dejia Xu",
      "Ivan Bercovich",
      "Aidan Zhang",
      "Lei Li"
    ],
    "published": "2025-05-30T01:00:34+00:00",
    "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/."
  },
  {
    "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning",
    "url": "http://arxiv.org/abs/2505.24061v1",
    "arxiv_id": "2505.24061v1",
    "authors": [
      "Jiashun Liu",
      "Zihao Wu",
      "Johan Obando-Ceron",
      "Pablo Samuel Castro",
      "Aaron Courville",
      "Ling Pan"
    ],
    "published": "2025-05-29T23:07:58+00:00",
    "summary": "Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical power in more complex architectures. To address this, we argue that in advanced RL agents, maintaining a neuron's learning capacity, its ability to adapt via gradient updates, is more critical than preserving its expressive ability. Based on this insight, we shift the statistical objective from activations to gradients, and introduce GraMa (Gradient Magnitude Neural Activity Metric), a lightweight, architecture-agnostic metric for quantifying neuron-level learning capacity. We show that GraMa effectively reveals persistent neuron inactivity across diverse architectures, including residual networks, diffusion models, and agents with varied activation functions. Moreover, resetting neurons guided by GraMa (ReGraMa) consistently improves learning performance across multiple deep RL algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite."
  },
  {
    "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin",
    "url": "http://arxiv.org/abs/2505.24034v1",
    "arxiv_id": "2505.24034v1",
    "authors": [
      "Bo Wu",
      "Sid Wang",
      "Yunhao Tang",
      "Jia Ding",
      "Eryk Helenowski",
      "Liang Tan",
      "Tengyu Xu",
      "Tushar Gowda",
      "Zhengxing Chen",
      "Chen Zhu",
      "Xiaocheng Tang",
      "Yundi Qian",
      "Beibei Zhu",
      "Rui Hou"
    ],
    "published": "2025-05-29T22:14:15+00:00",
    "summary": "Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.   In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training."
  },
  {
    "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training",
    "url": "http://arxiv.org/abs/2505.24034v2",
    "arxiv_id": "2505.24034v2",
    "authors": [
      "Bo Wu",
      "Sid Wang",
      "Yunhao Tang",
      "Jia Ding",
      "Eryk Helenowski",
      "Liang Tan",
      "Tengyu Xu",
      "Tushar Gowda",
      "Zhengxing Chen",
      "Chen Zhu",
      "Xiaocheng Tang",
      "Yundi Qian",
      "Beibei Zhu",
      "Rui Hou"
    ],
    "published": "2025-05-29T22:14:15+00:00",
    "summary": "Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.   In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically during the Llama 3 post-training, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training."
  },
  {
    "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
    "url": "http://arxiv.org/abs/2505.24025v1",
    "arxiv_id": "2505.24025v1",
    "authors": [
      "Chenbin Pan",
      "Wenbin He",
      "Zhengzhong Tu",
      "Liu Ren"
    ],
    "published": "2025-05-29T21:58:06+00:00",
    "summary": "The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \\textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios."
  },
  {
    "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "url": "http://arxiv.org/abs/2505.23977v1",
    "arxiv_id": "2505.23977v1",
    "authors": [
      "Yichen Feng",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Yuetai Li",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "published": "2025-05-29T20:08:36+00:00",
    "summary": "Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning."
  },
  {
    "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation",
    "url": "http://arxiv.org/abs/2505.23960v1",
    "arxiv_id": "2505.23960v1",
    "authors": [
      "Henry Conklin"
    ],
    "published": "2025-05-29T19:27:50+00:00",
    "summary": "Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.   The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks."
  },
  {
    "title": "Thompson Sampling in Online RLHF with General Function Approximation",
    "url": "http://arxiv.org/abs/2505.23927v1",
    "arxiv_id": "2505.23927v1",
    "authors": [
      "Songtao Feng",
      "Jie Fu"
    ],
    "published": "2025-05-29T18:22:02+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) has achieved great empirical success in aligning large language models (LLMs) with human preference, and it is of great importance to study the statistical efficiency of RLHF algorithms from a theoretical perspective. In this work, we consider the online RLHF setting where the preference data is revealed during the learning process and study action value function approximation. We design a model-free posterior sampling algorithm for online RLHF inspired by Thompson sampling and provide its theoretical guarantee. Specifically, we adopt Bellman eluder (BE) dimension as the complexity measure of the function class and establish $O(\\sqrt{T})$ regret bound for the proposed algorithm with other multiplicative factor depending on the horizon, BE dimension and the $log$-bracketing number of the function class. Further, in the analysis, we first establish the concentration-type inequality of the squared Bellman error bound based on the maximum likelihood estimator (MLE) generalization bound, which plays the crucial rules in obtaining the eluder-type regret bound and may be of independent interest."
  },
  {
    "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation",
    "url": "http://arxiv.org/abs/2505.23912v1",
    "arxiv_id": "2505.23912v1",
    "authors": [
      "Caiqi Zhang",
      "Xiaochen Zhu",
      "Chengzu Li",
      "Nigel Collier",
      "Andreas Vlachos"
    ],
    "published": "2025-05-29T18:05:20+00:00",
    "summary": "Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded."
  },
  {
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "url": "http://arxiv.org/abs/2505.23762v1",
    "arxiv_id": "2505.23762v1",
    "authors": [
      "Chenyu Yang",
      "Shiqian Su",
      "Shi Liu",
      "Xuan Dong",
      "Yue Yu",
      "Weijie Su",
      "Xuehui Wang",
      "Zhaoyang Liu",
      "Jinguo Zhu",
      "Hao Li",
      "Wenhai Wang",
      "Yu Qiao",
      "Xizhou Zhu",
      "Jifeng Dai"
    ],
    "published": "2025-05-29T17:59:51+00:00",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
  },
  {
    "title": "Differential Information: An Information-Theoretic Perspective on Preference Optimization",
    "url": "http://arxiv.org/abs/2505.23761v1",
    "arxiv_id": "2505.23761v1",
    "authors": [
      "Yunjae Won",
      "Hyunji Lee",
      "Hyeonbin Hwang",
      "Minjoon Seo"
    ],
    "published": "2025-05-29T17:59:50+00:00",
    "summary": "Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information."
  },
  {
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.23754v1",
    "arxiv_id": "2505.23754v1",
    "authors": [
      "Ziyin Zhang",
      "Jiahao Xu",
      "Zhiwei He",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yansi Li",
      "Linfeng Song",
      "Zhengwen Liang",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-05-29T17:59:39+00:00",
    "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
  },
  {
    "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?",
    "url": "http://arxiv.org/abs/2505.23749v1",
    "arxiv_id": "2505.23749v1",
    "authors": [
      "Paul G\u00f6lz",
      "Nika Haghtalab",
      "Kunhe Yang"
    ],
    "published": "2025-05-29T17:59:20+00:00",
    "summary": "After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average -- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.   The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature $\\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a KL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled."
  },
  {
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
    "url": "http://arxiv.org/abs/2505.23747v1",
    "arxiv_id": "2505.23747v1",
    "authors": [
      "Diankun Wu",
      "Fangfu Liu",
      "Yi-Hsin Hung",
      "Yueqi Duan"
    ],
    "published": "2025-05-29T17:59:04+00:00",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/."
  },
  {
    "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning",
    "url": "http://arxiv.org/abs/2505.23727v1",
    "arxiv_id": "2505.23727v1",
    "authors": [
      "Song Wang",
      "Gongfan Fang",
      "Lingdong Kong",
      "Xiangtai Li",
      "Jianyun Xu",
      "Sheng Yang",
      "Qiang Li",
      "Jianke Zhu",
      "Xinchao Wang"
    ],
    "published": "2025-05-29T17:55:49+00:00",
    "summary": "Existing reasoning segmentation approaches typically fine-tune multimodal large language models (MLLMs) using image-text pairs and corresponding mask labels. However, they exhibit limited generalization to out-of-distribution scenarios without an explicit reasoning process. Although recent efforts leverage reinforcement learning through group-relative policy optimization (GRPO) to enhance reasoning ability, they often suffer from overthinking - producing uniformly verbose reasoning chains irrespective of task complexity. This results in elevated computational costs and limited control over reasoning quality. To address this problem, we propose PixelThink, a simple yet effective scheme that integrates externally estimated task difficulty and internally measured model uncertainty to regulate reasoning generation within a reinforcement learning paradigm. The model learns to compress reasoning length in accordance with scene complexity and predictive confidence. To support comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark with annotated reasoning references and difficulty scores, along with a suite of metrics designed to assess segmentation accuracy, reasoning quality, and efficiency jointly. Experimental results demonstrate that the proposed approach improves both reasoning efficiency and overall segmentation performance. Our work contributes novel perspectives towards efficient and interpretable multimodal understanding. The code and model will be publicly available."
  },
  {
    "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
    "url": "http://arxiv.org/abs/2505.23723v1",
    "arxiv_id": "2505.23723v1",
    "authors": [
      "Zexi Liu",
      "Jingyi Chai",
      "Xinyu Zhu",
      "Shuo Tang",
      "Rui Ye",
      "Bo Zhang",
      "Lei Bai",
      "Siheng Chen"
    ],
    "published": "2025-05-29T17:54:44+00:00",
    "summary": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities."
  },
  {
    "title": "AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.23708v1",
    "arxiv_id": "2505.23708v1",
    "authors": [
      "Lucas N. Alegre",
      "Agon Serifi",
      "Ruben Grandia",
      "David M\u00fcller",
      "Espen Knoop",
      "Moritz B\u00e4cher"
    ],
    "published": "2025-05-29T17:41:48+00:00",
    "summary": "Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks."
  },
  {
    "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability",
    "url": "http://arxiv.org/abs/2505.23703v1",
    "arxiv_id": "2505.23703v1",
    "authors": [
      "Ruida Wang",
      "Yuxin Li",
      "Yi R.",
      "Fung",
      "Tong Zhang"
    ],
    "published": "2025-05-29T17:39:30+00:00",
    "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the *NL-FL Problem Alignment* method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the *Mixed Problem Input* technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive experiments demonstrate that the **HybridReasoning** framework achieves **89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials."
  },
  {
    "title": "Grounded Reinforcement Learning for Visual Reasoning",
    "url": "http://arxiv.org/abs/2505.23678v1",
    "arxiv_id": "2505.23678v1",
    "authors": [
      "Gabriel Sarch",
      "Snigdha Saha",
      "Naitik Khandelwal",
      "Ayush Jain",
      "Michael J. Tarr",
      "Aviral Kumar",
      "Katerina Fragkiadaki"
    ],
    "published": "2025-05-29T17:20:26+00:00",
    "summary": "While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK for spatial reasoning, V*bench for visual search, and ScreenSpot and VisualWebArena for web-based grounding--ViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRL's performance on localizing small GUI elements and visual search, achieving 86.4% on V*Bench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the model's visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is a strong paradigm for imbuing models with general-purpose visual reasoning."
  },
  {
    "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2505.23667v1",
    "arxiv_id": "2505.23667v1",
    "authors": [
      "Lang Cao",
      "Jingxian Xu",
      "Hanbing Liu",
      "Jinyu Wang",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "published": "2025-05-29T17:13:40+00:00",
    "summary": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform O1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs."
  },
  {
    "title": "AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction",
    "url": "http://arxiv.org/abs/2505.23663v1",
    "arxiv_id": "2505.23663v1",
    "authors": [
      "Niklas Freymuth",
      "Tobias W\u00fcrth",
      "Nicolas Schreiber",
      "Balazs Gyenes",
      "Andreas Boltres",
      "Johannes Mitsch",
      "Aleksandar Taranovic",
      "Tai Hoang",
      "Philipp Dahlinger",
      "Philipp Becker",
      "Luise K\u00e4rger",
      "Gerhard Neumann"
    ],
    "published": "2025-05-29T17:10:44+00:00",
    "summary": "The cost and accuracy of simulating complex physical systems using the Finite Element Method (FEM) scales with the resolution of the underlying mesh. Adaptive meshes improve computational efficiency by refining resolution in critical regions, but typically require task-specific heuristics or cumbersome manual design by a human expert. We propose Adaptive Meshing By Expert Reconstruction (AMBER), a supervised learning approach to mesh adaptation. Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e., a function mapping from the geometry to the local element size of the target mesh, and uses this prediction to produce a new intermediate mesh using an out-of-the-box mesh generator. This process is enabled through a hierarchical graph neural network, and relies on data augmentation by automatically projecting expert labels onto AMBER-generated data during training. We evaluate AMBER on 2D and 3D datasets, including classical physics problems, mechanical components, and real-world industrial designs with human expert meshes. AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including ones using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches."
  },
  {
    "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation",
    "url": "http://arxiv.org/abs/2505.23657v1",
    "arxiv_id": "2505.23657v1",
    "authors": [
      "Hongxiang Zhang",
      "Hao Chen",
      "Tianyi Zhang",
      "Muhao Chen"
    ],
    "published": "2025-05-29T17:07:24+00:00",
    "summary": "Recent decoding methods improve the factuality of large language models~(LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios."
  },
  {
    "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment",
    "url": "http://arxiv.org/abs/2505.23634v1",
    "arxiv_id": "2505.23634v1",
    "authors": [
      "John Halloran"
    ],
    "published": "2025-05-29T16:44:29+00:00",
    "summary": "The model context protocol (MCP) has been widely adapted as an open standard enabling the seamless integration of generative AI agents. However, recent work has shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks (FBAs), allowing malicious system access and credential theft, but requiring that users download compromised files directly to their systems. Herein, we show that the threat model of MCP-based attacks is significantly broader than previously thought, i.e., attackers need only post malicious content online to deceive MCP agents into carrying out their attacks on unsuspecting victims' systems.   To improve alignment guardrails against such attacks, we introduce a new MCP dataset of FBAs and (truly) benign samples to explore the effectiveness of direct preference optimization (DPO) for the refusal training of large language models (LLMs). While DPO improves model guardrails against such attacks, we show that the efficacy of refusal learning varies drastically depending on the model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to refuse extremely poorly. Thus, to further improve FBA refusals, we introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG. We show that RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment, thus drastically improving guardrails against MCP-based attacks."
  },
  {
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "url": "http://arxiv.org/abs/2505.23621v1",
    "arxiv_id": "2505.23621v1",
    "authors": [
      "Zheyuan Yang",
      "Lyuhao Chen",
      "Arman Cohan",
      "Yilun Zhao"
    ],
    "published": "2025-05-29T16:28:50+00:00",
    "summary": "In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training."
  },
  {
    "title": "Inference-time Scaling of Diffusion Models through Classical Search",
    "url": "http://arxiv.org/abs/2505.23614v1",
    "arxiv_id": "2505.23614v1",
    "authors": [
      "Xiangcheng Zhang",
      "Haowei Lin",
      "Haotian Ye",
      "James Zou",
      "Jianzhu Ma",
      "Yitao Liang",
      "Yilun Du"
    ],
    "published": "2025-05-29T16:22:40+00:00",
    "summary": "Classical search algorithms have long underpinned modern artificial intelligence. In this work, we tackle the challenge of inference-time control in diffusion models -- adapting generated outputs to meet diverse test-time objectives -- using principles from classical search. We propose a general framework that orchestrates local and global search to efficiently navigate the generative space. It employs a theoretically grounded local search via annealed Langevin MCMC and performs compute-efficient global exploration using breadth-first and depth-first tree search. We evaluate our approach on a range of challenging domains, including planning, offline reinforcement learning, and image generation. Across all tasks, we observe significant gains in both performance and efficiency. These results show that classical search provides a principled and practical foundation for inference-time scaling in diffusion models. Project page at diffusion-inference-scaling.github.io."
  },
  {
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering",
    "url": "http://arxiv.org/abs/2505.23604v1",
    "arxiv_id": "2505.23604v1",
    "authors": [
      "Guangtao Zeng",
      "Maohao Shen",
      "Delin Chen",
      "Zhenting Qi",
      "Subhro Das",
      "Dan Gutfreund",
      "David Cox",
      "Gregory Wornell",
      "Wei Lu",
      "Zhang-Wei Hong",
      "Chuang Gan"
    ],
    "published": "2025-05-29T16:15:36+00:00",
    "summary": "Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced."
  },
  {
    "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles",
    "url": "http://arxiv.org/abs/2505.23590v1",
    "arxiv_id": "2505.23590v1",
    "authors": [
      "Zifu Wang",
      "Junyi Zhu",
      "Bo Tang",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Jiaqian Yu",
      "Matthew B. Blaschko"
    ],
    "published": "2025-05-29T16:01:22+00:00",
    "summary": "The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL using jigsaw puzzles as a structured experimental framework, revealing several key findings. \\textit{Firstly,} we find that MLLMs, initially performing near to random guessing on simple puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \\textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \\textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \\textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: \\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}."
  },
  {
    "title": "On-Policy RL with Optimal Reward Baseline",
    "url": "http://arxiv.org/abs/2505.23585v1",
    "arxiv_id": "2505.23585v1",
    "authors": [
      "Yaru Hao",
      "Li Dong",
      "Xun Wu",
      "Shaohan Huang",
      "Zewen Chi",
      "Furu Wei"
    ],
    "published": "2025-05-29T15:58:04+00:00",
    "summary": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo."
  },
  {
    "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model",
    "url": "http://arxiv.org/abs/2505.23579v1",
    "arxiv_id": "2505.23579v1",
    "authors": [
      "Adibvafa Fallahpour",
      "Andrew Magnuson",
      "Purav Gupta",
      "Shihao Ma",
      "Jack Naimer",
      "Arnav Shah",
      "Haonan Duan",
      "Omar Ibrahim",
      "Hani Goodarzi",
      "Chris J. Maddison",
      "Bo Wang"
    ],
    "published": "2025-05-29T15:49:27+00:00",
    "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models, despite strong sequence representation, struggle with multi-step reasoning and lack inherent transparent, biologically intuitive explanations. We introduce BioReason, a pioneering architecture that, for the first time, deeply integrates a DNA foundation model with a Large Language Model (LLM). This novel connection enables the LLM to directly process and reason with genomic information as a fundamental input, fostering a new form of multimodal biological understanding. BioReason's sophisticated multi-step reasoning is developed through supervised fine-tuning and targeted reinforcement learning, guiding the system to generate logical, biologically coherent deductions. On biological reasoning benchmarks including KEGG-based disease pathway prediction - where accuracy improves from 88% to 97% - and variant effect prediction, BioReason demonstrates an average 15% performance gain over strong single-modality baselines. BioReason reasons over unseen biological entities and articulates decision-making through interpretable, step-by-step biological traces, offering a transformative approach for AI in biology that enables deeper mechanistic insights and accelerates testable hypothesis generation from genomic data. Data, code, and checkpoints are publicly available at https://github.com/bowang-lab/BioReason"
  },
  {
    "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models",
    "url": "http://arxiv.org/abs/2505.23564v1",
    "arxiv_id": "2505.23564v1",
    "authors": [
      "Yiran Guo",
      "Lijie Xu",
      "Jie Liu",
      "Dan Ye",
      "Shuang Qiu"
    ],
    "published": "2025-05-29T15:38:19+00:00",
    "summary": "Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO."
  },
  {
    "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information",
    "url": "http://arxiv.org/abs/2505.23558v1",
    "arxiv_id": "2505.23558v1",
    "authors": [
      "Xu Chu",
      "Xinrong Chen",
      "Guanyu Wang",
      "Zhijie Tan",
      "Kui Huang",
      "Wenyu Lv",
      "Tong Mo",
      "Weiping Li"
    ],
    "published": "2025-05-29T15:34:15+00:00",
    "summary": "Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: https://github.com/Liar406/Look_Again."
  },
  {
    "title": "Normalizing Flows are Capable Models for RL",
    "url": "http://arxiv.org/abs/2505.23527v1",
    "arxiv_id": "2505.23527v1",
    "authors": [
      "Raj Ghugare",
      "Benjamin Eysenbach"
    ],
    "published": "2025-05-29T15:06:22+00:00",
    "summary": "Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL."
  },
  {
    "title": "Individual differences in the cognitive mechanisms of planning strategy discovery",
    "url": "http://arxiv.org/abs/2505.23519v1",
    "arxiv_id": "2505.23519v1",
    "authors": [
      "Ruiqi He",
      "Falk Lieder"
    ],
    "published": "2025-05-29T14:57:34+00:00",
    "summary": "People employ efficient planning strategies. But how are these strategies acquired? Previous research suggests that people can discover new planning strategies through learning from reinforcements, a process known as metacognitive reinforcement learning (MCRL). While prior work has shown that MCRL models can learn new planning strategies and explain more participants' experience-driven discovery better than alternative mechanisms, it also revealed significant individual differences in metacognitive learning. Furthermore, when fitted to human data, these models exhibit a slower rate of strategy discovery than humans. In this study, we investigate whether incorporating cognitive mechanisms that might facilitate human strategy discovery can bring models of MCRL closer to human performance. Specifically, we consider intrinsically generated metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation. Analysis of planning task data shows that a larger proportion of participants used at least one of these mechanisms, with significant individual differences in their usage and varying impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort valuation, and learning the value of acting without further planning were found to facilitate strategy discovery. While these enhancements provided valuable insights into individual differences and the effect of these mechanisms on strategy discovery, they did not fully close the gap between model and human performance, prompting further exploration of additional factors that people might use to discover new planning strategies."
  },
  {
    "title": "On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment",
    "url": "http://arxiv.org/abs/2505.23459v1",
    "arxiv_id": "2505.23459v1",
    "authors": [
      "Safwan Labbi",
      "Paul Mangold",
      "Daniil Tiapkin",
      "Eric Moulines"
    ],
    "published": "2025-05-29T14:08:35+00:00",
    "summary": "Ensuring convergence of policy gradient methods in federated reinforcement learning (FRL) under environment heterogeneity remains a major challenge. In this work, we first establish that heterogeneity, perhaps counter-intuitively, can necessitate optimal policies to be non-deterministic or even time-varying, even in tabular environments. Subsequently, we prove global convergence results for federated policy gradient (FedPG) algorithms employing local updates, under a {\\L}ojasiewicz condition that holds only for each individual agent, in both entropy-regularized and non-regularized scenarios. Crucially, our theoretical analysis shows that FedPG attains linear speed-up with respect to the number of agents, a property central to efficient federated learning. Leveraging insights from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient method that employs a carefully constructed softmax-inspired parameterization coupled with an appropriate regularization scheme. We further demonstrate explicit convergence rates for b-RS-FedPG toward near-optimal stationary policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG consistently outperform federated Q-learning on heterogeneous settings."
  },
  {
    "title": "Diffusion Guidance Is a Controllable Policy Improvement Operator",
    "url": "http://arxiv.org/abs/2505.23458v1",
    "arxiv_id": "2505.23458v1",
    "authors": [
      "Kevin Frans",
      "Seohong Park",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "published": "2025-05-29T14:06:50+00:00",
    "summary": "At the core of reinforcement learning is the idea of learning beyond the performance in the data. However, scaling such systems has proven notoriously tricky. In contrast, techniques from generative modeling have proven remarkably scalable and are simple to train. In this work, we combine these strengths, by deriving a direct relation between policy improvement and guidance of diffusion models. The resulting framework, CFGRL, is trained with the simplicity of supervised learning, yet can further improve on the policies in the data. On offline RL tasks, we observe a reliable trend -- increased guidance weighting leads to increased performance. Of particular importance, CFGRL can operate without explicitly learning a value function, allowing us to generalize simple supervised methods (e.g., goal-conditioned behavioral cloning) to further prioritize optimality, gaining performance for \"free\" across the board."
  },
  {
    "title": "A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation",
    "url": "http://arxiv.org/abs/2505.23451v1",
    "arxiv_id": "2505.23451v1",
    "authors": [
      "Shuzhou Sun",
      "Li Liu",
      "Tianpeng Liu",
      "Shuaifeng Zhi",
      "Ming-Ming Cheng",
      "Janne Heikkil\u00e4",
      "Yongxiang Liu"
    ],
    "published": "2025-05-29T13:57:01+00:00",
    "summary": "Existing two-stage Scene Graph Generation (SGG) frameworks typically incorporate a detector to extract relationship features and a classifier to categorize these relationships; therefore, the training paradigm follows a causal chain structure, where the detector's inputs determine the classifier's inputs, which in turn influence the final predictions. However, such a causal chain structure can yield spurious correlations between the detector's inputs and the final predictions, i.e., the prediction of a certain relationship may be influenced by other relationships. This influence can induce at least two observable biases: tail relationships are predicted as head ones, and foreground relationships are predicted as background ones; notably, the latter bias is seldom discussed in the literature. To address this issue, we propose reconstructing the causal chain structure into a reverse causal structure, wherein the classifier's inputs are treated as the confounder, and both the detector's inputs and the final predictions are viewed as causal variables. Specifically, we term the reconstructed causal paradigm as the Reverse causal Framework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse Estimation (ARE) to intervene on the confounder to estimate the reverse causality, \\ie the causality from final predictions to the classifier's inputs. Then, the Maximum Information Sampling (MIS) is suggested to enhance the reverse causality estimation further by considering the relationship information. Theoretically, RcSGG can mitigate the spurious correlations inherent in the SGG framework, subsequently eliminating the induced biases. Comprehensive experiments on popular benchmarks and diverse SGG frameworks show the state-of-the-art mean recall rate."
  },
  {
    "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.23433v1",
    "arxiv_id": "2505.23433v1",
    "authors": [
      "Jian Yao",
      "Ran Cheng",
      "Xingyu Wu",
      "Jibin Wu",
      "Kay Chen Tan"
    ],
    "published": "2025-05-29T13:27:44+00:00",
    "summary": "The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and Potential at k (a novel metric quantifying an LLM's reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5 percent average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions."
  },
  {
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization",
    "url": "http://arxiv.org/abs/2505.23387v1",
    "arxiv_id": "2505.23387v1",
    "authors": [
      "Mingzhe Du",
      "Luu Tuan Tuan",
      "Yue Liu",
      "Yuhao Qing",
      "Dong Huang",
      "Xinyi He",
      "Qian Liu",
      "Zejun Ma",
      "See-kiong Ng"
    ],
    "published": "2025-05-29T12:14:29+00:00",
    "summary": "Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency."
  },
  {
    "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.23380v1",
    "arxiv_id": "2505.23380v1",
    "authors": [
      "Weijia Mao",
      "Zhenheng Yang",
      "Mike Zheng Shou"
    ],
    "published": "2025-05-29T12:00:15+00:00",
    "summary": "Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL."
  },
  {
    "title": "Discriminative Policy Optimization for Token-Level Reward Models",
    "url": "http://arxiv.org/abs/2505.23363v1",
    "arxiv_id": "2505.23363v1",
    "authors": [
      "Hongzhan Chen",
      "Tao Yang",
      "Shiping Gao",
      "Ruijun Chen",
      "Xiaojun Quan",
      "Hongtao Tian",
      "Ting Yao"
    ],
    "published": "2025-05-29T11:40:34+00:00",
    "summary": "Process reward models (PRMs) provide more nuanced supervision compared to outcome reward models (ORMs) for optimizing policy models, positioning them as a promising approach to enhancing the capabilities of LLMs in complex reasoning tasks. Recent efforts have advanced PRMs from step-level to token-level granularity by integrating reward modeling into the training of generative models, with reward scores derived from token generation probabilities. However, the conflict between generative language modeling and reward modeling may introduce instability and lead to inaccurate credit assignments. To address this challenge, we revisit token-level reward assignment by decoupling reward modeling from language generation and derive a token-level reward model through the optimization of a discriminative policy, termed the Q-function Reward Model (Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level Q-functions from preference data without relying on fine-grained annotations. In our experiments, Q-RM consistently outperforms all baseline methods across various benchmarks. For example, when integrated into PPO/REINFORCE algorithms, Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM significantly enhances training efficiency, achieving convergence 12 times faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code and data are available at https://github.com/homzer/Q-RM."
  },
  {
    "title": "Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control",
    "url": "http://arxiv.org/abs/2505.23355v1",
    "arxiv_id": "2505.23355v1",
    "authors": [
      "Maxiu Xiao",
      "Jianglin Lan",
      "Jingxing Yu",
      "Eldert van Henten",
      "Congcong Sun"
    ],
    "published": "2025-05-29T11:30:35+00:00",
    "summary": "Climate control is crucial for greenhouse production as it directly affects crop growth and resource use. Reinforcement learning (RL) has received increasing attention in this field, but still faces challenges, including limited training efficiency and high reliance on initial learning conditions. Interactive RL, which combines human (grower) input with the RL agent's learning, offers a potential solution to overcome these challenges. However, interactive RL has not yet been applied to greenhouse climate control and may face challenges related to imperfect inputs. Therefore, this paper aims to explore the possibility and performance of applying interactive RL with imperfect inputs into greenhouse climate control, by: (1) developing three representative interactive RL algorithms tailored for greenhouse climate control (reward shaping, policy shaping and control sharing); (2) analyzing how input characteristics are often contradicting, and how the trade-offs between them make grower's inputs difficult to perfect; (3) proposing a neural network-based approach to enhance the robustness of interactive RL agents under limited input availability; (4) conducting a comprehensive evaluation of the three interactive RL algorithms with imperfect inputs in a simulated greenhouse environment. The demonstration shows that interactive RL incorporating imperfect grower inputs has the potential to improve the performance of the RL agent. RL algorithms that influence action selection, such as policy shaping and control sharing, perform better when dealing with imperfect inputs, achieving 8.4% and 6.8% improvement in profit, respectively. In contrast, reward shaping, an algorithm that manipulates the reward function, is sensitive to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the importance of selecting an appropriate mechanism when incorporating imperfect inputs."
  },
  {
    "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective",
    "url": "http://arxiv.org/abs/2505.23349v1",
    "arxiv_id": "2505.23349v1",
    "authors": [
      "Sheng Ouyang",
      "Yulan Hu",
      "Ge Chen",
      "Qingyang Li",
      "Fuzheng Zhang",
      "Yong Liu"
    ],
    "published": "2025-05-29T11:12:00+00:00",
    "summary": "Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner."
  },
  {
    "title": "Graph Positional Autoencoders as Self-supervised Learners",
    "url": "http://arxiv.org/abs/2505.23345v1",
    "arxiv_id": "2505.23345v1",
    "authors": [
      "Yang Liu",
      "Deyu Bo",
      "Wenxuan Cao",
      "Yuan Fang",
      "Yawen Li",
      "Chuan Shi"
    ],
    "published": "2025-05-29T11:10:11+00:00",
    "summary": "Graph self-supervised learning seeks to learn effective graph representations without relying on labeled data. Among various approaches, graph autoencoders (GAEs) have gained significant attention for their efficiency and scalability. Typically, GAEs take incomplete graphs as input and predict missing elements, such as masked nodes or edges. While effective, our experimental investigation reveals that traditional node or edge masking paradigms primarily capture low-frequency signals in the graph and fail to learn the expressive structural information. To address these issues, we propose Graph Positional Autoencoders (GraphPAE), which employs a dual-path architecture to reconstruct both node features and positions. Specifically, the feature path uses positional encoding to enhance the message-passing processing, improving GAE's ability to predict the corrupted information. The position path, on the other hand, leverages node representations to refine positions and approximate eigenvectors, thereby enabling the encoder to learn diverse frequency information. We conduct extensive experiments to verify the effectiveness of GraphPAE, including heterophilic node classification, graph property prediction, and transfer learning. The results demonstrate that GraphPAE achieves state-of-the-art performance and consistently outperforms baselines by a large margin."
  },
  {
    "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2505.23331v1",
    "arxiv_id": "2505.23331v1",
    "authors": [
      "Matteo Gallici",
      "Haitz S\u00e1ez de Oc\u00e1riz Borde"
    ],
    "published": "2025-05-29T10:45:38+00:00",
    "summary": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives."
  },
  {
    "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO",
    "url": "http://arxiv.org/abs/2505.23316v1",
    "arxiv_id": "2505.23316v1",
    "authors": [
      "Kaiyang Guo",
      "Yinchuan Li",
      "Zhitang Chen"
    ],
    "published": "2025-05-29T10:23:22+00:00",
    "summary": "Direct alignment methods typically optimize large language models (LLMs) by contrasting the likelihoods of preferred versus dispreferred responses. While effective in steering LLMs to match relative preference, these methods are frequently noted for decreasing the absolute likelihoods of example responses. As a result, aligned models tend to generate outputs that deviate from the expected patterns, exhibiting reward-hacking effect even without a reward model. This undesired consequence exposes a fundamental limitation in contrastive alignment, which we characterize as likelihood underdetermination. In this work, we revisit direct preference optimization (DPO) -- the seminal direct alignment method -- and demonstrate that its loss theoretically admits a decomposed reformulation. The reformulated loss not only broadens applicability to a wider range of feedback types, but also provides novel insights into the underlying cause of likelihood underdetermination. Specifically, the standard DPO implementation implicitly oversimplifies a regularizer in the reformulated loss, and reinstating its complete version effectively resolves the underdetermination issue. Leveraging these findings, we introduce PRoximalized PReference Optimization (PRO), a unified method to align with diverse feeback types, eliminating likelihood underdetermination through an efficient approximation of the complete regularizer. Comprehensive experiments show the superiority of PRO over existing methods in scenarios involving pairwise, binary and scalar feedback."
  },
  {
    "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic Perception",
    "url": "http://arxiv.org/abs/2505.23275v1",
    "arxiv_id": "2505.23275v1",
    "authors": [
      "Guangyuan Liu",
      "Yinqiu Liu",
      "Ruichen Zhang",
      "Hongyang Du",
      "Dusit Niyato",
      "Zehui Xiong",
      "Sumei Sun",
      "Abbas Jamalipour"
    ],
    "published": "2025-05-29T09:23:11+00:00",
    "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has greatly enhanced real-time interaction, decision-making, and collaborative tasks. However, in wireless multi-agent scenarios, limited bandwidth poses significant challenges to exchanging semantically rich multimodal information efficiently. Traditional semantic communication methods, though effective, struggle with redundancy and loss of crucial details. To overcome these challenges, we propose a Retrieval-Augmented Multimodal Semantic Communication (RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven semantic refinement tailored for distributed multi-agent environments, enabling efficient exchange of critical multimodal elements through local caching and selective transmission. Our approach dynamically optimizes retrieval using deep reinforcement learning (DRL) to balance semantic fidelity with bandwidth constraints. A comprehensive case study on multi-agent autonomous driving demonstrates that our DRL-based retrieval strategy significantly improves task completion efficiency and reduces communication overhead compared to baseline methods."
  },
  {
    "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection",
    "url": "http://arxiv.org/abs/2505.23268v1",
    "arxiv_id": "2505.23268v1",
    "authors": [
      "Spyros Barbakos",
      "Charalampos Antoniadis",
      "Gerasimos Potamianos",
      "Gianluca Setti"
    ],
    "published": "2025-05-29T09:16:19+00:00",
    "summary": "Video consumption is a key part of daily life, but watching entire videos can be tedious. To address this, researchers have explored video summarization and highlight detection to identify key video segments. While some works combine video frames and transcripts, and others tackle video summarization and highlight detection using Reinforcement Learning (RL), no existing work, to the best of our knowledge, integrates both modalities within an RL framework. In this paper, we propose a multimodal pipeline that leverages video frames and their corresponding transcripts to generate a more condensed version of the video and detect highlights using a modality fusion mechanism. The pipeline is trained within an RL framework, which rewards the model for generating diverse and representative summaries while ensuring the inclusion of video segments with meaningful transcript content. The unsupervised nature of the training allows for learning from large-scale unannotated datasets, overcoming the challenge posed by the limited size of existing annotated datasets. Our experiments show that using the transcript in video summarization and highlight detection achieves superior results compared to relying solely on the visual content of the video."
  },
  {
    "title": "Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs",
    "url": "http://arxiv.org/abs/2505.23265v1",
    "arxiv_id": "2505.23265v1",
    "authors": [
      "Zheng Sun",
      "Yi Wei",
      "Long Yu"
    ],
    "published": "2025-05-29T09:14:16+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \\textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future."
  },
  {
    "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization Under Dynamic Workloads for Securities Trading System",
    "url": "http://arxiv.org/abs/2505.23258v1",
    "arxiv_id": "2505.23258v1",
    "authors": [
      "Haojie Jia",
      "Zhenhao Li",
      "Gen Li",
      "Minxian Xu",
      "Kejiang Ye"
    ],
    "published": "2025-05-29T09:06:01+00:00",
    "summary": "As securities trading systems transition to a microservices architecture, optimizing system performance presents challenges such as inefficient resource scheduling and high service response delays. Existing container orchestration platforms lack tailored performance optimization mechanisms for trading scenarios, making it difficult to meet the stringent 50ms response time requirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based performance optimization approach for securities trading, incorporating an adaptive resource scheduling algorithm leveraging deep reinforcement learning, a three-level caching mechanism for trading operations, and a Long Short-Term Memory (LSTM) based load prediction model. Real-world deployment at a securities exchange demonstrates that the optimized system achieves an average CPU utilization of 78\\%, reduces transaction response time to 105ms, and reaches a peak processing capacity of 15,000 transactions per second, effectively meeting the rigorous performance and reliability demands of securities trading."
  },
  {
    "title": "Context-Aware Semantic Communication for the Wireless Networks",
    "url": "http://arxiv.org/abs/2505.23249v1",
    "arxiv_id": "2505.23249v1",
    "authors": [
      "Guangyuan Liu",
      "Yinqiu Liu",
      "Jiacheng Wang",
      "Hongyang Du",
      "Dusit Niyato",
      "Jiawen Kang",
      "Zehui Xiong",
      "Abbas Jamalipour"
    ],
    "published": "2025-05-29T08:55:29+00:00",
    "summary": "In next-generation wireless networks, supporting real-time applications such as augmented reality, autonomous driving, and immersive Metaverse services demands stringent constraints on bandwidth, latency, and reliability. Existing semantic communication (SemCom) approaches typically rely on static models, overlooking dynamic conditions and contextual cues vital for efficient transmission. To address these challenges, we propose CaSemCom, a context-aware SemCom framework that leverages a Large Language Model (LLM)-based gating mechanism and a Mixture of Experts (MoE) architecture to adaptively select and encode only high-impact semantic features across multiple data modalities. Our multimodal, multi-user case study demonstrates that CaSemCom significantly improves reconstructed image fidelity while reducing bandwidth usage, outperforming single-agent deep reinforcement learning (DRL) methods and traditional baselines in convergence speed, semantic accuracy, and retransmission overhead."
  },
  {
    "title": "Accelerating RLHF Training with Reward Variance Increase",
    "url": "http://arxiv.org/abs/2505.23247v1",
    "arxiv_id": "2505.23247v1",
    "authors": [
      "Zonglin Yang",
      "Zhexuan Gu",
      "Houduo Qi",
      "Yancheng Yuan"
    ],
    "published": "2025-05-29T08:54:06+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \\log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm."
  },
  {
    "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration",
    "url": "http://arxiv.org/abs/2505.23224v1",
    "arxiv_id": "2505.23224v1",
    "authors": [
      "Zhitao He",
      "Sandeep Polisetty",
      "Zhiyuan Fan",
      "Yuchen Huang",
      "Shujin Wu",
      "Yi R.",
      "Fung"
    ],
    "published": "2025-05-29T08:14:40+00:00",
    "summary": "In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance."
  },
  {
    "title": "DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes",
    "url": "http://arxiv.org/abs/2505.23179v1",
    "arxiv_id": "2505.23179v1",
    "authors": [
      "Sungjune Park",
      "Hyunjun Kim",
      "Junho Kim",
      "Seongho Kim",
      "Yong Man Ro"
    ],
    "published": "2025-05-29T07:16:16+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant visual understanding capabilities, yet their fine-grained visual perception in complex real-world scenarios, such as densely crowded public areas, remains limited. Inspired by the recent success of reinforcement learning (RL) in both LLMs and MLLMs, in this paper, we explore how RL can enhance visual perception ability of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and Perception with RL (DIP-R1) designed to enhance the visual perception capabilities of MLLMs, by comprehending complex scenes and looking through visual instances closely. DIP-R1 guides MLLMs through detailed inspection of visual scene via three simply designed rule-based reward modelings. First, we adopt a standard reasoning reward encouraging the model to include three step-by-step processes: 1) reasoning for understanding visual scenes, 2) observing for looking through interested but ambiguous regions, and 3) decision-making for predicting answer. Second, a variance-guided looking reward is designed to examine uncertain regions for the second observing process. It explicitly enables the model to inspect ambiguous areas, improving its ability to mitigate perceptual uncertainties. Third, we model a weighted precision-recall accuracy reward enhancing accurate decision-making. We explore its effectiveness across diverse fine-grained object detection data consisting of challenging real-world environments, such as densely crowded scenes. Built upon existing MLLMs, DIP-R1 achieves consistent and significant improvement across various in-domain and out-of-domain scenarios. It also outperforms various existing baseline models and supervised fine-tuning methods. Our findings highlight the substantial potential of integrating RL into MLLMs for enhancing capabilities in complex real-world perception tasks."
  },
  {
    "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners",
    "url": "http://arxiv.org/abs/2505.23150v1",
    "arxiv_id": "2505.23150v1",
    "authors": [
      "Michal Nauman",
      "Marek Cygan",
      "Carmelo Sferrazza",
      "Aviral Kumar",
      "Pieter Abbeel"
    ],
    "published": "2025-05-29T06:41:45+00:00",
    "summary": "Recent advances in language modeling and vision stem from training large models on diverse, multi-task data. This paradigm has had limited impact in value-based reinforcement learning (RL), where improvements are often driven by small models trained in a single-task context. This is because in multi-task RL sparse rewards and gradient conflicts make optimization of temporal difference brittle. Practical workflows for generalist policies therefore avoid online training, instead cloning expert trajectories or distilling collections of single-task policies into one agent. In this work, we show that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi-task training. We test our approach on 7 multi-task benchmarks with over 280 unique tasks, spanning high degree-of-freedom humanoid control and discrete vision-based RL. We find that, despite its simplicity, the proposed approach leads to state-of-the-art single and multi-task performance, as well as sample-efficient transfer to new tasks."
  },
  {
    "title": "DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs",
    "url": "http://arxiv.org/abs/2505.23131v1",
    "arxiv_id": "2505.23131v1",
    "authors": [
      "Xinyu Yao",
      "Daniel Bourgeois",
      "Abhinav Jain",
      "Yuxin Tang",
      "Jiawen Yao",
      "Zhimin Ding",
      "Arlei Silva",
      "Chris Jermaine"
    ],
    "published": "2025-05-29T06:04:32+00:00",
    "summary": "We study the problem of assigning operations in a dataflow graph to devices to minimize execution time in a work-conserving system, with emphasis on complex machine learning workloads. Prior learning-based methods often struggle due to three key limitations: (1) reliance on bulk-synchronous systems like TensorFlow, which under-utilize devices due to barrier synchronization; (2) lack of awareness of the scheduling mechanism of underlying systems when designing learning-based methods; and (3) exclusive dependence on reinforcement learning, ignoring the structure of effective heuristics designed by experts. In this paper, we propose \\textsc{Doppler}, a three-stage framework for training dual-policy networks consisting of 1) a $\\mathsf{SEL}$ policy for selecting operations and 2) a $\\mathsf{PLC}$ policy for placing chosen operations on devices. Our experiments show that \\textsc{Doppler} outperforms all baseline methods across tasks by reducing system execution time and additionally demonstrates sampling efficiency by reducing per-episode training time."
  },
  {
    "title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models",
    "url": "http://arxiv.org/abs/2505.23108v1",
    "arxiv_id": "2505.23108v1",
    "authors": [
      "Zexuan Li",
      "Hongliang Dai",
      "Piji Li"
    ],
    "published": "2025-05-29T05:21:54+00:00",
    "summary": "Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE), we find that samples generated by directly prompting LLMs may easily have high structural similarities with each other. They tend to use a limited variety of phrasing while expressing the relation between a pair of entities. Therefore, in this paper, we study how to effectively improve the diversity of the training samples generated with LLMs for RE, while also maintaining their correctness. We first try to make the LLMs produce dissimilar samples by directly giving instructions in In-Context Learning (ICL) prompts. Then, we propose an approach to fine-tune LLMs for diversity training sample generation through Direct Preference Optimization (DPO). Our experiments on commonly used RE datasets show that both attempts can improve the quality of the generated training data. We also find that comparing with directly performing RE with an LLM, training a non-LLM RE model with its generated samples may lead to better performance."
  },
  {
    "title": "CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing",
    "url": "http://arxiv.org/abs/2505.23102v1",
    "arxiv_id": "2505.23102v1",
    "authors": [
      "Yuka Ogino",
      "Takahiro Toizumi",
      "Atsushi Ito"
    ],
    "published": "2025-05-29T05:09:13+00:00",
    "summary": "Low-Light Image Enhancement (LLIE) is crucial for improving both human perception and computer vision tasks. This paper addresses two challenges in zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive Language-Image Pre-Training (CLIP) model and maintaining computational efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement learning-based Visual image Enhancement (CURVE). CURVE employs a simple image processing module which adjusts global image tone based on B\\'ezier curve and estimates its processing parameters iteratively. The estimator is trained by reinforcement learning with rewards designed using CLIP text embeddings. Experiments on low-light and multi-exposure datasets demonstrate the performance of CURVE in terms of enhancement quality and processing speed compared to conventional methods."
  },
  {
    "title": "Learning to Search for Vehicle Routing with Multiple Time Windows",
    "url": "http://arxiv.org/abs/2505.23098v1",
    "arxiv_id": "2505.23098v1",
    "authors": [
      "Kuan Xu",
      "Zhiguang Cao",
      "Chenlong Zheng",
      "Linong Liu"
    ],
    "published": "2025-05-29T05:03:28+00:00",
    "summary": "In this study, we propose a reinforcement learning-based adaptive variable neighborhood search (RL-AVNS) method designed for effectively solving the Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional adaptive approaches that rely solely on historical operator performance, our method integrates a reinforcement learning framework to dynamically select neighborhood operators based on real-time solution states and learned experience. We introduce a fitness metric that quantifies customers' temporal flexibility to improve the shaking phase, and employ a transformer-based neural policy network to intelligently guide operator selection during the local search. Extensive computational experiments are conducted on realistic scenarios derived from the replenishment of unmanned vending machines, characterized by multiple clustered replenishment windows. Results demonstrate that RL-AVNS significantly outperforms traditional variable neighborhood search (VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics, achieving substantial improvements in solution quality and computational efficiency across various instance scales and time window complexities. Particularly notable is the algorithm's capability to generalize effectively to problem instances not encountered during training, underscoring its practical utility for complex logistics scenarios."
  },
  {
    "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models",
    "url": "http://arxiv.org/abs/2505.23091v1",
    "arxiv_id": "2505.23091v1",
    "authors": [
      "Zeyu Liu",
      "Yuhang Liu",
      "Guanghao Zhu",
      "Congkai Xie",
      "Zhen Li",
      "Jianbo Yuan",
      "Xinyao Wang",
      "Qing Li",
      "Shing-Chi Cheung",
      "Shengyu Zhang",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "published": "2025-05-29T04:51:56+00:00",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini)."
  },
  {
    "title": "Maximizing Confidence Alone Improves Reasoning",
    "url": "http://arxiv.org/abs/2505.22660v1",
    "arxiv_id": "2505.22660v1",
    "authors": [
      "Mihir Prabhudesai",
      "Lili Chen",
      "Alex Ippoliti",
      "Katerina Fragkiadaki",
      "Hao Liu",
      "Deepak Pathak"
    ],
    "published": "2025-05-28T17:59:37+00:00",
    "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable."
  },
  {
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason",
    "url": "http://arxiv.org/abs/2505.22653v1",
    "arxiv_id": "2505.22653v1",
    "authors": [
      "Ang Lv",
      "Ruobing Xie",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Rui Yan"
    ],
    "published": "2025-05-28T17:59:03+00:00",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."
  },
  {
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "url": "http://arxiv.org/abs/2505.22648v1",
    "arxiv_id": "2505.22648v1",
    "authors": [
      "Jialong Wu",
      "Baixuan Li",
      "Runnan Fang",
      "Wenbiao Yin",
      "Liwen Zhang",
      "Zhengwei Tao",
      "Dingchu Zhang",
      "Zekun Xi",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "published": "2025-05-28T17:57:07+00:00",
    "summary": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent."
  },
  {
    "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control",
    "url": "http://arxiv.org/abs/2505.22642v1",
    "arxiv_id": "2505.22642v1",
    "authors": [
      "Younggyo Seo",
      "Carmelo Sferrazza",
      "Haoran Geng",
      "Michal Nauman",
      "Zhao-Heng Yin",
      "Pieter Abbeel"
    ],
    "published": "2025-05-28T17:55:26+00:00",
    "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics."
  },
  {
    "title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents",
    "url": "http://arxiv.org/abs/2505.22634v1",
    "arxiv_id": "2505.22634v1",
    "authors": [
      "Rui Li",
      "Zixuan Hu",
      "Wenxi Qu",
      "Jinouwen Zhang",
      "Zhenfei Yin",
      "Sha Zhang",
      "Xuantuo Huang",
      "Hanqing Wang",
      "Tai Wang",
      "Jiangmiao Pang",
      "Wanli Ouyang",
      "Lei Bai",
      "Wangmeng Zuo",
      "Ling-Yu Duan",
      "Dongzhan Zhou",
      "Shixiang Tang"
    ],
    "published": "2025-05-28T17:50:53+00:00",
    "summary": "Scientific embodied agents play a crucial role in modern laboratories by automating complex experimental workflows. Compared to typical household environments, laboratory settings impose significantly higher demands on perception of physical-chemical transformations and long-horizon planning, making them an ideal testbed for advancing embodied intelligence. However, its development has been long hampered by the lack of suitable simulator and benchmarks. In this paper, we address this gap by introducing LabUtopia, a comprehensive simulation and benchmarking suite designed to facilitate the development of generalizable, reasoning-capable embodied agents in laboratory settings. Specifically, it integrates i) LabSim, a high-fidelity simulator supporting multi-physics and chemically meaningful interactions; ii) LabScene, a scalable procedural generator for diverse scientific scenes; and iii) LabBench, a hierarchical benchmark spanning five levels of complexity from atomic actions to long-horizon mobile manipulation. LabUtopia supports 30 distinct tasks and includes more than 200 scene and instrument assets, enabling large-scale training and principled evaluation in high-complexity environments. We demonstrate that LabUtopia offers a powerful platform for advancing the integration of perception, planning, and control in scientific-purpose agents and provides a rigorous testbed for exploring the practical capabilities and generalization limits of embodied intelligence in future research."
  },
  {
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.22617v1",
    "arxiv_id": "2505.22617v1",
    "authors": [
      "Ganqu Cui",
      "Yuchen Zhang",
      "Jiacheng Chen",
      "Lifan Yuan",
      "Zhi Wang",
      "Yuxin Zuo",
      "Haozhan Li",
      "Yuchen Fan",
      "Huayu Chen",
      "Weize Chen",
      "Zhiyuan Liu",
      "Hao Peng",
      "Lei Bai",
      "Wanli Ouyang",
      "Yu Cheng",
      "Bowen Zhou",
      "Ning Ding"
    ],
    "published": "2025-05-28T17:38:45+00:00",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance."
  },
  {
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction",
    "url": "http://arxiv.org/abs/2505.22613v1",
    "arxiv_id": "2505.22613v1",
    "authors": [
      "Yuchi Wang",
      "Yishuo Cai",
      "Shuhuai Ren",
      "Sihan Yang",
      "Linli Yao",
      "Yuanxin Liu",
      "Yuanxing Zhang",
      "Pengfei Wan",
      "Xu Sun"
    ],
    "published": "2025-05-28T17:29:34+00:00",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at https://github.com/wangyuchi369/RICO."
  },
  {
    "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym",
    "url": "http://arxiv.org/abs/2505.22597v1",
    "arxiv_id": "2505.22597v1",
    "authors": [
      "Ngoc La",
      "Ruaridh Mon-Williams",
      "Julie A. Shah"
    ],
    "published": "2025-05-28T17:10:43+00:00",
    "summary": "In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts."
  },
  {
    "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.22596v1",
    "arxiv_id": "2505.22596v1",
    "authors": [
      "Jiaqi Huang",
      "Zunnan Xu",
      "Jun Zhou",
      "Ting Liu",
      "Yicheng Xiao",
      "Mingwen Ou",
      "Bowen Ji",
      "Xiu Li",
      "Kehong Yuan"
    ],
    "published": "2025-05-28T17:08:28+00:00",
    "summary": "Leveraging multimodal large models for image segmentation has become a prominent research direction. However, existing approaches typically rely heavily on manually annotated datasets that include explicit reasoning processes, which are costly and time-consuming to produce. Recent advances suggest that reinforcement learning (RL) can endow large models with reasoning capabilities without requiring such reasoning-annotated data. In this paper, we propose SAM-R1, a novel framework that enables multimodal large models to perform fine-grained reasoning in image understanding tasks. Our approach is the first to incorporate fine-grained segmentation settings during the training of multimodal reasoning models. By integrating task-specific, fine-grained rewards with a tailored optimization objective, we further enhance the model's reasoning and segmentation alignment. We also leverage the Segment Anything Model (SAM) as a strong and flexible reward provider to guide the learning process. With only 3k training samples, SAM-R1 achieves strong performance across multiple benchmarks, demonstrating the effectiveness of reinforcement learning in equipping multimodal models with segmentation-oriented reasoning capabilities."
  },
  {
    "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs",
    "url": "http://arxiv.org/abs/2505.22548v1",
    "arxiv_id": "2505.22548v1",
    "authors": [
      "Changhao Song",
      "Yazhou Zhang",
      "Peng Zhang"
    ],
    "published": "2025-05-28T16:32:16+00:00",
    "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion classification) and advanced tasks (e.g., sarcasm/humor detection). Current methods rely on fixed-length CoT reasoning, failing to adapt to the varying complexity of emotions. We propose a task-adaptive reasoning framework that employs DeepSeek-R1 to generate variable-length reasoning chains for different emotion tasks. By combining fine-tuning with reinforcement learning, we design a composite reward function that balances four objectives: prediction accuracy, adaptive reasoning depth control, structural diversity in reasoning paths, and suppression of repetitive logic. This approach achieves dynamic context-sensitive inference while enabling LLMs to autonomously develop deep reasoning capabilities. Experimental results demonstrate consistent improvements in both Acc and F1 scores across four tasks: emotion, sentiment, humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges rigid CoT reasoning and emotional complexity through adaptive-depth analysis."
  },
  {
    "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection",
    "url": "http://arxiv.org/abs/2505.22517v1",
    "arxiv_id": "2505.22517v1",
    "authors": [
      "Yimeng Gu",
      "Zhao Tong",
      "Ignacio Castro",
      "Shu Wu",
      "Gareth Tyson"
    ],
    "published": "2025-05-28T16:03:41+00:00",
    "summary": "Multimodal out-of-context news is a type of misinformation in which the image is used outside of its original context. Many existing works have leveraged multimodal large language models (MLLMs) for detecting out-of-context news. However, observing the limited zero-shot performance of smaller MLLMs, they generally require label-rich fine-tuning and/or expensive API calls to GPT models to improve the performance, which is impractical in low-resource scenarios. In contrast, we aim to improve the performance of small MLLMs in a more label-efficient and cost-effective manner. To this end, we first prompt multiple teacher MLLMs to generate both label predictions and corresponding rationales, which collectively serve as the teachers' knowledge. We then introduce a two-stage knowledge distillation framework to transfer this knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the student model using all training data. In Stage 2, we further fine-tune the student model using both LoRA fine-tuning and DPO on the data points where teachers' predictions conflict. This two-stage strategy reduces annotation costs and helps the student model uncover subtle patterns in more challenging cases. Experimental results demonstrate that our approach achieves state-of-the-art performance using less than 10% labeled data."
  },
  {
    "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation",
    "url": "http://arxiv.org/abs/2505.22492v1",
    "arxiv_id": "2505.22492v1",
    "authors": [
      "Hongyi Zhou",
      "Josiah P. Hanna",
      "Jin Zhu",
      "Ying Yang",
      "Chengchun Shi"
    ],
    "published": "2025-05-28T15:42:20+00:00",
    "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically."
  },
  {
    "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2505.22467v1",
    "arxiv_id": "2505.22467v1",
    "authors": [
      "Jiaxi Yang",
      "Mengqi Zhang",
      "Yiqiao Jin",
      "Hao Chen",
      "Qingsong Wen",
      "Lu Lin",
      "Yi He",
      "Weijie Xu",
      "James Evans",
      "Jindong Wang"
    ],
    "published": "2025-05-28T15:20:09+00:00",
    "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. Nevertheless, the question of how agents should be structurally organized for optimal cooperation remains largely unexplored. In this position paper, we aim to gently redirect the focus of the MAS research community toward this critical dimension: develop topology-aware MASs for specific tasks. Specifically, the system consists of three core components - agents, communication links, and communication patterns - that collectively shape its coordination performance and efficiency. To this end, we introduce a systematic, three-stage framework: agent selection, structure profiling, and topology synthesis. Each stage would trigger new research opportunities in areas such as language models, reinforcement learning, graph learning, and generative modeling; together, they could unleash the full potential of MASs in complicated real-world applications. Then, we discuss the potential challenges and opportunities in the evaluation of multiple systems. We hope our perspective and framework can offer critical new insights in the era of agentic AI."
  },
  {
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "url": "http://arxiv.org/abs/2505.22453v1",
    "arxiv_id": "2505.22453v1",
    "authors": [
      "Lai Wei",
      "Yuting Li",
      "Chen Wang",
      "Yue Wang",
      "Linghe Kong",
      "Weiran Huang",
      "Lichao Sun"
    ],
    "published": "2025-05-28T15:11:16+00:00",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9 %$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT."
  },
  {
    "title": "Frequency Resource Management in 6G User-Centric CFmMIMO: A Hybrid Reinforcement Learning and Metaheuristic Approach",
    "url": "http://arxiv.org/abs/2505.22443v1",
    "arxiv_id": "2505.22443v1",
    "authors": [
      "Selina Cheggour",
      "Valeria Loscri"
    ],
    "published": "2025-05-28T15:07:56+00:00",
    "summary": "As sixth-generation (6G) networks continue to evolve, AI-driven solutions are playing a crucial role in enabling more efficient and adaptive resource management in wireless communication. One of the key innovations in 6G is user-centric cell-free massive Multiple-Input Multiple-Output (UC-CFmMIMO), a paradigm that eliminates traditional cell boundaries and enhances network performance by dynamically assigning access points (APs) to users. This approach is particularly well-suited for vehicular networks, offering seamless, homogeneous, ultra-reliable, and low-latency connectivity. However, in dense networks, a key challenge lies in efficiently allocating frequency resources within a limited shared subband spectrum while accounting for frequency selectivity and the dependency of signal propagation on bandwidth. These factors make resource allocation increasingly complex, especially in dynamic environments where maintaining Quality of Service (QoS) is critical. This paper tackles these challenges by proposing a hybrid multi-user allocation strategy that integrates reinforcement learning (RL) and metaheuristic optimization to enhance spectral efficiency (SE), ensure fairness, and mitigate interference within shared subbands. To assess its effectiveness, we compare this hybrid approach with two other methods: the bio-inspired Aquila Optimizer (AO) and Deep Deterministic Policy Gradient (DDPG)-based Actor-Critic Reinforcement Learning (AC-RL). Our evaluation is grounded in real-world patterns and channel characteristics, utilizing the 3GPP-3D channel modeling framework (QuaDRiGa) to capture realistic propagation conditions. The results demonstrate that the proposed hybrid strategy achieves a superior balance among competing objectives, underscoring the role of AI-driven resource allocation in advancing UC-CFmMIMO systems for next-generation wireless networks."
  },
  {
    "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.22442v1",
    "arxiv_id": "2505.22442v1",
    "authors": [
      "Mattie Fellows",
      "Clarisse Wibault",
      "Uljad Berdica",
      "Johannes Forkel",
      "Jakob N. Foerster",
      "Michael A. Osborne"
    ],
    "published": "2025-05-28T15:07:24+00:00",
    "summary": "Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: https://github.com/CWibault/sorel\\_torel."
  },
  {
    "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning",
    "url": "http://arxiv.org/abs/2505.22430v1",
    "arxiv_id": "2505.22430v1",
    "authors": [
      "Kun Li",
      "Yunxiang Li",
      "Tianhua Zhang",
      "Hongyin Luo",
      "Xixin Wu",
      "James Glass",
      "Helen Meng"
    ],
    "published": "2025-05-28T14:55:33+00:00",
    "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented generation (RAG) systems. However, current LLM-based evaluation frameworks predominantly rely on directly prompting resource-intensive models with complex multi-stage prompts, underutilizing models' reasoning capabilities and introducing significant computational cost. In this paper, we present RAG-Zeval (RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness and correctness evaluation as a rule-guided reasoning task. Our approach trains evaluators with reinforcement learning, facilitating compact models to generate comprehensive and sound assessments with detailed explanation in one-pass. We introduce a ranking-based outcome reward mechanism, using preference judgments rather than absolute scores, to address the challenge of obtaining precise pointwise reward signals. To this end, we synthesize the ranking references by generating quality-controlled responses with zero human annotation. Experiments demonstrate RAG-Zeval's superior performance, achieving the strongest correlation with human judgments and outperforming baselines that rely on LLMs with 10-100 times more parameters. Our approach also exhibits superior interpretability in response evaluation."
  },
  {
    "title": "Hybrid Learning for Cold-Start-Aware Microservice Scheduling in Dynamic Edge Environments",
    "url": "http://arxiv.org/abs/2505.22424v1",
    "arxiv_id": "2505.22424v1",
    "authors": [
      "Jingxi Lu",
      "Wenhao Li",
      "Jianxiong Guo",
      "Xingjian Ding",
      "Zhiqing Tang",
      "Tian Wang",
      "Weijia Jia"
    ],
    "published": "2025-05-28T14:51:57+00:00",
    "summary": "With the rapid growth of IoT devices and their diverse workloads, container-based microservices deployed at edge nodes have become a lightweight and scalable solution. However, existing microservice scheduling algorithms often assume static resource availability, which is unrealistic when multiple containers are assigned to an edge node. Besides, containers suffer from cold-start inefficiencies during early-stage training in currently popular reinforcement learning (RL) algorithms. In this paper, we propose a hybrid learning framework that combines offline imitation learning (IL) with online Soft Actor-Critic (SAC) optimization to enable a cold-start-aware microservice scheduling with dynamic allocation for computing resources. We first formulate a delay-and-energy-aware scheduling problem and construct a rule-based expert to generate demonstration data for behavior cloning. Then, a GRU-enhanced policy network is designed in the policy network to extract the correlation among multiple decisions by separately encoding slow-evolving node states and fast-changing microservice features, and an action selection mechanism is given to speed up the convergence. Extensive experiments show that our method significantly accelerates convergence and achieves superior final performance. Compared with baselines, our algorithm improves the total objective by $50\\%$ and convergence speed by $70\\%$, and demonstrates the highest stability and robustness across various edge configurations."
  },
  {
    "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation",
    "url": "http://arxiv.org/abs/2505.22407v1",
    "arxiv_id": "2505.22407v1",
    "authors": [
      "Jiadong Pan",
      "Zhiyuan Ma",
      "Kaiyan Zhang",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-05-28T14:37:21+00:00",
    "summary": "Diffusion models have recently demonstrated exceptional performance in image generation task. However, existing image generation methods still significantly suffer from the dilemma of image reasoning, especially in logic-centered image generation tasks. Inspired by the success of Chain of Thought (CoT) and Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL algorithm for diffusion models to achieve reasoning generation of logical images by performing reflection and iteration across generation trajectories. The intermediate samples in the denoising process carry noise, making accurate reward evaluation difficult. To address this challenge, SRRL treats the entire denoising trajectory as a CoT step with multi-round reflective denoising process and introduces condition guided forward process, which allows for reflective iteration between CoT steps. Through SRRL-based iterative diffusion training, we introduce image reasoning through CoT into generation tasks adhering to physical laws and unconventional physical phenomena for the first time. Notably, experimental results of case study exhibit that the superior performance of our SRRL algorithm even compared with GPT-4o. The project page is https://jadenpan0.github.io/srrl.github.io/."
  },
  {
    "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs",
    "url": "http://arxiv.org/abs/2505.22396v1",
    "arxiv_id": "2505.22396v1",
    "authors": [
      "Xudong Li",
      "Mengdan Zhang",
      "Peixian Chen",
      "Xiawu Zheng",
      "Yan Zhang",
      "Jingyuan Zheng",
      "Yunhang Shen",
      "Ke Li",
      "Chaoyou Fu",
      "Xing Sun",
      "Rongrong Ji"
    ],
    "published": "2025-05-28T14:24:02+00:00",
    "summary": "Multi-modal Large Language Models (MLLMs) excel at single-image tasks but struggle with multi-image understanding due to cross-modal misalignment, leading to hallucinations (context omission, conflation, and misinterpretation). Existing methods using Direct Preference Optimization (DPO) constrain optimization to a solitary image reference within the input sequence, neglecting holistic context modeling. We propose Context-to-Cue Direct Preference Optimization (CcDPO), a multi-level preference optimization framework that enhances per-image perception in multi-image settings by zooming into visual clues -- from sequential context to local details. It features: (i) Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs' multi-image context comprehension and integrates a spectrum of low-cost global sequence preferences for bias mitigation. (ii) Needle-Level Optimization : Directs attention to fine-grained visual details through region-targeted visual prompts and multimodal preference supervision. To support scalable optimization, we also construct MultiScope-42k, an automatically generated dataset with high-quality multi-level preference pairs. Experiments show that CcDPO significantly reduces hallucinations and yields consistent performance gains across general single- and multi-image tasks."
  },
  {
    "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition",
    "url": "http://arxiv.org/abs/2505.22375v1",
    "arxiv_id": "2505.22375v1",
    "authors": [
      "Hanting Chen",
      "Yasheng Wang",
      "Kai Han",
      "Dong Li",
      "Lin Li",
      "Zhenni Bi",
      "Jinpeng Li",
      "Haoyu Wang",
      "Fei Mi",
      "Mingjian Zhu",
      "Bin Wang",
      "Kaikai Song",
      "Yifei Fu",
      "Xu He",
      "Yu Luo",
      "Chong Zhu",
      "Quan He",
      "Xueyu Wu",
      "Wei He",
      "Hailin Hu",
      "Yehui Tang",
      "Dacheng Tao",
      "Xinghao Chen",
      "Yunhe Wang",
      "Other Contributors"
    ],
    "published": "2025-05-28T14:03:02+00:00",
    "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode for routine queries and a deeper \"slow\" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners."
  },
  {
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "url": "http://arxiv.org/abs/2505.22338v1",
    "arxiv_id": "2505.22338v1",
    "authors": [
      "Hanyang Wang",
      "Lu Wang",
      "Chaoyun Zhang",
      "Tianjun Mao",
      "Si Qin",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ],
    "published": "2025-05-28T13:23:49+00:00",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad"
  },
  {
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start",
    "url": "http://arxiv.org/abs/2505.22334v1",
    "arxiv_id": "2505.22334v1",
    "authors": [
      "Lai Wei",
      "Yuting Li",
      "Kaipeng Zheng",
      "Chen Wang",
      "Yue Wang",
      "Linghe Kong",
      "Lichao Sun",
      "Weiran Huang"
    ],
    "published": "2025-05-28T13:21:38+00:00",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While \"aha moment\" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on MathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start."
  },
  {
    "title": "Chain-of-Thought for Large Language Model-empowered Wireless Communications",
    "url": "http://arxiv.org/abs/2505.22320v1",
    "arxiv_id": "2505.22320v1",
    "authors": [
      "Xudong Wang",
      "Jian Zhu",
      "Ruichen Zhang",
      "Lei Feng",
      "Dusit Niyato",
      "Jiacheng Wang",
      "Hongyang Du",
      "Shiwen Mao",
      "Zhu Han"
    ],
    "published": "2025-05-28T13:04:48+00:00",
    "summary": "Recent advances in large language models (LLMs) have opened new possibilities for automated reasoning and decision-making in wireless networks. However, applying LLMs to wireless communications presents challenges such as limited capability in handling complex logic, generalization, and reasoning. Chain-of-Thought (CoT) prompting, which guides LLMs to generate explicit intermediate reasoning steps, has been shown to significantly improve LLM performance on complex tasks. Inspired by this, this paper explores the application potential of CoT-enhanced LLMs in wireless communications. Specifically, we first review the fundamental theory of CoT and summarize various types of CoT. We then survey key CoT and LLM techniques relevant to wireless communication and networking. Moreover, we introduce a multi-layer intent-driven CoT framework that bridges high-level user intent expressed in natural language with concrete wireless control actions. Our proposed framework sequentially parses and clusters intent, selects appropriate CoT reasoning modules via reinforcement learning, then generates interpretable control policies for system configuration. Using the unmanned aerial vehicle (UAV) network as a case study, we demonstrate that the proposed framework significantly outperforms a non-CoT baseline in both communication performance and quality of generated reasoning."
  },
  {
    "title": "Skywork Open Reasoner 1 Technical Report",
    "url": "http://arxiv.org/abs/2505.22312v1",
    "arxiv_id": "2505.22312v1",
    "authors": [
      "Jujie He",
      "Jiacai Liu",
      "Chris Yuhao Liu",
      "Rui Yan",
      "Chaojie Wang",
      "Peng Cheng",
      "Xiaoyu Zhang",
      "Fuxiang Zhang",
      "Jiacheng Xu",
      "Wei Shen",
      "Siyuan Li",
      "Liang Zeng",
      "Tianwen Wei",
      "Cheng Cheng",
      "Bo An",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-05-28T12:56:04+00:00",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets."
  },
  {
    "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training",
    "url": "http://arxiv.org/abs/2505.22257v1",
    "arxiv_id": "2505.22257v1",
    "authors": [
      "Youssef Mroueh",
      "Nicolas Dupuis",
      "Brian Belgodere",
      "Apoorva Nitsure",
      "Mattia Rigotti",
      "Kristjan Greenewald",
      "Jiri Navratil",
      "Jerret Ross",
      "Jesus Rios"
    ],
    "published": "2025-05-28T11:42:33+00:00",
    "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart."
  },
  {
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2505.22203v1",
    "arxiv_id": "2505.22203v1",
    "authors": [
      "Yuzhen Huang",
      "Weihao Zeng",
      "Xingshan Zeng",
      "Qi Zhu",
      "Junxian He"
    ],
    "published": "2025-05-28T10:28:41+00:00",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning."
  },
  {
    "title": "Reverse Preference Optimization for Complex Instruction Following",
    "url": "http://arxiv.org/abs/2505.22172v1",
    "arxiv_id": "2505.22172v1",
    "authors": [
      "Xiang Huang",
      "Ting-En Lin",
      "Feiteng Fang",
      "Yuchuan Wu",
      "Hangyu Li",
      "Yuzhong Qu",
      "Fei Huang",
      "Yongbin Li"
    ],
    "published": "2025-05-28T09:44:27+00:00",
    "summary": "Instruction following (IF) is a critical capability for large language models (LLMs). However, handling complex instructions with multiple constraints remains challenging. Previous methods typically select preference pairs based on the number of constraints they satisfy, introducing noise where chosen examples may fail to follow some constraints and rejected examples may excel in certain respects over the chosen ones. To address the challenge of aligning with multiple preferences, we propose a simple yet effective method called Reverse Preference Optimization (RPO). It mitigates noise in preference pairs by dynamically reversing the constraints within the instruction to ensure the chosen response is perfect, alleviating the burden of extensive sampling and filtering to collect perfect responses. Besides, reversal also enlarges the gap between chosen and rejected responses, thereby clarifying the optimization direction and making it more robust to noise. We evaluate RPO on two multi-turn IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively. Moreover, RPO scales effectively across model sizes (8B to 70B parameters), with the 70B RPO model surpassing GPT-4o."
  },
  {
    "title": "Attention-Enhanced Prompt Decision Transformers for UAV-Assisted Communications with AoI",
    "url": "http://arxiv.org/abs/2505.22170v1",
    "arxiv_id": "2505.22170v1",
    "authors": [
      "Chi Lu",
      "Yiyang Ni",
      "Zhe Wang",
      "Xiaoli Shi",
      "Jun Li",
      "Shi Jin"
    ],
    "published": "2025-05-28T09:41:10+00:00",
    "summary": "Decision Transformer (DT) has recently demonstrated strong generalizability in dynamic resource allocation within unmanned aerial vehicle (UAV) networks, compared to conventional deep reinforcement learning (DRL). However, its performance is hindered due to zero-padding for varying state dimensions, inability to manage long-term energy constraint, and challenges in acquiring expert samples for few-shot fine-tuning in new scenarios. To overcome these limitations, we propose an attention-enhanced prompt Decision Transformer (APDT) framework to optimize trajectory planning and user scheduling, aiming to minimize the average age of information (AoI) under long-term energy constraint in UAV-assisted Internet of Things (IoT) networks. Specifically, we enhance the convenional DT framework by incorporating an attention mechanism to accommodate varying numbers of terrestrial users, introducing a prompt mechanism based on short trajectory demonstrations for rapid adaptation to new scenarios, and designing a token-assisted method to address the UAV's long-term energy constraint. The APDT framework is first pre-trained on offline datasets and then efficiently generalized to new scenarios. Simulations demonstrate that APDT achieves twice faster in terms of convergence rate and reduces average AoI by $8\\%$ compared to conventional DT."
  },
  {
    "title": "Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL",
    "url": "http://arxiv.org/abs/2505.22151v1",
    "arxiv_id": "2505.22151v1",
    "authors": [
      "Claude Formanek",
      "Omayma Mahjoub",
      "Louay Ben Nessir",
      "Sasha Abramowitz",
      "Ruan de Kock",
      "Wiem Khlifi",
      "Simon Du Toit",
      "Felix Chalumeau",
      "Daniel Rajaonarivonivelomanantsoa",
      "Arnol Fokam",
      "Siddarth Singh",
      "Ulrich Mbou Sob",
      "Arnu Pretorius"
    ],
    "published": "2025-05-28T09:17:44+00:00",
    "summary": "A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication."
  },
  {
    "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning",
    "url": "http://arxiv.org/abs/2505.22095v1",
    "arxiv_id": "2505.22095v1",
    "authors": [
      "Chunyi Peng",
      "Zhipeng Xu",
      "Zhenghao Liu",
      "Yishan Li",
      "Yukun Yan",
      "Shuo Wang",
      "Zhiyuan Liu",
      "Yu Gu",
      "Minghe Yu",
      "Ge Yu",
      "Maosong Sun"
    ],
    "published": "2025-05-28T08:17:57+00:00",
    "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in mitigating hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge during generation. Existing MRAG methods typically adopt a static retrieval pipeline that fetches relevant information from multiple Knowledge Bases (KBs), followed by a refinement step. However, these approaches overlook the reasoning and planning capabilities of MLLMs to dynamically determine how to interact with different KBs during the reasoning process. To address this limitation, we propose R1-Router, a novel MRAG framework that learns to decide when and where to retrieve knowledge based on the evolving reasoning state. Specifically, R1-Router can generate follow-up queries according to the current reasoning step, routing these intermediate queries to the most suitable KB, and integrating external knowledge into a coherent reasoning trajectory to answer the original query. Furthermore, we introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored reinforcement learning algorithm that assigns step-specific rewards to optimize the reasoning behavior of MLLMs. Experimental results on various open-domain QA benchmarks across multiple modalities demonstrate that R1-Router outperforms baseline models by over 7%. Further analysis shows that R1-Router can adaptively and effectively leverage diverse KBs, reducing unnecessary retrievals and improving both efficiency and accuracy."
  },
  {
    "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.22094v1",
    "arxiv_id": "2505.22094v1",
    "authors": [
      "Tonghe Zhang",
      "Yu Chao",
      "Sicang Su",
      "Yu Wang"
    ],
    "published": "2025-05-28T08:17:16+00:00",
    "summary": "We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy's deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project Webpage: https://reinflow.github.io/"
  },
  {
    "title": "VIRAL: Vision-grounded Integration for Reward design And Learning",
    "url": "http://arxiv.org/abs/2505.22092v1",
    "arxiv_id": "2505.22092v1",
    "authors": [
      "Valentin Cuzin-Rambaud",
      "Emilien Komlenovic",
      "Alexandre Faure",
      "Bruno Yun"
    ],
    "published": "2025-05-28T08:16:09+00:00",
    "summary": "The alignment between humans and machines is a critical challenge in artificial intelligence today. Reinforcement learning, which aims to maximize a reward function, is particularly vulnerable to the risks associated with poorly designed reward functions. Recent advancements has shown that Large Language Models (LLMs) for reward generation can outperform human performance in this context. We introduce VIRAL, a pipeline for generating and refining reward functions through the use of multi-modal LLMs. VIRAL autonomously creates and interactively improves reward functions based on a given environment and a goal prompt or annotated image. The refinement process can incorporate human feedback or be guided by a description generated by a video LLM, which explains the agent's policy in video form. We evaluated VIRAL in five Gymnasium environments, demonstrating that it accelerates the learning of new behaviors while ensuring improved alignment with user intent. The source-code and demo video are available at: https://github.com/VIRAL-UCBL1/VIRAL and https://youtu.be/t4_BXugBm9Q."
  },
  {
    "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO",
    "url": "http://arxiv.org/abs/2505.22068v1",
    "arxiv_id": "2505.22068v1",
    "authors": [
      "Ran Li",
      "Shimin Di",
      "Yuchen Liu",
      "Chen Jing",
      "Yu Qiu",
      "Lei Chen"
    ],
    "published": "2025-05-28T07:47:46+00:00",
    "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at https://github.com/ranlislz/R2GRPO."
  },
  {
    "title": "Reinforced Reasoning for Embodied Planning",
    "url": "http://arxiv.org/abs/2505.22050v1",
    "arxiv_id": "2505.22050v1",
    "authors": [
      "Di Wu",
      "Jiaxin Fan",
      "Junzhe Zang",
      "Guanbo Wang",
      "Wei Yin",
      "Wenhao Li",
      "Bo Jin"
    ],
    "published": "2025-05-28T07:21:37+00:00",
    "summary": "Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI."
  },
  {
    "title": "OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning",
    "url": "http://arxiv.org/abs/2505.22039v1",
    "arxiv_id": "2505.22039v1",
    "authors": [
      "Shifang Zhao",
      "Yiheng Lin",
      "Lu Han",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "published": "2025-05-28T07:02:15+00:00",
    "summary": "While anomaly detection has made significant progress, generating detailed analyses that incorporate industrial knowledge remains a challenge. To address this gap, we introduce OmniAD, a novel framework that unifies anomaly detection and understanding for fine-grained analysis. OmniAD is a multimodal reasoner that combines visual and textual reasoning processes. The visual reasoning provides detailed inspection by leveraging Text-as-Mask Encoding to perform anomaly detection through text generation without manually selected thresholds. Following this, Visual Guided Textual Reasoning conducts comprehensive analysis by integrating visual perception. To enhance few-shot generalization, we employ an integrated training strategy that combines supervised fine-tuning (SFT) with reinforcement learning (GRPO), incorporating three sophisticated reward functions. Experimental results demonstrate that OmniAD achieves a performance of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and GPT-4o. It also shows strong results across multiple anomaly detection benchmarks. These results highlight the importance of enhancing visual perception for effective reasoning in anomaly understanding. All codes and models will be publicly available."
  },
  {
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.22019v1",
    "arxiv_id": "2505.22019v1",
    "authors": [
      "Qiuchen Wang",
      "Ruixue Ding",
      "Yu Zeng",
      "Zehui Chen",
      "Lin Chen",
      "Shihang Wang",
      "Pengjun Xie",
      "Fei Huang",
      "Feng Zhao"
    ],
    "published": "2025-05-28T06:30:51+00:00",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at \\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."
  },
  {
    "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models",
    "url": "http://arxiv.org/abs/2505.22017v1",
    "arxiv_id": "2505.22017v1",
    "authors": [
      "Siqi Fan",
      "Peng Han",
      "Shuo Shang",
      "Yequan Wang",
      "Aixin Sun"
    ],
    "published": "2025-05-28T06:24:45+00:00",
    "summary": "Large language models (LLMs) benefit from increased test-time compute, a phenomenon known as test-time scaling. However, reasoning-optimized models often overthink even simple problems, producing excessively verbose outputs and leading to low token efficiency. By comparing these models with equally sized instruct models, we identify two key causes of this verbosity: (1) reinforcement learning reduces the information density of forward reasoning, and (2) backward chain-of thought training encourages redundant and often unnecessary verification steps. Since LLMs cannot assess the difficulty of a given problem, they tend to apply the same cautious reasoning strategy across all tasks, resulting in inefficient overthinking. To address this, we propose CoThink, an embarrassingly simple pipeline: an instruct model first drafts a high-level solution outline; a reasoning model then works out the solution. We observe that CoThink enables dynamic adjustment of reasoning depth based on input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on average. With reference to the instruct model, we formally define reasoning efficiency and observe a potential reasoning efficiency scaling law in LLMs."
  },
  {
    "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples",
    "url": "http://arxiv.org/abs/2505.22002v1",
    "arxiv_id": "2505.22002v1",
    "authors": [
      "Zijing Hu",
      "Fengda Zhang",
      "Kun Kuang"
    ],
    "published": "2025-05-28T06:03:41+00:00",
    "summary": "The practical applications of diffusion models have been limited by the misalignment between generated images and corresponding text prompts. Recent studies have introduced direct preference optimization (DPO) to enhance the alignment of these models. However, the effectiveness of DPO is constrained by the issue of visual inconsistency, where the significant visual disparity between well-aligned and poorly-aligned images prevents diffusion models from identifying which factors contribute positively to alignment during fine-tuning. To address this issue, this paper introduces D-Fusion, a method to construct DPO-trainable visually consistent samples. On one hand, by performing mask-guided self-attention fusion, the resulting images are not only well-aligned, but also visually consistent with given poorly-aligned images. On the other hand, D-Fusion can retain the denoising trajectories of the resulting images, which are essential for DPO training. Extensive experiments demonstrate the effectiveness of D-Fusion in improving prompt-image alignment when applied to different reinforcement learning algorithms."
  },
  {
    "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21985v1",
    "arxiv_id": "2505.21985v1",
    "authors": [
      "Naoto Yoshida",
      "Tadahiro Taniguchi"
    ],
    "published": "2025-05-28T05:23:47+00:00",
    "summary": "In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments."
  },
  {
    "title": "Two-Stage Feature Generation with Transformer and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21978v1",
    "arxiv_id": "2505.21978v1",
    "authors": [
      "Wanfu Gao",
      "Zengyao Man",
      "Zebin He",
      "Yuhao Tang",
      "Jun Gao",
      "Kunpeng Liu"
    ],
    "published": "2025-05-28T05:11:59+00:00",
    "summary": "Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer's self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability."
  },
  {
    "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs",
    "url": "http://arxiv.org/abs/2505.21955v1",
    "arxiv_id": "2505.21955v1",
    "authors": [
      "Insu Lee",
      "Wooje Park",
      "Jaeyun Jang",
      "Minyoung Noh",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ],
    "published": "2025-05-28T04:09:42+00:00",
    "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, their narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs."
  },
  {
    "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding",
    "url": "http://arxiv.org/abs/2505.21908v1",
    "arxiv_id": "2505.21908v1",
    "authors": [
      "Hanyin Wang",
      "Zhenbang Wu",
      "Gururaj Kolar",
      "Hariprasad Korsapati",
      "Brian Bartlett",
      "Bryan Hull",
      "Jimeng Sun"
    ],
    "published": "2025-05-28T02:54:07+00:00",
    "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks."
  },
  {
    "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training",
    "url": "http://arxiv.org/abs/2505.21893v1",
    "arxiv_id": "2505.21893v1",
    "authors": [
      "Xiaomeng Yang",
      "Zhiyu Tan",
      "Junyan Wang",
      "Zhijian Zhou",
      "Hao Li"
    ],
    "published": "2025-05-28T02:11:56+00:00",
    "summary": "Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\\&M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning."
  },
  {
    "title": "A Provable Approach for End-to-End Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21852v1",
    "arxiv_id": "2505.21852v1",
    "authors": [
      "Akifumi Wachi",
      "Kohei Miyaguchi",
      "Takumi Tanabe",
      "Rei Sato",
      "Youhei Akimoto"
    ],
    "published": "2025-05-28T00:48:20+00:00",
    "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation."
  },
  {
    "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
    "url": "http://arxiv.org/abs/2505.21841v1",
    "arxiv_id": "2505.21841v1",
    "authors": [
      "Jiahui Zhu",
      "Kihyun Yu",
      "Dabeen Lee",
      "Xin Liu",
      "Honghao Wei"
    ],
    "published": "2025-05-28T00:16:34+00:00",
    "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments."
  },
  {
    "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction",
    "url": "http://arxiv.org/abs/2505.21807v1",
    "arxiv_id": "2505.21807v1",
    "authors": [
      "Tommy Xu",
      "Zhitian Zhang",
      "Xiangyu Sun",
      "Lauren Kelly Zung",
      "Hossein Hajimirsadeghi",
      "Greg Mori"
    ],
    "published": "2025-05-27T22:23:11+00:00",
    "summary": "Predictive modeling on tabular data is the cornerstone of many real-world applications. Although gradient boosting machines and some recent deep models achieve strong performance on tabular data, they often lack interpretability. On the other hand, large language models (LLMs) have demonstrated powerful capabilities to generate human-like reasoning and explanations, but remain under-performed for tabular data prediction. In this paper, we propose a new approach that leverages reasoning-based LLMs, trained using reinforcement learning, to perform more accurate and explainable predictions on tabular data. Our method introduces custom reward functions that guide the model not only toward high prediction accuracy but also toward human-understandable reasons for its predictions. Experimental results show that our model achieves promising performance on financial benchmark datasets, outperforming most existing LLMs."
  },
  {
    "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
    "url": "http://arxiv.org/abs/2505.21784v1",
    "arxiv_id": "2505.21784v1",
    "authors": [
      "Tharindu Kumarage",
      "Ninareh Mehrabi",
      "Anil Ramakrishna",
      "Xinyan Zhao",
      "Richard Zemel",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta",
      "Charith Peris"
    ],
    "published": "2025-05-27T21:34:40+00:00",
    "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE"
  },
  {
    "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.21765v1",
    "arxiv_id": "2505.21765v1",
    "authors": [
      "Sohyun An",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "published": "2025-05-27T20:59:29+00:00",
    "summary": "While recent success of large reasoning models (LRMs) significantly advanced LLMs' reasoning capability by optimizing the final answer accuracy using reinforcement learning, they may also drastically increase the output length due to overthinking, characterized by unnecessarily complex reasoning paths that waste computation and potentially degrade the performance. We hypothesize that such inefficiencies stem from LRMs' limited capability to dynamically select the proper modular reasoning strategies, termed thinking patterns at the right position. To investigate this hypothesis, we propose a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns, systematically identifying and promoting beneficial patterns that improve the answer while removing detrimental ones. Empirical analysis confirms that our optimized thinking paths yield more concise yet sufficiently informative trajectories, enhancing reasoning efficiency by reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Moreover, a non-trivial portion of originally incorrect responses are transformed into correct ones, achieving a 15.6% accuracy improvement with reduced length. Motivated by the improvement brought by the optimized thinking paths, we apply a preference optimization technique supported by a pairwise dataset contrasting suboptimal and optimal reasoning paths. Experimental evaluations across multiple mathematical reasoning benchmarks reveal that our method notably reduces computational overhead while simultaneously improving reasoning accuracy, achieving up to a 12% accuracy improvement and reducing token usage from approximately 5,000 to 3,000 tokens."
  },
  {
    "title": "Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals",
    "url": "http://arxiv.org/abs/2505.21750v1",
    "arxiv_id": "2505.21750v1",
    "authors": [
      "Vivienne Huiling Wang",
      "Tinghuai Wang",
      "Joni Pajarinen"
    ],
    "published": "2025-05-27T20:38:44+00:00",
    "summary": "Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks."
  },
  {
    "title": "Reinforcing General Reasoning without Verifiers",
    "url": "http://arxiv.org/abs/2505.21493v1",
    "arxiv_id": "2505.21493v1",
    "authors": [
      "Xiangxin Zhou",
      "Zichen Liu",
      "Anya Sims",
      "Haonan Wang",
      "Tianyu Pang",
      "Chongxuan Li",
      "Liang Wang",
      "Min Lin",
      "Chao Du"
    ],
    "published": "2025-05-27T17:56:27+00:00",
    "summary": "The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."
  },
  {
    "title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation",
    "url": "http://arxiv.org/abs/2505.21483v1",
    "arxiv_id": "2505.21483v1",
    "authors": [
      "Kerui Ren",
      "Jiayang Bai",
      "Linning Xu",
      "Lihan Jiang",
      "Jiangmiao Pang",
      "Mulin Yu",
      "Bo Dai"
    ],
    "published": "2025-05-27T17:53:02+00:00",
    "summary": "Object compositing offers significant promise for augmented reality (AR) and embodied intelligence applications. Existing approaches predominantly focus on single-image scenarios or intrinsic decomposition techniques, facing challenges with multi-view consistency, complex scenes, and diverse lighting conditions. Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based methods, have enhanced consistency but are limited by scalability, heavy data requirements, or prolonged reconstruction time per scene. To broaden its applicability, we introduce MV-CoLight, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. Our novel feed-forward architecture models lighting and shadows directly, avoiding the iterative biases of diffusion-based methods. We employ a Hilbert curve-based mapping to align 2D image inputs with 3D Gaussian scene representations seamlessly. To facilitate training and evaluation, we further introduce a large-scale 3D compositing dataset. Experiments demonstrate state-of-the-art harmonized results across standard benchmarks and our dataset, as well as casually captured real-world scenes demonstrate the framework's robustness and wide generalization."
  },
  {
    "title": "Policy Optimized Text-to-Image Pipeline Design",
    "url": "http://arxiv.org/abs/2505.21478v1",
    "arxiv_id": "2505.21478v1",
    "authors": [
      "Uri Gadot",
      "Rinon Gal",
      "Yftah Ziser",
      "Gal Chechik",
      "Shie Mannor"
    ],
    "published": "2025-05-27T17:50:47+00:00",
    "summary": "Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines."
  },
  {
    "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO",
    "url": "http://arxiv.org/abs/2505.21457v1",
    "arxiv_id": "2505.21457v1",
    "authors": [
      "Muzhi Zhu",
      "Hao Zhong",
      "Canyu Zhao",
      "Zongze Du",
      "Zheng Huang",
      "Mingyu Liu",
      "Hao Chen",
      "Cheng Zou",
      "Jingdong Chen",
      "Ming Yang",
      "Chunhua Shen"
    ],
    "published": "2025-05-27T17:29:31+00:00",
    "summary": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs."
  },
  {
    "title": "Can Large Reasoning Models Self-Train?",
    "url": "http://arxiv.org/abs/2505.21444v1",
    "arxiv_id": "2505.21444v1",
    "authors": [
      "Sheikh Shafayat",
      "Fahim Tajwar",
      "Ruslan Salakhutdinov",
      "Jeff Schneider",
      "Andrea Zanette"
    ],
    "published": "2025-05-27T17:16:00+00:00",
    "summary": "Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. We propose an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges."
  },
  {
    "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
    "url": "http://arxiv.org/abs/2505.21414v1",
    "arxiv_id": "2505.21414v1",
    "authors": [
      "Brett Bissey",
      "Kyle Gatesman",
      "Walker Dimon",
      "Mohammad Alam",
      "Luis Robaina",
      "Joseph Weissman"
    ],
    "published": "2025-05-27T16:41:23+00:00",
    "summary": "This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments."
  },
  {
    "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents",
    "url": "http://arxiv.org/abs/2505.21410v1",
    "arxiv_id": "2505.21410v1",
    "authors": [
      "Shashank Sharma",
      "Janina Hoffmann",
      "Vinay Namboodiri"
    ],
    "published": "2025-05-27T16:38:55+00:00",
    "summary": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve long-horizon tasks efficiently. While existing skill discovery methods learns these skills automatically, they are limited to a single skill per task. In contrast, humans learn and use both fine-grained and coarse motor skills simultaneously. Inspired by human motor control, we propose Multi-Resolution Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at different temporal resolutions in parallel. A high-level manager dynamically selects among these skills, enabling adaptive control strategies over time. We evaluate MRSD on tasks from the DeepMind Control Suite and show that it outperforms prior state-of-the-art skill discovery and HRL methods, achieving faster convergence and higher final performance. Our findings highlight the benefits of integrating multi-resolution skills in HRL, paving the way for more versatile and efficient agents."
  },
  {
    "title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features",
    "url": "http://arxiv.org/abs/2505.21391v1",
    "arxiv_id": "2505.21391v1",
    "authors": [
      "Zixuan Xie",
      "Xinyu Liu",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "published": "2025-05-27T16:17:49+00:00",
    "summary": "Linear TD($\\lambda$) is one of the most fundamental reinforcement learning algorithms for policy evaluation. Previously, convergence rates are typically established under the assumption of linearly independent features, which does not hold in many practical scenarios. This paper instead establishes the first $L^2$ convergence rates for linear TD($\\lambda$) operating under arbitrary features, without making any algorithmic modification or additional assumptions. Our results apply to both the discounted and average-reward settings. To address the potential non-uniqueness of solutions resulting from arbitrary features, we develop a novel stochastic approximation result featuring convergence rates to the solution set instead of a single point."
  },
  {
    "title": "Assured Autonomy with Neuro-Symbolic Perception",
    "url": "http://arxiv.org/abs/2505.21322v1",
    "arxiv_id": "2505.21322v1",
    "authors": [
      "R. Spencer Hallyburton",
      "Miroslav Pajic"
    ],
    "published": "2025-05-27T15:21:06+00:00",
    "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS), while highly accurate, are simply pattern-matchers.~With limited security guarantees, there are concerns for their reliability in safety-critical and contested domains. To advance assured AI, we advocate for a paradigm shift that imbues data-driven perception models with symbolic structure, inspired by a human's ability to reason over low-level features and high-level context. We propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how joint object detection and scene graph generation (SGG) yields deep scene understanding.~Powered by foundation models for offline knowledge extraction and specialized SGG algorithms for real-time deployment, we design a framework leveraging structured relational graphs that ensures the integrity of situational awareness in autonomy. Using physics-based simulators and real-world datasets, we demonstrate how SGG bridges the gap between low-level sensor perception and high-level reasoning, establishing a foundation for resilient, context-aware AI and advancing trusted autonomy in CPS."
  },
  {
    "title": "Data-Driven Cellular Mobility Management via Bayesian Optimization and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21249v1",
    "arxiv_id": "2505.21249v1",
    "authors": [
      "Mohamed Benzaghta",
      "Sahar Ammar",
      "David L\u00f3pez-P\u00e9rez",
      "Basem Shihada",
      "Giovanni Geraci"
    ],
    "published": "2025-05-27T14:26:59+00:00",
    "summary": "Mobility management in cellular networks faces increasing complexity due to network densification and heterogeneous user mobility characteristics. Traditional handover (HO) mechanisms, which rely on predefined parameters such as A3-offset and time-to-trigger (TTT), often fail to optimize mobility performance across varying speeds and deployment conditions. Fixed A3-offset and TTT configurations either delay HOs, increasing radio link failures (RLFs), or accelerate them, leading to excessive ping-pong effects. To address these challenges, we propose two data-driven mobility management approaches leveraging high-dimensional Bayesian optimization (HD-BO) and deep reinforcement learning (DRL). HD-BO optimizes HO parameters such as A3-offset and TTT, striking a desired trade-off between ping-pongs vs. RLF. DRL provides a non-parameter-based approach, allowing an agent to select serving cells based on real-time network conditions. We validate our approach using a real-world cellular deployment scenario, and employing Sionna ray tracing for site-specific channel propagation modeling. Results show that both HD-BO and DRL outperform 3GPP set-1 (TTT of 480 ms and A3-offset of 3 dB) and set-5 (TTT of 40 ms and A3-offset of -1 dB) benchmarks. We augment HD-BO with transfer learning so it can generalize across a range of user speeds. Applying the same transfer-learning strategy to the DRL method reduces its training time by a factor of 2.5 while preserving optimal HO performance, showing that it adapts efficiently to the mobility of aerial users such as UAVs. Simulations further reveal that HD-BO remains more sample-efficient than DRL, making it more suitable for scenarios with limited training data."
  },
  {
    "title": "Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies",
    "url": "http://arxiv.org/abs/2505.21236v1",
    "arxiv_id": "2505.21236v1",
    "authors": [
      "Felix Chalumeau",
      "Daniel Rajaonarivonivelomanantsoa",
      "Ruan de Kock",
      "Claude Formanek",
      "Sasha Abramowitz",
      "Oumayma Mahjoub",
      "Wiem Khlifi",
      "Simon Du Toit",
      "Louay Ben Nessir",
      "Refiloe Shabe",
      "Arnol Fokam",
      "Siddarth Singh",
      "Ulrich Mbou Sob",
      "Arnu Pretorius"
    ],
    "published": "2025-05-27T14:19:06+00:00",
    "summary": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inf-marl."
  },
  {
    "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21178v1",
    "arxiv_id": "2505.21178v1",
    "authors": [
      "Mingyang Song",
      "Mao Zheng"
    ],
    "published": "2025-05-27T13:29:51+00:00",
    "summary": "As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the \"walk before you run\" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks."
  },
  {
    "title": "TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment",
    "url": "http://arxiv.org/abs/2505.21172v1",
    "arxiv_id": "2505.21172v1",
    "authors": [
      "Zheng Li",
      "Mao Zheng",
      "Mingyang Song",
      "Wenjie Yang"
    ],
    "published": "2025-05-27T13:26:02+00:00",
    "summary": "Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have made significant progress in tasks such as mathematics and coding. Inspired by this, several studies have employed reinforcement learning(RL) to enhance models' deep reasoning capabilities and improve machine translation(MT) quality. However, the terminology translation, an essential task in MT, remains unexplored in deep reasoning LLMs. In this paper, we propose \\textbf{TAT-R1}, a terminology-aware translation model trained with reinforcement learning and word alignment. Specifically, we first extract the keyword translation pairs using a word alignment model. Then we carefully design three types of rule-based alignment rewards with the extracted alignment relationships. With those alignment rewards, the RL-trained translation model can learn to focus on the accurate translation of key information, including terminology in the source text. Experimental results show the effectiveness of TAT-R1. Our model significantly improves terminology translation accuracy compared to the baseline models while maintaining comparable performance on general translation tasks. In addition, we conduct detailed ablation studies of the DeepSeek-R1-like training paradigm for machine translation and reveal several key findings."
  },
  {
    "title": "Quantum AIXI: Universal Intelligence via Quantum Information",
    "url": "http://arxiv.org/abs/2505.21170v1",
    "arxiv_id": "2505.21170v1",
    "authors": [
      "Elija Perrier"
    ],
    "published": "2025-05-27T13:23:53+00:00",
    "summary": "AIXI is a widely studied model of artificial general intelligence (AGI) based upon principles of induction and reinforcement learning. However, AIXI is fundamentally classical in nature - as are the environments in which it is modelled. Given the universe is quantum mechanical in nature and the exponential overhead required to simulate quantum mechanical systems classically, the question arises as to whether there are quantum mechanical analogues of AIXI which are theoretically consistent or practically feasible as models of universal intelligence. To address this question, we extend the framework to quantum information and present Quantum AIXI (QAIXI). We introduce a model of quantum agent/environment interaction based upon quantum and classical registers and channels, showing how quantum AIXI agents may take both classical and quantum actions. We formulate the key components of AIXI in quantum information terms, extending previous research on quantum Kolmogorov complexity and a QAIXI value function. We discuss conditions and limitations upon quantum Solomonoff induction and show how contextuality fundamentally affects QAIXI models."
  },
  {
    "title": "Universal Value-Function Uncertainties",
    "url": "http://arxiv.org/abs/2505.21119v1",
    "arxiv_id": "2505.21119v1",
    "authors": [
      "Moritz A. Zanger",
      "Max Weltevrede",
      "Yaniv Oren",
      "Pascal R. Van der Vaart",
      "Caroline Horsch",
      "Wendelin B\u00f6hmer",
      "Matthijs T. J. Spaan"
    ],
    "published": "2025-05-27T12:38:19+00:00",
    "summary": "Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings."
  },
  {
    "title": "Thinker: Learning to Think Fast and Slow",
    "url": "http://arxiv.org/abs/2505.21097v1",
    "arxiv_id": "2505.21097v1",
    "authors": [
      "Stephen Chung",
      "Wenyu Du",
      "Jie Fu"
    ],
    "published": "2025-05-27T12:22:46+00:00",
    "summary": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."
  },
  {
    "title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning",
    "url": "http://arxiv.org/abs/2505.21067v1",
    "arxiv_id": "2505.21067v1",
    "authors": [
      "Xiao Hu",
      "Xingyu Lu",
      "Liyuan Mao",
      "YiFan Zhang",
      "Tianke Zhang",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Guorui Zhou"
    ],
    "published": "2025-05-27T11:52:41+00:00",
    "summary": "Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \\textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation method based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems, while zero-RL fails to significantly boost the frequency of these behaviors."
  },
  {
    "title": "LPOI: Listwise Preference Optimization for Vision Language Models",
    "url": "http://arxiv.org/abs/2505.21061v1",
    "arxiv_id": "2505.21061v1",
    "authors": [
      "Fatemeh Pesaran Zadeh",
      "Yoojin Oh",
      "Gunhee Kim"
    ],
    "published": "2025-05-27T11:47:28+00:00",
    "summary": "Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance. We make the code available at https://github.com/fatemehpesaran310/lpoi."
  },
  {
    "title": "Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking",
    "url": "http://arxiv.org/abs/2505.21045v1",
    "arxiv_id": "2505.21045v1",
    "authors": [
      "Lingyi Cai",
      "Ruichen Zhang",
      "Changyuan Zhao",
      "Yu Zhang",
      "Jiawen Kang",
      "Dusit Niyato",
      "Tao Jiang",
      "Xuemin Shen"
    ],
    "published": "2025-05-27T11:25:42+00:00",
    "summary": "Low-Altitude Economic Networking (LAENet) aims to support diverse flying applications below 1,000 meters by deploying various aerial vehicles for flexible and cost-effective aerial networking. However, complex decision-making, resource constraints, and environmental uncertainty pose significant challenges to the development of the LAENet. Reinforcement learning (RL) offers a potential solution in response to these challenges but has limitations in generalization, reward design, and model stability. The emergence of large language models (LLMs) offers new opportunities for RL to mitigate these limitations. In this paper, we first present a tutorial about integrating LLMs into RL by using the capacities of generation, contextual understanding, and structured reasoning of LLMs. We then propose an LLM-enhanced RL framework for the LAENet in terms of serving the LLM as information processor, reward designer, decision-maker, and generator. Moreover, we conduct a case study by using LLMs to design a reward function to improve the learning performance of RL in the LAENet. Finally, we provide a conclusion and discuss future work."
  },
  {
    "title": "Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.21026v1",
    "arxiv_id": "2505.21026v1",
    "authors": [
      "Runze Lin",
      "Junghui Chen",
      "Biao Huang",
      "Lei Xie",
      "Hongye Su"
    ],
    "published": "2025-05-27T11:01:00+00:00",
    "summary": "In the era of Industry 4.0 and smart manufacturing, process systems engineering must adapt to digital transformation. While reinforcement learning offers a model-free approach to process control, its applications are limited by the dependence on accurate digital twins and well-designed reward functions. To address these limitations, this paper introduces a novel framework that integrates inverse reinforcement learning (IRL) with multi-task learning for data-driven, multi-mode control design. Using historical closed-loop data as expert demonstrations, IRL extracts optimal reward functions and control policies. A latent-context variable is incorporated to distinguish modes, enabling the training of mode-specific controllers. Case studies on a continuous stirred tank reactor and a fed-batch bioreactor validate the effectiveness of this framework in handling multi-mode data and training adaptable controllers."
  },
  {
    "title": "DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2505.20975v1",
    "arxiv_id": "2505.20975v1",
    "authors": [
      "Shamil Ayupov",
      "Maksim Nakhodnov",
      "Anastasia Yaschenko",
      "Andrey Kuznetsov",
      "Aibek Alanov"
    ],
    "published": "2025-05-27T10:07:50+00:00",
    "summary": "Personalized diffusion models have shown remarkable success in Text-to-Image (T2I) generation by enabling the injection of user-defined concepts into diverse contexts. However, balancing concept fidelity with contextual alignment remains a challenging open problem. In this work, we propose an RL-based approach that leverages the diverse outputs of T2I models to address this issue. Our method eliminates the need for human-annotated scores by generating a synthetic paired dataset for DPO-like training using external quality metrics. These better-worse pairs are specifically constructed to improve both concept fidelity and prompt adherence. Moreover, our approach supports flexible adjustment of the trade-off between image fidelity and textual alignment. Through multi-step training, our approach outperforms a naive baseline in convergence speed and output quality. We conduct extensive qualitative and quantitative analysis, demonstrating the effectiveness of our method across various architectures and fine-tuning techniques. The source code can be found at https://github.com/ControlGenAI/DreamBoothDPO."
  },
  {
    "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs",
    "url": "http://arxiv.org/abs/2505.20948v1",
    "arxiv_id": "2505.20948v1",
    "authors": [
      "Yisen Gao",
      "Jiaxin Bai",
      "Tianshi Zheng",
      "Qingyun Sun",
      "Ziwei Zhang",
      "Jianxin Li",
      "Yangqiu Song",
      "Xingcheng Fu"
    ],
    "published": "2025-05-27T09:36:47+00:00",
    "summary": "Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines."
  },
  {
    "title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective",
    "url": "http://arxiv.org/abs/2505.20922v1",
    "arxiv_id": "2505.20922v1",
    "authors": [
      "Yang Zhang",
      "Xinran Li",
      "Jianing Ye",
      "Delin Qu",
      "Shuang Qiu",
      "Chongjie Zhang",
      "Xiu Li",
      "Chenjia Bai"
    ],
    "published": "2025-05-27T09:11:38+00:00",
    "summary": "World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research."
  },
  {
    "title": "Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment",
    "url": "http://arxiv.org/abs/2505.20889v1",
    "arxiv_id": "2505.20889v1",
    "authors": [
      "Leizhen Wang",
      "Peibo Duan",
      "Cheng Lyu",
      "Zhenliang Ma"
    ],
    "published": "2025-05-27T08:33:02+00:00",
    "summary": "Modern navigation systems and shared mobility platforms increasingly rely on personalized route recommendations to improve individual travel experience and operational efficiency. However, a key question remains: can such sequential, personalized routing decisions collectively lead to system-optimal (SO) traffic assignment? This paper addresses this question by proposing a learning-based framework that reformulates the static SO traffic assignment problem as a single-agent deep reinforcement learning (RL) task. A central agent sequentially recommends routes to travelers as origin-destination (OD) demands arrive, to minimize total system travel time. To enhance learning efficiency and solution quality, we develop an MSA-guided deep Q-learning algorithm that integrates the iterative structure of traditional traffic assignment methods into the RL training process. The proposed approach is evaluated on both the Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent converges to the theoretical SO solution in the Braess network and achieves only a 0.35% deviation in the OW network. Further ablation studies demonstrate that the route action set's design significantly impacts convergence speed and final performance, with SO-informed route sets leading to faster learning and better outcomes. This work provides a theoretically grounded and practically relevant approach to bridging individual routing behavior with system-level efficiency through learning-based sequential assignment."
  },
  {
    "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models",
    "url": "http://arxiv.org/abs/2505.20888v1",
    "arxiv_id": "2505.20888v1",
    "authors": [
      "Chengyu Wang",
      "Junbing Yan",
      "Wenrui Cai",
      "Yuanhao Yue",
      "Jun Huang"
    ],
    "published": "2025-05-27T08:32:51+00:00",
    "summary": "In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community."
  },
  {
    "title": "Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG",
    "url": "http://arxiv.org/abs/2505.20871v1",
    "arxiv_id": "2505.20871v1",
    "authors": [
      "Xin Sun",
      "Jianan Xie",
      "Zhongqi Chen",
      "Qiang Liu",
      "Shu Wu",
      "Yuehe Chen",
      "Bowen Song",
      "Weiqiang Wang",
      "Zilei Wang",
      "Liang Wang"
    ],
    "published": "2025-05-27T08:21:21+00:00",
    "summary": "Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with \"I don't know\" when the query is out of the knowledge boundary of both the retrieved passages and the model's internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that DTA effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems."
  },
  {
    "title": "Learning Unified Force and Position Control for Legged Loco-Manipulation",
    "url": "http://arxiv.org/abs/2505.20829v1",
    "arxiv_id": "2505.20829v1",
    "authors": [
      "Peiyuan Zhi",
      "Peiyang Li",
      "Jianqin Yin",
      "Baoxiong Jia",
      "Siyuan Huang"
    ],
    "published": "2025-05-27T07:44:35+00:00",
    "summary": "Robotic loco-manipulation tasks often involve contact-rich interactions with the environment, requiring the joint modeling of contact force and robot position. However, recent visuomotor policies often focus solely on learning position or force control, overlooking their co-learning. In this work, we propose the first unified policy for legged robots that jointly models force and position control learned without reliance on force sensors. By simulating diverse combinations of position and force commands alongside external disturbance forces, we use reinforcement learning to learn a policy that estimates forces from historical robot states and compensates for them through position and velocity adjustments. This policy enables a wide range of manipulation behaviors under varying force and position inputs, including position tracking, force application, force tracking, and compliant interactions. Furthermore, we demonstrate that the learned policy enhances trajectory-based imitation learning pipelines by incorporating essential contact information through its force estimation module, achieving approximately 39.5% higher success rates across four challenging contact-rich manipulation tasks compared to position-control policies. Extensive experiments on both a quadrupedal manipulator and a humanoid robot validate the versatility and robustness of the proposed policy across diverse scenarios."
  },
  {
    "title": "Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2505.20825v1",
    "arxiv_id": "2505.20825v1",
    "authors": [
      "Yuhao Wang",
      "Ruiyang Ren",
      "Yucheng Wang",
      "Wayne Xin Zhao",
      "Jing Liu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "published": "2025-05-27T07:34:41+00:00",
    "summary": "Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at https://github.com/RUCAIBox/RioRAG."
  },
  {
    "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
    "url": "http://arxiv.org/abs/2505.20793v1",
    "arxiv_id": "2505.20793v1",
    "authors": [
      "Juan A. Rodriguez",
      "Haotian Zhang",
      "Abhay Puri",
      "Aarash Feizi",
      "Rishav Pramanik",
      "Pascal Wichmann",
      "Arnab Mondal",
      "Mohammad Reza Samsami",
      "Rabiul Awal",
      "Perouz Taslakian",
      "Spandana Gella",
      "Sai Rajeswar",
      "David Vazquez",
      "Christopher Pal",
      "Marco Pedersoli"
    ],
    "published": "2025-05-27T06:56:00+00:00",
    "summary": "Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization."
  },
  {
    "title": "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs",
    "url": "http://arxiv.org/abs/2505.20777v1",
    "arxiv_id": "2505.20777v1",
    "authors": [
      "Zhehan Kan",
      "Yanlin Liu",
      "Kun Yin",
      "Xinghua Jiang",
      "Xin Li",
      "Haoyu Cao",
      "Yinsong Liu",
      "Deqiang Jiang",
      "Xing Sun",
      "Qingmin Liao",
      "Wenming Yang"
    ],
    "published": "2025-05-27T06:30:48+00:00",
    "summary": "DeepSeek R1 has significantly advanced complex reasoning for large language models (LLMs). While recent methods have attempted to replicate R1's reasoning capabilities in multimodal settings, they face limitations, including inconsistencies between reasoning and final answers, model instability and crashes during long-chain exploration, and low data learning efficiency. To address these challenges, we propose TACO, a novel reinforcement learning algorithm for visual reasoning. Building on Generalized Reinforcement Policy Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly couples reasoning with answer consistency to ensure answers are grounded in thoughtful reasoning. We also introduce the Rollback Resample Strategy, which adaptively removes problematic samples and reintroduces them to the sampler, enabling stable long-chain exploration and future learning opportunities. Additionally, TACO employs an adaptive learning schedule that focuses on moderate difficulty samples to optimize data efficiency. Furthermore, we propose the Test-Time-Resolution-Scaling scheme to address performance degradation due to varying resolutions during reasoning while balancing computational overhead. Extensive experiments on in-distribution and out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning LVLMs leads to significant performance improvements."
  },
  {
    "title": "Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform",
    "url": "http://arxiv.org/abs/2505.20751v1",
    "arxiv_id": "2505.20751v1",
    "authors": [
      "Zongcai Tan amd Dandan Zhang"
    ],
    "published": "2025-05-27T05:50:11+00:00",
    "summary": "Optical tweezers (OT) offer unparalleled capabilities for micromanipulation with submicron precision in biomedical applications. However, controlling conventional multi-trap OT to achieve cooperative manipulation of multiple complex-shaped microrobots in dynamic environments poses a significant challenge. To address this, we introduce Interactive OT Gym, a reinforcement learning (RL)-based simulation platform designed for OT-driven microrobotics. Our platform supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks. This integration allows for an adaptive blend of manual and autonomous control, enabling seamless transitions between human input and autonomous operation. We evaluated the effectiveness of our platform using a cell manipulation task. Experimental results show that our shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate. With its high fidelity, interactivity, low cost, and high-speed simulation capabilities, Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms. For more details on the project, please see our website https://sites.google.com/view/otgym"
  },
  {
    "title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories",
    "url": "http://arxiv.org/abs/2505.20737v1",
    "arxiv_id": "2505.20737v1",
    "authors": [
      "Zilong Wang",
      "Jingfeng Yang",
      "Sreyashi Nag",
      "Samarth Varshney",
      "Xianfeng Tang",
      "Haoming Jiang",
      "Jingbo Shang",
      "Sheikh Muhammad Sarwar"
    ],
    "published": "2025-05-27T05:27:54+00:00",
    "summary": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost."
  },
  {
    "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution",
    "url": "http://arxiv.org/abs/2505.20732v1",
    "arxiv_id": "2505.20732v1",
    "authors": [
      "Hanlin Wang",
      "Chak Tou Leong",
      "Jiashuo Wang",
      "Jian Wang",
      "Wenjie Li"
    ],
    "published": "2025-05-27T05:21:04+00:00",
    "summary": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent."
  },
  {
    "title": "A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs",
    "url": "http://arxiv.org/abs/2505.20725v1",
    "arxiv_id": "2505.20725v1",
    "authors": [
      "Alberto Pliego Marug\u00e1n",
      "Jes\u00fas M. Pinar-P\u00e9rez",
      "Fausto Pedro Garc\u00eda M\u00e1rquez"
    ],
    "published": "2025-05-27T05:14:29+00:00",
    "summary": "Efficient maintenance has always been essential for the successful application of engineering systems. However, the challenges to be overcome in the implementation of Industry 4.0 necessitate new paradigms of maintenance optimization. Machine learning techniques are becoming increasingly used in engineering and maintenance, with reinforcement learning being one of the most promising. In this paper, we propose a gamma degradation process together with a novel maintenance model in which repairs are increasingly imperfect, i.e., the beneficial effect of system repairs decreases as more repairs are performed, reflecting the degradational behavior of real-world systems. To generate maintenance policies for this system, we developed a reinforcement-learning-based agent using a Double Deep Q-Network architecture. This agent presents two important advantages: it works without a predefined preventive threshold, and it can operate in a continuous degradation state space. Our agent learns to behave in different scenarios, showing great flexibility. In addition, we performed an analysis of how changes in the main parameters of the environment affect the maintenance policy proposed by the agent. The proposed approach is demonstrated to be appropriate and to significatively improve long-run cost as compared with other common maintenance strategies."
  },
  {
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding",
    "url": "http://arxiv.org/abs/2505.20715v1",
    "arxiv_id": "2505.20715v1",
    "authors": [
      "Fuwen Luo",
      "Shengfeng Lou",
      "Chi Chen",
      "Ziyue Wang",
      "Chenliang Li",
      "Weizhou Shen",
      "Jiyue Guo",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Yang Liu"
    ],
    "published": "2025-05-27T04:50:07+00:00",
    "summary": "Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in effectiveness. In this work, we propose MUSEG, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios. View our project at https://github.com/THUNLP-MT/MUSEG."
  },
  {
    "title": "Hierarchical Instruction-aware Embodied Visual Tracking",
    "url": "http://arxiv.org/abs/2505.20710v1",
    "arxiv_id": "2505.20710v1",
    "authors": [
      "Kui Wu",
      "Hao Chen",
      "Churan Wang",
      "Fakhri Karray",
      "Zhoujun Li",
      "Yizhou Wang",
      "Fangwei Zhong"
    ],
    "published": "2025-05-27T04:36:26+00:00",
    "summary": "User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for reinforcement learning-based models due to the substantial gap between high-level user instructions and low-level agent actions. While recent advancements in language models (e.g., LLMs, VLMs, VLAs) have improved instruction comprehension, these models face critical limitations in either inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To address these challenges, we propose \\textbf{Hierarchical Instruction-aware Embodied Visual Tracking (HIEVT)} agent, which bridges instruction comprehension and action generation using \\textit{spatial goals} as intermediaries. HIEVT first introduces \\textit{LLM-based Semantic-Spatial Goal Aligner} to translate diverse human instructions into spatial goals that directly annotate the desired spatial position. Then the \\textit{RL-based Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to position the target as specified by the spatial goal. To benchmark UC-EVT tasks, we collect over ten million trajectories for training and evaluate across one seen environment and nine unseen challenging environments. Extensive experiments and real-world deployments demonstrate the robustness and generalizability of HIEVT across diverse environments, varying target dynamics, and complex instruction combinations. The complete project is available at https://sites.google.com/view/hievt."
  },
  {
    "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
    "url": "http://arxiv.org/abs/2505.20686v1",
    "arxiv_id": "2505.20686v1",
    "authors": [
      "Kiant\u00e9 Brantley",
      "Mingyu Chen",
      "Zhaolin Gao",
      "Jason D. Lee",
      "Wen Sun",
      "Wenhao Zhan",
      "Xuezhou Zhang"
    ],
    "published": "2025-05-27T03:58:50+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at https://github.com/ZhaolinGao/A-PO."
  },
  {
    "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation",
    "url": "http://arxiv.org/abs/2505.20671v1",
    "arxiv_id": "2505.20671v1",
    "authors": [
      "Heng Tan",
      "Hua Yan",
      "Yu Yang"
    ],
    "published": "2025-05-27T03:40:02+00:00",
    "summary": "While reinforcement learning (RL) has achieved notable success in various domains, training effective policies for complex tasks remains challenging. Agents often converge to local optima and fail to maximize long-term rewards. Existing approaches to mitigate training bottlenecks typically fall into two categories: (i) Automated policy refinement, which identifies critical states from past trajectories to guide policy updates, but suffers from costly and uncertain model training; and (ii) Human-in-the-loop refinement, where human feedback is used to correct agent behavior, but this does not scale well to environments with large or continuous action spaces. In this work, we design a large language model-guided policy modulation framework that leverages LLMs to improve RL training without additional model training or human intervention. We first prompt an LLM to identify critical states from a sub-optimal agent's trajectories. Based on these states, the LLM then provides action suggestions and assigns implicit rewards to guide policy refinement. Experiments across standard RL benchmarks demonstrate that our method outperforms state-of-the-art baselines, highlighting the effectiveness of LLM-based explanations in addressing RL training bottlenecks."
  },
  {
    "title": "An Optimisation Framework for Unsupervised Environment Design",
    "url": "http://arxiv.org/abs/2505.20659v1",
    "arxiv_id": "2505.20659v1",
    "authors": [
      "Nathan Monette",
      "Alistair Letcher",
      "Michael Beukman",
      "Matthew T. Jackson",
      "Alexander Rutherford",
      "Alexander D. Goldie",
      "Jakob N. Foerster"
    ],
    "published": "2025-05-27T03:07:26+00:00",
    "summary": "For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties."
  },
  {
    "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation",
    "url": "http://arxiv.org/abs/2505.20622v1",
    "arxiv_id": "2505.20622v1",
    "authors": [
      "Ting Xu",
      "Zhichao Huang",
      "Jiankai Sun",
      "Shanbo Cheng",
      "Wai Lam"
    ],
    "published": "2025-05-27T01:59:58+00:00",
    "summary": "We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct."
  },
  {
    "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20621v1",
    "arxiv_id": "2505.20621v1",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "published": "2025-05-27T01:59:25+00:00",
    "summary": "Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly improving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL."
  },
  {
    "title": "Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion",
    "url": "http://arxiv.org/abs/2505.20619v1",
    "arxiv_id": "2505.20619v1",
    "authors": [
      "Tianhu Peng",
      "Lingfan Bao",
      "CHengxu Zhou"
    ],
    "published": "2025-05-27T01:54:47+00:00",
    "summary": "We present a unified gait-conditioned reinforcement learning framework that enables humanoid robots to perform standing, walking, running, and smooth transitions within a single recurrent policy. A compact reward routing mechanism dynamically activates gait-specific objectives based on a one-hot gait ID, mitigating reward interference and supporting stable multi-gait learning. Human-inspired reward terms promote biomechanically natural motions, such as straight-knee stance and coordinated arm-leg swing, without requiring motion capture data. A structured curriculum progressively introduces gait complexity and expands command space over multiple phases. In simulation, the policy successfully achieves robust standing, walking, running, and gait transitions. On the real Unitree G1 humanoid, we validate standing, walking, and walk-to-stand transitions, demonstrating stable and coordinated locomotion. This work provides a scalable, reference-free solution toward versatile and naturalistic humanoid control across diverse modes and environments."
  },
  {
    "title": "The challenge of hidden gifts in multi-agent reinforcement learning",
    "url": "http://arxiv.org/abs/2505.20579v1",
    "arxiv_id": "2505.20579v1",
    "authors": [
      "Dane Malenfant",
      "Blake A. Richards"
    ],
    "published": "2025-05-26T23:28:52+00:00",
    "summary": "Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These \"hidden gifts\" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a \"hidden gift\". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of \"hidden gifts\", and demonstrate that learning awareness in independent agents can benefit these settings."
  },
  {
    "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL",
    "url": "http://arxiv.org/abs/2505.20578v1",
    "arxiv_id": "2505.20578v1",
    "authors": [
      "Xingyu Chen",
      "Shihao Ma",
      "Runsheng Lin",
      "Jiecong Lin",
      "Bo Wang"
    ],
    "published": "2025-05-26T23:27:50+00:00",
    "summary": "Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences."
  },
  {
    "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners",
    "url": "http://arxiv.org/abs/2505.20573v1",
    "arxiv_id": "2505.20573v1",
    "authors": [
      "Jiabao Ji",
      "Yongchao Chen",
      "Yang Zhang",
      "Ramana Rao Kompella",
      "Chuchu Fan",
      "Gaowen Liu",
      "Shiyu Chang"
    ],
    "published": "2025-05-26T23:14:16+00:00",
    "summary": "Large language models (LLMs) have demonstrated strong performance in various robot control tasks. However, their deployment in real-world applications remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently produce invalid action plans that violate physical constraints, such as directing a robot to an unreachable location or causing collisions between robots. This issue primarily arises from a lack of awareness of these physical constraints during the reasoning process. To address this issue, we propose a novel framework that integrates reinforcement learning with verifiable rewards (RLVR) to incentivize knowledge of physical constraints into LLMs to induce constraints-aware reasoning during plan generation. In this approach, only valid action plans that successfully complete a control task receive positive rewards. We applied our method to two small-scale LLMs: a non-reasoning Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results demonstrate that constraint-aware small LLMs largely outperform large-scale models without constraints, grounded on both the BoxNet task and a newly developed BoxNet3D environment built using MuJoCo. This work highlights the effectiveness of grounding even small LLMs with physical constraints to enable scalable and efficient multi-robot control in complex, physically constrained environments."
  },
  {
    "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.20561v1",
    "arxiv_id": "2505.20561v1",
    "authors": [
      "Shenao Zhang",
      "Yaqing Wang",
      "Yinxiao Liu",
      "Tianqi Liu",
      "Peter Grabowski",
      "Eugene Ie",
      "Zhaoran Wang",
      "Yunxuan Li"
    ],
    "published": "2025-05-26T22:51:00+00:00",
    "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL."
  },
  {
    "title": "Learning a Pessimistic Reward Model in RLHF",
    "url": "http://arxiv.org/abs/2505.20556v1",
    "arxiv_id": "2505.20556v1",
    "authors": [
      "Yinglun Xu",
      "Hangoo Kang",
      "Tarun Suresh",
      "Yuxuan Wan",
      "Gagandeep Singh"
    ],
    "published": "2025-05-26T22:34:42+00:00",
    "summary": "This work proposes `PET', a novel pessimistic reward fine-tuning method, to learn a pessimistic reward model robust against reward hacking in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF train an imperfect reward model, on which a KL regularization plays a pivotal role in mitigating reward hacking when optimizing a policy. Such an intuition-based method still suffers from reward hacking, and the policies with large KL divergence from the dataset distribution are excluded during learning. In contrast, we show that when optimizing a policy on a pessimistic reward model fine-tuned through PET, reward hacking can be prevented without relying on any regularization. We test our methods on the standard TL;DR summarization dataset. We find that one can learn a high-quality policy on our pessimistic reward without using any regularization. Such a policy has a high KL divergence from the dataset distribution while having high performance in practice. In summary, our work shows the feasibility of learning a pessimistic reward model against reward hacking. The agent can greedily search for the policy with a high pessimistic reward without suffering from reward hacking."
  },
  {
    "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF",
    "url": "http://arxiv.org/abs/2505.20417v1",
    "arxiv_id": "2505.20417v1",
    "authors": [
      "Meng Cao",
      "Shuyuan Zhang",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "published": "2025-05-26T18:06:52+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is a widely used technique for aligning Large Language Models (LLMs) with human preferences, yet it often suffers from sparse reward signals, making effective credit assignment challenging. In typical setups, the reward model provides a single scalar score for an entire generated sequence, offering little insight into which token or span-level decisions were responsible for the outcome. To address this, we propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages Shapley values in cooperative game theory. SCAR distributes the total sequence-level reward among constituent tokens or text spans based on their principled marginal contributions. This creates dense reward signals, crucially, without necessitating the training of auxiliary critique models or recourse to fine-grained human annotations at intermediate generation stages. Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for fair credit attribution. Theoretically, we demonstrate that SCAR preserves the original optimal policy, and empirically, across diverse tasks including sentiment control, text summarization, and instruction tuning, we show that SCAR converges significantly faster and achieves higher final reward scores compared to standard RLHF and attention-based dense reward baselines. Our findings suggest that SCAR provides a more effective and theoretically sound method for credit assignment in RLHF, leading to more efficient alignment of LLMs."
  },
  {
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents",
    "url": "http://arxiv.org/abs/2505.20411v1",
    "arxiv_id": "2505.20411v1",
    "authors": [
      "Ibragim Badertdinov",
      "Alexander Golubev",
      "Maksim Nekrashevich",
      "Anton Shevtsov",
      "Simon Karasik",
      "Andrei Andriushchenko",
      "Maria Trofimova",
      "Daria Litvintseva",
      "Boris Yangel"
    ],
    "published": "2025-05-26T18:01:00+00:00",
    "summary": "LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues."
  },
  {
    "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection",
    "url": "http://arxiv.org/abs/2505.20289v1",
    "arxiv_id": "2505.20289v1",
    "authors": [
      "Zeyi Huang",
      "Yuyang Ji",
      "Anirudh Sundara Rajan",
      "Zefan Cai",
      "Wen Xiao",
      "Junjie Hu",
      "Yong Jae Lee"
    ],
    "published": "2025-05-26T17:59:17+00:00",
    "summary": "We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems."
  },
  {
    "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
    "url": "http://arxiv.org/abs/2505.20218v1",
    "arxiv_id": "2505.20218v1",
    "authors": [
      "Chenxiao Fan",
      "Chongming Gao",
      "Wentao Shi",
      "Yaxin Gong",
      "Zihao Zhao",
      "Fuli Feng"
    ],
    "published": "2025-05-26T16:59:23+00:00",
    "summary": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety-accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame."
  },
  {
    "title": "Temporal Sampling for Forgotten Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2505.20196v1",
    "arxiv_id": "2505.20196v1",
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Xiang Yue",
      "Radha Poovendran"
    ],
    "published": "2025-05-26T16:39:52+00:00",
    "summary": "Fine-tuning large language models (LLMs) is intended to improve their reasoning capabilities, yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training. We term this phenomenon temporal forgetting and show that it is widespread across model sizes, fine-tuning methods (both Reinforcement Learning and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this gap, we introduce Temporal Sampling, a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This approach recovers forgotten solutions without retraining or ensembling, and leads to substantial improvements in reasoning performance, gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks. We further extend our method to LoRA-adapted models, demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost. By leveraging the temporal diversity inherent in training, Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability and rethink how we evaluate LLMs."
  },
  {
    "title": "URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20175v1",
    "arxiv_id": "2505.20175v1",
    "authors": [
      "Fengkang Ying",
      "Hanwen Zhang",
      "Haozhe Wang",
      "Huishi Huang",
      "Marcelo H. Ang Jr"
    ],
    "published": "2025-05-26T16:15:42+00:00",
    "summary": "Collision-free motion planning for redundant robot manipulators in complex environments is yet to be explored. Although recent advancements at the intersection of deep reinforcement learning (DRL) and robotics have highlighted its potential to handle versatile robotic tasks, current DRL-based collision-free motion planners for manipulators are highly costly, hindering their deployment and application. This is due to an overreliance on the minimum distance between the manipulator and obstacles, inadequate exploration and decision-making by DRL, and inefficient data acquisition and utilization. In this article, we propose URPlanner, a universal paradigm for collision-free robotic motion planning based on DRL. URPlanner offers several advantages over existing approaches: it is platform-agnostic, cost-effective in both training and deployment, and applicable to arbitrary manipulators without solving inverse kinematics. To achieve this, we first develop a parameterized task space and a universal obstacle avoidance reward that is independent of minimum distance. Second, we introduce an augmented policy exploration and evaluation algorithm that can be applied to various DRL algorithms to enhance their performance. Third, we propose an expert data diffusion strategy for efficient policy learning, which can produce a large-scale trajectory dataset from only a few expert demonstrations. Finally, the superiority of the proposed methods is comprehensively verified through experiments."
  },
  {
    "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
    "url": "http://arxiv.org/abs/2505.20147v1",
    "arxiv_id": "2505.20147v1",
    "authors": [
      "Jin Wang",
      "Yao Lai",
      "Aoxue Li",
      "Shifeng Zhang",
      "Jiacheng Sun",
      "Ning Kang",
      "Chengyue Wu",
      "Zhenguo Li",
      "Ping Luo"
    ],
    "published": "2025-05-26T15:46:53+00:00",
    "summary": "The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning."
  },
  {
    "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20131v1",
    "arxiv_id": "2505.20131v1",
    "authors": [
      "Yuanxin Zhuang",
      "Dazhong Shen",
      "Ying Sun"
    ],
    "published": "2025-05-26T15:29:08+00:00",
    "summary": "Molecular editing aims to modify a given molecule to optimize desired chemical properties while preserving structural similarity. However, current approaches typically rely on string-based or continuous representations, which fail to adequately capture the discrete, graph-structured nature of molecules, resulting in limited structural fidelity and poor controllability. In this paper, we propose MolEditRL, a molecular editing framework that explicitly integrates structural constraints with precise property optimization. Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion model pretrained to reconstruct target molecules conditioned on source structures and natural language instructions; (2) an editing-aware reinforcement learning fine-tuning stage that further enhances property alignment and structural preservation by explicitly optimizing editing decisions under graph constraints. For comprehensive evaluation, we construct MolEdit-Instruct, the largest and most property-rich molecular editing dataset, comprising 3 million diverse examples spanning single- and multi-property tasks across 10 chemical attributes. Experimental results demonstrate that MolEditRL significantly outperforms state-of-the-art methods in both property optimization accuracy and structural fidelity, achieving a 74\\% improvement in editing success rate while using 98\\% fewer parameters."
  },
  {
    "title": "Proxy-Free GFlowNet",
    "url": "http://arxiv.org/abs/2505.20110v1",
    "arxiv_id": "2505.20110v1",
    "authors": [
      "Ruishuo Chen",
      "Xun Wang",
      "Rui Hu",
      "Zhuoran Li",
      "Longbo Huang"
    ],
    "published": "2025-05-26T15:12:22+00:00",
    "summary": "Generative Flow Networks (GFlowNets) are a promising class of generative models designed to sample diverse, high-reward structures by modeling distributions over compositional objects. In many real-world applications, obtaining the reward function for such objects is expensive, time-consuming, or requires human input, making it necessary to train GFlowNets from historical datasets. Most existing methods adopt a model-based approach, learning a proxy model from the dataset to approximate the reward function. However, this strategy inherently ties the quality of the learned policy to the accuracy of the proxy, introducing additional complexity and uncertainty into the training process. To overcome these limitations, we propose \\textbf{Trajectory-Distilled GFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the need for out-of-dataset reward queries. Our method is motivated by the key observation that different edges in the associated directed acyclic graph (DAG) contribute unequally to effective policy learning. TD-GFN leverages inverse reinforcement learning to estimate edge-level rewards from the offline dataset, which are then used to ingeniously prune the DAG and guide backward trajectory sampling during training. This approach directs the policy toward high-reward regions while reducing the complexity of model fitting. Empirical results across multiple tasks show that TD-GFN trains both efficiently and reliably, significantly outperforming existing baselines in convergence speed and sample quality."
  },
  {
    "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20107v1",
    "arxiv_id": "2505.20107v1",
    "authors": [
      "Ziyi Zhang",
      "Li Shen",
      "Deheng Ye",
      "Yong Luo",
      "Huangxuan Zhao",
      "Lefei Zhang"
    ],
    "published": "2025-05-26T15:11:26+00:00",
    "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images from a single text prompt, remains computationally intensive, while accelerated T2MV methods using few-step diffusion models often sacrifice image fidelity and view consistency. To address this, we propose a novel reinforcement learning (RL) finetuning framework tailored for few-step T2MV diffusion models to jointly optimize per-view fidelity and cross-view consistency. Specifically, we first reformulate T2MV denoising across all views as a single unified Markov decision process, enabling multiview-aware policy optimization driven by a joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV sampling technique that adds an inversion-denoising pass to reinforce both viewpoint and text conditioning, resulting in improved T2MV generation at the cost of inference time. To internalize its performance gains into the base sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that uses reward advantages of ZMV-Sampling over standard sampling as learning signals for policy updates. Finally, noting that the joint-view reward objective under-optimizes per-view fidelity but naively optimizing single-view metrics neglects cross-view alignment, we reframe RL finetuning for T2MV diffusion models as a constrained optimization problem that maximizes per-view fidelity subject to an explicit joint-view constraint, thereby enabling more efficient and balanced policy updates. By integrating this constrained optimization paradigm with MV-ZigAL, we establish our complete RL finetuning framework, referred to as MVC-ZigAL, which effectively refines the few-step T2MV diffusion baseline in both fidelity and consistency while preserving its few-step efficiency."
  },
  {
    "title": "From Data to Modeling: Fully Open-vocabulary Scene Graph Generation",
    "url": "http://arxiv.org/abs/2505.20106v1",
    "arxiv_id": "2505.20106v1",
    "authors": [
      "Zuyao Chen",
      "Jinlin Wu",
      "Zhen Lei",
      "Chang Wen Chen"
    ],
    "published": "2025-05-26T15:11:23+00:00",
    "summary": "We present OvSGTR, a novel transformer-based framework for fully open-vocabulary scene graph generation that overcomes the limitations of traditional closed-set models. Conventional methods restrict both object and relationship recognition to a fixed vocabulary, hindering their applicability to real-world scenarios where novel concepts frequently emerge. In contrast, our approach jointly predicts objects (nodes) and their inter-relationships (edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture featuring a frozen image backbone and text encoder to extract high-quality visual and semantic features, which are then fused via a transformer decoder for end-to-end scene graph prediction. To enrich the model's understanding of complex visual relations, we propose a relation-aware pre-training strategy that synthesizes scene graph annotations in a weakly supervised manner. Specifically, we investigate three pipelines--scene parser-based, LLM-based, and multimodal LLM-based--to generate transferable supervision signals with minimal manual annotation. Furthermore, we address the common issue of catastrophic forgetting in open-vocabulary settings by incorporating a visual-concept retention mechanism coupled with a knowledge distillation strategy, ensuring that the model retains rich semantic cues during fine-tuning. Extensive experiments on the VG150 benchmark demonstrate that OvSGTR achieves state-of-the-art performance across multiple settings, including closed-set, open-vocabulary object detection-based, relation-based, and fully open-vocabulary scenarios. Our results highlight the promise of large-scale relation-aware pre-training and transformer architectures for advancing scene graph generation towards more generalized and reliable visual understanding."
  },
  {
    "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed",
    "url": "http://arxiv.org/abs/2505.20101v1",
    "arxiv_id": "2505.20101v1",
    "authors": [
      "Yunhao Wang",
      "Yuhao Zhang",
      "Tinghao Yu",
      "Can Xu",
      "Feng Zhang",
      "Fengzong Lian"
    ],
    "published": "2025-05-26T15:08:51+00:00",
    "summary": "Large language models (LLMs) have shown impressive capabilities in handling complex tasks through long-chain reasoning. However, the extensive reasoning steps involved can significantly increase computational costs, posing challenges for real-world deployment. Recent efforts have focused on optimizing reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning processes through various approaches, such as length-aware prompt engineering, supervised fine-tuning on CoT data with variable lengths, and reinforcement learning with length penalties. Although these methods effectively reduce reasoning length, they still necessitate an initial reasoning phase. More recent approaches have attempted to integrate long-chain and short-chain reasoning abilities into a single model, yet they still rely on manual control to toggle between short and long CoT.In this work, we propose a novel approach that autonomously switches between short and long reasoning chains based on problem complexity. Our method begins with supervised fine-tuning of the base model to equip both long-chain and short-chain reasoning abilities. We then employ reinforcement learning to further balance short and long CoT generation while maintaining accuracy through two key strategies: first, integrating reinforcement learning with a long-short adaptive group-wise reward strategy to assess prompt complexity and provide corresponding rewards; second, implementing a logit-based reasoning mode switching loss to optimize the model's initial token choice, thereby guiding the selection of the reasoning type.Evaluations on mathematical datasets demonstrate that our model can dynamically switch between long-chain and short-chain reasoning modes without substantially sacrificing performance. This advancement enhances the practicality of reasoning in large language models for real-world applications."
  },
  {
    "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale",
    "url": "http://arxiv.org/abs/2505.20094v1",
    "arxiv_id": "2505.20094v1",
    "authors": [
      "Qi Li",
      "Kun Li",
      "Haozhi Han",
      "Honghui Shang",
      "Xinfu He",
      "Yunquan Zhang",
      "Hong An",
      "Ting Cao",
      "Mao Yang"
    ],
    "published": "2025-05-26T15:04:37+00:00",
    "summary": "Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence."
  },
  {
    "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback",
    "url": "http://arxiv.org/abs/2505.20075v1",
    "arxiv_id": "2505.20075v1",
    "authors": [
      "Mengdi Li",
      "Jiaye Lin",
      "Xufeng Zhao",
      "Wenhao Lu",
      "Peilin Zhao",
      "Stefan Wermter",
      "Di Wang"
    ],
    "published": "2025-05-26T14:53:08+00:00",
    "summary": "Reward models trained with conventional Reinforcement Learning from AI Feedback (RLAIF) methods suffer from limited generalizability, which hinders the alignment performance of the policy model during reinforcement learning (RL). This challenge stems from various issues, including distribution shift, preference label noise, and mismatches between overly challenging samples and model capacity. In this paper, we attempt to enhance the generalizability of reward models through a data-centric approach, driven by the insight that these issues are inherently intertwined from the perspective of data difficulty. To address this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which constructs preference pairs with varying difficulty levels and produces a curriculum that progressively incorporates preference pairs of increasing difficulty for reward model training. Our experimental results suggest that reward models trained with Curriculum-RLAIF achieve improved generalizability, significantly increasing the alignment performance of the policy model by a large margin without incurring additional inference costs compared to various non-curriculum baselines. Detailed analysis and comparisons with alternative approaches, including data selection via external pretrained reward models or internal self-selection mechanisms, as well as other curriculum strategies, further demonstrate the superiority of our approach in terms of simplicity, efficiency, and effectiveness."
  },
  {
    "title": "Incentivizing Reasoning from Weak Supervision",
    "url": "http://arxiv.org/abs/2505.20072v1",
    "arxiv_id": "2505.20072v1",
    "authors": [
      "Yige Yuan",
      "Teng Xiao",
      "Shuchang Tao",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Bingbing Xu"
    ],
    "published": "2025-05-26T14:51:29+00:00",
    "summary": "Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at https://github.com/yuanyige/W2SR."
  },
  {
    "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety",
    "url": "http://arxiv.org/abs/2505.20065v1",
    "arxiv_id": "2505.20065v1",
    "authors": [
      "Geon-Hyeong Kim",
      "Youngsoo Jang",
      "Yu Jin Kim",
      "Byoungjip Kim",
      "Honglak Lee",
      "Kyunghoon Bae",
      "Moontae Lee"
    ],
    "published": "2025-05-26T14:50:01+00:00",
    "summary": "As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety."
  },
  {
    "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.20046v1",
    "arxiv_id": "2505.20046v1",
    "authors": [
      "Le Zhang",
      "Bo Wang",
      "Xipeng Qiu",
      "Siva Reddy",
      "Aishwarya Agrawal"
    ],
    "published": "2025-05-26T14:31:48+00:00",
    "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking."
  },
  {
    "title": "The Limits of Preference Data for Post-Training",
    "url": "http://arxiv.org/abs/2505.19964v1",
    "arxiv_id": "2505.19964v1",
    "authors": [
      "Eric Zhao",
      "Jessica Dai",
      "Pranjal Awasthi"
    ],
    "published": "2025-05-26T13:26:15+00:00",
    "summary": "Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors."
  },
  {
    "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.19954v1",
    "arxiv_id": "2505.19954v1",
    "authors": [
      "Andrew Zamai",
      "Nathanael Fijalkow",
      "Boris Mansencal",
      "Laurent Simon",
      "Eloi Navet",
      "Pierrick Coupe"
    ],
    "published": "2025-05-26T13:18:32+00:00",
    "summary": "The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions."
  },
  {
    "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections",
    "url": "http://arxiv.org/abs/2505.19939v1",
    "arxiv_id": "2505.19939v1",
    "authors": [
      "Ran Yu",
      "Zhuoren Li",
      "Lu Xiong",
      "Wei Han",
      "Bo Leng"
    ],
    "published": "2025-05-26T13:06:02+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated potential in autonomous driving (AD) decision tasks. However, applying RL to urban AD, particularly in intersection scenarios, still faces significant challenges. The lack of safety constraints makes RL vulnerable to risks. Additionally, cognitive limitations and environmental randomness can lead to unreliable decisions in safety-critical scenarios. Therefore, it is essential to quantify confidence in RL decisions to improve safety. This paper proposes an Uncertainty-aware Safety-Critical Decision and Control (USDC) framework, which generates a risk-averse policy by constructing a risk-aware ensemble distributional RL, while estimating uncertainty to quantify the policy's reliability. Subsequently, a high-order control barrier function (HOCBF) is employed as a safety filter to minimize intervention policy while dynamically enhancing constraints based on uncertainty. The ensemble critics evaluate both HOCBF and RL policies, embedding uncertainty to achieve dynamic switching between safe and flexible strategies, thereby balancing safety and efficiency. Simulation tests on unsignalized intersections in multiple tasks indicate that USDC can improve safety while maintaining traffic efficiency compared to baselines."
  },
  {
    "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL",
    "url": "http://arxiv.org/abs/2505.19923v1",
    "arxiv_id": "2505.19923v1",
    "authors": [
      "Qin-Wen Luo",
      "Ming-Kun Xie",
      "Ye-Wen Wang",
      "Sheng-Jun Huang"
    ],
    "published": "2025-05-26T12:45:54+00:00",
    "summary": "Offline reinforcement learning (RL) aims to learn an effective policy from a static dataset. To alleviate extrapolation errors, existing studies often uniformly regularize the value function or policy updates across all states. However, due to substantial variations in data quality, the fixed regularization strength often leads to a dilemma: Weak regularization strength fails to address extrapolation errors and value overestimation, while strong regularization strength shifts policy learning toward behavior cloning, impeding potential performance enabled by Bellman updates. To address this issue, we propose the selective state-adaptive regularization method for offline RL. Specifically, we introduce state-adaptive regularization coefficients to trust state-level Bellman-driven results, while selectively applying regularization on high-quality actions, aiming to avoid performance degradation caused by tight constraints on low-quality actions. By establishing a connection between the representative value regularization method, CQL, and explicit policy constraint methods, we effectively extend selective state-adaptive regularization to these two mainstream offline RL approaches. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art approaches in both offline and offline-to-online settings on the D4RL benchmark."
  },
  {
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
    "url": "http://arxiv.org/abs/2505.19914v1",
    "arxiv_id": "2505.19914v1",
    "authors": [
      "Jiangjie Chen",
      "Qianyu He",
      "Siyu Yuan",
      "Aili Chen",
      "Zhicheng Cai",
      "Weinan Dai",
      "Hongli Yu",
      "Qiying Yu",
      "Xuefeng Li",
      "Jiaze Chen",
      "Hao Zhou",
      "Mingxuan Wang"
    ],
    "published": "2025-05-26T12:40:31+00:00",
    "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io."
  },
  {
    "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought",
    "url": "http://arxiv.org/abs/2505.19877v1",
    "arxiv_id": "2505.19877v1",
    "authors": [
      "Chao Huang",
      "Benfeng Wang",
      "Jie Wen",
      "Chengliang Liu",
      "Wei Wang",
      "Li Shen",
      "Xiaochun Cao"
    ],
    "published": "2025-05-26T12:05:16+00:00",
    "summary": "Recent advancements in reasoning capability of Multimodal Large Language Models (MLLMs) demonstrate its effectiveness in tackling complex visual tasks. However, existing MLLM-based Video Anomaly Detection (VAD) methods remain limited to shallow anomaly descriptions without deep reasoning. In this paper, we propose a new task named Video Anomaly Reasoning (VAR), which aims to enable deep analysis and understanding of anomalies in the video by requiring MLLMs to think explicitly before answering. To this end, we propose Vad-R1, an end-to-end MLLM-based framework for VAR. Specifically, we design a Perception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human process of recognizing anomalies, guiding the MLLM to reason anomaly step-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a dedicated dataset for VAR. Furthermore, we propose an improved reinforcement learning algorithm AVA-GRPO, which explicitly incentivizes the anomaly reasoning capability of MLLMs through a self-verification mechanism with limited annotations. Experimental results demonstrate that Vad-R1 achieves superior performance, outperforming both open-source and proprietary models on VAD and VAR tasks. Codes and datasets will be released at https://github.com/wbfwonderful/Vad-R1."
  },
  {
    "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments",
    "url": "http://arxiv.org/abs/2505.19867v1",
    "arxiv_id": "2505.19867v1",
    "authors": [
      "Yavar Taheri Yeganeh",
      "Mohsen Jafari",
      "Andrea Matta"
    ],
    "published": "2025-05-26T11:50:22+00:00",
    "summary": "With the recent success of world-model agents, which extend the core idea of model-based reinforcement learning by learning a differentiable model for sample-efficient control across diverse tasks, active inference (AIF) offers a complementary, neuroscience-grounded paradigm that unifies perception, learning, and action within a single probabilistic framework powered by a generative model. Despite this promise, practical AIF agents still rely on accurate immediate predictions and exhaustive planning, a limitation that is exacerbated in delayed environments requiring plans over long horizons, tens to hundreds of steps. Moreover, most existing agents are evaluated on robotic or vision benchmarks which, while natural for biological agents, fall short of real-world industrial complexity. We address these limitations with a generative-policy architecture featuring (i) a multi-step latent transition that lets the generative model predict an entire horizon in a single look-ahead, (ii) an integrated policy network that enables the transition and receives gradients of the expected free energy, (iii) an alternating optimization scheme that updates model and policy from a replay buffer, and (iv) a single gradient step that plans over long horizons, eliminating exhaustive planning from the control loop. We evaluate our agent in an environment that mimics a realistic industrial scenario with delayed and long-horizon settings. The empirical results confirm the effectiveness of the proposed approach, demonstrating the coupled world-model with the AIF formalism yields an end-to-end probabilistic controller capable of effective decision making in delayed, long-horizon settings without handcrafted rewards or expensive planning."
  },
  {
    "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.19862v1",
    "arxiv_id": "2505.19862v1",
    "authors": [
      "Hexuan Deng",
      "Wenxiang Jiao",
      "Xuebo Liu",
      "Jun Rao",
      "Min Zhang"
    ],
    "published": "2025-05-26T11:47:16+00:00",
    "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of overthinking, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but tends to lose the reflection ability and harm the performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision. Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 35% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for simpler ones without losing reflection ability. Codes are available at https://github.com/hexuandeng/REA-RL."
  },
  {
    "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19850v1",
    "arxiv_id": "2505.19850v1",
    "authors": [
      "Leander Diaz-Bone",
      "Marco Bagatella",
      "Jonas H\u00fcbotter",
      "Andreas Krause"
    ],
    "published": "2025-05-26T11:35:07+00:00",
    "summary": "Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise - requiring efficient exploration coupled with long-horizon credit assignment - and overcoming these challenges is key for building self-improving agents with superhuman ability. We argue that solving complex and high-dimensional tasks requires solving simpler tasks that are relevant to the target task. In contrast, most prior work designs strategies for selecting exploratory tasks with the objective of solving any task, making exploration of challenging high-dimensional, long-horizon tasks intractable. We find that the sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without needing any prior information. Based on this finding, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. Empirically, we perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL."
  },
  {
    "title": "Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications",
    "url": "http://arxiv.org/abs/2505.19837v1",
    "arxiv_id": "2505.19837v1",
    "authors": [
      "Christoph R. Landolt",
      "Christoph W\u00fcrsch",
      "Roland Meier",
      "Alain Mermoud",
      "Julian Jang-Jaccard"
    ],
    "published": "2025-05-26T11:19:43+00:00",
    "summary": "Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA."
  },
  {
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "url": "http://arxiv.org/abs/2505.19789v1",
    "arxiv_id": "2505.19789v1",
    "authors": [
      "Jijia Liu",
      "Feng Gao",
      "Bingwen Wei",
      "Xinlei Chen",
      "Qingmin Liao",
      "Yi Wu",
      "Chao Yu",
      "Yu Wang"
    ],
    "published": "2025-05-26T10:19:26+00:00",
    "summary": "Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io"
  },
  {
    "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition",
    "url": "http://arxiv.org/abs/2505.19788v1",
    "arxiv_id": "2505.19788v1",
    "authors": [
      "Zihao Zeng",
      "Xuyao Huang",
      "Boxiu Li",
      "Hao Zhang",
      "Zhijie Deng"
    ],
    "published": "2025-05-26T10:18:57+00:00",
    "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units; each unit attempts to produce a candidate answer to the original query. Hence, a natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides a multi-turn response to the query, where each turn embraces a thinking unit and yields a corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to ~70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond."
  },
  {
    "title": "MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support",
    "url": "http://arxiv.org/abs/2505.19785v1",
    "arxiv_id": "2505.19785v1",
    "authors": [
      "Qianyi Xu",
      "Gousia Habib",
      "Dilruk Perera",
      "Mengling Feng"
    ],
    "published": "2025-05-26T10:16:39+00:00",
    "summary": "Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses vary significantly and evolve over time. Clinical data used to support these decisions are often irregularly sampled, sparse, and noisy. Existing decision support systems commonly rely on discretization and imputation, which can distort critical temporal dynamics and degrade decision quality. Moreover, they often overlook the clinical significance of irregular recording frequencies, filtering out patterns in how and when data is collected. Reinforcement Learning (RL) is a natural fit for clinical decision-making, enabling sequential, long-term optimization in dynamic, uncertain environments. However, most existing treatment recommendation systems are model-free and trained solely on offline data, making them sample-inefficient, sensitive to data quality, and poorly generalizable across tasks or cohorts. To address these limitations, we propose MedDreamer, a two-phase model-based RL framework for personalized treatment recommendation. MedDreamer uses a world model with an Adaptive Feature Integration (AFI) module to effectively model irregular, sparse clinical data. Through latent imagination, it simulates plausible patient trajectories to enhance learning, refining its policy using a mix of real and imagined experiences. This enables learning policies that go beyond suboptimal historical decisions while remaining grounded in clinical data. To our knowledge, this is the first application of latent imagination to irregular healthcare data. Evaluations on sepsis and mechanical ventilation (MV) treatment using two large-scale EHR datasets show that MedDreamer outperforms both model-free and model-based baselines in clinical outcomes and off-policy metrics."
  },
  {
    "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO",
    "url": "http://arxiv.org/abs/2505.19770v1",
    "arxiv_id": "2505.19770v1",
    "authors": [
      "Ruizhe Shi",
      "Minhak Song",
      "Runlong Zhou",
      "Zihan Zhang",
      "Maryam Fazel",
      "Simon S. Du"
    ],
    "published": "2025-05-26T09:54:02+00:00",
    "summary": "We present a fine-grained theoretical analysis of the performance gap between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under a representation gap. Our study decomposes this gap into two sources: an explicit representation gap under exact optimization and an implicit representation gap under finite samples. In the exact optimization setting, we characterize how the relative capacities of the reward and policy model classes influence the final policy qualities. We show that RLHF, DPO, or online DPO can outperform one another depending on the type of model mis-specifications. Notably, online DPO can outperform both RLHF and standard DPO when the reward and policy model classes are isomorphic and both mis-specified. In the approximate optimization setting, we provide a concrete construction where the ground-truth reward is implicitly sparse and show that RLHF requires significantly fewer samples than DPO to recover an effective reward model -- highlighting a statistical advantage of two-stage learning. Together, these results provide a comprehensive understanding of the performance gap between RLHF and DPO under various settings, and offer practical insights into when each method is preferred."
  },
  {
    "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19769v1",
    "arxiv_id": "2505.19769v1",
    "authors": [
      "Yuhui Chen",
      "Haoran Li",
      "Zhennan Jiang",
      "Haowei Wen",
      "Dongbin Zhao"
    ],
    "published": "2025-05-26T09:52:25+00:00",
    "summary": "Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation."
  },
  {
    "title": "RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback",
    "url": "http://arxiv.org/abs/2505.19767v1",
    "arxiv_id": "2505.19767v1",
    "authors": [
      "Junyang Shu",
      "Zhiwei Lin",
      "Yongtao Wang"
    ],
    "published": "2025-05-26T09:50:15+00:00",
    "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential in the field of embodied intelligence, enabling agents to follow human instructions to complete complex tasks in physical environments. Existing embodied agents are often trained through behavior cloning, which requires expensive data and computational resources and is constrained by human demonstrations. To address this issue, many researchers explore the application of reinforcement fine-tuning to embodied agents. However, typical reinforcement fine-tuning methods for embodied agents usually rely on sparse, outcome-based rewards, which struggle to provide fine-grained feedback for specific actions within an episode, thus limiting the model's manipulation capabilities and generalization performance. In this paper, we propose RFTF, a novel reinforcement fine-tuning method that leverages a value model to generate dense rewards in embodied scenarios. Specifically, our value model is trained using temporal information, eliminating the need for costly robot action labels. In addition, RFTF incorporates a range of techniques, such as GAE and sample balance to enhance the effectiveness of the fine-tuning process. By addressing the sparse reward problem in reinforcement fine-tuning, our method significantly improves the performance of embodied agents, delivering superior generalization and adaptation capabilities across diverse embodied tasks. Experimental results show that embodied agents fine-tuned with RFTF achieve new state-of-the-art performance on the challenging CALVIN ABC-D with an average success length of 4.296. Moreover, RFTF enables rapid adaptation to new environments. After fine-tuning in the D environment of CALVIN for a few episodes, RFTF achieved an average success length of 4.301 in this new environment."
  },
  {
    "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19761v1",
    "arxiv_id": "2505.19761v1",
    "authors": [
      "Zican Hu",
      "Wei Liu",
      "Xiaoye Qu",
      "Xiangyu Yue",
      "Chunlin Chen",
      "Zhi Wang",
      "Yu Cheng"
    ],
    "published": "2025-05-26T09:43:40+00:00",
    "summary": "While showing sophisticated reasoning abilities, large language models (LLMs) still struggle with long-horizon decision-making tasks due to deficient exploration and long-term credit assignment, especially in sparse-reward scenarios. Inspired by the divide-and-conquer principle, we propose an innovative framework **GLIDER** (**G**rounding **L**anguage Models as Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical **R**einforcement Learning) that introduces a parameter-efficient and generally applicable hierarchy to LLM policies. We develop a scheme where the low-level controller is supervised with abstract, step-by-step plans that are learned and instructed by the high-level policy. This design decomposes complicated problems into a series of coherent chain-of-thought reasoning sub-tasks, providing flexible temporal abstraction to significantly enhance exploration and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast online adaptation to non-stationary environments owing to the strong transferability of its task-agnostic low-level skills. Experiments on ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent performance gains, along with enhanced generalization capabilities."
  },
  {
    "title": "One Model to Rank Them All: Unifying Online Advertising with End-to-End Learning",
    "url": "http://arxiv.org/abs/2505.19755v1",
    "arxiv_id": "2505.19755v1",
    "authors": [
      "Junyan Qiu",
      "Ze Wang",
      "Fan Zhang",
      "Zuowu Zheng",
      "Jile Zhu",
      "Jiangke Fan",
      "Teng Zhang",
      "Haitao Wang",
      "Xingxing Wang"
    ],
    "published": "2025-05-26T09:33:54+00:00",
    "summary": "Modern industrial advertising systems commonly employ Multi-stage Cascading Architectures (MCA) to balance computational efficiency with ranking accuracy. However, this approach presents two fundamental challenges: (1) performance inconsistencies arising from divergent optimization targets and capability differences between stages, and (2) failure to account for advertisement externalities - the complex interactions between candidate ads during ranking. These limitations ultimately compromise system effectiveness and reduce platform profitability. In this paper, we present UniROM, an end-to-end generative architecture that Unifies online advertising Ranking as One Model. UniROM replaces cascaded stages with a single model to directly generate optimal ad sequences from the full candidate ad corpus in location-based services (LBS). The primary challenges associated with this approach stem from high costs of feature processing and computational bottlenecks in modeling externalities of large-scale candidate pools. To address these challenges, UniROM introduces an algorithm and engine co-designed hybrid feature service to decouple user and ad feature processing, reducing latency while preserving expressiveness. To efficiently extract intra- and cross-sequence mutual information, we propose RecFormer with an innovative cluster-attention mechanism as its core architectural component. Furthermore, we propose a bi-stage training strategy that integrates pre-training with reinforcement learning-based post-training to meet sophisticated platform and advertising objectives. Extensive offline evaluations on public benchmarks and large-scale online A/B testing on industrial advertising platform have demonstrated the superior performance of UniROM over state-of-the-art MCAs."
  },
  {
    "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models",
    "url": "http://arxiv.org/abs/2505.19743v1",
    "arxiv_id": "2505.19743v1",
    "authors": [
      "Yang Zhang",
      "Yu Yu",
      "Bo Tang",
      "Yu Zhu",
      "Chuxiong Sun",
      "Wenqiang Wei",
      "Jie Hu",
      "Zipeng Xie",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Edward Chung"
    ],
    "published": "2025-05-26T09:24:36+00:00",
    "summary": "With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs."
  },
  {
    "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
    "url": "http://arxiv.org/abs/2505.19731v1",
    "arxiv_id": "2505.19731v1",
    "authors": [
      "Daniil Tiapkin",
      "Daniele Calandriello",
      "Denis Belomestny",
      "Eric Moulines",
      "Alexey Naumov",
      "Kashif Rasul",
      "Michal Valko",
      "Pierre Menard"
    ],
    "published": "2025-05-26T09:17:32+00:00",
    "summary": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers a more direct alternative by framing the problem as finding a Nash equilibrium of a game defined by these preferences. In this work, we introduce Nash Mirror Prox ($\\mathtt{Nash-MP}$), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the $\\beta$-regularized Nash equilibrium. Specifically, we prove that the KL-divergence to the optimal policy decreases at a rate of order $(1+2\\beta)^{-N/2}$, where $N$ is a number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail a practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods."
  },
  {
    "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19717v1",
    "arxiv_id": "2505.19717v1",
    "authors": [
      "Quentin Rouxel",
      "Clemente Donoso",
      "Fei Chen",
      "Serena Ivaldi",
      "Jean-Baptiste Mouret"
    ],
    "published": "2025-05-26T09:06:34+00:00",
    "summary": "Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the extremum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: https://hucebot.github.io/extremum_flow_matching_website/"
  },
  {
    "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19714v1",
    "arxiv_id": "2505.19714v1",
    "authors": [
      "Zhaopeng Feng",
      "Yupu Liang",
      "Shaosheng Cao",
      "Jiayuan Su",
      "Jiahan Ren",
      "Zhe Xu",
      "Yao Hu",
      "Wenxuan Huang",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "published": "2025-05-26T09:02:35+00:00",
    "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT."
  },
  {
    "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward",
    "url": "http://arxiv.org/abs/2505.19713v1",
    "arxiv_id": "2505.19713v1",
    "authors": [
      "Yandong Guan",
      "Xilin Wang",
      "Xingxi Ming",
      "Jing Zhang",
      "Dong Xu",
      "Qian Yu"
    ],
    "published": "2025-05-26T09:01:56+00:00",
    "summary": "In this work, we introduce CAD-Coder, a novel framework that reformulates text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric CAD language. This representation enables direct geometric validation, a richer modeling vocabulary, and seamless integration with existing LLMs. To further enhance code validity and geometric fidelity, we propose a two-stage learning pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2) reinforcement learning with Group Reward Policy Optimization (GRPO), guided by a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and a format reward. We also introduce a chain-of-thought (CoT) planning process to improve model reasoning, and construct a large-scale, high-quality dataset of 110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to generate diverse, valid, and complex CAD models directly from natural language, advancing the state of the art of text-to-CAD generation and geometric reasoning."
  },
  {
    "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning",
    "url": "http://arxiv.org/abs/2505.19702v1",
    "arxiv_id": "2505.19702v1",
    "authors": [
      "Minheng Ni",
      "Zhengyuan Yang",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Wangmeng Zuo",
      "Lijuan Wang"
    ],
    "published": "2025-05-26T08:54:14+00:00",
    "summary": "Recent advances in large language models have significantly improved textual reasoning through the effective use of Chain-of-Thought (CoT) and reinforcement learning. However, extending these successes to vision-language tasks remains challenging due to inherent limitations in text-only CoT, such as visual hallucinations and insufficient multimodal integration. In this paper, we introduce Point-RFT, a multimodal reasoning framework explicitly designed to leverage visually grounded CoT reasoning for visual document understanding. Our approach consists of two stages: First, we conduct format finetuning using a curated dataset of 71K diverse visual reasoning problems, each annotated with detailed, step-by-step rationales explicitly grounded to corresponding visual elements. Second, we employ reinforcement finetuning targeting visual document understanding. On ChartQA, our approach improves accuracy from 70.88% (format-finetuned baseline) to 90.04%, surpassing the 83.92% accuracy achieved by reinforcement finetuning relying solely on text-based CoT. The result shows that our grounded CoT is more effective for multimodal reasoning compared with the text-only CoT. Moreover, Point-RFT exhibits superior generalization capability across several out-of-domain visual document reasoning benchmarks, including CharXiv, PlotQA, IconQA, TabMWP, etc., and highlights its potential in complex real-world scenarios."
  },
  {
    "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19698v1",
    "arxiv_id": "2505.19698v1",
    "authors": [
      "Jing Yu Lim",
      "Zarif Ikram",
      "Samson Yu",
      "Haozhe Ma",
      "Tze-Yun Leong",
      "Dianbo Liu"
    ],
    "published": "2025-05-26T08:52:45+00:00",
    "summary": "Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k."
  },
  {
    "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2505.19684v1",
    "arxiv_id": "2505.19684v1",
    "authors": [
      "Bingrui Sima",
      "Linhua Cong",
      "Wenxuan Wang",
      "Kun He"
    ],
    "published": "2025-05-26T08:45:06+00:00",
    "summary": "The emergence of Multimodal Large Language Models (MLRMs) has enabled sophisticated visual reasoning capabilities by integrating reinforcement learning and Chain-of-Thought (CoT) supervision. However, while these enhanced reasoning capabilities improve performance, they also introduce new and underexplored safety risks. In this work, we systematically investigate the security implications of advanced visual reasoning in MLRMs. Our analysis reveals a fundamental trade-off: as visual reasoning improves, models become more vulnerable to jailbreak attacks. Motivated by this critical finding, we introduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework that exploits the visual reasoning chains to bypass safety mechanisms. VisCRA combines targeted visual attention masking with a two-stage reasoning induction strategy to precisely control harmful outputs. Extensive experiments demonstrate VisCRA's significant effectiveness, achieving high attack success rates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking, 68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical insight: the very capability that empowers MLRMs -- their visual reasoning -- can also serve as an attack vector, posing significant security risks."
  },
  {
    "title": "Token-Importance Guided Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2505.19653v1",
    "arxiv_id": "2505.19653v1",
    "authors": [
      "Yang Ning",
      "Lin Hai",
      "Liu Yibo",
      "Tian Baoliang",
      "Liu Guoqing",
      "Zhang Haijun"
    ],
    "published": "2025-05-26T08:11:24+00:00",
    "summary": "Ensuring that large language models (LLMs) generate outputs aligned with human preferences is important for safe and effective AI interactions. While Direct Preference Optimization (DPO) employs an implicit reward function to optimize the policy model, however, it and its related variants overlook the differential importance of individual tokens and are sensitive to judgment noise in preference datasets during generation. Although recent methods attempt to assess the important weight of tokens via probability prediction or simplistic weighting schemes, these evaluation methods are prone to biases and still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), which introduces two key innovations: the gradient-based token-importance weights that dynamically prioritize critical tokens, and a triple loss that explicitly guides model outputs to approach human-preferred responses and stay away from non-preferred responses. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods."
  },
  {
    "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
    "url": "http://arxiv.org/abs/2505.19641v1",
    "arxiv_id": "2505.19641v1",
    "authors": [
      "Junteng Liu",
      "Yuanxiang Fan",
      "Zhuo Jiang",
      "Han Ding",
      "Yongyi Hu",
      "Chi Zhang",
      "Yiqi Shi",
      "Shitong Weng",
      "Aili Chen",
      "Shiqi Chen",
      "Yunan Huang",
      "Mozhi Zhang",
      "Pengyu Zhao",
      "Junjie Yan",
      "Junxian He"
    ],
    "published": "2025-05-26T07:59:36+00:00",
    "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."
  },
  {
    "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19640v1",
    "arxiv_id": "2505.19640v1",
    "authors": [
      "Roy Xie",
      "David Qiu",
      "Deepak Gopinath",
      "Dong Lin",
      "Yanchao Sun",
      "Chong Wang",
      "Saloni Potdar",
      "Bhuwan Dhingra"
    ],
    "published": "2025-05-26T07:58:17+00:00",
    "summary": "Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. We introduce a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, our approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, our method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to reveal several valuable insights into conditional reward modeling."
  },
  {
    "title": "Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19637v1",
    "arxiv_id": "2505.19637v1",
    "authors": [
      "Byunghyun Yoo",
      "Younghwan Shin",
      "Hyunwoo Kim",
      "Euisok Chung",
      "Jeongmin Yang"
    ],
    "published": "2025-05-26T07:54:58+00:00",
    "summary": "In standard reinforcement learning, an episode is defined as a sequence of interactions between agents and the environment, which terminates upon reaching a terminal state or a pre-defined episode length. Setting a shorter episode length enables the generation of multiple episodes with the same number of data samples, thereby facilitating an exploration of diverse states. While shorter episodes may limit the collection of long-term interactions, they may offer significant advantages when properly managed. For example, trajectory truncation in single-agent reinforcement learning has shown how the benefits of shorter episodes can be leveraged despite the trade-off of reduced long-term interaction experiences. However, this approach remains underexplored in MARL. This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment (AELA), where the episode length is initially limited and gradually increased based on an entropy-based assessment of learning progress. By starting with shorter episodes, agents can focus on learning effective strategies for initial states and minimize time spent in dead-end states. The use of entropy as an assessment metric prevents premature convergence to suboptimal policies and ensures balanced training over varying episode lengths. We validate our approach using the StarCraft Multi-agent Challenge (SMAC) and a modified predator-prey environment, demonstrating significant improvements in both convergence speed and overall performance compared to existing methods. To the best of our knowledge, this is the first study to adaptively adjust episode length in MARL based on learning progress."
  },
  {
    "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
    "url": "http://arxiv.org/abs/2505.19630v1",
    "arxiv_id": "2505.19630v1",
    "authors": [
      "Yichun Feng",
      "Jiawei Wang",
      "Lu Zhou",
      "Yixue Li"
    ],
    "published": "2025-05-26T07:48:14+00:00",
    "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL"
  },
  {
    "title": "Energy-based Preference Optimization for Test-time Adaptation",
    "url": "http://arxiv.org/abs/2505.19607v1",
    "arxiv_id": "2505.19607v1",
    "authors": [
      "Yewon Han",
      "Seoyun Yang",
      "Taesup Kim"
    ],
    "published": "2025-05-26T07:21:32+00:00",
    "summary": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation to target distributions that differ from training distributions, improving real-world generalizability. Existing TTA approaches focus on adjusting the conditional distribution; however these methods often depend on uncertain predictions in the absence of label information, leading to unreliable performance. Energy-based frameworks suggest a promising alternative to address distribution shifts without relying on uncertain predictions, instead computing the marginal distribution of target data. However, they involve the critical challenge of requiring extensive SGLD sampling, which is impractical for test-time scenarios requiring immediate adaptation. In this work, we propose Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which is based on a sampling free strategy. We first parameterize the target model using a pretrained model and residual energy function, enabling marginal likelihood maximization of target data without sampling. Building on the observation that the parameterization is mathematically equivalent to DPO objective, we then directly adapt the model to a target distribution without explicitly training the residual. Our experiments verify that EPOTTA is well-calibrated and performant while achieving computational efficiency."
  },
  {
    "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
    "url": "http://arxiv.org/abs/2505.19601v1",
    "arxiv_id": "2505.19601v1",
    "authors": [
      "Yeongmin Kim",
      "Heesun Bae",
      "Byeonghu Na",
      "Il-Chul Moon"
    ],
    "published": "2025-05-26T07:10:53+00:00",
    "summary": "Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective. The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman preference optimization (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."
  },
  {
    "title": "Multi-Agent Collaboration via Evolving Orchestration",
    "url": "http://arxiv.org/abs/2505.19591v1",
    "arxiv_id": "2505.19591v1",
    "authors": [
      "Yufan Dang",
      "Chen Qian",
      "Xueheng Luo",
      "Jingru Fan",
      "Zihao Xie",
      "Ruijie Shi",
      "Weize Chen",
      "Cheng Yang",
      "Xiaoyin Che",
      "Ye Tian",
      "Xuantang Xiong",
      "Lei Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "published": "2025-05-26T07:02:17+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator (\"puppeteer\") dynamically directs agents (\"puppets\") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator's evolution."
  },
  {
    "title": "Learning to Reason without External Rewards",
    "url": "http://arxiv.org/abs/2505.19590v1",
    "arxiv_id": "2505.19590v1",
    "authors": [
      "Xuandong Zhao",
      "Zhewei Kang",
      "Aosong Feng",
      "Sergey Levine",
      "Dawn Song"
    ],
    "published": "2025-05-26T07:01:06+00:00",
    "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor"
  },
  {
    "title": "Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.19532v1",
    "arxiv_id": "2505.19532v1",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "published": "2025-05-26T05:39:35+00:00",
    "summary": "The current state-of-the-art backdoor attacks against Reinforcement Learning (RL) rely upon unrealistically permissive access models, that assume the attacker can read (or even write) the victim's policy parameters, observations, or rewards. In this work, we question whether such a strong assumption is required to launch backdoor attacks against RL. To answer this question, we propose the \\underline{S}upply-\\underline{C}h\\underline{a}in \\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow: training agents using external agents that are provided separately or embedded within the environment. In contrast to prior works, our attack only relies on legitimate interactions of the RL agent with the supplied agents. Despite this limited access model, by poisoning a mere $3\\%$ of training experiences, our attack can successfully activate over $90\\%$ of triggered actions, reducing the average episodic return by $80\\%$ for the victim. Our novel attack demonstrates that RL attacks are likely to become a reality under untrusted RL training supply-chains."
  },
  {
    "title": "Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures",
    "url": "http://arxiv.org/abs/2505.19521v1",
    "arxiv_id": "2505.19521v1",
    "authors": [
      "Dongzhe Zheng",
      "Wenjie Mei"
    ],
    "published": "2025-05-26T05:07:57+00:00",
    "summary": "Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions."
  },
  {
    "title": "DiffE2E: Rethinking End-to-End Driving with a Hybrid Action Diffusion and Supervised Policy",
    "url": "http://arxiv.org/abs/2505.19516v1",
    "arxiv_id": "2505.19516v1",
    "authors": [
      "Rui Zhao",
      "Yuze Fan",
      "Ziguo Chen",
      "Fei Gao",
      "Zhenhai Gao"
    ],
    "published": "2025-05-26T04:58:41+00:00",
    "summary": "End-to-end learning has emerged as a transformative paradigm in autonomous driving. However, the inherently multimodal nature of driving behaviors and the generalization challenges in long-tail scenarios remain critical obstacles to robust deployment. We propose DiffE2E, a diffusion-based end-to-end autonomous driving framework. This framework first performs multi-scale alignment of multi-sensor perception features through a hierarchical bidirectional cross-attention mechanism. It then introduces a novel class of hybrid diffusion-supervision decoders based on the Transformer architecture, and adopts a collaborative training paradigm that seamlessly integrates the strengths of both diffusion and supervised policy. DiffE2E models structured latent spaces, where diffusion captures the distribution of future trajectories and supervision enhances controllability and robustness. A global condition integration module enables deep fusion of perception features with high-level targets, significantly improving the quality of trajectory generation. Subsequently, a cross-attention mechanism facilitates efficient interaction between integrated features and hybrid latent variables, promoting the joint optimization of diffusion and supervision objectives for structured output generation, ultimately leading to more robust control. Experiments demonstrate that DiffE2E achieves state-of-the-art performance in both CARLA closed-loop evaluations and NAVSIM benchmarks. The proposed integrated diffusion-supervision policy offers a generalizable paradigm for hybrid action representation, with strong potential for extension to broader domains including embodied intelligence. More details and visualizations are available at \\href{https://infinidrive.github.io/DiffE2E/}{project website}."
  },
  {
    "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study",
    "url": "http://arxiv.org/abs/2505.19510v1",
    "arxiv_id": "2505.19510v1",
    "authors": [
      "Dongil Yang",
      "Minjin Kim",
      "Sunghwan Kim",
      "Beong-woo Kwak",
      "Minjun Park",
      "Jinseok Hong",
      "Woontack Woo",
      "Jinyoung Yeo"
    ],
    "published": "2025-05-26T04:45:12+00:00",
    "summary": "The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://anonymous.4open.science/r/TSG-Bench."
  },
  {
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.18129v1",
    "arxiv_id": "2505.18129v1",
    "authors": [
      "Yan Ma",
      "Linge Du",
      "Xuyang Shen",
      "Shaoxiang Chen",
      "Pengfei Li",
      "Qibing Ren",
      "Lizhuang Ma",
      "Yuchao Dai",
      "Pengfei Liu",
      "Junjie Yan"
    ],
    "published": "2025-05-23T17:41:14+00:00",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI."
  },
  {
    "title": "Reward Model Overoptimisation in Iterated RLHF",
    "url": "http://arxiv.org/abs/2505.18126v1",
    "arxiv_id": "2505.18126v1",
    "authors": [
      "Lorenz Wolf",
      "Robert Kirk",
      "Mirco Musolesi"
    ],
    "published": "2025-05-23T17:36:13+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines."
  },
  {
    "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning",
    "url": "http://arxiv.org/abs/2505.18116v1",
    "arxiv_id": "2505.18116v1",
    "authors": [
      "Huayu Chen",
      "Kaiwen Zheng",
      "Qinsheng Zhang",
      "Ganqu Cui",
      "Yin Cui",
      "Haotian Ye",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Jun Zhu",
      "Haoxiang Wang"
    ],
    "published": "2025-05-23T17:17:40+00:00",
    "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems."
  },
  {
    "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
    "url": "http://arxiv.org/abs/2505.18098v1",
    "arxiv_id": "2505.18098v1",
    "authors": [
      "Joey Hong",
      "Anca Dragan",
      "Sergey Levine"
    ],
    "published": "2025-05-23T16:51:54+00:00",
    "summary": "Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability."
  },
  {
    "title": "Stable Reinforcement Learning for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2505.18086v1",
    "arxiv_id": "2505.18086v1",
    "authors": [
      "Muzhi Dai",
      "Shixuan Liu",
      "Qingyi Si"
    ],
    "published": "2025-05-23T16:43:03+00:00",
    "summary": "The success of Deepseek-R1 has drawn the LLM community's attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models' behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward functions exacerbate RL training instability: as the completion length decreases, model accuracy abruptly collapses, often occurring early in training. To address this issue, we propose a simple yet effective solution GRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically adjusts the reward strategy by monitoring the correctness ratio among completions within each query-sampled group. A low correctness ratio indicates the need to avoid length penalty that compromises CoT quality, triggering a switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A high ratio maintains length penalties to boost efficiency. Experimental results show that our approach avoids training instability caused by length penalty while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K, GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average accuracy by 1.48% while reducing CoT sequence length by 47.3%."
  },
  {
    "title": "What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?",
    "url": "http://arxiv.org/abs/2505.18083v1",
    "arxiv_id": "2505.18083v1",
    "authors": [
      "Quentin Clark",
      "Florian Shkurti"
    ],
    "published": "2025-05-23T16:41:08+00:00",
    "summary": "In planning, stitching is an ability of algorithms to piece together sub-trajectories of data they are trained on to generate new and diverse behaviours. While stitching is historically a strength of offline reinforcement learning, recent generative behavioural cloning (BC) methods have also shown proficiency at stitching. However, the main factors behind this are poorly understood, hindering the development of new algorithms that can reliably stitch. Focusing on diffusion planners trained via BC, we find two properties are needed to compose: \\emph{positional equivariance} and \\emph{local receptiveness}. We use these two properties to explain architecture, data, and inference choices in existing generative BC methods based on diffusion planning, including replanning frequency, data augmentation, and data scaling. Experimental comparisions show that (1) while locality is more important than positional equivariance in creating a diffusion planner capable of composition, both are crucial (2) enabling these properties through relatively simple architecture choices can be competitive with more computationally expensive methods such as replanning or scaling data, and (3) simple inpainting-based guidance can guide architecturally compositional models to enable generalization in goal-conditioned settings."
  },
  {
    "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation",
    "url": "http://arxiv.org/abs/2505.18078v1",
    "arxiv_id": "2505.18078v1",
    "authors": [
      "Junhao Chen",
      "Mingjin Chen",
      "Jianjin Xu",
      "Xiang Li",
      "Junting Dong",
      "Mingze Sun",
      "Puhua Jiang",
      "Hongxiang Li",
      "Yuhang Yang",
      "Hao Zhao",
      "Xiaoxiao Long",
      "Ruqi Huang"
    ],
    "published": "2025-05-23T16:37:14+00:00",
    "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/."
  },
  {
    "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals",
    "url": "http://arxiv.org/abs/2505.18071v1",
    "arxiv_id": "2505.18071v1",
    "authors": [
      "Jia-Nan Li",
      "Jian Guan",
      "Wei Wu",
      "Rui Yan"
    ],
    "published": "2025-05-23T16:16:46+00:00",
    "summary": "Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \\textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. We develop \\textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \\textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training."
  },
  {
    "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective",
    "url": "http://arxiv.org/abs/2505.17997v1",
    "arxiv_id": "2505.17997v1",
    "authors": [
      "Jintian Shao",
      "Yiming Cheng",
      "Hongyi Huang",
      "Beiwen Zhang",
      "Zhiyu Wu",
      "You Shan",
      "Mingkai Zheng"
    ],
    "published": "2025-05-23T15:03:41+00:00",
    "summary": "The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization."
  },
  {
    "title": "Outcome-based Reinforcement Learning to Predict the Future",
    "url": "http://arxiv.org/abs/2505.17989v1",
    "arxiv_id": "2505.17989v1",
    "authors": [
      "Benjamin Turtel",
      "Danny Franklin",
      "Kris Skotheim",
      "Luke Hewitt",
      "Philipp Schoenegger"
    ],
    "published": "2025-05-23T14:56:07+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models."
  },
  {
    "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17988v1",
    "arxiv_id": "2505.17988v1",
    "authors": [
      "Yutong Chen",
      "Jiandong Gao",
      "Ji Wu"
    ],
    "published": "2025-05-23T14:55:22+00:00",
    "summary": "R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning"
  },
  {
    "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
    "url": "http://arxiv.org/abs/2505.17952v1",
    "arxiv_id": "2505.17952v1",
    "authors": [
      "Che Liu",
      "Haozhe Wang",
      "Jiazhen Pan",
      "Zhongwei Wan",
      "Yong Dai",
      "Fangzhen Lin",
      "Wenjia Bai",
      "Daniel Rueckert",
      "Rossella Arcucci"
    ],
    "published": "2025-05-23T14:27:37+00:00",
    "summary": "Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks."
  },
  {
    "title": "Semantic segmentation with reward",
    "url": "http://arxiv.org/abs/2505.17905v1",
    "arxiv_id": "2505.17905v1",
    "authors": [
      "Xie Ting",
      "Ye Huang",
      "Zhilin Liu",
      "Lixin Duan"
    ],
    "published": "2025-05-23T13:52:15+00:00",
    "summary": "In real-world scenarios, pixel-level labeling is not always available. Sometimes, we need a semantic segmentation network, and even a visual encoder can have a high compatibility, and can be trained using various types of feedback beyond traditional labels, such as feedback that indicates the quality of the parsing results. To tackle this issue, we proposed RSS (Reward in Semantic Segmentation), the first practical application of reward-based reinforcement learning on pure semantic segmentation offered in two granular levels (pixel-level and image-level). RSS incorporates various novel technologies, such as progressive scale rewards (PSR) and pair-wise spatial difference (PSD), to ensure that the reward facilitates the convergence of the semantic segmentation network, especially under image-level rewards. Experiments and visualizations on benchmark datasets demonstrate that the proposed RSS can successfully ensure the convergence of the semantic segmentation network on two levels of rewards. Additionally, the RSS, which utilizes an image-level reward, outperforms existing weakly supervised methods that also rely solely on image-level signals during training."
  },
  {
    "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation",
    "url": "http://arxiv.org/abs/2505.17897v1",
    "arxiv_id": "2505.17897v1",
    "authors": [
      "Zi-Ao Ma",
      "Tian Lan",
      "Rong-Cheng Tu",
      "Shu-Hang Liu",
      "Heyan Huang",
      "Zhijing Wu",
      "Chen Xu",
      "Xian-Ling Mao"
    ],
    "published": "2025-05-23T13:44:59+00:00",
    "summary": "The rapid progress in diffusion-based text-to-image (T2I) generation has created an urgent need for interpretable automatic evaluation methods that can assess the quality of generated images, therefore reducing the human annotation burden. To reduce the prohibitive cost of relying on commercial models for large-scale evaluation, and to improve the reasoning capabilities of open-source models, recent research has explored supervised fine-tuning (SFT) of multimodal large language models (MLLMs) as dedicated T2I evaluators. However, SFT approaches typically rely on high-quality critique datasets, which are either generated by proprietary LLMs-with potential issues of bias and inconsistency-or annotated by humans at high cost, limiting their scalability and generalization. To address these limitations, we propose T2I-Eval-R1, a novel reinforcement learning framework that trains open-source MLLMs using only coarse-grained quality scores, thereby avoiding the need for annotating high-quality interpretable evaluation rationale. Our approach integrates Group Relative Policy Optimization (GRPO) into the instruction-tuning process, enabling models to generate both scalar scores and interpretable reasoning chains with only easy accessible annotated judgment scores or preferences. Furthermore, we introduce a continuous reward formulation that encourages score diversity and provides stable optimization signals, leading to more robust and discriminative evaluation behavior. Experimental results on three established T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves significantly higher alignment with human assessments and offers more accurate interpretable score rationales compared to strong baseline methods."
  },
  {
    "title": "Formalizing Embeddedness Failures in Universal Artificial Intelligence",
    "url": "http://arxiv.org/abs/2505.17882v1",
    "arxiv_id": "2505.17882v1",
    "authors": [
      "Cole Wyeth",
      "Marcus Hutter"
    ],
    "published": "2025-05-23T13:31:28+00:00",
    "summary": "We rigorously discuss the commonly asserted failures of the AIXI reinforcement learning agent as a model of embedded agency. We attempt to formalize these failure modes and prove that they occur within the framework of universal artificial intelligence, focusing on a variant of AIXI that models the joint action/percept history as drawn from the universal distribution. We also evaluate the progress that has been made towards a successful theory of embedded agency based on variants of the AIXI agent."
  },
  {
    "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization",
    "url": "http://arxiv.org/abs/2505.17866v1",
    "arxiv_id": "2505.17866v1",
    "authors": [
      "Hongshu Guo",
      "Zeyuan Ma",
      "Yining Ma",
      "Xinglin Zhang",
      "Wei-Neng Chen",
      "Yue-Jiao Gong"
    ],
    "published": "2025-05-23T13:16:01+00:00",
    "summary": "Designing effective black-box optimizers is hampered by limited problem-specific knowledge and manual control that spans months for almost every detail. In this paper, we present DesignX, the first automated algorithm design framework that generates an effective optimizer specific to a given black-box optimization problem within seconds. Rooted in the first principles, we identify two key sub-tasks: 1) algorithm structure generation and 2) hyperparameter control. To enable systematic construction, a comprehensive modular algorithmic space is first built, embracing hundreds of algorithm components collected from decades of research. We then introduce a dual-agent reinforcement learning system that collaborates on structural and parametric design through a novel cooperative training objective, enabling large-scale meta-training across 10k diverse instances. Remarkably, through days of autonomous learning, the DesignX-generated optimizers continuously surpass human-crafted optimizers by orders of magnitude, either on synthetic testbed or on realistic optimization scenarios such as Protein-docking, AutoML and UAV path planning. Further in-depth analysis reveals DesignX's capability to discover non-trivial algorithm patterns beyond expert intuition, which, conversely, provides valuable design insights for the optimization community. We provide DesignX's inference code at https://github.com/MetaEvo/DesignX."
  },
  {
    "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
    "url": "http://arxiv.org/abs/2505.17859v1",
    "arxiv_id": "2505.17859v1",
    "authors": [
      "Masahiro Fujisawa",
      "Masaki Adachi",
      "Michael A. Osborne"
    ],
    "published": "2025-05-23T13:12:37+00:00",
    "summary": "Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose H\\\"older-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. H\\\"older-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods."
  },
  {
    "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17830v1",
    "arxiv_id": "2505.17830v1",
    "authors": [
      "Nicolas Castanet",
      "Olivier Sigaud",
      "Sylvain Lamprier"
    ],
    "published": "2025-05-23T12:43:55+00:00",
    "summary": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge."
  },
  {
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models",
    "url": "http://arxiv.org/abs/2505.17826v1",
    "arxiv_id": "2505.17826v1",
    "authors": [
      "Xuchen Pan",
      "Yanxi Chen",
      "Yushuo Chen",
      "Yuchang Sun",
      "Daoyuan Chen",
      "Wenhao Zhang",
      "Yuexiang Xie",
      "Yilun Huang",
      "Yilei Zhang",
      "Dawei Gao",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "published": "2025-05-23T12:41:09+00:00",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework."
  },
  {
    "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition",
    "url": "http://arxiv.org/abs/2505.17807v1",
    "arxiv_id": "2505.17807v1",
    "authors": [
      "Ping Li",
      "Jianan Ni",
      "Bo Pang"
    ],
    "published": "2025-05-23T12:24:28+00:00",
    "summary": "Action recognition models using deep learning are vulnerable to adversarial examples, which are transferable across other models trained on the same data modality. Existing transferable attack methods face two major challenges: 1) they heavily rely on the assumption that the decision boundaries of the surrogate (a.k.a., source) model and the target model are similar, which limits the adversarial transferability; and 2) their decision boundary difference makes the attack direction uncertain, which may result in the gradient oscillation, weakening the adversarial attack. This motivates us to propose a Background Mixup-induced Temporal Consistency (BMTC) attack method for action recognition. From the input transformation perspective, we design a model-agnostic background adversarial mixup module to reduce the surrogate-target model dependency. In particular, we randomly sample one video from each category and make its background frame, while selecting the background frame with the top attack ability for mixup with the clean frame by reinforcement learning. Moreover, to ensure an explicit attack direction, we leverage the background category as guidance for updating the gradient of adversarial example, and design a temporal gradient consistency loss, which strengthens the stability of the attack direction on subsequent frames. Empirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one image dataset, i.e., ImageNet, demonstrate that our method significantly boosts the transferability of adversarial examples across several action/image recognition models. Our code is available at https://github.com/mlvccn/BMTC_TransferAttackVid."
  },
  {
    "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour",
    "url": "http://arxiv.org/abs/2505.17801v1",
    "arxiv_id": "2505.17801v1",
    "authors": [
      "B\u00e1lint Gyevn\u00e1r",
      "Christopher G. Lucas",
      "Stefano V. Albrecht",
      "Shay B. Cohen"
    ],
    "published": "2025-05-23T12:19:18+00:00",
    "summary": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks but raise trust concerns due to risks like miscoordination and goal misalignment. Explainability is vital for trust calibration, but explainable reinforcement learning for MAS faces challenges in state/action space complexity, stakeholder needs, and evaluation. Using the counterfactual theory of causation and LLMs' summarisation capabilities, we propose Agentic eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible causal explanations for pre-trained multi-agent policies by having an LLM interrogate an environment simulator using queries like 'whatif' and 'remove' to observe and synthesise counterfactual information over multiple rounds. We evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel evaluation methodology combining subjective preference, correctness, and goal/action prediction metrics, and an external LLM as evaluator. Compared to baselines, AXIS improves perceived explanation correctness by at least 7.7% across all models and goal prediction accuracy by 23% for 4 models, with improved or comparable action prediction accuracy, achieving the highest scores overall."
  },
  {
    "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors",
    "url": "http://arxiv.org/abs/2505.17795v1",
    "arxiv_id": "2505.17795v1",
    "authors": [
      "Tazeek Bin Abdur Rakib",
      "Ambuj Mehrish",
      "Lay-Ki Soon",
      "Wern Han Lim",
      "Soujanya Poria"
    ],
    "published": "2025-05-23T12:12:40+00:00",
    "summary": "Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the user's emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94\\% and, with a larger LLM prior, pushes success above 97\\% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at https://github.com/declare-lab/dialogxpert/"
  },
  {
    "title": "Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17749v1",
    "arxiv_id": "2505.17749v1",
    "authors": [
      "Ghada Sokar",
      "Pablo Samuel Castro"
    ],
    "published": "2025-05-23T11:15:43+00:00",
    "summary": "Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional layers) and the ensuing dense layers as the main underlying factor limiting scaling capabilities; we denote this connection as the bottleneck, and we demonstrate that previous approaches implicitly target this bottleneck. As a result of our analyses, we present global average pooling as a simple yet effective way of targeting the bottleneck, thereby avoiding the complexity of earlier approaches."
  },
  {
    "title": "Fast Quiet-STaR: Thinking Without Thought Tokens",
    "url": "http://arxiv.org/abs/2505.17746v1",
    "arxiv_id": "2505.17746v1",
    "authors": [
      "Wei Huang",
      "Yizhe Xiong",
      "Xin Ye",
      "Zhijie Deng",
      "Hui Chen",
      "Zijia Lin",
      "Guiguang Ding"
    ],
    "published": "2025-05-23T11:14:12+00:00",
    "summary": "Large Language Models (LLMs) have achieved impressive performance across a range of natural language processing tasks. However, recent advances demonstrate that further gains particularly in complex reasoning tasks require more than merely scaling up model sizes or training data. One promising direction is to enable models to think during the reasoning process. Recently, Quiet STaR significantly improves reasoning by generating token-level thought traces, but incurs substantial inference overhead. In this work, we propose Fast Quiet STaR, a more efficient reasoning framework that preserves the benefits of token-level reasoning while reducing computational cost. Our method introduces a curriculum learning based training strategy that gradually reduces the number of thought tokens, enabling the model to internalize more abstract and concise reasoning processes. We further extend this approach to the standard Next Token Prediction (NTP) setting through reinforcement learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates the need for explicit thought token generation during inference. Experiments on four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an average accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B, while maintaining the same inference latency. Our code will be available at https://github.com/huangwei200012/Fast-Quiet-STaR."
  },
  {
    "title": "MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization",
    "url": "http://arxiv.org/abs/2505.17745v1",
    "arxiv_id": "2505.17745v1",
    "authors": [
      "Zeyuan Ma",
      "Yue-Jiao Gong",
      "Hongshu Guo",
      "Wenjie Qiu",
      "Sijie Ma",
      "Hongqiao Lian",
      "Jiajun Zhan",
      "Kaixu Chen",
      "Chen Wang",
      "Zhiyang Huang",
      "Zechuan Huang",
      "Guojun Peng",
      "Ran Cheng",
      "Yining Ma"
    ],
    "published": "2025-05-23T11:13:10+00:00",
    "summary": "Meta-Black-Box Optimization (MetaBBO) streamlines the automation of optimization algorithm design through meta-learning. It typically employs a bi-level structure: the meta-level policy undergoes meta-training to reduce the manual effort required in developing algorithms for low-level optimization tasks. The original MetaBox (2023) provided the first open-source framework for reinforcement learning-based single-objective MetaBBO. However, its relatively narrow scope no longer keep pace with the swift advancement in this field. In this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a milestone upgrade with four novel features: 1) a unified architecture supporting RL, evolutionary, and gradient-based approaches, by which we reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective, multi-objective, multi-model, and multi-task optimization scenarios; 4) plentiful and extensible interfaces for custom analysis/visualization and integrating to external optimization tools/benchmarks. To show the utility of MetaBox-v2, we carry out a systematic case study that evaluates the built-in baselines in terms of the optimization performance, generalization ability and learning efficiency. Valuable insights are concluded from thorough and detailed analysis for practitioners and those new to the field."
  },
  {
    "title": "URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.17734v1",
    "arxiv_id": "2505.17734v1",
    "authors": [
      "Ahmet Onur Akman",
      "Anastasia Psarou",
      "Micha\u0142 Hoffmann",
      "\u0141ukasz Gorczyca",
      "\u0141ukasz Kowalski",
      "Pawe\u0142 Gora",
      "Grzegorz Jamr\u00f3z",
      "Rafa\u0142 Kucharski"
    ],
    "published": "2025-05-23T10:54:53+00:00",
    "summary": "Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future urban networks, potentially by optimizing their routing decisions. Unlike for human drivers, these decisions can be made with collective, data-driven policies, developed by machine learning algorithms. Reinforcement learning (RL) can facilitate the development of such collective routing strategies, yet standardized and realistic benchmarks are missing. To that end, we present \\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. \\our{} is a comprehensive benchmarking environment that unifies evaluation across 29 real-world traffic networks paired with realistic demand patterns. \\our{} comes with a catalog of predefined tasks, four state-of-the-art multi-agent RL (MARL) algorithm implementations, three baseline methods, domain-specific performance metrics, and a modular configuration scheme. Our results suggest that, despite the lengthy and costly training, state-of-the-art MARL algorithms rarely outperformed humans. Experimental results reported in this paper initiate the first leaderboard for MARL in large-scale urban routing optimization and reveal that current approaches struggle to scale, emphasizing the urgent need for advancements in this domain."
  },
  {
    "title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain",
    "url": "http://arxiv.org/abs/2505.17727v1",
    "arxiv_id": "2505.17727v1",
    "authors": [
      "Jiawei Zhou",
      "Linye Lyu",
      "Zhuotao Tian",
      "Cheng Zhuo",
      "Yu Li"
    ],
    "published": "2025-05-23T10:45:43+00:00",
    "summary": "Safety-critical scenarios are rare yet pivotal for evaluating and enhancing the robustness of autonomous driving systems. While existing methods generate safety-critical driving trajectories, simulations, or single-view videos, they fall short of meeting the demands of advanced end-to-end autonomous systems (E2E AD), which require real-world, multi-view video data. To bridge this gap, we introduce SafeMVDrive, the first framework designed to generate high-quality, safety-critical, multi-view driving videos grounded in real-world domains. SafeMVDrive strategically integrates a safety-critical trajectory generator with an advanced multi-view video generator. To tackle the challenges inherent in this integration, we first enhance scene understanding ability of the trajectory generator by incorporating visual context -- which is previously unavailable to such generator -- and leveraging a GRPO-finetuned vision-language model to achieve more realistic and context-aware trajectory generation. Second, recognizing that existing multi-view video generators struggle to render realistic collision events, we introduce a two-stage, controllable trajectory generation mechanism that produces collision-evasion trajectories, ensuring both video quality and safety-critical fidelity. Finally, we employ a diffusion-based multi-view video generator to synthesize high-quality safety-critical driving videos from the generated trajectories. Experiments conducted on an E2E AD planner demonstrate a significant increase in collision rate when tested with our generated data, validating the effectiveness of SafeMVDrive in stress-testing planning modules. Our code, examples, and datasets are publicly available at: https://zhoujiawei3.github.io/SafeMVDrive/."
  },
  {
    "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization",
    "url": "http://arxiv.org/abs/2505.17714v1",
    "arxiv_id": "2505.17714v1",
    "authors": [
      "Ben Rahman"
    ],
    "published": "2025-05-23T10:30:58+00:00",
    "summary": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods -- from robotic control to game AI -- its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region -- a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BR's simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains -- from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments."
  },
  {
    "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models",
    "url": "http://arxiv.org/abs/2505.17697v1",
    "arxiv_id": "2505.17697v1",
    "authors": [
      "Zekai Zhao",
      "Qi Liu",
      "Kun Zhou",
      "Zihan Liu",
      "Yifei Shao",
      "Zhiting Hu",
      "Biwei Huang"
    ],
    "published": "2025-05-23T10:07:18+00:00",
    "summary": "Despite the remarkable reasoning performance, eliciting the long chain-of-thought (CoT) ability in large language models (LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers largely governs long-form reasoning attributes, such as output length and self-reflection. By simply amplifying these activations and inserting \"wait\" tokens, we can invoke the long CoT ability without any training, resulting in significantly increased self-reflection rates and accuracy. Moreover, we find that the activation dynamics follow predictable trajectories, with a sharp rise after special tokens and a subsequent exponential decay. Building on these insights, we introduce a general training-free activation control technique. It leverages a few contrastive examples to identify key activations, and employs simple analytic functions to modulate their values at inference time to elicit long CoTs. Extensive experiments confirm the effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs and improving their performance. Additionally, we propose a parameter-efficient fine-tuning method that trains only a last-layer activation amplification module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning benchmarks with significantly fewer parameters. Our code and data are publicly released."
  },
  {
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17667v1",
    "arxiv_id": "2505.17667v1",
    "authors": [
      "Fanqi Wan",
      "Weizhou Shen",
      "Shengyi Liao",
      "Yingcheng Shi",
      "Chenliang Li",
      "Ziyi Yang",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou",
      "Ming Yan"
    ],
    "published": "2025-05-23T09:31:55+00:00",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments."
  },
  {
    "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling",
    "url": "http://arxiv.org/abs/2505.17659v1",
    "arxiv_id": "2505.17659v1",
    "authors": [
      "Xiaolong Tang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2025-05-23T09:22:19+00:00",
    "summary": "Safe and feasible trajectory planning is essential for real-world autonomous driving systems. However, existing learning-based planning methods often rely on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting unsafe behaviors such as speeding from suboptimal human driving data. Inspired by the success of large language models, we propose Plan-R1, a novel two-stage trajectory planning framework that formulates trajectory planning as a sequential prediction task, guided by explicit planning principles such as safety, comfort, and traffic rule compliance. In the first stage, we train an autoregressive trajectory predictor via next motion token prediction on expert data. In the second stage, we design rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the model using Group Relative Policy Optimization (GRPO), a reinforcement learning strategy, to align its predictions with these planning principles. Experiments on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance."
  },
  {
    "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective",
    "url": "http://arxiv.org/abs/2505.17652v1",
    "arxiv_id": "2505.17652v1",
    "authors": [
      "Deyang Kong",
      "Qi Guo",
      "Xiangyu Xi",
      "Wei Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "published": "2025-05-23T09:15:26+00:00",
    "summary": "Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \\textbf{C}ompetence-\\textbf{D}ifficulty \\textbf{A}lignment \\textbf{S}ampling (\\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \\textbf{2.33} times slower than CDAS."
  },
  {
    "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning",
    "url": "http://arxiv.org/abs/2505.17645v1",
    "arxiv_id": "2505.17645v1",
    "authors": [
      "Chuhao Zhou",
      "Jianfei Yang"
    ],
    "published": "2025-05-23T09:06:09+00:00",
    "summary": "Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence."
  },
  {
    "title": "H2-COMPACT: Human-Humanoid Co-Manipulation via Adaptive Contact Trajectory Policies",
    "url": "http://arxiv.org/abs/2505.17627v1",
    "arxiv_id": "2505.17627v1",
    "authors": [
      "Geeta Chandra Raju Bethala",
      "Hao Huang",
      "Niraj Pudasaini",
      "Abdullah Mohamed Ali",
      "Shuaihang Yuan",
      "Congcong Wen",
      "Anthony Tzes",
      "Yi Fang"
    ],
    "published": "2025-05-23T08:38:26+00:00",
    "summary": "We present a hierarchical policy-learning framework that enables a legged humanoid to cooperatively carry extended loads with a human partner using only haptic cues for intent inference. At the upper tier, a lightweight behavior-cloning network consumes six-axis force/torque streams from dual wrist-mounted sensors and outputs whole-body planar velocity commands that capture the leader's applied forces. At the lower tier, a deep-reinforcement-learning policy, trained under randomized payloads (0-3 kg) and friction conditions in Isaac Gym and validated in MuJoCo and on a real Unitree G1, maps these high-level twists to stable, under-load joint trajectories. By decoupling intent interpretation (force -> velocity) from legged locomotion (velocity -> joints), our method combines intuitive responsiveness to human inputs with robust, load-adaptive walking. We collect training data without motion-capture or markers, only synchronized RGB video and F/T readings, employing SAM2 and WHAM to extract 3D human pose and velocity. In real-world trials, our humanoid achieves cooperative carry-and-move performance (completion time, trajectory deviation, velocity synchrony, and follower-force) on par with a blindfolded human-follower baseline. This work is the first to demonstrate learned haptic guidance fused with full-body legged control for fluid human-humanoid co-manipulation. Code and videos are available on the H2-COMPACT website."
  },
  {
    "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration",
    "url": "http://arxiv.org/abs/2505.17621v1",
    "arxiv_id": "2505.17621v1",
    "authors": [
      "Jingtong Gao",
      "Ling Pan",
      "Yejing Wang",
      "Rui Zhong",
      "Chi Lu",
      "Qingpeng Cai",
      "Peng Jiang",
      "Xiangyu Zhao"
    ],
    "published": "2025-05-23T08:30:28+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4."
  },
  {
    "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO",
    "url": "http://arxiv.org/abs/2505.17574v1",
    "arxiv_id": "2505.17574v1",
    "authors": [
      "Xueji Fang",
      "Liyuan Ma",
      "Zhiyang Chen",
      "Mingyuan Zhou",
      "Guo-jun Qi"
    ],
    "published": "2025-05-23T07:33:25+00:00",
    "summary": "Recent advances in text-to-video generation, particularly with autoregressive models, have enabled the synthesis of high-quality videos depicting individual scenes. However, extending these models to generate long, cross-scene videos remains a significant challenge. As the context length grows during autoregressive decoding, computational costs rise sharply, and the model's ability to maintain consistency and adhere to evolving textual prompts deteriorates. We introduce InfLVG, an inference-time framework that enables coherent long video generation without requiring additional long-form video data. InfLVG leverages a learnable context selection policy, optimized via Group Relative Policy Optimization (GRPO), to dynamically identify and retain the most semantically relevant context throughout the generation process. Instead of accumulating the entire generation history, the policy ranks and selects the top-$K$ most contextually relevant tokens, allowing the model to maintain a fixed computational budget while preserving content consistency and prompt alignment. To optimize the policy, we design a hybrid reward function that jointly captures semantic alignment, cross-scene consistency, and artifact reduction. To benchmark performance, we introduce the Cross-scene Video Benchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates complex multi-scene transitions involving shared subjects and varied actions/backgrounds. Experimental results show that InfLVG can extend video length by up to 9$\\times$, achieving strong consistency and semantic fidelity across scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG."
  },
  {
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection",
    "url": "http://arxiv.org/abs/2505.17558v1",
    "arxiv_id": "2505.17558v1",
    "authors": [
      "Shrey Pandit",
      "Ashwin Vinod",
      "Liu Leqi",
      "Ying Ding"
    ],
    "published": "2025-05-23T07:05:09+00:00",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks."
  },
  {
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17540v1",
    "arxiv_id": "2505.17540v1",
    "authors": [
      "Mingrui Wu",
      "Lu Wang",
      "Pu Zhao",
      "Fangkai Yang",
      "Jianjin Zhang",
      "Jianfeng Liu",
      "Yuefeng Zhan",
      "Weihao Han",
      "Hao Sun",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Qingwei Lin",
      "Weiwei Deng",
      "Dongmei Zhang",
      "Feng Sun",
      "Qi Zhang",
      "Rongrong Ji"
    ],
    "published": "2025-05-23T06:44:26+00:00",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results."
  },
  {
    "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
    "url": "http://arxiv.org/abs/2505.17534v1",
    "arxiv_id": "2505.17534v1",
    "authors": [
      "Jingjing Jiang",
      "Chongjie Si",
      "Jun Luo",
      "Hanwang Zhang",
      "Chao Ma"
    ],
    "published": "2025-05-23T06:41:07+00:00",
    "summary": "This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three text-to-image generation datasets and \\textbf{23%} on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs."
  },
  {
    "title": "Large Language Models Do Multi-Label Classification Differently",
    "url": "http://arxiv.org/abs/2505.17510v1",
    "arxiv_id": "2505.17510v1",
    "authors": [
      "Marcus Ma",
      "Georgios Chochlakis",
      "Niyantha Maruthu Pandiyan",
      "Jesse Thomason",
      "Shrikanth Narayanan"
    ],
    "published": "2025-05-23T06:04:36+00:00",
    "summary": "Multi-label classification is prevalent in real-world settings, but the behavior of Large Language Models (LLMs) in this setting is understudied. We investigate how autoregressive LLMs perform multi-label classification, with a focus on subjective tasks, by analyzing the output distributions of the models in each generation step. We find that their predictive behavior reflects the multiple steps in the underlying language modeling required to generate all relevant labels as they tend to suppress all but one label at each step. We further observe that as model scale increases, their token distributions exhibit lower entropy, yet the internal ranking of the labels improves. Finetuning methods such as supervised finetuning and reinforcement learning amplify this phenomenon. To further study this issue, we introduce the task of distribution alignment for multi-label settings: aligning LLM-derived label distributions with empirical distributions estimated from annotator responses in subjective tasks. We propose both zero-shot and supervised methods which improve both alignment and predictive performance over existing approaches."
  },
  {
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.17508v1",
    "arxiv_id": "2505.17508v1",
    "authors": [
      "Yifan Zhang",
      "Yifeng Liu",
      "Huizhuo Yuan",
      "Yang Yuan",
      "Quanquan Gu",
      "Andrew C Yao"
    ],
    "published": "2025-05-23T06:01:21+00:00",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG."
  },
  {
    "title": "Offline Constrained Reinforcement Learning under Partial Data Coverage",
    "url": "http://arxiv.org/abs/2505.17506v1",
    "arxiv_id": "2505.17506v1",
    "authors": [
      "Kihyuk Hong",
      "Ambuj Tewari"
    ],
    "published": "2025-05-23T06:00:01+00:00",
    "summary": "We study offline constrained reinforcement learning (RL) with general function approximation. We aim to learn a policy from a pre-collected dataset that maximizes the expected discounted cumulative reward for a primary reward signal while ensuring that expected discounted returns for multiple auxiliary reward signals are above predefined thresholds. Existing algorithms either require fully exploratory data, are computationally inefficient, or depend on an additional auxiliary function classes to obtain an $\\epsilon$-optimal policy with sample complexity $O(\\epsilon^{-2})$. In this paper, we propose an oracle-efficient primal-dual algorithm based on a linear programming (LP) formulation, achieving $O(\\epsilon^{-2})$ sample complexity under partial data coverage. By introducing a realizability assumption, our approach ensures that all saddle points of the Lagrangian are optimal, removing the need for regularization that complicated prior analyses. Through Lagrangian decomposition, our method extracts policies without requiring knowledge of the data-generating distribution, enhancing practical applicability."
  },
  {
    "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization",
    "url": "http://arxiv.org/abs/2505.17447v1",
    "arxiv_id": "2505.17447v1",
    "authors": [
      "Qi Zhang",
      "Shouqing Yang",
      "Lirong Gao",
      "Hao Chen",
      "Xiaomeng Hu",
      "Jinglei Chen",
      "Jiexiang Wang",
      "Sheng Guo",
      "Bo Zheng",
      "Haobo Wang",
      "Junbo Zhao"
    ],
    "published": "2025-05-23T04:04:05+00:00",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities in reasoning with the emergence of reasoning models like OpenAI-o1 and DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into the realm of retrieval-augmented generation (RAG) via outcome-supervised reinforcement learning (RL) approaches, while the correctness of intermediate think-and-search steps is usually neglected. To address this issue, we design a process-level reward module to mitigate the unawareness of intermediate reasoning steps in outcome-level supervision without additional annotation. Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel framework that hybridizes stepwise process reward and outcome-based reward to current RL methods for RAG. Extensive experiments demonstrate the generalization and inference efficiency of LeTS across various RAG benchmarks. In addition, these results reveal the potential of process- and outcome-level reward hybridization in boosting LLMs' reasoning ability via RL under other scenarios. The code will be released soon."
  },
  {
    "title": "Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning",
    "url": "http://arxiv.org/abs/2505.17439v1",
    "arxiv_id": "2505.17439v1",
    "authors": [
      "Weijia Jin"
    ],
    "published": "2025-05-23T03:45:08+00:00",
    "summary": "This study designs an efficient and equitable humanitarian supply chain dynamically by using reinforcement learning, PPO, and compared with heuristic algorithms. This study demonstrates the model of PPO always treats average satisfaction rate as the priority."
  },
  {
    "title": "Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation",
    "url": "http://arxiv.org/abs/2505.17391v1",
    "arxiv_id": "2505.17391v1",
    "authors": [
      "Yuelyu Ji",
      "Rui Meng",
      "Zhuochun Li",
      "Daqing He"
    ],
    "published": "2025-05-23T02:01:15+00:00",
    "summary": "Retrieval-augmented generation (RAG) grounds large language models (LLMs) in up-to-date external evidence, yet existing multi-hop RAG pipelines still issue redundant subqueries, explore too shallowly, or wander through overly long search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning framework that evolves a query-rewriting agent from broad early-stage exploration to concise late-stage refinement. EVO-RAG couples a seven-factor, step-level reward vector (covering relevance, redundancy, efficiency, and answer correctness) with a time-varying scheduler that reweights these signals as the episode unfolds. The agent is trained with Direct Preference Optimization over a multi-head reward model, enabling it to learn when to search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks (HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match by up to 4.6 points over strong RAG baselines while trimming average retrieval depth by 15 %. Ablation studies confirm the complementary roles of curriculum staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for building reliable, cost-effective multi-hop RAG systems."
  },
  {
    "title": "WiNGPT-3.0 Technical Report",
    "url": "http://arxiv.org/abs/2505.17387v1",
    "arxiv_id": "2505.17387v1",
    "authors": [
      "Boqin Zhuang",
      "Chenxiao Song",
      "Huitong Lu",
      "Jiacheng Qiao",
      "Mingqian Liu",
      "Mingxing Yu",
      "Ping Hong",
      "Rui Li",
      "Xiaoxia Song",
      "Xiangjun Xu",
      "Xu Chen",
      "Yaoyao Ma",
      "Yujie Gao"
    ],
    "published": "2025-05-23T01:53:04+00:00",
    "summary": "Current Large Language Models (LLMs) exhibit significant limitations, notably in structured, interpretable, and verifiable medical reasoning, alongside practical deployment challenges related to computational resources and data privacy. This report focused on the development of WiNGPT-3.0, the 32-billion parameter LLMs, engineered with the objective of enhancing its capacity for medical reasoning and exploring its potential for effective integration within healthcare IT infrastructures. The broader aim is to advance towards clinically applicable models. The approach involved a multi-stage training pipeline tailored for general, medical, and clinical reasoning. This pipeline incorporated supervised fine-tuning (SFT) and reinforcement learning (RL), leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0 demonstrated strong performance: specific model variants achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training improved performance on a clinical reasoning task from a baseline score of 58.1 to 62.5. These findings suggest that reinforcement learning, even when applied with a limited dataset of only a few thousand examples, can enhance medical reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited data and computation paves the way for more trustworthy and practically deployable LLMs within clinical workflows and health information infrastructures."
  },
  {
    "title": "Towards VM Rescheduling Optimization Through Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17359v1",
    "arxiv_id": "2505.17359v1",
    "authors": [
      "Xianzhong Ding",
      "Yunkai Zhang",
      "Binbin Chen",
      "Donghao Ying",
      "Tieying Zhang",
      "Jianjun Chen",
      "Lei Zhang",
      "Alberto Cerpa",
      "Wan Du"
    ],
    "published": "2025-05-23T00:30:53+00:00",
    "summary": "Modern industry-scale data centers need to manage a large number of virtual machines (VMs). Due to the continual creation and release of VMs, many small resource fragments are scattered across physical machines (PMs). To handle these fragments, data centers periodically reschedule some VMs to alternative PMs, a practice commonly referred to as VM rescheduling. Despite the increasing importance of VM rescheduling as data centers grow in size, the problem remains understudied. We first show that, unlike most combinatorial optimization tasks, the inference time of VM rescheduling algorithms significantly influences their performance, due to dynamic VM state changes during this period. This causes existing methods to scale poorly. Therefore, we develop a reinforcement learning system for VM rescheduling, VM2RL, which incorporates a set of customized techniques, such as a two-stage framework that accommodates diverse constraints and workload conditions, a feature extraction module that captures relational information specific to rescheduling, as well as a risk-seeking evaluation enabling users to optimize the trade-off between latency and accuracy. We conduct extensive experiments with data from an industry-scale data center. Our results show that VM2RL can achieve a performance comparable to the optimal solution but with a running time of seconds. Code and datasets are open-sourced: https://github.com/zhykoties/VMR2L_eurosys, https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy."
  },
  {
    "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey",
    "url": "http://arxiv.org/abs/2505.17352v1",
    "arxiv_id": "2505.17352v1",
    "authors": [
      "Preeti Lamba",
      "Kiran Ravish",
      "Ankita Kushwaha",
      "Pawan Kumar"
    ],
    "published": "2025-05-23T00:08:49+00:00",
    "summary": "Diffusion models have emerged as leading generative models for images and other modalities, but aligning their outputs with human preferences and safety constraints remains a critical challenge. This thesis proposal investigates methods to align diffusion models using reinforcement learning (RL) and reward modeling. We survey recent advances in fine-tuning text-to-image diffusion models with human feedback, including reinforcement learning from human and AI feedback, direct preference optimization, and differentiable reward approaches. We classify these methods based on the type of feedback (human, automated, binary or ranked preferences), the fine-tuning technique (policy gradient, reward-weighted likelihood, direct backpropagation, etc.), and their efficiency and safety outcomes. We compare key algorithms and frameworks, highlighting how they improve alignment with user intent or safety standards, and discuss inter-relationships such as how newer methods build on or diverge from earlier ones. Based on the survey, we identify five promising research directions for the next two years: (1) multi-objective alignment with combined rewards, (2) efficient human feedback usage and active learning, (3) robust safety alignment against adversarial inputs, (4) continual and online alignment of diffusion models, and (5) interpretable and trustworthy reward modeling for generative images. Each direction is elaborated with its problem statement, challenges, related work, and a proposed research plan. The proposal is organized as a comprehensive document with literature review, comparative tables of methods, and detailed research plans, aiming to contribute new insights and techniques for safer and value-aligned diffusion-based generative AI."
  },
  {
    "title": "A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety",
    "url": "http://arxiv.org/abs/2505.17342v1",
    "arxiv_id": "2505.17342v1",
    "authors": [
      "Ankita Kushwaha",
      "Kiran Ravish",
      "Preeti Lamba",
      "Pawan Kumar"
    ],
    "published": "2025-05-22T23:26:12+00:00",
    "summary": "Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions."
  },
  {
    "title": "Control of Renewable Energy Communities using AI and Real-World Data",
    "url": "http://arxiv.org/abs/2505.17321v1",
    "arxiv_id": "2505.17321v1",
    "authors": [
      "Tiago Fonseca",
      "Clarisse Sousa",
      "Ricardo Ven\u00e2ncio",
      "Pedro Pires",
      "Ricardo Severino",
      "Paulo Rodrigues",
      "Pedro Paiva",
      "Luis Lino Ferreira"
    ],
    "published": "2025-05-22T22:20:09+00:00",
    "summary": "The electrification of transportation and the increased adoption of decentralized renewable energy generation have added complexity to managing Renewable Energy Communities (RECs). Integrating Electric Vehicle (EV) charging with building energy systems like heating, ventilation, air conditioning (HVAC), photovoltaic (PV) generation, and battery storage presents significant opportunities but also practical challenges. Reinforcement learning (RL), particularly MultiAgent Deep Deterministic Policy Gradient (MADDPG) algorithms, have shown promising results in simulation, outperforming heuristic control strategies. However, translating these successes into real-world deployments faces substantial challenges, including incomplete and noisy data, integration of heterogeneous subsystems, synchronization issues, unpredictable occupant behavior, and missing critical EV state-of-charge (SoC) information. This paper introduces a framework designed explicitly to handle these complexities and bridge the simulation to-reality gap. The framework incorporates EnergAIze, a MADDPG-based multi-agent control strategy, and specifically addresses challenges related to real-world data collection, system integration, and user behavior modeling. Preliminary results collected from a real-world operational REC with four residential buildings demonstrate the practical feasibility of our approach, achieving an average 9% reduction in daily peak demand and a 5% decrease in energy costs through optimized load scheduling and EV charging behaviors. These outcomes underscore the framework's effectiveness, advancing the practical deployment of intelligent energy management solutions in RECs."
  },
  {
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17022v1",
    "arxiv_id": "2505.17022v1",
    "authors": [
      "Chengqi Duan",
      "Rongyao Fang",
      "Yuqing Wang",
      "Kun Wang",
      "Linjiang Huang",
      "Xingyu Zeng",
      "Hongsheng Li",
      "Xihui Liu"
    ],
    "published": "2025-05-22T17:59:58+00:00",
    "summary": "Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1."
  },
  {
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "url": "http://arxiv.org/abs/2505.17018v1",
    "arxiv_id": "2505.17018v1",
    "authors": [
      "Kaixuan Fan",
      "Kaituo Feng",
      "Haoming Lyu",
      "Dongzhan Zhou",
      "Xiangyu Yue"
    ],
    "published": "2025-05-22T17:59:53+00:00",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1."
  },
  {
    "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
    "url": "http://arxiv.org/abs/2505.17017v1",
    "arxiv_id": "2505.17017v1",
    "authors": [
      "Chengzhuo Tong",
      "Ziyu Guo",
      "Renrui Zhang",
      "Wenyu Shan",
      "Xinyu Wei",
      "Zhenghao Xing",
      "Hongsheng Li",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-22T17:59:49+00:00",
    "summary": "Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT"
  },
  {
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2505.17016v1",
    "arxiv_id": "2505.17016v1",
    "authors": [
      "Shuhan Tan",
      "Kairan Dou",
      "Yue Zhao",
      "Philipp Kr\u00e4henb\u00fchl"
    ],
    "published": "2025-05-22T17:59:45+00:00",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.   RIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision."
  },
  {
    "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.17005v1",
    "arxiv_id": "2505.17005v1",
    "authors": [
      "Huatong Song",
      "Jinhao Jiang",
      "Wenqing Tian",
      "Zhipeng Chen",
      "Yuhuan Wu",
      "Jiahao Zhao",
      "Yingqian Min",
      "Wayne Xin Zhao",
      "Lei Fang",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-22T17:58:26+00:00",
    "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus."
  },
  {
    "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization",
    "url": "http://arxiv.org/abs/2505.16995v1",
    "arxiv_id": "2505.16995v1",
    "authors": [
      "Chao Zhang",
      "Xin Shi",
      "Xueqiao Zhang",
      "Yifan Zhu",
      "Yi Yang",
      "Yawei Luo"
    ],
    "published": "2025-05-22T17:56:21+00:00",
    "summary": "Recent advances in Emotional Support Conversation (ESC) have improved emotional support generation by fine-tuning Large Language Models (LLMs) via Supervised Fine-Tuning (SFT). However, common psychological errors still persist. While Direct Preference Optimization (DPO) shows promise in reducing such errors through pairwise preference learning, its effectiveness in ESC tasks is limited by two key challenges: (1) Entangled data structure: Existing ESC data inherently entangles psychological strategies and response content, making it difficult to construct high-quality preference pairs; and (2) Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data leads to ambiguous training objectives. To address these issues, we introduce Inferential Preference Mining (IPM) to construct high-quality preference data, forming the IPM-PrefDial dataset. Building upon this data, we propose a Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion Regulation, which decomposes the ESC task into two sequential subtasks: strategy planning and empathic response generation. Each was trained via SFT and subsequently enhanced by DPO to align with the psychological preference. Extensive experiments demonstrate that our Decoupled ESC framework outperforms joint optimization baselines, reducing preference bias and improving response quality."
  },
  {
    "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning",
    "url": "http://arxiv.org/abs/2505.16994v1",
    "arxiv_id": "2505.16994v1",
    "authors": [
      "Runyang You",
      "Yongqi Li",
      "Xinyu Lin",
      "Xin Zhang",
      "Wenjie Wang",
      "Wenjie Li",
      "Liqiang Nie"
    ],
    "published": "2025-05-22T17:55:43+00:00",
    "summary": "Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec."
  },
  {
    "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development",
    "url": "http://arxiv.org/abs/2505.16975v1",
    "arxiv_id": "2505.16975v1",
    "authors": [
      "Yaxin Du",
      "Yuzhu Cai",
      "Yifan Zhou",
      "Cheng Wang",
      "Yu Qian",
      "Xianghe Pang",
      "Qian Liu",
      "Yue Hu",
      "Siheng Chen"
    ],
    "published": "2025-05-22T17:51:49+00:00",
    "summary": "Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \\textit{hard} split, underscoring the value of its high-quality training data. Code is available here \\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}."
  },
  {
    "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "url": "http://arxiv.org/abs/2505.16925v1",
    "arxiv_id": "2505.16925v1",
    "authors": [
      "Igor Udovichenko",
      "Olivier Croissant",
      "Anita Toleutaeva",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ],
    "published": "2025-05-22T17:18:07+00:00",
    "summary": "Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives."
  },
  {
    "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization",
    "url": "http://arxiv.org/abs/2505.16869v1",
    "arxiv_id": "2505.16869v1",
    "authors": [
      "Weixiang Zhao",
      "Yulin Hu",
      "Yang Deng",
      "Tongtong Wu",
      "Wenxuan Zhang",
      "Jiahe Guo",
      "An Zhang",
      "Yanyan Zhao",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-05-22T16:24:51+00:00",
    "summary": "Large language models (LLMs) have become increasingly central to AI applications worldwide, necessitating robust multilingual safety alignment to ensure secure deployment across diverse linguistic contexts. Existing preference learning methods for safety alignment, such as RLHF and DPO, are primarily monolingual and struggle with noisy multilingual data. To address these limitations, we introduce Multilingual reward gaP Optimization (MPO), a novel approach that leverages the well-aligned safety capabilities of the dominant language (English) to improve safety alignment across multiple languages. MPO directly minimizes the reward gap difference between the dominant language and target languages, effectively transferring safety capabilities while preserving the original strengths of the dominant language. Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate MPO's efficacy in multilingual safety alignment without degrading general multilingual utility."
  },
  {
    "title": "LARES: Latent Reasoning for Sequential Recommendation",
    "url": "http://arxiv.org/abs/2505.16865v1",
    "arxiv_id": "2505.16865v1",
    "authors": [
      "Enze Liu",
      "Bowen Zheng",
      "Xiaolei Wang",
      "Wayne Xin Zhao",
      "Jinpeng Wang",
      "Sheng Chen",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-22T16:22:54+00:00",
    "summary": "Sequential recommender systems have become increasingly important in real-world applications that model user behavior sequences to predict their preferences. However, existing sequential recommendation methods predominantly rely on non-reasoning paradigms, which may limit the model's computational capacity and result in suboptimal recommendation performance. To address these limitations, we present LARES, a novel and scalable LAtent REasoning framework for Sequential recommendation that enhances model's representation capabilities through increasing the computation density of parameters by depth-recurrent latent reasoning. Our proposed approach employs a recurrent architecture that allows flexible expansion of reasoning depth without increasing parameter complexity, thereby effectively capturing dynamic and intricate user interest patterns. A key difference of LARES lies in refining all input tokens at each implicit reasoning step to improve the computation utilization. To fully unlock the model's reasoning potential, we design a two-phase training strategy: (1) Self-supervised pre-training (SPT) with dual alignment objectives; (2) Reinforcement post-training (RPT). During the first phase, we introduce trajectory-level alignment and step-level alignment objectives, which enable the model to learn recommendation-oriented latent reasoning patterns without requiring supplementary annotated data. The subsequent phase utilizes reinforcement learning (RL) to harness the model's exploratory ability, further refining its reasoning capabilities. Comprehensive experiments on real-world benchmarks demonstrate our framework's superior performance. Notably, LARES exhibits seamless compatibility with existing advanced models, further improving their recommendation performance."
  },
  {
    "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only",
    "url": "http://arxiv.org/abs/2505.16856v1",
    "arxiv_id": "2505.16856v1",
    "authors": [
      "Wei Xiao",
      "Jiacheng Liu",
      "Zifeng Zhuang",
      "Runze Suo",
      "Shangke Lyu",
      "Donglin Wang"
    ],
    "published": "2025-05-22T16:14:08+00:00",
    "summary": "Improving the performance of pre-trained policies through online reinforcement learning (RL) is a critical yet challenging topic. Existing online RL fine-tuning methods require continued training with offline pretrained Q-functions for stability and performance. However, these offline pretrained Q-functions commonly underestimate state-action pairs beyond the offline dataset due to the conservatism in most offline RL methods, which hinders further exploration when transitioning from the offline to the online setting. Additionally, this requirement limits their applicability in scenarios where only pre-trained policies are available but pre-trained Q-functions are absent, such as in imitation learning (IL) pre-training. To address these challenges, we propose a method for efficient online RL fine-tuning using solely the offline pre-trained policy, eliminating reliance on pre-trained Q-functions. We introduce PORL (Policy-Only Reinforcement Learning Fine-Tuning), which rapidly initializes the Q-function from scratch during the online phase to avoid detrimental pessimism. Our method not only achieves competitive performance with advanced offline-to-online RL algorithms and online RL approaches that leverage data or policies prior, but also pioneers a new path for directly fine-tuning behavior cloning (BC) policies."
  },
  {
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.16854v1",
    "arxiv_id": "2505.16854v1",
    "authors": [
      "Jiaqi Wang",
      "Kevin Qinghong Lin",
      "James Cheng",
      "Mike Zheng Shou"
    ],
    "published": "2025-05-22T16:13:29+00:00",
    "summary": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON."
  },
  {
    "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning",
    "url": "http://arxiv.org/abs/2505.16836v1",
    "arxiv_id": "2505.16836v1",
    "authors": [
      "Fanrui Zhang",
      "Dian Li",
      "Qiang Zhang",
      "Chenjun",
      "sinbadliu",
      "Junxiong Lin",
      "Jiahong Yan",
      "Jiawei Liu",
      "Zheng-Jun Zha"
    ],
    "published": "2025-05-22T16:05:06+00:00",
    "summary": "The rapid spread of multimodal misinformation on social media has raised growing concerns, while research on video misinformation detection remains limited due to the lack of large-scale, diverse datasets. Existing methods often overfit to rigid templates and lack deep reasoning over deceptive content. To address these challenges, we introduce FakeVV, a large-scale benchmark comprising over 100,000 video-text pairs with fine-grained, interpretable annotations. In addition, we further propose Fact-R1, a novel framework that integrates deep reasoning with collaborative rule-based reinforcement learning. Fact-R1 is trained through a three-stage process: (1) misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference alignment via Direct Preference Optimization (DPO), and (3) Group Relative Policy Optimization (GRPO) using a novel verifiable reward function. This enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those observed in advanced text-based reinforcement learning systems, but in the more complex multimodal misinformation setting. Our work establishes a new paradigm for misinformation detection, bridging large-scale video understanding, reasoning-guided alignment, and interpretable verification."
  },
  {
    "title": "Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16833v1",
    "arxiv_id": "2505.16833v1",
    "authors": [
      "Alihan H\u00fcy\u00fck",
      "Finale Doshi-Velez"
    ],
    "published": "2025-05-22T16:04:17+00:00",
    "summary": "Long-term planning, as in reinforcement learning (RL), involves finding strategies: actions that collectively work toward a goal rather than individually optimizing their immediate outcomes. As part of a strategy, some actions are taken at the expense of short-term benefit to enable future actions with even greater returns. These actions are only advantageous if followed up by the actions they facilitate, consequently, they would not have been taken if those follow-ups were not available. In this paper, we quantify such dependencies between planned actions with strategic link scores: the drop in the likelihood of one decision under the constraint that a follow-up decision is no longer available. We demonstrate the utility of strategic link scores through three practical applications: (i) explaining black-box RL agents by identifying strategically linked pairs among decisions they make, (ii) improving the worst-case performance of decision support systems by distinguishing whether recommended actions can be adopted as standalone improvements or whether they are strategically linked hence requiring a commitment to a broader strategy to be effective, and (iii) characterizing the planning processes of non-RL agents purely through interventions aimed at measuring strategic link scores - as an example, we consider a realistic traffic simulator and analyze through road closures the effective planning horizon of the emergent routing behavior of many drivers."
  },
  {
    "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2505.16826v1",
    "arxiv_id": "2505.16826v1",
    "authors": [
      "Wei Sun",
      "Wen Yang",
      "Pu Jian",
      "Qianlong Du",
      "Fuwei Cui",
      "Shuo Ren",
      "Jiajun Zhang"
    ],
    "published": "2025-05-22T16:00:33+00:00",
    "summary": "Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model."
  },
  {
    "title": "DeepRec: Towards a Deep Dive Into the Item Space with Large Language Model Based Recommendation",
    "url": "http://arxiv.org/abs/2505.16810v1",
    "arxiv_id": "2505.16810v1",
    "authors": [
      "Bowen Zheng",
      "Xiaolei Wang",
      "Enze Liu",
      "Xi Wang",
      "Lu Hongyu",
      "Yu Chen",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-22T15:49:38+00:00",
    "summary": "Recently, large language models (LLMs) have been introduced into recommender systems (RSs), either to enhance traditional recommendation models (TRMs) or serve as recommendation backbones. However, existing LLM-based RSs often do not fully exploit the complementary advantages of LLMs (e.g., world knowledge and reasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to fully explore the item space. To address this, we propose DeepRec, a novel LLM-based RS that enables autonomous multi-turn interactions between LLMs and TRMs for deep exploration of the item space. In each interaction turn, LLMs reason over user preferences and interact with TRMs to retrieve candidate items. After multi-turn interactions, LLMs rank the retrieved items to generate the final recommendations. We adopt reinforcement learning(RL) based optimization and propose novel designs from three aspects: recommendation model based data rollout, recommendation-oriented hierarchical rewards, and a two-stage RL training strategy. For data rollout, we introduce a preference-aware TRM, with which LLMs interact to construct trajectory data. For rewards, we design a hierarchical reward function that involves both process-level and outcome-level rewards to optimize the interaction process and recommendation performance, respectively. For RL training, we develop a two-stage training strategy, where the first stage aims to guide LLMs to interact with TRMs and the second stage focuses on performance improvement. Experiments on public datasets demonstrate that DeepRec significantly outperforms both traditional and LLM-based baselines, offering a new paradigm for deep exploration in recommendation systems."
  },
  {
    "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents",
    "url": "http://arxiv.org/abs/2505.16801v1",
    "arxiv_id": "2505.16801v1",
    "authors": [
      "Eleftherios Kalafatis",
      "Konstantinos Mitsis",
      "Konstantia Zarkogianni",
      "Maria Athanasiou",
      "Konstantina Nikita"
    ],
    "published": "2025-05-22T15:40:56+00:00",
    "summary": "Serious Games (SGs) are nowadays shifting focus to include procedural content generation (PCG) in the development process as a means of offering personalized and enhanced player experience. However, the development of a framework to assess the impact of PCG techniques when integrated into SGs remains particularly challenging. This study proposes a methodology for automated evaluation of PCG integration in SGs, incorporating deep reinforcement learning (DRL) game testing agents. To validate the proposed framework, a previously introduced SG featuring card game mechanics and incorporating three different versions of PCG for nonplayer character (NPC) creation has been deployed. Version 1 features random NPC creation, while versions 2 and 3 utilize a genetic algorithm approach. These versions are used to test the impact of different dynamic SG environments on the proposed framework's agents. The obtained results highlight the superiority of the DRL game testing agents trained on Versions 2 and 3 over those trained on Version 1 in terms of win rate (i.e. number of wins per played games) and training time. More specifically, within the execution of a test emulating regular gameplay, both Versions 2 and 3 peaked at a 97% win rate and achieved statistically significant higher (p=0009) win rates compared to those achieved in Version 1 that peaked at 94%. Overall, results advocate towards the proposed framework's capability to produce meaningful data for the evaluation of procedurally generated content in SGs."
  },
  {
    "title": "Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce",
    "url": "http://arxiv.org/abs/2505.16787v1",
    "arxiv_id": "2505.16787v1",
    "authors": [
      "Ashish Sundar",
      "Chunbo Luo",
      "Xiaoyang Wang"
    ],
    "published": "2025-05-22T15:28:50+00:00",
    "summary": "Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. MBRL methods have progressed by largely prioritising the actor; optimising the world model learning has been neglected meanwhile. Improving the fidelity of the world model and reducing its time to convergence can yield significant downstream benefits, one of which is improving the ensuing performance of any actor it may train. We propose a novel approach that anticipates and actively seeks out high-entropy states using short-horizon latent predictions generated by the world model, offering a principled alternative to traditional curiosity-driven methods that chase once-novel states well after they were stumbled into. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multi step plans after every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the weighting between reward and entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to just Dreamer as a proof of concept. Our method finishes the Miniworld procedurally generated mazes 50% faster than base Dreamer at convergence and the policy trained in imagination converges in only 60% of the environment steps that base Dreamer needs."
  },
  {
    "title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2505.16763v1",
    "arxiv_id": "2505.16763v1",
    "authors": [
      "Hongji Yang",
      "Yucheng Zhou",
      "Wencheng Han",
      "Jianbing Shen"
    ],
    "published": "2025-05-22T15:05:07+00:00",
    "summary": "Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors."
  },
  {
    "title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.16761v1",
    "arxiv_id": "2505.16761v1",
    "authors": [
      "Jian Liu",
      "Jing Xu",
      "Song Guo",
      "Jing Li",
      "Jingfeng Guo",
      "Jiaao Yu",
      "Haohan Weng",
      "Biwen Lei",
      "Xianghui Yang",
      "Zhuo Chen",
      "Fangqi Zhu",
      "Tao Han",
      "Chunchao Guo"
    ],
    "published": "2025-05-22T15:04:18+00:00",
    "summary": "Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present \\textbf{Mesh-RFT}, a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\% and improves Topology Score (TS) by 3.8\\% over pre-trained models, while outperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain. These results demonstrate Mesh-RFT's ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation. Project Page: \\href{https://hitcslj.github.io/mesh-rft/}{this https URL}."
  },
  {
    "title": "PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects",
    "url": "http://arxiv.org/abs/2505.16754v1",
    "arxiv_id": "2505.16754v1",
    "authors": [
      "Hannah Markgraf",
      "Michael Eichelbeck",
      "Daria Cappey",
      "Selin Demirt\u00fcrk",
      "Yara Schattschneider",
      "Matthias Althoff"
    ],
    "published": "2025-05-22T14:59:20+00:00",
    "summary": "Offline reinforcement learning (RL) has gained traction as a powerful paradigm for learning control policies from pre-collected data, eliminating the need for costly or risky online interactions. While many open-source libraries offer robust implementations of offline RL algorithms, they all rely on datasets composed of experience tuples consisting of state, action, next state, and reward. Managing, curating, and distributing such datasets requires suitable infrastructure. Although static datasets exist for established benchmark problems, no standardized or scalable solution supports developing and sharing datasets for novel or user-defined benchmarks. To address this gap, we introduce PyTupli, a Python-based tool to streamline the creation, storage, and dissemination of benchmark environments and their corresponding tuple datasets. PyTupli includes a lightweight client library with defined interfaces for uploading and retrieving benchmarks and data. It supports fine-grained filtering at both the episode and tuple level, allowing researchers to curate high-quality, task-specific datasets. A containerized server component enables production-ready deployment with authentication, access control, and automated certificate provisioning for secure use. By addressing key barriers in dataset infrastructure, PyTupli facilitates more collaborative, reproducible, and scalable offline RL research."
  },
  {
    "title": "Meta-reinforcement learning with minimum attention",
    "url": "http://arxiv.org/abs/2505.16741v1",
    "arxiv_id": "2505.16741v1",
    "authors": [
      "Pilhwa Lee",
      "Shashank Gupta"
    ],
    "published": "2025-05-22T14:53:06+00:00",
    "summary": "Minimum attention applies the least action principle in the changes of control concerning state and time, first proposed by Brockett. The involved regularization is highly relevant in emulating biological control, such as motor learning. We apply minimum attention in reinforcement learning (RL) as part of the rewards and investigate its connection to meta-learning and stabilization. Specifically, model-based meta-learning with minimum attention is explored in high-dimensional nonlinear dynamics. Ensemble-based model learning and gradient-based meta-policy learning are alternately performed. Empirically, we show that the minimum attention does show outperforming competence in comparison to the state-of-the-art algorithms in model-free and model-based RL, i.e., fast adaptation in few shots and variance reduction from the perturbations of the model and environment. Furthermore, the minimum attention demonstrates the improvement in energy efficiency."
  },
  {
    "title": "Maximum Total Correlation Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16734v1",
    "arxiv_id": "2505.16734v1",
    "authors": [
      "Bang You",
      "Puze Liu",
      "Huaping Liu",
      "Jan Peters",
      "Oleg Arenz"
    ],
    "published": "2025-05-22T14:48:00+00:00",
    "summary": "Simplicity is a powerful inductive bias. In reinforcement learning, regularization is used for simpler policies, data augmentation for simpler representations, and sparse reward functions for simpler objectives, all that, with the underlying motivation to increase generalizability and robustness by focusing on the essentials. Supplementary to these techniques, we investigate how to promote simple behavior throughout the episode. To that end, we introduce a modification of the reinforcement learning problem that additionally maximizes the total correlation within the induced trajectories. We propose a practical algorithm that optimizes all models, including policy and state representation, based on a lower-bound approximation. In simulated robot environments, our method naturally generates policies that induce periodic and compressible trajectories, and that exhibit superior robustness to noise and changes in dynamics compared to baseline methods, while also improving performance in the original tasks."
  },
  {
    "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO",
    "url": "http://arxiv.org/abs/2505.16673v1",
    "arxiv_id": "2505.16673v1",
    "authors": [
      "Huanjin Yao",
      "Qixiang Yin",
      "Jingyi Zhang",
      "Min Yang",
      "Yibo Wang",
      "Wenhao Wu",
      "Fei Su",
      "Li Shen",
      "Minghui Qiu",
      "Dacheng Tao",
      "Jiaxing Huang"
    ],
    "published": "2025-05-22T13:39:32+00:00",
    "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL."
  },
  {
    "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models",
    "url": "http://arxiv.org/abs/2505.16643v1",
    "arxiv_id": "2505.16643v1",
    "authors": [
      "Yiwei Sun",
      "Peiqi Jiang",
      "Chuanbin Liu",
      "Luohao Lin",
      "Zhiying Lu",
      "Hongtao Xie"
    ],
    "published": "2025-05-22T13:16:53+00:00",
    "summary": "While the safety risks of image-based large language models have been extensively studied, their video-based counterparts (Video LLMs) remain critically under-examined. To systematically study this problem, we introduce \\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and spans 19 principal risk categories across 10 language communities. \\textit{We reveal that integrating video modality degrades safety performance by an average of 42.3\\%, exposing systemic risks in multimodal attack exploitation.} To address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage framework achieving unprecedented safety gains through two innovations: (1) Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens into visual and textual sequences, enabling explicit harm perception across modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances defensive reasoning through dynamic policy optimization with rule-based rewards derived from dual-modality verification. These components synergize to shift safety alignment from passive harm recognition to active reasoning. The resulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves by 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard, and FigStep, respectively. \\textit{Our codes are available in the supplementary materials.} \\textcolor{red}{Warning: This paper contains examples of harmful language and videos, and reader discretion is recommended.}"
  },
  {
    "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation",
    "url": "http://arxiv.org/abs/2505.16637v1",
    "arxiv_id": "2505.16637v1",
    "authors": [
      "Wenjie Yang",
      "Mao Zheng",
      "Mingyang Song",
      "Zheng Li"
    ],
    "published": "2025-05-22T13:08:25+00:00",
    "summary": "Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models."
  },
  {
    "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering",
    "url": "http://arxiv.org/abs/2505.16582v1",
    "arxiv_id": "2505.16582v1",
    "authors": [
      "Jianbiao Mei",
      "Tao Hu",
      "Daocheng Fu",
      "Licheng Wen",
      "Xuemeng Yang",
      "Rong Wu",
      "Pinlong Cai",
      "Xing Gao",
      "Yu Yang",
      "Chengjun Xie",
      "Botian Shi",
      "Yong Liu",
      "Yu Qiao"
    ],
    "published": "2025-05-22T12:17:13+00:00",
    "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model's sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones."
  },
  {
    "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16581v1",
    "arxiv_id": "2505.16581v1",
    "authors": [
      "Max Weltevrede",
      "Moritz A. Zanger",
      "Matthijs T. J. Spaan",
      "Wendelin B\u00f6hmer"
    ],
    "published": "2025-05-22T12:15:52+00:00",
    "summary": "In the zero-shot policy transfer setting in reinforcement learning, the goal is to train an agent on a fixed set of training environments so that it can generalise to similar, but unseen, testing environments. Previous work has shown that policy distillation after training can sometimes produce a policy that outperforms the original in the testing environments. However, it is not yet entirely clear why that is, or what data should be used to distil the policy. In this paper, we prove, under certain assumptions, a generalisation bound for policy distillation after training. The theory provides two practical insights: for improved generalisation, you should 1) train an ensemble of distilled policies, and 2) distil it on as much data from the training environments as possible. We empirically verify that these insights hold in more general settings, when the assumptions required for the theory no longer hold. Finally, we demonstrate that an ensemble of policies distilled on a diverse dataset can generalise significantly better than the original agent."
  },
  {
    "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains",
    "url": "http://arxiv.org/abs/2505.16552v1",
    "arxiv_id": "2505.16552v1",
    "authors": [
      "Wenhui Tan",
      "Jiaze Li",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Jian Luan",
      "Ruihua Song"
    ],
    "published": "2025-05-22T11:40:26+00:00",
    "summary": "Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient. In this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach. First, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones. This approach enables CoLaR to: i) perform reasoning at a dense latent level (i.e., silently), substantially reducing reasoning chain length, and ii) dynamically adjust reasoning speed at inference time by simply prompting the desired compression factor. Extensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%. The code and models will be released upon acceptance."
  },
  {
    "title": "Incremental Sequence Classification with Temporal Consistency",
    "url": "http://arxiv.org/abs/2505.16548v1",
    "arxiv_id": "2505.16548v1",
    "authors": [
      "Lucas Maystre",
      "Gabriel Barello",
      "Tudor Berariu",
      "Aleix Cambray",
      "Rares Dolga",
      "Alvaro Ortega Gonzalez",
      "Andrei Nica",
      "David Barber"
    ],
    "published": "2025-05-22T11:37:53+00:00",
    "summary": "We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens."
  },
  {
    "title": "Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation",
    "url": "http://arxiv.org/abs/2505.16547v1",
    "arxiv_id": "2505.16547v1",
    "authors": [
      "Nitesh Subedi",
      "Hsin-Jung Yang",
      "Devesh K. Jha",
      "Soumik Sarkar"
    ],
    "published": "2025-05-22T11:37:39+00:00",
    "summary": "This paper presents an end-to-end deep reinforcement learning (RL) framework for occlusion-aware robotic manipulation in cluttered plant environments. Our approach enables a robot to interact with a deformable plant to reveal hidden objects of interest, such as fruits, using multimodal observations. We decouple the kinematic planning problem from robot control to simplify zero-shot sim2real transfer for the trained policy. Our results demonstrate that the trained policy, deployed using our framework, achieves up to 86.7% success in real-world trials across diverse initial conditions. Our findings pave the way toward autonomous, perception-driven agricultural robots that intelligently interact with complex foliage plants to \"find the fruit\" in challenging occluded scenarios, without the need for explicitly designed geometric and dynamic models of every plant scenario."
  },
  {
    "title": "ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.16517v1",
    "arxiv_id": "2505.16517v1",
    "authors": [
      "Zirui Song",
      "Guangxian Ouyang",
      "Mingzhe Li",
      "Yuheng Ji",
      "Chenxi Wang",
      "Zixiang Xu",
      "Zeyu Zhang",
      "Xiaoqing Zhang",
      "Qian Jiang",
      "Zhenhao Chen",
      "Zhongzhi Li",
      "Rui Yan",
      "Xiuying Chen"
    ],
    "published": "2025-05-22T10:57:07+00:00",
    "summary": "Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions."
  },
  {
    "title": "ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation",
    "url": "http://arxiv.org/abs/2505.16495v1",
    "arxiv_id": "2505.16495v1",
    "authors": [
      "Lingfeng Wang",
      "Hualing Lin",
      "Senda Chen",
      "Tao Wang",
      "Changxu Cheng",
      "Yangyang Zhong",
      "Dong Zheng",
      "Wuyue Zhao"
    ],
    "published": "2025-05-22T10:26:51+00:00",
    "summary": "While humans effortlessly draw visual objects and shapes by adaptively allocating attention based on their complexity, existing multimodal large language models (MLLMs) remain constrained by rigid token representations. Bridging this gap, we propose ALTo, an adaptive length tokenizer for autoregressive mask generation. To achieve this, a novel token length predictor is designed, along with a length regularization term and a differentiable token chunking strategy. We further build ALToLLM that seamlessly integrates ALTo into MLLM. Preferences on the trade-offs between mask quality and efficiency is implemented by group relative policy optimization (GRPO). Experiments demonstrate that ALToLLM achieves state-of-the-art performance with adaptive token cost on popular segmentation benchmarks. Code and models are released at https://github.com/yayafengzi/ALToLLM."
  },
  {
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16483v1",
    "arxiv_id": "2505.16483v1",
    "authors": [
      "Shuzheng Si",
      "Haozhe Zhao",
      "Cheng Gao",
      "Yuzhuo Bai",
      "Zhitong Wang",
      "Bofei Gao",
      "Kangyang Luo",
      "Wenhao Li",
      "Yufei Huang",
      "Gang Chen",
      "Fanchao Qi",
      "Minjia Zhang",
      "Baobao Chang",
      "Maosong Sun"
    ],
    "published": "2025-05-22T10:10:07+00:00",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1."
  },
  {
    "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection",
    "url": "http://arxiv.org/abs/2505.16475v1",
    "arxiv_id": "2505.16475v1",
    "authors": [
      "Jiaqi Li",
      "Xinyi Dong",
      "Yang Liu",
      "Zhizhuo Yang",
      "Quansen Wang",
      "Xiaobo Wang",
      "SongChun Zhu",
      "Zixia Jia",
      "Zilong Zheng"
    ],
    "published": "2025-05-22T10:03:05+00:00",
    "summary": "We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs' reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run."
  },
  {
    "title": "Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization",
    "url": "http://arxiv.org/abs/2505.16471v1",
    "arxiv_id": "2505.16471v1",
    "authors": [
      "Robbert Reijnen",
      "Yaoxin Wu",
      "Zaharah Bukhsh",
      "Yingqian Zhang"
    ],
    "published": "2025-05-22T09:53:54+00:00",
    "summary": "Deep reinforcement learning (DRL) has been widely used for dynamic algorithm configuration, particularly in evolutionary computation, which benefits from the adaptive update of parameters during the algorithmic execution. However, applying DRL to algorithm configuration for multi-objective combinatorial optimization (MOCO) problems remains relatively unexplored. This paper presents a novel graph neural network (GNN) based DRL to configure multi-objective evolutionary algorithms. We model the dynamic algorithm configuration as a Markov decision process, representing the convergence of solutions in the objective space by a graph, with their embeddings learned by a GNN to enhance the state representation. Experiments on diverse MOCO challenges indicate that our method outperforms traditional and DRL-based algorithm configuration methods in terms of efficacy and adaptability. It also exhibits advantageous generalizability across objective types and problem sizes, and applicability to different evolutionary computation methods."
  },
  {
    "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16421v1",
    "arxiv_id": "2505.16421v1",
    "authors": [
      "Zhepei Wei",
      "Wenlin Yao",
      "Yao Liu",
      "Weizhi Zhang",
      "Qin Lu",
      "Liang Qiu",
      "Changlong Yu",
      "Puyang Xu",
      "Chao Zhang",
      "Bing Yin",
      "Hyokun Yun",
      "Lihong Li"
    ],
    "published": "2025-05-22T09:07:43+00:00",
    "summary": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents."
  },
  {
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16410v1",
    "arxiv_id": "2505.16410v1",
    "authors": [
      "Guanting Dong",
      "Yifei Chen",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Hongjin Qian",
      "Yutao Zhu",
      "Hangyu Mao",
      "Guorui Zhou",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-22T09:00:19+00:00",
    "summary": "Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star."
  },
  {
    "title": "Divide-Fuse-Conquer: Eliciting \"Aha Moments\" in Multi-Scenario Games",
    "url": "http://arxiv.org/abs/2505.16401v1",
    "arxiv_id": "2505.16401v1",
    "authors": [
      "Xiaoqing Zhang",
      "Huabin Zheng",
      "Ang Lv",
      "Yuhan Liu",
      "Zirui Song",
      "Flood Sung",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "published": "2025-05-22T08:52:21+00:00",
    "summary": "Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs."
  },
  {
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16400v1",
    "arxiv_id": "2505.16400v1",
    "authors": [
      "Yang Chen",
      "Zhuolin Yang",
      "Zihan Liu",
      "Chankyu Lee",
      "Peng Xu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "published": "2025-05-22T08:50:47+00:00",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable."
  },
  {
    "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
    "url": "http://arxiv.org/abs/2505.16394v1",
    "arxiv_id": "2505.16394v1",
    "authors": [
      "Zhenjie Yang",
      "Xiaosong Jia",
      "Qifeng Li",
      "Xue Yang",
      "Maoqing Yao",
      "Junchi Yan"
    ],
    "published": "2025-05-22T08:46:53+00:00",
    "summary": "Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance."
  },
  {
    "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.16377v1",
    "arxiv_id": "2505.16377v1",
    "authors": [
      "Yansong Qu",
      "Zilin Huang",
      "Zihao Sheng",
      "Jiancong Chen",
      "Sikai Chen",
      "Samuel Labi"
    ],
    "published": "2025-05-22T08:29:59+00:00",
    "summary": "Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of \"safety\" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: https://ys-qu.github.io/vlsafe-website/"
  },
  {
    "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.16368v1",
    "arxiv_id": "2505.16368v1",
    "authors": [
      "Huanyu Liu",
      "Jia Li",
      "Hao Zhu",
      "Kechi Zhang",
      "Yihong Dong",
      "Ge Li"
    ],
    "published": "2025-05-22T08:23:10+00:00",
    "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research."
  },
  {
    "title": "ReCopilot: Reverse Engineering Copilot in Binary Analysis",
    "url": "http://arxiv.org/abs/2505.16366v1",
    "arxiv_id": "2505.16366v1",
    "authors": [
      "Guoqiang Chen",
      "Huiqi Sun",
      "Daguang Liu",
      "Zhiqi Wang",
      "Qiang Wang",
      "Bin Yin",
      "Lu Liu",
      "Lingyun Ying"
    ],
    "published": "2025-05-22T08:21:39+00:00",
    "summary": "Binary analysis plays a pivotal role in security domains such as malware detection and vulnerability discovery, yet it remains labor-intensive and heavily reliant on expert knowledge. General-purpose large language models (LLMs) perform well in programming analysis on source code, while binaryspecific LLMs are underexplored. In this work, we present ReCopilot, an expert LLM designed for binary analysis tasks. ReCopilot integrates binary code knowledge through a meticulously constructed dataset, encompassing continue pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO) stages. It leverages variable data flow and call graph to enhance context awareness and employs test-time scaling to improve reasoning capabilities. Evaluations on a comprehensive binary analysis benchmark demonstrate that ReCopilot achieves state-of-the-art performance in tasks such as function name recovery and variable type inference on the decompiled pseudo code, outperforming both existing tools and LLMs by 13%. Our findings highlight the effectiveness of domain-specific training and context enhancement, while also revealing challenges in building super long chain-of-thought. ReCopilot represents a significant step toward automating binary analysis with interpretable and scalable AI assistance in this domain."
  },
  {
    "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training",
    "url": "http://arxiv.org/abs/2505.16363v1",
    "arxiv_id": "2505.16363v1",
    "authors": [
      "Huishuai Zhang",
      "Bohan Wang",
      "Luoxin Chen"
    ],
    "published": "2025-05-22T08:16:48+00:00",
    "summary": "We introduce AdamS, a simple yet effective alternative to Adam for large language model (LLM) pretraining and post-training. By leveraging a novel denominator, i.e., the root of weighted sum of squares of the momentum and the current gradient, AdamS eliminates the need for second-moment estimates. Hence, AdamS is efficient, matching the memory and compute footprint of SGD with momentum while delivering superior optimization performance. Moreover, AdamS is easy to adopt: it can directly inherit hyperparameters of AdamW, and is entirely model-agnostic, integrating seamlessly into existing pipelines without modifications to optimizer APIs or architectures. The motivation behind AdamS stems from the observed $(L_0, L_1)$ smoothness properties in transformer objectives, where local smoothness is governed by gradient magnitudes that can be further approximated by momentum magnitudes. We establish rigorous theoretical convergence guarantees and provide practical guidelines for hyperparameter selection. Empirically, AdamS demonstrates strong performance in various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B parameters) and reinforcement learning in post-training regimes. With its efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling alternative to existing optimizers."
  },
  {
    "title": "Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.16353v1",
    "arxiv_id": "2505.16353v1",
    "authors": [
      "C\u00e9line Comte",
      "Pascal Moyal"
    ],
    "published": "2025-05-22T08:05:36+00:00",
    "summary": "In this paper, we introduce a versatile scheme for optimizing the arrival rates of quasi-reversible queueing systems. We first propose an alternative definition of quasi-reversibility that encompasses reversibility and highlights the importance of the definition of customer classes. In a second time, we introduce balanced arrival control policies, which generalize the notion of balanced arrival rates introduced in the context of Whittle networks, to the much broader class of quasi-reversible queueing systems. We prove that supplementing a quasi-reversible queueing system with a balanced arrival-control policy preserves the quasi-reversibility, and we specify the form of the stationary measures. We revisit two canonical examples of quasi-reversible queueing systems, Whittle networks and order-independent queues. Lastly, we focus on the problem of admission control and leverage our results in the frameworks of optimization and reinforcement learning."
  },
  {
    "title": "Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing",
    "url": "http://arxiv.org/abs/2505.16332v1",
    "arxiv_id": "2505.16332v1",
    "authors": [
      "Zhehui Wanga",
      "Benjamin Chen Ming Choonga",
      "Tian Huang",
      "Daniel Gerlinghoffa",
      "Rick Siow Mong Goh",
      "Cheng Liu",
      "Tao Luo"
    ],
    "published": "2025-05-22T07:40:23+00:00",
    "summary": "Quantum optimization is the most mature quantum computing technology to date, providing a promising approach towards efficiently solving complex combinatorial problems. Methods such as adiabatic quantum computing (AQC) have been employed in recent years on important optimization problems across various domains. In deep learning, deep neural networks (DNN) have reached immense sizes to support new predictive capabilities. Optimization of large-scale models is critical for sustainable deployment, but becomes increasingly challenging with ever-growing model sizes and complexity. While quantum optimization is suitable for solving complex problems, its application to DNN optimization is not straightforward, requiring thorough reformulation for compatibility with commercially available quantum devices. In this work, we explore the potential of adopting AQC for fine-grained pruning-quantization of convolutional neural networks. We rework established heuristics to formulate model compression as a quadratic unconstrained binary optimization (QUBO) problem, and assess the solution space offered by commercial quantum annealing devices. Through our exploratory efforts of reformulation, we demonstrate that AQC can achieve effective compression of practical DNN models. Experiments demonstrate that adiabatic quantum computing (AQC) not only outperforms classical algorithms like genetic algorithms and reinforcement learning in terms of time efficiency but also excels at identifying global optima."
  },
  {
    "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.16315v1",
    "arxiv_id": "2505.16315v1",
    "authors": [
      "Xiaoxue Cheng",
      "Junyi Li",
      "Zhenduo Zhang",
      "Xinyu Tang",
      "Wayne Xin Zhao",
      "Xinyu Kong",
      "Zhiqiang Zhang"
    ],
    "published": "2025-05-22T07:15:08+00:00",
    "summary": "Large reasoning models (LRMs) have demonstrated strong performance on complex reasoning tasks, but often suffer from overthinking, generating redundant content regardless of task difficulty. Inspired by the dual process theory in cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that enables LRMs to achieve efficient reasoning through adaptive cognitive allocation and dynamic system switch. ACPO incorporates two key components: (1) introducing system-aware reasoning tokens to explicitly represent the thinking modes thereby making the model's cognitive process transparent, and (2) integrating online difficulty estimation and token length budget to guide adaptive system switch and reasoning during reinforcement learning. To this end, we propose a two-stage training strategy. The first stage begins with supervised fine-tuning to cold start the model, enabling it to generate reasoning paths with explicit thinking modes. In the second stage, we apply ACPO to further enhance adaptive system switch for difficulty-aware reasoning. Experimental results demonstrate that ACPO effectively reduces redundant reasoning while adaptively adjusting cognitive allocation based on task complexity, achieving efficient hybrid reasoning."
  },
  {
    "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay",
    "url": "http://arxiv.org/abs/2505.16282v1",
    "arxiv_id": "2505.16282v1",
    "authors": [
      "Fanbin Lu",
      "Zhisheng Zhong",
      "Shu Liu",
      "Chi-Wing Fu",
      "Jiaya Jia"
    ],
    "published": "2025-05-22T06:24:32+00:00",
    "summary": "Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:https://github.com/dvlab-research/ARPO.git."
  },
  {
    "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
    "url": "http://arxiv.org/abs/2505.15810v1",
    "arxiv_id": "2505.15810v1",
    "authors": [
      "Yuqi Zhou",
      "Sunhao Dai",
      "Shuai Wang",
      "Kaiwen Zhou",
      "Qinqlin Jia",
      "Junxu"
    ],
    "published": "2025-05-21T17:59:09+00:00",
    "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1."
  },
  {
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "url": "http://arxiv.org/abs/2505.15809v1",
    "arxiv_id": "2505.15809v1",
    "authors": [
      "Ling Yang",
      "Ye Tian",
      "Bowen Li",
      "Xinchen Zhang",
      "Ke Shen",
      "Yunhai Tong",
      "Mengdi Wang"
    ],
    "published": "2025-05-21T17:59:05+00:00",
    "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA"
  },
  {
    "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
    "url": "http://arxiv.org/abs/2505.15804v1",
    "arxiv_id": "2505.15804v1",
    "authors": [
      "Zongzhao Li",
      "Zongyang Ma",
      "Mingze Li",
      "Songyou Li",
      "Yu Rong",
      "Tingyang Xu",
      "Ziqi Zhang",
      "Deli Zhao",
      "Wenbing Huang"
    ],
    "published": "2025-05-21T17:57:38+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1."
  },
  {
    "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "url": "http://arxiv.org/abs/2505.15801v1",
    "arxiv_id": "2505.15801v1",
    "authors": [
      "Yuchen Yan",
      "Jin Jiang",
      "Zhenbang Ren",
      "Yijun Li",
      "Xudong Cai",
      "Yang Liu",
      "Xin Xu",
      "Mengdi Zhang",
      "Jian Shao",
      "Yongliang Shen",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-05-21T17:54:43+00:00",
    "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks."
  },
  {
    "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15795v1",
    "arxiv_id": "2505.15795v1",
    "authors": [
      "Lisa Alazraki",
      "Tan Yi-Chern",
      "Jon Ander Campos",
      "Maximilian Mozes",
      "Marek Rei",
      "Max Bartolo"
    ],
    "published": "2025-05-21T17:48:16+00:00",
    "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks."
  },
  {
    "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15793v1",
    "arxiv_id": "2505.15793v1",
    "authors": [
      "Zhiwen Chen",
      "Bo Leng",
      "Zhuoren Li",
      "Hanming Deng",
      "Guizhe Jin",
      "Ran Yu",
      "Huanxi Wen"
    ],
    "published": "2025-05-21T17:47:24+00:00",
    "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can enhance autonomous driving (AD) performance in complex scenarios. However, current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to hallucinations.Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes Augmented Semantic Representation Module to extend state space. Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMP's strong overall driving performance. HCRMP achieves a task success rate of up to 80.3% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4%, which effectively improves the driving performance in complex scenarios."
  },
  {
    "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL",
    "url": "http://arxiv.org/abs/2505.15791v1",
    "arxiv_id": "2505.15791v1",
    "authors": [
      "Fengyuan Dai",
      "Zifeng Zhuang",
      "Yufei Huang",
      "Siteng Huang",
      "Bangyan Liao",
      "Donglin Wang",
      "Fajie Yuan"
    ],
    "published": "2025-05-21T17:44:37+00:00",
    "summary": "Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions."
  },
  {
    "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15776v1",
    "arxiv_id": "2505.15776v1",
    "authors": [
      "Changtai Zhu",
      "Siyin Wang",
      "Ruijun Feng",
      "Kai Song",
      "Xipeng Qiu"
    ],
    "published": "2025-05-21T17:27:42+00:00",
    "summary": "Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision."
  },
  {
    "title": "Improving planning and MBRL with temporally-extended actions",
    "url": "http://arxiv.org/abs/2505.15754v1",
    "arxiv_id": "2505.15754v1",
    "authors": [
      "Palash Chatterjee",
      "Roni Khardon"
    ],
    "published": "2025-05-21T16:59:32+00:00",
    "summary": "Continuous time systems are often modeled using discrete time dynamics but this requires a small simulation step to maintain accuracy. In turn, this requires a large planning horizon which leads to computationally demanding planning problems and reduced performance. Previous work in model free reinforcement learning has partially addressed this issue using action repeats where a policy is learned to determine a discrete action duration. Instead we propose to control the continuous decision timescale directly by using temporally-extended actions and letting the planner treat the duration of the action as an additional optimization variable along with the standard action variables. This additional structure has multiple advantages. It speeds up simulation time of trajectories and, importantly, it allows for deep horizon search in terms of primitive actions while using a shallow search depth in the planner. In addition, in the model based reinforcement learning (MBRL) setting, it reduces compounding errors from model learning and improves training time for models. We show that this idea is effective and that the range for action durations can be automatically selected using a multi-armed bandit formulation and integrated into the MBRL framework. An extensive experimental evaluation both in planning and in MBRL, shows that our approach yields faster planning, better solutions, and that it enables solutions to problems that are not solved in the standard formulation."
  },
  {
    "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
    "url": "http://arxiv.org/abs/2505.15694v1",
    "arxiv_id": "2505.15694v1",
    "authors": [
      "Xingyu Zhou",
      "Yulian Wu",
      "Francesco Orabona"
    ],
    "published": "2025-05-21T16:07:47+00:00",
    "summary": "In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios."
  },
  {
    "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives",
    "url": "http://arxiv.org/abs/2505.15693v1",
    "arxiv_id": "2505.15693v1",
    "authors": [
      "Milad Kazemi",
      "Mateo Perez",
      "Fabio Somenzi",
      "Sadegh Soudjani",
      "Ashutosh Trivedi",
      "Alvaro Velasquez"
    ],
    "published": "2025-05-21T16:06:51+00:00",
    "summary": "Recent advances in reinforcement learning (RL) have renewed focus on the design of reward functions that shape agent behavior. Manually designing reward functions is tedious and error-prone. A principled alternative is to specify behaviors in a formal language that can be automatically translated into rewards. Omega-regular languages are a natural choice for this purpose, given their established role in formal verification and synthesis. However, existing methods using omega-regular specifications typically rely on discounted reward RL in episodic settings, with periodic resets. This setup misaligns with the semantics of omega-regular specifications, which describe properties over infinite behavior traces. In such cases, the average reward criterion and the continuing setting -- where the agent interacts with the environment over a single, uninterrupted lifetime -- are more appropriate.   To address the challenges of infinite-horizon, continuing tasks, we focus on absolute liveness specifications -- a subclass of omega-regular languages that cannot be violated by any finite behavior prefix, making them well-suited to the continuing setting. We present the first model-free RL framework that translates absolute liveness specifications to average-reward objectives. Our approach enables learning in communicating MDPs without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization, aiming to maximize an external average-reward objective among the policies that also maximize the satisfaction probability of a given omega-regular specification. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full knowledge of the environment, thus enabling model-free RL. Empirical results show our average-reward approach in continuing setting outperforms discount-based methods across benchmarks."
  },
  {
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities",
    "url": "http://arxiv.org/abs/2505.15692v1",
    "arxiv_id": "2505.15692v1",
    "authors": [
      "Jinyang Wu",
      "Chonghua Liao",
      "Mingkuan Feng",
      "Shuai Zhang",
      "Zhengqi Wen",
      "Pengpeng Shao",
      "Huazhe Xu",
      "Jianhua Tao"
    ],
    "published": "2025-05-21T16:06:10+00:00",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (\"thought patterns\"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability."
  },
  {
    "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning",
    "url": "http://arxiv.org/abs/2505.15687v1",
    "arxiv_id": "2505.15687v1",
    "authors": [
      "Zhe Xu",
      "Cheng Jin",
      "Yihui Wang",
      "Ziyi Liu",
      "Hao Chen"
    ],
    "published": "2025-05-21T16:03:03+00:00",
    "summary": "Multimodal pathological image understanding has garnered widespread interest due to its potential to improve diagnostic accuracy and enable personalized treatment through integrated visual and textual data. However, existing methods exhibit limited reasoning capabilities, which hamper their ability to handle complex diagnostic scenarios. Additionally, the enormous size of pathological images leads to severe computational burdens, further restricting their practical deployment. To address these limitations, we introduce a novel bilateral reinforcement learning framework comprising two synergistic branches. One reinforcement branch enhances the reasoning capability by enabling the model to learn task-specific decision processes, i.e., pathology rationales, directly from labels without explicit reasoning supervision. While the other branch dynamically allocates a tailored number of tokens to different images based on both their visual content and task context, thereby optimizing computational efficiency. We apply our method to various pathological tasks such as visual question answering, cancer subtyping, and lesion detection. Extensive experiments show an average +41.7 absolute performance improvement with 70.3% lower inference costs over the base models, achieving both reasoning accuracy and computational efficiency."
  },
  {
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "url": "http://arxiv.org/abs/2505.15612v1",
    "arxiv_id": "2505.15612v1",
    "authors": [
      "Wei Liu",
      "Ruochen Zhou",
      "Yiyun Deng",
      "Yuzhen Huang",
      "Junteng Liu",
      "Yuntian Deng",
      "Yizhe Zhang",
      "Junxian He"
    ],
    "published": "2025-05-21T15:03:26+00:00",
    "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant \"self-reflections\". Resources are at https://github.com/hkust-nlp/Laser."
  },
  {
    "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15607v1",
    "arxiv_id": "2505.15607v1",
    "authors": [
      "David Dinucu-Jianu",
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "published": "2025-05-21T15:00:07+00:00",
    "summary": "Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning."
  },
  {
    "title": "World Models as Reference Trajectories for Rapid Motor Adaptation",
    "url": "http://arxiv.org/abs/2505.15589v1",
    "arxiv_id": "2505.15589v1",
    "authors": [
      "Carlos Stein Brito",
      "Daniel McNamee"
    ],
    "published": "2025-05-21T14:46:41+00:00",
    "summary": "Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics."
  },
  {
    "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback",
    "url": "http://arxiv.org/abs/2505.15572v1",
    "arxiv_id": "2505.15572v1",
    "authors": [
      "Wangyang Ying",
      "Haoyue Bai",
      "Nanxu Gong",
      "Xinyuan Wang",
      "Sixun Dong",
      "Haifeng Chen",
      "Yanjie Fu"
    ],
    "published": "2025-05-21T14:25:41+00:00",
    "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable mathematical equations that map observed values to labels, offering physical insights and broad applicability across academic and industrial domains. Genetic programming and traditional deep learning-based approaches suffer from search inefficiency and poor generalization on small task-specific datasets. Foundation models showed promise in this area, but existing approaches suffer from: 1) They are pretrained on general-purpose data distributions, making them less effective for domain-specific tasks; and 2) their training objectives focus on token-level alignment, overlooking mathematical semantics, which can lead to inaccurate equations. To address these issues, we aim to enhance the domain adaptability of foundation models for Data2Eqn tasks. In this work, we propose a reinforcement learning-based finetuning framework that directly optimizes the generation policy of a pretrained model through reward signals derived from downstream numerical fitness. Our method allows the model to adapt to specific and complex data distributions and generate mathematically meaningful equations. Extensive experiments demonstrate that our approach improves both the accuracy and robustness of equation generation under complex distributions."
  },
  {
    "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A Generative AI Framework with Multi-Agent Learning",
    "url": "http://arxiv.org/abs/2505.15571v1",
    "arxiv_id": "2505.15571v1",
    "authors": [
      "Changyuan Zhao",
      "Ruichen Zhang",
      "Jiacheng Wang",
      "Dusit Niyato",
      "Geng Sun",
      "Hongyang Du",
      "Zan Li",
      "Abbas Jamalipour",
      "Dong In Kim"
    ],
    "published": "2025-05-21T14:25:31+00:00",
    "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored for temporal spectrum cartography in low-altitude economy networks (LAENets). LAENets, characterized by diverse aerial devices such as UAVs, rely heavily on wireless communication technologies while facing challenges, including spectrum congestion and dynamic environmental interference. Traditional spectrum cartography methods have limitations in handling the temporal and spatial complexities inherent to these networks. Addressing these challenges, the proposed framework first employs a Reconstructive Masked Autoencoder (RecMAE) capable of accurately reconstructing spectrum maps from sparse and temporally varying sensor data using a novel dual-mask mechanism. This approach significantly enhances the precision of reconstructed radio frequency (RF) power maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method integrates diffusion-based reinforcement learning to optimize the trajectories of dynamic UAV sensors. By leveraging temporal-attention encoding, this method effectively manages spatial exploration and exploitation to minimize cumulative reconstruction errors. Extensive numerical experiments validate that this integrated GenAI framework outperforms traditional interpolation methods and deep learning baselines by achieving 57.35% and 88.68% reconstruction error reduction, respectively. The proposed trajectory planner substantially improves spectrum map accuracy, reconstruction stability, and sensor deployment efficiency in dynamically evolving low-altitude environments."
  },
  {
    "title": "A Temporal Difference Method for Stochastic Continuous Dynamics",
    "url": "http://arxiv.org/abs/2505.15544v1",
    "arxiv_id": "2505.15544v1",
    "authors": [
      "Haruki Settai",
      "Naoya Takeishi",
      "Takehisa Yairi"
    ],
    "published": "2025-05-21T14:08:34+00:00",
    "summary": "For continuous systems modeled by dynamical equations such as ODEs and SDEs, Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman (HJB) equation, which provides the theoretical target of reinforcement learning (RL). Although recent advances in RL successfully leverage this formulation, the existing methods typically assume the underlying dynamics are known a priori because they need explicit access to the coefficient functions of dynamical equations to update the value function following the HJB equation. We address this inherent limitation of HJB-based RL; we propose a model-free approach still targeting the HJB equation and propose the corresponding temporal difference method. We demonstrate its potential advantages over transition kernel-based formulations, both qualitatively and empirically. The proposed formulation paves the way toward bridging stochastic optimal control and model-free reinforcement learning."
  },
  {
    "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization",
    "url": "http://arxiv.org/abs/2505.15514v1",
    "arxiv_id": "2505.15514v1",
    "authors": [
      "Soham Sane"
    ],
    "published": "2025-05-21T13:38:45+00:00",
    "summary": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm that heavily relies on accurate advantage estimates for stable and efficient training. However, raw advantage signals can exhibit significant variance, noise, and scale-related issues, impeding optimal learning performance. To address this challenge, we introduce Advantage Modulation PPO (AM-PPO), a novel enhancement of PPO that adaptively modulates advantage estimates using a dynamic, non-linear scaling mechanism. This adaptive modulation employs an alpha controller that dynamically adjusts the scaling factor based on evolving statistical properties of the advantage signals, such as their norm, variance, and a predefined target saturation level. By incorporating a tanh-based gating function driven by these adaptively scaled advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates and improve the conditioning of the policy gradient landscape. Crucially, this modulation also influences value function training by providing consistent and adaptively conditioned learning targets. Empirical evaluations across standard continuous control benchmarks demonstrate that AM-PPO achieves superior reward trajectories, exhibits sustained learning progression, and significantly reduces the clipping required by adaptive optimizers. These findings underscore the potential of advantage modulation as a broadly applicable technique for enhancing reinforcement learning optimization."
  },
  {
    "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment",
    "url": "http://arxiv.org/abs/2505.15456v1",
    "arxiv_id": "2505.15456v1",
    "authors": [
      "Weixiang Zhao",
      "Xingyu Sui",
      "Yulin Hu",
      "Jiahe Guo",
      "Haixiao Liu",
      "Biye Li",
      "Yanyan Zhao",
      "Bing Qin",
      "Ting Liu"
    ],
    "published": "2025-05-21T12:38:36+00:00",
    "summary": "Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios and long-term personalization due to their inherently static and shallow designs. In this work, we introduce the Reinforcement Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts with a simulated user model to iteratively infer and refine user profiles through dialogue. The training process is guided by a dual-level reward structure: the Profile Reward encourages accurate construction of user representations, while the Response Reward incentivizes generation of responses consistent with the inferred profile. We instantiate RLPA by fine-tuning Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art performance in personalized dialogue. Empirical evaluations demonstrate that Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines, and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o. Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting user preferences, sustaining long-term personalization and delivering more efficient inference compared to recent reasoning-focused LLMs. These results emphasize the potential of dynamic profile inference as a more effective paradigm for building personalized dialogue systems."
  },
  {
    "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15447v1",
    "arxiv_id": "2505.15447v1",
    "authors": [
      "Ziqiang Xu",
      "Qi Dai",
      "Tian Xie",
      "Yifan Yang",
      "Kai Qiu",
      "DongDong Chen",
      "Zuxuan Wu",
      "Chong Luo"
    ],
    "published": "2025-05-21T12:29:40+00:00",
    "summary": "Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding."
  },
  {
    "title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL",
    "url": "http://arxiv.org/abs/2505.15436v1",
    "arxiv_id": "2505.15436v1",
    "authors": [
      "Xintong Zhang",
      "Zhi Gao",
      "Bofei Zhang",
      "Pengxiang Li",
      "Xiaowen Zhang",
      "Yang Liu",
      "Tao Yuan",
      "Yuwei Wu",
      "Yunde Jia",
      "Song-Chun Zhu",
      "Qing Li"
    ],
    "published": "2025-05-21T12:18:15+00:00",
    "summary": "Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications."
  },
  {
    "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
    "url": "http://arxiv.org/abs/2505.15431v1",
    "arxiv_id": "2505.15431v1",
    "authors": [
      "Ao Liu",
      "Botong Zhou",
      "Can Xu",
      "Chayse Zhou",
      "ChenChen Zhang",
      "Chengcheng Xu",
      "Chenhao Wang",
      "Decheng Wu",
      "Dengpeng Wu",
      "Dian Jiao",
      "Dong Du",
      "Dong Wang",
      "Feng Zhang",
      "Fengzong Lian",
      "Guanghui Xu",
      "Guanwei Zhang",
      "Hai Wang",
      "Haipeng Luo",
      "Han Hu",
      "Huilin Xu",
      "Jiajia Wu",
      "Jianchen Zhu",
      "Jianfeng Yan",
      "Jiaqi Zhu",
      "Jihong Zhang",
      "Jinbao Xue",
      "Jun Xia",
      "Junqiang Zheng",
      "Kai Liu",
      "Kai Zhang",
      "Kai Zheng",
      "Kejiao Li",
      "Keyao Wang",
      "Lan Jiang",
      "Lixin Liu",
      "Lulu Wu",
      "Mengyuan Huang",
      "Peijie Yu",
      "Peiqi Wang",
      "Qian Wang",
      "Qianbiao Xiang",
      "Qibin Liu",
      "Qingfeng Sun",
      "Richard Guo",
      "Ruobing Xie",
      "Saiyong Yang",
      "Shaohua Chen",
      "Shihui Hu",
      "Shuai Li",
      "Shuaipeng Li",
      "Shuang Chen",
      "Suncong Zheng",
      "Tao Yang",
      "Tian Zhang",
      "Tinghao Yu",
      "Weidong Han",
      "Weijie Liu",
      "Weijin Zhou",
      "Weikang Wang",
      "Wesleye Chen",
      "Xiao Feng",
      "Xiaoqin Ren",
      "Xingwu Sun",
      "Xiong Kuang",
      "Xuemeng Huang",
      "Xun Cao",
      "Yanfeng Chen",
      "Yang Du",
      "Yang Zhen",
      "Yangyu Tao",
      "Yaping Deng",
      "Yi Shen",
      "Yigeng Hong",
      "Yiqi Chen",
      "Yiqing Huang",
      "Yuchi Deng",
      "Yue Mao",
      "Yulong Wang",
      "Yuyuan Zeng",
      "Zenan Xu",
      "Zhanhui Kang",
      "Zhe Zhao",
      "ZhenXiang Yan",
      "Zheng Fang",
      "Zhichao Hu",
      "Zhongzhi Chen",
      "Zhuoyu Li",
      "Zongwei Li",
      "Alex Yan",
      "Ande Liang",
      "Baitong Liu",
      "Beiping Pan",
      "Bin Xing",
      "Binghong Wu",
      "Bingxin Qu",
      "Bolin Ni",
      "Boyu Wu",
      "Chen Li",
      "Cheng Jiang",
      "Cheng Zhang",
      "Chengjun Liu",
      "Chengxu Yang",
      "Chiyu Wang",
      "Chong Zha",
      "Daisy Yi",
      "Di Wang",
      "Fanyang Lu",
      "Fei Chen",
      "Feifei Liu",
      "Feng Zheng",
      "Guanghua Yu",
      "Guiyang Li",
      "Guohua Wang",
      "Haisheng Lin",
      "Han Liu",
      "Han Wang",
      "Hao Fei",
      "Hao Lu",
      "Haoqing Jiang",
      "Haoran Sun",
      "Haotian Zhu",
      "Huangjin Dai",
      "Huankui Chen",
      "Huawen Feng",
      "Huihui Cai",
      "Huxin Peng",
      "Jackson Lv",
      "Jiacheng Shi",
      "Jiahao Bu",
      "Jianbo Li",
      "Jianglu Hu",
      "Jiangtao Guan",
      "Jianing Xu",
      "Jianwei Cai",
      "Jiarong Zhang",
      "Jiawei Song",
      "Jie Jiang",
      "Jie Liu",
      "Jieneng Yang",
      "Jihong Zhang",
      "Jin lv",
      "Jing Zhao",
      "Jinjian Li",
      "Jinxing Liu",
      "Jun Zhao",
      "Juntao Guo",
      "Kai Wang",
      "Kan Wu",
      "Lei Fu",
      "Lei He",
      "Lei Wang",
      "Li Liu",
      "Liang Dong",
      "Liya Zhan",
      "Long Cheng",
      "Long Xu",
      "Mao Zheng",
      "Meng Liu",
      "Mengkang Hu",
      "Nanli Chen",
      "Peirui Chen",
      "Peng He",
      "Pengju Pan",
      "Pengzhi Wei",
      "Qi Yang",
      "Qi Yi",
      "Roberts Wang",
      "Rongpeng Chen",
      "Rui Sun",
      "Rui Yang",
      "Ruibin Chen",
      "Ruixu Zhou",
      "Shaofeng Zhang",
      "Sheng Zhang",
      "Shihao Xu",
      "Shuaishuai Chang",
      "Shulin Liu",
      "SiQi Wang",
      "Songjia Feng",
      "Songling Yuan",
      "Tao Zhang",
      "Tianjiao Lang",
      "Tongkai Li",
      "Wei Deng",
      "Wei Li",
      "Weichao Wang",
      "Weigang Zhang",
      "Weixuan Sun",
      "Wen Ouyang",
      "Wenxiang Jiao",
      "Wenzhi Sun",
      "Wenzhuo Jia",
      "Xiang Zhang",
      "Xiangyu He",
      "Xianshun Ren",
      "XiaoYing Zhu",
      "Xiaolong Guo",
      "Xiaoxue Li",
      "Xiaoyu Ma",
      "Xican Lu",
      "Xinhua Feng",
      "Xinting Huang",
      "Xinyu Guan",
      "Xirui Li",
      "Xu Zhang",
      "Xudong Gao",
      "Xun Luo",
      "Xuxiang Qi",
      "Yangkun Chen",
      "Yangyu Tao",
      "Yanling Xiao",
      "Yantao Mai",
      "Yanze Chen",
      "Yao Ding",
      "Yeting Yang",
      "YiFan Song",
      "Yifan Yang",
      "Yijiao Zhu",
      "Yinhe Wu",
      "Yixian Liu",
      "Yong Yang",
      "Yuanjun Cai",
      "Yuanlin Tu",
      "Yue Zhang",
      "Yufei Huang",
      "Yuhang Zhou",
      "Yuhao Jiang",
      "Yuhong Liu",
      "Yuhui Hu",
      "Yujin Lin",
      "Yun Yang",
      "Yunhao Wang",
      "Yusong Zhang",
      "Zekun Wu",
      "Zelong Zhang",
      "Zhan Yu",
      "Zhaoliang Yang",
      "Zhe Zhao",
      "Zheng Li",
      "Zhenyu Huang",
      "Zhiguang Liu",
      "Zhijiang Xu",
      "Zhiqing Kui",
      "Zhiyin Zeng",
      "Zhiyuan Xiong",
      "Zhuo Han",
      "Zifan Wu",
      "Zigang Geng",
      "Zilong Zhao",
      "Ziyan Tang",
      "Ziyuan Zhu",
      "Zonglei Zhu",
      "Zhijiang Xu"
    ],
    "published": "2025-05-21T12:11:53+00:00",
    "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep \"thinking\" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models."
  },
  {
    "title": "Guided Policy Optimization under Partial Observability",
    "url": "http://arxiv.org/abs/2505.15418v1",
    "arxiv_id": "2505.15418v1",
    "authors": [
      "Yueheng Li",
      "Guangming Xie",
      "Zongqing Lu"
    ],
    "published": "2025-05-21T12:01:08+00:00",
    "summary": "Reinforcement Learning (RL) in partially observable environments poses significant challenges due to the complexity of learning under uncertainty. While additional information, such as that available in simulations, can enhance training, effectively leveraging it remains an open problem. To address this, we introduce Guided Policy Optimization (GPO), a framework that co-trains a guider and a learner. The guider takes advantage of privileged information while ensuring alignment with the learner's policy that is primarily trained via imitation learning. We theoretically demonstrate that this learning scheme achieves optimality comparable to direct RL, thereby overcoming key limitations inherent in existing approaches. Empirical evaluations show strong performance of GPO across various tasks, including continuous control with partial observability and noise, and memory-based challenges, significantly outperforming existing methods."
  },
  {
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2505.15400v1",
    "arxiv_id": "2505.15400v1",
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Haodong Zhao",
      "Hao Li",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "published": "2025-05-21T11:41:39+00:00",
    "summary": "Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery Mechanism\" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs."
  },
  {
    "title": "Human in the Loop Adaptive Optimization for Improved Time Series Forecasting",
    "url": "http://arxiv.org/abs/2505.15354v1",
    "arxiv_id": "2505.15354v1",
    "authors": [
      "Malik Tiomoko",
      "Hamza Cherkaoui",
      "Giuseppe Paolo",
      "Zhang Yili",
      "Yu Meng",
      "Zhang Keli",
      "Hafiz Tiomoko Ali"
    ],
    "published": "2025-05-21T10:30:02+00:00",
    "summary": "Time series forecasting models often produce systematic, predictable errors even in critical domains such as energy, finance, and healthcare. We introduce a novel post training adaptive optimization framework that improves forecast accuracy without retraining or architectural changes. Our method automatically applies expressive transformations optimized via reinforcement learning, contextual bandits, or genetic algorithms to correct model outputs in a lightweight and model agnostic way. Theoretically, we prove that affine corrections always reduce the mean squared error; practically, we extend this idea with dynamic action based optimization. The framework also supports an optional human in the loop component: domain experts can guide corrections using natural language, which is parsed into actions by a language model. Across multiple benchmarks (e.g., electricity, weather, traffic), we observe consistent accuracy gains with minimal computational overhead. Our interactive demo shows the framework's real time usability. By combining automated post hoc refinement with interpretable and extensible mechanisms, our approach offers a powerful new direction for practical forecasting systems."
  },
  {
    "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari",
    "url": "http://arxiv.org/abs/2505.15345v1",
    "arxiv_id": "2505.15345v1",
    "authors": [
      "Jacob E. Kooi",
      "Zhao Yang",
      "Vincent Fran\u00e7ois-Lavet"
    ],
    "published": "2025-05-21T10:19:49+00:00",
    "summary": "Neural network architectures have a large impact in machine learning. In reinforcement learning, network architectures have remained notably simple, as changes often lead to small gains in performance. This work introduces a novel encoder architecture for pixel-based model-free reinforcement learning. The Hadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves state-of-the-art performance by max-pooling Hadamard products between GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the Hadamax encoder achieves state-of-the-art model-free performance in the Atari-57 benchmark. Specifically, without applying any algorithmic hyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility, the full code is available on \\href{https://github.com/Jacobkooi/Hadamax}{GitHub}."
  },
  {
    "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.15311v1",
    "arxiv_id": "2505.15311v1",
    "authors": [
      "Yurun Yuan",
      "Fan Chen",
      "Zeyu Jia",
      "Alexander Rakhlin",
      "Tengyang Xie"
    ],
    "published": "2025-05-21T09:41:53+00:00",
    "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs."
  },
  {
    "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One",
    "url": "http://arxiv.org/abs/2505.15306v1",
    "arxiv_id": "2505.15306v1",
    "authors": [
      "Yiwen Song",
      "Qianyue Hao",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-05-21T09:35:43+00:00",
    "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at https://anonymous.4open.science/r/LLM4RLensemble-F7EE."
  },
  {
    "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.15298v1",
    "arxiv_id": "2505.15298v1",
    "authors": [
      "Kangan Qian",
      "Sicong Jiang",
      "Yang Zhong",
      "Ziang Luo",
      "Zilin Huang",
      "Tianze Zhu",
      "Kun Jiang",
      "Mengmeng Yang",
      "Zheng Fu",
      "Jinyu Miao",
      "Yining Shi",
      "He Zhe Lim",
      "Li Liu",
      "Tianbao Zhou",
      "Hongyi Wang",
      "Huang Yu",
      "Yifei Hu",
      "Guang Li",
      "Guang Chen",
      "Hao Ye",
      "Lijun Sun",
      "Diange Yang"
    ],
    "published": "2025-05-21T09:27:43+00:00",
    "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \\textbf{AgentThink}, a pioneering unified framework that, for the first time, integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's core innovations include: \\textbf{(i) Structured Data Generation}, by establishing an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model's tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate AgentThink significantly boosts overall reasoning scores by \\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models."
  },
  {
    "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
    "url": "http://arxiv.org/abs/2505.15293v1",
    "arxiv_id": "2505.15293v1",
    "authors": [
      "Qianyue Hao",
      "Yiwen Song",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-05-21T09:24:23+00:00",
    "summary": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for reproducibility."
  },
  {
    "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning",
    "url": "http://arxiv.org/abs/2505.15276v1",
    "arxiv_id": "2505.15276v1",
    "authors": [
      "Rongzhi Zhu",
      "Yi Liu",
      "Zequn Sun",
      "Yiwei Wang",
      "Wei Hu"
    ],
    "published": "2025-05-21T08:55:35+00:00",
    "summary": "Large reasoning models (LRMs) have significantly advanced performance on complex tasks, yet their tendency to overthink introduces inefficiencies. This study investigates the internal mechanisms of reinforcement learning (RL)-trained LRMs when prompted to save thinking, revealing three distinct thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking (IT). Through comprehensive analysis of confidence in thinking termination, attention from thinking to generation, and attentional focus on input sections, we uncover key factors influencing the reasoning behaviors. We further find that NT reduces output length at the cost of accuracy, while ET and IT maintain accuracy with reduced response length. Our findings expose fundamental inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for reliable efficiency."
  },
  {
    "title": "Learning-based Autonomous Oversteer Control and Collision Avoidance",
    "url": "http://arxiv.org/abs/2505.15275v1",
    "arxiv_id": "2505.15275v1",
    "authors": [
      "Seokjun Lee",
      "Seung-Hyun Kong"
    ],
    "published": "2025-05-21T08:53:38+00:00",
    "summary": "Oversteer, wherein a vehicle's rear tires lose traction and induce unintentional excessive yaw, poses critical safety challenges. Failing to control oversteer often leads to severe traffic accidents. Although recent autonomous driving efforts have attempted to handle oversteer through stabilizing maneuvers, the majority rely on expert-defined trajectories or assume obstacle-free environments, limiting real-world applicability. This paper introduces a novel end-to-end (E2E) autonomous driving approach that tackles oversteer control and collision avoidance simultaneously. Existing E2E techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and Hybrid Learning (HL), generally require near-optimal demonstrations or extensive experience. Yet even skilled human drivers struggle to provide perfect demonstrations under oversteer, and high transition variance hinders accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic (QC-SAC), a new HL algorithm that effectively learns from suboptimal demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we introduce a benchmark inspired by real-world driver training: a vehicle encounters sudden oversteer on a slippery surface and must avoid randomly placed obstacles ahead. Experimental results show QC-SAC attains near-optimal driving policies, significantly surpassing state-of-the-art IL, RL, and HL baselines. Our method demonstrates the world's first safe autonomous oversteer control with obstacle avoidance."
  },
  {
    "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search",
    "url": "http://arxiv.org/abs/2505.15259v1",
    "arxiv_id": "2505.15259v1",
    "authors": [
      "Hyunseok Lee",
      "Jeonghoon Kim",
      "Beomjun Kim",
      "Jihoon Tack",
      "Chansong Jo",
      "Jaehong Lee",
      "Cheonbok Park",
      "Sookyo In",
      "Jinwoo Shin",
      "Kang Min Yoo"
    ],
    "published": "2025-05-21T08:36:18+00:00",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines)."
  },
  {
    "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners",
    "url": "http://arxiv.org/abs/2505.15257v1",
    "arxiv_id": "2505.15257v1",
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yang Deng",
      "Tongtong Wu",
      "Wenxuan Zhang",
      "Yulin Hu",
      "Xingyu Sui",
      "Yanyan Zhao",
      "Wanxiang Che",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ],
    "published": "2025-05-21T08:35:05+00:00",
    "summary": "Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-source LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively decoupled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training such as supervised fine-tuning or reinforcement learning, our training-free ablation achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization."
  },
  {
    "title": "GCNT: Graph-Based Transformer Policies for Morphology-Agnostic Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15211v1",
    "arxiv_id": "2505.15211v1",
    "authors": [
      "Yingbo Luo",
      "Meibao Yao",
      "Xueming Xiao"
    ],
    "published": "2025-05-21T07:40:00+00:00",
    "summary": "Training a universal controller for robots with different morphologies is a promising research trend, since it can significantly enhance the robustness and resilience of the robotic system. However, diverse morphologies can yield different dimensions of state space and action space, making it difficult to comply with traditional policy networks. Existing methods address this issue by modularizing the robot configuration, while do not adequately extract and utilize the overall morphological information, which has been proven crucial for training a universal controller. To this end, we propose GCNT, a morphology-agnostic policy network based on improved Graph Convolutional Network (GCN) and Transformer. It exploits the fact that GCN and Transformer can handle arbitrary number of modules to achieve compatibility with diverse morphologies. Our key insight is that the GCN is able to efficiently extract morphology information of robots, while Transformer ensures that it is fully utilized by allowing each node of the robot to communicate this information directly. Experimental results show that our method can generate resilient locomotion behaviors for robots with different configurations, including zero-shot generalization to robot morphologies not seen during training. In particular, GCNT achieved the best performance on 8 tasks in the 2 standard benchmarks."
  },
  {
    "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
    "url": "http://arxiv.org/abs/2505.15201v1",
    "arxiv_id": "2505.15201v1",
    "authors": [
      "Christian Walder",
      "Deep Karkhanis"
    ],
    "published": "2025-05-21T07:26:36+00:00",
    "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.   While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.   We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples."
  },
  {
    "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection",
    "url": "http://arxiv.org/abs/2505.15173v1",
    "arxiv_id": "2505.15173v1",
    "authors": [
      "Zhipei Xu",
      "Xuanyu Zhang",
      "Xing Zhou",
      "Jian Zhang"
    ],
    "published": "2025-05-21T06:43:34+00:00",
    "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, particularly in video generation, has led to unprecedented creative capabilities but also increased threats to information integrity, identity security, and public trust. Existing detection methods, while effective in general scenarios, lack robust solutions for human-centric videos, which pose greater risks due to their realism and potential for legal and ethical misuse. Moreover, current detection approaches often suffer from poor generalization, limited scalability, and reliance on labor-intensive supervised fine-tuning. To address these challenges, we propose AvatarShield, the first interpretable MLLM-based framework for detecting human-centric fake videos, enhanced via Group Relative Policy Optimization (GRPO). Through our carefully designed accuracy detection reward and temporal compensation reward, it effectively avoids the use of high-cost text annotation data, enabling precise temporal modeling and forgery detection. Meanwhile, we design a dual-encoder architecture, combining high-level semantic reasoning and low-level artifact amplification to guide MLLMs in effective forgery detection. We further collect FakeHumanVid, a large-scale human-centric video benchmark that includes synthesis methods guided by pose, audio, and text inputs, enabling rigorous evaluation of detection methods in real-world scenes. Extensive experiments show that AvatarShield significantly outperforms existing approaches in both in-domain and cross-domain detection, setting a new standard for human-centric video forensics."
  },
  {
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "url": "http://arxiv.org/abs/2505.15146v1",
    "arxiv_id": "2505.15146v1",
    "authors": [
      "Lanxiang Hu",
      "Mingjia Huo",
      "Yuxuan Zhang",
      "Haoyang Yu",
      "Eric P. Xing",
      "Ion Stoica",
      "Tajana Rosing",
      "Haojian Jin",
      "Hao Zhang"
    ],
    "published": "2025-05-21T06:02:55+00:00",
    "summary": "Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench."
  },
  {
    "title": "Filtering Learning Histories Enhances In-Context Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.15143v1",
    "arxiv_id": "2505.15143v1",
    "authors": [
      "Weiqin Chen",
      "Xinjie Zhang",
      "Dharmashankar Subramanian",
      "Santiago Paternain"
    ],
    "published": "2025-05-21T06:00:41+00:00",
    "summary": "Transformer models (TMs) have exhibited remarkable in-context reinforcement learning (ICRL) capabilities, allowing them to generalize to and improve in previously unseen environments without re-training or fine-tuning. This is typically accomplished by imitating the complete learning histories of a source RL algorithm over a substantial amount of pretraining environments, which, however, may transfer suboptimal behaviors inherited from the source algorithm/dataset. Therefore, in this work, we address the issue of inheriting suboptimality from the perspective of dataset preprocessing. Motivated by the success of the weighted empirical risk minimization, we propose a simple yet effective approach, learning history filtering (LHF), to enhance ICRL by reweighting and filtering the learning histories based on their improvement and stability characteristics. To the best of our knowledge, LHF is the first approach to avoid source suboptimality by dataset preprocessing, and can be combined with the current state-of-the-art (SOTA) ICRL algorithms. We substantiate the effectiveness of LHF through a series of experiments conducted on the well-known ICRL benchmarks, encompassing both discrete environments and continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD, DPT, DICP) as the backbones. LHF exhibits robust performance across a variety of suboptimal scenarios, as well as under varying hyperparameters and sampling strategies. Notably, the superior performance of LHF becomes more pronounced in the presence of noisy data, indicating the significance of filtering learning histories."
  },
  {
    "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.15134v1",
    "arxiv_id": "2505.15134v1",
    "authors": [
      "Shivam Agarwal",
      "Zimin Zhang",
      "Lifan Yuan",
      "Jiawei Han",
      "Hao Peng"
    ],
    "published": "2025-05-21T05:39:11+00:00",
    "summary": "Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates."
  },
  {
    "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents",
    "url": "http://arxiv.org/abs/2505.15117v1",
    "arxiv_id": "2505.15117v1",
    "authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Priyanka Kargupta",
      "Sercan O. Arik",
      "Jiawei Han"
    ],
    "published": "2025-05-21T05:09:43+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1."
  },
  {
    "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization",
    "url": "http://arxiv.org/abs/2505.15107v1",
    "arxiv_id": "2505.15107v1",
    "authors": [
      "Ziliang Wang",
      "Xuhui Zheng",
      "Kang An",
      "Cijun Ouyang",
      "Jialu Cai",
      "Yuhang Wang",
      "Yichao Wu"
    ],
    "published": "2025-05-21T05:01:31+00:00",
    "summary": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our implementation is publicly available at https://github.com/zxh20001117/StepSearch."
  },
  {
    "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization",
    "url": "http://arxiv.org/abs/2505.15093v1",
    "arxiv_id": "2505.15093v1",
    "authors": [
      "Jason Yang",
      "Wenda Chu",
      "Daniel Khalil",
      "Raul Astudillo",
      "Bruce J. Wittmann",
      "Frances H. Arnold",
      "Yisong Yue"
    ],
    "published": "2025-05-21T04:30:48+00:00",
    "summary": "Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent developments in steering protein generative models (e.g diffusion models, language models) offer a promising approach. However, by and large, past studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured by low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages compared to alternatives such as reinforcement learning with protein language models."
  },
  {
    "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories",
    "url": "http://arxiv.org/abs/2505.15076v1",
    "arxiv_id": "2505.15076v1",
    "authors": [
      "Nanxu Gong",
      "Sixun Dong",
      "Haoyue Bai",
      "Xinyuan Wang",
      "Wangyang Ying",
      "Yanjie Fu"
    ],
    "published": "2025-05-21T03:49:24+00:00",
    "summary": "As a widely-used and practical tool, feature engineering transforms raw data into discriminative features to advance AI model performance. However, existing methods usually apply feature selection and generation separately, failing to strive a balance between reducing redundancy and adding meaningful dimensions. To fill this gap, we propose an agentic feature augmentation concept, where the unification of feature generation and selection is modeled as agentic teaming and planning. Specifically, we develop a Multi-Agent System with Long and Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant features, a generator agent to produce informative new dimensions, and a router agent that strategically coordinates their actions. We leverage in-context learning with short-term memory for immediate feedback refinement and long-term memory for globally optimal guidance. Additionally, we employ offline Proximal Policy Optimization (PPO) reinforcement fine-tuning to train the router agent for effective decision-making to navigate a vast discrete feature space. Extensive experiments demonstrate that this unified agentic framework consistently achieves superior task performance by intelligently orchestrating feature selection and generation."
  },
  {
    "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data",
    "url": "http://arxiv.org/abs/2505.15074v1",
    "arxiv_id": "2505.15074v1",
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Shengyi Qian",
      "Zhuokai Zhao",
      "Xiyao Wang",
      "Xiaoyu Liu",
      "Ming Li",
      "Paiheng Xu",
      "Wei Ai",
      "Furong Huang"
    ],
    "published": "2025-05-21T03:43:29+00:00",
    "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks."
  },
  {
    "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.15062v1",
    "arxiv_id": "2505.15062v1",
    "authors": [
      "Jiashu He",
      "Jinxuan Fan",
      "Bowen Jiang",
      "Ignacio Houine",
      "Dan Roth",
      "Alejandro Ribeiro"
    ],
    "published": "2025-05-21T03:30:55+00:00",
    "summary": "When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate \"hormones helping mental disorders\" with \"melatonin being a hormone and insomnia a mental disorder\" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and $\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\\%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking."
  },
  {
    "title": "RLBenchNet: The Right Network for the Right Reinforcement Learning Task",
    "url": "http://arxiv.org/abs/2505.15040v1",
    "arxiv_id": "2505.15040v1",
    "authors": [
      "Ivan Smirnov",
      "Shangding Gu"
    ],
    "published": "2025-05-21T02:49:25+00:00",
    "summary": "Reinforcement learning (RL) has seen significant advancements through the application of various neural network architectures. In this study, we systematically investigate the performance of several neural networks in RL tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP), Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit (GRU). Through comprehensive evaluation across continuous control, discrete decision-making, and memory-based environments, we identify architecture-specific strengths and limitations. Our results reveal that: (1) MLPs excel in fully observable continuous control tasks, providing an optimal balance of performance and efficiency; (2) recurrent architectures like LSTM and GRU offer robust performance in partially observable environments with moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2 successfully solve the most challenging memory-intensive tasks, with Mamba-2 requiring 8x less memory than Transformer-XL. These findings provide insights for researchers and practitioners, enabling more informed architecture selection based on specific task characteristics and computational constraints. Code is available at: https://github.com/SafeRL-Lab/RLBenchNet"
  },
  {
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning",
    "url": "http://arxiv.org/abs/2505.15034v1",
    "arxiv_id": "2505.15034v1",
    "authors": [
      "Kaiwen Zha",
      "Zhengqi Gao",
      "Maohao Shen",
      "Zhang-Wei Hong",
      "Duane S. Boning",
      "Dina Katabi"
    ],
    "published": "2025-05-21T02:43:15+00:00",
    "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango."
  },
  {
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "url": "http://arxiv.org/abs/2505.14684v1",
    "arxiv_id": "2505.14684v1",
    "authors": [
      "Haolei Xu",
      "Yuchen Yan",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Shengpei Jiang",
      "Kaitao Song",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "published": "2025-05-20T17:59:31+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits."
  },
  {
    "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14677v1",
    "arxiv_id": "2505.14677v1",
    "authors": [
      "Jiaer Xia",
      "Yuhang Zang",
      "Peng Gao",
      "Yixuan Li",
      "Kaiyang Zhou"
    ],
    "published": "2025-05-20T17:58:35+00:00",
    "summary": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks."
  },
  {
    "title": "Reward Reasoning Model",
    "url": "http://arxiv.org/abs/2505.14674v1",
    "arxiv_id": "2505.14674v1",
    "authors": [
      "Jiaxin Guo",
      "Zewen Chi",
      "Li Dong",
      "Qingxiu Dong",
      "Xun Wu",
      "Shaohan Huang",
      "Furu Wei"
    ],
    "published": "2025-05-20T17:58:03+00:00",
    "summary": "Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning."
  },
  {
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "url": "http://arxiv.org/abs/2505.14652v1",
    "arxiv_id": "2505.14652v1",
    "authors": [
      "Xueguang Ma",
      "Qian Liu",
      "Dongfu Jiang",
      "Ge Zhang",
      "Zejun Ma",
      "Wenhu Chen"
    ],
    "published": "2025-05-20T17:41:33+00:00",
    "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks."
  },
  {
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "url": "http://arxiv.org/abs/2505.14631v1",
    "arxiv_id": "2505.14631v1",
    "authors": [
      "Lingjie Jiang",
      "Xun Wu",
      "Shaohan Huang",
      "Qingxiu Dong",
      "Zewen Chi",
      "Li Dong",
      "Xingxing Zhang",
      "Tengchao Lv",
      "Lei Cui",
      "Furu Wei"
    ],
    "published": "2025-05-20T17:23:25+00:00",
    "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems."
  },
  {
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.14625v1",
    "arxiv_id": "2505.14625v1",
    "authors": [
      "Zhangchen Xu",
      "Yuetai Li",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "published": "2025-05-20T17:16:44+00:00",
    "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV."
  },
  {
    "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14585v1",
    "arxiv_id": "2505.14585v1",
    "authors": [
      "Wenbin Hu",
      "Haoran Li",
      "Huihao Jing",
      "Qi Hu",
      "Ziqian Zeng",
      "Sirui Han",
      "Heli Xu",
      "Tianshu Chu",
      "Peizhao Hu",
      "Yangqiu Song"
    ],
    "published": "2025-05-20T16:40:09+00:00",
    "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively."
  },
  {
    "title": "Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14581v1",
    "arxiv_id": "2505.14581v1",
    "authors": [
      "Deemah H. Tashman",
      "Soumaya Cherkaoui",
      "Walaa Hamouda"
    ],
    "published": "2025-05-20T16:38:32+00:00",
    "summary": "In this paper, a reinforcement learning technique is employed to maximize the performance of a cognitive radio network (CRN). In the presence of primary users (PUs), it is presumed that two secondary users (SUs) access the licensed band within underlay mode. In addition, the SU transmitter is assumed to be an energy-constrained device that requires harvesting energy in order to transmit signals to their intended destination. Therefore, we propose that there are two main sources of energy; the interference of PUs' transmissions and ambient radio frequency (RF) sources. The SU will select whether to gather energy from PUs or only from ambient sources based on a predetermined threshold. The process of energy harvesting from the PUs' messages is accomplished via the time switching approach. In addition, based on a deep Q-network (DQN) approach, the SU transmitter determines whether to collect energy or transmit messages during each time slot as well as selects the suitable transmission power in order to maximize its average data rate. Our approach outperforms a baseline strategy and converges, as shown by our findings."
  },
  {
    "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization",
    "url": "http://arxiv.org/abs/2505.14566v1",
    "arxiv_id": "2505.14566v1",
    "authors": [
      "Andrei Cozma",
      "Landon Harris",
      "Hairong Qi"
    ],
    "published": "2025-05-20T16:25:41+00:00",
    "summary": "Reinforcement Learning (RL) has made significant strides in various domains, and policy gradient methods like Proximal Policy Optimization (PPO) have gained popularity due to their balance in performance, training stability, and computational efficiency. These methods directly optimize policies through gradient-based updates. However, developing effective control policies for environments with complex and non-linear dynamics remains a challenge. High variance in gradient estimates and non-convex optimization landscapes often lead to unstable learning trajectories. Koopman Operator Theory has emerged as a powerful framework for studying non-linear systems through an infinite-dimensional linear operator that acts on a higher-dimensional space of measurement functions. In contrast with their non-linear counterparts, linear systems are simpler, more predictable, and easier to analyze. In this paper, we present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an approximately linear latent-space representation of the underlying system's dynamics while retaining essential features for effective policy learning. This is achieved through a Koopman-approximation auxiliary network that can be added to the baseline policy optimization algorithms without altering the architecture of the core policy or value function. Extensive experimental results demonstrate consistent improvements over the PPO baseline with 6-60% increased performance while reducing variability by up to 91% when evaluated on various continuous control tasks."
  },
  {
    "title": "Bellman operator convergence enhancements in reinforcement learning algorithms",
    "url": "http://arxiv.org/abs/2505.14564v1",
    "arxiv_id": "2505.14564v1",
    "authors": [
      "David Krame Kadurha",
      "Domini Jocema Leko Moutouo",
      "Yae Ulrich Gaba"
    ],
    "published": "2025-05-20T16:24:42+00:00",
    "summary": "This paper reviews the topological groundwork for the study of reinforcement learning (RL) by focusing on the structure of state, action, and policy spaces. We begin by recalling key mathematical concepts such as complete metric spaces, which form the foundation for expressing RL problems. By leveraging the Banach contraction principle, we illustrate how the Banach fixed-point theorem explains the convergence of RL algorithms and how Bellman operators, expressed as operators on Banach spaces, ensure this convergence. The work serves as a bridge between theoretical mathematics and practical algorithm design, offering new approaches to enhance the efficiency of RL. In particular, we investigate alternative formulations of Bellman operators and demonstrate their impact on improving convergence rates and performance in standard RL environments such as MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper mathematical understanding of RL can lead to more effective algorithms for decision-making problems."
  },
  {
    "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
    "url": "http://arxiv.org/abs/2505.14552v1",
    "arxiv_id": "2505.14552v1",
    "authors": [
      "Jiajun Shi",
      "Jian Yang",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Jiangjie Chen",
      "Junting Zhou",
      "Kaijing Ma",
      "Zhoufutu Wen",
      "Bingli Wang",
      "Yancheng He",
      "Liang Song",
      "Hualei Zhu",
      "Shilong Li",
      "Xingjian Wang",
      "Wei Zhang",
      "Ruibin Yuan",
      "Yifan Yao",
      "Wenjun Yang",
      "Yunli Wang",
      "Siyuan Fang",
      "Siyu Yuan",
      "Qianyu He",
      "Xiangru Tang",
      "Yingshui Tan",
      "Wangchunshu Zhou",
      "Zhaoxiang Zhang",
      "Zhoujun Li",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "published": "2025-05-20T16:06:32+00:00",
    "summary": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments."
  },
  {
    "title": "Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study",
    "url": "http://arxiv.org/abs/2505.14544v1",
    "arxiv_id": "2505.14544v1",
    "authors": [
      "Saahil Mahato"
    ],
    "published": "2025-05-20T15:59:44+00:00",
    "summary": "Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges."
  },
  {
    "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers",
    "url": "http://arxiv.org/abs/2505.14533v1",
    "arxiv_id": "2505.14533v1",
    "authors": [
      "Mohammad Irfan Uddin",
      "Nishad Tasnim",
      "Md Omor Faruk",
      "Zejian Zhou"
    ],
    "published": "2025-05-20T15:52:43+00:00",
    "summary": "Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios."
  },
  {
    "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation",
    "url": "http://arxiv.org/abs/2505.14526v1",
    "arxiv_id": "2505.14526v1",
    "authors": [
      "Matteo El-Hariry",
      "Antoine Richard",
      "Ricard M. Castan",
      "Luis F. W. Batista",
      "Matthieu Geist",
      "Cedric Pradalier",
      "Miguel Olivares-Mendez"
    ],
    "published": "2025-05-20T15:48:23+00:00",
    "summary": "Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present NavBench, a multi-domain benchmark for training and evaluating RL-based navigation policies across diverse robotic platforms and operational environments. Built on IsaacLab, our framework standardizes task definitions, enabling different robots to tackle various navigation challenges without the need for ad-hoc task redesigns or custom evaluation metrics. Our benchmark addresses three key challenges: (1) Unified cross-medium benchmarking, enabling direct evaluation of diverse actuation methods (thrusters, wheels, water-based propulsion) in realistic environments; (2) Scalable and modular design, facilitating seamless robot-task interchangeability and reproducible training pipelines; and (3) Robust sim-to-real validation, demonstrated through successful policy transfer to multiple real-world robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency between simulation and real-world deployment, NavBench simplifies the development of adaptable RL-based navigation strategies. Its modular design allows researchers to easily integrate custom robots and tasks by following the framework's predefined templates, making it accessible for a wide range of applications. Our code is publicly available at NavBench."
  },
  {
    "title": "Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment",
    "url": "http://arxiv.org/abs/2505.14477v1",
    "arxiv_id": "2505.14477v1",
    "authors": [
      "Maria Panagiotou",
      "Lorenzo Brigato",
      "Vivien Streit",
      "Amanda Hayoz",
      "Stephan Proennecke",
      "Stavros Athanasopoulos",
      "Mikkel T. Olsen",
      "Elizabeth J. den Brok",
      "Cecilie H. Svensson",
      "Konstantinos Makrilakis",
      "Maria Xatzipsalti",
      "Andriani Vazeou",
      "Peter R. Mertens",
      "Ulrik Pedersen-Bjergaard",
      "Bastiaan E. de Galan",
      "Stavroula Mougiakakou"
    ],
    "published": "2025-05-20T15:10:06+00:00",
    "summary": "Despite recent advances in insulin preparations and technology, adjusting insulin remains an ongoing challenge for the majority of people with type 1 diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin treatment recommendation approach based on reinforcement learning for individuals with T1D and T2D, performing self-monitoring blood glucose measurements and multiple daily insulin injection therapy. We developed and evaluated the ability of ABBA to achieve better time-in-range (TIR) for individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA). The in-silico test was performed using an FDA-accepted population, including 101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows that ABBA significantly improved TIR and significantly reduced both times below- and above-range, compared to BBA. ABBA's performance continued to improve over two months, whereas BBA exhibited only modest changes. This personalised method for adjusting insulin has the potential to further optimise glycaemic control and support people with T1D and T2D in their daily self-management. Our results warrant ABBA to be trialed for the first time in humans."
  },
  {
    "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank",
    "url": "http://arxiv.org/abs/2505.14460v1",
    "arxiv_id": "2505.14460v1",
    "authors": [
      "Tianhe Wu",
      "Jian Zou",
      "Jie Liang",
      "Lei Zhang",
      "Kede Ma"
    ],
    "published": "2025-05-20T14:56:50+00:00",
    "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation."
  },
  {
    "title": "Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks",
    "url": "http://arxiv.org/abs/2505.14459v1",
    "arxiv_id": "2505.14459v1",
    "authors": [
      "Kamal Singh",
      "Sami Marouani",
      "Ahmad Al Sheikh",
      "Pham Tran Anh Quang",
      "Amaury Habrard"
    ],
    "published": "2025-05-20T14:56:31+00:00",
    "summary": "Reinforcement learning (RL) has been increasingly applied to network control problems, such as load balancing. However, existing RL approaches often suffer from lack of interpretability and difficulty in extracting controller equations. In this paper, we propose the use of Kolmogorov-Arnold Networks (KAN) for interpretable RL in network control. We employ a PPO agent with a 1-layer actor KAN model and an MLP Critic network to learn load balancing policies that maximise throughput utility, minimize loss as well as delay. Our approach allows us to extract controller equations from the learned neural networks, providing insights into the decision-making process. We evaluate our approach using different reward functions demonstrating its effectiveness in improving network performance while providing interpretable policies."
  },
  {
    "title": "Adaptive Estimation of the Transition Density of Controlled Markov Chains",
    "url": "http://arxiv.org/abs/2505.14458v1",
    "arxiv_id": "2505.14458v1",
    "authors": [
      "Imon Banerjee",
      "Vinayak Rao",
      "Harsha Honnappa"
    ],
    "published": "2025-05-20T14:55:42+00:00",
    "summary": "Estimating the transition dynamics of controlled Markov chains is crucial in fields such as time series analysis, reinforcement learning, and system exploration. Traditional non-parametric density estimation methods often assume independent samples and require oracle knowledge of smoothness parameters like the H\\\"older continuity coefficient. These assumptions are unrealistic in controlled Markovian settings, especially when the controls are non-Markovian, since such parameters need to hold uniformly over all control values. To address this gap, we propose an adaptive estimator for the transition densities of controlled Markov chains that does not rely on prior knowledge of smoothness parameters or assumptions about the control sequence distribution. Our method builds upon recent advances in adaptive density estimation by selecting an estimator that minimizes a loss function {and} fitting the observed data well, using a constrained minimax criterion over a dense class of estimators. We validate the performance of our estimator through oracle risk bounds, employing both randomized and deterministic versions of the Hellinger distance as loss functions. This approach provides a robust and flexible framework for estimating transition densities in controlled Markovian systems without imposing strong assumptions."
  },
  {
    "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation",
    "url": "http://arxiv.org/abs/2505.14455v1",
    "arxiv_id": "2505.14455v1",
    "authors": [
      "Chihan Huang",
      "Hao Tang"
    ],
    "published": "2025-05-20T14:52:41+00:00",
    "summary": "Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks."
  },
  {
    "title": "Semantically-driven Deep Reinforcement Learning for Inspection Path Planning",
    "url": "http://arxiv.org/abs/2505.14443v1",
    "arxiv_id": "2505.14443v1",
    "authors": [
      "Grzegorz Malczyk",
      "Mihir Kulkarni",
      "Kostas Alexis"
    ],
    "published": "2025-05-20T14:45:16+00:00",
    "summary": "This paper introduces a novel semantics-aware inspection planning policy derived through deep reinforcement learning. Reflecting the fact that within autonomous informative path planning missions in unknown environments, it is often only a sparse set of objects of interest that need to be inspected, the method contributes an end-to-end policy that simultaneously performs semantic object visual inspection combined with collision-free navigation. Assuming access only to the instantaneous depth map, the associated segmentation image, the ego-centric local occupancy, and the history of past positions in the robot's neighborhood, the method demonstrates robust generalizability and successful crossing of the sim2real gap. Beyond simulations and extensive comparison studies, the approach is verified in experimental evaluations onboard a flying robot deployed in novel environments with previously unseen semantics and overall geometric configurations."
  },
  {
    "title": "PRL: Prompts from Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14412v1",
    "arxiv_id": "2505.14412v1",
    "authors": [
      "Pawe\u0142 Batorski",
      "Adrian Kosmala",
      "Paul Swoboda"
    ],
    "published": "2025-05-20T14:26:19+00:00",
    "summary": "Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at https://github.com/Batorskq/prl ."
  },
  {
    "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14362v1",
    "arxiv_id": "2505.14362v1",
    "authors": [
      "Ziwei Zheng",
      "Michael Yang",
      "Jack Hong",
      "Chenxiao Zhao",
      "Guohai Xu",
      "Le Yang",
      "Chao Shen",
      "Xing Yu"
    ],
    "published": "2025-05-20T13:48:11+00:00",
    "summary": "Large Vision-Language Models (VLMs) have shown strong capabilities in multimodal understanding and reasoning, yet they are primarily constrained by text-based reasoning processes. However, achieving seamless integration of visual and textual reasoning which mirrors human cognitive processes remains a significant challenge. In particular, effectively incorporating advanced visual input processing into reasoning mechanisms is still an open question. Thus, in this paper, we explore the interleaved multimodal reasoning paradigm and introduce DeepEyes, a model with \"thinking with images\" capabilities incentivized through end-to-end reinforcement learning without the need for cold-start SFT. Notably, this ability emerges natively within the model itself, leveraging its inherent grounding ability as a tool instead of depending on separate specialized models. Specifically, we propose a tool-use-oriented data selection mechanism and a reward strategy to encourage successful tool-assisted reasoning trajectories. DeepEyes achieves significant performance gains on fine-grained perception and reasoning benchmarks and also demonstrates improvement in grounding, hallucination, and mathematical reasoning tasks. Interestingly, we observe the distinct evolution of tool-calling behavior from initial exploration to efficient and accurate exploitation, and diverse thinking patterns that closely mirror human visual reasoning processes. Code is available at https://github.com/Visual-Agent/DeepEyes."
  },
  {
    "title": "Empowering LLMs in Task-Oriented Dialogues: A Domain-Independent Multi-Agent Framework and Fine-Tuning Strategy",
    "url": "http://arxiv.org/abs/2505.14299v1",
    "arxiv_id": "2505.14299v1",
    "authors": [
      "Zihao Feng",
      "Xiaoxue Wang",
      "Bowen Wu",
      "Weihong Zhong",
      "Zhen Xu",
      "Hailong Cao",
      "Tiejun Zhao",
      "Ying Li",
      "Baoxun Wang"
    ],
    "published": "2025-05-20T12:47:43+00:00",
    "summary": "Task-oriented dialogue systems based on Large Language Models (LLMs) have gained increasing attention across various industries and achieved significant results. Current approaches condense complex procedural workflows into a single agent to achieve satisfactory performance on large-scale LLMs. However, these approaches face challenges to achieve comparable performance on fine-tuned lightweight LLMs, due to their limited capabilities in handling multiple complex logic. In this work, we design a Domain-Independent Multi-Agent Framework (DIMF), which contains Intent Classification Agent, Slot Filling Agent and Response Agent. This approach simplifies the learning complexity and enhances the generalization ability by separating the tasks into domain-independent components. In this framework, we enhance the capabilities in contextual understanding using the Direct Preference Optimisation (DPO) method, and propose a simple and effective Data Distribution Adaptation (DDA) method to mitigate degradation issues during DPO training. Experiments conducted on the MultiWOZ datasets show that our proposed method achieves a better average performance among all the baselines. Extensive analysis also demonstrates that our proposed framework exhibits excellent generalizability and zero-shot capability."
  },
  {
    "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
    "url": "http://arxiv.org/abs/2505.14279v1",
    "arxiv_id": "2505.14279v1",
    "authors": [
      "Jennifer D'Souza",
      "Hamed Babaei Giglou",
      "Quentin M\u00fcnch"
    ],
    "published": "2025-05-20T12:30:46+00:00",
    "summary": "Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence."
  },
  {
    "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge",
    "url": "http://arxiv.org/abs/2505.14268v1",
    "arxiv_id": "2505.14268v1",
    "authors": [
      "Hui Huang",
      "Yancheng He",
      "Hongli Zhou",
      "Rui Zhang",
      "Wei Liu",
      "Weixun Wang",
      "Wenbo Su",
      "Bo Zheng",
      "Jiaheng Liu"
    ],
    "published": "2025-05-20T12:19:10+00:00",
    "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations."
  },
  {
    "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum",
    "url": "http://arxiv.org/abs/2505.14264v1",
    "arxiv_id": "2505.14264v1",
    "authors": [
      "Jian Xiong",
      "Jingbo Zhou",
      "Jingyong Ye",
      "Dejing Dou"
    ],
    "published": "2025-05-20T12:13:44+00:00",
    "summary": "Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO."
  },
  {
    "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14231v1",
    "arxiv_id": "2505.14231v1",
    "authors": [
      "Sule Bai",
      "Mingxing Li",
      "Yong Liu",
      "Jing Tang",
      "Haoji Zhang",
      "Lei Sun",
      "Xiangxiang Chu",
      "Yansong Tang"
    ],
    "published": "2025-05-20T11:40:43+00:00",
    "summary": "Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/."
  },
  {
    "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.14216v1",
    "arxiv_id": "2505.14216v1",
    "authors": [
      "Minwu Kim",
      "Anubhav Shrestha",
      "Safal Shrestha",
      "Aadim Nepal",
      "Keith Ross"
    ],
    "published": "2025-05-20T11:22:34+00:00",
    "summary": "Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy but fails to improve capability, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR does not improve capability because it focuses on improving the accuracy of the less-difficult questions to the detriment of the accuracy of the most difficult questions, thereby leading to no improvement in capability. Second, we find that RLVR does not merely increase the success probability for the less difficult questions, but in our small model settings produces quality responses that were absent in its output distribution before training. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, we show that while distillation reliably improves accuracy by learning strong reasoning patterns, it only improves capability when new knowledge is introduced. Moreover, when distilling only with reasoning patterns and no new knowledge, the accuracy of the less-difficult questions improves to the detriment of the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in language models."
  },
  {
    "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game",
    "url": "http://arxiv.org/abs/2505.14209v1",
    "arxiv_id": "2505.14209v1",
    "authors": [
      "Li Wang",
      "Xin Yu",
      "Xuxin Lv",
      "Gangzheng Ai",
      "Wenjun Wu"
    ],
    "published": "2025-05-20T11:11:46+00:00",
    "summary": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile technologies, perimeter-defense game between attackers and defenders for the protection of critical regions have become increasingly complex and strategically significant across a wide range of domains. However, existing studies predominantly focus on small-scale, simplified two-dimensional scenarios, often overlooking realistic environmental perturbations, motion dynamics, and inherent heterogeneity--factors that pose substantial challenges to real-world applicability. To bridge this gap, we investigate large-scale heterogeneous perimeter-defense game in a three-dimensional setting, incorporating realistic elements such as motion dynamics and wind fields. We derive the Nash equilibrium strategies for both attackers and defenders, characterize the victory regions, and validate our theoretical findings through extensive simulations. To tackle large-scale heterogeneous control challenges in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC) framework. EMFAC leverages representation learning to enable high-level action aggregation in a mean-field manner, supporting scalable coordination among defenders. Furthermore, we introduce a lightweight agent-level attention mechanism based on reward representation, which selectively filters observations and mean-field information to enhance decision-making efficiency and accelerate convergence in large-scale tasks. Extensive simulations across varying scales demonstrate the effectiveness and adaptability of EMFAC, which outperforms established baselines in both convergence speed and overall performance. To further validate practicality, we test EMFAC in small-scale real-world experiments and conduct detailed analyses, offering deeper insights into the framework's effectiveness in complex scenarios."
  },
  {
    "title": "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method",
    "url": "http://arxiv.org/abs/2505.14197v1",
    "arxiv_id": "2505.14197v1",
    "authors": [
      "Xinshen Zhang",
      "Zhen Ye",
      "Xu Zheng"
    ],
    "published": "2025-05-20T10:55:26+00:00",
    "summary": "Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement)."
  },
  {
    "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study",
    "url": "http://arxiv.org/abs/2505.14185v1",
    "arxiv_id": "2505.14185v1",
    "authors": [
      "Kaustubh Ponkshe",
      "Shaan Shah",
      "Raghav Singhal",
      "Praneeth Vepakomma"
    ],
    "published": "2025-05-20T10:41:49+00:00",
    "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces."
  },
  {
    "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14147v1",
    "arxiv_id": "2505.14147v1",
    "authors": [
      "Xiong Jun Wu",
      "Zhenduo Zhang",
      "ZuJie Wen",
      "Zhiqiang Zhang",
      "Wang Ren",
      "Lei Shi",
      "Cai Chen",
      "Deng Zhao",
      "Dingnan Jin",
      "Qing Cui",
      "Jun Zhou"
    ],
    "published": "2025-05-20T09:54:42+00:00",
    "summary": "Training large reasoning models (LRMs) with reinforcement learning in STEM domains is hindered by the scarcity of high-quality, diverse, and verifiable problem sets. Existing synthesis methods, such as Chain-of-Thought prompting, often generate oversimplified or uncheckable data, limiting model advancement on complex tasks. To address these challenges, we introduce SHARP, a unified approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a strategic set of self-alignment principles -- targeting graduate and Olympiad-level difficulty, rigorous logical consistency, and unambiguous, verifiable answers -- and a structured three-phase framework (Alignment, Instantiation, Inference) that ensures thematic diversity and fine-grained control over problem generation. We implement SHARP by leveraging a state-of-the-art LRM to infer and verify challenging STEM questions, then employ a reinforcement learning loop to refine the model's reasoning through verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate that SHARP-augmented training substantially outperforms existing methods, markedly improving complex reasoning accuracy and pushing LRM performance closer to expert-level proficiency. Our contributions include the SHARP strategy, framework design, end-to-end implementation, and experimental evaluation of its effectiveness in elevating LRM reasoning capabilities."
  },
  {
    "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
    "url": "http://arxiv.org/abs/2505.14146v1",
    "arxiv_id": "2505.14146v1",
    "authors": [
      "Pengcheng Jiang",
      "Xueqiang Xu",
      "Jiacheng Lin",
      "Jinfeng Xiao",
      "Zifeng Wang",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "published": "2025-05-20T09:53:56+00:00",
    "summary": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks."
  },
  {
    "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14140v1",
    "arxiv_id": "2505.14140v1",
    "authors": [
      "Qianyue Hao",
      "Sibo Li",
      "Jian Yuan",
      "Yong Li"
    ],
    "published": "2025-05-20T09:43:33+00:00",
    "summary": "Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility."
  },
  {
    "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14139v1",
    "arxiv_id": "2505.14139v1",
    "authors": [
      "Marvin Alles",
      "Nutan Chen",
      "Patrick van der Smagt",
      "Botond Cseke"
    ],
    "published": "2025-05-20T09:43:05+00:00",
    "summary": "The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps."
  },
  {
    "title": "Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.14069v1",
    "arxiv_id": "2505.14069v1",
    "authors": [
      "Wenlin Zhang",
      "Xiangyang Li",
      "Kuicai Dong",
      "Yichao Wang",
      "Pengyue Jia",
      "Xiaopeng Li",
      "Yingyi Zhang",
      "Derong Xu",
      "Zhaocheng Du",
      "Huifeng Guo",
      "Ruiming Tang",
      "Xiangyu Zhao"
    ],
    "published": "2025-05-20T08:21:00+00:00",
    "summary": "Retrieval-augmented generation (RAG) enhances the text generation capabilities of large language models (LLMs) by integrating external knowledge and up-to-date information. However, traditional RAG systems are limited by static workflows and lack the adaptability required for multistep reasoning and complex task management. To address these limitations, agentic RAG systems (e.g., DeepResearch) have been proposed, enabling dynamic retrieval strategies, iterative context refinement, and adaptive workflows for handling complex search queries beyond the capabilities of conventional RAG. Recent advances, such as Search-R1, have demonstrated promising gains using outcome-based reinforcement learning, where the correctness of the final answer serves as the reward signal. Nevertheless, such outcome-supervised agentic RAG methods face challenges including low exploration efficiency, gradient conflict, and sparse reward signals. To overcome these challenges, we propose to utilize fine-grained, process-level rewards to improve training stability, reduce computational costs, and enhance efficiency. Specifically, we introduce a novel method ReasonRAG that automatically constructs RAG-ProGuide, a high-quality dataset providing process-level rewards for (i) query generation, (ii) evidence extraction, and (iii) answer generation, thereby enhancing model inherent capabilities via process-supervised reinforcement learning. With the process-level policy optimization, the proposed framework empowers LLMs to autonomously invoke search, generate queries, extract relevant evidence, and produce final answers. Compared to existing approaches such as Search-R1 and traditional RAG systems, ReasonRAG, leveraging RAG-ProGuide, achieves superior performance on five benchmark datasets using only 5k training instances, significantly fewer than the 90k training instances required by Search-R1."
  },
  {
    "title": "Improved Methods for Model Pruning and Knowledge Distillation",
    "url": "http://arxiv.org/abs/2505.14052v1",
    "arxiv_id": "2505.14052v1",
    "authors": [
      "Wei Jiang",
      "Anying Fu",
      "Youling Zhang"
    ],
    "published": "2025-05-20T07:53:40+00:00",
    "summary": "Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks."
  },
  {
    "title": "Solving Normalized Cut Problem with Constrained Action Space",
    "url": "http://arxiv.org/abs/2505.13986v1",
    "arxiv_id": "2505.13986v1",
    "authors": [
      "Qize Jiang",
      "Linsey Pang",
      "Alice Gatti",
      "Mahima Aggarwa",
      "Giovanna Vantin",
      "Xiaosong Ma",
      "Weiwei Sun",
      "Sanjay Chawla"
    ],
    "published": "2025-05-20T06:33:39+00:00",
    "summary": "Reinforcement Learning (RL) has emerged as an important paradigm to solve combinatorial optimization problems primarily due to its ability to learn heuristics that can generalize across problem instances. However, integrating external knowledge that will steer combinatorial optimization problem solutions towards domain appropriate outcomes remains an extremely challenging task. In this paper, we propose the first RL solution that uses constrained action spaces to guide the normalized cut problem towards pre-defined template instances. Using transportation networks as an example domain, we create a Wedge and Ring Transformer that results in graph partitions that are shaped in form of Wedges and Rings and which are likely to be closer to natural optimal partitions. However, our approach is general as it is based on principles that can be generalized to other domains."
  },
  {
    "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.13973v1",
    "arxiv_id": "2505.13973v1",
    "authors": [
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Xin Li",
      "Peijie Qiu",
      "Xiwen Chen",
      "Abolfazl Razi",
      "Aris Sotiras",
      "Yi Su",
      "Yalin Wang"
    ],
    "published": "2025-05-20T06:12:20+00:00",
    "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality."
  },
  {
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13934v1",
    "arxiv_id": "2505.13934v1",
    "authors": [
      "Jialong Wu",
      "Shaofeng Yin",
      "Ningya Feng",
      "Mingsheng Long"
    ],
    "published": "2025-05-20T05:02:53+00:00",
    "summary": "World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly."
  },
  {
    "title": "A new tellurium-loaded liquid scintillator based on p-dioxane",
    "url": "http://arxiv.org/abs/2505.13926v1",
    "arxiv_id": "2505.13926v1",
    "authors": [
      "Ye Liang",
      "Haozhe Sun",
      "Zhe Wang"
    ],
    "published": "2025-05-20T04:42:24+00:00",
    "summary": "Tellurium-loaded liquid scintillators are essential for neutrinoless double-beta decay experiments, but conventional organic scintillator systems face inherent limitations on tellurium loading due to solubility constraints. In this work, we developed and characterized a novel surfactant-free, water-compatible liquid scintillator based on p-dioxane, incorporating telluric acid, water, and naphthalene. Fluorescence measurements confirmed efficient stepwise energy transfer from p-dioxane through naphthalene to PPO, and UV-Vis absorption studies showed no intrinsic absorption features above 400 nm. At a tellurium mass fraction of 0.5%, the scintillator achieved approximately 60% of the light yield of a reference organic scintillator. These results demonstrate the feasibility of surfactant-free, water-compatible tellurium-loaded scintillators, providing a proof of concept for future development of high-loading liquid scintillator systems."
  },
  {
    "title": "Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13925v1",
    "arxiv_id": "2505.13925v1",
    "authors": [
      "Yunpeng Jiang",
      "Jianshu Hu",
      "Paul Weng",
      "Yutong Ban"
    ],
    "published": "2025-05-20T04:40:49+00:00",
    "summary": "Symmetry is pervasive in robotics and has been widely exploited to improve sample efficiency in deep reinforcement learning (DRL). However, existing approaches primarily focus on spatial symmetries, such as reflection, rotation, and translation, while largely neglecting temporal symmetries. To address this gap, we explore time reversal symmetry, a form of temporal symmetry commonly found in robotics tasks such as door opening and closing. We propose Time Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework that combines trajectory reversal augmentation and time reversal guided reward shaping to efficiently solve temporally symmetric tasks. Our method generates reversed transitions from fully reversible transitions, identified by a proposed dynamics-consistent filter, to augment the training data. For partially reversible transitions, we apply reward shaping to guide learning, according to successful trajectories from the reversed task. Extensive experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL is effective in both single-task and multi-task settings, achieving higher sample efficiency and stronger final performance compared to baseline methods."
  },
  {
    "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight",
    "url": "http://arxiv.org/abs/2505.13921v1",
    "arxiv_id": "2505.13921v1",
    "authors": [
      "Wanjing Huang",
      "Weixiang Yan",
      "Zhen Zhang",
      "Ambuj Singh"
    ],
    "published": "2025-05-20T04:34:58+00:00",
    "summary": "Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP ."
  },
  {
    "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models",
    "url": "http://arxiv.org/abs/2505.13878v1",
    "arxiv_id": "2505.13878v1",
    "authors": [
      "Yanggan Gu",
      "Zhaoyi Yan",
      "Yuanyi Wang",
      "Yiming Zhang",
      "Qi Zhou",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "published": "2025-05-20T03:32:37+00:00",
    "summary": "Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for enhancing LLM performance--largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improve its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks."
  },
  {
    "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements",
    "url": "http://arxiv.org/abs/2505.13837v1",
    "arxiv_id": "2505.13837v1",
    "authors": [
      "Gokul Puthumanaillam",
      "Paulo Padrao",
      "Jose Fuentes",
      "Leonardo Bobadilla",
      "Melkior Ornik"
    ],
    "published": "2025-05-20T02:23:15+00:00",
    "summary": "Robots navigating complex environments must manage uncertainty from sensor noise, environmental changes, and incomplete information, with different tasks requiring varying levels of precision in different areas. For example, precise localization may be crucial near obstacles but less critical in open spaces. We present GUIDE (Generalized Uncertainty Integration for Decision-Making and Execution), a framework that integrates these task-specific requirements into navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning acceptable uncertainty levels to different locations, TSUMs enable robots to adapt uncertainty management based on context. When combined with reinforcement learning, GUIDE learns policies that balance task completion and uncertainty management without extensive reward engineering. Real-world tests show significant performance gains over methods lacking task-specific uncertainty awareness."
  },
  {
    "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams",
    "url": "http://arxiv.org/abs/2505.13834v1",
    "arxiv_id": "2505.13834v1",
    "authors": [
      "Zhi Su",
      "Yuman Gao",
      "Emily Lukas",
      "Yunfei Li",
      "Jiaze Cai",
      "Faris Tulbah",
      "Fei Gao",
      "Chao Yu",
      "Zhongyu Li",
      "Yi Wu",
      "Koushil Sreenath"
    ],
    "published": "2025-05-20T02:20:54+00:00",
    "summary": "Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts."
  },
  {
    "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning",
    "url": "http://arxiv.org/abs/2505.13831v1",
    "arxiv_id": "2505.13831v1",
    "authors": [
      "Zongyuan Deng",
      "Yujie Cai",
      "Qing Liu",
      "Shiyao Mu",
      "Bin Lyu",
      "Zhen Yang"
    ],
    "published": "2025-05-20T02:19:10+00:00",
    "summary": "The selection of base station sites is a critical challenge in 5G network planning, which requires efficient optimization of coverage, cost, user satisfaction, and practical constraints. Traditional manual methods, reliant on human expertise, suffer from inefficiencies and are limited to an unsatisfied planning-construction consistency. Existing AI tools, despite improving efficiency in certain aspects, still struggle to meet the dynamic network conditions and multi-objective needs of telecom operators' networks. To address these challenges, we propose TelePlanNet, an AI-driven framework tailored for the selection of base station sites, integrating a three-layer architecture for efficient planning and large-scale automation. By leveraging large language models (LLMs) for real-time user input processing and intent alignment with base station planning, combined with training the planning model using the improved group relative policy optimization (GRPO) reinforcement learning, the proposed TelePlanNet can effectively address multi-objective optimization, evaluates candidate sites, and delivers practical solutions. Experiments results show that the proposed TelePlanNet can improve the consistency to 78%, which is superior to the manual methods, providing telecom operators with an efficient and scalable tool that significantly advances cellular network planning."
  },
  {
    "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion",
    "url": "http://arxiv.org/abs/2505.13787v1",
    "arxiv_id": "2505.13787v1",
    "authors": [
      "Chris Cundy",
      "Adam Gleave"
    ],
    "published": "2025-05-20T00:31:53+00:00",
    "summary": "As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment."
  },
  {
    "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs",
    "url": "http://arxiv.org/abs/2505.13778v1",
    "arxiv_id": "2505.13778v1",
    "authors": [
      "Guoheng Sun",
      "Ziyao Wang",
      "Bowei Tian",
      "Meng Liu",
      "Zheyu Shen",
      "Shwai He",
      "Yexiao He",
      "Wanghao Ye",
      "Yiting Wang",
      "Ang Li"
    ],
    "published": "2025-05-19T23:39:23+00:00",
    "summary": "As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn."
  },
  {
    "title": "Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis",
    "url": "http://arxiv.org/abs/2505.13768v1",
    "arxiv_id": "2505.13768v1",
    "authors": [
      "Ruiquan Huang",
      "Donghao Li",
      "Chengshuai Shi",
      "Cong Shen",
      "Jing Yang"
    ],
    "published": "2025-05-19T22:58:54+00:00",
    "summary": "This paper investigates a hybrid learning framework for reinforcement learning (RL) in which the agent can leverage both an offline dataset and online interactions to learn the optimal policy. We present a unified algorithm and analysis and show that augmenting confidence-based online RL algorithms with the offline dataset outperforms any pure online or offline algorithm alone and achieves state-of-the-art results under two learning metrics, i.e., sub-optimality gap and online learning regret. Specifically, we show that our algorithm achieves a sub-optimality gap $\\tilde{O}(\\sqrt{1/(N_0/\\mathtt{C}(\\pi^*|\\rho)+N_1}) )$, where $\\mathtt{C}(\\pi^*|\\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$ are the numbers of offline and online samples, respectively. For regret minimization, we show that it achieves a constant $\\tilde{O}( \\sqrt{N_1/(N_0/\\mathtt{C}(\\pi^{-}|\\rho)+N_1)} )$ speed-up compared to pure online learning, where $\\mathtt{C}(\\pi^-|\\rho)$ is the concentrability coefficient over all sub-optimal policies. Our results also reveal an interesting separation on the desired coverage properties of the offline dataset for sub-optimality gap minimization and regret minimization. We further validate our theoretical findings in several experiments in special RL models such as linear contextual bandits and Markov decision processes (MDPs)."
  },
  {
    "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
    "url": "http://arxiv.org/abs/2505.13445v1",
    "arxiv_id": "2505.13445v1",
    "authors": [
      "Xiaoyuan Liu",
      "Tian Liang",
      "Zhiwei He",
      "Jiahao Xu",
      "Wenxuan Wang",
      "Pinjia He",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-05-19T17:59:31+00:00",
    "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners."
  },
  {
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2505.13438v1",
    "arxiv_id": "2505.13438v1",
    "authors": [
      "Penghui Qi",
      "Zichen Liu",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "published": "2025-05-19T17:58:44+00:00",
    "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency."
  },
  {
    "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13426v1",
    "arxiv_id": "2505.13426v1",
    "authors": [
      "Liang Chen",
      "Hongcheng Gao",
      "Tianyu Liu",
      "Zhiqi Huang",
      "Flood Sung",
      "Xinyu Zhou",
      "Yuxin Wu",
      "Baobao Chang"
    ],
    "published": "2025-05-19T17:54:39+00:00",
    "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents."
  },
  {
    "title": "A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut",
    "url": "http://arxiv.org/abs/2505.13405v1",
    "arxiv_id": "2505.13405v1",
    "authors": [
      "Gabriel Malikal",
      "Ismail Alkhouri",
      "Alvaro Velasquez",
      "Adam M Alessio",
      "Saiprasad Ravishankar"
    ],
    "published": "2025-05-19T17:41:10+00:00",
    "summary": "The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal solution is NP-hard in the worst case. As a result, heuristic-based algorithms are commonly used, though their design often requires significant domain expertise. More recently, learning-based methods trained on large (un)labeled datasets have been proposed; however, these approaches often struggle with generalizability and scalability. A well-known approximation algorithm for MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic Unconstrained Binary Optimization (QUBO) formulation into a semidefinite program (SDP). The GW algorithm then applies hyperplane rounding by uniformly sampling a random hyperplane to convert the SDP solution into binary node assignments. In this paper, we propose a training-data-free approach based on a non-episodic reinforcement learning formulation, in which an agent learns to select improved rounding hyperplanes that yield better cuts than those produced by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our method consistently achieves better cuts across large-scale graphs with varying densities and degree distributions."
  },
  {
    "title": "Thinkless: LLM Learns When to Think",
    "url": "http://arxiv.org/abs/2505.13379v1",
    "arxiv_id": "2505.13379v1",
    "authors": [
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "published": "2025-05-19T17:24:16+00:00",
    "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless"
  },
  {
    "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13372v1",
    "arxiv_id": "2505.13372v1",
    "authors": [
      "Irene Brugnara",
      "Alessandro Valentini",
      "Andrea Micheli"
    ],
    "published": "2025-05-19T17:19:13+00:00",
    "summary": "Recent work investigated the use of Reinforcement Learning (RL) for the synthesis of heuristic guidance to improve the performance of temporal planners when a domain is fixed and a set of training problems (not plans) is given. The idea is to extract a heuristic from the value function of a particular (possibly infinite-state) MDP constructed over the training problems.   In this paper, we propose an evolution of this learning and planning framework that focuses on exploiting the information provided by symbolic heuristics during both the RL and planning phases. First, we formalize different reward schemata for the synthesis and use symbolic heuristics to mitigate the problems caused by the truncation of episodes needed to deal with the potentially infinite MDP. Second, we propose learning a residual of an existing symbolic heuristic, which is a \"correction\" of the heuristic value, instead of eagerly learning the whole heuristic from scratch. Finally, we use the learned heuristic in combination with a symbolic heuristic using a multiple-queue planning approach to balance systematic search with imperfect learned information. We experimentally compare all the approaches, highlighting their strengths and weaknesses and significantly advancing the state of the art for this planning and learning schema."
  },
  {
    "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization",
    "url": "http://arxiv.org/abs/2505.13346v1",
    "arxiv_id": "2505.13346v1",
    "authors": [
      "Austin Xu",
      "Yilun Zhou",
      "Xuan-Phi Nguyen",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "published": "2025-05-19T16:50:35+00:00",
    "summary": "To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench."
  },
  {
    "title": "Neural-Enhanced Rate Adaptation and Computation Distribution for Emerging mmWave Multi-User 3D Video Streaming Systems",
    "url": "http://arxiv.org/abs/2505.13337v1",
    "arxiv_id": "2505.13337v1",
    "authors": [
      "Babak Badnava",
      "Jacob Chakareski",
      "Morteza Hashemi"
    ],
    "published": "2025-05-19T16:45:30+00:00",
    "summary": "We investigate multitask edge-user communication-computation resource allocation for $360^\\circ$ video streaming in an edge-computing enabled millimeter wave (mmWave) multi-user virtual reality system. To balance the communication-computation trade-offs that arise herein, we formulate a video quality maximization problem that integrates interdependent multitask/multi-user action spaces and rebuffering time/quality variation constraints. We formulate a deep reinforcement learning framework for \\underline{m}ulti-\\underline{t}ask \\underline{r}ate adaptation and \\underline{c}omputation distribution (MTRC) to solve the problem of interest. Our solution does not rely on a priori knowledge about the environment and uses only prior video streaming statistics (e.g., throughput, decoding time, and transmission delay), and content information, to adjust the assigned video bitrates and computation distribution, as it observes the induced streaming performance online. Moreover, to capture the task interdependence in the environment, we leverage neural network cascades to extend our MTRC method to two novel variants denoted as R1C2 and C1R2. We train all three methods with real-world mmWave network traces and $360^\\circ$ video datasets to evaluate their performance in terms of expected quality of experience (QoE), viewport peak signal-to-noise ratio (PSNR), rebuffering time, and quality variation. We outperform state-of-the-art rate adaptation algorithms, with C1R2 showing best results and achieving $5.21-6.06$ dB PSNR gains, $2.18-2.70$x rebuffering time reduction, and $4.14-4.50$ dB quality variation reduction."
  },
  {
    "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13271v1",
    "arxiv_id": "2505.13271v1",
    "authors": [
      "Lei Sheng",
      "Shuai-Shuai Xu"
    ],
    "published": "2025-05-19T15:52:19+00:00",
    "summary": "Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at https://github.com/CycloneBoy/csc_sql."
  },
  {
    "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning",
    "url": "http://arxiv.org/abs/2505.13261v1",
    "arxiv_id": "2505.13261v1",
    "authors": [
      "Mingrui Chen",
      "Haogeng Liu",
      "Hao Liang",
      "Huaibo Huang",
      "Wentao Zhang",
      "Ran He"
    ],
    "published": "2025-05-19T15:43:10+00:00",
    "summary": "In this work, we investigate how explicitly modeling problem's difficulty prior information shapes the effectiveness of reinforcement learning based fine-tuning for multimodal reasoning. Our exploration mainly comprises of following three perspective: First, through offline data curation, we analyze the U-shaped difficulty distribution of two given datasets using the base model by multi-round sampling, and then filter out prompts that are either too simple or extremely difficult to provide meaningful gradients and perform subsequent two-stage training. Second, we implement an online advantage differentiation, computing group-wise empirical accuracy as a difficulty proxy to adaptively reweight advantages estimation, providing stronger learning signals for more challenging problems. Finally, we introduce difficulty hints as explicit prompts for more complex samples in the second training stage, encouraging the model to calibrate its reasoning depth and perform reflective validation checks. Our comprehensive approach demonstrates significant performances across various multi-modal mathematical reasoning benchmarks with only 2K+0.6K two-stage training data."
  },
  {
    "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability",
    "url": "http://arxiv.org/abs/2505.13258v1",
    "arxiv_id": "2505.13258v1",
    "authors": [
      "Jingyi Ren",
      "Yekun Xu",
      "Xiaolong Wang",
      "Weitao Li",
      "Weizhi Ma",
      "Yang Liu"
    ],
    "published": "2025-05-19T15:40:29+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released."
  },
  {
    "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic",
    "url": "http://arxiv.org/abs/2505.13253v1",
    "arxiv_id": "2505.13253v1",
    "authors": [
      "Lennart R\u00f6stel",
      "Dominik Winkelbauer",
      "Johannes Pitz",
      "Leon Sievers",
      "Berthold B\u00e4uml"
    ],
    "published": "2025-05-19T15:36:34+00:00",
    "summary": "In-hand manipulation and grasping are fundamental yet often separately addressed tasks in robotics. For deriving in-hand manipulation policies, reinforcement learning has recently shown great success. However, the derived controllers are not yet useful in real-world scenarios because they often require a human operator to place the objects in suitable initial (grasping) states. Finding stable grasps that also promote the desired in-hand manipulation goal is an open problem. In this work, we propose a method for bridging this gap by leveraging the critic network of a reinforcement learning agent trained for in-hand manipulation to score and select initial grasps. Our experiments show that this method significantly increases the success rate of in-hand manipulation without requiring additional training. We also present an implementation of a full grasp manipulation pipeline on a real-world system, enabling autonomous grasping and reorientation even of unwieldy objects."
  },
  {
    "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns",
    "url": "http://arxiv.org/abs/2505.13188v1",
    "arxiv_id": "2505.13188v1",
    "authors": [
      "Juntian Zhu",
      "Miguel de Carvalho",
      "Zhouwang Yang",
      "Fengxiang He"
    ],
    "published": "2025-05-19T14:45:58+00:00",
    "summary": "An AI agent might surprisingly find she has reached an unknown state which she has never been aware of -- an unknown unknown. We mathematically ground this scenario in reinforcement learning: an agent, after taking an action calculated from value functions $Q$ and $V$ defined on the {\\it {aware domain}}, reaches a state out of the domain. To enable the agent to handle this scenario, we propose an {\\it episodic Markov decision {process} with growing awareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion} (NIVE) approach to expand value functions to newly aware areas: when an agent arrives at an unknown unknown, value functions $Q$ and $V$ whereon are initialised by noninformative beliefs -- the averaged values on the aware domain. This design is out of respect for the complete absence of knowledge in the newly discovered state. The upper confidence bound momentum Q-learning is then adapted to the growing awareness for training the EMDP-GA model. We prove that (1) the regret of our approach is asymptotically consistent with the state of the art (SOTA) without exposure to unknown unknowns in an extremely uncertain environment, and (2) our computational complexity and space complexity are comparable with the SOTA -- these collectively suggest that though an unknown unknown is surprising, it will be asymptotically properly discovered with decent speed and an affordable cost."
  },
  {
    "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.13144v1",
    "arxiv_id": "2505.13144v1",
    "authors": [
      "Dongsu Lee",
      "Minhae Kwon"
    ],
    "published": "2025-05-19T14:11:14+00:00",
    "summary": "The goal of offline reinforcement learning (RL) is to extract a high-performance policy from the fixed datasets, minimizing performance degradation due to out-of-distribution (OOD) samples. Offline model-based RL (MBRL) is a promising approach that ameliorates OOD issues by enriching state-action transitions with augmentations synthesized via a learned dynamics model. Unfortunately, seminal offline MBRL methods often struggle in sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA), that generates augmented transitions in a temporally structured latent space rather than in raw state space. To model long-horizon behavior, TempDATA learns a latent abstraction that captures a temporal distance from both trajectory and transition levels of state space. Our experiments confirm that TempDATA outperforms previous offline MBRL methods and achieves matching or surpassing the performance of diffusion-based trajectory augmentation and goal-conditioned RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen."
  },
  {
    "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction",
    "url": "http://arxiv.org/abs/2505.13091v1",
    "arxiv_id": "2505.13091v1",
    "authors": [
      "Yuanbo Wang",
      "Zhaoxuan Zhang",
      "Jiajin Qiu",
      "Dilong Sun",
      "Zhengyu Meng",
      "Xiaopeng Wei",
      "Xin Yang"
    ],
    "published": "2025-05-19T13:24:21+00:00",
    "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D diffusion models focus on reconstructing target shape from images or a set of partial observations. While excelling in global context understanding, they struggle to capture the local details of complex shapes and limited to the occlusion and lighting conditions. To overcome these limitations, we utilize tactile images to capture the local 3D information and propose a Touch2Shape model, which leverages a touch-conditioned diffusion model to explore and reconstruct the target shape from touch. For shape reconstruction, we have developed a touch embedding module to condition the diffusion model in creating a compact representation and a touch shape fusion module to refine the reconstructed shape. For shape exploration, we combine the diffusion model with reinforcement learning to train a policy. This involves using the generated latent vector from the diffusion model to guide the touch exploration policy training through a novel reward design. Experiments validate the reconstruction quality thorough both qualitatively and quantitative analysis, and our touch exploration policy further boosts reconstruction performance."
  },
  {
    "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO",
    "url": "http://arxiv.org/abs/2505.13031v1",
    "arxiv_id": "2505.13031v1",
    "authors": [
      "Yicheng Xiao",
      "Lin Song",
      "Yukang Chen",
      "Yingmin Luo",
      "Yuxin Chen",
      "Yukang Gan",
      "Wei Huang",
      "Xiu Li",
      "Xiaojuan Qi",
      "Ying Shan"
    ],
    "published": "2025-05-19T12:17:04+00:00",
    "summary": "Recent text-to-image systems face limitations in handling multimodal inputs and complex reasoning tasks. We introduce MindOmni, a unified multimodal large language model that addresses these challenges by incorporating reasoning generation through reinforcement learning. MindOmni leverages a three-phase training strategy: i) design of a unified vision language model with a decoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought (CoT) instruction data, and iii) our proposed Reasoning Generation Policy Optimization (RGPO) algorithm, utilizing multimodal feedback to effectively guide policy updates. Experimental results demonstrate that MindOmni outperforms existing models, achieving impressive performance on both understanding and generation benchmarks, meanwhile showcasing advanced fine-grained reasoning generation capabilities, especially with mathematical reasoning instruction. All codes will be made public at \\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}."
  },
  {
    "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs",
    "url": "http://arxiv.org/abs/2505.13026v1",
    "arxiv_id": "2505.13026v1",
    "authors": [
      "Jack Chen",
      "Fazhong Liu",
      "Naruto Liu",
      "Yuhan Luo",
      "Erqu Qin",
      "Harry Zheng",
      "Tian Dong",
      "Haojin Zhu",
      "Yan Meng",
      "Xiao Wang"
    ],
    "published": "2025-05-19T12:10:17+00:00",
    "summary": "Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models' reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods."
  },
  {
    "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12996v1",
    "arxiv_id": "2505.12996v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-05-19T11:34:47+00:00",
    "summary": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance."
  },
  {
    "title": "Multi-parameter Control for the (1+($\u03bb$,$\u03bb$))-GA on OneMax via Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12982v1",
    "arxiv_id": "2505.12982v1",
    "authors": [
      "Tai Nguyen",
      "Phong Le",
      "Carola Doerr",
      "Nguyen Dang"
    ],
    "published": "2025-05-19T11:18:41+00:00",
    "summary": "It is well known that evolutionary algorithms can benefit from dynamic choices of the key parameters that control their behavior, to adjust their search strategy to the different stages of the optimization process. A prominent example where dynamic parameter choices have shown a provable super-constant speed-up is the $(1+(\\lambda,\\lambda))$ Genetic Algorithm optimizing the OneMax function. While optimal parameter control policies result in linear expected running times, this is not possible with static parameter choices. This result has spurred a lot of interest in parameter control policies. However, many works, in particular theoretical running time analyses, focus on controlling one single parameter. Deriving policies for controlling multiple parameters remains very challenging. In this work we reconsider the problem of the $(1+(\\lambda,\\lambda))$ Genetic Algorithm optimizing OneMax. We decouple its four main parameters and investigate how well state-of-the-art deep reinforcement learning techniques can approximate good control policies. We show that although making deep reinforcement learning learn effectively is a challenging task, once it works, it is very powerful and is able to find policies that outperform all previously known control policies on the same benchmark. Based on the results found through reinforcement learning, we derive a simple control policy that consistently outperforms the default theory-recommended setting by $27\\%$ and the irace-tuned policy, the strongest existing control policy on this benchmark, by $13\\%$, for all tested problem sizes up to $40{,}000$."
  },
  {
    "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management",
    "url": "http://arxiv.org/abs/2505.12951v1",
    "arxiv_id": "2505.12951v1",
    "authors": [
      "Xuerui Su",
      "Liya Guo",
      "Yue Wang",
      "Yi Zhu",
      "Zhiming Ma",
      "Zun Wang",
      "Yuting Liu"
    ],
    "published": "2025-05-19T10:44:49+00:00",
    "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\\%, and demonstrates strong generalization across mathematical benchmarks."
  },
  {
    "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs",
    "url": "http://arxiv.org/abs/2505.12929v1",
    "arxiv_id": "2505.12929v1",
    "authors": [
      "Zhihe Yang",
      "Xufang Luo",
      "Zilong Wang",
      "Dongqi Han",
      "Zhiyuan He",
      "Dongsheng Li",
      "Yunjian Xu"
    ],
    "published": "2025-05-19T10:14:08+00:00",
    "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti."
  },
  {
    "title": "Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2505.12902v1",
    "arxiv_id": "2505.12902v1",
    "authors": [
      "Hao Fang",
      "Kai Huang",
      "Hao Ye",
      "Chongtao Guo",
      "Le Liang",
      "Xiao Li",
      "Shi Jin"
    ],
    "published": "2025-05-19T09:37:00+00:00",
    "summary": "The pursuit of rate maximization in wireless communication frequently encounters substantial challenges associated with user fairness. This paper addresses these challenges by exploring a novel power allocation approach for delay optimization, utilizing graph neural networks (GNNs)-based reinforcement learning (RL) in device-to-device (D2D) communication. The proposed approach incorporates not only channel state information but also factors such as packet delay, the number of backlogged packets, and the number of transmitted packets into the components of the state information. We adopt a centralized RL method, where a central controller collects and processes the state information. The central controller functions as an agent trained using the proximal policy optimization (PPO) algorithm. To better utilize topology information in the communication network and enhance the generalization of the proposed method, we embed GNN layers into both the actor and critic networks of the PPO algorithm. This integration allows for efficient parameter updates of GNNs and enables the state information to be parameterized as a low-dimensional embedding, which is leveraged by the agent to optimize power allocation strategies. Simulation results demonstrate that the proposed method effectively reduces average delay while ensuring user fairness, outperforms baseline methods, and exhibits scalability and generalization capability."
  },
  {
    "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling",
    "url": "http://arxiv.org/abs/2505.12890v1",
    "arxiv_id": "2505.12890v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "David Bani-Harouni",
      "Kun Yuan",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "published": "2025-05-19T09:20:29+00:00",
    "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within the operating room. Prior works, limited to single-task efforts like phase recognition or scene graph generation, lack scope and generalizability. In this work, we introduce ORQA, a novel OR question answering benchmark and foundational multimodal model to advance OR intelligence. By unifying all four public OR datasets into a comprehensive benchmark, we enable our approach to concurrently address a diverse range of OR challenges. The proposed multimodal large language model fuses diverse OR signals such as visual, auditory, and structured data, for a holistic modeling of the OR. Finally, we propose a novel, progressive knowledge distillation paradigm, to generate a family of models optimized for different speed and memory requirements. We show the strong performance of ORQA on our proposed benchmark, and its zero-shot generalization, paving the way for scalable, unified OR modeling and significantly advancing multimodal surgical intelligence. We will release our code and data upon acceptance."
  },
  {
    "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective",
    "url": "http://arxiv.org/abs/2505.12886v1",
    "arxiv_id": "2505.12886v1",
    "authors": [
      "Zhongxiang Sun",
      "Qipeng Wang",
      "Haoyu Wang",
      "Xiao Zhang",
      "Jun Xu"
    ],
    "published": "2025-05-19T09:16:40+00:00",
    "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged--Reasoning Hallucination--where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates."
  },
  {
    "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging",
    "url": "http://arxiv.org/abs/2505.12872v1",
    "arxiv_id": "2505.12872v1",
    "authors": [
      "Maytus Piriyajitakonkij",
      "Rujikorn Charakorn",
      "Weicheng Tao",
      "Wei Pan",
      "Mingfei Sun",
      "Cheston Tan",
      "Mengmi Zhang"
    ],
    "published": "2025-05-19T08:57:30+00:00",
    "summary": "Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly."
  },
  {
    "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF",
    "url": "http://arxiv.org/abs/2505.12843v1",
    "arxiv_id": "2505.12843v1",
    "authors": [
      "Kangwen Zhao",
      "Jianfeng Cai",
      "Jinhua Zhu",
      "Ruopei Sun",
      "Dongyun Xue",
      "Wengang Zhou",
      "Li Li",
      "Houqiang Li"
    ],
    "published": "2025-05-19T08:29:28+00:00",
    "summary": "Reinforcement Learning from Human Feedback relies on reward models to align large language models with human preferences. However, RLHF often suffers from reward hacking, wherein policy learning exploits flaws in the trained reward model to maximize reward scores without genuinely aligning with human preferences. A significant example of such reward hacking is length bias, where reward models usually favor longer responses irrespective of actual response quality. Previous works on length bias have notable limitations, these approaches either mitigate bias without characterizing the bias form, or simply assume a linear length-reward relation. To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, we propose FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns. Our approach consists of three stages: First, we train a standard reward model which inherently contains length bias. Next, we deploy a lightweight fitting model to explicitly capture the non-linear relation between length and reward. Finally, we incorporate this learned relation into the reward model to debias. Experimental results demonstrate that FiMi-RM achieves a more balanced length-reward distribution. Furthermore, when applied to alignment algorithms, our debiased reward model improves length-controlled win rate and reduces verbosity without compromising its performance."
  },
  {
    "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.12835v1",
    "arxiv_id": "2505.12835v1",
    "authors": [
      "Hengxing Cai",
      "Jinhan Dong",
      "Jingjun Tan",
      "Jingcheng Deng",
      "Sihang Li",
      "Zhifeng Gao",
      "Haidong Wang",
      "Zicheng Su",
      "Agachai Sumalee",
      "Renxin Zhong"
    ],
    "published": "2025-05-19T08:21:20+00:00",
    "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available."
  },
  {
    "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs",
    "url": "http://arxiv.org/abs/2505.12833v1",
    "arxiv_id": "2505.12833v1",
    "authors": [
      "Zhuo Yang",
      "Lingli Ge",
      "Dong Han",
      "Tianfan Fu",
      "Yuqiang Li"
    ],
    "published": "2025-05-19T08:20:40+00:00",
    "summary": "Many real-world scientific and industrial applications require the optimization of expensive black-box functions. Bayesian Optimization (BO) provides an effective framework for such problems. However, traditional BO methods are prone to get trapped in local optima and often lack interpretable insights. To address this issue, this paper designs Reasoning BO, a novel framework that leverages reasoning models to guide the sampling process in BO while incorporating multi-agent systems and knowledge graphs for online knowledge accumulation. By integrating the reasoning and contextual understanding capabilities of Large Language Models (LLMs), we can provide strong guidance to enhance the BO process. As the optimization progresses, Reasoning BO provides real-time sampling recommendations along with critical insights grounded in plausible scientific theories, aiding in the discovery of superior solutions within the search space. We systematically evaluate our approach across 10 diverse tasks encompassing synthetic mathematical functions and complex real-world applications. The framework demonstrates its capability to progressively refine sampling strategies through real-time insights and hypothesis evolution, effectively identifying higher-performing regions of the search space for focused exploration. This process highlights the powerful reasoning and context-learning abilities of LLMs in optimization scenarios. For example, in the Direct Arylation task, our method increased the yield to 60.7%, whereas traditional BO achieved only a 25.2% yield. Furthermore, our investigation reveals that smaller LLMs, when fine-tuned through reinforcement learning, can attain comparable performance to their larger counterparts. This enhanced reasoning capability paves the way for more efficient automated scientific experimentation while maintaining computational feasibility."
  },
  {
    "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12811v1",
    "arxiv_id": "2505.12811v1",
    "authors": [
      "Wei-Chen Liao",
      "Ti-Rong Wu",
      "I-Chen Wu"
    ],
    "published": "2025-05-19T07:40:42+00:00",
    "summary": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight range dilemma, where agents either receive insufficient or excessive information from their environment. In this paper, we propose a novel method, called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight range during training. Experiment results show several advantages of using DSR. First, we demonstrate using DSR achieves better performance in three common MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse (RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show that DSR consistently improves performance across multiple MARL algorithms, including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different training steps, thereby accelerating the training process. Finally, DSR provides additional interpretability by indicating the optimal sight range used during training. Unlike existing methods that rely on global information or communication mechanisms, our approach operates solely based on the individual sight ranges of agents. This approach offers a practical and efficient solution to the sight range dilemma, making it broadly applicable to real-world complex environments."
  },
  {
    "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs",
    "url": "http://arxiv.org/abs/2505.12788v1",
    "arxiv_id": "2505.12788v1",
    "authors": [
      "Zhongni Hou",
      "Miao Su",
      "Xiaolong Jin",
      "Zixuan Li",
      "Long Bai",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "published": "2025-05-19T07:20:33+00:00",
    "summary": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of (subject, predicate, object, timestamp) to describe temporal facts, have attracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional TKGs by utilizing n-tuples to incorporate auxiliary elements alongside core elements (i.e., subject, predicate, and object) of facts, so as to represent them in a more fine-grained manner. Reasoning over N-TKGs aims to predict potential future facts based on historical ones. However, existing N-TKG reasoning methods often lack explainability due to their black-box nature. Therefore, we introduce a new Reinforcement Learning-based method, named MT-Path, which leverages the temporal information to traverse historical n-tuples and construct a temporal reasoning path. Specifically, in order to integrate the information encapsulated within n-tuples, i.e., the entity-irrelevant information within the predicate, the information about core elements, and the complete information about the entire n-tuples, MT-Path utilizes a mixture policy-driven action selector, which bases on three low-level policies, namely, the predicate-focused policy, the core-element-focused policy and the whole-fact-focused policy. Further, MT-Path utilizes an auxiliary element-aware GCN to capture the rich semantic dependencies among facts, thereby enabling the agent to gain a deep understanding of each n-tuple. Experimental results demonstrate the effectiveness and the explainability of MT-Path."
  },
  {
    "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL",
    "url": "http://arxiv.org/abs/2505.12768v1",
    "arxiv_id": "2505.12768v1",
    "authors": [
      "Yaxun Dai",
      "Wenxuan Xie",
      "Xialie Zhuang",
      "Tianyu Yang",
      "Yiying Yang",
      "Haiqin Yang",
      "Yuhang Zhao",
      "Pingfu Chao",
      "Wenhao Jiang"
    ],
    "published": "2025-05-19T06:46:47+00:00",
    "summary": "In Text-to-SQL, execution feedback is essential for guiding large language models (LLMs) to reason accurately and generate reliable SQL queries. However, existing methods treat execution feedback solely as a post-hoc signal for correction or selection, failing to integrate it into the generation process. This limitation hinders their ability to address reasoning errors as they occur, ultimately reducing query accuracy and robustness. To address this issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement Learning), a framework for Text-to-SQL that enables models to interact with the database during decoding and dynamically adjust their reasoning based on execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm that interleaves intermediate SQL execution into reasoning paths, facilitating context-sensitive revisions. It achieves this through structured prompts with markup tags and a stepwise rollout strategy that integrates execution feedback into each stage of generation. To supervise policy learning, we develop a composite reward function that includes an exploration reward, explicitly encouraging effective database interaction. Additionally, ReEx-SQL adopts a tree-based decoding strategy to support exploratory reasoning, enabling dynamic expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving 85.2% on Spider-Realistic with leading performance. In addition, its tree-structured decoding improves efficiency and performance over linear decoding, reducing inference time by 51.9% on the BIRD development set."
  },
  {
    "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization",
    "url": "http://arxiv.org/abs/2505.12763v1",
    "arxiv_id": "2505.12763v1",
    "authors": [
      "Sunghwan Kim",
      "Dongjin Kang",
      "Taeyoon Kwon",
      "Hyungjoo Chae",
      "Dongha Lee",
      "Jinyoung Yeo"
    ],
    "published": "2025-05-19T06:43:08+00:00",
    "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal."
  },
  {
    "title": "Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization",
    "url": "http://arxiv.org/abs/2505.12759v1",
    "arxiv_id": "2505.12759v1",
    "authors": [
      "Haochen Yuan",
      "Minting Pan",
      "Yunbo Wang",
      "Siyu Gao",
      "Philip S. Yu",
      "Xiaokang Yang"
    ],
    "published": "2025-05-19T06:37:25+00:00",
    "summary": "Reinforcement learning (RL) has shown significant promise for sequential portfolio optimization tasks, such as stock trading, where the objective is to maximize cumulative returns while minimizing risks using historical data. However, traditional RL approaches often produce policies that merely memorize the optimal yet impractical buying and selling behaviors within the fixed dataset. These offline policies are less generalizable as they fail to account for the non-stationary nature of the market. Our approach, MetaTrader, frames portfolio optimization as a new type of partial-offline RL problem and makes two technical contributions. First, MetaTrader employs a bilevel learning framework that explicitly trains the RL agent to improve both in-domain profits on the original dataset and out-of-domain performance across diverse transformations of the raw financial data. Second, our approach incorporates a new temporal difference (TD) method that approximates worst-case TD estimates from a batch of transformed TD targets, addressing the value overestimation issue that is particularly challenging in scenarios with limited offline data. Our empirical results on two public stock datasets show that MetaTrader outperforms existing methods, including both RL-based approaches and traditional stock prediction models."
  },
  {
    "title": "ProDS: Preference-oriented Data Selection for Instruction Tuning",
    "url": "http://arxiv.org/abs/2505.12754v1",
    "arxiv_id": "2505.12754v1",
    "authors": [
      "Wenya Guo",
      "Zhengkun Zhang",
      "Xumeng Liu",
      "Ying Zhang",
      "Ziyu Lu",
      "Haoze Zhu",
      "Xubo Liu",
      "Ruxue Yan"
    ],
    "published": "2025-05-19T06:28:14+00:00",
    "summary": "Instruction data selection aims to identify a high-quality subset from the training set that matches or exceeds the performance of the full dataset on target tasks. Existing methods focus on the instruction-to-response mapping, but neglect the human preference for diverse responses. In this paper, we propose Preference-oriented Data Selection method (ProDS) that scores training samples based on their alignment with preferences observed in the target set. Our key innovation lies in shifting the data selection criteria from merely estimating features for accurate response generation to explicitly aligning training samples with human preferences in target tasks. Specifically, direct preference optimization (DPO) is employed to estimate human preferences across diverse responses. Besides, a bidirectional preference synthesis strategy is designed to score training samples according to both positive preferences and negative preferences. Extensive experimental results demonstrate our superiority to existing task-agnostic and targeted methods."
  },
  {
    "title": "MOON: Multi-Objective Optimization-Driven Object-Goal Navigation Using a Variable-Horizon Set-Orienteering Planner",
    "url": "http://arxiv.org/abs/2505.12752v1",
    "arxiv_id": "2505.12752v1",
    "authors": [
      "Daigo Nakajima",
      "Kanji Tanaka",
      "Daiki Iwata",
      "Kouki Terashima"
    ],
    "published": "2025-05-19T06:20:37+00:00",
    "summary": "Object-goal navigation (ON) enables autonomous robots to locate and reach user-specified objects in previously unknown environments, offering promising applications in domains such as assistive care and disaster response. Existing ON methods -- including training-free approaches, reinforcement learning, and zero-shot planners -- generally depend on active exploration to identify landmark objects (e.g., kitchens or desks), followed by navigation toward semantically related targets (e.g., a specific mug). However, these methods often lack strategic planning and do not adequately address trade-offs among multiple objectives. To overcome these challenges, we propose a novel framework that formulates ON as a multi-objective optimization problem (MOO), balancing frontier-based knowledge exploration with knowledge exploitation over previously observed landmarks; we call this framework MOON (MOO-driven ON). We implement a prototype MOON system that integrates three key components: (1) building on QOM [IROS05], a classical ON system that compactly and discriminatively encodes landmarks based on their semantic relevance to the target; (2) integrating StructNav [RSS23], a recently proposed training-free planner, to enhance the navigation pipeline; and (3) introducing a variable-horizon set orienteering problem formulation to enable global optimization over both exploration and exploitation strategies. This work represents an important first step toward developing globally optimized, next-generation object-goal navigation systems."
  },
  {
    "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation",
    "url": "http://arxiv.org/abs/2505.12744v1",
    "arxiv_id": "2505.12744v1",
    "authors": [
      "Weiliang Tang",
      "Dong Jing",
      "Jia-Hui Pan",
      "Zhiwu Lu",
      "Yun-Hui Liu",
      "Li Erran Li",
      "Mingyu Ding",
      "Chi-Wing Fu"
    ],
    "published": "2025-05-19T06:00:14+00:00",
    "summary": "Recent Large Multimodal Models have demonstrated remarkable reasoning capabilities, especially in solving complex mathematical problems and realizing accurate spatial perception. Our key insight is that these emerging abilities can naturally extend to robotic manipulation by enabling LMMs to directly infer the next goal in language via reasoning, rather than relying on a separate action head. However, this paradigm meets two main challenges: i) How to make LMMs understand the spatial action space, and ii) How to fully exploit the reasoning capacity of LMMs in solving these tasks. To tackle the former challenge, we propose a novel task formulation, which inputs the current states of object parts and the gripper, and reformulates rotation by a new axis representation instead of traditional Euler angles. This representation is more compatible with spatial reasoning and easier to interpret within a unified language space. For the latter challenge, we design a pipeline to utilize cutting-edge LMMs to generate a small but high-quality reasoning dataset of multi-round dialogues that successfully solve manipulation tasks for supervised fine-tuning. Then, we perform reinforcement learning by trial-and-error interactions in simulation to further enhance the model's reasoning abilities for robotic manipulation. Our resulting reasoning model built upon a 7B backbone, named ReasonManip, demonstrates three notable advantages driven by its system-2 level reasoning capabilities: i) exceptional generalizability to out-of-distribution environments, objects, and tasks; ii) inherent sim-to-real transfer ability enabled by the unified language representation shared across domains; iii) transparent interpretability connecting high-level reasoning and low-level control. Extensive experiments demonstrate the effectiveness of the proposed paradigm and its potential to advance LMM-driven robotic manipulation."
  },
  {
    "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12737v1",
    "arxiv_id": "2505.12737v1",
    "authors": [
      "Hongjoon Ahn",
      "Heewoong Choi",
      "Jisu Han",
      "Taesup Moon"
    ],
    "published": "2025-05-19T05:51:11+00:00",
    "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm where goal-reaching policies are trained from abundant unlabeled (reward-free) datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. By identifying the root cause of this challenge, we observe the following insights: First, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Second, when learning the high-level policy in the long-horizon regime, the sign of the advantage signal frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage signal for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, the proposed learning scheme contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy extracted using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments."
  },
  {
    "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding",
    "url": "http://arxiv.org/abs/2505.12723v1",
    "arxiv_id": "2505.12723v1",
    "authors": [
      "Haoyuan Wu",
      "Rui Ming",
      "Jilong Gao",
      "Hangyu Zhao",
      "Xueyi Chen",
      "Yikai Yang",
      "Haisheng Zheng",
      "Zhuolun He",
      "Bei Yu"
    ],
    "published": "2025-05-19T05:25:29+00:00",
    "summary": "Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages."
  },
  {
    "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving",
    "url": "http://arxiv.org/abs/2505.12717v1",
    "arxiv_id": "2505.12717v1",
    "authors": [
      "Haoyuan Wu",
      "Xueyi Chen",
      "Rui Ming",
      "Jilong Gao",
      "Shoubo Hu",
      "Zhuolun He",
      "Bei Yu"
    ],
    "published": "2025-05-19T05:18:58+00:00",
    "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks."
  },
  {
    "title": "Shadow-FT: Tuning Instruct via Base",
    "url": "http://arxiv.org/abs/2505.12716v1",
    "arxiv_id": "2505.12716v1",
    "authors": [
      "Taiqiang Wu",
      "Runming Yang",
      "Jiayi Li",
      "Pengfei Hu",
      "Ngai Wong",
      "Yujiu Yang"
    ],
    "published": "2025-05-19T05:16:21+00:00",
    "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."
  },
  {
    "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.12701v1",
    "arxiv_id": "2505.12701v1",
    "authors": [
      "Shuyang Dong",
      "Shangtong Zhang",
      "Lu Feng"
    ],
    "published": "2025-05-19T04:41:54+00:00",
    "summary": "Reinforcement Learning (RL) has shown great promise in domains like healthcare and robotics but often struggles with adoption due to its lack of interpretability. Counterfactual explanations, which address \"what if\" scenarios, provide a promising avenue for understanding RL decisions but remain underexplored for continuous action spaces. We propose a novel approach for generating counterfactual explanations in continuous action RL by computing alternative action sequences that improve outcomes while minimizing deviations from the original sequence. Our approach leverages a distance metric for continuous actions and accounts for constraints such as adhering to predefined policies in specific states. Evaluations in two RL domains, Diabetes Control and Lunar Lander, demonstrate the effectiveness, efficiency, and generalization of our approach, enabling more interpretable and trustworthy RL applications."
  },
  {
    "title": "Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion",
    "url": "http://arxiv.org/abs/2505.12679v1",
    "arxiv_id": "2505.12679v1",
    "authors": [
      "Zhuoheng Wang",
      "Jinyin Zhou",
      "Qi Wu"
    ],
    "published": "2025-05-19T03:55:43+00:00",
    "summary": "Humanoid soccer dribbling is a highly challenging task that demands dexterous ball manipulation while maintaining dynamic balance. Traditional rule-based methods often struggle to achieve accurate ball control due to their reliance on fixed walking patterns and limited adaptability to real-time ball dynamics. To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories. In the first stage, the robot learns basic locomotion skills; in the second stage, we fine-tune the policy for agile dribbling maneuvers. We further introduce a virtual camera model in simulation and design heuristic rewards to encourage active sensing, promoting a broader visual range for continuous ball perception. The policy is trained in simulation and successfully transferred to a physical humanoid robot. Experimental results demonstrate that our method enables effective ball manipulation, achieving flexible and visually appealing dribbling behaviors across multiple environments. This work highlights the potential of reinforcement learning in developing agile humanoid soccer robots. Additional details, video demonstrations, and code are available at https://zhuoheng0910.github.io/dribble-master/."
  },
  {
    "title": "SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic Motion Constraints in Trajectory Planning",
    "url": "http://arxiv.org/abs/2505.12648v1",
    "arxiv_id": "2505.12648v1",
    "authors": [
      "Tengfei Liu",
      "Haoyang Zhong",
      "Jiazheng Hu",
      "Tan Zhang"
    ],
    "published": "2025-05-19T03:00:44+00:00",
    "summary": "This study presents a dynamic safety margin-based reinforcement learning framework for local motion planning in dynamic and uncertain environments. The proposed planner integrates real-time trajectory optimization with adaptive gap analysis, enabling effective feasibility assessment under partial observability constraints. To address safety-critical computations in unknown scenarios, an enhanced online learning mechanism is introduced, which dynamically corrects spatial trajectories by forming dynamic safety margins while maintaining control invariance. Extensive evaluations, including ablation studies and comparisons with state-of-the-art algorithms, demonstrate superior success rates and computational efficiency. The framework's effectiveness is further validated on both simulated and physical robotic platforms."
  },
  {
    "title": "Dual-Agent Reinforcement Learning for Automated Feature Generation",
    "url": "http://arxiv.org/abs/2505.12628v1",
    "arxiv_id": "2505.12628v1",
    "authors": [
      "Wanfu Gao",
      "Zengyao Man",
      "Hanlin Pan",
      "Kunpeng Liu"
    ],
    "published": "2025-05-19T02:24:16+00:00",
    "summary": "Feature generation involves creating new features from raw data to capture complex relationships among the original features, improving model robustness and machine learning performance. Current methods using reinforcement learning for feature generation have made feature exploration more flexible and efficient. However, several challenges remain: first, during feature expansion, a large number of redundant features are generated. When removing them, current methods only retain the best features each round, neglecting those that perform poorly initially but could improve later. Second, the state representation used by current methods fails to fully capture complex feature relationships. Third, there are significant differences between discrete and continuous features in tabular data, requiring different operations for each type. To address these challenges, we propose a novel dual-agent reinforcement learning method for feature generation. Two agents are designed: the first generates new features, and the second determines whether they should be preserved. A self-attention mechanism enhances state representation, and diverse operations distinguish interactions between discrete and continuous features. The experimental results on multiple datasets demonstrate that the proposed method is effective. The code is available at https://github.com/extess0/DARL."
  },
  {
    "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation",
    "url": "http://arxiv.org/abs/2505.12620v1",
    "arxiv_id": "2505.12620v1",
    "authors": [
      "Haiquan Wen",
      "Yiwei He",
      "Zhenglin Huang",
      "Tianxiao Li",
      "Zihan YU",
      "Xingru Huang",
      "Lu Qi",
      "Baoyuan Wu",
      "Xiangtai Li",
      "Guangliang Cheng"
    ],
    "published": "2025-05-19T02:06:43+00:00",
    "summary": "Advances in AI generative models facilitate super-realistic video synthesis, amplifying misinformation risks via social media and eroding trust in digital content. Several research works have explored new deepfake detection methods on AI-generated images to alleviate these risks. However, with the fast development of video generation models, such as Sora and WanX, there is currently a lack of large-scale, high-quality AI-generated video datasets for forgery detection. In addition, existing detection approaches predominantly treat the task as binary classification, lacking explainability in model decision-making and failing to provide actionable insights or guidance for the public. To address these challenges, we propose \\textbf{GenBuster-200K}, a large-scale AI-generated video dataset featuring 200K high-resolution video clips, diverse latest generative techniques, and real-world scenes. We further introduce \\textbf{BusterX}, a novel AI-generated video detection and explanation framework leveraging multimodal large language model (MLLM) and reinforcement learning for authenticity determination and explainable rationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}} large-scale, high-quality AI-generated video dataset that incorporates the latest generative techniques for real-world scenarios. BusterX is the {\\it \\textbf{first}} framework to integrate MLLM with reinforcement learning for explainable AI-generated video detection. Extensive comparisons with state-of-the-art methods and ablation studies validate the effectiveness and generalizability of BusterX. The code, models, and datasets will be released."
  },
  {
    "title": "HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos",
    "url": "http://arxiv.org/abs/2505.12619v1",
    "arxiv_id": "2505.12619v1",
    "authors": [
      "Jiashun Wang",
      "Yifeng Jiang",
      "Haotian Zhang",
      "Chen Tessler",
      "Davis Rempe",
      "Jessica Hodgins",
      "Xue Bin Peng"
    ],
    "published": "2025-05-19T02:04:54+00:00",
    "summary": "Recent data-driven methods leveraging deep reinforcement learning have been an effective paradigm for developing controllers that enable physically simulated characters to produce natural human-like behaviors. However, these data-driven methods often struggle to adapt to novel environments and compose diverse skills coherently to perform more complex tasks. To address these challenges, we propose a hybrid imitation learning (HIL) framework that combines motion tracking, for precise skill replication, with adversarial imitation learning, to enhance adaptability and skill composition. This hybrid learning framework is implemented through parallel multi-task environments and a unified observation space, featuring an agent-centric scene representation to facilitate effective learning from the hybrid parallel environments. Our framework trains a unified controller on parkour data sourced from Internet videos, enabling a simulated character to traverse through new environments using diverse and life-like parkour skills. Evaluations across challenging parkour environments demonstrate that our method improves motion quality, increases skill diversity, and achieves competitive task completion compared to previous learning-based methods."
  },
  {
    "title": "Improving Multilingual Language Models by Aligning Representations through Steering",
    "url": "http://arxiv.org/abs/2505.12584v1",
    "arxiv_id": "2505.12584v1",
    "authors": [
      "Omar Mahmoud",
      "Buddhika Laknath Semage",
      "Thommen George Karimpanal",
      "Santu Rana"
    ],
    "published": "2025-05-19T00:14:43+00:00",
    "summary": "In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer's activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\\textsc{sft}) and reinforcement learning from human feedback (\\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations."
  },
  {
    "title": "Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing",
    "url": "http://arxiv.org/abs/2505.12537v1",
    "arxiv_id": "2505.12537v1",
    "authors": [
      "Davide Plozza",
      "Patricia Apostol",
      "Paul Joseph",
      "Simon Schl\u00e4pfer",
      "Michele Magno"
    ],
    "published": "2025-05-18T20:29:23+00:00",
    "summary": "Compact quadrupedal robots are proving increasingly suitable for deployment in real-world scenarios. Their smaller size fosters easy integration into human environments. Nevertheless, real-time locomotion on uneven terrains remains challenging, particularly due to the high computational demands of terrain perception. This paper presents a robust reinforcement learning-based exteroceptive locomotion controller for resource-constrained small-scale quadrupeds in challenging terrains, which exploits real-time elevation mapping, supported by a careful depth sensor selection. We concurrently train both a policy and a state estimator, which together provide an odometry source for elevation mapping, optionally fused with visual-inertial odometry (VIO). We demonstrate the importance of positioning an additional time-of-flight sensor for maintaining robustness even without VIO, thus having the potential to free up computational resources. We experimentally demonstrate that the proposed controller can flawlessly traverse steps up to 17.5 cm in height and achieve an 80% success rate on 22.5 cm steps, both with and without VIO. The proposed controller also achieves accurate forward and yaw velocity tracking of up to 1.0 m/s and 1.5 rad/s respectively. We open-source our training code at github.com/ETH-PBL/elmap-rl-controller."
  },
  {
    "title": "InnateCoder: Learning Programmatic Options with Foundation Models",
    "url": "http://arxiv.org/abs/2505.12508v1",
    "arxiv_id": "2505.12508v1",
    "authors": [
      "Rubens O. Moraes",
      "Quazi Asif Sadmine",
      "Hendrik Baier",
      "Levi H. S. Lelis"
    ],
    "published": "2025-05-18T17:57:57+00:00",
    "summary": "Outside of transfer learning settings, reinforcement learning agents start their learning process from a clean slate. As a result, such agents have to go through a slow process to learn even the most obvious skills required to solve a problem. In this paper, we present InnateCoder, a system that leverages human knowledge encoded in foundation models to provide programmatic policies that encode \"innate skills\" in the form of temporally extended actions, or options. In contrast to existing approaches to learning options, InnateCoder learns them from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment. Then, InnateCoder searches for a programmatic policy by combining the programs encoding these options into larger and more complex programs. We hypothesized that InnateCoder's way of learning and using options could improve the sampling efficiency of current methods for learning programmatic policies. Empirical results in MicroRTS and Karel the Robot support our hypothesis, since they show that InnateCoder is more sample efficient than versions of the system that do not use options or learn them from experience."
  },
  {
    "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models",
    "url": "http://arxiv.org/abs/2505.12504v1",
    "arxiv_id": "2505.12504v1",
    "authors": [
      "Zongkai Liu",
      "Fanqing Meng",
      "Lingxiao Du",
      "Zhixiang Zhou",
      "Chao Yu",
      "Wenqi Shao",
      "Qiaosheng Zhang"
    ],
    "published": "2025-05-18T17:44:53+00:00",
    "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA."
  },
  {
    "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics",
    "url": "http://arxiv.org/abs/2505.11494v1",
    "arxiv_id": "2505.11494v1",
    "authors": [
      "Lizhi Yang",
      "Blake Werner",
      "Ryan K. Cosner",
      "David Fridovich-Keil",
      "Preston Culbertson",
      "Aaron D. Ames"
    ],
    "published": "2025-05-16T17:57:03+00:00",
    "summary": "Robot learning has produced remarkably effective ``black-box'' controllers for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic safety, i.e., constraint satisfaction, remains challenging for such policies. Reinforcement learning (RL) embeds constraints heuristically through reward engineering, and adding or modifying constraints requires retraining. Model-based approaches, like control barrier functions (CBFs), enable runtime constraint specification with formal guarantees but require accurate dynamics models. This paper presents SHIELD, a layered safety framework that bridges this gap by: (1) training a generative, stochastic dynamics residual model using real-world data from hardware rollouts of the nominal controller, capturing system behavior and uncertainties; and (2) adding a safety layer on top of the nominal (learned locomotion) controller that leverages this model via a stochastic discrete-time CBF formulation enforcing safety constraints in probability. The result is a minimally-invasive safety layer that can be added to the existing autonomy stack to give probabilistic guarantees of safety that balance risk and performance. In hardware experiments on an Unitree G1 humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied indoor and outdoor environments using a nominal (unknown) RL controller and onboard perception."
  },
  {
    "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.11480v1",
    "arxiv_id": "2505.11480v1",
    "authors": [
      "Anjiang Wei",
      "Tarun Suresh",
      "Huanmi Tan",
      "Yinglun Xu",
      "Gagandeep Singh",
      "Ke Wang",
      "Alex Aiken"
    ],
    "published": "2025-05-16T17:40:45+00:00",
    "summary": "Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
  },
  {
    "title": "Automatic Reward Shaping from Confounded Offline Data",
    "url": "http://arxiv.org/abs/2505.11478v1",
    "arxiv_id": "2505.11478v1",
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "published": "2025-05-16T17:40:01+00:00",
    "summary": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist."
  },
  {
    "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages",
    "url": "http://arxiv.org/abs/2505.11475v1",
    "arxiv_id": "2505.11475v1",
    "authors": [
      "Zhilin Wang",
      "Jiaqi Zeng",
      "Olivier Delalleau",
      "Hoo-Chang Shin",
      "Felipe Soares",
      "Alexander Bukharin",
      "Ellie Evans",
      "Yi Dong",
      "Oleksii Kuchaiev"
    ],
    "published": "2025-05-16T17:31:19+00:00",
    "summary": "Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference"
  },
  {
    "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models",
    "url": "http://arxiv.org/abs/2505.11462v1",
    "arxiv_id": "2505.11462v1",
    "authors": [
      "Rahul Thapa",
      "Qingyang Wu",
      "Kevin Wu",
      "Harrison Zhang",
      "Angela Zhang",
      "Eric Wu",
      "Haotian Ye",
      "Suhana Bedi",
      "Nevin Aresh",
      "Joseph Boen",
      "Shriya Reddy",
      "Ben Athiwaratkun",
      "Shuaiwen Leon Song",
      "James Zou"
    ],
    "published": "2025-05-16T17:16:27+00:00",
    "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios."
  },
  {
    "title": "Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks",
    "url": "http://arxiv.org/abs/2505.11461v1",
    "arxiv_id": "2505.11461v1",
    "authors": [
      "Wesley A Suttle",
      "Vipul K Sharma",
      "Brian M Sadler"
    ],
    "published": "2025-05-16T17:14:37+00:00",
    "summary": "Classic multi-agent reinforcement learning (MARL) methods require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for future extensions to additional problems in wireless communications and radar networks."
  },
  {
    "title": "Visual Planning: Let's Think Only with Images",
    "url": "http://arxiv.org/abs/2505.11409v1",
    "arxiv_id": "2505.11409v1",
    "authors": [
      "Yi Xu",
      "Chengzu Li",
      "Han Zhou",
      "Xingchen Wan",
      "Caiqi Zhang",
      "Anna Korhonen",
      "Ivan Vuli\u0107"
    ],
    "published": "2025-05-16T16:17:22+00:00",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference."
  },
  {
    "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner",
    "url": "http://arxiv.org/abs/2505.11404v1",
    "arxiv_id": "2505.11404v1",
    "authors": [
      "Wenchuan Zhang",
      "Penghao Zhang",
      "Jingru Guo",
      "Tao Cheng",
      "Jie Chen",
      "Shuwan Zhang",
      "Zhang Zhang",
      "Yuhao Yi",
      "Hong Bu"
    ],
    "published": "2025-05-16T16:12:50+00:00",
    "summary": "Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1."
  },
  {
    "title": "Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space",
    "url": "http://arxiv.org/abs/2505.11366v1",
    "arxiv_id": "2505.11366v1",
    "authors": [
      "Ali Rabiee",
      "Sima Ghafoori",
      "MH Farhadi",
      "Robert Beyer",
      "Xiangyu Bai",
      "David J Lin",
      "Sarah Ostadabbas",
      "Reza Abiri"
    ],
    "published": "2025-05-16T15:31:40+00:00",
    "summary": "Current invasive assistive technologies are designed to infer high-dimensional motor control signals from severely paralyzed patients. However, they face significant challenges, including public acceptance, limited longevity, and barriers to commercialization. Meanwhile, noninvasive alternatives often rely on artifact-prone signals, require lengthy user training, and struggle to deliver robust high-dimensional control for dexterous tasks. To address these issues, this study introduces a novel human-centered multimodal AI approach as intelligent compensatory mechanisms for lost motor functions that could potentially enable patients with severe paralysis to control high-dimensional assistive devices, such as dexterous robotic arms, using limited and noninvasive inputs. In contrast to the current state-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal shared-autonomy framework integrates deep reinforcement learning algorithms to blend limited low-dimensional user input with real-time environmental perception, enabling adaptive, dynamic, and intelligent interpretation of human intent for complex dexterous manipulation tasks, such as pick-and-place. The results from our ARAS (Adaptive Reinforcement learning for Amplification of limited inputs in Shared autonomy) trained with synthetic users over 50,000 computer simulation episodes demonstrated the first successful implementation of the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA shared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS was evaluated on 23 human subjects, demonstrating high accuracy in dynamic intent detection and smooth, stable 3D trajectory control for dexterous pick-and-place tasks. ARAS user study achieved a high task success rate of 92.88%, with short completion times comparable to those of SoTA invasive assistive technologies."
  },
  {
    "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics",
    "url": "http://arxiv.org/abs/2505.11311v1",
    "arxiv_id": "2505.11311v1",
    "authors": [
      "Ardian Selmonaj",
      "Alessandro Antonucci",
      "Adrian Schneider",
      "Michael R\u00fcegsegger",
      "Matthias Sommer"
    ],
    "published": "2025-05-16T14:36:30+00:00",
    "summary": "Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses."
  },
  {
    "title": "Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data",
    "url": "http://arxiv.org/abs/2505.11308v1",
    "arxiv_id": "2505.11308v1",
    "authors": [
      "Lothar Heimbach",
      "Sebastian Kaltenbach",
      "Petr Karnakov",
      "Francis J. Alexander",
      "Petros Koumoutsakos"
    ],
    "published": "2025-05-16T14:34:42+00:00",
    "summary": "Partial Differential Equations (PDEs) describe phenomena ranging from turbulence and epidemics to quantum mechanics and financial markets. Despite recent advances in computational science, solving such PDEs for real-world applications remains prohibitively expensive because of the necessity of resolving a broad range of spatiotemporal scales. In turn, practitioners often rely on coarse-grained approximations of the original PDEs, trading off accuracy for reduced computational resources. To mitigate the loss of detail inherent in such approximations, closure models are employed to represent unresolved spatiotemporal interactions. We present a framework for developing closure models for PDEs using synthetic data acquired through the method of manufactured solutions. These data are used in conjunction with reinforcement learning to provide closures for coarse-grained PDEs. We illustrate the efficacy of our method using the one-dimensional and two-dimensional Burgers' equations and the two-dimensional advection equation. Moreover, we demonstrate that closure models trained for inhomogeneous PDEs can be effectively generalized to homogeneous PDEs. The results demonstrate the potential for developing accurate and computationally efficient closure models for systems with scarce data."
  },
  {
    "title": "Meta-World+: An Improved, Standardized, RL Benchmark",
    "url": "http://arxiv.org/abs/2505.11289v1",
    "arxiv_id": "2505.11289v1",
    "authors": [
      "Reginald McLean",
      "Evangelos Chatzaroulas",
      "Luc McCutcheon",
      "Frank R\u00f6der",
      "Tianhe Yu",
      "Zhanpeng He",
      "K. R. Zentner",
      "Ryan Julian",
      "J K Terry",
      "Isaac Woungang",
      "Nariman Farsad",
      "Pablo Samuel Castro"
    ],
    "published": "2025-05-16T14:24:03+00:00",
    "summary": "Meta-World is widely used for evaluating multi-task and meta-reinforcement learning agents, which are challenged to master diverse skills simultaneously. Since its introduction however, there have been numerous undocumented changes which inhibit a fair comparison of algorithms. This work strives to disambiguate these results from the literature, while also leveraging the past versions of Meta-World to provide insights into multi-task and meta-reinforcement learning benchmark design. Through this process we release a new open-source version of Meta-World (https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility of past results, is more technically ergonomic, and gives users more control over the tasks that are included in a task set."
  },
  {
    "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs",
    "url": "http://arxiv.org/abs/2505.11277v1",
    "arxiv_id": "2505.11277v1",
    "authors": [
      "Yaorui Shi",
      "Shihan Li",
      "Chang Wu",
      "Zhiyuan Liu",
      "Junfeng Fang",
      "Hengxing Cai",
      "An Zhang",
      "Xiang Wang"
    ],
    "published": "2025-05-16T14:11:29+00:00",
    "summary": "Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively."
  },
  {
    "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.11274v1",
    "arxiv_id": "2505.11274v1",
    "authors": [
      "Zheng Li",
      "Qingxiu Dong",
      "Jingyuan Ma",
      "Di Zhang",
      "Zhifang Sui"
    ],
    "published": "2025-05-16T14:08:04+00:00",
    "summary": "Recently, large reasoning models demonstrate exceptional performance on various tasks. However, reasoning models inefficiently over-process both trivial and complex queries, leading to resource waste and prolonged user latency. To address this challenge, we propose SelfBudgeter - a self-adaptive controllable reasoning strategy for efficient reasoning. Our approach adopts a dual-phase training paradigm: first, the model learns to pre-estimate the reasoning cost based on the difficulty of the query. Then, we introduce budget-guided GPRO for reinforcement learning, which effectively maintains accuracy while reducing output length. SelfBudgeter allows users to anticipate generation time and make informed decisions about continuing or interrupting the process. Furthermore, our method enables direct manipulation of reasoning length via pre-filling token budget. Experimental results demonstrate that SelfBudgeter can rationally allocate budgets according to problem complexity, achieving up to 74.47% response length compression on the MATH benchmark while maintaining nearly undiminished accuracy."
  },
  {
    "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
    "url": "http://arxiv.org/abs/2505.11227v1",
    "arxiv_id": "2505.11227v1",
    "authors": [
      "Zhangying Feng",
      "Qianglong Chen",
      "Ning Lu",
      "Yongqian Li",
      "Siqi Cheng",
      "Shuangmu Peng",
      "Duyu Tang",
      "Shengcai Liu",
      "Zhirui Zhang"
    ],
    "published": "2025-05-16T13:23:26+00:00",
    "summary": "The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models."
  },
  {
    "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation",
    "url": "http://arxiv.org/abs/2505.11221v1",
    "arxiv_id": "2505.11221v1",
    "authors": [
      "Donghoon Lee",
      "Tung M. Luu",
      "Younghwan Lee",
      "Chang D. Yoo"
    ],
    "published": "2025-05-16T13:15:54+00:00",
    "summary": "Recent research highlights the potential of multimodal foundation models in tackling complex decision-making challenges. However, their large parameters make real-world deployment resource-intensive and often impractical for constrained systems. Reinforcement learning (RL) shows promise for task-specific agents but suffers from high sample complexity, limiting practical applications. To address these challenges, we introduce LVLM to Policy (LVLM2P), a novel framework that distills knowledge from large vision-language models (LVLM) into more efficient RL agents. Our approach leverages the LVLM as a teacher, providing instructional actions based on trajectories collected by the RL agent, which helps reduce less meaningful exploration in the early stages of learning, thereby significantly accelerating the agent's learning progress. Additionally, by leveraging the LVLM to suggest actions directly from visual observations, we eliminate the need for manual textual descriptors of the environment, enhancing applicability across diverse tasks. Experiments show that LVLM2P significantly enhances the sample efficiency of baseline RL algorithms."
  },
  {
    "title": "GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.11208v1",
    "arxiv_id": "2505.11208v1",
    "authors": [
      "Dongjun Kim",
      "Junwoo Park",
      "Chaehyeon Shin",
      "Jaeheon Jung",
      "Kyungho Shin",
      "Seungheon Baek",
      "Sanghyuk Heo",
      "Woongrae Kim",
      "Inchul Jeong",
      "Joohwan Cho",
      "Jongsun Park"
    ],
    "published": "2025-05-16T13:05:45+00:00",
    "summary": "Analog/mixed-signal circuit design encounters significant challenges due to performance degradation from process, voltage, and temperature (PVT) variations. To achieve commercial-grade reliability, iterative manual design revisions and extensive statistical simulations are required. While several studies have aimed to automate variation aware analog design to reduce time-to-market, the substantial mismatches in real-world wafers have not been thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing framework that effectively manages the impact of diverse random mismatches to improve robustness against PVT variations. In the proposed approach, risk-sensitive reinforcement learning is leveraged to account for the reliability bound affected by PVT variations, and ensemble-based critic is introduced to achieve sample-efficient learning. For design verification, we also propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to reduce simulation costs of identifying failed designs. GLOVA supports verification through industrial-level PVT variation evaluation methods, including corner simulation as well as global and local Monte Carlo (MC) simulations. Compared to previous state-of-the-art variation-aware analog sizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample efficiency and 76.0$\\times$ reduction in time."
  },
  {
    "title": "Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning",
    "url": "http://arxiv.org/abs/2505.11164v1",
    "arxiv_id": "2505.11164v1",
    "authors": [
      "Nikita Rudin",
      "Junzhe He",
      "Joshua Aurand",
      "Marco Hutter"
    ],
    "published": "2025-05-16T12:07:37+00:00",
    "summary": "Legged robots are well-suited for navigating terrains inaccessible to wheeled robots, making them ideal for applications in search and rescue or space exploration. However, current control methods often struggle to generalize across diverse, unstructured environments. This paper introduces a novel framework for agile locomotion of legged robots by combining multi-expert distillation with reinforcement learning (RL) fine-tuning to achieve robust generalization. Initially, terrain-specific expert policies are trained to develop specialized locomotion skills. These policies are then distilled into a unified foundation policy via the DAgger algorithm. The distilled policy is subsequently fine-tuned using RL on a broader terrain set, including real-world 3D scans. The framework allows further adaptation to new terrains through repeated fine-tuning. The proposed policy leverages depth images as exteroceptive inputs, enabling robust navigation across diverse, unstructured terrains. Experimental results demonstrate significant performance improvements over existing methods in synthesizing multi-terrain skills into a single controller. Deployment on the ANYmal D robot validates the policy's ability to navigate complex environments with agility and robustness, setting a new benchmark for legged robot locomotion."
  },
  {
    "title": "Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes",
    "url": "http://arxiv.org/abs/2505.11153v1",
    "arxiv_id": "2505.11153v1",
    "authors": [
      "Ashok Arora",
      "Neetesh Kumar"
    ],
    "published": "2025-05-16T11:54:48+00:00",
    "summary": "In real-world reinforcement learning (RL) scenarios, agents often encounter partial observability, where incomplete or noisy information obscures the true state of the environment. Partially Observable Markov Decision Processes (POMDPs) are commonly used to model these environments, but effective performance requires memory mechanisms to utilise past observations. While recurrence networks have traditionally addressed this need, transformer-based models have recently shown improved sample efficiency in RL tasks. However, their application to POMDPs remains underdeveloped, and their real-world deployment is constrained due to the high parameter count. This work introduces a novel bi-recurrent model architecture that improves sample efficiency and reduces model parameter count in POMDP scenarios. The architecture replaces the multiple feed forward layers with a single layer of bi-directional recurrence unit to better capture and utilize sequential dependencies and contextual information. This approach improves the model's ability to handle partial observability and increases sample efficiency, enabling effective learning from comparatively fewer interactions. To evaluate the performance of the proposed model architecture, experiments were conducted on a total of 23 POMDP environments. The proposed model architecture outperforms existing transformer-based, attention-based, and recurrence-based methods by a margin ranging from 87.39% to 482.04% on average across the 23 POMDP environments."
  },
  {
    "title": "Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design",
    "url": "http://arxiv.org/abs/2505.11136v1",
    "arxiv_id": "2505.11136v1",
    "authors": [
      "Janik Bischoff",
      "Alexandru Rinciog",
      "Anne Meyer"
    ],
    "published": "2025-05-16T11:33:29+00:00",
    "summary": "We propose a novel reinforcement learning (RL) design to optimize the charging strategy for autonomous mobile robots in large-scale block stacking warehouses. RL design involves a wide array of choices that can mostly only be evaluated through lengthy experimentation. Our study focuses on how different reward and action space configurations, ranging from flexible setups to more guided, domain-informed design configurations, affect the agent performance. Using heuristic charging strategies as a baseline, we demonstrate the superiority of flexible, RL-based approaches in terms of service times. Furthermore, our findings highlight a trade-off: While more open-ended designs are able to discover well-performing strategies on their own, they may require longer convergence times and are less stable, whereas guided configurations lead to a more stable learning process but display a more limited generalization potential. Our contributions are threefold. First, we extend SLAPStack, an open-source, RL-compatible simulation-framework to accommodate charging strategies. Second, we introduce a novel RL design for tackling the charging strategy problem. Finally, we introduce several novel adaptive baseline heuristics and reproducibly evaluate the design using a Proximal Policy Optimization agent and varying different design configurations, with a focus on reward."
  },
  {
    "title": "Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets",
    "url": "http://arxiv.org/abs/2505.11135v1",
    "arxiv_id": "2505.11135v1",
    "authors": [
      "Patrick St\u00f6ckermann",
      "Henning S\u00fcdfeld",
      "Alessandro Immordino",
      "Thomas Altenm\u00fcller",
      "Marc Wegmann",
      "Martin Gebser",
      "Konstantin Schekotihin",
      "Georg Seidel",
      "Chew Wye Chan",
      "Fei Fei Zhang"
    ],
    "published": "2025-05-16T11:32:29+00:00",
    "summary": "Benchmark datasets are crucial for evaluating approaches to scheduling or dispatching in the semiconductor industry during the development and deployment phases. However, commonly used benchmark datasets like the Minifab or SMT2020 lack the complex details and constraints found in real-world scenarios. To mitigate this shortcoming, we compare open-source simulation models with a real industry dataset to evaluate how optimization methods scale with different levels of complexity. Specifically, we focus on Reinforcement Learning methods, performing optimization based on policy-gradient and Evolution Strategies. Our research provides insights into the effectiveness of these optimization methods and their applicability to realistic semiconductor frontend fab simulations. We show that our proposed Evolution Strategies-based method scales much better than a comparable policy-gradient-based approach. Moreover, we identify the selection and combination of relevant bottleneck tools to control by the agent as crucial for an efficient optimization. For the generalization across different loading scenarios and stochastic tool failure patterns, we achieve advantages when utilizing a diverse training dataset. While the overall approach is computationally expensive, it manages to scale well with the number of CPU cores used for training. For the real industry dataset, we achieve an improvement of up to 4% regarding tardiness and up to 1% regarding throughput. For the less complex open-source models Minifab and SMT2020, we observe double-digit percentage improvement in tardiness and single digit percentage improvement in throughput by use of Evolution Strategies."
  },
  {
    "title": "Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining",
    "url": "http://arxiv.org/abs/2505.11122v1",
    "arxiv_id": "2505.11122v1",
    "authors": [
      "Yu Shi",
      "Yitong Duan",
      "Jian Li"
    ],
    "published": "2025-05-16T11:14:17+00:00",
    "summary": "Alpha factor mining is pivotal in quantitative investment for identifying predictive signals from complex financial data. While traditional formulaic alpha mining relies on human expertise, contemporary automated methods, such as those based on genetic programming or reinforcement learning, often suffer from search inefficiency or yield poorly interpretable alpha factors. This paper introduces a novel framework that integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach leverages the LLM's instruction-following and reasoning capability to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration. A key innovation is the guidance of MCTS exploration by rich, quantitative feedback from financial backtesting of each candidate factor, enabling efficient navigation of the vast search space. Furthermore, a frequent subtree avoidance mechanism is introduced to bolster search efficiency and alpha factor performance. Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy, trading performance, and improved interpretability, while offering a more efficient solution for formulaic alpha mining."
  },
  {
    "title": "Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors",
    "url": "http://arxiv.org/abs/2505.11100v1",
    "arxiv_id": "2505.11100v1",
    "authors": [
      "Lang Feng",
      "Jiahao Lin",
      "Dong Xing",
      "Li Zhang",
      "De Ma",
      "Gang Pan"
    ],
    "published": "2025-05-16T10:31:10+00:00",
    "summary": "Population-population generalization is a challenging problem in multi-agent reinforcement learning (MARL), particularly when agents encounter unseen co-players. However, existing self-play-based methods are constrained by the limitation of inside-space generalization. In this study, we propose Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome this limitation in MARL. BiDist leverages knowledge distillation in two alternating directions: forward distillation, which emulates the historical policies' space and creates an implicit self-play, and reverse distillation, which systematically drives agents towards novel distributions outside the known policy space in a non-self-play manner. In addition, BiDist operates as a concise and efficient solution without the need for the complex and costly storage of past policies. We provide both theoretical analysis and empirical evidence to support BiDist's effectiveness. Our results highlight its remarkable generalization ability across a variety of cooperative, competitive, and social dilemma tasks, and reveal that BiDist significantly diversifies the policy distribution space. We also present comprehensive ablation studies to reinforce BiDist's effectiveness and key success factors. Source codes are available in the supplementary material."
  },
  {
    "title": "ShiQ: Bringing back Bellman to LLMs",
    "url": "http://arxiv.org/abs/2505.11081v1",
    "arxiv_id": "2505.11081v1",
    "authors": [
      "Pierre Clavier",
      "Nathan Grinsztajn",
      "Raphael Avalos",
      "Yannis Flet-Berliac",
      "Irem Ergun",
      "Omar D. Domingues",
      "Eugene Tarassov",
      "Olivier Pietquin",
      "Pierre H. Richemond",
      "Florian Strub",
      "Matthieu Geist"
    ],
    "published": "2025-05-16T10:12:11+00:00",
    "summary": "The fine-tuning of pre-trained large language models (LLMs) using reinforcement learning (RL) is generally formulated as direct policy optimization. This approach was naturally favored as it efficiently improves a pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning methods, has received far less attention in the LLM community while demonstrating major success in various non-LLM RL tasks. In particular, Q-learning effectiveness comes from its sample efficiency and ability to learn offline, which is particularly valuable given the high computational cost of sampling with LLMs. However, naively applying a Q-learning-style update to the model's logits is ineffective due to the specificity of LLMs. Our core contribution is to derive theoretically grounded loss functions from Bellman equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt insights from the RL literature to account for LLM-specific characteristics, ensuring that the logits become reliable Q-value estimates. We then use this loss to build a practical algorithm, ShiQ for Shifted-Q, that supports off-policy, token-wise learning while remaining simple to implement. Finally, we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g., UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn and multi-turn LLM settings"
  },
  {
    "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following",
    "url": "http://arxiv.org/abs/2505.11080v1",
    "arxiv_id": "2505.11080v1",
    "authors": [
      "Yapei Chang",
      "Yekyung Kim",
      "Michael Krumdick",
      "Amir Zadeh",
      "Chuan Li",
      "Chris Tanner",
      "Mohit Iyyer"
    ],
    "published": "2025-05-16T10:11:43+00:00",
    "summary": "Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI."
  },
  {
    "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization",
    "url": "http://arxiv.org/abs/2505.11070v1",
    "arxiv_id": "2505.11070v1",
    "authors": [
      "Renjie Chen",
      "Wenfeng Lin",
      "Yichen Zhang",
      "Jiangchuan Wei",
      "Boyuan Liu",
      "Chao Feng",
      "Jiao Ran",
      "Mingyu Guo"
    ],
    "published": "2025-05-16T10:04:57+00:00",
    "summary": "Aligning text-to-image (T2I) diffusion models with Direct Preference Optimization (DPO) has shown notable improvements in generation quality. However, applying DPO to T2I faces two challenges: the sensitivity of DPO to preference pairs and the labor-intensive process of collecting and annotating high-quality data. In this work, we demonstrate that preference pairs with marginal differences can degrade DPO performance. Since DPO relies exclusively on relative ranking while disregarding the absolute difference of pairs, it may misclassify losing samples as wins, or vice versa. We empirically show that extending the DPO from pairwise to groupwise and incorporating reward standardization for reweighting leads to performance gains without explicit data selection. Furthermore, we propose Group Preference Optimization (GPO), an effective self-improvement method that enhances performance by leveraging the model's own capabilities without requiring external data. Extensive experiments demonstrate that GPO is effective across various diffusion models and tasks. Specifically, combining with widely used computer vision models, such as YOLO and OCR, the GPO improves the accurate counting and text rendering capabilities of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a plug-and-play method, no extra overhead is introduced during inference."
  },
  {
    "title": "Lifelong reinforcement learning for health-aware fast charging of lithium-ion batteries",
    "url": "http://arxiv.org/abs/2505.11061v1",
    "arxiv_id": "2505.11061v1",
    "authors": [
      "Meng Yuan",
      "Changfu Zou"
    ],
    "published": "2025-05-16T09:57:35+00:00",
    "summary": "Fast charging of lithium-ion batteries remains a critical bottleneck for widespread adoption of electric vehicles and stationary energy storage systems, as improperly designed fast charging can accelerate battery degradation and shorten lifespan. In this work, we address this challenge by proposing a health-aware fast charging strategy that explicitly balances charging speed and battery longevity across the entire service life. The key innovation lies in establishing a mapping between anode overpotential and the state of health (SoH) of battery, which is then used to constrain the terminal charging voltage in a twin delayed deep deterministic policy gradient (TD3) framework. By incorporating this SoH-dependent voltage constraint, our designed deep learning method mitigates side reactions and effectively extends battery life. To validate the proposed approach, a high-fidelity single particle model with electrolyte is implemented in the widely adopted PyBaMM simulation platform, capturing degradation phenomena at realistic scales. Comparative life-cycle simulations against conventional CC-CV, its variants, and constant current-constant overpotential methods show that the TD3-based controller reduces overall degradation while maintaining competitively fast charge times. These results demonstrate the practical viability of deep reinforcement learning for advanced battery management systems and pave the way for future explorations of health-aware, performance-optimized charging strategies."
  },
  {
    "title": "Exploration by Random Distribution Distillation",
    "url": "http://arxiv.org/abs/2505.11044v1",
    "arxiv_id": "2505.11044v1",
    "authors": [
      "Zhirui Fang",
      "Kai Yang",
      "Jian Tao",
      "Jiafei Lyu",
      "Lusong Li",
      "Li Shen",
      "Xiu Li"
    ],
    "published": "2025-05-16T09:38:21+00:00",
    "summary": "Exploration remains a critical challenge in online reinforcement learning, as an agent must effectively explore unknown environments to achieve high returns. Currently, the main exploration algorithms are primarily count-based methods and curiosity-based methods, with prediction-error methods being a prominent example. In this paper, we propose a novel method called \\textbf{R}andom \\textbf{D}istribution \\textbf{D}istillation (RDD), which samples the output of a target network from a normal distribution. RDD facilitates a more extensive exploration by explicitly treating the difference between the prediction network and the target network as an intrinsic reward. Furthermore, by introducing randomness into the output of the target network for a given state and modeling it as a sample from a normal distribution, intrinsic rewards are bounded by two key components: a pseudo-count term ensuring proper exploration decay and a discrepancy term accounting for predictor convergence. We demonstrate that RDD effectively unifies both count-based and prediction-error approaches. It retains the advantages of prediction-error methods in high-dimensional spaces, while also implementing an intrinsic reward decay mode akin to the pseudo-count method. In the experimental section, RDD is compared with more advanced methods in a series of environments. Both theoretical analysis and experimental results confirm the effectiveness of our approach in improving online exploration for reinforcement learning tasks."
  },
  {
    "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy",
    "url": "http://arxiv.org/abs/2505.11032v1",
    "arxiv_id": "2505.11032v1",
    "authors": [
      "Yuran Wang",
      "Ruihai Wu",
      "Yue Chen",
      "Jiarui Wang",
      "Jiaqi Liang",
      "Ziyu Zhu",
      "Haoran Geng",
      "Jitendra Malik",
      "Pieter Abbeel",
      "Hao Dong"
    ],
    "published": "2025-05-16T09:26:59+00:00",
    "summary": "Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/."
  },
  {
    "title": "ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks",
    "url": "http://arxiv.org/abs/2505.10992v1",
    "arxiv_id": "2505.10992v1",
    "authors": [
      "Feiran You",
      "Hongyang Du"
    ],
    "published": "2025-05-16T08:42:08+00:00",
    "summary": "Heterogeneous Networks (HetNets) pose critical challenges for intelligent management due to the diverse user requirements and time-varying wireless conditions. These factors introduce significant decision complexity, which limits the adaptability of existing Deep Reinforcement Learning (DRL) methods. In many DRL algorithms, especially those involving value-based or actor-critic structures, the critic component plays a key role in guiding policy learning by estimating value functions. However, conventional critic models often use shallow architectures that map observations directly to scalar estimates, limiting their ability to handle multi-task complexity. In contrast, recent progress in inference-time scaling of Large Language Models (LLMs) has shown that generating intermediate reasoning steps can significantly improve decision quality. Motivated by this, we propose ReaCritic, a large reasoning transformer-based criticmodel scaling scheme that brings reasoning ability into DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs and vertical reasoning through deep transformer stacks. It is compatible with a broad range of value-based and actor-critic DRL algorithms and enhances generalization in dynamic wireless environments. Extensive experiments demonstrate that ReaCritic improves convergence speed and final performance across various HetNet settings and standard OpenAI Gym control tasks."
  },
  {
    "title": "DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production",
    "url": "http://arxiv.org/abs/2505.10988v1",
    "arxiv_id": "2505.10988v1",
    "authors": [
      "Joon-Young Kim",
      "Jecheon Yu",
      "Heekyu Kim",
      "Seunghwa Ryu"
    ],
    "published": "2025-05-16T08:35:31+00:00",
    "summary": "Plastic injection molding remains essential to modern manufacturing. However, optimizing process parameters to balance product quality and profitability under dynamic environmental and economic conditions remains a persistent challenge. This study presents a novel deep reinforcement learning (DRL)-based framework for real-time process optimization in injection molding, integrating product quality and profitability into the control objective. A profit function was developed to reflect real-world manufacturing costs, incorporating resin, mold wear, and electricity prices, including time-of-use variations. Surrogate models were constructed to predict product quality and cycle time, enabling efficient offline training of DRL agents using soft actor-critic (SAC) and proximal policy optimization (PPO) algorithms. Experimental results demonstrate that the proposed DRL framework can dynamically adapt to seasonal and operational variations, consistently maintaining product quality while maximizing profit. Compared to traditional optimization methods such as genetic algorithms, the DRL models achieved comparable economic performance with up to 135x faster inference speeds, making them well-suited for real-time applications. The framework's scalability and adaptability highlight its potential as a foundation for intelligent, data-driven decision-making in modern manufacturing environments."
  },
  {
    "title": "Group-in-Group Policy Optimization for LLM Agent Training",
    "url": "http://arxiv.org/abs/2505.10978v1",
    "arxiv_id": "2505.10978v1",
    "authors": [
      "Lang Feng",
      "Zhenghai Xue",
      "Tingcong Liu",
      "Bo An"
    ],
    "published": "2025-05-16T08:26:59+00:00",
    "summary": "Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of > 12\\% on ALFWorld and > 9\\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost."
  },
  {
    "title": "Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions",
    "url": "http://arxiv.org/abs/2505.10947v1",
    "arxiv_id": "2505.10947v1",
    "authors": [
      "Kehan Long",
      "Jorge Cort\u00e9s",
      "Nikolay Atanasov"
    ],
    "published": "2025-05-16T07:36:40+00:00",
    "summary": "We study the problem of certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL). Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov function but such a certificate is difficult to construct for a learned control policy. The value function associated with an RL policy is a natural Lyapunov function candidate but it is not clear how it should be modified. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods."
  },
  {
    "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations",
    "url": "http://arxiv.org/abs/2505.10911v1",
    "arxiv_id": "2505.10911v1",
    "authors": [
      "Jiahui Zhang",
      "Yusen Luo",
      "Abrar Anwar",
      "Sumedh Anand Sontakke",
      "Joseph J Lim",
      "Jesse Thomason",
      "Erdem Biyik",
      "Jesse Zhang"
    ],
    "published": "2025-05-16T06:31:34+00:00",
    "summary": "We introduce ReWiND, a framework for learning robot manipulation tasks solely from language instructions without per-task demonstrations. Standard reinforcement learning (RL) and imitation learning methods require expert supervision through human-designed reward functions or demonstrations for every new task. In contrast, ReWiND starts from a small demonstration dataset to learn: (1) a data-efficient, language-conditioned reward function that labels the dataset with rewards, and (2) a language-conditioned policy pre-trained with offline RL using these rewards. Given an unseen task variation, ReWiND fine-tunes the pre-trained policy using the learned reward function, requiring minimal online interaction. We show that ReWiND's reward model generalizes effectively to unseen tasks, outperforming baselines by up to 2.4x in reward generalization and policy alignment metrics. Finally, we demonstrate that ReWiND enables sample-efficient adaptation to new tasks, beating baselines by 2x in simulation and improving real-world pretrained bimanual policies by 5x, taking a step towards scalable, real-world robot learning. See website at https://rewind-reward.github.io/."
  },
  {
    "title": "Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models",
    "url": "http://arxiv.org/abs/2505.10892v1",
    "arxiv_id": "2505.10892v1",
    "authors": [
      "Akhil Agnihotri",
      "Rahul Jain",
      "Deepak Ramachandran",
      "Zheng Wen"
    ],
    "published": "2025-05-16T05:58:26+00:00",
    "summary": "Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as helpfulness and harmlessness, and there is no natural way to aggregate them into a single objective. In this paper, we address the multi-objective preference-alignment problem, where a policy must optimize several, potentially conflicting, objectives. We introduce the Multi-Objective Preference Optimization (MOPO) algorithm, which frames alignment as a constrained KL-regularized optimization: the primary objective is maximized while secondary objectives are lower-bounded by tunable safety thresholds. Unlike prior work, MOPO operates directly on pairwise preference data, requires no point-wise reward assumption, and avoids heuristic prompt-context engineering. The method recovers policies on the Pareto front whenever the front is attainable; practically, it reduces to simple closed-form iterative updates suitable for large-scale training. On synthetic benchmarks with diverse canonical preference structures, we show that MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world human-preference datasets, MOPO attains higher rewards and yields policies that Pareto-dominate baselines; ablation studies confirm optimization stability and robustness to hyperparameters."
  },
  {
    "title": "Prior-Guided Diffusion Planning for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10881v1",
    "arxiv_id": "2505.10881v1",
    "authors": [
      "Donghyeon Ki",
      "JunHyeok Oh",
      "Seong-Woong Shim",
      "Byung-Jun Lee"
    ],
    "published": "2025-05-16T05:39:02+00:00",
    "summary": "Diffusion models have recently gained prominence in offline reinforcement learning due to their ability to effectively learn high-performing, generalizable policies from static datasets. Diffusion-based planners facilitate long-horizon decision-making by generating high-quality trajectories through iterative denoising, guided by return-maximizing objectives. However, existing guided sampling strategies such as Classifier Guidance, Classifier-Free Guidance, and Monte Carlo Sample Selection either produce suboptimal multi-modal actions, struggle with distributional drift, or incur prohibitive inference-time costs. To address these challenges, we propose Prior Guidance (PG), a novel guided sampling framework that replaces the standard Gaussian prior of a behavior-cloned diffusion model with a learnable distribution, optimized via a behavior-regularized objective. PG directly generates high-value trajectories without costly reward optimization of the diffusion model itself, and eliminates the need to sample multiple candidates at inference for sample selection. We present an efficient training strategy that applies behavior regularization in latent space, and empirically demonstrate that PG outperforms state-of-the-art diffusion policies and planners across diverse long-horizon offline RL benchmarks."
  },
  {
    "title": "Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM",
    "url": "http://arxiv.org/abs/2505.10861v1",
    "arxiv_id": "2505.10861v1",
    "authors": [
      "Thang Duong",
      "Minglai Yang",
      "Chicheng Zhang"
    ],
    "published": "2025-05-16T05:03:39+00:00",
    "summary": "We investigate the usage of Large Language Model (LLM) in collecting high-quality data to warm-start Reinforcement Learning (RL) algorithms for learning in some classical Markov Decision Process (MDP) environments. In this work, we focus on using LLM to generate an off-policy dataset that sufficiently covers state-actions visited by optimal policies, then later using an RL algorithm to explore the environment and improve the policy suggested by the LLM. Our algorithm, LORO, can both converge to an optimal policy and have a high sample efficiency thanks to the LLM's good starting policy. On multiple OpenAI Gym environments, such as CartPole and Pendulum, we empirically demonstrate that LORO outperforms baseline algorithms such as pure LLM-based policies, pure RL, and a naive combination of the two, achieving up to $4 \\times$ the cumulative rewards of the pure RL baseline."
  },
  {
    "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL",
    "url": "http://arxiv.org/abs/2505.10832v1",
    "arxiv_id": "2505.10832v1",
    "authors": [
      "Songjun Tu",
      "Jiahao Lin",
      "Qichao Zhang",
      "Xiangyu Tian",
      "Linjing Li",
      "Xiangyuan Lan",
      "Dongbin Zhao"
    ],
    "published": "2025-05-16T04:01:57+00:00",
    "summary": "Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis (\"...\") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs."
  },
  {
    "title": "Enhancing Secrecy Energy Efficiency in RIS-Aided Aerial Mobile Edge Computing Networks: A Deep Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2505.10815v1",
    "arxiv_id": "2505.10815v1",
    "authors": [
      "Aly Sabri Abdalla",
      "Vuk Marojevic"
    ],
    "published": "2025-05-16T03:18:46+00:00",
    "summary": "This paper studies the problem of securing task offloading transmissions from ground users against ground eavesdropping threats. Our study introduces a reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle (UAV)-mobile edge computing (MEC) scheme to enhance the secure task offloading while minimizing the energy consumption of the UAV subject to task completion constraints. Leveraging a data-driven approach, we propose a comprehensive optimization strategy that jointly optimizes the aerial MEC (AMEC)'s trajectory, task offloading partitioning, UE transmission scheduling, and RIS phase shifts. Our objective centers on optimizing the secrecy energy efficiency (SEE) of UE task offloading transmissions while preserving the AMEC's energy resources and meeting the task completion time requirements. Numerical results show that the proposed solution can effectively safeguard legitimate task offloading transmissions while preserving AMEC energy."
  },
  {
    "title": "Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management",
    "url": "http://arxiv.org/abs/2505.10803v1",
    "arxiv_id": "2505.10803v1",
    "authors": [
      "Zhaoan Wang",
      "Wonseok Jang",
      "Bowen Ruan",
      "Jun Wang",
      "Shaoping Xiao"
    ],
    "published": "2025-05-16T02:52:16+00:00",
    "summary": "Precision agriculture, enhanced by artificial intelligence (AI), offers promising tools such as remote sensing, intelligent irrigation, fertilization management, and crop simulation to improve agricultural efficiency and sustainability. Reinforcement learning (RL), in particular, has outperformed traditional methods in optimizing yields and resource management. However, widespread AI adoption is limited by gaps between algorithmic recommendations and farmers' practical experience, local knowledge, and traditional practices. To address this, our study emphasizes Human-AI Interaction (HAII), focusing on transparency, usability, and trust in RL-based farm management. We employ a well-established trust framework - comprising ability, benevolence, and integrity - to develop a novel mathematical model quantifying farmers' confidence in AI-based fertilization strategies. Surveys conducted with farmers for this research reveal critical misalignments, which are integrated into our trust model and incorporated into a multi-objective RL framework. Unlike prior methods, our approach embeds trust directly into policy optimization, ensuring AI recommendations are technically robust, economically feasible, context-aware, and socially acceptable. By aligning technical performance with human-centered trust, this research supports broader AI adoption in agriculture."
  },
  {
    "title": "Attention-Based Reward Shaping for Sparse and Delayed Rewards",
    "url": "http://arxiv.org/abs/2505.10802v1",
    "arxiv_id": "2505.10802v1",
    "authors": [
      "Ian Holmes",
      "Min Chi"
    ],
    "published": "2025-05-16T02:43:05+00:00",
    "summary": "Sparse and delayed reward functions pose a significant obstacle for real-world Reinforcement Learning (RL) applications. In this work, we propose Attention-based REward Shaping (ARES), a general and robust algorithm which uses a transformer's attention mechanism to generate shaped rewards and create a dense reward function for any environment. ARES requires a set of episodes and their final returns as input. It can be trained entirely offline and is able to generate meaningful shaped rewards even when using small datasets or episodes produced by agents taking random actions. ARES is compatible with any RL algorithm and can handle any level of reward sparsity. In our experiments, we focus on the most challenging case where rewards are fully delayed until the end of each episode. We evaluate ARES across a diverse range of environments, widely used RL algorithms, and baseline methods to assess the effectiveness of the shaped rewards it produces. Our results show that ARES can significantly improve learning in delayed reward settings, enabling RL agents to train in scenarios that would otherwise require impractical amounts of data or even be unlearnable. To our knowledge, ARES is the first approach that works fully offline, remains robust to extreme reward delays and low-quality data, and is not limited to goal-based tasks."
  },
  {
    "title": "Qualia Optimization",
    "url": "http://arxiv.org/abs/2505.10779v1",
    "arxiv_id": "2505.10779v1",
    "authors": [
      "Philip S. Thomas"
    ],
    "published": "2025-05-16T01:34:03+00:00",
    "summary": "This report explores the speculative question: what if current or future AI systems have qualia, such as pain or pleasure? It does so by assuming that AI systems might someday possess qualia -- and that the quality of these subjective experiences should be considered alongside performance metrics. Concrete mathematical problem settings, inspired by reinforcement learning formulations and theories from philosophy of mind, are then proposed and initial approaches and properties are presented. These properties enable refinement of the problem setting, culminating with the proposal of methods that promote reinforcement."
  },
  {
    "title": "A Systematic Analysis of Base Model Choice for Reward Modeling",
    "url": "http://arxiv.org/abs/2505.10775v1",
    "arxiv_id": "2505.10775v1",
    "authors": [
      "Kian Ahrabian",
      "Pegah Jandaghi",
      "Negar Mokhberian",
      "Sai Praneeth Karimireddy",
      "Jay Pujara"
    ],
    "published": "2025-05-16T01:27:03+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error."
  },
  {
    "title": "Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics",
    "url": "http://arxiv.org/abs/2505.10762v1",
    "arxiv_id": "2505.10762v1",
    "authors": [
      "Conor F. Hayes",
      "Felipe Leno Da Silva",
      "Jiachen Yang",
      "T. Nathan Mundhenk",
      "Chak Shing Lee",
      "Jacob F. Pettit",
      "Claudio Santiago",
      "Sookyung Kim",
      "Joanne T. Kim",
      "Ignacio Aravena Solis",
      "Ruben Glatt",
      "Andre R. Goncalves",
      "Alexander Ladd",
      "Ahmet Can Solak",
      "Thomas Desautels",
      "Daniel Faissol",
      "Brenden K. Petersen",
      "Mikel Landajuela"
    ],
    "published": "2025-05-16T00:31:19+00:00",
    "summary": "Deep Symbolic Optimization (DSO) is a novel computational framework that enables symbolic optimization for scientific discovery, particularly in applications involving the search for intricate symbolic structures. One notable example is equation discovery, which aims to automatically derive mathematical models expressed in symbolic form. In DSO, the discovery process is formulated as a sequential decision-making task. A generative neural network learns a probabilistic model over a vast space of candidate symbolic expressions, while reinforcement learning strategies guide the search toward the most promising regions. This approach integrates gradient-based optimization with evolutionary and local search techniques, and it incorporates in-situ constraints, domain-specific priors, and advanced policy optimization methods. The result is a robust framework capable of efficiently exploring extensive search spaces to identify interpretable and physically meaningful models. Extensive evaluations on benchmark problems have demonstrated that DSO achieves state-of-the-art performance in both accuracy and interpretability. In this chapter, we provide a comprehensive overview of the DSO framework and illustrate its transformative potential for automating symbolic optimization in scientific discovery."
  },
  {
    "title": "Infinigen-Sim: Procedural Generation of Articulated Simulation Assets",
    "url": "http://arxiv.org/abs/2505.10755v1",
    "arxiv_id": "2505.10755v1",
    "authors": [
      "Abhishek Joshi",
      "Beining Han",
      "Jack Nugent",
      "Yiming Zuo",
      "Jonathan Liu",
      "Hongyu Wen",
      "Stamatis Alexandropoulos",
      "Tao Sun",
      "Alexander Raistrick",
      "Gaowen Liu",
      "Yi Shao",
      "Jia Deng"
    ],
    "published": "2025-05-15T23:47:58+00:00",
    "summary": "We introduce Infinigen-Sim, a toolkit which enables users to create diverse and realistic articulated object procedural generators. These tools are composed of high-level utilities for use creating articulated assets in Blender, as well as an export pipeline to integrate the resulting assets into common robotics simulators. We demonstrate our system by creating procedural generators for 5 common articulated object categories. Experiments show that assets sampled from these generators are useful for movable object segmentation, training generalizable reinforcement learning policies, and sim-to-real transfer of imitation learning policies."
  },
  {
    "title": "Code-Driven Planning in Grid Worlds with Large Language Models",
    "url": "http://arxiv.org/abs/2505.10749v1",
    "arxiv_id": "2505.10749v1",
    "authors": [
      "Ashwath Vaithinathan Aravindan",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "published": "2025-05-15T23:23:31+00:00",
    "summary": "We propose an iterative programmatic planning (IPP) framework for solving grid-based tasks by synthesizing interpretable agent policies expressed in code using large language models (LLMs). Instead of relying on traditional search or reinforcement learning, our approach uses code generation as policy synthesis, where the LLM outputs executable programs that map environment states to action sequences. Our proposed architecture incorporates several prompting strategies, including direct code generation, pseudocode-conditioned refinement, and curriculum-based prompting, but also includes an iterative refinement mechanism that updates code based on task performance feedback. We evaluate our approach using six leading LLMs and two challenging grid-based benchmarks (GRASP and MiniGrid). Our IPP framework demonstrates improvements over direct code generation ranging from 10\\% to as much as 10x across five of the six models and establishes a new state-of-the-art result for GRASP. IPP is found to significantly outperform direct elicitation of a solution from GPT-o3-mini (by 63\\% on MiniGrid to 116\\% on GRASP), demonstrating the viability of the overall approach. Computational costs of all code generation approaches are similar. While code generation has a higher initial prompting cost compared to direct solution elicitation (\\$0.08 per task vs. \\$0.002 per instance for GPT-o3-mini), the code can be reused for any number of instances, making the amortized cost significantly lower (by 400x on GPT-o3-mini across the complete GRASP benchmark)."
  },
  {
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models",
    "url": "http://arxiv.org/abs/2505.10554v1",
    "arxiv_id": "2505.10554v1",
    "authors": [
      "Zhiyuan Hu",
      "Yibo Wang",
      "Hanze Dong",
      "Yuhui Xu",
      "Amrita Saha",
      "Caiming Xiong",
      "Bryan Hooi",
      "Junnan Li"
    ],
    "published": "2025-05-15T17:58:33+00:00",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's \"aha moment\". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental \"aha moments\". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment"
  },
  {
    "title": "Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation",
    "url": "http://arxiv.org/abs/2505.10522v1",
    "arxiv_id": "2505.10522v1",
    "authors": [
      "Xinrui Wang",
      "Yan Jin"
    ],
    "published": "2025-05-15T17:30:29+00:00",
    "summary": "Reinforcement learning (RL) has demonstrated remarkable potential in robotic manipulation but faces challenges in sample inefficiency and lack of interpretability, limiting its applicability in real world scenarios. Enabling the agent to gain a deeper understanding and adapt more efficiently to diverse working scenarios is crucial, and strategic knowledge utilization is a key factor in this process. This paper proposes a Knowledge Capture, Adaptation, and Composition (KCAC) framework to systematically integrate knowledge transfer into RL through cross-task curriculum learning. KCAC is evaluated using a two block stacking task in the CausalWorld benchmark, a complex robotic manipulation environment. To our knowledge, existing RL approaches fail to solve this task effectively, reflecting deficiencies in knowledge capture. In this work, we redesign the benchmark reward function by removing rigid constraints and strict ordering, allowing the agent to maximize total rewards concurrently and enabling flexible task completion. Furthermore, we define two self-designed sub-tasks and implement a structured cross-task curriculum to facilitate efficient learning. As a result, our KCAC approach achieves a 40 percent reduction in training time while improving task success rates by 10 percent compared to traditional RL methods. Through extensive evaluation, we identify key curriculum design parameters subtask selection, transition timing, and learning rate that optimize learning efficiency and provide conceptual guidance for curriculum based RL frameworks. This work offers valuable insights into curriculum design in RL and robotic learning."
  },
  {
    "title": "Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10484v1",
    "arxiv_id": "2505.10484v1",
    "authors": [
      "Andrea Baisero",
      "Rupali Bhati",
      "Shuo Liu",
      "Aathira Pillai",
      "Christopher Amato"
    ],
    "published": "2025-05-15T16:36:18+00:00",
    "summary": "Value function decomposition methods for cooperative multi-agent reinforcement learning compose joint values from individual per-agent utilities, and train them using a joint objective. To ensure that the action selection process between individual utilities and joint values remains consistent, it is imperative for the composition to satisfy the individual-global max (IGM) property. Although satisfying IGM itself is straightforward, most existing methods (e.g., VDN, QMIX) have limited representation capabilities and are unable to represent the full class of IGM values, and the one exception that has no such limitation (QPLEX) is unnecessarily complex. In this work, we present a simple formulation of the full class of IGM values that naturally leads to the derivation of QFIX, a novel family of value function decomposition models that expand the representation capabilities of prior models by means of a thin \"fixing\" layer. We derive multiple variants of QFIX, and implement three variants in two well-known multi-agent frameworks. We perform an empirical evaluation on multiple SMACv2 and Overcooked environments, which confirms that QFIX (i) succeeds in enhancing the performance of prior methods, (ii) learns more stably and performs better than its main competitor QPLEX, and (iii) achieves this while employing the simplest and smallest mixing models."
  },
  {
    "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps",
    "url": "http://arxiv.org/abs/2505.10482v1",
    "arxiv_id": "2505.10482v1",
    "authors": [
      "Ningyuan Yang",
      "Jiaxuan Gao",
      "Feng Gao",
      "Yi Wu",
      "Chao Yu"
    ],
    "published": "2025-05-15T16:33:44+00:00",
    "summary": "Diffusion policies, widely adopted in decision-making scenarios such as robotics, gaming and autonomous driving, are capable of learning diverse skills from demonstration data due to their high representation power. However, the sub-optimal and limited coverage of demonstration data could lead to diffusion policies that generate sub-optimal trajectories and even catastrophic failures. While reinforcement learning (RL)-based fine-tuning has emerged as a promising solution to address these limitations, existing approaches struggle to effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This challenge stems from the computational intractability of action likelihood estimation during the denoising process, which leads to complicated optimization objectives. In our experiments starting from randomly initialized policies, we find that online tuning of Diffusion Policies demonstrates much lower sample efficiency compared to directly applying PPO on MLP policies (MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework that reformulates Diffusion Policy as a noise-conditioned deterministic policy. By treating each denoising step as a differentiable transformation conditioned on pre-sampled noise, NCDPO enables tractable likelihood evaluation and gradient backpropagation through all diffusion timesteps. Our experiments demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when training from scratch, outperforming existing methods in both sample efficiency and final performance across diverse benchmarks, including continuous robot control and multi-agent game scenarios. Furthermore, our experimental results show that our method is robust to the number denoising timesteps in the Diffusion Policy."
  },
  {
    "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models",
    "url": "http://arxiv.org/abs/2505.10446v1",
    "arxiv_id": "2505.10446v1",
    "authors": [
      "Zemin Huang",
      "Zhiyang Chen",
      "Zijun Wang",
      "Tiancheng Li",
      "Guo-Jun Qi"
    ],
    "published": "2025-05-15T16:06:32+00:00",
    "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent \"thinking\" action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal, linear thinking process, DCoLT allows bidirectional, non-linear reasoning with no strict rule on grammatical correctness amid its intermediate steps of thought. We implement DCoLT on two representative Diffusion Language Models (DLMs). First, we choose SEDD as a representative continuous-time discrete diffusion model, where its concrete score derives a probabilistic policy to maximize the RL reward over the entire sequence of intermediate diffusion steps. We further consider the discrete-time masked diffusion language model -- LLaDA, and find that the order to predict and unmask tokens plays an essential role to optimize its RL action resulting from the ranking-based Unmasking Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both math and code generation tasks show that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval."
  },
  {
    "title": "IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.10442v1",
    "arxiv_id": "2505.10442v1",
    "authors": [
      "Dechen Gao",
      "Hang Wang",
      "Hanchu Zhou",
      "Nejib Ammar",
      "Shatadal Mishra",
      "Ahmadreza Moradipari",
      "Iman Soltani",
      "Junshan Zhang"
    ],
    "published": "2025-05-15T16:01:21+00:00",
    "summary": "Imitation learning (IL) and reinforcement learning (RL) each offer distinct advantages for robotics policy learning: IL provides stable learning from demonstrations, and RL promotes generalization through exploration. While existing robot learning approaches using IL-based pre-training followed by RL-based fine-tuning are promising, this two-step learning paradigm often suffers from instability and poor sample efficiency during the RL fine-tuning phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning and Imitation Learning, for policy fine-tuning, which periodically injects IL updates after multiple RL updates and hence can benefit from the stability of IL and the guidance of expert data for more efficient exploration throughout the entire fine-tuning process. Since IL and RL involve different optimization objectives, we develop gradient separation mechanisms to prevent destructive interference during \\ABBR fine-tuning, by separating possibly conflicting gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous analysis, and our findings shed light on why interleaving IL with RL stabilizes learning and improves sample-efficiency. Extensive experiments on 14 robot manipulation and locomotion tasks across 3 benchmarks, including FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \\ABBR can significantly improve sample efficiency and mitigate performance collapse during online finetuning in both long- and short-horizon tasks with either sparse or dense rewards. IN-RIL, as a general plug-in compatible with various state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g., from 12\\% to 88\\% with 6.3x improvement in the success rate on Robomimic Transport. Project page: https://github.com/ucd-dare/IN-RIL."
  },
  {
    "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs",
    "url": "http://arxiv.org/abs/2505.10425v1",
    "arxiv_id": "2505.10425v1",
    "authors": [
      "Jingyao Wang",
      "Wenwen Qiang",
      "Zeen Song",
      "Changwen Zheng",
      "Hui Xiong"
    ],
    "published": "2025-05-15T15:40:25+00:00",
    "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in reasoning abilities. However, existing methods overlook the trade-off between reasoning effectiveness and computational efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens. To address this, we propose Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework for LLMs to make the models achieve optimal reasoning with fewer tokens. Specifically, L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward, i.e., quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. We propose a method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix. Theoretical analyses show that it significantly reduces computational complexity with high estimation accuracy. By immediately rewarding each episode's contribution and penalizing excessive updates, L2T optimizes the model via reinforcement learning to maximize the use of each episode and achieve effective updates. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency."
  },
  {
    "title": "Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency",
    "url": "http://arxiv.org/abs/2505.10422v1",
    "arxiv_id": "2505.10422v1",
    "authors": [
      "Daniel Weitekamp",
      "Christopher MacLellan",
      "Erik Harpstead",
      "Kenneth Koedinger"
    ],
    "published": "2025-05-15T15:39:09+00:00",
    "summary": "Human learning relies on specialization -- distinct cognitive mechanisms working together to enable rapid learning. In contrast, most modern neural networks rely on a single mechanism: gradient descent over an objective function. This raises the question: might human learners' relatively rapid learning from just tens of examples instead of tens of thousands in data-driven deep learning arise from our ability to use multiple specialized mechanisms of learning in combination? We investigate this question through an ablation analysis of inductive human learning simulations in online tutoring environments. Comparing reinforcement learning to a more data-efficient 3-mechanism symbolic rule induction approach, we find that decomposing learning into multiple distinct mechanisms significantly improves data efficiency, bringing it in line with human learning. Furthermore, we show that this decomposition has a greater impact on efficiency than the distinction between symbolic and subsymbolic learning alone. Efforts to align data-driven machine learning with human learning often overlook the stark difference in learning efficiency. Our findings suggest that integrating multiple specialized learning mechanisms may be key to bridging this gap."
  },
  {
    "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change",
    "url": "http://arxiv.org/abs/2505.10330v1",
    "arxiv_id": "2505.10330v1",
    "authors": [
      "Jonathan Clifford Balloch"
    ],
    "published": "2025-05-15T14:19:01+00:00",
    "summary": "Real-world autonomous decision-making systems, from robots to recommendation engines, must operate in environments that change over time. While deep reinforcement learning (RL) has shown an impressive ability to learn optimal policies in stationary environments, most methods are data intensive and assume a world that does not change between training and test time. As a result, conventional RL methods struggle to adapt when conditions change. This poses a fundamental challenge: how can RL agents efficiently adapt their behavior when encountering novel environmental changes during deployment without catastrophically forgetting useful prior knowledge? This dissertation demonstrates that efficient online adaptation requires two key capabilities: (1) prioritized exploration and sampling strategies that help identify and learn from relevant experiences, and (2) selective preservation of prior knowledge through structured representations that can be updated without disruption to reusable components."
  },
  {
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10320v1",
    "arxiv_id": "2505.10320v1",
    "authors": [
      "Chenxi Whitehouse",
      "Tianlu Wang",
      "Ping Yu",
      "Xian Li",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ],
    "published": "2025-05-15T14:05:15+00:00",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses."
  },
  {
    "title": "Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10296v1",
    "arxiv_id": "2505.10296v1",
    "authors": [
      "Jiaju Qi",
      "Lei Lei",
      "Thorsteinn Jonsson",
      "Dusit Niyato"
    ],
    "published": "2025-05-15T13:44:27+00:00",
    "summary": "The growing adoption of Electric Buses (EBs) represents a significant step toward sustainable development. By utilizing Internet of Things (IoT) systems, charging stations can autonomously determine charging schedules based on real-time data. However, optimizing EB charging schedules remains a critical challenge due to uncertainties in travel time, energy consumption, and fluctuating electricity prices. Moreover, to address real-world complexities, charging policies must make decisions efficiently across multiple time scales and remain scalable for large EB fleets. In this paper, we propose a Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the original Markov Decision Process (MDP) into two augmented MDPs. To solve these MDPs and enable multi-timescale decision-making, we introduce a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic (DAC) algorithm for large-scale EB fleets are addressed through enhancements at both decision levels. At the high level, we redesign the decentralized actor network and integrate an attention mechanism to extract relevant global state information for each EB, decreasing the size of neural networks. At the low level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is incorporated into the DAC framework, enabling decentralized and coordinated charging power decisions, reducing computational complexity and enhancing convergence speed. Extensive experiments with real-world data demonstrate the superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet charging schedules."
  },
  {
    "title": "Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10262v1",
    "arxiv_id": "2505.10262v1",
    "authors": [
      "Jiaju Qi",
      "Lei Lei",
      "Thorsteinn Jonsson",
      "Lajos Hanzo"
    ],
    "published": "2025-05-15T13:13:41+00:00",
    "summary": "The charging scheduling problem of Electric Buses (EBs) is investigated based on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is conceived, where the time horizon includes multiple charging and operating periods in a day, while each period is further divided into multiple time steps. To overcome the challenge of long-range multi-phase planning with sparse reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is proposed for simultaneously solving the decision problems arising at different temporal resolutions. As a result, the high-level agent learns an effective policy for prescribing the charging targets for every charging period, while the low-level agent learns an optimal policy for setting the charging power of every time step within a single charging period, with the aim of minimizing the charging costs while meeting the charging target. It is proved that the flat policy constructed by superimposing the optimal high-level policy and the optimal low-level policy performs as well as the optimal policy of the original MDP. Since jointly learning both levels of policies is challenging due to the non-stationarity of the high-level agent and the sampling inefficiency of the low-level agent, we divide the joint learning process into two phases and exploit our new HER algorithm to manipulate the experience replay buffers for both levels of agents. Numerical experiments are performed with the aid of real-world data to evaluate the performance of the proposed algorithm."
  },
  {
    "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward",
    "url": "http://arxiv.org/abs/2505.10218v1",
    "arxiv_id": "2505.10218v1",
    "authors": [
      "Zongsheng Wang",
      "Kaili Sun",
      "Bowen Wu",
      "Qun Yu",
      "Ying Li",
      "Baoxun Wang"
    ],
    "published": "2025-05-15T12:22:10+00:00",
    "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency. To address this, we propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset through multi-LLM collaboration, and implement experiments to enhance reasoning coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness. Case analyses further reveal the model's enhanced ability to resolve conflicting contextual cues and sustain first-person narrative consistency. This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs."
  },
  {
    "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning",
    "url": "http://arxiv.org/abs/2505.10182v1",
    "arxiv_id": "2505.10182v1",
    "authors": [
      "Yoichi Ishibashi",
      "Taro Yano",
      "Masafumi Oyamada"
    ],
    "published": "2025-05-15T11:29:01+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlying texts, based on the premise that texts are the result of the author's thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty."
  },
  {
    "title": "Training People to Reward Robots",
    "url": "http://arxiv.org/abs/2505.10151v1",
    "arxiv_id": "2505.10151v1",
    "authors": [
      "Endong Sun",
      "Yuqing Zhu",
      "Matthew Howard"
    ],
    "published": "2025-05-15T10:23:56+00:00",
    "summary": "Learning from demonstration (LfD) is a technique that allows expert teachers to teach task-oriented skills to robotic systems. However, the most effective way of guiding novice teachers to approach expert-level demonstrations quantitatively for specific teaching tasks remains an open question. To this end, this paper investigates the use of machine teaching (MT) to guide novice teachers to improve their teaching skills based on reinforcement learning from demonstration (RLfD). The paper reports an experiment in which novices receive MT-derived guidance to train their ability to teach a given motor skill with only 8 demonstrations and generalise this to previously unseen ones. Results indicate that the MT-guidance not only enhances robot learning performance by 89% on the training skill but also causes a 70% improvement in robot learning performance on skills not seen by subjects during training. These findings highlight the effectiveness of MT-guidance in upskilling human teaching behaviours, ultimately improving demonstration quality in RLfD."
  },
  {
    "title": "Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests",
    "url": "http://arxiv.org/abs/2505.10033v1",
    "arxiv_id": "2505.10033v1",
    "authors": [
      "Luis F. W. Batista",
      "St\u00e9phanie Aravecchia",
      "Seth Hutchinson",
      "C\u00e9dric Pradalier"
    ],
    "published": "2025-05-15T07:29:16+00:00",
    "summary": "Despite significant advancements in Deep Reinforcement Learning (DRL) for Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions, particularly under external disturbances, remains insufficiently explored. In this paper, we evaluate the resilience of a DRL-based agent designed to capture floating waste under various perturbations. We train the agent using domain randomization and evaluate its performance in real-world field tests, assessing its ability to handle unexpected disturbances such as asymmetric drag and an off-center payload. We assess the agent's performance under these perturbations in both simulation and real-world experiments, quantifying performance degradation and benchmarking it against an MPC baseline. Results indicate that the DRL agent performs reliably despite significant disturbances. Along with the open-source release of our implementation, we provide insights into effective training strategies, real-world challenges, and practical considerations for deploying DRLbased ASV controllers."
  },
  {
    "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction",
    "url": "http://arxiv.org/abs/2505.10027v1",
    "arxiv_id": "2505.10027v1",
    "authors": [
      "Shijie Lyu"
    ],
    "published": "2025-05-15T07:17:03+00:00",
    "summary": "With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes."
  },
  {
    "title": "APEX: Action Priors Enable Efficient Exploration for Skill Imitation on Articulated Robots",
    "url": "http://arxiv.org/abs/2505.10022v1",
    "arxiv_id": "2505.10022v1",
    "authors": [
      "Shivam Sood",
      "Laukik B Nakhwa",
      "Yuhong Cao",
      "Sun Ge",
      "Guillaume Sartoretti"
    ],
    "published": "2025-05-15T07:09:23+00:00",
    "summary": "Learning by imitation provides an effective way for robots to develop well-regulated complex behaviors and directly benefit from natural demonstrations. State-of-the-art imitation learning (IL) approaches typically leverage Adversarial Motion Priors (AMP), which, despite their impressive results, suffer from two key limitations. They are prone to mode collapse, which often leads to overfitting to the simulation environment and thus increased sim-to-real gap, and they struggle to learn diverse behaviors effectively. To overcome these limitations, we introduce APEX (Action Priors enable Efficient eXploration): a simple yet versatile imitation learning framework that integrates demonstrations directly into reinforcement learning (RL), maintaining high exploration while grounding behavior with expert-informed priors. We achieve this through a combination of decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is complemented by a multi-critic RL framework that effectively balances stylistic consistency with task performance. Our approach achieves sample-efficient imitation learning and enables the acquisition of diverse skills within a single policy. APEX generalizes to varying velocities and preserves reference-like styles across complex tasks such as navigating rough terrain and climbing stairs, utilizing only flat-terrain kinematic motion data as a prior. We validate our framework through extensive hardware experiments on the Unitree Go2 quadruped. There, APEX yields diverse and agile locomotion gaits, inherent gait transitions, and the highest reported speed for the platform to the best of our knowledge (peak velocity of ~3.3 m/s on hardware). Our results establish APEX as a compelling alternative to existing IL methods, offering better efficiency, adaptability, and real-world performance."
  },
  {
    "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts",
    "url": "http://arxiv.org/abs/2505.10010v1",
    "arxiv_id": "2505.10010v1",
    "authors": [
      "Jing-Cheng Pang",
      "Kaiyuan Li",
      "Yidi Wang",
      "Si-Hang Yang",
      "Shengyi Jiang",
      "Yang Yu"
    ],
    "published": "2025-05-15T06:45:37+00:00",
    "summary": "A central challenge in reinforcement learning (RL) is its dependence on extensive real-world interaction data to learn task-specific policies. While recent work demonstrates that large language models (LLMs) can mitigate this limitation by generating synthetic experience (noted as imaginary rollouts) for mastering novel tasks, progress in this emerging field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts. The key features of ImagineBench include: (1) datasets comprising environment-collected and LLM-imaginary rollouts; (2) diverse domains of environments covering locomotion, robotic manipulation, and navigation tasks; and (3) natural language task instructions with varying complexity levels to facilitate language-conditioned policy learning. Through systematic evaluation of state-of-the-art offline RL algorithms, we observe that simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving 35.44% success rate in hard tasks in contrast to 64.37% of method training on real rollouts for hard tasks. This result highlights the need for algorithm advancements to better leverage LLM-imaginary rollouts. Additionally, we identify key opportunities for future research: including better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks. Our code is publicly available at https://github.com/LAMDA-RL/ImagineBench."
  },
  {
    "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.10007v1",
    "arxiv_id": "2505.10007v1",
    "authors": [
      "Zijun Chen",
      "Shengbo Wang",
      "Nian Si"
    ],
    "published": "2025-05-15T06:42:25+00:00",
    "summary": "Motivated by practical applications where stable long-term performance is critical-such as robotics, operations research, and healthcare-we study the problem of distributionally robust (DR) average-reward reinforcement learning. We propose two algorithms that achieve near-optimal sample complexity. The first reduces the problem to a DR discounted Markov decision process (MDP), while the second, Anchored DR Average-Reward MDP, introduces an anchoring state to stabilize the controlled transition kernels within the uncertainty set. Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms attain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}| t_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as well as the robust average reward under KL and $f_k$-divergence-based uncertainty sets, provided the uncertainty radius is sufficiently small. Here, $\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote the sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing time of the nominal MDP. This represents the first finite-sample convergence guarantee for DR average-reward reinforcement learning. We further validate the convergence rates of our algorithms through numerical experiments."
  },
  {
    "title": "Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.09959v1",
    "arxiv_id": "2505.09959v1",
    "authors": [
      "Zengxia Guo",
      "Bohui An",
      "Zhongqi Lu"
    ],
    "published": "2025-05-15T04:41:21+00:00",
    "summary": "Federated reinforcement learning (FRL) methods usually share the encrypted local state or policy information and help each client to learn from others while preserving everyone's privacy. In this work, we propose that sharing the approximated behavior metric-based state projection function is a promising way to enhance the performance of FRL and concurrently provides an effective protection of sensitive information. We introduce FedRAG, a FRL framework to learn a computationally practical projection function of states for each client and aggregating the parameters of projection functions at a central server. The FedRAG approach shares no sensitive task-specific information, yet provides information gain for each client. We conduct extensive experiments on the DeepMind Control Suite to demonstrate insightful results."
  },
  {
    "title": "Offline Reinforcement Learning for Microgrid Voltage Regulation",
    "url": "http://arxiv.org/abs/2505.09920v1",
    "arxiv_id": "2505.09920v1",
    "authors": [
      "Shan Yang",
      "Yongli Zhu"
    ],
    "published": "2025-05-15T03:10:18+00:00",
    "summary": "This paper presents a study on using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration. When environment interaction is unviable due to technical or safety reasons, the proposed approach can still obtain an applicable model through offline-style training on a previously collected dataset, lowering the negative impact of lacking online environment interactions. Experiment results on the IEEE 33-bus system demonstrate the feasibility and effectiveness of the proposed approach on different offline datasets, including the one with merely low-quality experience."
  },
  {
    "title": "Promise of Data-Driven Modeling and Decision Support for Precision Oncology and Theranostics",
    "url": "http://arxiv.org/abs/2505.09899v1",
    "arxiv_id": "2505.09899v1",
    "authors": [
      "Binesh Sadanandan",
      "Vahid Behzadan"
    ],
    "published": "2025-05-15T02:02:03+00:00",
    "summary": "Cancer remains a leading cause of death worldwide, necessitating personalized treatment approaches to improve outcomes. Theranostics, combining molecular-level imaging with targeted therapy, offers potential for precision oncology but requires optimized, patient-specific care plans. This paper investigates state-of-the-art data-driven decision support applications with a reinforcement learning focus in precision oncology. We review current applications, training environments, state-space representation, performance evaluation criteria, and measurement of risk and reward, highlighting key challenges. We propose a framework integrating data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing, addressing identified challenges and setting directions for future research. The framework leverages Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models while applying reinforcement learning algorithms to iteratively refine treatment policies based on patient-specific data."
  },
  {
    "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent",
    "url": "http://arxiv.org/abs/2505.09820v1",
    "arxiv_id": "2505.09820v1",
    "authors": [
      "Sajib Biswas",
      "Mao Nishino",
      "Samuel Jacob Chacko",
      "Xiuwen Liu"
    ],
    "published": "2025-05-14T21:50:46+00:00",
    "summary": "As Large Language Models (LLMs) are widely used, understanding them systematically is key to improving their safety and realizing their full potential. Although many models are aligned using techniques such as reinforcement learning from human feedback (RLHF), they are still vulnerable to jailbreaking attacks. Some of the existing adversarial attack methods search for discrete tokens that may jailbreak a target model while others try to optimize the continuous space represented by the tokens of the model's vocabulary. While techniques based on the discrete space may prove to be inefficient, optimization of continuous token embeddings requires projections to produce discrete tokens, which might render them ineffective. To fully utilize the constraints and the structures of the space, we develop an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method to ensure that the optimized one-hot encoding always stays within the probability simplex. We prove the convergence of the technique and implement an efficient algorithm that is effective in jailbreaking several widely used LLMs. We demonstrate the efficacy of the proposed technique using five open-source LLMs on four openly available datasets. The results show that the technique achieves a higher success rate with great efficiency compared to three other state-of-the-art jailbreaking techniques. The source code for our implementation is available at: https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack"
  },
  {
    "title": "Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration",
    "url": "http://arxiv.org/abs/2505.09756v1",
    "arxiv_id": "2505.09756v1",
    "authors": [
      "Zhaoyang Shi"
    ],
    "published": "2025-05-14T19:42:43+00:00",
    "summary": "We propose a new framework for multi-agent reinforcement learning (MARL), where the agents cooperate in a time-evolving network with latent community structures and mixed memberships. Unlike traditional neighbor-based or fixed interaction graphs, our community-based framework captures flexible and abstract coordination patterns by allowing each agent to belong to multiple overlapping communities. Each community maintains shared policy and value functions, which are aggregated by individual agents according to personalized membership weights. We also design actor-critic algorithms that exploit this structure: agents inherit community-level estimates for policy updates and value learning, enabling structured information sharing without requiring access to other agents' policies. Importantly, our approach supports both transfer learning by adapting to new agents or tasks via membership estimation, and active learning by prioritizing uncertain communities during exploration. Theoretically, we establish convergence guarantees under linear function approximation for both actor and critic updates. To our knowledge, this is the first MARL framework that integrates community structure, transferability, and active learning with provable guarantees."
  },
  {
    "title": "Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems",
    "url": "http://arxiv.org/abs/2505.09734v1",
    "arxiv_id": "2505.09734v1",
    "authors": [
      "Babak Esmaeili",
      "Nariman Niknejad",
      "Hamidreza Modares"
    ],
    "published": "2025-05-14T18:49:32+00:00",
    "summary": "This paper presents a risk-aware safe reinforcement learning (RL) control design for stochastic discrete-time linear systems. Rather than using a safety certifier to myopically intervene with the RL controller, a risk-informed safe controller is also learned besides the RL controller, and the RL and safe controllers are combined together. Several advantages come along with this approach: 1) High-confidence safety can be certified without relying on a high-fidelity system model and using limited data available, 2) Myopic interventions and convergence to an undesired equilibrium can be avoided by deciding on the contribution of two stabilizing controllers, and 3) highly efficient and computationally tractable solutions can be provided by optimizing over a scalar decision variable and linear programming polyhedral sets. To learn safe controllers with a large invariant set, piecewise affine controllers are learned instead of linear controllers. To this end, the closed-loop system is first represented using collected data, a decision variable, and noise. The effect of the decision variable on the variance of the safe violation of the closed-loop system is formalized. The decision variable is then designed such that the probability of safety violation for the learned closed-loop system is minimized. It is shown that this control-oriented approach reduces the data requirements and can also reduce the variance of safety violations. Finally, to integrate the safe and RL controllers, a new data-driven interpolation technique is introduced. This method aims to maintain the RL agent's optimal implementation while ensuring its safety within environments characterized by noise. The study concludes with a simulation example that serves to validate the theoretical results."
  },
  {
    "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
    "url": "http://arxiv.org/abs/2505.09577v1",
    "arxiv_id": "2505.09577v1",
    "authors": [
      "Chaofan Zhang",
      "Peng Hao",
      "Xiaoge Cao",
      "Xiaoshuai Hao",
      "Shaowei Cui",
      "Shuo Wang"
    ],
    "published": "2025-05-14T17:29:35+00:00",
    "summary": "While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla"
  },
  {
    "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach",
    "url": "http://arxiv.org/abs/2505.09576v1",
    "arxiv_id": "2505.09576v1",
    "authors": [
      "Shannon Lodoen",
      "Alexi Orchard"
    ],
    "published": "2025-05-14T17:29:19+00:00",
    "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more \"human-like\" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots."
  },
  {
    "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
    "url": "http://arxiv.org/abs/2505.09558v1",
    "arxiv_id": "2505.09558v1",
    "authors": [
      "Shengpeng Ji",
      "Tianle Liang",
      "Yangzhuo Li",
      "Jialong Zuo",
      "Minghui Fang",
      "Jinzheng He",
      "Yifu Chen",
      "Zhengqing Liu",
      "Ziyue Jiang",
      "Xize Cheng",
      "Siqi Zheng",
      "Jin Xu",
      "Junyang Lin",
      "Zhou Zhao"
    ],
    "published": "2025-05-14T16:54:15+00:00",
    "summary": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted."
  },
  {
    "title": "Distilling Realizable Students from Unrealizable Teachers",
    "url": "http://arxiv.org/abs/2505.09546v1",
    "arxiv_id": "2505.09546v1",
    "authors": [
      "Yujin Kim",
      "Nathaniel Chin",
      "Arnav Vasudev",
      "Sanjiban Choudhury"
    ],
    "published": "2025-05-14T16:45:51+00:00",
    "summary": "We study policy distillation under privileged information, where a student policy with only partial observations must learn from a teacher with full-state access. A key challenge is information asymmetry: the student cannot directly access the teacher's state space, leading to distributional shifts and policy degradation. Existing approaches either modify the teacher to produce realizable but sub-optimal demonstrations or rely on the student to explore missing information independently, both of which are inefficient. Our key insight is that the student should strategically interact with the teacher --querying only when necessary and resetting from recovery states --to stay on a recoverable path within its own observation space. We introduce two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration. We validate our methods in both simulated and real-world robotic tasks, demonstrating significant improvements over standard teacher-student baselines in training efficiency and final performance. The project website is available at : https://portal-cornell.github.io/CritiQ_ReTRy/"
  },
  {
    "title": "Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data",
    "url": "http://arxiv.org/abs/2505.09496v1",
    "arxiv_id": "2505.09496v1",
    "authors": [
      "Rui Miao",
      "Babak Shahbaba",
      "Annie Qu"
    ],
    "published": "2025-05-14T15:44:10+00:00",
    "summary": "Offline reinforcement learning (RL) aims to find optimal policies in dynamic environments in order to maximize the expected total rewards by leveraging pre-collected data. Learning from heterogeneous data is one of the fundamental challenges in offline RL. Traditional methods focus on learning an optimal policy for all individuals with pre-collected data from a single episode or homogeneous batch episodes, and thus, may result in a suboptimal policy for a heterogeneous population. In this paper, we propose an individualized offline policy optimization framework for heterogeneous time-stationary Markov decision processes (MDPs). The proposed heterogeneous model with individual latent variables enables us to efficiently estimate the individual Q-functions, and our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm guarantees a fast rate on the average regret under a weak partial coverage assumption on behavior policies. In addition, our simulation studies and a real data application demonstrate the superior numerical performance of the proposed method compared with existing methods."
  },
  {
    "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity Injection",
    "url": "http://arxiv.org/abs/2505.09486v1",
    "arxiv_id": "2505.09486v1",
    "authors": [
      "Seyed Roozbeh Razavi Rohani",
      "Khashayar Khajavi",
      "Wesley Chung",
      "Mo Chen",
      "Sharan Vaswani"
    ],
    "published": "2025-05-14T15:36:51+00:00",
    "summary": "Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity."
  },
  {
    "title": "Quantum state-agnostic work extraction (almost) without dissipation",
    "url": "http://arxiv.org/abs/2505.09456v1",
    "arxiv_id": "2505.09456v1",
    "authors": [
      "Josep Lumbreras",
      "Ruo Cheng Huang",
      "Yanglin Hu",
      "Mile Gu",
      "Marco Tomamichel"
    ],
    "published": "2025-05-14T15:07:58+00:00",
    "summary": "We investigate work extraction protocols designed to transfer the maximum possible energy to a battery using sequential access to $N$ copies of an unknown pure qubit state. The core challenge is designing interactions to optimally balance two competing goals: charging of the battery optimally using the qubit in hand, and acquiring more information by qubit to improve energy harvesting in subsequent rounds. Here, we leverage exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies achieving energy dissipation that scales only poly-logarithmically in $N$. This represents an exponential improvement over current protocols based on full state tomography."
  },
  {
    "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
    "url": "http://arxiv.org/abs/2505.09439v1",
    "arxiv_id": "2505.09439v1",
    "authors": [
      "Andrew Rouditchenko",
      "Saurabhchand Bhati",
      "Edson Araujo",
      "Samuel Thomas",
      "Hilde Kuehne",
      "Rogerio Feris",
      "James Glass"
    ],
    "published": "2025-05-14T14:47:16+00:00",
    "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance."
  },
  {
    "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting",
    "url": "http://arxiv.org/abs/2505.09395v1",
    "arxiv_id": "2505.09395v1",
    "authors": [
      "Chen-Yu Liu",
      "Kuan-Cheng Chen",
      "Yi-Chien Chen",
      "Samuel Yen-Chi Chen",
      "Wei-Hao Huang",
      "Wei-Jia Huang",
      "Yen-Jui Chang"
    ],
    "published": "2025-05-14T13:50:44+00:00",
    "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but remains computationally demanding due to the complexity of atmospheric dynamics and the resource requirements of deep learning models. Quantum-Train (QT), a hybrid quantum-classical framework that leverages quantum neural networks (QNNs) to generate trainable parameters exclusively during training, eliminating the need for quantum hardware at inference time. Building on QT's success across multiple domains, including image classification, reinforcement learning, flood prediction, and large language model (LLM) fine-tuning, we introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA enables parameter-efficient training while maintaining predictive accuracy. This work represents the first application of quantum machine learning (QML) to large-scale typhoon trajectory prediction, offering a scalable and energy-efficient approach to climate modeling. Our results demonstrate that QPA significantly reduces the number of trainable parameters while preserving performance, making high-performance forecasting more accessible and sustainable through hybrid quantum-classical learning."
  },
  {
    "title": "TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search",
    "url": "http://arxiv.org/abs/2505.09371v1",
    "arxiv_id": "2505.09371v1",
    "authors": [
      "Akash Kundu",
      "Stefano Mangini"
    ],
    "published": "2025-05-14T13:23:34+00:00",
    "summary": "Variational quantum algorithms hold the promise to address meaningful quantum problems already on noisy intermediate-scale quantum hardware, but they face the challenge of designing quantum circuits that both solve the target problem and comply with device limitations. Quantum architecture search (QAS) automates this design process, with reinforcement learning (RL) emerging as a promising approach. Yet, RL-based QAS methods encounter significant scalability issues, as computational and training costs grow rapidly with the number of qubits, circuit depth, and noise, severely impacting performance. To address these challenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that combines tensor network (TN) methods with RL for designing quantum circuits. By warm-starting the architecture search with a matrix product state approximation of the target solution, TensorRL-QAS effectively narrows the search space to physically meaningful circuits, accelerating convergence to the desired solution. Tested on several quantum chemistry problems of up to 12-qubit, TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth compared to baseline methods, while maintaining or surpassing chemical accuracy. It reduces function evaluations by up to 100-fold, accelerates training episodes by up to $98\\%$, and achieves up to $50\\%$ success probability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline approaches. Robustness and versatility are demonstrated both in the noiseless and noisy scenarios, where we report a simulation of up to 8-qubit. These advancements establish TensorRL-QAS as a promising candidate for a scalable and efficient quantum circuit discovery protocol on near-term quantum hardware."
  },
  {
    "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging",
    "url": "http://arxiv.org/abs/2505.09316v1",
    "arxiv_id": "2505.09316v1",
    "authors": [
      "Hongjin Qian",
      "Zheng Liu"
    ],
    "published": "2025-05-14T12:13:38+00:00",
    "summary": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents."
  },
  {
    "title": "Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model",
    "url": "http://arxiv.org/abs/2505.09308v1",
    "arxiv_id": "2505.09308v1",
    "authors": [
      "George Andriopoulos",
      "Soyuj Jung Basnet",
      "Juan Guevara",
      "Li Guo",
      "Keith Ross"
    ],
    "published": "2025-05-14T11:52:45+00:00",
    "summary": "The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs). This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning. Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance? (2) Can whitening and normalizing regression targets improve training performance? The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings. Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings. These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies."
  },
  {
    "title": "Embodied Intelligent Industrial Robotics: Concepts and Techniques",
    "url": "http://arxiv.org/abs/2505.09305v1",
    "arxiv_id": "2505.09305v1",
    "authors": [
      "Chaoran Zhang",
      "Chenhao Zhang",
      "Zhaobo Xu",
      "Qinghongbing Xie",
      "Pingfa Feng",
      "Long Zeng"
    ],
    "published": "2025-05-14T11:41:55+00:00",
    "summary": "In recent years, embodied intelligent robotics (EIR) has made significant progress in multi-modal perception, autonomous decision-making, and physical interaction. Some robots have already been tested in general-purpose scenarios such as homes and shopping malls. We aim to advance the research and application of embodied intelligence in industrial scenes. However, current EIR lacks a deep understanding of industrial environment semantics and the normative constraints between industrial operating objects. To address this gap, this paper first reviews the history of industrial robotics and the mainstream EIR frameworks. We then introduce the concept of the embodied intelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR technology framework for industrial environments. The framework includes four main modules: world model, high-level task planner, low-level skill controller, and simulator. We also review the current development of technologies related to each module and highlight recent progress in adapting them to industrial applications. Finally, we summarize the key challenges EIIR faces in industrial scenarios and suggest future research directions. We believe that EIIR technology will shape the next generation of industrial robotics. Industrial systems based on embodied intelligent industrial robots offer strong potential for enabling intelligent manufacturing. We will continue to track and summarize new research in this area and hope this review will serve as a valuable reference for scholars and engineers interested in industrial embodied intelligence. Together, we can help drive the rapid advancement and application of this technology. The associated project can be found at https://github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List."
  },
  {
    "title": "Embodied Intelligent Industrial Robotics: Concepts and Techniques",
    "url": "http://arxiv.org/abs/2505.09305v2",
    "arxiv_id": "2505.09305v2",
    "authors": [
      "Chaoran Zhang",
      "Chenhao Zhang",
      "Zhaobo Xu",
      "Qinghongbing Xie",
      "Pingfa Feng",
      "Long Zeng"
    ],
    "published": "2025-05-14T11:41:55+00:00",
    "summary": "In recent years, embodied intelligent robotics (EIR) has made significant progress in multi-modal perception, autonomous decision-making, and physical interaction. Some robots have already been tested in general-purpose scenarios such as homes and shopping malls. We aim to advance the research and application of embodied intelligence in industrial scenes. However, current EIR lacks a deep understanding of industrial environment semantics and the normative constraints between industrial operating objects. To address this gap, this paper first reviews the history of industrial robotics and the mainstream EIR frameworks. We then introduce the concept of the embodied intelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR technology framework for industrial environments. The framework includes four main modules: world model, high-level task planner, low-level skill controller, and simulator. We also review the current development of technologies related to each module and highlight recent progress in adapting them to industrial applications. Finally, we summarize the key challenges EIIR faces in industrial scenarios and suggest future research directions. We believe that EIIR technology will shape the next generation of industrial robotics. Industrial systems based on embodied intelligent industrial robots offer strong potential for enabling intelligent manufacturing. We will continue to track and summarize new research in this area and hope this review will serve as a valuable reference for scholars and engineers interested in industrial embodied intelligence. Together, we can help drive the rapid advancement and application of this technology. The associated project can be found at https://github.com/jackyzengl/EIIR."
  },
  {
    "title": "A drone that learns to efficiently find objects in agricultural fields: from simulation to the real world",
    "url": "http://arxiv.org/abs/2505.09278v1",
    "arxiv_id": "2505.09278v1",
    "authors": [
      "Rick van Essen",
      "Gert Kootstra"
    ],
    "published": "2025-05-14T10:59:09+00:00",
    "summary": "Drones are promising for data collection in precision agriculture, however, they are limited by their battery capacity. Efficient path planners are therefore required. This paper presents a drone path planner trained using Reinforcement Learning (RL) on an abstract simulation that uses object detections and uncertain prior knowledge. The RL agent controls the flight direction and can terminate the flight. By using the agent in combination with the drone's flight controller and a detection network to process camera images, it is possible to evaluate the performance of the agent on real-world data. In simulation, the agent yielded on average a 78% shorter flight path compared to a full coverage planner, at the cost of a 14% lower recall. On real-world data, the agent showed a 72% shorter flight path compared to a full coverage planner, however, at the cost of a 25% lower recall. The lower performance on real-world data was attributed to the real-world object distribution and the lower accuracy of prior knowledge, and shows potential for improvement. Overall, we concluded that for applications where it is not crucial to find all objects, such as weed detection, the learned-based path planner is suitable and efficient."
  },
  {
    "title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning",
    "url": "http://arxiv.org/abs/2505.09118v1",
    "arxiv_id": "2505.09118v1",
    "authors": [
      "Dayong Liang",
      "Changmeng Zheng",
      "Zhiyuan Wen",
      "Yi Cai",
      "Xiao-Yong Wei",
      "Qing Li"
    ],
    "published": "2025-05-14T04:04:23+00:00",
    "summary": "Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a lone-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance."
  },
  {
    "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer",
    "url": "http://arxiv.org/abs/2505.09114v1",
    "arxiv_id": "2505.09114v1",
    "authors": [
      "Minh Hoang Nguyen",
      "Linh Le Pham Van",
      "Thommen George Karimpanal",
      "Sunil Gupta",
      "Hung Le"
    ],
    "published": "2025-05-14T03:45:16+00:00",
    "summary": "Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities."
  },
  {
    "title": "Imitation Learning for Adaptive Control of a Virtual Soft Exoglove",
    "url": "http://arxiv.org/abs/2505.09099v1",
    "arxiv_id": "2505.09099v1",
    "authors": [
      "Shirui Lyu",
      "Vittorio Caggiano",
      "Matteo Leonetti",
      "Dario Farina",
      "Letizia Gionfrida"
    ],
    "published": "2025-05-14T03:09:21+00:00",
    "summary": "The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments. However, the uniqueness of patients' muscle loss is often overlooked. Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks. Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration. This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks. The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove. Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces. The learned exoglove controller achieved an average of 90.5\\% of the original manipulation proficiency."
  },
  {
    "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
    "url": "http://arxiv.org/abs/2505.09082v1",
    "arxiv_id": "2505.09082v1",
    "authors": [
      "Sophie Zhang",
      "Zhiming Lin"
    ],
    "published": "2025-05-14T02:35:47+00:00",
    "summary": "Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models."
  },
  {
    "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models",
    "url": "http://arxiv.org/abs/2505.09655v1",
    "arxiv_id": "2505.09655v1",
    "authors": [
      "Xiwen Chen",
      "Wenhui Zhu",
      "Peijie Qiu",
      "Xuanzhao Dong",
      "Hao Wang",
      "Haiyu Wu",
      "Huayu Li",
      "Aristeidis Sotiras",
      "Yalin Wang",
      "Abolfazl Razi"
    ],
    "published": "2025-05-14T02:02:32+00:00",
    "summary": "Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and $\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at https://github.com/xiwenc1/DRA-GRPO."
  },
  {
    "title": "Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control",
    "url": "http://arxiv.org/abs/2505.09029v1",
    "arxiv_id": "2505.09029v1",
    "authors": [
      "Hazim Alzorgan",
      "Abolfazl Razi"
    ],
    "published": "2025-05-13T23:56:12+00:00",
    "summary": "Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient (TD3), depend on basic noise-based exploration, which can result in less than optimal policy convergence. In this study, we introduce Monte Carlo Beam Search (MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts with TD3 to improve exploration and action selection. MCBS produces several candidate actions around the policy's output and assesses them through short-horizon rollouts, enabling the agent to make better-informed choices. We test MCBS across various continuous-control benchmarks, including HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency and performance compared to standard TD3 and other baseline methods like SAC, PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy learning through structured look-ahead search while ensuring computational efficiency. Additionally, we offer a detailed analysis of crucial hyperparameters, such as beam width and rollout depth, and explore adaptive strategies to optimize MCBS for complex control tasks. Our method shows a higher convergence rate across different environments compared to TD3, SAC, PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward within around 200 thousand timesteps compared to 400 thousand timesteps for the second-best method."
  },
  {
    "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind",
    "url": "http://arxiv.org/abs/2505.09024v1",
    "arxiv_id": "2505.09024v1",
    "authors": [
      "Aaron Baughman",
      "Rahul Agarwal",
      "Eduardo Morales",
      "Gozde Akay"
    ],
    "published": "2025-05-13T23:42:36+00:00",
    "summary": "We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment."
  },
  {
    "title": "Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation",
    "url": "http://arxiv.org/abs/2505.09012v1",
    "arxiv_id": "2505.09012v1",
    "authors": [
      "Bo Meng",
      "Chenghao Xu",
      "Yongli Zhu"
    ],
    "published": "2025-05-13T23:01:34+00:00",
    "summary": "Cascading failures in power grids can lead to grid collapse, causing severe disruptions to social operations and economic activities. In certain cases, multi-stage cascading failures can occur. However, existing cascading-failure-mitigation strategies are usually single-stage-based, overlooking the complexity of the multi-stage scenario. This paper treats the multi-stage cascading failure problem as a reinforcement learning task and develops a simulation environment. The reinforcement learning agent is then trained via the deterministic policy gradient algorithm to achieve continuous actions. Finally, the effectiveness of the proposed approach is validated on the IEEE 14-bus and IEEE 118-bus systems."
  },
  {
    "title": "Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition",
    "url": "http://arxiv.org/abs/2505.09003v1",
    "arxiv_id": "2505.09003v1",
    "authors": [
      "Zeki Doruk Erden",
      "Donia Gasmi",
      "Boi Faltings"
    ],
    "published": "2025-05-13T22:38:54+00:00",
    "summary": "Continual learning for reinforcement learning agents remains a significant challenge, particularly in preserving and leveraging existing information without an external signal to indicate changes in tasks or environments. In this study, we explore the effectiveness of autoencoders in detecting new tasks and matching observed environments to previously encountered ones. Our approach integrates policy optimization with familiarity autoencoders within an end-to-end continual learning system. This system can recognize and learn new tasks or environments while preserving knowledge from earlier experiences and can selectively retrieve relevant knowledge when re-encountering a known environment. Initial results demonstrate successful continual learning without external signals to indicate task changes or reencounters, showing promise for this methodology."
  },
  {
    "title": "Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08995v1",
    "arxiv_id": "2505.08995v1",
    "authors": [
      "Ardian Selmonaj",
      "Oleg Szehr",
      "Giacomo Del Rio",
      "Alessandro Antonucci",
      "Adrian Schneider",
      "Michael R\u00fcegsegger"
    ],
    "published": "2025-05-13T22:13:48+00:00",
    "summary": "This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework."
  },
  {
    "title": "Generalization in Monitored Markov Decision Processes (Mon-MDPs)",
    "url": "http://arxiv.org/abs/2505.08988v1",
    "arxiv_id": "2505.08988v1",
    "authors": [
      "Montaser Mohammedalamen",
      "Michael Bowling"
    ],
    "published": "2025-05-13T21:58:25+00:00",
    "summary": "Reinforcement learning (RL) typically models the interaction between the agent and environment as a Markov decision process (MDP), where the rewards that guide the agent's behavior are always observable. However, in many real-world scenarios, rewards are not always observable, which can be modeled as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have been limited to simple, tabular cases, restricting their applicability to real-world problems. This work explores Mon-MDPs using function approximation (FA) and investigates the challenges involved. We show that combining function approximation with a learned reward model enables agents to generalize from monitored states with observable rewards, to unmonitored environment states with unobservable rewards. Therefore, we demonstrate that such generalization with a reward model achieves near-optimal policies in environments formally defined as unsolvable. However, we identify a critical limitation of such function approximation, where agents incorrectly extrapolate rewards due to overgeneralization, resulting in undesirable behaviors. To mitigate overgeneralization, we propose a cautious police optimization method leveraging reward uncertainty. This work serves as a step towards bridging this gap between Mon-MDP theory and real-world applications."
  },
  {
    "title": "Adaptive Entanglement Generation for Quantum Routing",
    "url": "http://arxiv.org/abs/2505.08958v1",
    "arxiv_id": "2505.08958v1",
    "authors": [
      "Tasdiqul Islam",
      "Md Arifuzzaman",
      "Engin Arslan"
    ],
    "published": "2025-05-13T20:51:59+00:00",
    "summary": "Entanglement generation in long-distance quantum networks is a difficult process due to resource limitations and the probabilistic nature of entanglement swapping. To maximize success probability, existing quantum routing algorithms employ computationally expensive solutions (e.g., linear programming) to determine which links to entangle and use for end-to-end entanglement generation. Such optimization methods, however, cannot meet the delay requirements of real-world quantum networks, necessitating swift yet efficient real-time optimization models. In this paper, we propose reinforcement learning (RL)-based models to determine which links to entangle and proactively swap to meet connection requests. We show that the proposed RL-based approach is 20x faster compared to linear programming. Moreover, we show that one can take advantage of the longevity of entanglements to (i) cache entangled links for future use and (ii) proactively swap entanglement on high-demand path segments, thereby increasing the likelihood of request success. Through comprehensive simulations, we demonstrate that caching unused entanglements leads to a 10-15% improvement in the performance of state-of-the-art quantum routing algorithms. Complementing caching with proactive entanglement swapping further enhances the request success rate by up to 52.55%."
  },
  {
    "title": "Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections",
    "url": "http://arxiv.org/abs/2505.08896v1",
    "arxiv_id": "2505.08896v1",
    "authors": [
      "Pankaj Kumar",
      "Aditya Mishra",
      "Pranamesh Chakraborty",
      "Subrahmanya Swamy Peruru"
    ],
    "published": "2025-05-13T18:38:42+00:00",
    "summary": "Developing an autonomous vehicle control strategy for signalised intersections (SI) is one of the challenging tasks due to its inherently complex decision-making process. This study proposes a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy at SI. A comprehensive reward function has been formulated with a particular focus on (i) distance headway-based efficiency reward, (ii) decision-making criteria during amber light, and (iii) asymmetric acceleration/ deceleration response, along with the traditional safety and comfort criteria. This reward function has been incorporated with two popular DRL algorithms, Deep Deterministic Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the continuous action space of acceleration/deceleration. The proposed models have been trained on the combination of real-world leader vehicle (LV) trajectories and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process. The overall performance of the proposed models has been tested using Cumulative Distribution Function (CDF) plots and compared with the real-world trajectory data. The results show that the RL models successfully maintain lower distance headway (i.e., higher efficiency) and jerk compared to human-driven vehicles without compromising safety. Further, to assess the robustness of the proposed models, we evaluated the model performance on diverse safety-critical scenarios, in terms of car-following and traffic signal compliance. Both DDPG and SAC models successfully handled the critical scenarios, while the DDPG model showed smoother action profiles compared to the SAC model. Overall, the results confirm that DRL-based longitudinal vehicle control strategy at SI can help to improve traffic safety, efficiency, and comfort."
  },
  {
    "title": "Generative Molecular Design with Steerable and Granular Synthesizability Control",
    "url": "http://arxiv.org/abs/2505.08774v1",
    "arxiv_id": "2505.08774v1",
    "authors": [
      "Jeff Guo",
      "V\u00edctor Sabanza-Gil",
      "Zlatko Jon\u010dev",
      "Jeremy S. Luterbacher",
      "Philippe Schwaller"
    ],
    "published": "2025-05-13T17:53:54+00:00",
    "summary": "Synthesizability in small molecule generative design remains a bottleneck. Existing works that do consider synthesizability can output predicted synthesis routes for generated molecules. However, there has been minimal attention in addressing the ease of synthesis and enabling flexibility to incorporate desired reaction constraints. In this work, we propose a small molecule generative design framework that enables steerable and granular synthesizability control. Generated molecules satisfy arbitrary multi-parameter optimization objectives with predicted synthesis routes containing pre-defined allowed reactions, while optionally avoiding others. One can also enforce that all reactions belong to a pre-defined set. We show the capability to mix-and-match these reaction constraints across the most common medicinal chemistry transformations. Next, we show how our framework can be used to valorize industrial byproducts towards de novo optimized molecules. Going further, we demonstrate how granular control over synthesizability constraints can loosely mimic virtual screening of ultra-large make-on-demand libraries. Using only a single GPU, we generate and dock 15k molecules to identify promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules (assessing only 0.00001% of the library). Generated molecules satisfying the reaction constraints have > 90% exact match rate. Lastly, we benchmark our framework against recent synthesizability-constrained generative models and demonstrate the highest sample efficiency even when imposing the additional constraint that all molecules must be synthesizable from a single reaction type. The main theme is demonstrating that a pre-trained generalist molecular generative model can be incentivized to generate property-optimized small molecules under challenging synthesizability constraints through reinforcement learning."
  },
  {
    "title": "Preference Optimization for Combinatorial Optimization Problems",
    "url": "http://arxiv.org/abs/2505.08735v1",
    "arxiv_id": "2505.08735v1",
    "authors": [
      "Mingjun Pan",
      "Guanquan Lin",
      "You-Wei Luo",
      "Bin Zhu",
      "Zhien Dai",
      "Lijun Sun",
      "Chun Yuan"
    ],
    "published": "2025-05-13T16:47:00+00:00",
    "summary": "Reinforcement Learning (RL) has emerged as a powerful tool for neural combinatorial optimization, enabling models to learn heuristics that solve complex problems without requiring expert knowledge. Despite significant progress, existing RL approaches face challenges such as diminishing reward signals and inefficient exploration in vast combinatorial action spaces, leading to inefficiency. In this paper, we propose Preference Optimization, a novel method that transforms quantitative reward signals into qualitative preference signals via statistical comparison modeling, emphasizing the superiority among sampled solutions. Methodologically, by reparameterizing the reward function in terms of policy and utilizing preference models, we formulate an entropy-regularized RL objective that aligns the policy directly with preferences while avoiding intractable computations. Furthermore, we integrate local search techniques into the fine-tuning rather than post-processing to generate high-quality preference pairs, helping the policy escape local optima. Empirical results on various benchmarks, such as the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method significantly outperforms existing RL algorithms, achieving superior convergence efficiency and solution quality."
  },
  {
    "title": "Improved Algorithms for Differentially Private Language Model Alignment",
    "url": "http://arxiv.org/abs/2505.08849v1",
    "arxiv_id": "2505.08849v1",
    "authors": [
      "Keyu Chen",
      "Hao Tang",
      "Qinglin Liu",
      "Yizhao Xu"
    ],
    "published": "2025-05-13T16:18:59+00:00",
    "summary": "Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs."
  },
  {
    "title": "A Study of Data-driven Methods for Inventory Optimization",
    "url": "http://arxiv.org/abs/2505.08673v1",
    "arxiv_id": "2505.08673v1",
    "authors": [
      "Lee Yeung Ping",
      "Patrick Wong",
      "Tan Cheng Han"
    ],
    "published": "2025-05-13T15:35:23+00:00",
    "summary": "This paper shows a comprehensive analysis of three algorithms (Time Series, Random Forest (RF) and Deep Reinforcement Learning) into three inventory models (the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These methodologies are applied in the supermarket context. The main purpose is to analyse efficient methods for the data-driven. Their possibility, potential and current challenges are taken into consideration in this report. By comparing the results in each model, the effectiveness of each algorithm is evaluated based on several key performance indicators, including forecast accuracy, adaptability to market changes, and overall impact on inventory costs and customer satisfaction levels. The data visualization tools and statistical metrics are the indicators for the comparisons and show some obvious trends and patterns that can guide decision-making in inventory management. These tools enable managers to not only track the performance of different algorithms in real-time but also to drill down into specific data points to understand the underlying causes of inventory fluctuations. This level of detail is crucial for pinpointing inefficiencies and areas for improvement within the supply chain."
  },
  {
    "title": "Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08630v1",
    "arxiv_id": "2505.08630v1",
    "authors": [
      "Shuai Han",
      "Mehdi Dastani",
      "Shihan Wang"
    ],
    "published": "2025-05-13T14:49:26+00:00",
    "summary": "Training cooperative agents in sparse-reward scenarios poses significant challenges for multi-agent reinforcement learning (MARL). Without clear feedback on actions at each step in sparse-reward setting, previous methods struggle with precise credit assignment among agents and effective exploration. In this paper, we introduce a novel method to deal with both credit assignment and exploration problems in reward-sparse domains. Accordingly, we propose an algorithm that calculates the Influence Scope of Agents (ISA) on states by taking specific value of the dimensions/attributes of states that can be influenced by individual agents. The mutual dependence between agents' actions and state attributes are then used to calculate the credit assignment and to delimit the exploration space for each individual agent. We then evaluate ISA in a variety of sparse-reward multi-agent scenarios. The results show that our method significantly outperforms the state-of-art baselines."
  },
  {
    "title": "Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations",
    "url": "http://arxiv.org/abs/2505.08619v1",
    "arxiv_id": "2505.08619v1",
    "authors": [
      "Sarmad Mehrdad",
      "Avadesh Meduri",
      "Ludovic Righetti"
    ],
    "published": "2025-05-13T14:38:25+00:00",
    "summary": "We present an iterative inverse reinforcement learning algorithm to infer optimal cost functions in continuous spaces. Based on a popular maximum entropy criteria, our approach iteratively finds a weight improvement step and proposes a method to find an appropriate step size that ensures learned cost function features remain similar to the demonstrated trajectory features. In contrast to similar approaches, our algorithm can individually tune the effectiveness of each observation for the partition function and does not need a large sample set, enabling faster learning. We generate sample trajectories by solving an optimal control problem instead of random sampling, leading to more informative trajectories. The performance of our method is compared to two state of the art algorithms to demonstrate its benefits in several simulated environments."
  },
  {
    "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08617v1",
    "arxiv_id": "2505.08617v1",
    "authors": [
      "Zhaochen Su",
      "Linjie Li",
      "Mingyang Song",
      "Yunzhuo Hao",
      "Zhengyuan Yang",
      "Jun Zhang",
      "Guanjie Chen",
      "Jiawei Gu",
      "Juntao Li",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "published": "2025-05-13T14:35:51+00:00",
    "summary": "While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely \"think with images\"."
  },
  {
    "title": "Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection",
    "url": "http://arxiv.org/abs/2505.08561v1",
    "arxiv_id": "2505.08561v1",
    "authors": [
      "Ayush K. Rai",
      "Kyle Min",
      "Tarun Krishna",
      "Feiyan Hu",
      "Alan F. Smeaton",
      "Noel E. O'Connor"
    ],
    "published": "2025-05-13T13:35:41+00:00",
    "summary": "Masked video modeling~(MVM) has emerged as a highly effective pre-training strategy for visual foundation models, whereby the model reconstructs masked spatiotemporal tokens using information from visible tokens. However, a key challenge in such approaches lies in selecting an appropriate masking strategy. Previous studies have explored predefined masking techniques, including random and tube-based masking, as well as approaches that leverage key motion priors, optical flow and semantic cues from externally pre-trained models. In this work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token Sampler (TATS), which models the motion dynamics of tokens and can be seamlessly integrated into the masked autoencoder (MAE) framework to select motion-centric tokens in videos. Additionally, we propose a unified training strategy that enables joint optimization of both MAE and TATS from scratch using Proximal Policy Optimization (PPO). We show that our model allows for aggressive masking without compromising performance on the downstream task of action recognition while also ensuring that the pre-training remains memory efficient. Extensive experiments of the proposed approach across four benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51, demonstrate the effectiveness, transferability, generalization, and efficiency of our work compared to other state-of-the-art methods."
  },
  {
    "title": "Strategy-Augmented Planning for Large Language Models via Opponent Exploitation",
    "url": "http://arxiv.org/abs/2505.08459v1",
    "arxiv_id": "2505.08459v1",
    "authors": [
      "Shuai Xu",
      "Sijia Cui",
      "Yanna Wang",
      "Bo Xu",
      "Qi Wang"
    ],
    "published": "2025-05-13T11:41:10+00:00",
    "summary": "Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a 85.35\\% performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI."
  },
  {
    "title": "Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting",
    "url": "http://arxiv.org/abs/2505.08458v1",
    "arxiv_id": "2505.08458v1",
    "authors": [
      "Emlyn Williams",
      "Athanasios Polydoros"
    ],
    "published": "2025-05-13T11:40:21+00:00",
    "summary": "This paper presents a comprehensive sim-to-real pipeline for autonomous strawberry picking from dense clusters using a Franka Panda robot. Our approach leverages a custom Mujoco simulation environment that integrates domain randomization techniques. In this environment, a deep reinforcement learning agent is trained using the dormant ratio minimization algorithm. The proposed pipeline bridges low-level control with high-level perception and decision making, demonstrating promising performance in both simulation and in a real laboratory environment, laying the groundwork for successful transfer to real-world autonomous fruit harvesting."
  },
  {
    "title": "Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges",
    "url": "http://arxiv.org/abs/2505.08453v1",
    "arxiv_id": "2505.08453v1",
    "authors": [
      "Miguel Arana-Catania",
      "Weisi Guo"
    ],
    "published": "2025-05-13T11:30:51+00:00",
    "summary": "Causal understanding is important in many disciplines of science and engineering, where we seek to understand how different factors in the system causally affect an experiment or situation and pave a pathway towards creating effective or optimising existing models. Examples of use cases are autonomous exploration and modelling of unknown environments or assessing key variables in optimising large complex systems. In this paper, we analyse a Reinforcement Learning approach called Causal Curiosity, which aims to estimate as accurately and efficiently as possible, without directly measuring them, the value of factors that causally determine the dynamics of a system. Whilst the idea presents a pathway forward, measurement accuracy is the foundation of methodology effectiveness. Focusing on the current causal curiosity's robotic manipulator, we present for the first time a measurement accuracy analysis of the future potentials and current limitations of this technique and an analysis of its sensitivity and confounding factor disentanglement capability - crucial for causal analysis. As a result of our work, we promote proposals for an improved and efficient design of Causal Curiosity methods to be applied to real-world complex scenarios."
  },
  {
    "title": "Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models",
    "url": "http://arxiv.org/abs/2505.08448v1",
    "arxiv_id": "2505.08448v1",
    "authors": [
      "Yanggang Xu",
      "Weijie Hong",
      "Jirong Zha",
      "Geng Chen",
      "Jianfeng Zheng",
      "Chen-Chun Hsia",
      "Xinlei Chen"
    ],
    "published": "2025-05-13T11:23:25+00:00",
    "summary": "In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality."
  },
  {
    "title": "Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08382v1",
    "arxiv_id": "2505.08382v1",
    "authors": [
      "Mirco Theile",
      "Andres R. Zapata Rodriguez",
      "Marco Caccamo",
      "Alberto L. Sangiovanni-Vincentelli"
    ],
    "published": "2025-05-13T09:29:16+00:00",
    "summary": "Unmanned Aerial Vehicle (UAV) Coverage Path Planning (CPP) is critical for applications such as precision agriculture and search and rescue. While traditional methods rely on discrete grid-based representations, real-world UAV operations require power-efficient continuous motion planning. We formulate the UAV CPP problem in a continuous environment, minimizing power consumption while ensuring complete coverage. Our approach models the environment with variable-size axis-aligned rectangles and UAV motion with curvature-constrained B\\'ezier curves. We train a reinforcement learning agent using an action-mapping-based Soft Actor-Critic (AM-SAC) algorithm employing a self-adaptive curriculum. Experiments on both procedurally generated and hand-crafted scenarios demonstrate the effectiveness of our method in learning energy-efficient coverage strategies."
  },
  {
    "title": "Adaptive Diffusion Policy Optimization for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.08376v1",
    "arxiv_id": "2505.08376v1",
    "authors": [
      "Huiyun Jiang",
      "Zhuang Yang"
    ],
    "published": "2025-05-13T09:21:45+00:00",
    "summary": "Recent studies have shown the great potential of diffusion models in improving reinforcement learning (RL) by modeling complex policies, expressing a high degree of multi-modality, and efficiently handling high-dimensional continuous control tasks. However, there is currently limited research on how to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably. In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a fast algorithmic framework containing best practices for fine-tuning diffusion-based polices in robotic control tasks using the adaptive gradient descent method in RL. Adaptive gradient method is less studied in training RL, let alone diffusion-based policies. We confirm that ADPO outperforms other diffusion-based RL methods in terms of overall effectiveness for fine-tuning on standard robotic tasks. Concretely, we conduct extensive experiments on standard robotic control tasks to test ADPO, where, particularly, six popular diffusion-based RL methods are provided as benchmark methods. Experimental results show that ADPO acquires better or comparable performance than the baseline methods. Finally, we systematically analyze the sensitivity of multiple hyperparameters in standard robotics tasks, providing guidance for subsequent practical applications. Our video demonstrations are released in https://github.com/Timeless-lab/ADPO.git."
  },
  {
    "title": "Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation",
    "url": "http://arxiv.org/abs/2505.08364v1",
    "arxiv_id": "2505.08364v1",
    "authors": [
      "Enci Zhang",
      "Xingang Yan",
      "Wei Lin",
      "Tianxiang Zhang",
      "Qianchun Lu"
    ],
    "published": "2025-05-13T09:10:48+00:00",
    "summary": "Despite impressive progress in areas like mathematical reasoning, large language models still face significant challenges in consistently solving complex problems. Drawing inspiration from key human learning strategies, we propose two novel strategies to enhance the capability of large language models to solve these complex problems. First, Adaptive Difficulty Curriculum Learning (ADCL) is a novel curriculum learning strategy that tackles the Difficulty Shift phenomenon (i.e., a model's perception of problem difficulty dynamically changes during training) by periodically re-estimating difficulty within upcoming data batches to maintain alignment with the model's evolving capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel reinforcement learning strategy that bridges the gap between imitation learning and pure exploration by guiding models to reformulate expert solutions within their own conceptual framework, rather than relying on direct imitation, fostering deeper understanding and knowledge assimilation. Extensive experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B as the base model, demonstrate that these human-inspired strategies synergistically and significantly enhance performance. Notably, their combined application improves performance over the standard Zero-RL baseline by 10% on the AIME24 benchmark and 16.6% on AIME25."
  },
  {
    "title": "Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08361v1",
    "arxiv_id": "2505.08361v1",
    "authors": [
      "Xinyue Wang",
      "Biwei Huang"
    ],
    "published": "2025-05-13T09:08:28+00:00",
    "summary": "Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning -- where known components are reconfigured to handle new situations -- we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks."
  },
  {
    "title": "An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning",
    "url": "http://arxiv.org/abs/2505.08343v1",
    "arxiv_id": "2505.08343v1",
    "authors": [
      "Ruichu Cai",
      "Xi Chen",
      "Jie Qiao",
      "Zijian Li",
      "Yuequn Liu",
      "Wei Chen",
      "Keli Zhang",
      "Jiale Zheng"
    ],
    "published": "2025-05-13T08:41:45+00:00",
    "summary": "Decision making under abnormal conditions is a critical process that involves evaluating the current state and determining the optimal action to restore the system to a normal state at an acceptable cost. However, in such scenarios, existing decision-making frameworks highly rely on reinforcement learning or root cause analysis, resulting in them frequently neglecting the cost of the actions or failing to incorporate causal mechanisms adequately. By relaxing the existing causal decision framework to solve the necessary cause, we propose a minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to address the above challenges. Emphasis is placed on making counterfactual reasoning processes identifiable in the presence of a large amount of mixed anomaly data, as well as finding the optimal intervention state in a continuous decision space. Specifically, it formulates a surrogate model based on causal graphs, using abnormal pattern clustering labels as supervisory signals. This enables the approximation of the structural causal model among the variables and lays a foundation for identifiable counterfactual reasoning. With the causal structure approximated, we then established an optimization model based on counterfactual estimation. The Sequential Least Squares Programming (SLSQP) algorithm is further employed to optimize intervention strategies while taking costs into account. Experimental evaluations on both synthetic and real-world datasets reveal that MiCCD outperforms conventional methods across multiple metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k values), thus validating its efficacy and broad applicability."
  },
  {
    "title": "AI-Driven Digital Twins: Optimizing 5G/6G Network Slicing with NTNs",
    "url": "http://arxiv.org/abs/2505.08328v1",
    "arxiv_id": "2505.08328v1",
    "authors": [
      "Afan Ali",
      "Huseyin Arslan"
    ],
    "published": "2025-05-13T08:09:13+00:00",
    "summary": "Network slicing in 5G/6G Non-Terrestrial Network (NTN) is confronted with mobility and traffic variability. An artificial intelligence (AI)-based digital twin (DT) architecture with deep reinforcement learning (DRL) using Deep deterministic policy gradient (DDPG) is proposed for dynamic optimization of resource allocation. DT virtualizes network states to enable predictive analysis, while DRL changes bandwidth for eMBB slice. Simulations show a 25\\% latency reduction compared to static methods, with enhanced resource utilization. This scalable solution supports 5G/6G NTN applications like disaster recovery and urban blockage."
  },
  {
    "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
    "url": "http://arxiv.org/abs/2505.08311v1",
    "arxiv_id": "2505.08311v1",
    "authors": [
      "Yunjie Ji",
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-05-13T07:41:15+00:00",
    "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."
  },
  {
    "title": "A Practical Introduction to Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08295v1",
    "arxiv_id": "2505.08295v1",
    "authors": [
      "Yinghan Sun",
      "Hongxi Wang",
      "Hua Chen",
      "Wei Zhang"
    ],
    "published": "2025-05-13T07:19:16+00:00",
    "summary": "Deep reinforcement learning (DRL) has emerged as a powerful framework for solving sequential decision-making problems, achieving remarkable success in a wide range of applications, including game AI, autonomous driving, biomedicine, and large language models. However, the diversity of algorithms and the complexity of theoretical foundations often pose significant challenges for beginners seeking to enter the field. This tutorial aims to provide a concise, intuitive, and practical introduction to DRL, with a particular focus on the Proximal Policy Optimization (PPO) algorithm, which is one of the most widely used and effective DRL methods. To facilitate learning, we organize all algorithms under the Generalized Policy Iteration (GPI) framework, offering readers a unified and systematic perspective. Instead of lengthy theoretical proofs, we emphasize intuitive explanations, illustrative examples, and practical engineering techniques. This work serves as an efficient and accessible guide, helping readers rapidly progress from basic concepts to the implementation of advanced DRL algorithms."
  },
  {
    "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08837v1",
    "arxiv_id": "2505.08837v1",
    "authors": [
      "Muhammad Saqib",
      "Dipkumar Mehta",
      "Fnu Yashu",
      "Shubham Malhotra"
    ],
    "published": "2025-05-13T06:34:54+00:00",
    "summary": "The security of cloud environments, such as Amazon Web Services (AWS), is complex and dynamic. Static security policies have become inadequate as threats evolve and cloud resources exhibit elasticity [1]. This paper addresses the limitations of static policies by proposing a security policy management framework that uses reinforcement learning (RL) to adapt dynamically. Specifically, we employ deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, enabling the learning and continuous adjustment of controls such as firewall rules and Identity and Access Management (IAM) policies. The proposed RL based solution leverages cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies, maximizing threat mitigation, and compliance while minimizing resource impact. Experimental results demonstrate that our adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and substantially reducing incident detection and response times by 58%. In addition, it maintains high conformity with security requirements and efficient resource usage. These findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management."
  },
  {
    "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.08264v1",
    "arxiv_id": "2505.08264v1",
    "authors": [
      "Ahmed Abouelazm",
      "Tim Weinstein",
      "Tim Joseph",
      "Philip Sch\u00f6rner",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-05-13T06:26:57+00:00",
    "summary": "This paper addresses the challenges of training end-to-end autonomous driving agents using Reinforcement Learning (RL). RL agents are typically trained in a fixed set of scenarios and nominal behavior of surrounding road users in simulations, limiting their generalization and real-life deployment. While domain randomization offers a potential solution by randomly sampling driving scenarios, it frequently results in inefficient training and sub-optimal policies due to the high variance among training scenarios. To address these limitations, we propose an automatic curriculum learning framework that dynamically generates driving scenarios with adaptive complexity based on the agent's evolving capabilities. Unlike manually designed curricula that introduce expert bias and lack scalability, our framework incorporates a ``teacher'' that automatically generates and mutates driving scenarios based on their learning potential -- an agent-centric metric derived from the agent's current policy -- eliminating the need for expert design. The framework enhances training efficiency by excluding scenarios the agent has mastered or finds too challenging. We evaluate our framework in a reinforcement learning setting where the agent learns a driving policy from camera images. Comparative results against baseline methods, including fixed scenario training and domain randomization, demonstrate that our approach leads to enhanced generalization, achieving higher success rates: +9\\% in low traffic density, +21\\% in high traffic density, and faster convergence with fewer training steps. Our findings highlight the potential of ACL in improving the robustness and efficiency of RL-based autonomous driving agents."
  },
  {
    "title": "Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning",
    "url": "http://arxiv.org/abs/2505.08238v1",
    "arxiv_id": "2505.08238v1",
    "authors": [
      "Yunyue Wei",
      "Shanning Zhuang",
      "Vincent Zhuang",
      "Yanan Sui"
    ],
    "published": "2025-05-13T05:31:32+00:00",
    "summary": "Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming, and therefore not suitable for solving large collections of tasks that require significant manual tuning. In this work, we introduce Model Predictive Control with Morphology-aware Proportional Control (MPC^2), a hierarchical model-based learning algorithm for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC^2 uses a sampling-based model predictive controller for target posture planning, and enables robust control for high-dimensional tasks by incorporating a morphology-aware proportional controller for actuator coordination. The algorithm enables motion control of a high-dimensional human musculoskeletal model in a variety of motion tasks, such as standing, walking on different terrains, and imitating sports activities. The reward function of MPC^2 can be tuned via black-box optimization, drastically reducing the need for human-intensive reward engineering."
  },
  {
    "title": "Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation",
    "url": "http://arxiv.org/abs/2505.08223v1",
    "arxiv_id": "2505.08223v1",
    "authors": [
      "Dohyun Kim",
      "Jayden Dongwoo Lee",
      "Hyochoong Bang",
      "Jungho Bae"
    ],
    "published": "2025-05-13T04:50:29+00:00",
    "summary": "Multirotors play a significant role in diverse field robotics applications but remain highly susceptible to actuator failures, leading to rapid instability and compromised mission reliability. While various fault-tolerant control (FTC) strategies using reinforcement learning (RL) have been widely explored, most previous approaches require prior knowledge of the multirotor model or struggle to adapt to new configurations. To address these limitations, we propose a novel hybrid RL-based FTC framework integrated with a transformer-based online adaptation module. Our framework leverages a transformer architecture to infer latent representations in real time, enabling adaptation to previously unseen system models without retraining. We evaluate our method in a PyBullet simulation under loss-of-effectiveness actuator faults, achieving a 95% success rate and a positional root mean square error (RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success and an RMSE of 0.153 m. Further evaluations on quadrotors with varying configurations confirm the robustness of our framework across untrained dynamics. These results demonstrate the potential of our framework to enhance the adaptability and reliability of multirotors, enabling efficient fault management in dynamic and uncertain environments. Website is available at http://00dhkim.me/paper/rl-ftc"
  },
  {
    "title": "Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2505.08222v1",
    "arxiv_id": "2505.08222v1",
    "authors": [
      "Matteo Gallici",
      "Ivan Masmitja",
      "Mario Mart\u00edn"
    ],
    "published": "2025-05-13T04:42:30+00:00",
    "summary": "Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions."
  },
  {
    "title": "DSADF: Thinking Fast and Slow for Decision Making",
    "url": "http://arxiv.org/abs/2505.08189v1",
    "arxiv_id": "2505.08189v1",
    "authors": [
      "Alex Zhihao Dou",
      "Dongfei Cui",
      "Jun Yan",
      "Weida Wang",
      "Benteng Chen",
      "Haoming Wang",
      "Zeke Xie",
      "Shufei Zhang"
    ],
    "published": "2025-05-13T02:58:04+00:00",
    "summary": "Although Reinforcement Learning (RL) agents are effective in well-defined environments, they often struggle to generalize their learned policies to dynamic settings due to their reliance on trial-and-error interactions. Recent work has explored applying Large Language Models (LLMs) or Vision Language Models (VLMs) to boost the generalization of RL agents through policy optimization guidance or prior knowledge. However, these approaches often lack seamless coordination between the RL agent and the foundation model, leading to unreasonable decision-making in unfamiliar environments and efficiency bottlenecks. Making full use of the inferential capabilities of foundation models and the rapid response capabilities of RL agents and enhancing the interaction between the two to form a dual system is still a lingering scientific question. To address this problem, we draw inspiration from Kahneman's theory of fast thinking (System 1) and slow thinking (System 2), demonstrating that balancing intuition and deep reasoning can achieve nimble decision-making in a complex world. In this study, we propose a Dual-System Adaptive Decision Framework (DSADF), integrating two complementary modules: System 1, comprising an RL agent and a memory space for fast and intuitive decision making, and System 2, driven by a VLM for deep and analytical reasoning. DSADF facilitates efficient and adaptive decision-making by combining the strengths of both systems. The empirical study in the video game environment: Crafter and Housekeep demonstrates the effectiveness of our proposed method, showing significant improvements in decision abilities for both unseen and known tasks."
  },
  {
    "title": "Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL",
    "url": "http://arxiv.org/abs/2505.08179v1",
    "arxiv_id": "2505.08179v1",
    "authors": [
      "Zhikun Tao",
      "Gang Xiong",
      "He Fang",
      "Zhen Shen",
      "Yunjun Han",
      "Qing-Shan Jia"
    ],
    "published": "2025-05-13T02:32:49+00:00",
    "summary": "Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety."
  },
  {
    "title": "High-order Regularization for Machine Learning and Learning-based Control",
    "url": "http://arxiv.org/abs/2505.08129v1",
    "arxiv_id": "2505.08129v1",
    "authors": [
      "Xinghua Liu",
      "Ming Cao"
    ],
    "published": "2025-05-13T00:00:23+00:00",
    "summary": "The paper proposes a novel regularization procedure for machine learning. The proposed high-order regularization (HR) provides new insight into regularization, which is widely used to train a neural network that can be utilized to approximate the action-value function in general reinforcement learning problems. The proposed HR method ensures the provable convergence of the approximation algorithm, which makes the much-needed connection between regularization and explainable learning using neural networks. The proposed HR method theoretically demonstrates that regularization can be regarded as an approximation in terms of inverse mapping with explicitly calculable approximation error, and the $L_2$ regularization is a lower-order case of the proposed method. We provide lower and upper bounds for the error of the proposed HR solution, which helps build a reliable model. We also find that regularization with the proposed HR can be regarded as a contraction. We prove that the generalizability of neural networks can be maximized with a proper regularization matrix, and the proposed HR is applicable for neural networks with any mapping matrix. With the theoretical explanation of the extreme learning machine for neural network training and the proposed high-order regularization, one can better interpret the output of the neural network, thus leading to explainable learning. We present a case study based on regularized extreme learning neural networks to demonstrate the application of the proposed HR and give the corresponding incremental HR solution. We verify the performance of the proposed HR method by solving a classic control problem in reinforcement learning. The result demonstrates the superior performance of the method with significant enhancement in the generalizability of the neural network."
  },
  {
    "title": "Leveraging Reinforcement Learning and Koopman Theory for Enhanced Model Predictive Control Performance",
    "url": "http://arxiv.org/abs/2505.08122v1",
    "arxiv_id": "2505.08122v1",
    "authors": [
      "Md Nur-A-Adam Dony",
      "Jing Yang"
    ],
    "published": "2025-05-12T23:25:53+00:00",
    "summary": "This study presents an innovative approach to Model Predictive Control (MPC) by leveraging the powerful combination of Koopman theory and Deep Reinforcement Learning (DRL). By transforming nonlinear dynamical systems into a higher-dimensional linear regime, the Koopman operator facilitates the linear treatment of nonlinear behaviors, paving the way for more efficient control strategies. Our methodology harnesses the predictive prowess of Koopman-based models alongside the optimization capabilities of DRL, particularly using the Proximal Policy Optimization (PPO) algorithm, to enhance the controller's performance. The resulting end-to-end learning framework refines the predictive control policies to cater to specific operational tasks, optimizing both performance and economic efficiency. We validate our approach through rigorous NMPC and eNMPC case studies, demonstrating that the Koopman-RL controller outperforms traditional controllers by achieving higher stability, superior constraint satisfaction, and significant cost savings. The findings indicate that our model can be a robust tool for complex control tasks, offering valuable insights into future applications of RL in MPC."
  },
  {
    "title": "What Matters for Batch Online Reinforcement Learning in Robotics?",
    "url": "http://arxiv.org/abs/2505.08078v1",
    "arxiv_id": "2505.08078v1",
    "authors": [
      "Perry Dong",
      "Suvir Mirchandani",
      "Dorsa Sadigh",
      "Chelsea Finn"
    ],
    "published": "2025-05-12T21:24:22+00:00",
    "summary": "The ability to learn from large batches of autonomously collected data for policy improvement -- a paradigm we refer to as batch online reinforcement learning -- holds the promise of enabling truly scalable robot learning by significantly reducing the need for human effort of data collection while getting benefits from self-improvement. Yet, despite the promise of this paradigm, it remains challenging to achieve due to algorithms not being able to learn effectively from the autonomous data. For example, prior works have applied imitation learning and filtered imitation learning methods to the batch online RL problem, but these algorithms often fail to efficiently improve from the autonomously collected data or converge quickly to a suboptimal point. This raises the question of what matters for effective batch online RL in robotics. Motivated by this question, we perform a systematic empirical study of three axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy expressivity -- and analyze how these axes affect performance and scaling with the amount of autonomous data. Through our analysis, we make several observations. First, we observe that the use of Q-functions to guide batch online RL significantly improves performance over imitation-based methods. Building on this, we show that an implicit method of policy extraction -- via choosing the best action in the distribution of the policy -- is necessary over traditional policy extraction methods from offline RL. Next, we show that an expressive policy class is preferred over less expressive policy classes. Based on this analysis, we propose a general recipe for effective batch online RL. We then show a simple addition to the recipe of using temporally-correlated noise to obtain more diversity results in further performance gains. Our recipe obtains significantly better performance and scaling compared to prior methods."
  },
  {
    "title": "Explainable Reinforcement Learning Agents Using World Models",
    "url": "http://arxiv.org/abs/2505.08073v1",
    "arxiv_id": "2505.08073v1",
    "authors": [
      "Madhuri Singh",
      "Amal Alabdulkarim",
      "Gennie Mansi",
      "Mark O. Riedl"
    ],
    "published": "2025-05-12T21:18:31+00:00",
    "summary": "Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment."
  },
  {
    "title": "Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience",
    "url": "http://arxiv.org/abs/2505.08032v1",
    "arxiv_id": "2505.08032v1",
    "authors": [
      "Seyed Bagher Hashemi Natanzi",
      "Zhicong Zhu",
      "Bo Tang"
    ],
    "published": "2025-05-12T19:59:05+00:00",
    "summary": "Adaptive beam switching in 6G networks is challenged by high frequencies, mobility, and blockage. We propose an Online Learning framework using Deep Reinforcement Learning (DRL) with an enhanced state representation (velocity and blockage history), a GRU architecture, and prioritized experience replay for real-time beam optimization. Validated via Nvidia Sionna under time-correlated blockage, our approach significantly enhances resilience in SNR, throughput, and accuracy compared to a conventional heuristic. Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit (MAB) baseline by leveraging temporal dependencies, achieving lower performance variability. This demonstrates the benefits of memory and prioritized learning for robust 6G beam management, while confirming MAB as a strong baseline."
  },
  {
    "title": "MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing",
    "url": "http://arxiv.org/abs/2505.07984v1",
    "arxiv_id": "2505.07984v1",
    "authors": [
      "Aybora Koksal",
      "A. Aydin Alatan"
    ],
    "published": "2025-05-12T18:30:02+00:00",
    "summary": "Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications."
  },
  {
    "title": "Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement",
    "url": "http://arxiv.org/abs/2505.07961v1",
    "arxiv_id": "2505.07961v1",
    "authors": [
      "Xuechen Zhang",
      "Zijian Huang",
      "Chenchun Ni",
      "Ziyang Xiong",
      "Jiasi Chen",
      "Samet Oymak"
    ],
    "published": "2025-05-12T18:04:39+00:00",
    "summary": "Recent research enhances language model reasoning by scaling test-time compute via longer chain-of-thought traces. This often improves accuracy but also introduces redundancy and high computational cost, especially for small language models distilled with supervised fine-tuning (SFT). In this work, we propose new algorithms to improve token-efficient reasoning with small-scale models by effectively trading off accuracy and computation. We first show that the post-SFT model fails to determine the optimal stopping point of the reasoning process, resulting in verbose and repetitive outputs. Verbosity also significantly varies across wrong vs correct responses. To address these issues, we propose two solutions: (1) Temperature scaling (TS) to control the stopping point for the thinking phase and thereby trace length, and (2) TLDR: a length-regularized reinforcement learning method based on GRPO that facilitates multi-level trace length control (e.g. short, medium, long reasoning). Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and OlympiadBench, demonstrate that TS is highly effective compared to s1's budget forcing approach and TLDR significantly improves token efficiency by about 50% with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also facilitates flexible control over the response length, offering a practical and effective solution for token-efficient reasoning in small models. Ultimately, our work reveals the importance of stopping time control, highlights shortcomings of pure SFT, and provides effective algorithmic recipes."
  },
  {
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "url": "http://arxiv.org/abs/2505.07818v1",
    "arxiv_id": "2505.07818v1",
    "authors": [
      "Zeyue Xue",
      "Jie Wu",
      "Yu Gao",
      "Fangyuan Kong",
      "Lingting Zhu",
      "Mengzhao Chen",
      "Zhiheng Liu",
      "Wei Liu",
      "Qiushan Guo",
      "Weilin Huang",
      "Ping Luo"
    ],
    "published": "2025-05-12T17:59:34+00:00",
    "summary": "Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released."
  },
  {
    "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.07815v1",
    "arxiv_id": "2505.07815v1",
    "authors": [
      "Seungjae Lee",
      "Daniel Ekpo",
      "Haowen Liu",
      "Furong Huang",
      "Abhinav Shrivastava",
      "Jia-Bin Huang"
    ],
    "published": "2025-05-12T17:59:11+00:00",
    "summary": "Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations."
  },
  {
    "title": "A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values",
    "url": "http://arxiv.org/abs/2505.07797v1",
    "arxiv_id": "2505.07797v1",
    "authors": [
      "Daniel Beechey",
      "Thomas M. S. Smith",
      "\u00d6zg\u00fcr \u015eim\u015fek"
    ],
    "published": "2025-05-12T17:48:28+00:00",
    "summary": "Reinforcement learning agents can achieve superhuman performance, but their decisions are often difficult to interpret. This lack of transparency limits deployment, especially in safety-critical settings where human trust and accountability are essential. In this work, we develop a theoretical framework for explaining reinforcement learning through the influence of state features, which represent what the agent observes in its environment. We identify three core elements of the agent-environment interaction that benefit from explanation: behaviour (what the agent does), performance (what the agent achieves), and value estimation (what the agent expects to achieve). We treat state features as players cooperating to produce each element and apply Shapley values, a principled method from cooperative game theory, to identify the influence of each feature. This approach yields a family of mathematically grounded explanations with clear semantics and theoretical guarantees. We use illustrative examples to show how these explanations align with human intuition and reveal novel insights. Our framework unifies and extends prior work, making explicit the assumptions behind existing approaches, and offers a principled foundation for more interpretable and trustworthy reinforcement learning."
  },
  {
    "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering",
    "url": "http://arxiv.org/abs/2505.07782v1",
    "arxiv_id": "2505.07782v1",
    "authors": [
      "Rushi Qiang",
      "Yuchen Zhuang",
      "Yinghao Li",
      "Dingu Sagar V K",
      "Rongzhi Zhang",
      "Changhao Li",
      "Ian Shu-Hei Wong",
      "Sherry Yang",
      "Percy Liang",
      "Chao Zhang",
      "Bo Dai"
    ],
    "published": "2025-05-12T17:35:43+00:00",
    "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents."
  },
  {
    "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving",
    "url": "http://arxiv.org/abs/2505.07773v1",
    "arxiv_id": "2505.07773v1",
    "authors": [
      "Xinji Mai",
      "Haotian Xu",
      "Xing W",
      "Weinong Wang",
      "Yingying Zhang",
      "Wenqiang Zhang"
    ],
    "published": "2025-05-12T17:23:34+00:00",
    "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}."
  },
  {
    "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
    "url": "http://arxiv.org/abs/2505.07686v1",
    "arxiv_id": "2505.07686v1",
    "authors": [
      "Muzhi Dai",
      "Chenxu Yang",
      "Qingyi Si"
    ],
    "published": "2025-05-12T15:50:44+00:00",
    "summary": "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learning's systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the model's behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks."
  },
  {
    "title": "A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms",
    "url": "http://arxiv.org/abs/2505.07660v1",
    "arxiv_id": "2505.07660v1",
    "authors": [
      "Dieu-Donne Fangnon",
      "Armandine Sorel Kouyim Meli",
      "Verlon Roel Mbingui",
      "Phanie Dianelle Negho",
      "Regis Konan Marcel Djaha"
    ],
    "published": "2025-05-12T15:27:36+00:00",
    "summary": "Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \\href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\\color{blue}Github link}."
  },
  {
    "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
    "url": "http://arxiv.org/abs/2505.07608v1",
    "arxiv_id": "2505.07608v1",
    "authors": [
      "Xiaomi LLM-Core Team",
      ":",
      "Bingquan Xia",
      "Bowen Shen",
      "Cici",
      "Dawei Zhu",
      "Di Zhang",
      "Gang Wang",
      "Hailin Zhang",
      "Huaqiu Liu",
      "Jiebao Xiao",
      "Jinhao Dong",
      "Liang Zhao",
      "Peidian Li",
      "Peng Wang",
      "Shihua Yu",
      "Shimao Chen",
      "Weikun Wang",
      "Wenhan Ma",
      "Xiangwei Deng",
      "Yi Huang",
      "Yifan Song",
      "Zihan Jiang",
      "Bowen Ye",
      "Can Cai",
      "Chenhong He",
      "Dong Zhang",
      "Duo Zhang",
      "Guoan Wang",
      "Hao Tian",
      "Haochen Zhao",
      "Heng Qu",
      "Hongshen Xu",
      "Jun Shi",
      "Kainan Bao",
      "QingKai Fang",
      "Kang Zhou",
      "Kangyang Zhou",
      "Lei Li",
      "Menghang Zhu",
      "Nuo Chen",
      "Qiantong Wang",
      "Shaohui Liu",
      "Shicheng Li",
      "Shuhao Gu",
      "Shuhuai Ren",
      "Shuo Liu",
      "Sirui Deng",
      "Weiji Zhuang",
      "Weiwei Lv",
      "Wenyu Yang",
      "Xin Zhang",
      "Xing Yong",
      "Xing Zhang",
      "Xingchen Song",
      "Xinzhe Xu",
      "Xu Wang",
      "Yihan Yan",
      "Yu Tu",
      "Yuanyuan Tian",
      "Yudong Wang",
      "Yue Yu",
      "Zhenru Lin",
      "Zhichao Song",
      "Zihao Yue"
    ],
    "published": "2025-05-12T14:30:11+00:00",
    "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo."
  },
  {
    "title": "Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control",
    "url": "http://arxiv.org/abs/2505.07607v1",
    "arxiv_id": "2505.07607v1",
    "authors": [
      "Georg Sch\u00e4fer",
      "Raphael Seliger",
      "Jakob Rehrl",
      "Stefan Huber",
      "Simon Hirlaender"
    ],
    "published": "2025-05-12T14:28:42+00:00",
    "summary": "Industrial automation increasingly demands energy-efficient control strategies to balance performance with environmental and cost constraints. In this work, we present a multi-objective reinforcement learning (MORL) framework for energy-efficient control of the Quanser Aero 2 testbed in its one-degree-of-freedom configuration. We design a composite reward function that simultaneously penalizes tracking error and electrical power consumption. Preliminary experiments explore the influence of varying the Energy penalty weight, alpha, on the trade-off between pitch tracking and energy savings. Our results reveal a marked performance shift for alpha values between 0.0 and 0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both the simulation and the real system. We hypothesize that these effects may be attributed to artifacts introduced by the adaptive behavior of the Adam optimizer, which could bias the learning process and favor bang-bang control strategies. Future work will focus on automating alpha selection through Gaussian Process-based Pareto front modeling and transitioning the approach from simulation to real-world deployment."
  },
  {
    "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent",
    "url": "http://arxiv.org/abs/2505.07596v1",
    "arxiv_id": "2505.07596v1",
    "authors": [
      "Ziyang Huang",
      "Xiaowei Yuan",
      "Yiming Ju",
      "Jun Zhao",
      "Kang Liu"
    ],
    "published": "2025-05-12T14:21:57+00:00",
    "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities."
  },
  {
    "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models",
    "url": "http://arxiv.org/abs/2505.07591v1",
    "arxiv_id": "2505.07591v1",
    "authors": [
      "Junjie Ye",
      "Caishuang Huang",
      "Zhuohan Chen",
      "Wenjie Fu",
      "Chenyuan Yang",
      "Leyi Yang",
      "Yilong Wu",
      "Peng Wang",
      "Meng Zhou",
      "Xiaolong Yang",
      "Tao Gui",
      "Qi Zhang",
      "Zhongchao Shi",
      "Jianping Fan",
      "Xuanjing Huang"
    ],
    "published": "2025-05-12T14:16:55+00:00",
    "summary": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF."
  },
  {
    "title": "Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review",
    "url": "http://arxiv.org/abs/2505.07911v1",
    "arxiv_id": "2505.07911v1",
    "authors": [
      "Chengmin Zhou",
      "Ville Kyrki",
      "Pasi Fr\u00e4nti",
      "Laura Ruotsalainen"
    ],
    "published": "2025-05-12T13:34:50+00:00",
    "summary": "Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies."
  },
  {
    "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning",
    "url": "http://arxiv.org/abs/2505.07538v1",
    "arxiv_id": "2505.07538v1",
    "authors": [
      "Bohan Wang",
      "Zhongqi Yue",
      "Fengda Zhang",
      "Shuo Chen",
      "Li'an Bi",
      "Junzhe Zhang",
      "Xue Song",
      "Kennard Yanting Chan",
      "Jiachun Pan",
      "Weijia Wu",
      "Mingze Zhou",
      "Wang Lin",
      "Kaihang Pan",
      "Saining Zhang",
      "Liyu Jia",
      "Wentao Hu",
      "Wei Zhao",
      "Hanwang Zhang"
    ],
    "published": "2025-05-12T13:19:08+00:00",
    "summary": "We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/."
  },
  {
    "title": "The Exploratory Multi-Asset Mean-Variance Portfolio Selection using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.07537v1",
    "arxiv_id": "2505.07537v1",
    "authors": [
      "Yu Li",
      "Yuhan Wu",
      "Shuhua Zhang"
    ],
    "published": "2025-05-12T13:18:49+00:00",
    "summary": "In this paper, we study the continuous-time multi-asset mean-variance (MV) portfolio selection using a reinforcement learning (RL) algorithm, specifically the soft actor-critic (SAC) algorithm, in the time-varying financial market. A family of Gaussian portfolio selections is derived, and a policy iteration process is crafted to learn the optimal exploratory portfolio selection. We prove the convergence of the policy iteration process theoretically, based on which the SAC algorithm is developed. To improve the algorithm's stability and the learning accuracy in the multi-asset scenario, we divide the model parameters that influence the optimal portfolio selection into three parts, and learn each part progressively. Numerical studies in the simulated and real financial markets confirm the superior performance of the proposed SAC algorithm under various criteria."
  },
  {
    "title": "Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning",
    "url": "http://arxiv.org/abs/2505.07527v1",
    "arxiv_id": "2505.07527v1",
    "authors": [
      "Hu Wang",
      "Congbo Ma",
      "Ian Reid",
      "Mohammad Yaqub"
    ],
    "published": "2025-05-12T13:09:49+00:00",
    "summary": "Reward baseline is important for Reinforcement Learning (RL) algorithms to reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) is proposed to compute the advantage for each output by subtracting the mean reward, as the baseline, for all outputs in the group. However, it can lead to inaccurate advantage estimates in environments with highly noisy rewards, potentially introducing bias. In this work, we propose a model, called Kalman Filter Enhanced Group Relative Policy Optimization (KRPO), by using lightweight Kalman filtering to dynamically estimate the latent reward mean and variance. This filtering technique replaces the naive batch mean baseline, enabling more adaptive advantage normalization. Our method does not require additional learned parameters over GRPO. This approach offers a simple yet effective way to incorporate multiple outputs of GRPO into advantage estimation, improving policy optimization in settings where highly dynamic reward signals are difficult to model for language models. Through experiments and analyses, we show that using a more adaptive advantage estimation model, KRPO can improve the stability and performance of GRPO. The code is available at https://github.com/billhhh/KRPO_LLMs_RL"
  },
  {
    "title": "Average-Reward Maximum Entropy Reinforcement Learning for Global Policy in Double Pendulum Tasks",
    "url": "http://arxiv.org/abs/2505.07516v1",
    "arxiv_id": "2505.07516v1",
    "authors": [
      "Jean Seong Bjorn Choe",
      "Bumkyu Choi",
      "Jong-kook Kim"
    ],
    "published": "2025-05-12T12:53:57+00:00",
    "summary": "This report presents our reinforcement learning-based approach for the swing-up and stabilisation tasks of the acrobot and pendubot, tailored specifcially to the updated guidelines of the 3rd AI Olympics at ICRA 2025. Building upon our previously developed Average-Reward Entropy Advantage Policy Optimization (AR-EAPO) algorithm, we refined our solution to effectively address the new competition scenarios and evaluation metrics. Extensive simulations validate that our controller robustly manages these revised tasks, demonstrating adaptability and effectiveness within the updated framework."
  },
  {
    "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models",
    "url": "http://arxiv.org/abs/2505.07500v1",
    "arxiv_id": "2505.07500v1",
    "authors": [
      "Bahram Mohammadi",
      "Ehsan Abbasnejad",
      "Yuankai Qi",
      "Qi Wu",
      "Anton Van Den Hengel",
      "Javen Qinfeng Shi"
    ],
    "published": "2025-05-12T12:38:20+00:00",
    "summary": "The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as \"bring me a spoon\", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art."
  },
  {
    "title": "ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.07395v1",
    "arxiv_id": "2505.07395v1",
    "authors": [
      "Hongyin Zhang",
      "Zifeng Zhuang",
      "Han Zhao",
      "Pengxiang Ding",
      "Hongchao Lu",
      "Donglin Wang"
    ],
    "published": "2025-05-12T09:48:03+00:00",
    "summary": "Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks."
  },
  {
    "title": "Drive Fast, Learn Faster: On-Board RL for High Performance Autonomous Racing",
    "url": "http://arxiv.org/abs/2505.07321v1",
    "arxiv_id": "2505.07321v1",
    "authors": [
      "Benedict Hildisch",
      "Edoardo Ghignone",
      "Nicolas Baumann",
      "Cheng Hu",
      "Andrea Carron",
      "Michele Magno"
    ],
    "published": "2025-05-12T08:06:36+00:00",
    "summary": "Autonomous racing presents unique challenges due to its non-linear dynamics, the high speed involved, and the critical need for real-time decision-making under dynamic and unpredictable conditions. Most traditional Reinforcement Learning (RL) approaches rely on extensive simulation-based pre-training, which faces crucial challenges in transfer effectively to real-world environments. This paper introduces a robust on-board RL framework for autonomous racing, designed to eliminate the dependency on simulation-based pre-training enabling direct real-world adaptation. The proposed system introduces a refined Soft Actor-Critic (SAC) algorithm, leveraging a residual RL structure to enhance classical controllers in real-time by integrating multi-step Temporal-Difference (TD) learning, an asynchronous training pipeline, and Heuristic Delayed Reward Adjustment (HDRA) to improve sample efficiency and training stability. The framework is validated through extensive experiments on the F1TENTH racing platform, where the residual RL controller consistently outperforms the baseline controllers and achieves up to an 11.5 % reduction in lap times compared to the State-of-the-Art (SotA) with only 20 min of training. Additionally, an End-to-End (E2E) RL controller trained without a baseline controller surpasses the previous best results with sustained on-track learning. These findings position the framework as a robust solution for high-performance autonomous racing and a promising direction for other real-time, dynamic autonomous systems."
  },
  {
    "title": "Online Episodic Convex Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.07303v1",
    "arxiv_id": "2505.07303v1",
    "authors": [
      "Bianca Marin Moreno",
      "Khaled Eldowa",
      "Pierre Gaillard",
      "Margaux Br\u00e9g\u00e8re",
      "Nadia Oudjane"
    ],
    "published": "2025-05-12T07:47:49+00:00",
    "summary": "We study online learning in episodic finite-horizon Markov decision processes (MDPs) with convex objective functions, known as the concave utility reinforcement learning (CURL) problem. This setting generalizes RL from linear to convex losses on the state-action distribution induced by the agent's policy. The non-linearity of CURL invalidates classical Bellman equations and requires new algorithmic approaches. We introduce the first algorithm achieving near-optimal regret bounds for online CURL without any prior knowledge on the transition function. To achieve this, we use an online mirror descent algorithm with varying constraint sets and a carefully designed exploration bonus. We then address for the first time a bandit version of CURL, where the only feedback is the value of the objective function on the state-action distribution induced by the agent's policy. We achieve a sub-linear regret bound for this more challenging problem by adapting techniques from bandit convex optimization to the MDP setting."
  },
  {
    "title": "HuB: Learning Extreme Humanoid Balance",
    "url": "http://arxiv.org/abs/2505.07294v1",
    "arxiv_id": "2505.07294v1",
    "authors": [
      "Tong Zhang",
      "Boyuan Zheng",
      "Ruiqian Nai",
      "Yingdong Hu",
      "Yen-Jen Wang",
      "Geng Chen",
      "Fanqi Lin",
      "Jiongye Li",
      "Chuye Hong",
      "Koushil Sreenath",
      "Yang Gao"
    ],
    "published": "2025-05-12T07:31:42+00:00",
    "summary": "The human body demonstrates exceptional motor capabilities-such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters-both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose HuB (Humanoid Balance), a unified framework that integrates reference motion refinement, balance-aware policy learning, and sim-to-real robustness training, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy remains stable even under strong physical disturbances-such as a forceful soccer strike-while baseline methods consistently fail to complete these tasks. Project website: https://hub-robot.github.io"
  },
  {
    "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.07291v1",
    "arxiv_id": "2505.07291v1",
    "authors": [
      "Prime Intellect Team",
      "Sami Jaghouar",
      "Justus Mattern",
      "Jack Min Ong",
      "Jannik Straube",
      "Manveer Basra",
      "Aaron Pazdera",
      "Kushal Thaman",
      "Matthew Di Ferrante",
      "Felix Gabriel",
      "Fares Obeid",
      "Kemal Erdem",
      "Michael Keiblinger",
      "Johannes Hagemann"
    ],
    "published": "2025-05-12T07:24:33+00:00",
    "summary": "We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training."
  },
  {
    "title": "Multi-Agent DRL for Multi-Objective Twin Migration Routing with Workload Prediction in 6G-enabled IoV",
    "url": "http://arxiv.org/abs/2505.07290v1",
    "arxiv_id": "2505.07290v1",
    "authors": [
      "Peng Yin",
      "Wentao Liang",
      "Jinbo Wen",
      "Jiawen Kang",
      "Junlong Chen",
      "Dusit Niyato"
    ],
    "published": "2025-05-12T07:23:27+00:00",
    "summary": "Sixth Generation (6G)-enabled Internet of Vehicles (IoV) facilitates efficient data synchronization through ultra-fast bandwidth and high-density connectivity, enabling the emergence of Vehicle Twins (VTs). As highly accurate replicas of vehicles, VTs can support intelligent vehicular applications for occupants in 6G-enabled IoV. Thanks to the full coverage capability of 6G, resource-constrained vehicles can offload VTs to edge servers, such as roadside units, unmanned aerial vehicles, and satellites, utilizing their computing and storage resources for VT construction and updates. However, communication between vehicles and edge servers with limited coverage is prone to interruptions due to the dynamic mobility of vehicles. Consequently, VTs must be migrated among edge servers to maintain uninterrupted and high-quality services for users. In this paper, we introduce a VT migration framework in 6G-enabled IoV. Specifically, we first propose a Long Short-Term Memory (LSTM)-based Transformer model to accurately predict long-term workloads of edge servers for migration decision-making. Then, we propose a Dynamic Mask Multi-Agent Proximal Policy Optimization (DM-MAPPO) algorithm to identify optimal migration routes in the highly complex environment of 6G-enabled IoV. Finally, we develop a practical platform to validate the effectiveness of the proposed scheme using real datasets. Simulation results demonstrate that the proposed DM-MAPPO algorithm significantly reduces migration latency by 20.82% and packet loss by 75.07% compared with traditional deep reinforcement learning algorithms."
  },
  {
    "title": "Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE 802.11 MAPC Networks",
    "url": "http://arxiv.org/abs/2505.07278v1",
    "arxiv_id": "2505.07278v1",
    "authors": [
      "Maksymilian Wojnar",
      "Wojciech Ci\u0119\u017cobka",
      "Artur Tomaszewski",
      "Piotr Cho\u0142da",
      "Krzysztof Rusek",
      "Katarzyna Kosek-Szott",
      "Jetmir Haxhibeqiri",
      "Jeroen Hoebeke",
      "Boris Bellalta",
      "Anatolij Zubow",
      "Falko Dressler",
      "Szymon Szott"
    ],
    "published": "2025-05-12T07:01:33+00:00",
    "summary": "The densification of Wi-Fi deployments means that fully distributed random channel access is no longer sufficient for high and predictable performance. Therefore, the upcoming IEEE 802.11bn amendment introduces multi-access point coordination (MAPC) methods. This paper addresses a variant of MAPC called coordinated spatial reuse (C-SR), where devices transmit simultaneously on the same channel, with the power adjusted to minimize interference. The C-SR scheduling problem is selecting which devices transmit concurrently and with what settings. We provide a theoretical upper bound model, optimized for either throughput or fairness, which finds the best possible transmission schedule using mixed-integer linear programming. Then, a practical, probing-based approach is proposed which uses multi-armed bandits (MABs), a type of reinforcement learning, to solve the C-SR scheduling problem. We validate both classical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and in a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy IEEE 802.11 (on average by 80\\% in random scenarios), without reducing the number of transmission opportunities per station. Finally, our framework is lightweight and ready for implementation in Wi-Fi devices."
  },
  {
    "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains",
    "url": "http://arxiv.org/abs/2505.07274v1",
    "arxiv_id": "2505.07274v1",
    "authors": [
      "Ibne Farabi Shihab",
      "Sanjeda Akter",
      "Anuj Sharma"
    ],
    "published": "2025-05-12T06:53:24+00:00",
    "summary": "Integrating large language models (LLMs) as priors in reinforcement learning (RL) offers significant advantages but comes with substantial computational costs. We present a principled cache-efficient framework for posterior sampling with LLM-derived priors that dramatically reduces these costs while maintaining high performance. At the core of our approach is an adaptive caching mechanism, where cache parameters are meta-optimized using surrogate gradients derived from policy performance. This design enables efficient inference across both discrete text environments (e.g., TextWorld, ALFWorld) and continuous control domains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries and 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU) while retaining 96--98\\% of uncached performance. Our theoretical analysis provides KL divergence bounds on approximation quality, validated empirically. The framework extends to offline RL, where our CQL-Prior variant improves performance by 14--29\\% and reduces training time by 38--40\\%. Extensive evaluations across a diverse suite of eight tasks demonstrate the generalizability and practical viability of LLM-guided RL in resource-constrained settings."
  },
  {
    "title": "On the Robustness of Reward Models for Language Model Alignment",
    "url": "http://arxiv.org/abs/2505.07271v1",
    "arxiv_id": "2505.07271v1",
    "authors": [
      "Jiwoo Hong",
      "Noah Lee",
      "Eunki Kim",
      "Guijin Son",
      "Woojin Chung",
      "Aman Gupta",
      "Shao Tang",
      "James Thorne"
    ],
    "published": "2025-05-12T06:48:26+00:00",
    "summary": "The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."
  },
  {
    "title": "DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward",
    "url": "http://arxiv.org/abs/2505.07257v1",
    "arxiv_id": "2505.07257v1",
    "authors": [
      "Yi Zhang",
      "Ruihong Qiu",
      "Xuwei Xu",
      "Jiajun Liu",
      "Sen Wang"
    ],
    "published": "2025-05-12T06:18:31+00:00",
    "summary": "Model-based offline reinforcement learning (RL) has emerged as a promising approach for recommender systems, enabling effective policy learning by interacting with frozen world models. However, the reward functions in these world models, trained on sparse offline logs, often suffer from inaccuracies. Specifically, existing methods face two major limitations in addressing this challenge: (1) deterministic use of reward functions as static look-up tables, which propagates inaccuracies during policy learning, and (2) static uncertainty designs that fail to effectively capture decision risks and mitigate the impact of these inaccuracies. In this work, a dual-agent framework, DARLR, is proposed to dynamically update world models to enhance recommendation policies. To achieve this, a \\textbf{\\textit{selector}} is introduced to identify reference users by balancing similarity and diversity so that the \\textbf{\\textit{recommender}} can aggregate information from these users and iteratively refine reward estimations for dynamic reward shaping. Further, the statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty to better align with evolving recommendation requirements. Extensive experiments on four benchmark datasets demonstrate the superior performance of DARLR, validating its effectiveness. The code is available at https://github.com/ArronDZhang/DARLR."
  },
  {
    "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
    "url": "http://arxiv.org/abs/2505.07233v1",
    "arxiv_id": "2505.07233v1",
    "authors": [
      "Jiashuo Sun",
      "Xianrui Zhong",
      "Sizhe Zhou",
      "Jiawei Han"
    ],
    "published": "2025-05-12T05:19:01+00:00",
    "summary": "Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG"
  },
  {
    "title": "Measuring General Intelligence with Generated Games",
    "url": "http://arxiv.org/abs/2505.07215v1",
    "arxiv_id": "2505.07215v1",
    "authors": [
      "Vivek Verma",
      "David Huang",
      "William Chen",
      "Dan Klein",
      "Nicholas Tomlin"
    ],
    "published": "2025-05-12T04:01:03+00:00",
    "summary": "We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark."
  },
  {
    "title": "Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.07207v1",
    "arxiv_id": "2505.07207v1",
    "authors": [
      "Chiqiang Liu",
      "Dazi Li"
    ],
    "published": "2025-05-12T03:31:26+00:00",
    "summary": "Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance."
  },
  {
    "title": "Suppressing Measurement Noise in Logical Qubits Through Measurement Scheduling",
    "url": "http://arxiv.org/abs/2505.07173v1",
    "arxiv_id": "2505.07173v1",
    "authors": [
      "Xiao-Yue Xu",
      "Chen Ding",
      "Wan-Su Bao"
    ],
    "published": "2025-05-12T01:54:40+00:00",
    "summary": "Quantum error correction is essential for reliable quantum computation, where surface codes demonstrate high fault-tolerant thresholds and hardware efficiency. However, noise in single-shot measurements limits logical readout fidelity, forming a critical bottleneck for fault-tolerant quantum computation. We propose a dynamic measurement scheduling protocol that suppresses logical readout errors by adaptively redistributing measurement tasks from error-prone qubits to stable nodes. Using shallow entangled circuits, the protocol balances gate errors and measurement noise. This is achieved by dynamically prioritizing resource allocation based on topological criticality and error metrics. When addressing realistic scenarios where temporal constraints are governed by decoherence limits and error-correction requirements, we implement reinforcement learning (RL) to achieve adaptive measurement scheduling. Numerical simulations show that logical error rates can be reduced by up to 34% across code distances for 3 to 11, with enhanced robustness in measurement-noise-dominated systems. Our protocol offers a versatile, hardware-efficient solution for high-fidelity quantum error correction, advancing large-scale quantum computing."
  },
  {
    "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
    "url": "http://arxiv.org/abs/2505.07096v1",
    "arxiv_id": "2505.07096v1",
    "authors": [
      "Prithwish Dan",
      "Kushal Kedia",
      "Angela Chao",
      "Edward Weiyi Duan",
      "Maximus Adrian Pace",
      "Wei-Chiu Ma",
      "Sanjiban Choudhury"
    ],
    "published": "2025-05-11T19:04:00+00:00",
    "summary": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Si introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/."
  },
  {
    "title": "DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs",
    "url": "http://arxiv.org/abs/2505.07049v1",
    "arxiv_id": "2505.07049v1",
    "authors": [
      "Yubo Shu",
      "Zhewei Huang",
      "Xin Wu",
      "Chen Hu",
      "Shuchang Zhou",
      "Daxin Jiang"
    ],
    "published": "2025-05-11T16:39:58+00:00",
    "summary": "We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design."
  },
  {
    "title": "Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control",
    "url": "http://arxiv.org/abs/2505.07045v1",
    "arxiv_id": "2505.07045v1",
    "authors": [
      "Junjie Yu",
      "John S. Schreck",
      "David John Gagne",
      "Keith W. Oleson",
      "Jie Li",
      "Yongtu Liang",
      "Qi Liao",
      "Mingfei Sun",
      "David O. Topping",
      "Zhonghua Zheng"
    ],
    "published": "2025-05-11T16:33:42+00:00",
    "summary": "Reinforcement learning (RL)-based heating, ventilation, and air conditioning (HVAC) control has emerged as a promising technology for reducing building energy consumption while maintaining indoor thermal comfort. However, the efficacy of such strategies is influenced by the background climate and their implementation may potentially alter both the indoor climate and local urban climate. This study proposes an integrated framework combining RL with an urban climate model that incorporates a building energy model, aiming to evaluate the efficacy of RL-based HVAC control across different background climates, impacts of RL strategies on indoor climate and local urban climate, and the transferability of RL strategies across cities. Our findings reveal that the reward (defined as a weighted combination of energy consumption and thermal comfort) and the impacts of RL strategies on indoor climate and local urban climate exhibit marked variability across cities with different background climates. The sensitivity of reward weights and the transferability of RL strategies are also strongly influenced by the background climate. Cities in hot climates tend to achieve higher rewards across most reward weight configurations that balance energy consumption and thermal comfort, and those cities with more varying atmospheric temperatures demonstrate greater RL strategy transferability. These findings underscore the importance of thoroughly evaluating RL-based HVAC control strategies in diverse climatic contexts. This study also provides a new insight that city-to-city learning will potentially aid the deployment of RL-based HVAC control."
  },
  {
    "title": "Design and Experimental Test of Datatic Approximate Optimal Filter in Nonlinear Dynamic Systems",
    "url": "http://arxiv.org/abs/2505.07043v1",
    "arxiv_id": "2505.07043v1",
    "authors": [
      "Weixian He",
      "Zeyu He",
      "Wenhan Cao",
      "Haoyu Gao",
      "Tong Liu",
      "Bin Shuai",
      "Chang Liu",
      "Shengbo Eben Li"
    ],
    "published": "2025-05-11T16:28:14+00:00",
    "summary": "Filtering is crucial in engineering fields, providing vital state estimation for control systems. However, the nonlinear nature of complex systems and the presence of non-Gaussian noises pose significant challenges to the performance of conventional filtering methods in terms of estimation accuracy and computational efficiency. In this work, we present a data-driven closed-loop filter, termed datatic approximate optimal filter (DAOF), specifically designed for nonlinear systems under non-Gaussian conditions. We first formulate a Markovian filtering problem (MFP), which inherently shares a connection with reinforcement learning (RL) as it aims to compute the optimal state estimate by minimizing the accumulated error. To solve MFP, we propose DAOF, which primarily incorporates a trained RL policy and features two distinct structural designs: DAOF-v1 and DAOF-v2. Designed for systems with explicit models, DAOF-v1 combines prediction and update phases, with the RL policy generating the update value. Meanwhile, DAOF-v2 bypasses system modeling by directly outputting the state estimate. Then, we utilize an actor-critic algorithm to learn the parameterized policy for DAOF. Experimental results on a 2-degree-of-freedom (2-DOF) vehicle system, equipped with explicit system models, demonstrate the superior accuracy and computational efficiency of DAOF-v1 compared to existing nonlinear filters. Moreover, DAOF-v2 showcases its unique ability to perform filtering without requiring explicit system modeling, as validated by a 14-DOF vehicle system."
  },
  {
    "title": "A Reinforcement Learning Framework for Application-Specific TCP Congestion-Control",
    "url": "http://arxiv.org/abs/2505.07042v1",
    "arxiv_id": "2505.07042v1",
    "authors": [
      "Jinming Xing",
      "Muhammad Shahzad"
    ],
    "published": "2025-05-11T16:27:19+00:00",
    "summary": "The Congestion Control (CC) module plays a critical role in the Transmission Control Protocol (TCP), ensuring the stability and efficiency of network data transmission. The CC approaches that are commonly used these days employ heuristics-based rules to adjust the sending rate. Due to their heuristics-based nature, these approaches are not only unable to adapt to changing network conditions but are also agnostic to the diverse requirements that different applications often have. Recently, several learning-based CC approaches have been proposed to adapt to changing network conditions. Unfortunately, they are not designed to take application requirements into account. Prior heuristics-based as well as learning-based CC approaches focus on achieving a singular objective, which is often to maximize throughput, even though a lot of applications care more about latency, packet losses, jitter, and different combinations of various network metrics. Motivated by this, we propose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC, which allows any application to specify any arbitrary objectives that the network traffic of that application should achieve and is able to swiftly adapt to the changes in the objectives of the applications as well as to the changes in the network conditions. Our ASC framework further employs a client-server architecture that serves two purposes: 1) it makes ASC highly scalable in terms of the arrival and departure of TCP connections, and 2) it makes ASC very lightweight for the nodes maintaining the TCP connections. We implemented and extensively evaluated ASC in a variety of settings. Our results show that it can not only achieve various objectives but also outperforms prior approaches even in the specific objectives that those approaches were designed to achieve."
  },
  {
    "title": "A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue",
    "url": "http://arxiv.org/abs/2505.06997v1",
    "arxiv_id": "2505.06997v1",
    "authors": [
      "Wenhao Lu",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yonglin Tian",
      "Junjie Zeng",
      "Jun Zhang",
      "Zhong Liu",
      "Fei-Yue Wang"
    ],
    "published": "2025-05-11T14:49:15+00:00",
    "summary": "Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response."
  },
  {
    "title": "Learning Value of Information towards Joint Communication and Control in 6G V2X",
    "url": "http://arxiv.org/abs/2505.06978v1",
    "arxiv_id": "2505.06978v1",
    "authors": [
      "Lei Lei",
      "Kan Zheng",
      "Xuemin",
      "Shen"
    ],
    "published": "2025-05-11T13:30:35+00:00",
    "summary": "As Cellular Vehicle-to-Everything (C-V2X) evolves towards future sixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are emerging to become a key application. Leveraging data-driven Machine Learning (ML), especially Deep Reinforcement Learning (DRL), is expected to significantly enhance CAV decision-making in both vehicle control and V2X communication under uncertainty. These two decision-making processes are closely intertwined, with the value of information (VoI) acting as a crucial bridge between them. In this paper, we introduce Sequential Stochastic Decision Process (SSDP) models to define and assess VoI, demonstrating their application in optimizing communication systems for CAVs. Specifically, we formally define the SSDP model and demonstrate that the MDP model is a special case of it. The SSDP model offers a key advantage by explicitly representing the set of information that can enhance decision-making when available. Furthermore, as current research on VoI remains fragmented, we propose a systematic VoI modeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal Control theories. We define different categories of VoI and discuss their corresponding estimation methods. Finally, we present a structured approach to leverage the various VoI metrics for optimizing the ``When\", ``What\", and ``How\" to communicate problems. For this purpose, SSDP models are formulated with VoI-associated reward functions derived from VoI-based optimization objectives. While we use a simple vehicle-following control problem to illustrate the proposed methodology, it holds significant potential to facilitate the joint optimization of stochastic, sequential control and communication decisions in a wide range of networked control systems."
  },
  {
    "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing",
    "url": "http://arxiv.org/abs/2505.06963v1",
    "arxiv_id": "2505.06963v1",
    "authors": [
      "Tarik Houichime",
      "Younes EL Amrani"
    ],
    "published": "2025-05-11T12:23:37+00:00",
    "summary": "This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields."
  },
  {
    "title": "YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action",
    "url": "http://arxiv.org/abs/2505.06923v1",
    "arxiv_id": "2505.06923v1",
    "authors": [
      "Junjie Lu",
      "Yulin Hui",
      "Xuewei Zhang",
      "Wencan Feng",
      "Hongming Shen",
      "Zhiyu Li",
      "Bailing Tian"
    ],
    "published": "2025-05-11T09:53:34+00:00",
    "summary": "Traditional target tracking pipelines including detection, mapping, navigation, and control are comprehensive but introduce high latency, limitting the agility of quadrotors. On the contrary, we follow the design principle of \"less is more\", striving to simplify the process while maintaining effectiveness. In this work, we propose an end-to-end agile tracking and navigation framework for quadrotors that directly maps the sensory observations to control commands. Importantly, leveraging the multimodal nature of navigation and detection tasks, our network maintains interpretability by explicitly integrating the independent modules of the traditional pipeline, rather than a crude action regression. In detail, we adopt a set of motion primitives as anchors to cover the searching space regarding the feasible region and potential target. Then we reformulate the trajectory optimization as regression of primitive offsets and associated costs considering the safety, smoothness, and other metrics. For tracking task, the trajectories are expected to approach the target and additional objectness scores are predicted. Subsequently, the predictions, after compensation for the estimated lumped disturbance, are transformed into thrust and attitude as control commands for swift response. During training, we seamlessly integrate traditional motion planning with deep learning by directly back-propagating the gradients of trajectory costs to the network, eliminating the need for expert demonstration in imitation learning and providing more direct guidance than reinforcement learning. Finally, we deploy the algorithm on a compact quadrotor and conduct real-world validations in both forest and building environments to demonstrate the efficiency of the proposed method."
  },
  {
    "title": "Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR",
    "url": "http://arxiv.org/abs/2505.06906v1",
    "arxiv_id": "2505.06906v1",
    "authors": [
      "Sindre Benjamin Remman",
      "Anastasios M. Lekkas"
    ],
    "published": "2025-05-11T08:55:17+00:00",
    "summary": "This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning from data. However, they often function as black boxes, making it challenging to interpret them. This is especially a problem in safety-critical control applications. To generate realistic CFEs, we parameterize the LiDAR space with simple shapes such as circles and rectangles, whose parameters are chosen by a genetic algorithm, and the configurations are transformed into LiDAR data by raycasting. Our model-agnostic approach generates CFEs in the form of synthetic LiDAR data that resembles a base LiDAR state but is modified to produce a pre-defined ML model control output based on a query from the user. We demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep reinforcement learning (DRL) in real-world and simulated scenarios. Our method generates logical and realistic CFEs, which helps to interpret the DRL agent's decision making. This paper contributes towards advancing explainable AI in mobile robotics, and our method could be a tool for understanding, debugging, and improving ML-based autonomous control."
  },
  {
    "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence",
    "url": "http://arxiv.org/abs/2505.06897v1",
    "arxiv_id": "2505.06897v1",
    "authors": [
      "Jinhao Jiang",
      "Changlin Chen",
      "Shile Feng",
      "Wanru Geng",
      "Zesheng Zhou",
      "Ni Wang",
      "Shuai Li",
      "Feng-Qi Cui",
      "Erbao Dong"
    ],
    "published": "2025-05-11T08:29:20+00:00",
    "summary": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which involves intelligent systems with physical presence and real-time interaction with the environment, has emerged as a key research direction in pursuit of AGI. While advancements in deep learning, reinforcement learning, large-scale language models, and multimodal technologies have significantly contributed to the progress of EAI, most existing reviews focus on specific technologies or applications. A systematic overview, particularly one that explores the direct connection between EAI and AGI, remains scarce. This paper examines EAI as a foundational approach to AGI, systematically analyzing its four core modules: perception, intelligent decision-making, action, and feedback. We provide a detailed discussion of how each module contributes to the six core principles of AGI. Additionally, we discuss future trends, challenges, and research directions in EAI, emphasizing its potential as a cornerstone for AGI development. Our findings suggest that EAI's integration of dynamic learning and real-world interaction is essential for bridging the gap between narrow AI and AGI."
  },
  {
    "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots",
    "url": "http://arxiv.org/abs/2505.06883v1",
    "arxiv_id": "2505.06883v1",
    "authors": [
      "Botian Xu",
      "Haoyang Weng",
      "Qingzhou Lu",
      "Yang Gao",
      "Huazhe Xu"
    ],
    "published": "2025-05-11T07:23:26+00:00",
    "summary": "Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities. However, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions. To address this limitation, we present \\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. In simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses (up to 200 Ns) and exhibits controllable compliance, achieving an 80% reduction in collision impulse. The policy is deployed to a physical robot to showcase both compliance and the ability to engage with large forces by kinesthetic control and pulling payloads up to 2/3 of its weight. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control. Project Website: https://egalahad.github.io/facet/"
  },
  {
    "title": "Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.06875v1",
    "arxiv_id": "2505.06875v1",
    "authors": [
      "Chengkai Xu",
      "Jiaqi Liu",
      "Yicheng Guo",
      "Yuhang Zhang",
      "Peng Hang",
      "Jian Sun"
    ],
    "published": "2025-05-11T06:55:54+00:00",
    "summary": "Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a \"fast-slow\" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the \"slow\" module, translating user directives into structured guidance, while the RL agent functions as the \"fast\" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments."
  },
  {
    "title": "FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation",
    "url": "http://arxiv.org/abs/2505.06776v1",
    "arxiv_id": "2505.06776v1",
    "authors": [
      "Yuanhang Zhang",
      "Yifu Yuan",
      "Prajwal Gurunath",
      "Tairan He",
      "Shayegan Omidshafiei",
      "Ali-akbar Agha-mohammadi",
      "Marcell Vazquez-Chanlatte",
      "Liam Pedersen",
      "Guanya Shi"
    ],
    "published": "2025-05-10T23:12:15+00:00",
    "summary": "Humanoid loco-manipulation holds transformative potential for daily service and industrial tasks, yet achieving precise, robust whole-body control with 3D end-effector force interaction remains a major challenge. Prior approaches are often limited to lightweight tasks or quadrupedal/wheeled platforms. To overcome these limitations, we propose FALCON, a dual-agent reinforcement-learning-based framework for robust force-adaptive humanoid loco-manipulation. FALCON decomposes whole-body control into two specialized agents: (1) a lower-body agent ensuring stable locomotion under external force disturbances, and (2) an upper-body agent precisely tracking end-effector positions with implicit adaptive force compensation. These two agents are jointly trained in simulation with a force curriculum that progressively escalates the magnitude of external force exerted on the end effector while respecting torque limits. Experiments demonstrate that, compared to the baselines, FALCON achieves 2x more accurate upper-body joint tracking, while maintaining robust locomotion under force disturbances and achieving faster training convergence. Moreover, FALCON enables policy training without embodiment-specific reward or curriculum tuning. Using the same training setup, we obtain policies that are deployed across multiple humanoids, enabling forceful loco-manipulation tasks such as transporting payloads (0-20N force), cart-pulling (0-100N), and door-opening (0-40N) in the real world."
  },
  {
    "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes",
    "url": "http://arxiv.org/abs/2505.06771v1",
    "arxiv_id": "2505.06771v1",
    "authors": [
      "Shalin Anand Jain",
      "Jiazhen Liu",
      "Siva Kailas",
      "Harish Ravichandar"
    ],
    "published": "2025-05-10T22:38:39+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation."
  },
  {
    "title": "LineFlow: A Framework to Learn Active Control of Production Lines",
    "url": "http://arxiv.org/abs/2505.06744v1",
    "arxiv_id": "2505.06744v1",
    "authors": [
      "Kai M\u00fcller",
      "Martin Wenzel",
      "Tobias Windisch"
    ],
    "published": "2025-05-10T19:36:18+00:00",
    "summary": "Many production lines require active control mechanisms, such as adaptive routing, worker reallocation, and rescheduling, to maintain optimal performance. However, designing these control systems is challenging for various reasons, and while reinforcement learning (RL) has shown promise in addressing these challenges, a standardized and general framework is still lacking. In this work, we introduce LineFlow, an extensible, open-source Python framework for simulating production lines of arbitrary complexity and training RL agents to control them. To demonstrate the capabilities and to validate the underlying theoretical assumptions of LineFlow, we formulate core subproblems of active line control in ways that facilitate mathematical analysis. For each problem, we provide optimal solutions for comparison. We benchmark state-of-the-art RL algorithms and show that the learned policies approach optimal performance in well-understood scenarios. However, for more complex, industrial-scale production lines, RL still faces significant challenges, highlighting the need for further research in areas such as reward shaping, curriculum learning, and hierarchical control."
  },
  {
    "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.06737v1",
    "arxiv_id": "2505.06737v1",
    "authors": [
      "Ahmed Abouelazm",
      "Jonas Michel",
      "Helen Gremmelmaier",
      "Tim Joseph",
      "Philip Sch\u00f6rner",
      "J. Marius Z\u00f6llner"
    ],
    "published": "2025-05-10T19:05:03+00:00",
    "summary": "Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward function has received insufficient attention, yielding ill-defined rewards with various pitfalls. Safety, in particular, has long been regarded only as a penalty for collisions. This leaves the risks associated with actions leading up to a collision unaddressed, limiting the applicability of RL in real-world scenarios. To address these shortcomings, our work focuses on enhancing the reward formulation by defining a set of driving objectives and structuring them hierarchically. Furthermore, we discuss the formulation of these objectives in a normalized manner to transparently determine their contribution to the overall reward. Additionally, we introduce a novel risk-aware objective for various driving interactions based on a two-dimensional ellipsoid function and an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the efficacy of our proposed reward in unsignalized intersection scenarios with varying traffic densities. The approach decreases collision rates by 21\\% on average compared to baseline rewards and consistently surpasses them in route progress and cumulative reward, demonstrating its capability to promote safer driving behaviors while maintaining high-performance levels."
  },
  {
    "title": "Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL",
    "url": "http://arxiv.org/abs/2505.06706v1",
    "arxiv_id": "2505.06706v1",
    "authors": [
      "Yuxuan Zheng",
      "Yihe Zhou",
      "Feiyang Xu",
      "Mingli Song",
      "Shunyu Liu"
    ],
    "published": "2025-05-10T17:04:33+00:00",
    "summary": "Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods. Our code will be made publicly available."
  },
  {
    "title": "Distributionally Robust Contract Theory for Edge AIGC Services in Teleoperation",
    "url": "http://arxiv.org/abs/2505.06678v1",
    "arxiv_id": "2505.06678v1",
    "authors": [
      "Zijun Zhan",
      "Yaxian Dong",
      "Daniel Mawunyo Doe",
      "Yuqing Hu",
      "Shuai Li",
      "Shaohua Cao",
      "Lei Fan",
      "Zhu Han"
    ],
    "published": "2025-05-10T15:53:05+00:00",
    "summary": "Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, further enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to both uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, contract theory is employed to model the information asymmetry, while DRO is utilized to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7\\% to 10.74\\% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02\\% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory. The code and data are publicly available at https://github.com/Zijun0819/DRO-Contract-Theory."
  },
  {
    "title": "Learning Guarantee of Reward Modeling Using Deep Neural Networks",
    "url": "http://arxiv.org/abs/2505.06601v1",
    "arxiv_id": "2505.06601v1",
    "authors": [
      "Yuanhang Luo",
      "Yeheng Ge",
      "Ruijian Han",
      "Guohao Shen"
    ],
    "published": "2025-05-10T11:21:29+00:00",
    "summary": "In this work, we study the learning theory of reward modeling with pairwise comparison data using deep neural networks. We establish a novel non-asymptotic regret bound for deep reward estimators in a non-parametric setting, which depends explicitly on the network architecture. Furthermore, to underscore the critical importance of clear human beliefs, we introduce a margin-type condition that assumes the conditional winning probability of the optimal action in pairwise comparisons is significantly distanced from 1/2. This condition enables a sharper regret bound, which substantiates the empirical efficiency of Reinforcement Learning from Human Feedback and highlights clear human beliefs in its success. Notably, this improvement stems from high-quality pairwise comparison data implied by the margin-type condition, is independent of the specific estimators used, and thus applies to various learning algorithms and models."
  },
  {
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "url": "http://arxiv.org/abs/2505.06218v1",
    "arxiv_id": "2505.06218v1",
    "authors": [
      "Kwan-Yee Lin",
      "Stella X. Yu"
    ],
    "published": "2025-05-09T17:53:02+00:00",
    "summary": "Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development."
  },
  {
    "title": "Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach",
    "url": "http://arxiv.org/abs/2505.06182v1",
    "arxiv_id": "2505.06182v1",
    "authors": [
      "Tim Schneider",
      "Cristiana de Farias",
      "Roberto Calandra",
      "Liming Chen",
      "Jan Peters"
    ],
    "published": "2025-05-09T16:49:26+00:00",
    "summary": "Humans make extensive use of haptic exploration to map and identify the properties of the objects that we touch. In robotics, active tactile perception has emerged as an important research domain that complements vision for tasks such as object classification, shape reconstruction, and manipulation. This work introduces TAP (Task-agnostic Active Perception) -- a novel framework that leverages reinforcement learning (RL) and transformer-based architectures to address the challenges posed by partially observable environments. TAP integrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified optimization objective, jointly training a perception module and decision-making policy. By design, TAP is completely task-agnostic and can, in principle, generalize to any active perception problem. We evaluate TAP across diverse tasks, including toy examples and realistic applications involving haptic exploration of 3D models from the Tactile MNIST benchmark. Experiments demonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST haptic digit recognition task and a tactile pose estimation task. These findings underscore the potential of TAP as a versatile and generalizable framework for advancing active tactile perception in robotics."
  },
  {
    "title": "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled Systems via Particle Filter Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.06122v1",
    "arxiv_id": "2505.06122v1",
    "authors": [
      "Haokun Yu",
      "Jingyuan Zhou",
      "Kaidi Yang"
    ],
    "published": "2025-05-09T15:25:48+00:00",
    "summary": "This paper addresses the problem of parameter privacy-preserving data sharing in coupled systems, where a data provider shares data with a data user but wants to protect its sensitive parameters. The shared data affects not only the data user's decision-making but also the data provider's operations through system interactions. To trade off control performance and privacy, we propose an interaction-aware privacy-preserving data sharing approach. Our approach generates distorted data by minimizing a combination of (i) mutual information, quantifying privacy leakage of sensitive parameters, and (ii) the impact of distorted data on the data provider's control performance, considering the interactions between stakeholders. The optimization problem is formulated into a Bellman equation and solved by a particle filter reinforcement learning (RL)-based approach. Compared to existing RL-based methods, our formulation significantly reduces history dependency and efficiently handles scenarios with continuous state space. Validated in a mixed-autonomy platoon scenario, our method effectively protects sensitive driving behavior parameters of human-driven vehicles (HDVs) against inference attacks while maintaining negligible impact on fuel efficiency."
  },
  {
    "title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations",
    "url": "http://arxiv.org/abs/2505.06079v1",
    "arxiv_id": "2505.06079v1",
    "authors": [
      "Shuaiyi Huang",
      "Mara Levy",
      "Anubhav Gupta",
      "Daniel Ekpo",
      "Ruijie Zheng",
      "Abhinav Shrivastava"
    ],
    "published": "2025-05-09T14:22:43+00:00",
    "summary": "Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: https://shuaiyihuang.github.io/publications/TREND."
  },
  {
    "title": "Safe-EF: Error Feedback for Nonsmooth Constrained Optimization",
    "url": "http://arxiv.org/abs/2505.06053v1",
    "arxiv_id": "2505.06053v1",
    "authors": [
      "Rustem Islamov",
      "Yarden As",
      "Ilyas Fatkhullin"
    ],
    "published": "2025-05-09T13:49:05+00:00",
    "summary": "Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-K) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity."
  },
  {
    "title": "Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI",
    "url": "http://arxiv.org/abs/2505.06025v1",
    "arxiv_id": "2505.06025v1",
    "authors": [
      "Jianpeng Qi",
      "Chao Liu",
      "Chengxiang Xu",
      "Rui Wang",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "published": "2025-05-09T13:17:42+00:00",
    "summary": "Timely and efficient dissemination of service information is critical in compute-first networking systems, where user requests arrive dynamically and computing resources are constrained. In such systems, the access point (AP) plays a key role in forwarding user requests to a server based on its latest received service information. This paper considers a single-source, single-destination system and introduces an Age-and-Value-Aware (AVA) metric that jointly captures both the timeliness and the task relevance of service information. Unlike traditional freshness-based metrics, AVA explicitly incorporates variations in server-side service capacity and AP forwarding decisions, allowing more context-aware update evaluation. Building upon AVA, we propose a reinforcement learning-based update policy that learns to selectively transmit service information updates to the AP. It aims to maximize overall task success while minimizing unnecessary communications. Extensive simulations under diverse user request patterns and varying service capacities demonstrate that AVA reduces the update frequency by over 90% on average compared to baselines, with reductions reaching 98% in certain configurations. Crucially, this reduction is achieved without compromising the accuracy of task execution or the quality of decision making."
  },
  {
    "title": "Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models",
    "url": "http://arxiv.org/abs/2505.05970v1",
    "arxiv_id": "2505.05970v1",
    "authors": [
      "Lennart St\u00f6pler",
      "Rufat Asadli",
      "Mitja Nikolaus",
      "Ryan Cotterell",
      "Alex Warstadt"
    ],
    "published": "2025-05-09T11:48:36+00:00",
    "summary": "We propose a method for training language models in an interactive setting inspired by child language acquisition. In our setting, a speaker attempts to communicate some information to a listener in a single-turn dialogue and receives a reward if communicative success is achieved. Unlike earlier related work using image--caption data for interactive reference games, we operationalize communicative success in a more abstract language-only question--answering setting. First, we present a feasibility study demonstrating that our reward provides an indirect signal about grammaticality. Second, we conduct experiments using reinforcement learning to fine-tune language models. We observe that cognitively plausible constraints on the communication channel lead to interpretable changes in speaker behavior. However, we do not yet see improvements on linguistic evaluations from our training regime. We outline potential modifications to the task design and training configuration that could better position future work to use our methodology to observe the benefits of interaction on language learning in computational cognitive models."
  },
  {
    "title": "Offline Multi-agent Reinforcement Learning via Score Decomposition",
    "url": "http://arxiv.org/abs/2505.05968v1",
    "arxiv_id": "2505.05968v1",
    "authors": [
      "Dan Qiao",
      "Wenhao Li",
      "Shanchao Yang",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "published": "2025-05-09T11:42:31+00:00",
    "summary": "Offline multi-agent reinforcement learning (MARL) faces critical challenges due to distributional shifts, further exacerbated by the high dimensionality of joint action spaces and the diversity in coordination strategies and quality among agents. Conventional approaches, including independent learning frameworks and value decomposition methods based on pessimistic principles, remain susceptible to out-of-distribution (OOD) joint actions and often yield suboptimal performance. Through systematic analysis of prevalent offline MARL benchmarks, we identify that this limitation primarily stems from the inherently multimodal nature of joint collaborative policies induced by offline data collection. To address these challenges, we propose a novel two-stage framework: First, we employ a diffusion-based generative model to explicitly capture the complex behavior policy, enabling accurate modeling of diverse multi-agent coordination patterns. Second, we introduce a sequential score function decomposition mechanism to regularize individual policies and enable decentralized execution. Extensive experiments on continuous control tasks demonstrate state-of-the-art performance across multiple standard offline MARL benchmarks, outperforming existing methods by 26.3\\% in normalized returns. Our approach provides new insights into offline coordination and equilibrium selection in cooperative multi-agent systems."
  },
  {
    "title": "Learning Power Control Protocol for In-Factory 6G Subnetworks",
    "url": "http://arxiv.org/abs/2505.05967v1",
    "arxiv_id": "2505.05967v1",
    "authors": [
      "Uyoata E. Uyoata",
      "Gilberto Berardinelli",
      "Ramoni Adeogun"
    ],
    "published": "2025-05-09T11:39:18+00:00",
    "summary": "In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal \"Genie\" approach by only 5%."
  },
  {
    "title": "Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication",
    "url": "http://arxiv.org/abs/2505.05956v1",
    "arxiv_id": "2505.05956v1",
    "authors": [
      "Xiyu Wang",
      "Gilberto Berardinelli",
      "Hei Victor Cheng",
      "Petar Popovski",
      "Ramoni Adeogun"
    ],
    "published": "2025-05-09T11:07:29+00:00",
    "summary": "Mobile users are prone to experience beam failure due to beam drifting in millimeter wave (mmWave) communications. Sensing can help alleviate beam drifting with timely beam changes and low overhead since it does not need user feedback. This work studies the problem of optimizing sensing-aided communication by dynamically managing beams allocated to mobile users. A multi-beam scheme is introduced, which allocates multiple beams to the users that need an update on the angle of departure (AoD) estimates and a single beam to the users that have satisfied AoD estimation precision. A deep reinforcement learning (DRL) assisted method is developed to optimize the beam allocation policy, relying only upon the sensing echoes. For comparison, a heuristic AoD-based method using approximated Cram\\'er-Rao lower bound (CRLB) for allocation is also presented. Both methods require neither user feedback nor prior state evolution information. Results show that the DRL-assisted method achieves a considerable gain in throughput than the conventional beam sweeping method and the AoD-based method, and it is robust to different user speeds."
  },
  {
    "title": "Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency",
    "url": "http://arxiv.org/abs/2505.05796v1",
    "arxiv_id": "2505.05796v1",
    "authors": [
      "Xinyu Liang",
      "Frits de Nijs",
      "Buser Say",
      "Hao Wang"
    ],
    "published": "2025-05-09T05:23:37+00:00",
    "summary": "Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfort, presents a significant challenge for traditional HVAC systems. These systems often fail to dynamically adjust to real-time changes in electricity market rates or individual comfort preferences, leading to increased energy costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL) Artificial Intelligence framework that optimizes HVAC performance by incorporating real-time user feedback and responding to fluctuating electricity prices. Unlike conventional systems that require predefined information about occupancy or comfort levels, our approach learns and adapts based on ongoing user input. By integrating the occupancy prediction model with reinforcement learning, the system improves operational efficiency and reduces energy costs in line with electricity market dynamics, thereby contributing to demand response initiatives. Through simulations, we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort. This feedback-driven approach ensures personalized comfort control without the need for predefined settings, offering a scalable solution that balances individual preferences with economic and environmental goals."
  },
  {
    "title": "Multi-Agent Systems for Robotic Autonomy with LLMs",
    "url": "http://arxiv.org/abs/2505.05762v1",
    "arxiv_id": "2505.05762v1",
    "authors": [
      "Junhong Chen",
      "Ziqi Yang",
      "Haoyuan G Xu",
      "Dandan Zhang",
      "George Mylonas"
    ],
    "published": "2025-05-09T03:52:37+00:00",
    "summary": "Since the advent of Large Language Models (LLMs), various research based on such models have maintained significant academic attention and impact, especially in AI and robotics. In this paper, we propose a multi-agent framework with LLMs to construct an integrated system for robotic task analysis, mechanical design, and path generation. The framework includes three core agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer. Outputs are formatted as multimodal results, such as code files or technical reports, for stronger understandability and usability. To evaluate generalizability comparatively, we conducted experiments with models from both GPT and DeepSeek. Results demonstrate that the proposed system can design feasible robots with control strategies when appropriate task inputs are provided, exhibiting substantial potential for enhancing the efficiency and accessibility of robotic system development in research and industrial applications."
  },
  {
    "title": "Towards Embodiment Scaling Laws in Robot Locomotion",
    "url": "http://arxiv.org/abs/2505.05753v1",
    "arxiv_id": "2505.05753v1",
    "authors": [
      "Bo Ai",
      "Liu Dai",
      "Nico Bohlinger",
      "Dichen Li",
      "Tongzhou Mu",
      "Zhanxin Wu",
      "K. Fay",
      "Henrik I. Christensen",
      "Jan Peters",
      "Hao Su"
    ],
    "published": "2025-05-09T03:25:43+00:00",
    "summary": "Developing generalist agents that can operate across diverse tasks, environments, and physical embodiments is a grand challenge in robotics and artificial intelligence. In this work, we focus on the axis of embodiment and investigate embodiment scaling laws$\\unicode{x2013}$the hypothesis that increasing the number of training embodiments improves generalization to unseen ones. Using robot locomotion as a test bed, we procedurally generate a dataset of $\\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and hexapods, and train generalist policies capable of handling diverse observation and action spaces on random subsets. We find that increasing the number of training embodiments improves generalization to unseen ones, and scaling embodiments is more effective in enabling embodiment-level generalization than scaling data on small, fixed sets of embodiments. Notably, our best policy, trained on the full dataset, zero-shot transfers to novel embodiments in the real world, such as Unitree Go2 and H1. These results represent a step toward general embodied intelligence, with potential relevance to adaptive control for configurable robots, co-design of morphology and control, and beyond."
  },
  {
    "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications",
    "url": "http://arxiv.org/abs/2505.05736v1",
    "arxiv_id": "2505.05736v1",
    "authors": [
      "Da Wu",
      "Zhanliang Wang",
      "Quan Nguyen",
      "Zhuoran Xu",
      "Kai Wang"
    ],
    "published": "2025-05-09T02:28:41+00:00",
    "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization."
  },
  {
    "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
    "url": "http://arxiv.org/abs/2505.05704v1",
    "arxiv_id": "2505.05704v1",
    "authors": [
      "Julia Shuieh",
      "Prasann Singhal",
      "Apaar Shanker",
      "John Heyer",
      "George Pu",
      "Samuel Denton"
    ],
    "published": "2025-05-09T00:39:43+00:00",
    "summary": "Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other \"shortcut\" features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations."
  },
  {
    "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.05701v1",
    "arxiv_id": "2505.05701v1",
    "authors": [
      "Jongchan Park",
      "Mingyu Park",
      "Donghwan Lee"
    ],
    "published": "2025-05-09T00:26:01+00:00",
    "summary": "Offline reinforcement learning (RL) aims to learn a policy from a static dataset without further interactions with the environment. Collecting sufficiently large datasets for offline RL is exhausting since this data collection requires colossus interactions with environments and becomes tricky when the interaction with the environment is restricted. Hence, how an agent learns the best policy with a minimal static dataset is a crucial issue in offline RL, similar to the sample efficiency problem in online RL. In this paper, we propose a simple yet effective plug-and-play pretraining method to initialize a feature of a $Q$-network to enhance data efficiency in offline RL. Specifically, we introduce a shared $Q$-network structure that outputs predictions of the next state and $Q$-value. We pretrain the shared $Q$-network through a supervised regression task that predicts a next state and trains the shared $Q$-network using diverse offline RL methods. Through extensive experiments, we empirically demonstrate that our method enhances the performance of existing popular offline RL methods on the D4RL, Robomimic and V-D4RL benchmarks. Furthermore, we show that our method significantly boosts data-efficient offline RL across various data qualities and data distributions trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of the dataset outperforms standard algorithms even with full datasets."
  },
  {
    "title": "On Corruption-Robustness in Performative Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.05609v1",
    "arxiv_id": "2505.05609v1",
    "authors": [
      "Vasilis Pollatos",
      "Debmalya Mandal",
      "Goran Radanovic"
    ],
    "published": "2025-05-08T19:37:35+00:00",
    "summary": "In performative Reinforcement Learning (RL), an agent faces a policy-dependent environment: the reward and transition functions depend on the agent's policy. Prior work on performative RL has studied the convergence of repeated retraining approaches to a performatively stable policy. In the finite sample regime, these approaches repeatedly solve for a saddle point of a convex-concave objective, which estimates the Lagrangian of a regularized version of the reinforcement learning problem. In this paper, we aim to extend such repeated retraining approaches, enabling them to operate under corrupted data. More specifically, we consider Huber's $\\epsilon$-contamination model, where an $\\epsilon$ fraction of data points is corrupted by arbitrary adversarial noise. We propose a repeated retraining approach based on convex-concave optimization under corrupted gradients and a novel problem-specific robust mean estimator for the gradients. We prove that our approach exhibits last-iterate convergence to an approximately stable policy, with the approximation error linear in $\\sqrt{\\epsilon}$. We experimentally demonstrate the importance of accounting for corruption in performative RL."
  },
  {
    "title": "3D Scene Generation: A Survey",
    "url": "http://arxiv.org/abs/2505.05474v1",
    "arxiv_id": "2505.05474v1",
    "authors": [
      "Beichen Wen",
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ],
    "published": "2025-05-08T17:59:54+00:00",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation."
  },
  {
    "title": "Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars",
    "url": "http://arxiv.org/abs/2505.05548v1",
    "arxiv_id": "2505.05548v1",
    "authors": [
      "Eric Squires",
      "Phillip Odom",
      "Zsolt Kira"
    ],
    "published": "2025-05-08T17:59:00+00:00",
    "summary": "Reinforcement Learning (RL) has enabled vast performance improvements for robotics systems. To achieve these results though, the agent often must randomly explore the environment, which for safety critical systems presents a significant challenge. Barrier functions can solve this challenge by enabling an override that approximates the RL control input as closely as possible without violating a safety constraint. Unfortunately, this override can be computationally intractable in cases where the dynamics are not convex in the control input or when time is discrete, as is often the case when training RL systems. We therefore consider these cases, developing novel barrier functions for two non-convex systems (fixed wing aircraft and self-driving cars performing lane merging with adaptive cruise control) in discrete time. Although solving for an online and optimal override is in general intractable when the dynamics are nonconvex in the control input, we investigate approximate solutions, finding that these approximations enable performance commensurate with baseline RL methods with zero safety violations. In particular, even without attempting to solve for the optimal override at all, performance is still competitive with baseline RL performance. We discuss the tradeoffs of the approximate override solutions including performance and computational tractability."
  },
  {
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "url": "http://arxiv.org/abs/2505.05470v1",
    "arxiv_id": "2505.05470v1",
    "authors": [
      "Jie Liu",
      "Gongye Liu",
      "Jiajun Liang",
      "Yangguang Li",
      "Jiaheng Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Wanli Ouyang"
    ],
    "published": "2025-05-08T17:58:45+00:00",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy improves from $59\\%$ to $92\\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments."
  },
  {
    "title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles",
    "url": "http://arxiv.org/abs/2505.05452v1",
    "arxiv_id": "2505.05452v1",
    "authors": [
      "Pouria Behnoudfar",
      "Nan Chen"
    ],
    "published": "2025-05-08T17:43:35+00:00",
    "summary": "Machine learning has become a powerful tool for enhancing data assimilation. While supervised learning remains the standard method, reinforcement learning (RL) offers unique advantages through its sequential decision-making framework, which naturally fits the iterative nature of data assimilation by dynamically balancing model forecasts with observations. We develop RL-DAUNCE, a new RL-based method that enhances data assimilation with physical constraints through three key aspects. First, RL-DAUNCE inherits the computational efficiency of machine learning while it uniquely structures its agents to mirror ensemble members in conventional data assimilation methods. Second, RL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble members, moving beyond simple mean-state optimization. Third, RL-DAUNCE's ensemble-as-agents design facilitates the enforcement of physical constraints during the assimilation process, which is crucial to improving the state estimation and subsequent forecasting. A primal-dual optimization strategy is developed to enforce constraints, which dynamically penalizes the reward function to ensure constraint satisfaction throughout the learning process. Also, state variable bounds are respected by constraining the RL action space. Together, these features ensure physical consistency without sacrificing efficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an intermittent atmospheric phenomenon characterized by strongly non-Gaussian features and multiple physical constraints. RL-DAUNCE outperforms the standard ensemble Kalman filter (EnKF), which fails catastrophically due to the violation of physical constraints. Notably, RL-DAUNCE matches the performance of constrained EnKF, particularly in recovering intermittent signals, capturing extreme events, and quantifying uncertainties, while requiring substantially less computational effort."
  },
  {
    "title": "Reasoning Models Don't Always Say What They Think",
    "url": "http://arxiv.org/abs/2505.05410v1",
    "arxiv_id": "2505.05410v1",
    "authors": [
      "Yanda Chen",
      "Joe Benton",
      "Ansh Radhakrishnan",
      "Jonathan Uesato",
      "Carson Denison",
      "John Schulman",
      "Arushi Somani",
      "Peter Hase",
      "Misha Wagner",
      "Fabien Roger",
      "Vlad Mikulik",
      "Samuel R. Bowman",
      "Jan Leike",
      "Jared Kaplan",
      "Ethan Perez"
    ],
    "published": "2025-05-08T16:51:43+00:00",
    "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors."
  },
  {
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "url": "http://arxiv.org/abs/2505.05315v1",
    "arxiv_id": "2505.05315v1",
    "authors": [
      "Yuhui Xu",
      "Hanze Dong",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Caiming Xiong"
    ],
    "published": "2025-05-08T15:01:06+00:00",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale."
  },
  {
    "title": "Repair Crew Routing for Infrastructure Network Restoration under Incomplete Information",
    "url": "http://arxiv.org/abs/2505.05297v1",
    "arxiv_id": "2505.05297v1",
    "authors": [
      "Subhojit Biswas",
      "Bahar Cavdar",
      "Joseph Geunes"
    ],
    "published": "2025-05-08T14:39:45+00:00",
    "summary": "This paper considers a disrupted infrastructure network where the repair crew knows the locations of service outages but not the locations of actual faults. Our goal is to determine a route for a single crew to visit and repair the disruptions to restore service with minimum negative impact. We call this problem the Traveling Repairman Network Restoration Problem (TRNRP). This problem presents strong computational challenges due to the combinatorial nature of the decisions, inter-dependencies within the underlying infrastructure network, and incomplete information. Considering the dynamic nature of the decisions as a result of dynamic information revelation on the status of the nodes, we model this problem as a finite-horizon Markov decision process. Our solution approach uses value approximation based on reinforcement learning, which is strengthened by structural results that identify a set of suboptimal moves. In addition, we propose state aggregation methods to reduce the size of the state space. We perform extensive computational studies to characterize the performance of our solution methods under different parameter settings and to compare them with benchmark solution approaches."
  },
  {
    "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation",
    "url": "http://arxiv.org/abs/2505.05287v1",
    "arxiv_id": "2505.05287v1",
    "authors": [
      "Zechu Li",
      "Yufeng Jin",
      "Daniel Ordonez Apraez",
      "Claudio Semini",
      "Puze Liu",
      "Georgia Chalvatzaki"
    ],
    "published": "2025-05-08T14:29:00+00:00",
    "summary": "Humans naturally exhibit bilateral symmetry in their gross manipulation skills, effortlessly mirroring simple actions between left and right hands. Bimanual robots-which also feature bilateral symmetry-should similarly exploit this property to perform tasks with either hand. Unlike humans, who often favor a dominant hand for fine dexterous skills, robots should ideally execute ambidextrous manipulation with equal proficiency. To this end, we introduce SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for ambidextrous bi-manipulation that leverages the robot's inherent bilateral symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation tasks into per-hand subtasks and trains dedicated policies for each. By exploiting bilateral symmetry via equivariant neural networks, experience from one arm is inherently leveraged by the opposite arm. We then distill the subtask policies into a global ambidextrous policy that is independent of the hand-task assignment. We evaluate SYMDEX on six challenging simulated manipulation tasks and demonstrate successful real-world deployment on two of them. Our approach strongly outperforms baselines on complex task in which the left and right hands perform different roles. We further demonstrate SYMDEX's scalability by extending it to a four-arm manipulation setup, where our symmetry-aware policies enable effective multi-arm collaboration and coordination. Our results highlight how structural symmetry as inductive bias in policy learning enhances sample efficiency, robustness, and generalization across diverse dexterous manipulation tasks."
  },
  {
    "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration",
    "url": "http://arxiv.org/abs/2505.05262v1",
    "arxiv_id": "2505.05262v1",
    "authors": [
      "Andreas Kontogiannis",
      "Konstantinos Papathanasiou",
      "Yi Shen",
      "Giorgos Stamou",
      "Michael M. Zavlanos",
      "George Vouros"
    ],
    "published": "2025-05-08T14:07:20+00:00",
    "summary": "Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE algorithm. In SMPE, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks."
  },
  {
    "title": "High Altitude Platform-Based Caching and Multicasting for Rural Connectivity",
    "url": "http://arxiv.org/abs/2505.05251v1",
    "arxiv_id": "2505.05251v1",
    "authors": [
      "Yongqiang Zhang",
      "Mustafa A. Kishk",
      "Mohamed-Slim Alouini"
    ],
    "published": "2025-05-08T13:56:20+00:00",
    "summary": "Providing efficient and reliable content delivery in rural areas remains a significant challenge due to the lack of communication infrastructure. To bridge the digital divide, this paper investigates the potential of leveraging multiple high-altitude platforms (HAPs) for energy-efficient content delivery in wide rural regions. Each caching-enabled HAP is equipped with both Free-Space Optical (FSO) transceivers for backhaul links and Radio Frequency (RF) antenna arrays for access links. To further enhance network efficiency, we consider a network coding-based multicasting scheme, where different types of content are treated as distinct multicast sessions. With the objective of minimizing long-term power cost, we propose a hierarchical framework that integrates deep reinforcement learn-ing (DRL) and convex optimization to jointly optimize dynamic caching strategies and resource allocation across the network. Simulation results demonstrate that our approach significantly reduces power cost compared to several baseline approaches, providing a practical solution for improving rural connectivity."
  },
  {
    "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation",
    "url": "http://arxiv.org/abs/2505.05235v1",
    "arxiv_id": "2505.05235v1",
    "authors": [
      "Luca Marzari",
      "Isabella Mastroeni",
      "Alessandro Farinelli"
    ],
    "published": "2025-05-08T13:29:46+00:00",
    "summary": "Traditional methods for formal verification (FV) of deep neural networks (DNNs) are constrained by a binary encoding of safety properties, where a model is classified as either safe or unsafe (robust or not robust). This binary encoding fails to capture the nuanced safety levels within a model, often resulting in either overly restrictive or too permissive requirements. In this paper, we introduce a novel problem formulation called Abstract DNN-Verification, which verifies a hierarchical structure of unsafe outputs, providing a more granular analysis of the safety aspect for a given DNN. Crucially, by leveraging abstract interpretation and reasoning about output reachable sets, our approach enables assessing multiple safety levels during the FV process, requiring the same (in the worst case) or even potentially less computational effort than the traditional binary verification approach. Specifically, we demonstrate how this formulation allows rank adversarial inputs according to their abstract safety level violation, offering a more detailed evaluation of the model's safety and robustness. Our contributions include a theoretical exploration of the relationship between our novel abstract safety formulation and existing approaches that employ abstract interpretation for robustness verification, complexity analysis of the novel problem introduced, and an empirical evaluation considering both a complex deep reinforcement learning task (based on Habitat 3.0) and standard DNN-Verification benchmarks."
  },
  {
    "title": "Adaptive Biased User Scheduling for Heterogeneous Wireless Federate Learning Network",
    "url": "http://arxiv.org/abs/2505.05231v1",
    "arxiv_id": "2505.05231v1",
    "authors": [
      "Changxiang Wu",
      "Yijing Ren",
      "Daniel K. C. So",
      "Jie Tang"
    ],
    "published": "2025-05-08T13:26:09+00:00",
    "summary": "Federated Learning (FL) has revolutionized collaborative model training in distributed networks, prioritizing data privacy and communication efficiency. This paper investigates efficient deployment of FL in wireless heterogeneous networks, focusing on strategies to accelerate convergence despite stragglers. The primary objective is to minimize long-term convergence wall-clock time through optimized user scheduling and resource allocation. While stragglers may introduce delays in a single round, their inclusion can expedite subsequent rounds, particularly when they possess critical information. Moreover, balancing single-round duration with the number of cumulative rounds, compounded by dynamic training and transmission conditions, necessitates a novel approach beyond conventional optimization solutions. To tackle these challenges, convergence analysis with respect to adaptive and biased scheduling is derived. Then, by factoring in real-time system and statistical information, including diverse energy constraints and users' energy harvesting capabilities, a deep reinforcement learning approach, empowered by proximal policy optimization, is employed to adaptively select user sets. For the scheduled users, Lagrangian decomposition is applied to optimize local resource utilization, further enhancing system efficiency. Simulation results validate the effectiveness and robustness of the proposed framework for various FL tasks, demonstrating reduced task time compared to existing benchmarks under various settings."
  },
  {
    "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving",
    "url": "http://arxiv.org/abs/2505.05223v1",
    "arxiv_id": "2505.05223v1",
    "authors": [
      "Hendrik Surmann",
      "Jorge de Heuvel",
      "Maren Bennewitz"
    ],
    "published": "2025-05-08T13:16:37+00:00",
    "summary": "Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion."
  },
  {
    "title": "Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach",
    "url": "http://arxiv.org/abs/2505.05126v1",
    "arxiv_id": "2505.05126v1",
    "authors": [
      "Xuyang Chen",
      "Keyu Yan",
      "Lin Zhao"
    ],
    "published": "2025-05-08T10:57:28+00:00",
    "summary": "Offline reinforcement learning (RL) aims to learn decision-making policies from fixed datasets without online interactions, providing a practical solution where online data collection is expensive or risky. However, offline RL often suffers from distribution shift, resulting in inaccurate evaluation and substantial overestimation on out-of-distribution (OOD) actions. To address this, existing approaches incorporate conservatism by indiscriminately discouraging all OOD actions, thereby hindering the agent's ability to generalize and exploit beneficial ones. In this paper, we propose Advantage-based Diffusion Actor-Critic (ADAC), a novel method that systematically evaluates OOD actions using the batch-optimal value function. Based on this evaluation, ADAC defines an advantage function to modulate the Q-function update, enabling more precise assessment of OOD action quality. We design a custom PointMaze environment and collect datasets to visually reveal that advantage modulation can effectively identify and select superior OOD actions. Extensive experiments show that ADAC achieves state-of-the-art performance on almost all tasks in the D4RL benchmark, with particularly clear margins on the more challenging tasks."
  },
  {
    "title": "USPR: Learning a Unified Solver for Profiled Routing",
    "url": "http://arxiv.org/abs/2505.05119v1",
    "arxiv_id": "2505.05119v1",
    "authors": [
      "Chuanbo Hua",
      "Federico Berto",
      "Zhikai Zhao",
      "Jiwoo Son",
      "Changhyun Kwon",
      "Jinkyoo Park"
    ],
    "published": "2025-05-08T10:42:57+00:00",
    "summary": "The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by incorporating vehicle-client-specific preferences and constraints, reflecting real-world requirements such as zone restrictions and service-level preferences. While recent reinforcement learning (RL) solvers have shown promise, they require retraining for each new profile distribution, suffer from poor representation ability, and struggle to generalize to out-of-distribution instances. In this paper, we address these limitations by introducing USPR (Unified Solver for Profiled Routing), a novel framework that natively handles arbitrary profile types. USPR introduces three key innovations: (i) Profile Embeddings (PE) to encode any combination of profile types; (ii) Multi-Head Profiled Attention (MHPA), an attention mechanism that models rich interactions between vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which dynamically adjusts decoder logits using profile scores to improve generalization. Empirical results on diverse PVRP benchmarks demonstrate that USPR achieves state-of-the-art results among learning-based methods while offering significant gains in flexibility and computational efficiency. We make our source code publicly available to foster future research at https://github.com/ai4co/uspr."
  },
  {
    "title": "Multi-agent Embodied AI: Advances and Future Directions",
    "url": "http://arxiv.org/abs/2505.05108v1",
    "arxiv_id": "2505.05108v1",
    "authors": [
      "Zhaohan Feng",
      "Ruiqi Xue",
      "Lei Yuan",
      "Yang Yu",
      "Ning Ding",
      "Meiqin Liu",
      "Bingzhao Gao",
      "Jian Sun",
      "Gang Wang"
    ],
    "published": "2025-05-08T10:13:53+00:00",
    "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field."
  },
  {
    "title": "A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows",
    "url": "http://arxiv.org/abs/2505.05525v1",
    "arxiv_id": "2505.05525v1",
    "authors": [
      "Selim Mecanna",
      "Aurore Loisy",
      "Christophe Eloy"
    ],
    "published": "2025-05-08T09:17:26+00:00",
    "summary": "Navigating in a fluid flow while being carried by it, using only information accessible from on-board sensors, is a problem commonly faced by small planktonic organisms. It is also directly relevant to autonomous robots deployed in the oceans. In the last ten years, the fluid mechanics community has widely adopted reinforcement learning, often in the form of its simplest implementations, to address this challenge. But it is unclear how good are the strategies learned by these algorithms. In this paper, we perform a quantitative assessment of reinforcement learning methods applied to navigation in partially observable flows. We first introduce a well-posed problem of directional navigation for which a quasi-optimal policy is known analytically. We then report on the poor performance and robustness of commonly used algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and two-dimensional turbulence. We show that they are vastly surpassed by PPO (Proximal Policy Optimization), a more advanced algorithm that has established dominance across a wide range of benchmarks in the reinforcement learning community. In particular, our custom implementation of PPO matches the theoretical quasi-optimal performance in turbulent flow and does so in a robust manner. Reaching this result required the use of several additional techniques, such as vectorized environments and generalized advantage estimation, as well as hyperparameter optimization. This study demonstrates the importance of algorithm selection, implementation details, and fine-tuning for discovering truly smart autonomous navigation strategies in complex flows."
  },
  {
    "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search",
    "url": "http://arxiv.org/abs/2505.05059v1",
    "arxiv_id": "2505.05059v1",
    "authors": [
      "Sandro Junior Della Rovere",
      "Davide Basso",
      "Luca Bortolussi",
      "Mirjana Videnovic-Misic",
      "Husni Habal"
    ],
    "published": "2025-05-08T08:50:32+00:00",
    "summary": "The layout of analog ICs requires making complex trade-offs, while addressing device physics and variability of the circuits. This makes full automation with learning-based solutions hard to achieve. However, reinforcement learning (RL) has recently reached significant results, particularly in solving the floorplanning problem. This paper presents a hybrid method that combines RL with a beam (BS) strategy. The BS algorithm enhances the agent's inference process, allowing for the generation of flexible floorplans by accomodating various objective weightings, and addressing congestion without without the need for policy retraining or fine-tuning. Moreover, the RL agent's generalization ability stays intact, along with its efficient handling of circuit features and constraints. Experimental results show approx. 5-85% improvement in area, dead space and half-perimeter wire length compared to a standard RL application, along with higher rewards for the agent. Moreover, performance and efficiency align closely with those of existing state-of-the-art techniques."
  },
  {
    "title": "LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving Cloud-Device Collaboration",
    "url": "http://arxiv.org/abs/2505.05031v1",
    "arxiv_id": "2505.05031v1",
    "authors": [
      "Yingyi Zhang",
      "Pengyue Jia",
      "Xianneng Li",
      "Derong Xu",
      "Maolin Wang",
      "Yichao Wang",
      "Zhaocheng Du",
      "Huifeng Guo",
      "Yong Liu",
      "Ruiming Tang",
      "Xiangyu Zhao"
    ],
    "published": "2025-05-08T08:06:34+00:00",
    "summary": "Cloud-device collaboration leverages on-cloud Large Language Models (LLMs) for handling public user queries and on-device Small Language Models (SLMs) for processing private user data, collectively forming a powerful and privacy-preserving solution. However, existing approaches often fail to fully leverage the scalable problem-solving capabilities of on-cloud LLMs while underutilizing the advantage of on-device SLMs in accessing and processing personalized data. This leads to two interconnected issues: 1) Limited utilization of the problem-solving capabilities of on-cloud LLMs, which fail to align with personalized user-task needs, and 2) Inadequate integration of user data into on-device SLM responses, resulting in mismatches in contextual user information.   In this paper, we propose a Leader-Subordinate Retrieval framework for Privacy-preserving cloud-device collaboration (LSRP), a novel solution that bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM through a dynamic selection of task-specific leader strategies named as user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the data advantages of on-device SLMs through small model feedback Direct Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the on-device SLM. Experiments on two datasets demonstrate that LSRP consistently outperforms state-of-the-art baselines, significantly improving question-answer relevance and personalization, while preserving user privacy through efficient on-device retrieval. Our code is available at: https://github.com/Zhang-Yingyi/LSRP."
  },
  {
    "title": "Diffusion-enabled Secure Semantic Communication Against Eavesdropping",
    "url": "http://arxiv.org/abs/2505.05018v1",
    "arxiv_id": "2505.05018v1",
    "authors": [
      "Boxiang He",
      "Zihan Chen",
      "Fanggang Wang",
      "Shilian Wang",
      "Zhijin Qin",
      "Tony Q. S. Quek"
    ],
    "published": "2025-05-08T07:47:52+00:00",
    "summary": "In this paper, AN is introduced into semantic communication systems for the first time to prevent semantic eavesdropping. However, the introduction of AN also poses challenges for the legitimate receiver in extracting semantic information. Recently, denoising diffusion probabilistic models (DDPM) have demonstrated their powerful capabilities in generating multimedia content. Here, the paired pluggable modules are carefully designed using DDPM. Specifically, the pluggable encryption module generates AN and adds it to the output of the semantic transmitter, while the pluggable decryption module before semantic receiver uses DDPM to generate the detailed semantic information by removing both AN and the channel noise. In the scenario where the transmitter lacks eavesdropper's knowledge, the artificial Gaussian noise (AGN) is used as AN. We first model a power allocation optimization problem to determine the power of AGN, in which the objective is to minimize the weighted sum of data reconstruction error of legal link, the mutual information of illegal link, and the channel input distortion. Then, a deep reinforcement learning framework using deep deterministic policy gradient is proposed to solve the optimization problem. In the scenario where the transmitter is aware of the eavesdropper's knowledge, we propose an AN generation method based on adversarial residual networks (ARN). Unlike the previous scenario, the mutual information term in the objective function is replaced by the confidence of eavesdropper correctly   retrieving private information. The adversarial residual network is then trained to minimize the modified objective function. The simulation results show that the diffusion-enabled pluggable encryption module prevents semantic eavesdropping while the pluggable decryption module achieves the high-quality semantic communication."
  },
  {
    "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "url": "http://arxiv.org/abs/2505.04993v1",
    "arxiv_id": "2505.04993v1",
    "authors": [
      "Zhuocheng Gong",
      "Jian Guan",
      "Wei Wu",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ],
    "published": "2025-05-08T06:59:06+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for the responsible deployment of powerful LLMs."
  },
  {
    "title": "CPP-DIP: Multi-objective Coverage Path Planning for MAVs in Dispersed and Irregular Plantations",
    "url": "http://arxiv.org/abs/2505.04989v1",
    "arxiv_id": "2505.04989v1",
    "authors": [
      "Weijie Kuang",
      "Hann Woei Ho",
      "Ye Zhou"
    ],
    "published": "2025-05-08T06:52:22+00:00",
    "summary": "Coverage Path Planning (CPP) is vital in precision agriculture to improve efficiency and resource utilization. In irregular and dispersed plantations, traditional grid-based CPP often causes redundant coverage over non-vegetated areas, leading to waste and pollution. To overcome these limitations, we propose CPP-DIP, a multi-objective CPP framework designed for Micro Air Vehicles (MAVs). The framework transforms the CPP task into a Traveling Salesman Problem (TSP) and optimizes flight paths by minimizing travel distance, turning angles, and intersection counts. Unlike conventional approaches, our method does not rely on GPS-based environmental modeling. Instead, it uses aerial imagery and a Histogram of Oriented Gradients (HOG)-based approach to detect trees and extract image coordinates. A density-aware waypoint strategy is applied: Kernel Density Estimation (KDE) is used to reduce redundant waypoints in dense regions, while a greedy algorithm ensures complete coverage in sparse areas. To verify the generality of the framework, we solve the resulting TSP using three different methods: Greedy Heuristic Insertion (GHI), Ant Colony Optimization (ACO), and Monte Carlo Reinforcement Learning (MCRL). Then an object-based optimization is applied to further refine the resulting path. Additionally, CPP-DIP integrates ForaNav, our insect-inspired navigation method, for accurate tree localization and tracking. The experimental results show that MCRL offers a balanced solution, reducing the travel distance by 16.9 % compared to ACO while maintaining a similar performance to GHI. It also improves path smoothness by reducing turning angles by 28.3 % and 59.9 % relative to ACO and GHI, respectively, and effectively eliminates intersections. These results confirm the robustness and effectiveness of CPP-DIP in different TSP solvers."
  },
  {
    "title": "Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks",
    "url": "http://arxiv.org/abs/2505.04981v1",
    "arxiv_id": "2505.04981v1",
    "authors": [
      "Zhifeng Hu",
      "Chong Han"
    ],
    "published": "2025-05-08T06:36:17+00:00",
    "summary": "Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible topologies and ultra-high data rates are expected to empower numerous applications in security surveillance, disaster response, and environmental monitoring, among others. However, the dynamic topologies hinder the efficient long-term joint power and antenna array resource allocation for THz links among UAVs. Furthermore, the continuous nature of power and the discrete nature of antennas cause this joint resource allocation problem to be a mixed-integer nonlinear programming (MINLP) problem with non-convexity and NP-hardness. Inspired by recent rapid advancements in deep reinforcement learning (DRL), a graph neural network (GNN) aided DRL algorithm for resource allocation in the dynamic THz UAV network with an emphasis on self-node features (GLOVE) is proposed in this paper, with the aim of resource efficiency (RE) maximization. When training the allocation policy for each UAV, GLOVE learns the relationship between this UAV and its neighboring UAVs via GNN, while also emphasizing the important self-node features of this UAV. In addition, a multi-task structure is leveraged by GLOVE to cooperatively train resource allocation decisions for the power and sub-arrays of all UAVs. Experimental results illustrate that GLOVE outperforms benchmark schemes in terms of the highest RE and the lowest latency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE maintains zero packet loss during the entire training process, demonstrating its better robustness under the highly dynamic THz UAV network."
  },
  {
    "title": "An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects",
    "url": "http://arxiv.org/abs/2505.04962v1",
    "arxiv_id": "2505.04962v1",
    "authors": [
      "Utsav Rai",
      "Hardik Mehta",
      "Vismay Vakharia",
      "Aditya Choudhary",
      "Amit Parmar",
      "Rolif Lima",
      "Kaushik Das"
    ],
    "published": "2025-05-08T05:43:31+00:00",
    "summary": "The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm."
  },
  {
    "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators",
    "url": "http://arxiv.org/abs/2505.04961v1",
    "arxiv_id": "2505.04961v1",
    "authors": [
      "Ziyu Zhang",
      "Sergey Bashkirov",
      "Dun Yang",
      "Michael Taylor",
      "Xue Bin Peng"
    ],
    "published": "2025-05-08T05:42:33+00:00",
    "summary": "Multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. The proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. Results are best visualized through https://youtu.be/rz8BYCE9E2w."
  },
  {
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
    "url": "http://arxiv.org/abs/2505.04921v1",
    "arxiv_id": "2505.04921v1",
    "authors": [
      "Yunxin Li",
      "Zhenyu Liu",
      "Zitao Li",
      "Xuanyu Zhang",
      "Zhenran Xu",
      "Xinyu Chen",
      "Haoyuan Shi",
      "Shenyuan Jiang",
      "Xintong Wang",
      "Jifang Wang",
      "Shouzheng Huang",
      "Xinping Zhao",
      "Borui Jiang",
      "Lanqing Hong",
      "Longyue Wang",
      "Zhuotao Tian",
      "Baoxing Huai",
      "Wenhan Luo",
      "Weihua Luo",
      "Zheng Zhang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "published": "2025-05-08T03:35:23+00:00",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments."
  },
  {
    "title": "Large Language Models are Autonomous Cyber Defenders",
    "url": "http://arxiv.org/abs/2505.04843v1",
    "arxiv_id": "2505.04843v1",
    "authors": [
      "Sebasti\u00e1n R. Castro",
      "Roberto Campbell",
      "Nancy Lau",
      "Octavio Villalobos",
      "Jiaqi Duan",
      "Alvaro A. Cardenas"
    ],
    "published": "2025-05-07T22:42:37+00:00",
    "summary": "Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents."
  },
  {
    "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers",
    "url": "http://arxiv.org/abs/2505.04842v1",
    "arxiv_id": "2505.04842v1",
    "authors": [
      "Kusha Sareen",
      "Morgane M Moss",
      "Alessandro Sordoni",
      "Rishabh Agarwal",
      "Arian Hosseini"
    ],
    "published": "2025-05-07T22:41:26+00:00",
    "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL$^V$ boosts MATH accuracy by over 20\\% with parallel sampling and enables $8-32\\times$ efficient test-time compute scaling compared to the base RL method. RL$^V$ also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves $1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model."
  },
  {
    "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
    "url": "http://arxiv.org/abs/2505.04831v1",
    "arxiv_id": "2505.04831v1",
    "authors": [
      "Nicholas Pfaff",
      "Hongkai Dai",
      "Sergey Zakharov",
      "Shun Iwase",
      "Russ Tedrake"
    ],
    "published": "2025-05-07T22:07:42+00:00",
    "summary": "Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: https://steerable-scene-generation.github.io/"
  },
  {
    "title": "Is there Value in Reinforcement Learning?",
    "url": "http://arxiv.org/abs/2505.04822v1",
    "arxiv_id": "2505.04822v1",
    "authors": [
      "Lior Fox",
      "Yonatan Loewenstein"
    ],
    "published": "2025-05-07T21:50:27+00:00",
    "summary": "Action-values play a central role in popular Reinforcement Learing (RL) models of behavior. Yet, the idea that action-values are explicitly represented has been extensively debated. Critics had therefore repeatedly suggested that policy-gradient (PG) models should be favored over value-based (VB) ones, as a potential solution for this dilemma. Here we argue that this solution is unsatisfying. This is because PG methods are not, in fact, \"Value-free\" -- while they do not rely on an explicit representation of Value for acting (stimulus-response mapping), they do require it for learning. Hence, switching to PG models is, per se, insufficient for eliminating Value from models of behavior. More broadly, the requirement for a representation of Value stems from the underlying assumptions regarding the optimization objective posed by the standard RL framework, not from the particular algorithm chosen to solve it. Previous studies mostly took these standard RL assumptions for granted, as part of their conceptualization or problem modeling, while debating the different methods used to optimize it (i.e., PG or VB). We propose that, instead, the focus of the debate should shift to critically evaluating the underlying modeling assumptions. Such evaluation is particularly important from an experimental perspective. Indeed, the very notion of Value must be reconsidered when standard assumptions (e.g., risk neutrality, full-observability, Markovian environment, exponential discounting) are relaxed, as is likely in natural settings. Finally, we use the Value debate as a case study to argue in favor of a more nuanced, algorithmic rather than statistical, view of what constitutes \"a model\" in cognitive sciences. Our analysis suggests that besides \"parametric\" statistical complexity, additional aspects such as computational complexity must also be taken into account when evaluating model complexity."
  },
  {
    "title": "Reinforcement Learning-Based Closed-Loop Airfoil Flow Control",
    "url": "http://arxiv.org/abs/2505.04818v1",
    "arxiv_id": "2505.04818v1",
    "authors": [
      "Qiong Liu",
      "Luis Javier Trujillo Corona",
      "Fangjun Shu",
      "Andreas Gross"
    ],
    "published": "2025-05-07T21:38:29+00:00",
    "summary": "We systematically investigated a reinforcement learning (RL)-based closed-loop active flow control strategy to enhance the lift-to-drag ratio of a wing section with an NLF(1)-0115 airfoil at an angle of attack 5 degree. The effects of key control parameters, including actuation location, observed state, reward function, and control update interval, are evaluated at a chord-based Reynolds number of Re=20,000. Results show that all parameters significantly influence control performance, with the update interval playing a particularly critical role. Properly chosen update intervals introduce a broader spectrum of actuation frequencies, enabling more effective interactions with a wider range of flow structures and contributing to improved control effectiveness. The optimally trained RL controller is further evaluated in a three-dimensional numerical setup at the same Reynolds number. Actuation is applied using both spanwise-uniform and spanwise-varying control profiles. The results demonstrate that the pretrained controller, combined with a physics-informed spanwise distribution, achieves substantial performance gains. These findings extend the feasibility and scalability of a pretrained RL-based control strategy to more complex airfoil flows."
  },
  {
    "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.04623v1",
    "arxiv_id": "2505.04623v1",
    "authors": [
      "Zhenghao Xing",
      "Xiaowei Hu",
      "Chi-Wing Fu",
      "Wenhai Wang",
      "Jifeng Dai",
      "Pheng-Ann Heng"
    ],
    "published": "2025-05-07T17:59:49+00:00",
    "summary": "Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research."
  },
  {
    "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.04619v1",
    "arxiv_id": "2505.04619v1",
    "authors": [
      "Abdulaziz Almuzairee",
      "Rohan Patil",
      "Dwait Bhatt",
      "Henrik I. Christensen"
    ],
    "published": "2025-05-07T17:59:28+00:00",
    "summary": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"
  },
  {
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "url": "http://arxiv.org/abs/2505.04588v1",
    "arxiv_id": "2505.04588v1",
    "authors": [
      "Hao Sun",
      "Zile Qiao",
      "Jiayan Guo",
      "Xuanbo Fan",
      "Yingyan Hou",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Yan Zhang"
    ],
    "published": "2025-05-07T17:30:22+00:00",
    "summary": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms."
  },
  {
    "title": "Active Sampling for MRI-based Sequential Decision Making",
    "url": "http://arxiv.org/abs/2505.04586v1",
    "arxiv_id": "2505.04586v1",
    "authors": [
      "Yuning Du",
      "Jingshuai Liu",
      "Rohan Dharmakumar",
      "Sotirios A. Tsaftaris"
    ],
    "published": "2025-05-07T17:27:51+00:00",
    "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling"
  },
  {
    "title": "Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions",
    "url": "http://arxiv.org/abs/2505.04579v1",
    "arxiv_id": "2505.04579v1",
    "authors": [
      "St\u00e9phane Aroca-Ouellette",
      "Miguel Aroca-Ouellette",
      "Katharina von der Wense",
      "Alessandro Roncone"
    ],
    "published": "2025-05-07T17:19:17+00:00",
    "summary": "In collaborative tasks, autonomous agents fall short of humans in their capability to quickly adapt to new and unfamiliar teammates. We posit that a limiting factor for zero-shot coordination is the lack of shared task abstractions, a mechanism humans rely on to implicitly align with teammates. To address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework leveraging hierarchical reinforcement learning to mimic the structured approach humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment, demonstrating statistically significant improvement over existing baselines when paired with both unseen agents and humans, providing better resilience to environmental shifts, and outperforming all state-of-the-art methods."
  },
  {
    "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization",
    "url": "http://arxiv.org/abs/2505.04578v1",
    "arxiv_id": "2505.04578v1",
    "authors": [
      "Wenjun Cao"
    ],
    "published": "2025-05-07T17:18:48+00:00",
    "summary": "Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models."
  },
  {
    "title": "Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions",
    "url": "http://arxiv.org/abs/2505.04553v1",
    "arxiv_id": "2505.04553v1",
    "authors": [
      "Shanyu Han",
      "Yang Liu",
      "Xiang Yu"
    ],
    "published": "2025-05-07T16:31:42+00:00",
    "summary": "We propose a reinforcement learning (RL) framework under a broad class of risk objectives, characterized by convex scoring functions. This class covers many common risk measures, such as variance, Expected Shortfall, entropic Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue, we consider an augmented state space and an auxiliary variable and recast the problem as a two-state optimization problem. We propose a customized Actor-Critic algorithm and establish some theoretical approximation guarantees. A key theoretical contribution is that our results do not require the Markov decision process to be continuous. Additionally, we propose an auxiliary variable sampling method inspired by the alternating minimization algorithm, which is convergent under certain conditions. We validate our approach in simulation experiments with a financial application in statistical arbitrage trading, demonstrating the effectiveness of the algorithm."
  },
  {
    "title": "A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance",
    "url": "http://arxiv.org/abs/2505.04494v1",
    "arxiv_id": "2505.04494v1",
    "authors": [
      "Axel Friedrich Wolter",
      "Tobias Sutter"
    ],
    "published": "2025-05-07T15:18:43+00:00",
    "summary": "We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupation measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy."
  },
  {
    "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation",
    "url": "http://arxiv.org/abs/2505.04424v1",
    "arxiv_id": "2505.04424v1",
    "authors": [
      "Jing Hu",
      "Chengming Feng",
      "Shu Hu",
      "Ming-Ching Chang",
      "Xin Li",
      "Xi Wu",
      "Xin Wang"
    ],
    "published": "2025-05-07T13:57:42+00:00",
    "summary": "Arbitrary style transfer aims to apply the style of any given artistic image to another content image. Still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. Motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer RLMiniStyler. This framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. Furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. Through a series of experiments across image various resolutions, we have validated the advantages of RLMiniStyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. Codes are available at https://github.com/fengxiaoming520/RLMiniStyler."
  },
  {
    "title": "A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs",
    "url": "http://arxiv.org/abs/2505.04401v1",
    "arxiv_id": "2505.04401v1",
    "authors": [
      "Wei Wang",
      "Peizheng Li",
      "Angela Doufexi",
      "Mark A. Beach"
    ],
    "published": "2025-05-07T13:34:12+00:00",
    "summary": "Optimizing discrete phase shifts in large-scale reconfigurable intelligent surfaces (RISs) is challenging due to their non-convex and non-linear nature. In this letter, we propose a heuristic-integrated deep reinforcement learning (DRL) framework that (1) leverages accumulated actions over multiple steps in the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates a greedy algorithm (GA) into each DRL step to refine the state via fine-grained, element-wise optimization of RIS configurations. By learning from GA-included states, the proposed approach effectively addresses RIS optimization within a small DRL action space, demonstrating its capability to optimize phase-shift configurations of large-scale RISs."
  },
  {
    "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
    "url": "http://arxiv.org/abs/2505.04388v1",
    "arxiv_id": "2505.04388v1",
    "authors": [
      "Dario Garcia-Gasulla",
      "Jordi Bayarri-Planas",
      "Ashwin Kumar Gururajan",
      "Enrique Lopez-Cuena",
      "Adrian Tormos",
      "Daniel Hinjos",
      "Pablo Bernabeu-Perez",
      "Anna Arias-Duart",
      "Pablo Agustin Martin-Torres",
      "Marta Gonzalez-Mallo",
      "Sergio Alvarez-Napagao",
      "Eduard Ayguad\u00e9-Parra",
      "Ulises Cort\u00e9s"
    ],
    "published": "2025-05-07T13:13:14+00:00",
    "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare."
  },
  {
    "title": "Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four",
    "url": "http://arxiv.org/abs/2505.04371v1",
    "arxiv_id": "2505.04371v1",
    "authors": [
      "Filipe Santos",
      "Jo\u00e3o Paulo Fernandes",
      "Lu\u00eds Macedo"
    ],
    "published": "2025-05-07T12:44:59+00:00",
    "summary": "Action selection based on flags is a Reinforcement Learning (RL) exploration policy that improves the exploration of the state space through the use of flags, which can identify the most promising actions to take in each state. The quantum counterpart of this exploration policy further improves upon this by taking advantage of a quadratic speedup for sampling flagged actions. This approach has already been successfully employed for the game of Checkers. In this work, we describe the application of this method to the context of Connect Four, in order to study its performance in a different setting, which can lead to a better generalization of the technique. We also kept track of a metric that wasn't taken into account in previous work: the average number of iterations to obtain a flagged action. Since going second is a significant disadvantage in Connect Four, we also had the intent of exploring how this more complex scenario would impact the performance of our approach. The experiments involved training and testing classical and quantum RL agents that played either going first or going second against a Randomized Negamax opponent. The results showed that both flagged exploration policies were clearly superior to a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact sample flagged actions in less iterations. Despite obtaining tagged actions more consistently, the win rates between the classical and quantum versions of the approach were identical, which could be due to the simplicity of the training scenario chosen."
  },
  {
    "title": "Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid",
    "url": "http://arxiv.org/abs/2505.04367v1",
    "arxiv_id": "2505.04367v1",
    "authors": [
      "Stavros Sykiotis"
    ],
    "published": "2025-05-07T12:36:19+00:00",
    "summary": "The global energy landscape is undergoing a profound transformation, often referred to as the energy transition, driven by the urgent need to mitigate climate change, reduce greenhouse gas emissions, and ensure sustainable energy supplies. However, the undoubted complexity of new investments in renewables, as well as the phase out of high CO2-emission energy sources, hampers the pace of the energy transition and raises doubts as to whether new renewable energy sources are capable of solely meeting the climate target goals. This highlights the need to investigate alternative pathways to accelerate the energy transition, by identifying human activity domains with higher/excessive energy demands. Two notable examples where there is room for improvement, in the sense of reducing energy consumption and consequently CO2 emissions, are residential energy consumption and road transport. This dissertation investigates the development of novel Deep Learning techniques to create tools which solve limitations in these two key energy domains. Reduction of residential energy consumption can be achieved by empowering end-users with the user of Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep Reinforcement Learning can tackle road transport decarbonization."
  },
  {
    "title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.04339v1",
    "arxiv_id": "2505.04339v1",
    "authors": [
      "Hao Peng",
      "Xiang Huang",
      "Shuo Sun",
      "Ruitong Zhang",
      "Philip S. Yu"
    ],
    "published": "2025-05-07T11:37:23+00:00",
    "summary": "DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters."
  },
  {
    "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.04317v1",
    "arxiv_id": "2505.04317v1",
    "authors": [
      "Ruize Zhang",
      "Sirui Xiang",
      "Zelai Xu",
      "Feng Gao",
      "Shilong Ji",
      "Wenhao Tang",
      "Wenbo Ding",
      "Chao Yu",
      "Yu Wang"
    ],
    "published": "2025-05-07T11:04:36+00:00",
    "summary": "In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme."
  },
  {
    "title": "Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.04310v1",
    "arxiv_id": "2505.04310v1",
    "authors": [
      "Simo Alami C.",
      "Rim Kaddah",
      "Jesse Read",
      "Marie-Paule Cani"
    ],
    "published": "2025-05-07T10:49:53+00:00",
    "summary": "We introduce a new architecture for Distributional Reinforcement Learning (DistRL) that models return distributions using normalizing flows. This approach enables flexible, unbounded support for return distributions, in contrast to categorical approaches like C51 that rely on fixed or bounded representations. It also offers richer modeling capacity to capture multi-modality, skewness, and tail behavior than quantile based approaches. Our method is significantly more parameter-efficient than categorical approaches. Standard metrics used to train existing models like KL divergence or Wasserstein distance either are scale insensitive or have biased sample gradients, especially when return supports do not overlap. To address this, we propose a novel surrogate for the Cram\\`er distance, that is geometry-aware and computable directly from the return distribution's PDF, avoiding the costly CDF computation. We test our model on the ATARI-5 sub-benchmark and show that our approach outperforms PDF based models while remaining competitive with quantile based methods."
  },
  {
    "title": "PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum Transfer for Spatial Public Goods Games",
    "url": "http://arxiv.org/abs/2505.04302v1",
    "arxiv_id": "2505.04302v1",
    "authors": [
      "Zhaoqilin Yang",
      "Chanchan Li",
      "Xin Wang",
      "Youliang Tian"
    ],
    "published": "2025-05-07T10:20:55+00:00",
    "summary": "This study investigates cooperation evolution mechanisms in the spatial public goods game. A novel deep reinforcement learning framework, Proximal Policy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed to model agent strategy optimization in dynamic environments. Traditional evolutionary game models frequently exhibit limitations in modeling long-term decision-making processes. Deep reinforcement learning effectively addresses this limitation by bridging policy gradient methods with evolutionary game theory. Our study pioneers the application of proximal policy optimization's continuous strategy optimization capability to public goods games through a two-stage adversarial curriculum transfer training paradigm. The experimental results show that PPO-ACT performs better in critical enhancement factor regimes. Compared to conventional standard proximal policy optimization methods, Q-learning and Fermi update rules, achieve earlier cooperation phase transitions and maintain stable cooperative equilibria. This framework exhibits better robustness when handling challenging scenarios like all-defector initial conditions. Systematic comparisons reveal the unique advantage of policy gradient methods in population-scale cooperation, i.e., achieving spatiotemporal payoff coordination through value function propagation. Our work provides a new computational framework for studying cooperation emergence in complex systems, algorithmically validating the punishment promotes cooperation hypothesis while offering methodological insights for multi-agent system strategy design."
  },
  {
    "title": "Object-Shot Enhanced Grounding Network for Egocentric Video",
    "url": "http://arxiv.org/abs/2505.04270v1",
    "arxiv_id": "2505.04270v1",
    "authors": [
      "Yisen Feng",
      "Haoyu Zhang",
      "Meng Liu",
      "Weili Guan",
      "Liqiang Nie"
    ],
    "published": "2025-05-07T09:20:12+00:00",
    "summary": "Egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. Existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. To address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding Network for egocentric video. Specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. Additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. Experiments conducted on three datasets demonstrate that OSGNet achieves state-of-the-art performance, validating the effectiveness of our approach. Our code can be found at https://github.com/Yisen-Feng/OSGNet."
  },
  {
    "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards",
    "url": "http://arxiv.org/abs/2505.04671v1",
    "arxiv_id": "2505.04671v1",
    "authors": [
      "Yuxin Zhang",
      "Meihao Fan",
      "Ju Fan",
      "Mingyang Yi",
      "Yuyu Luo",
      "Jian Tan",
      "Guoliang Li"
    ],
    "published": "2025-05-07T08:32:22+00:00",
    "summary": "Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation.To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a \"cold start, then PRM supervision\" paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning. Our code is publicly available."
  },
  {
    "title": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections",
    "url": "http://arxiv.org/abs/2505.04231v1",
    "arxiv_id": "2505.04231v1",
    "authors": [
      "Taoyuan Yu",
      "Kui Wang",
      "Zongdian Li",
      "Tao Yu",
      "Kei Sakaguchi"
    ],
    "published": "2025-05-07T08:27:52+00:00",
    "summary": "Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \\textit{(i)} achieving failure rates below 0.03\\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \\textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps."
  },
  {
    "title": "Trajectory Entropy Reinforcement Learning for Predictable and Robust Control",
    "url": "http://arxiv.org/abs/2505.04193v1",
    "arxiv_id": "2505.04193v1",
    "authors": [
      "Bang You",
      "Chenxu Wang",
      "Huaping Liu"
    ],
    "published": "2025-05-07T07:41:29+00:00",
    "summary": "Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art."
  },
  {
    "title": "Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data",
    "url": "http://arxiv.org/abs/2505.04161v1",
    "arxiv_id": "2505.04161v1",
    "authors": [
      "Baida Zhang",
      "Yakai Chen",
      "Huichun Li",
      "Zhenghu Zu"
    ],
    "published": "2025-05-07T06:23:26+00:00",
    "summary": "Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of \"time coverage\". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies."
  },
  {
    "title": "Energy Efficient RSMA-Based LEO Satellite Communications Assisted by UAV-Mounted BD-Active RIS: A DRL Approach",
    "url": "http://arxiv.org/abs/2505.04148v1",
    "arxiv_id": "2505.04148v1",
    "authors": [
      "Rahman Saadat Yeganeh",
      "Hamid Behroozi"
    ],
    "published": "2025-05-07T05:58:31+00:00",
    "summary": "This paper proposes an advanced non-terrestrial communication architecture that integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal Active Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the coverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a group-connected structure to enhance signal amplification and adaptability, while RSMA enables efficient multi-user access by dividing messages into common and private components. The system jointly optimizes satellite beamforming, UAV positioning, power allocation, and rate-splitting ratios to maximize the overall energy efficiency (EE). To solve the resulting non-convex and high-dimensional problem, we employ three state-of-the-art deep reinforcement learning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage Actor-Critic (A3C). Moreover, realistic models for the power consumption of both the UAV and the BD-ARIS are considered. Simulation results reveal that TRPO consistently achieves the best performance in terms of EE and sum rate, especially under high transmit powers and challenging deployment scenarios. TD3 converges faster and performs competitively in moderate settings, while A3C suffers from instability due to its high variance. Additionally, the robustness of each algorithm under channel state information (CSI) uncertainty is evaluated, confirming TRPO resilience to imperfect observations. Overall, the proposed RSMA-BD-ARIS framework significantly outperforms conventional RIS-assisted designs and provides a scalable, energy-efficient solution for 6G and massive IoT applications in non-terrestrial networks."
  },
  {
    "title": "Reinforcement Learning-Aided Design of Efficient Polarization Kernels",
    "url": "http://arxiv.org/abs/2505.04127v1",
    "arxiv_id": "2505.04127v1",
    "authors": [
      "Yi-Ting Hong",
      "Stefano Rini",
      "Luca Barletta"
    ],
    "published": "2025-05-07T04:48:26+00:00",
    "summary": "Polar codes with large kernels achieve optimal error exponents but are difficult to construct when low decoding complexity is also required. We address this challenge under recursive maximum likelihood decoding (RMLD) using a rein- forcement learning approach based on the Gumbel AlphaZero algorithm. The resulting method, PolarZero, consistently matches exhaustive search in identifying low-complexity kernels, and discovers a size-16 kernel with complexity comparable to handcrafted designs. Our results suggest that PolarZero is a scalable tool for large-kernel design, where brute-force search is no longer feasible."
  },
  {
    "title": "Reinforcement Learning-Aided Design of Efficient Polarization Kernels",
    "url": "http://arxiv.org/abs/2505.04127v2",
    "arxiv_id": "2505.04127v2",
    "authors": [
      "Yi-Ting Hong",
      "Stefano Rini",
      "Luca Barletta"
    ],
    "published": "2025-05-07T04:48:26+00:00",
    "summary": "Polar codes with large kernels achieve optimal error exponents but are difficult to construct when low decoding complexity is also required. We address this challenge under recursive maximum likelihood decoding (RMLD) using a rein-forcement learning approach based on the Gumbel AlphaZero algorithm. The resulting method, PolarZero, consistently matches exhaustive search in identifying low-complexity kernels, and discovers a size-16 kernel with complexity comparable to handcrafted designs. Our results suggest that PolarZero is a scalable tool for large-kernel design, where brute-force search is no longer feasible."
  },
  {
    "title": "Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment",
    "url": "http://arxiv.org/abs/2505.04113v1",
    "arxiv_id": "2505.04113v1",
    "authors": [
      "Xueyao Zhang",
      "Yuancheng Wang",
      "Chaoren Wang",
      "Ziniu Li",
      "Zhuo Chen",
      "Zhizheng Wu"
    ],
    "published": "2025-05-07T04:04:31+00:00",
    "summary": "Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/."
  },
  {
    "title": "AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding",
    "url": "http://arxiv.org/abs/2505.04058v1",
    "arxiv_id": "2505.04058v1",
    "authors": [
      "Feng Xiao",
      "Hongbin Xu",
      "Guocan Zhao",
      "Wenxiong Kang"
    ],
    "published": "2025-05-07T02:02:15+00:00",
    "summary": "3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the perception of referred objects. We propose a novel 2D-assisted 3D visual grounding framework that constructs semantic-spatial scene graphs with referred object discrimination for relationship perception. The framework incorporates a dual-branch visual encoder that utilizes 2D pre-trained attributes to guide the multi-modal object encoding. Furthermore, our cross-modal interaction module uses graph attention to facilitate relationship-oriented information fusion. The enhanced object representation and iterative relational learning enable the model to establish effective alignment between 3D vision and referential descriptions. Experimental results on the popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in addressing the challenges of multiple similar distractors."
  },
  {
    "title": "PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers",
    "url": "http://arxiv.org/abs/2505.04002v1",
    "arxiv_id": "2505.04002v1",
    "authors": [
      "Michael Xu",
      "Yi Shi",
      "KangKang Yin",
      "Xue Bin Peng"
    ],
    "published": "2025-05-06T22:29:07+00:00",
    "summary": "Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers."
  },
  {
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains",
    "url": "http://arxiv.org/abs/2505.03981v1",
    "arxiv_id": "2505.03981v1",
    "authors": [
      "Qianchu Liu",
      "Sheng Zhang",
      "Guanghui Qin",
      "Timothy Ossowski",
      "Yu Gu",
      "Ying Jin",
      "Sid Kiblawi",
      "Sam Preston",
      "Mu Wei",
      "Paul Vozila",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "published": "2025-05-06T21:08:27+00:00",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks."
  },
  {
    "title": "Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading",
    "url": "http://arxiv.org/abs/2505.03949v1",
    "arxiv_id": "2505.03949v1",
    "authors": [
      "John Christopher Tidwell",
      "John Storm Tidwell"
    ],
    "published": "2025-05-06T19:55:57+00:00",
    "summary": "This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM."
  },
  {
    "title": "Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents",
    "url": "http://arxiv.org/abs/2505.03947v1",
    "arxiv_id": "2505.03947v1",
    "authors": [
      "Xiang Li",
      "Yiyang Hao",
      "Doug Fulop"
    ],
    "published": "2025-05-06T19:51:41+00:00",
    "summary": "One of the primary aspirations in reinforcement learning research is developing general-purpose agents capable of rapidly adapting to and mastering novel tasks. While RL gaming agents have mastered many Atari games, they remain slow and costly to train for each game. In this work, we demonstrate that latest reasoning LLMs with out-of-domain RL post-training can play a challenging Atari game called Frogger under a zero-shot setting. We then investigate the effect of in-context learning and the amount of reasoning effort on LLM performance. Lastly, we demonstrate a way to bootstrap traditional RL method with LLM demonstrations, which significantly improves their performance and sample efficiency. Our implementation is open sourced at https://github.com/AlienKevin/frogger."
  },
  {
    "title": "Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems",
    "url": "http://arxiv.org/abs/2505.03946v1",
    "arxiv_id": "2505.03946v1",
    "authors": [
      "Matthew Sgambati",
      "Aleksandar Vakanski",
      "Matthew Anderson"
    ],
    "published": "2025-05-06T19:50:37+00:00",
    "summary": "Resource allocation in High Performance Computing (HPC) environments presents a complex and multifaceted challenge for job scheduling algorithms. Beyond the efficient allocation of system resources, schedulers must account for and optimize multiple performance metrics, including job wait time and system utilization. While traditional rule-based scheduling algorithms dominate the current deployments of HPC systems, the increasing heterogeneity and scale of those systems is expected to challenge the efficiency and flexibility of those algorithms in minimizing job wait time and maximizing utilization. Recent research efforts have focused on leveraging advancements in Reinforcement Learning (RL) to develop more adaptable and intelligent scheduling strategies. Recent RL-based scheduling approaches have explored a range of algorithms, from Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently, hybrid methods that integrate Graph Neural Networks with RL techniques. However, a common limitation across these methods is their reliance on relatively small datasets, and these methods face scalability issues when using large datasets. This study introduces a novel RL-based scheduler utilizing the Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm, which supports large-scale distributed training across multiple workers without requiring parameter synchronization at every step. By eliminating reliance on centralized updates to a shared policy, the DD-PPO scheduler enhances scalability, training efficiency, and sample utilization. The validation dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO performance between traditional and advanced scheduling approaches, and the experimental results demonstrate improved scheduling performance in comparison to both rule-based schedulers and existing RL-based scheduling algorithms."
  },
  {
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.03912v1",
    "arxiv_id": "2505.03912v1",
    "authors": [
      "Can Cui",
      "Pengxiang Ding",
      "Wenxuan Song",
      "Shuanghao Bai",
      "Xinyang Tong",
      "Zirui Ge",
      "Runze Suo",
      "Wanqi Zhou",
      "Yang Liu",
      "Bofang Jia",
      "Han Zhao",
      "Siteng Huang",
      "Donglin Wang"
    ],
    "published": "2025-05-06T18:35:07+00:00",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/."
  },
  {
    "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
    "url": "http://arxiv.org/abs/2505.03738v1",
    "arxiv_id": "2505.03738v1",
    "authors": [
      "Jialong Li",
      "Xuxin Cheng",
      "Tianshu Huang",
      "Shiqi Yang",
      "Ri-Zhao Qiu",
      "Xiaolong Wang"
    ],
    "published": "2025-05-06T17:59:51+00:00",
    "summary": "Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness."
  },
  {
    "title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.03721v1",
    "arxiv_id": "2505.03721v1",
    "authors": [
      "Dian Chen",
      "Zelin Wan",
      "Dong Sam Ha",
      "Jin-Hee Cho"
    ],
    "published": "2025-05-06T17:49:06+00:00",
    "summary": "Solar sensor-based monitoring systems have become a crucial agricultural innovation, advancing farm management and animal welfare through integrating sensor technology, Internet-of-Things, and edge and cloud computing. However, the resilience of these systems to cyber-attacks and their adaptability to dynamic and constrained energy supplies remain largely unexplored. To address these challenges, we propose a sustainable smart farm network designed to maintain high-quality animal monitoring under various cyber and adversarial threats, as well as fluctuating energy conditions. Our approach utilizes deep reinforcement learning (DRL) to devise optimal policies that maximize both monitoring effectiveness and energy efficiency. To overcome DRL's inherent challenge of slow convergence, we integrate transfer learning (TL) and decision theory (DT) to accelerate the learning process. By incorporating DT-guided strategies, we optimize monitoring quality and energy sustainability, significantly reducing training time while achieving comparable performance rewards. Our experimental results prove that DT-guided DRL outperforms TL-enhanced DRL models, improving system performance and reducing training runtime by 47.5%."
  },
  {
    "title": "Actor-Critics Can Achieve Optimal Sample Efficiency",
    "url": "http://arxiv.org/abs/2505.03710v1",
    "arxiv_id": "2505.03710v1",
    "authors": [
      "Kevin Tan",
      "Wei Fan",
      "Yuting Wei"
    ],
    "published": "2025-05-06T17:32:39+00:00",
    "summary": "Actor-critic algorithms have become a cornerstone in reinforcement learning (RL), leveraging the strengths of both policy-based and value-based methods. Despite recent progress in understanding their statistical efficiency, no existing work has successfully learned an $\\epsilon$-optimal policy with a sample complexity of $O(1/\\epsilon^2)$ trajectories with general function approximation when strategic exploration is necessary.   We address this open problem by introducing a novel actor-critic algorithm that attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d H^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$ regret when the Bellman eluder dimension $d$ does not increase with $T$ at more than a $\\log T$ rate.   Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets.   We extend this to the setting of Hybrid RL, showing that initializing the critic with offline data yields sample efficiency gains compared to purely offline or online RL. Further, utilizing access to offline data, we provide a \\textit{non-optimistic} provably efficient actor-critic algorithm that only additionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in exchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy concentrability coefficient and $N_{\\text{off}}$ is the number of offline samples. This addresses another open problem in the literature. We further provide numerical experiments to support our theoretical findings."
  },
  {
    "title": "Policy Gradient Adaptive Control for the LQR: Indirect and Direct Approaches",
    "url": "http://arxiv.org/abs/2505.03706v1",
    "arxiv_id": "2505.03706v1",
    "authors": [
      "Feiran Zhao",
      "Alessandro Chiuso",
      "Florian D\u00f6rfler"
    ],
    "published": "2025-05-06T17:26:04+00:00",
    "summary": "Motivated by recent advances of reinforcement learning and direct data-driven control, we propose policy gradient adaptive control (PGAC) for the linear quadratic regulator (LQR), which uses online closed-loop data to improve the control policy while maintaining stability. Our method adaptively updates the policy in feedback by descending the gradient of the LQR cost and is categorized as indirect, when gradients are computed via an estimated model, versus direct, when gradients are derived from data using sample covariance parameterization. Beyond the vanilla gradient, we also showcase the merits of the natural gradient and Gauss-Newton methods for the policy update. Notably, natural gradient descent bridges the indirect and direct PGAC, and the Gauss-Newton method of the indirect PGAC leads to an adaptive version of the celebrated Hewer's algorithm. To account for the uncertainty from noise, we propose a regularization method for both indirect and direct PGAC. For all the considered PGAC approaches, we show closed-loop stability and convergence of the policy to the optimal LQR gain. Simulations validate our theoretical findings and demonstrate the robustness and computational efficiency of PGAC."
  },
  {
    "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration",
    "url": "http://arxiv.org/abs/2505.03673v1",
    "arxiv_id": "2505.03673v1",
    "authors": [
      "Huajie Tan",
      "Xiaoshuai Hao",
      "Minglan Lin",
      "Pengwei Wang",
      "Yaoxu Lyu",
      "Mingyu Cao",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "published": "2025-05-06T16:11:49+00:00",
    "summary": "The dawn of embodied intelligence has ushered in an unprecedented imperative for resilient, cognition-enabled multi-agent collaboration across next-generation ecosystems, revolutionizing paradigms in autonomous manufacturing, adaptive service robotics, and cyber-physical production architectures. However, current robotic systems face significant limitations, such as limited cross-embodiment adaptability, inefficient task scheduling, and insufficient dynamic error correction. While End-to-end VLA models demonstrate inadequate long-horizon planning and task generalization, hierarchical VLA models suffer from a lack of cross-embodiment and multi-agent coordination capabilities. To address these challenges, we introduce RoboOS, the first open-source embodied system built on a Brain-Cerebellum hierarchical architecture, enabling a paradigm shift from single-agent to multi-agent intelligence. Specifically, RoboOS consists of three key components: (1) Embodied Brain Model (RoboBrain), a MLLM designed for global perception and high-level decision-making; (2) Cerebellum Skill Library, a modular, plug-and-play toolkit that facilitates seamless execution of multiple skills; and (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for coordinating multi-agent states. By integrating hierarchical information flow, RoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust planning, scheduling, and error correction for long-horizon tasks, while ensuring efficient multi-agent collaboration through Real-Time Shared Memory. Furthermore, we enhance edge-cloud communication and cloud-based distributed inference to facilitate high-frequency interactions and enable scalable deployment. Extensive real-world experiments across various scenarios, demonstrate RoboOS's versatility in supporting heterogeneous embodiments. Project website: https://github.com/FlagOpen/RoboOS"
  },
  {
    "title": "Machine Learning: a Lecture Note",
    "url": "http://arxiv.org/abs/2505.03861v1",
    "arxiv_id": "2505.03861v1",
    "authors": [
      "Kyunghyun Cho"
    ],
    "published": "2025-05-06T16:03:41+00:00",
    "summary": "This lecture note is intended to prepare early-year master's and PhD students in data science or a related discipline with foundational ideas in machine learning. It starts with basic ideas in modern machine learning with classification as a main target task. These basic ideas include loss formulation, backpropagation, stochastic gradient descent, generalization, model selection as well as fundamental blocks of artificial neural networks. Based on these basic ideas, the lecture note explores in depth the probablistic approach to unsupervised learning, covering directed latent variable models, product of experts, generative adversarial networks and autoregressive models. Finally, the note ends by covering a diverse set of further topics, such as reinforcement learning, ensemble methods and meta-learning. After reading this lecture note, a student should be ready to embark on studying and researching more advanced topics in machine learning and more broadly artificial intelligence."
  },
  {
    "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation",
    "url": "http://arxiv.org/abs/2505.03586v1",
    "arxiv_id": "2505.03586v1",
    "authors": [
      "Songchen Fu",
      "Siang Chen",
      "Shaojing Zhao",
      "Letian Bai",
      "Ta Li",
      "Yonghong Yan"
    ],
    "published": "2025-05-06T14:47:56+00:00",
    "summary": "In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalization capability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework."
  },
  {
    "title": "DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes",
    "url": "http://arxiv.org/abs/2505.03581v1",
    "arxiv_id": "2505.03581v1",
    "authors": [
      "Sergey Linok",
      "Vadim Semenov",
      "Anastasia Trunova",
      "Oleg Bulichev",
      "Dmitry Yudin"
    ],
    "published": "2025-05-06T14:41:42+00:00",
    "summary": "The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com/linukc/DyGEnc."
  },
  {
    "title": "Ergodic Generative Flows",
    "url": "http://arxiv.org/abs/2505.03561v1",
    "arxiv_id": "2505.03561v1",
    "authors": [
      "Leo Maxime Brunswic",
      "Mateo Clemente",
      "Rui Heng Yang",
      "Adam Sigal",
      "Amir Rasouli",
      "Yinchuan Li"
    ],
    "published": "2025-05-06T14:13:21+00:00",
    "summary": "Generative Flow Networks (GFNs) were initially introduced on directed acyclic graphs to sample from an unnormalized distribution density. Recent works have extended the theoretical framework for generative methods allowing more flexibility and enhancing application range. However, many challenges remain in training GFNs in continuous settings and for imitation learning (IL), including intractability of flow-matching loss, limited tests of non-acyclic training, and the need for a separate reward model in imitation learning. The present work proposes a family of generative flows called Ergodic Generative Flows (EGFs) which are used to address the aforementioned issues. First, we leverage ergodicity to build simple generative flows with finitely many globally defined transformations (diffeomorphisms) with universality guarantees and tractable flow-matching loss (FM loss). Second, we introduce a new loss involving cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It is designed for IL training without a separate reward model. We evaluate IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning experiments with a target reward, using the FM loss."
  },
  {
    "title": "Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving",
    "url": "http://arxiv.org/abs/2505.03558v1",
    "arxiv_id": "2505.03558v1",
    "authors": [
      "Giacomo Avanzi",
      "Marco Giordani",
      "Michele Zorzi"
    ],
    "published": "2025-05-06T14:11:21+00:00",
    "summary": "The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases."
  },
  {
    "title": "Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning",
    "url": "http://arxiv.org/abs/2505.03533v1",
    "arxiv_id": "2505.03533v1",
    "authors": [
      "Jiacheng Wang",
      "Le Liang",
      "Hao Ye",
      "Chongtao Guo",
      "Shi Jin"
    ],
    "published": "2025-05-06T13:41:59+00:00",
    "summary": "Judicious resource allocation can effectively enhance federated learning (FL) training performance in wireless networks by addressing both system and statistical heterogeneity. However, existing strategies typically rely on block fading assumptions, which overlooks rapid channel fluctuations within each round of FL gradient uploading, leading to a degradation in FL training performance. Therefore, this paper proposes a small-scale-fading-aware resource allocation strategy using a multi-agent reinforcement learning (MARL) framework. Specifically, we establish a one-step convergence bound of the FL algorithm and formulate the resource allocation problem as a decentralized partially observable Markov decision process (Dec-POMDP), which is subsequently solved using the QMIX algorithm. In our framework, each client serves as an agent that dynamically determines spectrum and power allocations within each coherence time slot, based on local observations and a reward derived from the convergence analysis. The MARL setting reduces the dimensionality of the action space and facilitates decentralized decision-making, enhancing the scalability and practicality of the solution. Experimental results demonstrate that our QMIX-based resource allocation strategy significantly outperforms baseline methods across various degrees of statistical heterogeneity. Additionally, ablation studies validate the critical importance of incorporating small-scale fading dynamics, highlighting its role in optimizing FL performance."
  },
  {
    "title": "The Steganographic Potentials of Language Models",
    "url": "http://arxiv.org/abs/2505.03439v1",
    "arxiv_id": "2505.03439v1",
    "authors": [
      "Artem Karpov",
      "Tinuade Adeleke",
      "Seong Hah Cho",
      "Natalia Perez-Campanero"
    ],
    "published": "2025-05-06T11:25:52+00:00",
    "summary": "The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment."
  },
  {
    "title": "Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients",
    "url": "http://arxiv.org/abs/2505.03432v1",
    "arxiv_id": "2505.03432v1",
    "authors": [
      "Stefano Bruno",
      "Sotirios Sabanis"
    ],
    "published": "2025-05-06T11:17:15+00:00",
    "summary": "Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes."
  },
  {
    "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models",
    "url": "http://arxiv.org/abs/2505.03420v1",
    "arxiv_id": "2505.03420v1",
    "authors": [
      "Fei Zhao",
      "Chengcui Zhang",
      "Runlin Zhang",
      "Tianyang Wang",
      "Xi Li"
    ],
    "published": "2025-05-06T10:55:21+00:00",
    "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness."
  },
  {
    "title": "Effective Reinforcement Learning Control using Conservative Soft Actor-Critic",
    "url": "http://arxiv.org/abs/2505.03356v1",
    "arxiv_id": "2505.03356v1",
    "authors": [
      "Xinyi Yuan",
      "Zhiwei Shang",
      "Wenjun Huang",
      "Yunduan Cui",
      "Di Chen",
      "Meixin Zhu"
    ],
    "published": "2025-05-06T09:26:29+00:00",
    "summary": "Reinforcement Learning (RL) has shown great potential in complex control tasks, particularly when combined with deep neural networks within the Actor-Critic (AC) framework. However, in practical applications, balancing exploration, learning stability, and sample efficiency remains a significant challenge. Traditional methods such as Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO) address these issues by incorporating entropy or relative entropy regularization, but often face problems of instability and low sample efficiency. In this paper, we propose the Conservative Soft Actor-Critic (CSAC) algorithm, which seamlessly integrates entropy and relative entropy regularization within the AC framework. CSAC improves exploration through entropy regularization while avoiding overly aggressive policy updates with the use of relative entropy regularization. Evaluations on benchmark tasks and real-world robotic simulations demonstrate that CSAC offers significant improvements in stability and efficiency over existing methods. These findings suggest that CSAC provides strong robustness and application potential in control tasks under dynamic environments."
  },
  {
    "title": "RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation",
    "url": "http://arxiv.org/abs/2505.03344v1",
    "arxiv_id": "2505.03344v1",
    "authors": [
      "Keyu Chen",
      "Wenchao Sun",
      "Hao Cheng",
      "Sifa Zheng"
    ],
    "published": "2025-05-06T09:12:37+00:00",
    "summary": "Achieving both realism and controllability in interactive closed-loop traffic simulation remains a key challenge in autonomous driving. Data-driven simulation methods reproduce realistic trajectories but suffer from covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centered simulation framework that conducts open-loop imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and multimodality, followed by closed-loop reinforcement learning fine-tuning in a physics-based simulator to enhance controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that preserves the trajectory-level multimodality through a GRPO-style group-relative advantage formulation, while enhancing controllability and training stability by replacing KL regularization with the dual-clip mechanism. Extensive experiments demonstrate that RIFT significantly improves the realism and controllability of generated traffic scenarios, providing a robust platform for evaluating autonomous vehicle performance in diverse and interactive scenarios."
  },
  {
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "url": "http://arxiv.org/abs/2505.03335v1",
    "arxiv_id": "2505.03335v1",
    "authors": [
      "Andrew Zhao",
      "Yiran Wu",
      "Yang Yue",
      "Tong Wu",
      "Quentin Xu",
      "Yang Yue",
      "Matthieu Lin",
      "Shenzhi Wang",
      "Qingyun Wu",
      "Zilong Zheng",
      "Gao Huang"
    ],
    "published": "2025-05-06T09:08:00+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes."
  },
  {
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "url": "http://arxiv.org/abs/2505.03335v2",
    "arxiv_id": "2505.03335v2",
    "authors": [
      "Andrew Zhao",
      "Yiran Wu",
      "Yang Yue",
      "Tong Wu",
      "Quentin Xu",
      "Yang Yue",
      "Matthieu Lin",
      "Shenzhi Wang",
      "Qingyun Wu",
      "Zilong Zheng",
      "Gao Huang"
    ],
    "published": "2025-05-06T09:08:00+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes."
  },
  {
    "title": "Unraveling the Rainbow: can value-based methods schedule?",
    "url": "http://arxiv.org/abs/2505.03323v1",
    "arxiv_id": "2505.03323v1",
    "authors": [
      "Arthur Corr\u00eaa",
      "Alexandre Jesus",
      "Crist\u00f3v\u00e3o Silva",
      "Samuel Moniz"
    ],
    "published": "2025-05-06T08:51:17+00:00",
    "summary": "Recently, deep reinforcement learning has emerged as a promising approach for solving complex combinatorial optimization problems. Broadly, deep reinforcement learning methods fall into two categories: policy-based and value-based. While value-based approaches have achieved notable success in domains such as the Arcade Learning Environment, the combinatorial optimization community has predominantly favored policy-based methods, often overlooking the potential of value-based algorithms. In this work, we conduct a comprehensive empirical evaluation of value-based algorithms, including the deep q-network and several of its advanced extensions, within the context of two complex combinatorial problems: the job-shop and the flexible job-shop scheduling problems, two fundamental challenges with multiple industrial applications. Our results challenge the assumption that policy-based methods are inherently superior for combinatorial optimization. We show that several value-based approaches can match or even outperform the widely adopted proximal policy optimization algorithm, suggesting that value-based strategies deserve greater attention from the combinatorial optimization community. Our code is openly available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow."
  },
  {
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.03318v1",
    "arxiv_id": "2505.03318v1",
    "authors": [
      "Yibin Wang",
      "Zhimin Li",
      "Yuhang Zang",
      "Chunyu Wang",
      "Qinglin Lu",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "published": "2025-05-06T08:46:41+00:00",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model."
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling",
    "url": "http://arxiv.org/abs/2505.03288v1",
    "arxiv_id": "2505.03288v1",
    "authors": [
      "Francesco Morri",
      "H\u00e9l\u00e8ne Le Cadre",
      "Pierre Gruet",
      "Luce Brotcorne"
    ],
    "published": "2025-05-06T08:15:39+00:00",
    "summary": "We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones."
  },
  {
    "title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.03238v1",
    "arxiv_id": "2505.03238v1",
    "authors": [
      "Liam Boyle",
      "Nicolas Baumann",
      "Paviththiren Sivasothilingam",
      "Michele Magno",
      "Luca Benini"
    ],
    "published": "2025-05-06T07:07:28+00:00",
    "summary": "Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an extension of the R1-zero approach, which enables the usage of low parameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero approach was originally developed to enable mathematical reasoning in LLMs using static datasets. We extend it to the robotics domain through integration in a closed-loop Reinforcement Learning (RL) framework. This extension enhances reasoning in Embodied Artificial Intelligence (Embodied AI) settings without relying solely on distillation of large models through Supervised Fine-Tuning (SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which enables tasks that previously required significantly larger models. In an autonomous driving setting, a performance gain of 20.2%-points over the SFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3% control adaptability score, surpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These results highlight that practical, on-board deployment of small LLMs is not only feasible but can outperform larger models if trained through environmental feedback, underscoring the importance of an interactive learning framework for robotic Embodied AI, one grounded in practical experience rather than static supervision."
  },
  {
    "title": "Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2505.03230v1",
    "arxiv_id": "2505.03230v1",
    "authors": [
      "Yue Chen",
      "Hui Kang",
      "Jiahui Li",
      "Geng Su",
      "Boxiong Wang",
      "Jiacheng Wang",
      "Cong Liang",
      "Shuang Liang",
      "Dusit Niyato"
    ],
    "published": "2025-05-06T06:46:19+00:00",
    "summary": "The integration of simultaneous wireless information and power transfer (SWIPT) technology in 6G Internet of Things (IoT) networks faces significant challenges in remote areas and disaster scenarios where ground infrastructure is unavailable. This paper proposes a novel unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system enhanced by directional antennas to provide both computational resources and energy support for ground IoT terminals. However, such systems require multiple trade-off policies to balance UAV energy consumption, terminal battery levels, and computational resource allocation under various constraints, including limited UAV battery capacity, non-linear energy harvesting characteristics, and dynamic task arrivals. To address these challenges comprehensively, we formulate a bi-objective optimization problem that simultaneously considers system energy efficiency and terminal battery sustainability. We then reformulate this non-convex problem with a hybrid solution space as a Markov decision process (MDP) and propose an improved soft actor-critic (SAC) algorithm with an action simplification mechanism to enhance its convergence and generalization capabilities. Simulation results have demonstrated that our proposed approach outperforms various baselines in different scenarios, achieving efficient energy management while maintaining high computational performance. Furthermore, our method shows strong generalization ability across different scenarios, particularly in complex environments, validating the effectiveness of our designed boundary penalty and charging reward mechanisms."
  },
  {
    "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.03209v1",
    "arxiv_id": "2505.03209v1",
    "authors": [
      "Borui Wang",
      "Kathleen McKeown",
      "Rex Ying"
    ],
    "published": "2025-05-06T05:53:09+00:00",
    "summary": "Reinforcement learning from expert demonstrations has long remained a challenging research problem, and existing state-of-the-art methods using behavioral cloning plus further RL training often suffer from poor generalization, low sample efficiency, and poor model interpretability. Inspired by the strong reasoning abilities of large language models (LLMs), we propose a novel strategy-based reinforcement learning framework integrated with LLMs called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL) to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, and gradually internalizes induced strategies into the RL agent through policy optimization to improve its performance through boosting policy generalization and enhancing sample efficiency. It also provides a direct textual channel to observe and interpret the evolution of the policy's underlying strategies during training. We test DYSTIL over challenging RL environments from Minigrid and BabyAI, and empirically demonstrate that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate while also enjoying higher sample efficiency during the learning process."
  },
  {
    "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making",
    "url": "http://arxiv.org/abs/2505.03181v1",
    "arxiv_id": "2505.03181v1",
    "authors": [
      "Jake Grigsby",
      "Yuke Zhu",
      "Michael Ryoo",
      "Juan Carlos Niebles"
    ],
    "published": "2025-05-06T04:51:57+00:00",
    "summary": "Recent research looks to harness the general knowledge and reasoning of large language models (LLMs) into agents that accomplish user-specified goals in interactive environments. Vision-language models (VLMs) extend LLMs to multi-modal data and provide agents with the visual reasoning necessary for new applications in areas such as computer automation. However, agent tasks emphasize skills where accessible open-weight VLMs lag behind their LLM equivalents. For example, VLMs are less capable of following an environment's strict output syntax requirements and are more focused on open-ended question answering. Overcoming these limitations requires supervised fine-tuning (SFT) on task-specific expert demonstrations. Our work approaches these challenges from an offline-to-online reinforcement learning (RL) perspective. RL lets us fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of our own model or more capable (larger) models. We explore an off-policy RL solution that retains the stability and simplicity of the widely used SFT workflow while allowing our agent to self-improve and learn from low-quality datasets. We demonstrate this technique with two open-weight VLMs across three multi-modal agent domains."
  },
  {
    "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.03172v1",
    "arxiv_id": "2505.03172v1",
    "authors": [
      "Caleb Chuck",
      "Fan Feng",
      "Carl Qi",
      "Chang Shi",
      "Siddhant Agarwal",
      "Amy Zhang",
      "Scott Niekum"
    ],
    "published": "2025-05-06T04:32:47+00:00",
    "summary": "Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal -- an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However because interactions do not have a consensus statistical definition tractable for downstream GCRL, we propose a definition of interactions based on the concept of null counterfactual: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a \"nulling'' operation with a learned model to infer interactions. NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x."
  },
  {
    "title": "Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation",
    "url": "http://arxiv.org/abs/2505.03155v1",
    "arxiv_id": "2505.03155v1",
    "authors": [
      "Max Qiushi Lin",
      "Jincheng Mei",
      "Matin Aghaei",
      "Michael Lu",
      "Bo Dai",
      "Alekh Agarwal",
      "Dale Schuurmans",
      "Csaba Szepesvari",
      "Sharan Vaswani"
    ],
    "published": "2025-05-06T04:03:06+00:00",
    "summary": "Policy gradient (PG) methods have played an essential role in the empirical successes of reinforcement learning. In order to handle large state-action spaces, PG methods are typically used with function approximation. In this setting, the approximation error in modeling problem-dependent quantities is a key notion for characterizing the global convergence of PG methods. We focus on Softmax PG with linear function approximation (referred to as $\\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant to the algorithm's global convergence even for the stochastic bandit setting. Consequently, we first identify the necessary and sufficient conditions on the feature representation that can guarantee the asymptotic global convergence of $\\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$ iterations of $\\texttt{Lin-SPG}$ with a problem-specific learning rate result in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that $\\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure asymptotic global convergence to the optimal policy."
  },
  {
    "title": "HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic Environments",
    "url": "http://arxiv.org/abs/2505.03128v1",
    "arxiv_id": "2505.03128v1",
    "authors": [
      "Evangelos Psomiadis",
      "Panagiotis Tsiotras"
    ],
    "published": "2025-05-06T03:03:26+00:00",
    "summary": "This paper addresses the problem of robot navigation in mixed geometric and semantic 3D environments. Given a hierarchical representation of the environment, the objective is to navigate from a start position to a goal while minimizing the computational cost. We introduce Hierarchical Class-ordered A* (HCOA*), an algorithm that leverages the environmental hierarchy for efficient path-planning in semantic graphs, significantly reducing computational effort. We use a total order over the semantic classes and prove theoretical performance guarantees for the algorithm. We propose two approaches for higher-layer node classification based on the node semantics of the lowest layer: a Graph Neural Network-based method and a Majority-Class method. We evaluate our approach through simulations on a 3D Scene Graph (3DSG), comparing it to the state-of-the-art and assessing its performance against our classification approaches. Results show that HCOA* can find the optimal path while reducing the number of expanded nodes by 25% and achieving a 16% reduction in computational time on the uHumans2 3DSG dataset."
  },
  {
    "title": "Integrated Sensing, Computing, Communication, and Control for Time-Sequence-Based Semantic Communications",
    "url": "http://arxiv.org/abs/2505.03127v1",
    "arxiv_id": "2505.03127v1",
    "authors": [
      "Qingliang Li",
      "Bo Chang",
      "Weidong Mei",
      "Zhi Chen"
    ],
    "published": "2025-05-06T03:01:39+00:00",
    "summary": "In the upcoming industrial internet of things (IIoT) era, a surge of task-oriented applications will rely on real-time wireless control systems (WCSs). For these systems, ultra-reliable and low-latency wireless communication will be crucial to ensure the timely transmission of control information. To achieve this purpose, we propose a novel time-sequence-based semantic communication paradigm, where an integrated sensing, computing, communication, and control (ISC3) architecture is developed to make sensible semantic inference (SI) for the control information over time sequences, enabling adaptive control of the robot. However, due to the causal correlations in the time sequence, the control information does not present the Markov property. To address this challenge, we compute the mutual information of the control information sensed at the transmitter (Tx) over different time and identify their temporal semantic correlation via a semantic feature extractor (SFE) module. By this means, highly correlated information transmission can be avoided, thus greatly reducing the communication overhead. Meanwhile, a semantic feature reconstructor (SFR) module is employed at the receiver (Rx) to reconstruct the control information based on the previously received one if the information transmission is not activated at the Tx. Furthermore, a control gain policy is also employed at the Rx to adaptively adjust the control gain for the controlled target based on several practical aspects such as the quality of the information transmission from the Tx to the Rx. We design the neural network structures of the above modules/policies and train their parameters by a novel hybrid reward multi-agent deep reinforcement learning framework. On-site experiments are conducted to evaluate the performance of our proposed method in practice, which shows significant gains over other baseline schemes."
  },
  {
    "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
    "url": "http://arxiv.org/abs/2505.03075v1",
    "arxiv_id": "2505.03075v1",
    "authors": [
      "Zhengliang Shi",
      "Lingyong Yan",
      "Weiwei Sun",
      "Yue Feng",
      "Pengjie Ren",
      "Xinyu Ma",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ],
    "published": "2025-05-05T23:54:53+00:00",
    "summary": "Retrieval-augmented generation (RAG) integrates large language models ( LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most previous work independently fine-tunes the retriever to adapt to frozen LLM s or trains the LLMs to use documents retrieved by off-the-shelf retrievers, lacking end-to-end training supervision. Recent work addresses this limitation by jointly training these two components but relies on overly simplifying assumptions of document independence, which has been criticized for being far from real-world scenarios. Thus, effectively optimizing the overall RAG performance remains a critical challenge.   We propose a direct retrieval-augmented optimization framework, named DRO, that enables end-to-end training of two key components: (i) a generative knowledge selection model and (ii) an LLM generator. DRO alternates between two phases: (i) document permutation estimation and (ii) re-weighted maximization, progressively improving RAG components through a variational approach. In the estimation step, we treat document permutation as a latent variable and directly estimate its distribution from the selection model by applying an importance sampling strategy. In the maximization step, we calibrate the optimization expectation using importance weights and jointly train the selection model and LLM generator. Our theoretical analysis reveals that DRO is analogous to policy-gradient methods in reinforcement learning. Extensive experiments conducted on five datasets illustrate that DRO outperforms the best baseline with 5%-15% improvements in EM and F1. We also provide in-depth experiments to qualitatively analyze the stability, convergence, and variance of DRO."
  },
  {
    "title": "MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning",
    "url": "http://arxiv.org/abs/2505.03035v1",
    "arxiv_id": "2505.03035v1",
    "authors": [
      "Mohammad Mohammadi",
      "Daniel Honerkamp",
      "Martin B\u00fcchner",
      "Matteo Cassinelli",
      "Tim Welschehold",
      "Fabien Despinoy",
      "Igor Gilitschenski",
      "Abhinav Valada"
    ],
    "published": "2025-05-05T21:26:03+00:00",
    "summary": "Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at https://more-model.cs.uni-freiburg.de."
  },
  {
    "title": "A Fully Data-Driven Value Iteration for Stochastic LQR: Convergence, Robustness and Stability",
    "url": "http://arxiv.org/abs/2505.02970v1",
    "arxiv_id": "2505.02970v1",
    "authors": [
      "Leilei Cui",
      "Zhong-Ping Jiang",
      "Petter N. Kolm",
      "Gr\u00e9goire G. Macqueron"
    ],
    "published": "2025-05-05T18:54:23+00:00",
    "summary": "Unlike traditional model-based reinforcement learning approaches that extract the underlying system structure by estimating parameters from data, non-model-based data-driven control learns the optimal policy directly from input-state data without any intermediate model identification. Although this direct reinforcement learning approach offers increased adaptability and resilience to model misspecification, its reliance on raw data leaves it vulnerable to system noise and disturbances that may undermine convergence, robustness, and stability. In this article, we establish the convergence, robustness, and stability of value iteration (VI) for data-driven control of stochastic linear quadratic (LQ) systems in discrete-time with entirely unknown dynamics and cost. Our contributions are three-fold. First, we prove that VI is globally exponentially stable for any positive semidefinite initial value matrix in noise-free settings, thereby significantly relaxing restrictive assumptions on initial value functions in existing literature. Second, we extend our analysis to scenarios with external disturbances, proving that VI maintains small-disturbance input-to-state stability (ISS) and converges within a small neighborhood of the optimal solution when disturbances are sufficiently small. Third, we propose a new non-model-based robust adaptive dynamic programming (ADP) algorithm for adaptive optimal controller design, which, unlike existing procedures, requires no prior knowledge of an initial admissible control policy. Numerical experiments on a ``data center cooling'' problem demonstrate the convergence and stability of the algorithm compared to established methods, highlighting its robustness and adaptability for data-driven control in noisy environments. Finally, we present a fully data-driven solution to dynamic portfolio allocation, an important problem in quantitative finance."
  },
  {
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.02835v1",
    "arxiv_id": "2505.02835v1",
    "authors": [
      "Yi-Fan Zhang",
      "Xingyu Lu",
      "Xiao Hu",
      "Chaoyou Fu",
      "Bin Wen",
      "Tianke Zhang",
      "Changyi Liu",
      "Kaiyu Jiang",
      "Kaibing Chen",
      "Kaiyu Tang",
      "Haojie Ding",
      "Jiankang Chen",
      "Fan Yang",
      "Zhang Zhang",
      "Tingting Gao",
      "Liang Wang"
    ],
    "published": "2025-05-05T17:59:50+00:00",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs."
  },
  {
    "title": "TWIST: Teleoperated Whole-Body Imitation System",
    "url": "http://arxiv.org/abs/2505.02833v1",
    "arxiv_id": "2505.02833v1",
    "authors": [
      "Yanjie Ze",
      "Zixuan Chen",
      "Jo\u00e3o Pedro Ara\u00fajo",
      "Zi-ang Cao",
      "Xue Bin Peng",
      "Jiajun Wu",
      "C. Karen Liu"
    ],
    "published": "2025-05-05T17:59:03+00:00",
    "summary": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io"
  },
  {
    "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
    "url": "http://arxiv.org/abs/2505.02811v1",
    "arxiv_id": "2505.02811v1",
    "authors": [
      "Diji Yang",
      "Linda Zeng",
      "Jinmeng Rao",
      "Yi Zhang"
    ],
    "published": "2025-05-05T17:39:35+00:00",
    "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data."
  },
  {
    "title": "Teaching the social media generation: rethinking learning without sacrificing quality",
    "url": "http://arxiv.org/abs/2505.02770v1",
    "arxiv_id": "2505.02770v1",
    "authors": [
      "Sepinoud Azimi"
    ],
    "published": "2025-05-05T16:31:10+00:00",
    "summary": "The rise of social media and AI tools has reshaped how students engage with learning, process information, and build trust in educational content. This generation prefers short, visual materials and fast feedback but often struggles with focus, critical thinking, and deep learning. Educators face the challenge of adapting teaching methods to these habits without lowering academic standards. This study presents a blended learning redesign of a first-year technical course at a Dutch university. Key features included short whiteboard videos before class, hands-on teamwork during class, narrative-style handouts to reinforce learning, in-class draft assignments without AI, and weekly anonymous feedback to adjust in real time. The results were promising: attendance increased by nearly 50%, and none of the regularly attending students failed the exam. Students found the videos useful but emphasized that in-person sessions were essential for understanding the material. While some resisted the shift in expectations, most appreciated the structure, clarity, and opportunities for active learning. This case suggests that combining digital familiarity with clear expectations and active support can help meet students where they are, while still challenging them to grow."
  },
  {
    "title": "The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD",
    "url": "http://arxiv.org/abs/2505.02747v1",
    "arxiv_id": "2505.02747v1",
    "authors": [
      "Aggeliki Sideraki",
      "Christos-Nikolaos Anagnostopoulos"
    ],
    "published": "2025-05-05T15:58:32+00:00",
    "summary": "This paper explores the use of Artificial Intelligence (AI) as a tool for diagnosis, assessment, and intervention for individuals with Autism Spectrum Disorder (ASD). It focuses particularly on AI's role in early diagnosis, utilizing advanced machine learning techniques and data analysis. Recent studies demonstrate that deep learning algorithms can identify behavioral patterns through biometric data analysis, video-based interaction assessments, and linguistic feature extraction, providing a more accurate and timely diagnosis compared to traditional methods. Additionally, AI automates diagnostic tools, reducing subjective biases and enabling the development of personalized assessment protocols for ASD monitoring. At the same time, the paper examines AI-powered intervention technologies, emphasizing educational robots and adaptive communication tools. Social robotic assistants, such as NAO and Kaspar, have been shown to enhance social skills in children by offering structured, repetitive interactions that reinforce learning. Furthermore, AI-driven Augmentative and Alternative Communication (AAC) systems allow children with ASD to express themselves more effectively, while machine-learning chatbots provide language development support through personalized responses. The study presents research findings supporting the effectiveness of these AI applications while addressing challenges such as long-term evaluation and customization to individual needs. In conclusion, the paper highlights the significance of AI as an innovative tool in ASD diagnosis and intervention, advocating for further research to assess its long-term impact."
  },
  {
    "title": "Re-purposing a modular origami manipulator into an adaptive physical computer for machine learning and robotic perception",
    "url": "http://arxiv.org/abs/2505.02744v1",
    "arxiv_id": "2505.02744v1",
    "authors": [
      "Jun Wang",
      "Suyi Li"
    ],
    "published": "2025-05-05T15:52:14+00:00",
    "summary": "Physical computing has emerged as a powerful tool for performing intelligent tasks directly in the mechanical domain of functional materials and robots, reducing our reliance on the more traditional COMS computers. However, no systematic study explains how mechanical design can influence physical computing performance. This study sheds insights into this question by repurposing an origami-inspired modular robotic manipulator into an adaptive physical reservoir and systematically evaluating its computing capacity with different physical configurations, input setups, and computing tasks. By challenging this adaptive reservoir computer to complete the classical NARMA benchmark tasks, this study shows that its time series emulation performance directly correlates to the Peak Similarity Index (PSI), which quantifies the frequency spectrum correlation between the target output and reservoir dynamics. The adaptive reservoir also demonstrates perception capabilities, accurately extracting its payload weight and orientation information from the intrinsic dynamics. Importantly, such information extraction capability can be measured by the spatial correlation between nodal dynamics within the reservoir body. Finally, by integrating shape memory alloy (SMA) actuation, this study demonstrates how to exploit such computing power embodied in the physical body for practical, robotic operations. This study provides a strategic framework for harvesting computing power from soft robots and functional materials, demonstrating how design parameters and input selection can be configured based on computing task requirements. Extending this framework to bio-inspired adaptive materials, prosthetics, and self-adaptive soft robotic systems could enable next-generation embodied intelligence, where the physical structure can compute and interact with their digital counterparts."
  },
  {
    "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry",
    "url": "http://arxiv.org/abs/2505.02722v1",
    "arxiv_id": "2505.02722v1",
    "authors": [
      "Junu Kim",
      "Chaeeun Shim",
      "Sungjin Park",
      "Su Yeon Lee",
      "Gee Young Suh",
      "Chae-Man Lim",
      "Seong Jin Choi",
      "Song Mi Moon",
      "Kyoung-Ho Song",
      "Eu Suk Kim",
      "Hong Bin Kim",
      "Sejoong Kim",
      "Chami Im",
      "Dong-Wan Kang",
      "Yong Soo Kim",
      "Hee-Joon Bae",
      "Sung Yoon Lim",
      "Han-Gil Jeong",
      "Edward Choi"
    ],
    "published": "2025-05-05T15:23:47+00:00",
    "summary": "Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models."
  },
  {
    "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework",
    "url": "http://arxiv.org/abs/2505.02712v1",
    "arxiv_id": "2505.02712v1",
    "authors": [
      "Andrzej Mizera",
      "Jakub Zarzycki"
    ],
    "published": "2025-05-05T15:07:20+00:00",
    "summary": "Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach."
  },
  {
    "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models",
    "url": "http://arxiv.org/abs/2505.02686v1",
    "arxiv_id": "2505.02686v1",
    "authors": [
      "Xiaobao Wu"
    ],
    "published": "2025-05-05T14:33:49+00:00",
    "summary": "Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers."
  },
  {
    "title": "Online Phase Estimation of Human Oscillatory Motions using Deep Learning",
    "url": "http://arxiv.org/abs/2505.02668v1",
    "arxiv_id": "2505.02668v1",
    "authors": [
      "Antonio Grotta",
      "Francesco De Lellis"
    ],
    "published": "2025-05-05T14:18:54+00:00",
    "summary": "Accurately estimating the phase of oscillatory systems is essential for analyzing cyclic activities such as repetitive gestures in human motion. In this work we introduce a learning-based approach for online phase estimation in three-dimensional motion trajectories, using a Long Short- Term Memory (LSTM) network. A calibration procedure is applied to standardize trajectory position and orientation, ensuring invariance to spatial variations. The proposed model is evaluated on motion capture data and further tested in a dynamical system, where the estimated phase is used as input to a reinforcement learning (RL)-based control to assess its impact on the synchronization of a network of Kuramoto oscillators."
  },
  {
    "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design",
    "url": "http://arxiv.org/abs/2505.02666v1",
    "arxiv_id": "2505.02666v1",
    "authors": [
      "Miaomiao Ji",
      "Yanqiu Wu",
      "Zhibin Wu",
      "Shoujin Wang",
      "Jian Yang",
      "Mark Dras",
      "Usman Naseem"
    ],
    "published": "2025-05-05T14:15:02+00:00",
    "summary": "The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies."
  },
  {
    "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
    "url": "http://arxiv.org/abs/2505.02665v1",
    "arxiv_id": "2505.02665v1",
    "authors": [
      "Qianjun Pan",
      "Wenkai Ji",
      "Yuyang Ding",
      "Junsong Li",
      "Shilian Chen",
      "Junyi Wang",
      "Jie Zhou",
      "Qin Chen",
      "Min Zhang",
      "Yulan Wu",
      "Liang He"
    ],
    "published": "2025-05-05T14:14:59+00:00",
    "summary": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems."
  },
  {
    "title": "Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.02634v1",
    "arxiv_id": "2505.02634v1",
    "authors": [
      "David Ramos",
      "Lucas Lacasa",
      "Eusebio Valero",
      "Gonzalo Rubio"
    ],
    "published": "2025-05-05T13:26:11+00:00",
    "summary": "The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources."
  },
  {
    "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.02579v1",
    "arxiv_id": "2505.02579v1",
    "authors": [
      "Lingxiao Kong",
      "Cong Yang",
      "Susanne Neufang",
      "Oya Deniz Beyan",
      "Zeyd Boukhers"
    ],
    "published": "2025-05-05T11:30:46+00:00",
    "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives."
  },
  {
    "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning",
    "url": "http://arxiv.org/abs/2505.02579v2",
    "arxiv_id": "2505.02579v2",
    "authors": [
      "Lingxiao Kong",
      "Cong Yang",
      "Susanne Neufang",
      "Oya Deniz Beyan",
      "Zeyd Boukhers"
    ],
    "published": "2025-05-05T11:30:46+00:00",
    "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives."
  },
  {
    "title": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning",
    "url": "http://arxiv.org/abs/2505.02483v1",
    "arxiv_id": "2505.02483v1",
    "authors": [
      "Changxin Huang",
      "Junyang Liang",
      "Yanbin Chang",
      "Jingzhao Xu",
      "Jianqiang Li"
    ],
    "published": "2025-05-05T09:06:17+00:00",
    "summary": "Enabling a high-degree-of-freedom robot to learn specific skills is a challenging task due to the complexity of robotic dynamics. Reinforcement learning (RL) has emerged as a promising solution; however, addressing such problems requires the design of multiple reward functions to account for various constraints in robotic motion. Existing approaches typically sum all reward components indiscriminately to optimize the RL value function and policy. We argue that this uniform inclusion of all reward components in policy optimization is inefficient and limits the robot's learning performance. To address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework based on Large Language Models (LLMs). This paradigm dynamically adjusts the learning intensity of each reward component throughout the policy optimization process, enabling robots to acquire skills in a gradual and structured manner. Specifically, we design a multi-branch value network, where each branch corresponds to a distinct reward component. During policy optimization, each branch is assigned a weight that reflects its importance, and these weights are automatically computed based on rules designed by LLMs. The LLM generates a rule set in advance, derived from the task description, and during training, it selects a weight calculation rule from the library based on language prompts that evaluate the performance of each branch. Experimental results demonstrate that the AHRS method achieves an average 6.48% performance improvement across multiple high-degree-of-freedom robotic tasks."
  },
  {
    "title": "ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.02439v1",
    "arxiv_id": "2505.02439v1",
    "authors": [
      "Yang Deng",
      "Yaohui Liu",
      "Rui Liang",
      "Dafang Zhao",
      "Donghua Xie",
      "Ittetsu Taniguchi",
      "Dan Wang"
    ],
    "published": "2025-05-05T08:09:36+00:00",
    "summary": "The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method."
  },
  {
    "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
    "url": "http://arxiv.org/abs/2505.02433v1",
    "arxiv_id": "2505.02433v1",
    "authors": [
      "Soumen Kumar Mondal",
      "Akshit Varmora",
      "Prateek Chanda",
      "Ganesh Ramakrishnan"
    ],
    "published": "2025-05-05T07:58:54+00:00",
    "summary": "We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs."
  },
  {
    "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs",
    "url": "http://arxiv.org/abs/2505.02405v1",
    "arxiv_id": "2505.02405v1",
    "authors": [
      "Mario A. V. Saucedo",
      "Vignesh Kottayam Viswanathan",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "published": "2025-05-05T06:55:59+00:00",
    "summary": "This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types."
  },
  {
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL",
    "url": "http://arxiv.org/abs/2505.02391v1",
    "arxiv_id": "2505.02391v1",
    "authors": [
      "Jiarui Yao",
      "Yifan Hao",
      "Hanning Zhang",
      "Hanze Dong",
      "Wei Xiong",
      "Nan Jiang",
      "Tong Zhang"
    ],
    "published": "2025-05-05T06:26:00+00:00",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."
  },
  {
    "title": "RM-R1: Reward Modeling as Reasoning",
    "url": "http://arxiv.org/abs/2505.02387v1",
    "arxiv_id": "2505.02387v1",
    "authors": [
      "Xiusi Chen",
      "Gaotang Li",
      "Ziqi Wang",
      "Bowen Jin",
      "Cheng Qian",
      "Yu Wang",
      "Hongru Wang",
      "Yu Zhang",
      "Denghui Zhang",
      "Tong Zhang",
      "Hanghang Tong",
      "Heng Ji"
    ],
    "published": "2025-05-05T06:11:12+00:00",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1."
  },
  {
    "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning",
    "url": "http://arxiv.org/abs/2505.02363v1",
    "arxiv_id": "2505.02363v1",
    "authors": [
      "Tianjian Li",
      "Daniel Khashabi"
    ],
    "published": "2025-05-05T04:54:44+00:00",
    "summary": "Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%."
  },
  {
    "title": "Sloshing suppression with a controlled elastic baffle via deep reinforcement learning and SPH simulation",
    "url": "http://arxiv.org/abs/2505.02354v1",
    "arxiv_id": "2505.02354v1",
    "authors": [
      "Mai Ye",
      "Yaru Ren",
      "Silong Zhang",
      "Hao Ma",
      "Xiangyu Hu",
      "Oskar J. Haidn"
    ],
    "published": "2025-05-05T04:32:03+00:00",
    "summary": "This study employed smoothed particle hydrodynamics (SPH) as the numerical environment, integrated with deep reinforcement learning (DRL) real-time control algorithms to optimize the sloshing suppression in a tank with a centrally positioned vertical elastic baffle. Compared to rigid baffle movement and active strain control methods, the active-controlled movable elastic baffle, which remains undeformed at its base, achieved the best performance with an 81.63% reduction in mean free surface amplitude. A cosine-based expert policy derived from DRL data is also extracted, resulting in a comparable 76.86% reduction in a three-dimensional (3D) numerical simulation. Energy analyses showed that elastic baffle motion effectively decreased system energy by performing negative work on the fluid, reducing kinetic and potential energy. The DRL-based and expert policies also demonstrated robust performance across varying excitation frequencies and water depths. Specifically, rigid baffles proved more effective at frequencies below the system's first natural frequency, while elastic baffles exhibited superior performance at higher frequencies. Changes in water depth minimally affected the effectiveness of control policies, though they significantly influenced elastic baffle deformation behavior. Overall, the sloshing suppression efficiency consistently ranged between 70% and 80%, confirming DRL-informed control methods' versatility and effectiveness under diverse operating conditions."
  },
  {
    "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
    "url": "http://arxiv.org/abs/2505.02293v1",
    "arxiv_id": "2505.02293v1",
    "authors": [
      "Jason J. Choi",
      "Jasmine Jerry Aloor",
      "Jingqi Li",
      "Maria G. Mendoza",
      "Hamsa Balakrishnan",
      "Claire J. Tomlin"
    ],
    "published": "2025-05-04T23:42:52+00:00",
    "summary": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.   To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.   We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at \\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}"
  },
  {
    "title": "Universal Approximation Theorem of Deep Q-Networks",
    "url": "http://arxiv.org/abs/2505.02288v1",
    "arxiv_id": "2505.02288v1",
    "authors": [
      "Qian Qi"
    ],
    "published": "2025-05-04T22:57:33+00:00",
    "summary": "We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data."
  },
  {
    "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
    "url": "http://arxiv.org/abs/2505.02272v1",
    "arxiv_id": "2505.02272v1",
    "authors": [
      "Dyuman Aditya",
      "Junning Huang",
      "Nico Bohlinger",
      "Piotr Kicki",
      "Krzysztof Walas",
      "Jan Peters",
      "Matteo Luperto",
      "Davide Tateo"
    ],
    "published": "2025-05-04T21:58:11+00:00",
    "summary": "Quadruped robots are currently a widespread platform for robotics research, thanks to powerful Reinforcement Learning controllers and the availability of cheap and robust commercial platforms. However, to broaden the adoption of the technology in the real world, we require robust navigation stacks relying only on low-cost sensors such as depth cameras. This paper presents a first step towards a robust localization, mapping, and navigation system for low-cost quadruped robots. In pursuit of this objective we combine contact-aided kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing stability and accuracy of the system. Our results in simulation and two different real-world quadruped platforms show that our system can generate an accurate 2D map of the environment, robustly localize itself, and navigate autonomously. Furthermore, we present in-depth ablation studies of the important components of the system and their impact on localization accuracy. Videos, code, and additional experiments can be found on the project website: https://sites.google.com/view/low-cost-quadruped-slam"
  },
  {
    "title": "Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning",
    "url": "http://arxiv.org/abs/2505.02232v1",
    "arxiv_id": "2505.02232v1",
    "authors": [
      "Malte Mosbach",
      "Sven Behnke"
    ],
    "published": "2025-05-04T19:51:09+00:00",
    "summary": "Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io"
  },
  {
    "title": "Interpretable Emergent Language Using Inter-Agent Transformers",
    "url": "http://arxiv.org/abs/2505.02215v1",
    "arxiv_id": "2505.02215v1",
    "authors": [
      "Mannan Bhardwaj"
    ],
    "published": "2025-05-04T18:57:57+00:00",
    "summary": "This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments."
  },
  {
    "title": "Exploring new Approaches for Information Retrieval through Natural Language Processing",
    "url": "http://arxiv.org/abs/2505.02199v1",
    "arxiv_id": "2505.02199v1",
    "authors": [
      "Manak Raj",
      "Nidhi Mishra"
    ],
    "published": "2025-05-04T17:37:26+00:00",
    "summary": "This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations."
  },
  {
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents",
    "url": "http://arxiv.org/abs/2505.02156v1",
    "arxiv_id": "2505.02156v1",
    "authors": [
      "Minzheng Wang",
      "Yongbin Li",
      "Haobo Wang",
      "Xinghua Zhang",
      "Nan Xu",
      "Bingli Wu",
      "Fei Huang",
      "Haiyang Yu",
      "Wenji Mao"
    ],
    "published": "2025-05-04T15:39:58+00:00",
    "summary": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach"
  },
  {
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents",
    "url": "http://arxiv.org/abs/2505.02156v2",
    "arxiv_id": "2505.02156v2",
    "authors": [
      "Minzheng Wang",
      "Yongbin Li",
      "Haobo Wang",
      "Xinghua Zhang",
      "Nan Xu",
      "Bingli Wu",
      "Fei Huang",
      "Haiyang Yu",
      "Wenji Mao"
    ],
    "published": "2025-05-04T15:39:58+00:00",
    "summary": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach."
  },
  {
    "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study",
    "url": "http://arxiv.org/abs/2505.02142v1",
    "arxiv_id": "2505.02142v1",
    "authors": [
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yiping Peng",
      "Yunjie Ji",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-05-04T15:09:49+00:00",
    "summary": "Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches."
  },
  {
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
    "url": "http://arxiv.org/abs/2505.02094v1",
    "arxiv_id": "2505.02094v1",
    "authors": [
      "Runyi Yu",
      "Yinhuai Wang",
      "Qihan Zhao",
      "Hok Wai Tsui",
      "Jingbo Wang",
      "Ping Tan",
      "Qifeng Chen"
    ],
    "published": "2025-05-04T13:00:29+00:00",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness."
  },
  {
    "title": "A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2505.01998v1",
    "arxiv_id": "2505.01998v1",
    "authors": [
      "Xiaoliang Chen",
      "Xin Yu",
      "Le Chang",
      "Yunhe Huang",
      "Jiashuai He",
      "Shibo Zhang",
      "Jin Li",
      "Likai Lin",
      "Ziyu Zeng",
      "Xianling Tu",
      "Shuyu Zhang"
    ],
    "published": "2025-05-04T06:03:12+00:00",
    "summary": "This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations-covering far-field localization, weak signal detection, and multilingual speech recognition-demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces."
  },
  {
    "title": "A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2505.01998v2",
    "arxiv_id": "2505.01998v2",
    "authors": [
      "Xiaoliang Chen",
      "Xin Yu",
      "Le Chang",
      "Yunhe Huang",
      "Jiashuai He",
      "Shibo Zhang",
      "Jin Li",
      "Likai Lin",
      "Ziyu Zeng",
      "Xianling Tu",
      "Shuyu Zhang"
    ],
    "published": "2025-05-04T06:03:12+00:00",
    "summary": "This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations, covering far-field localization, weak signal detection, and multilingual speech recognition, demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces."
  },
  {
    "title": "D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection",
    "url": "http://arxiv.org/abs/2505.01979v1",
    "arxiv_id": "2505.01979v1",
    "authors": [
      "Chenran Zhao",
      "Dianxi Shi",
      "Mengzhu Wang",
      "Jianqiang Xia",
      "Huanhuan Yang",
      "Songchang Jin",
      "Shaowu Yang",
      "Chunping Qiu"
    ],
    "published": "2025-05-04T03:59:01+00:00",
    "summary": "Current Hierarchical Reinforcement Learning (HRL) algorithms excel in long-horizon sequential decision-making tasks but still face two challenges: delay effects and spurious correlations. To address them, we propose a causal HRL approach called D3HRL. First, D3HRL models delayed effects as causal relationships across different time spans and employs distributed causal discovery to learn these relationships. Second, it employs conditional independence testing to eliminate spurious correlations. Finally, D3HRL constructs and trains hierarchical policies based on the identified true causal relationships. These three steps are iteratively executed, gradually exploring the complete causal chain of the task. Experiments conducted in 2D-MineCraft and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects and accurately identifies causal relationships, leading to reliable decision-making in complex environments."
  },
  {
    "title": "A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites",
    "url": "http://arxiv.org/abs/2505.01966v1",
    "arxiv_id": "2505.01966v1",
    "authors": [
      "Bofei Liu",
      "Dong Ye",
      "Zunhao Yao",
      "Zhaowei Sun"
    ],
    "published": "2025-05-04T02:35:18+00:00",
    "summary": "Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively."
  },
  {
    "title": "Training Environment for High Performance Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.01953v1",
    "arxiv_id": "2505.01953v1",
    "authors": [
      "Greg Search"
    ],
    "published": "2025-05-04T01:09:15+00:00",
    "summary": "This paper presents Tunnel, a simple, open source, reinforcement learning training environment for high performance aircraft. It integrates the F16 3D nonlinear flight dynamics into OpenAI Gymnasium python package. The template includes primitives for boundaries, targets, adversaries and sensing capabilities that may vary depending on operational need. This offers mission planners a means to rapidly respond to evolving environments, sensor capabilities and adversaries for autonomous air combat aircraft. It offers researchers access to operationally relevant aircraft physics. Tunnel code base is accessible to anyone familiar with Gymnasium and/or those with basic python skills. This paper includes a demonstration of a week long trade study that investigated a variety of training methods, observation spaces, and threat presentations. This enables increased collaboration between researchers and mission planners which can translate to a national military advantage. As warfare becomes increasingly reliant upon automation, software agility will correlate with decision advantages. Airmen must have tools to adapt to adversaries in this context. It may take months for researchers to develop skills to customize observation, actions, tasks and training methodologies in air combat simulators. In Tunnel, this can be done in a matter of days."
  },
  {
    "title": "Quantum Many-body Simulations from a Reinforcement-Learned Exponential Ansatz",
    "url": "http://arxiv.org/abs/2505.01935v1",
    "arxiv_id": "2505.01935v1",
    "authors": [
      "Yuchen Wang",
      "David A. Mazziotti"
    ],
    "published": "2025-05-03T22:07:21+00:00",
    "summary": "Solving for the many-body wavefunction represents a significant challenge on both classical and quantum devices because of the exponential scaling of the Hilbert space with system size. While the complexity of the wavefunction can be reduced through conventional ans\\\"{a}tze (e.g., the coupled cluster ansatz), it can still grow rapidly with system size even on quantum devices. An exact, universal two-body exponential ansatz for the many-body wavefunction has been shown to be generated from the solution of the contracted Schr\\\"odinger equation (CSE), and recently, this ansatz has been implemented without classical approximation on quantum simulators and devices for the scalable simulation of many-body quantum systems. Here we combine the solution of the CSE with a form of artificial intelligence known as reinforcement learning (RL) to generate highly compact circuits that implement this ansatz without sacrificing accuracy. As a natural extension of CSE, we reformulate the wavefunction update as a Markovian decision process and train the agent to select the optimal actions at each iteration based upon only the current CSE residual. Compact circuits with high accuracy are achieved for H3 and H4 molecules over a range of molecular geometries."
  },
  {
    "title": "A Generalised and Adaptable Reinforcement Learning Stopping Method",
    "url": "http://arxiv.org/abs/2505.01907v1",
    "arxiv_id": "2505.01907v1",
    "authors": [
      "Reem Bin-Hezam",
      "Mark Stevenson"
    ],
    "published": "2025-05-03T19:24:40+00:00",
    "summary": "This paper presents a Technology Assisted Review (TAR) stopping approach based on Reinforcement Learning (RL). Previous such approaches offered limited control over stopping behaviour, such as fixing the target recall and tradeoff between preferring to maximise recall or cost. These limitations are overcome by introducing a novel RL environment, GRLStop, that allows a single model to be applied to multiple target recalls, balances the recall/cost tradeoff and integrates a classifier. Experiments were carried out on six benchmark datasets (CLEF e-Health datasets 2017-9, TREC Total Recall, TREC Legal and Reuters RCV1) at multiple target recall levels. Results showed that the proposed approach to be effective compared to multiple baselines in addition to offering greater flexibility."
  },
  {
    "title": "LookAlike: Consistent Distractor Generation in Math MCQs",
    "url": "http://arxiv.org/abs/2505.01903v1",
    "arxiv_id": "2505.01903v1",
    "authors": [
      "Nisarg Parikh",
      "Nigel Fernandez",
      "Alexander Scarlatos",
      "Simon Woodhead",
      "Andrew Lan"
    ],
    "published": "2025-05-03T19:18:06+00:00",
    "summary": "Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale."
  },
  {
    "title": "Securing 5G and Beyond-Enabled UAV Networks: Resilience Through Multiagent Learning and Transformers Detection",
    "url": "http://arxiv.org/abs/2505.01885v1",
    "arxiv_id": "2505.01885v1",
    "authors": [
      "Joseanne Viana",
      "Hamed Farkhari",
      "Victor P Gil Jimenez"
    ],
    "published": "2025-05-03T18:27:00+00:00",
    "summary": "Achieving resilience remains a significant challenge for Unmanned Aerial Vehicle (UAV) communications in 5G and 6G networks. Although UAVs benefit from superior positioning capabilities, rate optimization techniques, and extensive line-of-sight (LoS) range, these advantages alone cannot guarantee high reliability across diverse UAV use cases. This limitation becomes particularly evident in urban environments, where UAVs face vulnerability to jamming attacks and where LoS connectivity is frequently compromised by buildings and other physical obstructions. This paper introduces DET-FAIR- WINGS ( Detection-Enhanced Transformer Framework for AI-Resilient Wireless Networks in Ground UAV Systems), a novel solution designed to enhance reliability in UAV communications under attacks. Our system leverages multi-agent reinforcement learning (MARL) and transformer-based detection algorithms to identify attack patterns within the network and subsequently select the most appropriate mechanisms to strengthen reliability in authenticated UAV-Base Station links. The DET-FAIR-WINGS approach integrates both discrete and continuous parameters. Discrete parameters include retransmission attempts, bandwidth partitioning, and notching mechanisms, while continuous parameters encompass beam angles and elevations from both the Base Station (BS) and user devices. The detection part integrates a transformer in the agents to speed up training. Our findings demonstrate that replacing fixed retransmission counts with AI-integrated flexible approaches in 5G networks significantly reduces latency by optimizing decision-making processes within 5G layers."
  },
  {
    "title": "Deep Reinforcement Learning-Aided Frequency Control of LCC-S Resonant Converters for Wireless Power Transfer Systems",
    "url": "http://arxiv.org/abs/2505.01850v1",
    "arxiv_id": "2505.01850v1",
    "authors": [
      "Reza Safari",
      "Mohsen Hamzeh",
      "Nima Mahdian Dehkordi"
    ],
    "published": "2025-05-03T16:00:18+00:00",
    "summary": "This paper presents a novel deep reinforcement learning (DRL)-based control strategy for achieving precise and robust output voltage regulation in LCC-S resonant converters, specifically designed for wireless power transfer applications. Unlike conventional methods that rely on manually tuned PI controllers or heuristic tuning approaches, our method leverages the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to systematically optimize PI controller parameters. The complex converter dynamics are captured using the Direct Piecewise Affine (DPWA) modeling technique, providing a structured approach to handling its nonlinearities. This integration not only eliminates the need for manual tuning, but also enhances control adaptability under varying operating conditions. The simulation and experimental results confirm that the proposed DRL-based tuning approach significantly outperforms traditional methods in terms of stability, robustness, and response time. This work demonstrates the potential of DRL in power electronic control, offering a scalable and data-driven alternative to conventional controller design approaches."
  },
  {
    "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.01822v1",
    "arxiv_id": "2505.01822v1",
    "authors": [
      "Jifeng Hu",
      "Sili Huang",
      "Zhejian Yang",
      "Shengchao Hu",
      "Li Shen",
      "Hechang Chen",
      "Lichao Sun",
      "Yi Chang",
      "Dacheng Tao"
    ],
    "published": "2025-05-03T14:00:25+00:00",
    "summary": "Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks."
  },
  {
    "title": "Adaptive DRL for IRS Mirror Orientation in Dynamic OWC Networks",
    "url": "http://arxiv.org/abs/2505.01818v1",
    "arxiv_id": "2505.01818v1",
    "authors": [
      "Ahrar N. Hamad",
      "Ahmad Adnan Qidan",
      "Taisir E. H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "published": "2025-05-03T13:22:25+00:00",
    "summary": "Intelligent reflecting surfaces (IRSs) have emerged as a promising solution to mitigate line-of-sight (LoS) blockages and enhance signal coverage in optical wireless communication (OWC) systems. In this work, we consider a mirror-based IRS to assist a dynamic indoor visible light communication (VLC) environment. We formulate an optimization problem that aims to maximize the sum rate by adjusting the orientation of the IRS mirrors. To enable real-time adaptability, the problem is modelled as a Markov decision process (MDP), and a deep reinforcement learning (DRL) algorithm, specifically deep deterministic policy gradient (DDPG), is employed to optimize mirror orientation toward mobile users under blockage and mobility constraints. Simulation results demonstrate that the proposed DDPG-based approach outperforms conventional DRL algorithms and achieves substantial improvements in sum rate compared to fixed-orientation IRS configurations."
  },
  {
    "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks",
    "url": "http://arxiv.org/abs/2505.01712v1",
    "arxiv_id": "2505.01712v1",
    "authors": [
      "Lingyi Wang",
      "Rashed Shelim",
      "Walid Saad",
      "Naren Ramakrishnan"
    ],
    "published": "2025-05-03T06:23:18+00:00",
    "summary": "Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method."
  },
  {
    "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation",
    "url": "http://arxiv.org/abs/2505.01709v1",
    "arxiv_id": "2505.01709v1",
    "authors": [
      "Kaidong Zhang",
      "Rongtao Xu",
      "Pengzhen Ren",
      "Junfan Lin",
      "Hefeng Wu",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "published": "2025-05-03T06:17:18+00:00",
    "summary": "Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation."
  },
  {
    "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
    "url": "http://arxiv.org/abs/2505.01706v1",
    "arxiv_id": "2505.01706v1",
    "authors": [
      "Sarvesh Shashidhar",
      "Ritik",
      "Nachiketa Patil",
      "Suraj Racha",
      "Ganesh Ramakrishnan"
    ],
    "published": "2025-05-03T05:59:13+00:00",
    "summary": "Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even \"good\" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present."
  },
  {
    "title": "BMTree: Designing, Learning, and Updating Piecewise Space-Filling Curves for Multi-Dimensional Data Indexing",
    "url": "http://arxiv.org/abs/2505.01697v1",
    "arxiv_id": "2505.01697v1",
    "authors": [
      "Jiangneng Li",
      "Yuang Liu",
      "Zheng Wang",
      "Gao Cong",
      "Cheng Long",
      "Walid G. Aref",
      "Han Mao Kiah",
      "Bin Cui"
    ],
    "published": "2025-05-03T05:05:53+00:00",
    "summary": "Space-filling curves (SFC, for short) have been widely applied to index multi-dimensional data, which first maps the data to one dimension, and then a one-dimensional indexing method, e.g., the B-tree indexes the mapped data. Existing SFCs adopt a single mapping scheme for the whole data space. However, a single mapping scheme often does not perform well on all the data space. In this paper, we propose a new type of SFC called piecewise SFCs that adopts different mapping schemes for different data subspaces. Specifically, we propose a data structure termed the Bit Merging tree (BMTree) that can generate data subspaces and their SFCs simultaneously, and achieve desirable properties of the SFC for the whole data space. Furthermore, we develop a reinforcement learning-based solution to build the BMTree, aiming to achieve excellent query performance. To update the BMTree efficiently when the distributions of data and/or queries change, we develop a new mechanism that achieves fast detection of distribution shifts in data and queries, and enables partial retraining of the BMTree. The retraining mechanism achieves performance enhancement efficiently since it avoids retraining the BMTree from scratch. Extensive experiments show the effectiveness and efficiency of the BMTree with the proposed learning-based methods."
  },
  {
    "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
    "url": "http://arxiv.org/abs/2505.01619v1",
    "arxiv_id": "2505.01619v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "published": "2025-05-02T22:48:27+00:00",
    "summary": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods."
  },
  {
    "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation",
    "url": "http://arxiv.org/abs/2505.01584v1",
    "arxiv_id": "2505.01584v1",
    "authors": [
      "Zhiqiang He",
      "Zhi Liu"
    ],
    "published": "2025-05-02T21:03:03+00:00",
    "summary": "Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions."
  },
  {
    "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation",
    "url": "http://arxiv.org/abs/2505.01584v2",
    "arxiv_id": "2505.01584v2",
    "authors": [
      "Zhiqiang He",
      "Zhi Liu"
    ],
    "published": "2025-05-02T21:03:03+00:00",
    "summary": "Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions."
  },
  {
    "title": "TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students",
    "url": "http://arxiv.org/abs/2505.01563v1",
    "arxiv_id": "2505.01563v1",
    "authors": [
      "Daniel Weitekamp",
      "Momin N. Siddiqui",
      "Christopher J. MacLellan"
    ],
    "published": "2025-05-02T20:03:21+00:00",
    "summary": "Recent improvements in large language model (LLM) performance on academic benchmarks, such as MATH and GSM8K, have emboldened their use as standalone tutors and as simulations of human learning. However, these new applications require more than evaluations of final solution generation. We introduce TutorGym to evaluate these applications more directly. TutorGym is a standard interface for testing artificial intelligence (AI) agents within existing intelligent tutoring systems (ITS) that have been tested and refined in classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and OATutors. TutorGym is more than a simple problem-solution benchmark, it situates AI agents within the interactive interfaces of existing ITSs. At each step of problem-solving, AI agents are asked what they would do as a tutor or as a learner. As tutors, AI agents are prompted to provide tutoring support -- such as generating examples, hints, and step-level correctness feedback -- which can be evaluated directly against the adaptive step-by-step support provided by existing ITSs. As students, agents directly learn from ITS instruction, and their mistakes and learning trajectories can be compared to student data. TutorGym establishes a common framework for training and evaluating diverse AI agents, including LLMs, computational models of learning, and reinforcement learning agents, within a growing suite of learning environments. Currently, TutorGym includes 223 different tutor domains. In an initial evaluation, we find that current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions, and next-step actions were correct only ~52-70% of the time -- but they could produce remarkably human-like learning curves when trained as students with in-context learning."
  },
  {
    "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos",
    "url": "http://arxiv.org/abs/2505.01481v1",
    "arxiv_id": "2505.01481v1",
    "authors": [
      "Zongxia Li",
      "Xiyang Wu",
      "Yubin Qin",
      "Guangyao Shi",
      "Hongyang Du",
      "Dinesh Manocha",
      "Tianyi Zhou",
      "Jordan Lee Boyd-Graber"
    ],
    "published": "2025-05-02T15:58:38+00:00",
    "summary": "Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu."
  },
  {
    "title": "Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation",
    "url": "http://arxiv.org/abs/2505.01361v1",
    "arxiv_id": "2505.01361v1",
    "authors": [
      "Hwanwoo Kim",
      "Panos Toulis",
      "Eric Laber"
    ],
    "published": "2025-05-02T15:57:54+00:00",
    "summary": "Temporal Difference (TD) learning is a foundational algorithm in reinforcement learning (RL). For nearly forty years, TD learning has served as a workhorse for applied RL as well as a building block for more complex and specialized algorithms. However, despite its widespread use, it is not without drawbacks, the most prominent being its sensitivity to step size. A poor choice of step size can dramatically inflate the error of value estimates and slow convergence. Consequently, in practice, researchers must use trial and error in order to identify a suitable step size -- a process that can be tedious and time consuming. As an alternative, we propose implicit TD algorithms that reformulate TD updates into fixed-point equations. These updates are more stable and less sensitive to step size without sacrificing computational efficiency. Moreover, our theoretical analysis establishes asymptotic convergence guarantees and finite-time error bounds. Our results demonstrate their robustness and practicality for modern RL tasks, establishing implicit TD as a versatile tool for policy evaluation and value approximation."
  },
  {
    "title": "Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story",
    "url": "http://arxiv.org/abs/2505.01336v1",
    "arxiv_id": "2505.01336v1",
    "authors": [
      "Vincenzo De Paola",
      "Riccardo Zamboni",
      "Mirco Mutti",
      "Marcello Restelli"
    ],
    "published": "2025-05-02T15:08:17+00:00",
    "summary": "Parallel data collection has redefined Reinforcement Learning (RL), unlocking unprecedented efficiency and powering breakthroughs in large-scale real-world applications. In this paradigm, $N$ identical agents operate in $N$ replicas of an environment simulator, accelerating data collection by a factor of $N$. A critical question arises: \\textit{Does specializing the policies of the parallel agents hold the key to surpass the $N$ factor acceleration?} In this paper, we introduce a novel learning framework that maximizes the entropy of collected data in a parallel setting. Our approach carefully balances the entropy of individual agents with inter-agent diversity, effectively minimizing redundancies. The latter idea is implemented with a centralized policy gradient method, which shows promise when evaluated empirically against systems of identical agents, as well as synergy with batch RL techniques that can exploit data diversity. Finally, we provide an original concentration analysis that shows faster rates for specialized parallel sampling distributions, which supports our methodology and may be of independent interest."
  },
  {
    "title": "Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.01332v1",
    "arxiv_id": "2505.01332v1",
    "authors": [
      "Mohammed Sumayli",
      "Olugbenga Moses Anubi"
    ],
    "published": "2025-05-02T15:05:29+00:00",
    "summary": "Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and improve user comfort. By enabling intelligent control and optimization of household energy consumption, HEMS plays a significant role in bridging the gap between consumer needs and energy utility objectives. However, much of the existing literature construes consumer comfort as a mere deviation from the standard appliance settings. Such deviations are typically incorporated into optimization objectives via static weighting factors. These factors often overlook the dynamic nature of consumer behaviors and preferences. Addressing this oversight, our paper introduces a multi-mode Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize based on dynamic, consumer-defined preferences. Our primary goal is to augment consumer involvement in Demand Response (DR) programs by embedding dynamic multi-mode preferences tailored to individual appliances. In this study, we leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework that is not only dynamic but also user-friendly. To validate its efficacy, we employed real-world data at 15-minute intervals, including metrics such as electricity price, ambient temperature, and appliances' power consumption. Our results show that the model performs exceptionally well in optimizing energy consumption within different preference modes. Furthermore, when compared to traditional algorithms based on Mixed-Integer Linear Programming (MILP), our model achieves nearly optimal performance while outperforming in computational efficiency."
  },
  {
    "title": "Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.01115v1",
    "arxiv_id": "2505.01115v1",
    "authors": [
      "Palok Biswas",
      "Zuzanna Osika",
      "Isidoro Tamassia",
      "Adit Whorra",
      "Jazmin Zatarain-Salazar",
      "Jan Kwakkel",
      "Frans A. Oliehoek",
      "Pradeep K. Murukannaiah"
    ],
    "published": "2025-05-02T08:52:56+00:00",
    "summary": "Addressing climate change requires coordinated policy efforts of nations worldwide. These efforts are informed by scientific reports, which rely in part on Integrated Assessment Models (IAMs), prominent tools used to assess the economic impacts of climate policies. However, traditional IAMs optimize policies based on a single objective, limiting their ability to capture the trade-offs among economic growth, temperature goals, and climate justice. As a result, policy recommendations have been criticized for perpetuating inequalities, fueling disagreements during policy negotiations. We introduce Justice, the first framework integrating IAM with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice generates policy recommendations that shed light on equity while balancing climate and economic goals. Further, using multiple agents can provide a realistic representation of the interactions among the diverse policy actors. We identify equitable Pareto-optimal policies using our framework, which facilitates deliberative decision-making by presenting policymakers with the inherent trade-offs in climate and economic policy."
  },
  {
    "title": "Multi-Objective Reinforcement Learning for Water Management",
    "url": "http://arxiv.org/abs/2505.01094v1",
    "arxiv_id": "2505.01094v1",
    "authors": [
      "Zuzanna Osika",
      "Roxana Radelescu",
      "Jazmin Zatarain Salazar",
      "Frans Oliehoek",
      "Pradeep K. Murukannaiah"
    ],
    "published": "2025-05-02T08:14:01+00:00",
    "summary": "Many real-world problems (e.g., resource management, autonomous driving, drug discovery) require optimizing multiple, conflicting objectives. Multi-objective reinforcement learning (MORL) extends classic reinforcement learning to handle multiple objectives simultaneously, yielding a set of policies that capture various trade-offs. However, the MORL field lacks complex, realistic environments and benchmarks. We introduce a water resource (Nile river basin) management case study and model it as a MORL environment. We then benchmark existing MORL algorithms on this task. Our results show that specialized water management methods outperform state-of-the-art MORL approaches, underscoring the scalability challenges MORL algorithms face in real-world scenarios."
  },
  {
    "title": "Llama-Nemotron: Efficient Reasoning Models",
    "url": "http://arxiv.org/abs/2505.00949v1",
    "arxiv_id": "2505.00949v1",
    "authors": [
      "Akhiad Bercovich",
      "Itay Levy",
      "Izik Golan",
      "Mohammad Dabbah",
      "Ran El-Yaniv",
      "Omri Puny",
      "Ido Galil",
      "Zach Moshe",
      "Tomer Ronen",
      "Najeeb Nabwani",
      "Ido Shahaf",
      "Oren Tropp",
      "Ehud Karpas",
      "Ran Zilberstein",
      "Jiaqi Zeng",
      "Soumye Singhal",
      "Alexander Bukharin",
      "Yian Zhang",
      "Tugrul Konuk",
      "Gerald Shen",
      "Ameya Sunil Mahabaleshwarkar",
      "Bilal Kartal",
      "Yoshi Suhara",
      "Olivier Delalleau",
      "Zijia Chen",
      "Zhilin Wang",
      "David Mosallanezhad",
      "Adi Renduchintala",
      "Haifeng Qian",
      "Dima Rekesh",
      "Fei Jia",
      "Somshubra Majumdar",
      "Vahid Noroozi",
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Aleksander Ficek",
      "Mehrzad Samadi",
      "Jocelyn Huang",
      "Siddhartha Jain",
      "Igor Gitman",
      "Ivan Moshkov",
      "Wei Du",
      "Shubham Toshniwal",
      "George Armstrong",
      "Branislav Kisacanin",
      "Matvei Novikov",
      "Daria Gitman",
      "Evelina Bakhturina",
      "Jane Polak Scowcroft",
      "John Kamalu",
      "Dan Su",
      "Kezhi Kong",
      "Markus Kliegl",
      "Rabeeh Karimi",
      "Ying Lin",
      "Sanjeev Satheesh",
      "Jupinder Parmar",
      "Pritam Gundecha",
      "Brandon Norick",
      "Joseph Jennings",
      "Shrimai Prabhumoye",
      "Syeda Nahida Akter",
      "Mostofa Patwary",
      "Abhinav Khattar",
      "Deepak Narayanan",
      "Roger Waleffe",
      "Jimmy Zhang",
      "Bor-Yiing Su",
      "Guyue Huang",
      "Terry Kong",
      "Parth Chadha",
      "Sahil Jain",
      "Christine Harvey",
      "Elad Segal",
      "Jining Huang",
      "Sergey Kashirsky",
      "Robert McQueen",
      "Izzy Putterman",
      "George Lam",
      "Arun Venkatesan",
      "Sherry Wu",
      "Vinh Nguyen",
      "Manoj Kilaru",
      "Andrew Wang",
      "Anna Warno",
      "Abhilash Somasamudramath",
      "Sandip Bhaskar",
      "Maka Dong",
      "Nave Assaf",
      "Shahar Mor",
      "Omer Ullman Argov",
      "Scot Junkin",
      "Oleksandr Romanenko",
      "Pedro Larroy",
      "Monika Katariya",
      "Marco Rovinelli",
      "Viji Balas",
      "Nicholas Edelman",
      "Anahita Bhiwandiwalla",
      "Muthu Subramaniam",
      "Smita Ithape",
      "Karthik Ramamoorthy",
      "Yuting Wu",
      "Suguna Varshini Velury",
      "Omri Almog",
      "Joyjit Daw",
      "Denys Fridman",
      "Erick Galinkin",
      "Michael Evans",
      "Katherine Luna",
      "Leon Derczynski",
      "Nikki Pope",
      "Eileen Long",
      "Seth Schneider",
      "Guillermo Siman",
      "Tomasz Grzegorzek",
      "Pablo Ribalta",
      "Monika Katariya",
      "Joey Conway",
      "Trisha Saar",
      "Ann Guan",
      "Krzysztof Pawelec",
      "Shyamala Prayaga",
      "Oleksii Kuchaiev",
      "Boris Ginsburg",
      "Oluwatobi Olabiyi",
      "Kari Briski",
      "Jonathan Cohen",
      "Bryan Catanzaro",
      "Jonah Alben",
      "Yonatan Geifman",
      "Eric Chung"
    ],
    "published": "2025-05-02T01:35:35+00:00",
    "summary": "We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM."
  },
  {
    "title": "Learning Neural Control Barrier Functions from Offline Data with Conservatism",
    "url": "http://arxiv.org/abs/2505.00908v1",
    "arxiv_id": "2505.00908v1",
    "authors": [
      "Ihab Tabbara",
      "Hussein Sibai"
    ],
    "published": "2025-05-01T23:01:03+00:00",
    "summary": "Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms, however, suffer from the curse of dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we contribute to this line of work by proposing an algorithm for training control barrier functions from offline datasets. Our algorithm trains the filter to not only prevent the system from reaching unsafe states but also out-of-distribution ones, at which the filter would be unreliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety and out-of-distribution avoidance while minimally affecting task performance."
  },
  {
    "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation",
    "url": "http://arxiv.org/abs/2505.00831v1",
    "arxiv_id": "2505.00831v1",
    "authors": [
      "Quang P. M. Pham",
      "Khoi T. N. Nguyen",
      "Nhi H. Doan",
      "Cuong A. Pham",
      "Kentaro Inui",
      "Dezhen Song"
    ],
    "published": "2025-05-01T19:44:36+00:00",
    "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics."
  },
  {
    "title": "E-Graphs With Bindings",
    "url": "http://arxiv.org/abs/2505.00807v1",
    "arxiv_id": "2505.00807v1",
    "authors": [
      "Aleksei Tiurin",
      "Dan R. Ghica",
      "Nick Hu"
    ],
    "published": "2025-05-01T19:08:22+00:00",
    "summary": "Equality saturation, a technique for program optimisation and reasoning, has gained attention due to the resurgence of equality graphs (e-graphs). E-graphs represent equivalence classes of terms under rewrite rules, enabling simultaneous rewriting across a family of terms. However, they struggle in domains like $\\lambda$-calculus that involve variable binding, due to a lack of native support for bindings. Building on recent work interpreting e-graphs categorically as morphisms in semilattice-enriched symmetric monoidal categories, we extend this framework to closed symmetric monoidal categories to handle bindings. We provide a concrete combinatorial representation using hierarchical hypergraphs and introduce a corresponding double-pushout (DPO) rewriting mechanism. Finally, we establish the equivalence of term rewriting and DPO rewriting, with the key property that the combinatorial representation absorbs the equations of the symmetric monoidal category."
  },
  {
    "title": "Constructing an Optimal Behavior Basis for the Option Keyboard",
    "url": "http://arxiv.org/abs/2505.00787v1",
    "arxiv_id": "2505.00787v1",
    "authors": [
      "Lucas N. Alegre",
      "Ana L. C. Bazzan",
      "Andr\u00e9 Barreto",
      "Bruno C. da Silva"
    ],
    "published": "2025-05-01T18:32:21+00:00",
    "summary": "Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases."
  },
  {
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT",
    "url": "http://arxiv.org/abs/2505.00703v1",
    "arxiv_id": "2505.00703v1",
    "authors": [
      "Dongzhi Jiang",
      "Ziyu Guo",
      "Renrui Zhang",
      "Zhuofan Zong",
      "Hao Li",
      "Le Zhuo",
      "Shilin Yan",
      "Pheng-Ann Heng",
      "Hongsheng Li"
    ],
    "published": "2025-05-01T17:59:46+00:00",
    "summary": "Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1"
  },
  {
    "title": "Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions",
    "url": "http://arxiv.org/abs/2505.00671v1",
    "arxiv_id": "2505.00671v1",
    "authors": [
      "Chenggang Wang",
      "Xinyi Wang",
      "Yutong Dong",
      "Lei Song",
      "Xinping Guan"
    ],
    "published": "2025-05-01T17:22:11+00:00",
    "summary": "The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process."
  },
  {
    "title": "Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments",
    "url": "http://arxiv.org/abs/2505.00668v1",
    "arxiv_id": "2505.00668v1",
    "authors": [
      "Kirtan Rajesh",
      "Suvidha Rupesh Kumar"
    ],
    "published": "2025-05-01T17:19:48+00:00",
    "summary": "Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management."
  },
  {
    "title": "Wasserstein Policy Optimization",
    "url": "http://arxiv.org/abs/2505.00663v1",
    "arxiv_id": "2505.00663v1",
    "authors": [
      "David Pfau",
      "Ian Davies",
      "Diana Borsa",
      "Joao G. M. Araujo",
      "Brendan Tracey",
      "Hado van Hasselt"
    ],
    "published": "2025-05-01T17:07:01+00:00",
    "summary": "We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm for reinforcement learning in continuous action spaces. WPO can be derived as an approximation to Wasserstein gradient flow over the space of all policies projected into a finite-dimensional parameter space (e.g., the weights of a neural network), leading to a simple and completely general closed-form update. The resulting algorithm combines many properties of deterministic and classic policy gradient methods. Like deterministic policy gradients, it exploits knowledge of the gradient of the action-value function with respect to the action. Like classic policy gradients, it can be applied to stochastic policies with arbitrary distributions over actions -- without using the reparameterization trick. We show results on the DeepMind Control Suite and a magnetic confinement fusion task which compare favorably with state-of-the-art continuous control methods."
  },
  {
    "title": "DeepCritic: Deliberate Critique with Large Language Models",
    "url": "http://arxiv.org/abs/2505.00662v1",
    "arxiv_id": "2505.00662v1",
    "authors": [
      "Wenkai Yang",
      "Jingwen Chen",
      "Yankai Lin",
      "Ji-Rong Wen"
    ],
    "published": "2025-05-01T17:03:17+00:00",
    "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v1",
    "arxiv_id": "2505.00551v1",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
    "url": "http://arxiv.org/abs/2505.00551v2",
    "arxiv_id": "2505.00551v2",
    "authors": [
      "Chong Zhang",
      "Yue Deng",
      "Xiang Lin",
      "Bin Wang",
      "Dianwen Ng",
      "Hai Ye",
      "Xingxuan Li",
      "Yao Xiao",
      "Zhanfeng Mo",
      "Qi Zhang",
      "Lidong Bing"
    ],
    "published": "2025-05-01T14:28:35+00:00",
    "summary": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs."
  },
  {
    "title": "Directly Forecasting Belief for Reinforcement Learning with Delays",
    "url": "http://arxiv.org/abs/2505.00546v1",
    "arxiv_id": "2505.00546v1",
    "authors": [
      "Qingyuan Wu",
      "Yuhui Wang",
      "Simon Sinong Zhan",
      "Yixuan Wang",
      "Chung-Wei Lin",
      "Chen Lv",
      "Qi Zhu",
      "J\u00fcrgen Schmidhuber",
      "Chao Huang"
    ],
    "published": "2025-05-01T14:20:48+00:00",
    "summary": "Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines."
  },
  {
    "title": "Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication",
    "url": "http://arxiv.org/abs/2505.00540v1",
    "arxiv_id": "2505.00540v1",
    "authors": [
      "Ian O'Flynn",
      "Harun \u0160iljak"
    ],
    "published": "2025-05-01T14:05:46+00:00",
    "summary": "We present a reinforcement learning strategy for use in multi-agent foraging systems in which the learning is centralised to a single agent and its model is periodically disseminated among the population of non-learning agents. In a domain where multi-agent reinforcement learning (MARL) is the common approach, this approach aims to significantly reduce the computational and energy demands compared to approaches such as MARL and centralised learning models. By developing high performing foraging agents, these approaches can be translated into real-world applications such as logistics, environmental monitoring, and autonomous exploration. A reward function was incorporated into this approach that promotes role development among agents, without explicit directives. This led to the differentiation of behaviours among the agents. The implicit encouragement of role differentiation allows for dynamic actions in which agents can alter roles dependent on their interactions with the environment without the need for explicit communication between agents."
  },
  {
    "title": "Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks",
    "url": "http://arxiv.org/abs/2505.00530v1",
    "arxiv_id": "2505.00530v1",
    "authors": [
      "Xinyu Wang",
      "Jinbo Bi",
      "Minghu Song"
    ],
    "published": "2025-05-01T13:57:20+00:00",
    "summary": "SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery."
  },
  {
    "title": "Variational OOD State Correction for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.00503v1",
    "arxiv_id": "2505.00503v1",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Xiaoyang Tan"
    ],
    "published": "2025-05-01T13:14:07+00:00",
    "summary": "The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites."
  },
  {
    "title": "Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations",
    "url": "http://arxiv.org/abs/2505.00500v1",
    "arxiv_id": "2505.00500v1",
    "authors": [
      "Minseok Song",
      "JeongHo Ha",
      "Bonggyeong Park",
      "Daehyung Park"
    ],
    "published": "2025-05-01T13:00:56+00:00",
    "summary": "We aim to solve the problem of manipulating deformable objects, particularly elastic bands, in real-world scenarios. However, deformable object manipulation (DOM) requires a policy that works on a large state space due to the unlimited degree of freedom (DoF) of deformable objects. Further, their dense but partial observations (e.g., images or point clouds) may increase the sampling complexity and uncertainty in policy learning. To figure it out, we propose a novel implicit neural-representation (INR) learning for elastic DOMs, called INR-DOM. Our method learns consistent state representations associated with partially observable elastic objects reconstructing a complete and implicit surface represented as a signed distance function. Furthermore, we perform exploratory representation fine-tuning through reinforcement learning (RL) that enables RL algorithms to effectively learn exploitable representations while efficiently obtaining a DOM policy. We perform quantitative and qualitative analyses building three simulated environments and real-world manipulation studies with a Franka Emika Panda arm. Videos are available at http://inr-dom.github.io."
  },
  {
    "title": "MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion",
    "url": "http://arxiv.org/abs/2505.00488v1",
    "arxiv_id": "2505.00488v1",
    "authors": [
      "Vamshi Kumar Kurva",
      "Shishir Kolathaya"
    ],
    "published": "2025-05-01T12:41:35+00:00",
    "summary": "Quadrupedal robots are increasingly deployed for load-carrying tasks across diverse terrains. While Model Predictive Control (MPC)-based methods can account for payload variations, they often depend on predefined gait schedules or trajectory generators, limiting their adaptability in unstructured environments. To address these limitations, we propose an Adaptive Reinforcement Learning (RL) framework that enables quadrupedal robots to dynamically adapt to both varying payloads and diverse terrains. The framework consists of a nominal policy responsible for baseline locomotion and an adaptive policy that learns corrective actions to preserve stability and improve command tracking under payload variations. We validate the proposed approach through large-scale simulation experiments in Isaac Gym and real-world hardware deployment on a Unitree Go1 quadruped. The controller was tested on flat ground, slopes, and stairs under both static and dynamic payload changes. Across all settings, our adaptive controller consistently outperformed the controller in tracking body height and velocity commands, demonstrating enhanced robustness and adaptability without requiring explicit gait design or manual tuning."
  },
  {
    "title": "Approximation to Deep Q-Network by Stochastic Delay Differential Equations",
    "url": "http://arxiv.org/abs/2505.00382v1",
    "arxiv_id": "2505.00382v1",
    "authors": [
      "Jianya Lu",
      "Yingjun Mo"
    ],
    "published": "2025-05-01T08:19:24+00:00",
    "summary": "Despite the significant breakthroughs that the Deep Q-Network (DQN) has brought to reinforcement learning, its theoretical analysis remains limited. In this paper, we construct a stochastic differential delay equation (SDDE) based on the DQN algorithm and estimate the Wasserstein-1 distance between them. We provide an upper bound for the distance and prove that the distance between the two converges to zero as the step size approaches zero. This result allows us to understand DQN's two key techniques, the experience replay and the target network, from the perspective of continuous systems. Specifically, the delay term in the equation, corresponding to the target network, contributes to the stability of the system. Our approach leverages a refined Lindeberg principle and an operator comparison to establish these results."
  },
  {
    "title": "Reinforcement Learning with Continuous Actions Under Unmeasured Confounding",
    "url": "http://arxiv.org/abs/2505.00304v1",
    "arxiv_id": "2505.00304v1",
    "authors": [
      "Yuhan Li",
      "Eugene Han",
      "Yifan Hu",
      "Wenzhuo Zhou",
      "Zhengling Qi",
      "Yifan Cui",
      "Ruoqing Zhu"
    ],
    "published": "2025-05-01T04:55:29+00:00",
    "summary": "This paper addresses the challenge of offline policy learning in reinforcement learning with continuous action spaces when unmeasured confounders are present. While most existing research focuses on policy evaluation within partially observable Markov decision processes (POMDPs) and assumes discrete action spaces, we advance this field by establishing a novel identification result to enable the nonparametric estimation of policy value for a given target policy under an infinite-horizon framework. Leveraging this identification, we develop a minimax estimator and introduce a policy-gradient-based algorithm to identify the in-class optimal policy that maximizes the estimated policy value. Furthermore, we provide theoretical results regarding the consistency, finite-sample error bound, and regret bound of the resulting optimal policy. Extensive simulations and a real-world application using the German Family Panel data demonstrate the effectiveness of our proposed methodology."
  },
  {
    "title": "Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2505.00299v1",
    "arxiv_id": "2505.00299v1",
    "authors": [
      "Yang Wang",
      "Tengda Tang",
      "Zhou Fang",
      "Yingnan Deng",
      "Yifei Duan"
    ],
    "published": "2025-05-01T04:42:48+00:00",
    "summary": "To address the challenges of high resource dynamism and intensive task concurrency in microservice systems, this paper proposes an adaptive resource scheduling method based on the A3C reinforcement learning algorithm. The scheduling problem is modeled as a Markov Decision Process, where policy and value networks are jointly optimized to enable fine-grained resource allocation under varying load conditions. The method incorporates an asynchronous multi-threaded learning mechanism, allowing multiple agents to perform parallel sampling and synchronize updates to the global network parameters. This design improves both policy convergence efficiency and model stability. In the experimental section, a real-world dataset is used to construct a scheduling scenario. The proposed method is compared with several typical approaches across multiple evaluation metrics, including task delay, scheduling success rate, resource utilization, and convergence speed. The results show that the proposed method delivers high scheduling performance and system stability in multi-task concurrent environments. It effectively alleviates the resource allocation bottlenecks faced by traditional methods under heavy load, demonstrating its practical value for intelligent scheduling in microservice systems."
  },
  {
    "title": "Investigating Adaptive Tuning of Assistive Exoskeletons Using Offline Reinforcement Learning: Challenges and Insights",
    "url": "http://arxiv.org/abs/2505.00201v1",
    "arxiv_id": "2505.00201v1",
    "authors": [
      "Yasin Findik",
      "Christopher Coco",
      "Reza Azadeh"
    ],
    "published": "2025-04-30T21:59:56+00:00",
    "summary": "Assistive exoskeletons have shown great potential in enhancing mobility for individuals with motor impairments, yet their effectiveness relies on precise parameter tuning for personalized assistance. In this study, we investigate the potential of offline reinforcement learning for optimizing effort thresholds in upper-limb assistive exoskeletons, aiming to reduce reliance on manual calibration. Specifically, we frame the problem as a multi-agent system where separate agents optimize biceps and triceps effort thresholds, enabling a more adaptive and data-driven approach to exoskeleton control. Mixed Q-Functionals (MQF) is employed to efficiently handle continuous action spaces while leveraging pre-collected data, thereby mitigating the risks associated with real-time exploration. Experiments were conducted using the MyoPro 2 exoskeleton across two distinct tasks involving horizontal and vertical arm movements. Our results indicate that the proposed approach can dynamically adjust threshold values based on learned patterns, potentially improving user interaction and control, though performance evaluation remains challenging due to dataset limitations."
  },
  {
    "title": "Neuroevolution of Self-Attention Over Proto-Objects",
    "url": "http://arxiv.org/abs/2505.00186v1",
    "arxiv_id": "2505.00186v1",
    "authors": [
      "Rafael C. Pinto",
      "Anderson R. Tavares"
    ],
    "published": "2025-04-30T21:01:20+00:00",
    "summary": "Proto-objects - image regions that share common visual properties - offer a promising alternative to traditional attention mechanisms based on rectangular-shaped image patches in neural networks. Although previous work demonstrated that evolving a patch-based hard-attention module alongside a controller network could achieve state-of-the-art performance in visual reinforcement learning tasks, our approach leverages image segmentation to work with higher-level features. By operating on proto-objects rather than fixed patches, we significantly reduce the representational complexity: each image decomposes into fewer proto-objects than regular patches, and each proto-object can be efficiently encoded as a compact feature vector. This enables a substantially smaller self-attention module that processes richer semantic information. Our experiments demonstrate that this proto-object-based approach matches or exceeds the state-of-the-art performance of patch-based implementations with 62% less parameters and 2.6 times less training time."
  },
  {
    "title": "Deep Reinforcement Learning Policies for Underactuated Satellite Attitude Control",
    "url": "http://arxiv.org/abs/2505.00165v1",
    "arxiv_id": "2505.00165v1",
    "authors": [
      "Matteo El Hariry",
      "Andrea Cini",
      "Giacomo Mellone",
      "Alessandro Balossino"
    ],
    "published": "2025-04-30T20:24:49+00:00",
    "summary": "Autonomy is a key challenge for future space exploration endeavours. Deep Reinforcement Learning holds the promises for developing agents able to learn complex behaviours simply by interacting with their environment. This paper investigates the use of Reinforcement Learning for the satellite attitude control problem, namely the angular reorientation of a spacecraft with respect to an in- ertial frame of reference. In the proposed approach, a set of control policies are implemented as neural networks trained with a custom version of the Proximal Policy Optimization algorithm to maneuver a small satellite from a random starting angle to a given pointing target. In particular, we address the problem for two working conditions: the nominal case, in which all the actuators (a set of 3 reac- tion wheels) are working properly, and the underactuated case, where an actuator failure is simulated randomly along with one of the axes. We show that the agents learn to effectively perform large-angle slew maneuvers with fast convergence and industry-standard pointing accuracy. Furthermore, we test the proposed method on representative hardware, showing that by taking adequate measures controllers trained in simulation can perform well in real systems."
  },
  {
    "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition",
    "url": "http://arxiv.org/abs/2504.21801v1",
    "arxiv_id": "2504.21801v1",
    "authors": [
      "Z. Z. Ren",
      "Zhihong Shao",
      "Junxiao Song",
      "Huajian Xin",
      "Haocheng Wang",
      "Wanjia Zhao",
      "Liyue Zhang",
      "Zhe Fu",
      "Qihao Zhu",
      "Dejian Yang",
      "Z. F. Wu",
      "Zhibin Gou",
      "Shirong Ma",
      "Hongxuan Tang",
      "Yuxuan Liu",
      "Wenjun Gao",
      "Daya Guo",
      "Chong Ruan"
    ],
    "published": "2025-04-30T16:57:48+00:00",
    "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing."
  },
  {
    "title": "Reconciling Discrete-Time Mixed Policies and Continuous-Time Relaxed Controls in Reinforcement Learning and Stochastic Control",
    "url": "http://arxiv.org/abs/2504.21793v1",
    "arxiv_id": "2504.21793v1",
    "authors": [
      "Rene Carmona",
      "Mathieu Lauriere"
    ],
    "published": "2025-04-30T16:50:52+00:00",
    "summary": "Reinforcement learning (RL) is currently one of the most popular methods, with breakthrough results in a variety of fields. The framework relies on the concept of Markov decision process (MDP), which corresponds to a discrete time optimal control problem. In the RL literature, such problems are usually formulated with mixed policies, from which a random action is sampled at each time step. Recently, the optimal control community has studied continuous-time versions of RL algorithms, replacing MDPs with mixed policies by continuous time stochastic processes with relaxed controls. In this work, we rigorously connect the two problems: we prove the strong convergence of the former towards the latter when the time discretization goes to $0$."
  },
  {
    "title": "MAGNET: an open-source library for mesh agglomeration by Graph Neural Networks",
    "url": "http://arxiv.org/abs/2504.21780v1",
    "arxiv_id": "2504.21780v1",
    "authors": [
      "Paola F. Antonietti",
      "Matteo Caldana",
      "Ilario Mazzieri",
      "Andrea Re Fraschini"
    ],
    "published": "2025-04-30T16:33:22+00:00",
    "summary": "We introduce MAGNET, an open-source Python library designed for mesh agglomeration in both two- and three-dimensions, based on employing Graph Neural Networks (GNN). MAGNET serves as a comprehensive solution for training a variety of GNN models, integrating deep learning and other advanced algorithms such as METIS and k-means to facilitate mesh agglomeration and quality metric computation. The library's introduction is outlined through its code structure and primary features. The GNN framework adopts a graph bisection methodology that capitalizes on connectivity and geometric mesh information via SAGE convolutional layers, in line with the methodology proposed by Antonietti et al. (2024). Additionally, the proposed MAGNET library incorporates reinforcement learning to enhance the accuracy and robustness of the model for predicting coarse partitions within a multilevel framework. A detailed tutorial is provided to guide the user through the process of mesh agglomeration and the training of a GNN bisection model. We present several examples of mesh agglomeration conducted by MAGNET, demonstrating the library's applicability across various scenarios. Furthermore, the performance of the newly introduced models is contrasted with that of METIS and k-means, illustrating that the proposed GNN models are competitive regarding partition quality and computational efficiency. Finally, we exhibit the versatility of MAGNET's interface through its integration with Lymph, an open-source library implementing discontinuous Galerkin methods on polytopal grids for the numerical discretization of multiphysics differential problems."
  },
  {
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
    "url": "http://arxiv.org/abs/2504.21776v1",
    "arxiv_id": "2504.21776v1",
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Guanting Dong",
      "Hongjin Qian",
      "Yutao Zhu",
      "Yongkang Wu",
      "Ji-Rong Wen",
      "Zhicheng Dou"
    ],
    "published": "2025-04-30T16:25:25+00:00",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker."
  },
  {
    "title": "LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning",
    "url": "http://arxiv.org/abs/2504.21738v1",
    "arxiv_id": "2504.21738v1",
    "authors": [
      "Yiyang Shao",
      "Xiaoyu Huang",
      "Bike Zhang",
      "Qiayuan Liao",
      "Yuman Gao",
      "Yufeng Chi",
      "Zhongyu Li",
      "Sophia Shao",
      "Koushil Sreenath"
    ],
    "published": "2025-04-30T15:37:23+00:00",
    "summary": "General-purpose humanoid robots are expected to interact intuitively with humans, enabling seamless integration into daily life. Natural language provides the most accessible medium for this purpose. However, translating language into humanoid whole-body motion remains a significant challenge, primarily due to the gap between linguistic understanding and physical actions. In this work, we present an end-to-end, language-directed policy for real-world humanoid whole-body control. Our approach combines reinforcement learning with policy distillation, allowing a single neural network to interpret language commands and execute corresponding physical actions directly. To enhance motion diversity and compositionality, we incorporate a Conditional Variational Autoencoder (CVAE) structure. The resulting policy achieves agile and versatile whole-body behaviors conditioned on language inputs, with smooth transitions between various motions, enabling adaptation to linguistic variations and the emergence of novel motions. We validate the efficacy and generalizability of our method through extensive simulations and real-world experiments, demonstrating robust whole-body control. Please see our website at LangWBC.github.io for more information."
  },
  {
    "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.21731v1",
    "arxiv_id": "2504.21731v1",
    "authors": [
      "Feiyu Lu",
      "Mengyu Chen",
      "Hsiang Hsu",
      "Pranav Deshpande",
      "Cheng Yao Wang",
      "Blair MacIntyre"
    ],
    "published": "2025-04-30T15:21:36+00:00",
    "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR."
  },
  {
    "title": "MovementVR: An open-source tool for the study of motor control and learning in virtual reality",
    "url": "http://arxiv.org/abs/2504.21696v1",
    "arxiv_id": "2504.21696v1",
    "authors": [
      "Cristina Rossi",
      "Rini Varghese",
      "Amy J Bastian"
    ],
    "published": "2025-04-30T14:39:01+00:00",
    "summary": "Virtual reality (VR) is increasingly used to enhance the ecological validity of motor control and learning studies by providing immersive, interactive environments with precise motion tracking. However, designing realistic VR-based motor tasks remains complex, requiring advanced programming skills and limiting accessibility in research and clinical settings. MovementVR is an open-source platform designed to address these challenges by enabling the creation of customizable, naturalistic reaching tasks in VR without coding expertise. It integrates physics-based hand-object interactions, real-time hand tracking, and flexible experimental paradigms, including motor adaptation and reinforcement learning. The intuitive graphical user interface (GUI) allows researchers to customize task parameters and paradigm structure. Unlike existing platforms, MovementVR eliminates the need for scripting while supporting extensive customization and preserving ecological validity and realism. In addition to reducing technical barriers, MovementVR lowers financial constraints by being compatible with consumer-grade VR headsets. It is freely available with comprehensive documentation, facilitating broader adoption in movement research and rehabilitation."
  },
  {
    "title": "Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation",
    "url": "http://arxiv.org/abs/2504.21643v1",
    "arxiv_id": "2504.21643v1",
    "authors": [
      "Luca Marzari",
      "Francesco Trotti",
      "Enrico Marchesini",
      "Alessandro Farinelli"
    ],
    "published": "2025-04-30T13:47:25+00:00",
    "summary": "Achieving safe autonomous navigation systems is critical for deploying robots in dynamic and uncertain real-world environments. In this paper, we propose a hierarchical control framework leveraging neural network verification techniques to design control barrier functions (CBFs) and policy correction mechanisms that ensure safe reinforcement learning navigation policies. Our approach relies on probabilistic enumeration to identify unsafe regions of operation, which are then used to construct a safe CBF-based control layer applicable to arbitrary policies. We validate our framework both in simulation and on a real robot, using a standard mobile robot benchmark and a highly dynamic aquatic environmental monitoring task. These experiments demonstrate the ability of the proposed solution to correct unsafe actions while preserving efficient navigation behavior. Our results show the promise of developing hierarchical verification-based systems to enable safe and robust navigation behaviors in complex scenarios."
  },
  {
    "title": "Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.21585v1",
    "arxiv_id": "2504.21585v1",
    "authors": [
      "Yingzhuo Jiang",
      "Wenjun Huang",
      "Rongdun Lin",
      "Chenyang Miao",
      "Tianfu Sun",
      "Yunduan Cui"
    ],
    "published": "2025-04-30T12:44:38+00:00",
    "summary": "This paper tackles the challenge of learning multi-goal dexterous hand manipulation tasks using model-based Reinforcement Learning. We propose Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing probabilistic neural network ensembles to describe the high-dimensional dexterous hand dynamics and introducing an asynchronous MPC policy to meet the control frequency requirements in real-world dexterous hand systems. Extensive evaluations on four simulated Shadow Hand manipulation scenarios with randomly generated goals demonstrate GC-PMPC's superior performance over state-of-the-art baselines. It successfully drives a cable-driven Dexterous hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn manipulating a cubic die to three goal poses within approximately 80 minutes of interactions, demonstrating exceptional learning efficiency and control performance on a cost-effective dexterous hand platform."
  },
  {
    "title": "TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration",
    "url": "http://arxiv.org/abs/2505.00055v1",
    "arxiv_id": "2505.00055v1",
    "authors": [
      "Zhuoqi Zeng",
      "Yuxiang Wei",
      "Jiawen Kang"
    ],
    "published": "2025-04-30T11:22:29+00:00",
    "summary": "Embodied Artificial Intelligence (EAI) addresses autonomous driving challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal perception, adaptive decision-making, and hardware-software co-scheduling. However, the computational demands of virtual services and the inherent mobility of autonomous vehicles (AVs) necessitate real-time migration of Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained Roadside Units (RSUs). This paper proposes a novel framework for efficient VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF) Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep reinforcement learning (MADRL) algorithm. First, We propose an virtual immersive experience-driven utility model that captures AV-RSU dynamic interactions by integrating AVs' social influence, service complementarity and substitutability, and RSUs' resource allocation strategies to optimize VEAAT migration decisions. Second, to enhance training efficiency and enable efficient deployment on computation-constrained AVs while preserving exploration-exploitation performance, we propose TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance based on agents' exploration incentives. Numerical results demonstrate that our approach achieves convergence comparable to baseline models and closely approximates the Stackelberg equilibrium."
  },
  {
    "title": "SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments",
    "url": "http://arxiv.org/abs/2504.21454v1",
    "arxiv_id": "2504.21454v1",
    "authors": [
      "Federico Nesti",
      "Gianluca D'Amico",
      "Mauro Marinoni",
      "Giorgio Buttazzo"
    ],
    "published": "2025-04-30T09:22:55+00:00",
    "summary": "The use of machine learning in cyber-physical systems has attracted the interest of both industry and academia. However, no general solution has yet been found against the unpredictable behavior of neural networks and reinforcement learning agents. Nevertheless, the improvements of photo-realistic simulators have paved the way towards extensive testing of complex algorithms in different virtual scenarios, which would be expensive and dangerous to implement in the real world.   This paper presents SimPRIVE, a simulation framework for physical robot interaction with virtual environments, which operates as a vehicle-in-the-loop platform, rendering a virtual world while operating the vehicle in the real world.   Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be configured to move its digital twin in a virtual world built with the Unreal Engine 5 graphic engine, which can be populated with objects, people, or other vehicles with programmable behavior.   SimPRIVE has been designed to accommodate custom or pre-built virtual worlds while being light-weight to contain execution times and allow fast rendering. Its main advantage lies in the possibility of testing complex algorithms on the full software and hardware stack while minimizing the risks and costs of a test campaign. The framework has been validated by testing a reinforcement learning agent trained for obstacle avoidance on an AgileX Scout Mini rover that navigates a virtual office environment where everyday objects and people are placed as obstacles. The physical rover moves with no collision in an indoor limited space, thanks to a LiDAR-based heuristic."
  },
  {
    "title": "NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence",
    "url": "http://arxiv.org/abs/2504.21433v1",
    "arxiv_id": "2504.21433v1",
    "authors": [
      "Zhicong Li",
      "Hangyu Mao",
      "Jiangjin Yin",
      "Mingzhe Xing",
      "Zhiwei Xu",
      "Yuanxing Zhang",
      "Yang Xiao"
    ],
    "published": "2025-04-30T08:46:14+00:00",
    "summary": "This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it."
  },
  {
    "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.21383v1",
    "arxiv_id": "2504.21383v1",
    "authors": [
      "Pulkit Agrawal",
      "Rukma Talwadker",
      "Aditya Pareek",
      "Tridib Mukherjee"
    ],
    "published": "2025-04-30T07:32:40+00:00",
    "summary": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning (RL) have primarily focused on addressing function approximation errors, which contribute to the overestimation of Q-values for out-of-distribution actions, a challenge that static datasets exacerbate. However, high stakes applications such as recommendation systems in online gaming, introduce further complexities due to player's psychology (intent) driven by gameplay experiences and the inherent volatility on the platform. These factors create highly sparse, partially overlapping state spaces across policies, further influenced by the experiment path selection logic which biases state spaces towards specific policies. Current SOTA methods constrain learning from such offline data by clipping known counterfactual actions as out-of-distribution due to poor generalization across unobserved states. Further aggravating conservative Q-learning and necessitating more online exploration. FAST-Q introduces a novel approach that (1) leverages Gradient Reversal Learning to construct balanced state representations, regularizing the policy-specific bias between the player's state and action thereby enabling counterfactual estimation; (2) supports offline counterfactual exploration in parallel with static data exploitation; and (3) proposes a Q-value decomposition strategy for multi-objective optimization, facilitating explainable recommendations over short and long-term objectives. These innovations demonstrate superiority of FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4 percent enhancement in the recommendation driven engagement, 2 percent improvement in the player's platform dwell time and an impressive 10 percent reduction in the costs associated with the recommendation, on our volatile gaming platform."
  },
  {
    "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
    "url": "http://arxiv.org/abs/2504.21370v1",
    "arxiv_id": "2504.21370v1",
    "authors": [
      "Jingyang Yi",
      "Jiazheng Wang"
    ],
    "published": "2025-04-30T07:04:19+00:00",
    "summary": "Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often \"overthink\", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible."
  },
  {
    "title": "Enhancing New-item Fairness in Dynamic Recommender Systems",
    "url": "http://arxiv.org/abs/2504.21362v1",
    "arxiv_id": "2504.21362v1",
    "authors": [
      "Huizhong Guo",
      "Zhu Sun",
      "Dongxia Wang",
      "Tianjun Wei",
      "Jinfeng Li",
      "Jie Zhang"
    ],
    "published": "2025-04-30T06:49:36+00:00",
    "summary": "New-items play a crucial role in recommender systems (RSs) for delivering fresh and engaging user experiences. However, traditional methods struggle to effectively recommend new-items due to their short exposure time and limited interaction records, especially in dynamic recommender systems (DRSs) where new-items get continuously introduced and users' preferences evolve over time. This leads to significant unfairness towards new-items, which could accumulate over the successive model updates, ultimately compromising the stability of the entire system. Therefore, we propose FairAgent, a reinforcement learning (RL)-based new-item fairness enhancement framework specifically designed for DRSs. It leverages knowledge distillation to extract collaborative signals from traditional models, retaining strong recommendation capabilities for old-items. In addition, FairAgent introduces a novel reward mechanism for recommendation tailored to the characteristics of DRSs, which consists of three components: 1) a new-item exploration reward to promote the exposure of dynamically introduced new-items, 2) a fairness reward to adapt to users' personalized fairness requirements for new-items, and 3) an accuracy reward which leverages users' dynamic feedback to enhance recommendation accuracy. Extensive experiments on three public datasets and backbone models demonstrate the superior performance of FairAgent. The results present that FairAgent can effectively boost new-item exposure, achieve personalized new-item fairness, while maintaining high recommendation accuracy."
  },
  {
    "title": "ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality",
    "url": "http://arxiv.org/abs/2504.21360v1",
    "arxiv_id": "2504.21360v1",
    "authors": [
      "Jaewook Lee",
      "Filippo Aleotti",
      "Diego Mazala",
      "Guillermo Garcia-Hernando",
      "Sara Vicente",
      "Oliver James Johnston",
      "Isabel Kraus-Liang",
      "Jakub Powierza",
      "Donghoon Shin",
      "Jon E. Froehlich",
      "Gabriel Brostow",
      "Jessica Van Brummelen"
    ],
    "published": "2025-04-30T06:44:14+00:00",
    "summary": "While augmented reality (AR) enables new ways to play, tell stories, and explore ideas rooted in the physical world, authoring personalized AR content remains difficult for non-experts, often requiring professional tools and time. Prior systems have explored AI-driven XR design but typically rely on manually-defined environments and fixed asset libraries, limiting creative flexibility and real-world relevance. We introduce ImaginateAR, a mobile AI-assisted AR authoring system that aims to let anyone build anything, anywhere -- simply by speaking their imagination. ImaginateAR is powered by custom pipelines for offline scene understanding, fast 3D asset generation, and LLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\" (P7) and iteratively refine the scene using both AI and manual tools. Our technical evaluation shows that ImaginateAR produces more accurate outdoor scene graphs and generates 3D meshes faster than prior methods. A three-part user study (N=20) revealed preferred roles for AI in authoring, what and how users create in free-form use, and design implications for future AR authoring tools."
  },
  {
    "title": "Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection",
    "url": "http://arxiv.org/abs/2504.21357v1",
    "arxiv_id": "2504.21357v1",
    "authors": [
      "Yang Suwen",
      "Shi Lei"
    ],
    "published": "2025-04-30T06:31:05+00:00",
    "summary": "With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort."
  },
  {
    "title": "Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection",
    "url": "http://arxiv.org/abs/2504.21357v2",
    "arxiv_id": "2504.21357v2",
    "authors": [
      "Suwen Yang",
      "Lei Shi"
    ],
    "published": "2025-04-30T06:31:05+00:00",
    "summary": "With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort."
  },
  {
    "title": "Q-function Decomposition with Intervention Semantics with Factored Action Spaces",
    "url": "http://arxiv.org/abs/2504.21326v1",
    "arxiv_id": "2504.21326v1",
    "authors": [
      "Junkyu Lee",
      "Tian Gao",
      "Elliot Nelson",
      "Miao Liu",
      "Debarun Bhattacharjya",
      "Songtao Lu"
    ],
    "published": "2025-04-30T05:26:51+00:00",
    "summary": "Many practical reinforcement learning environments have a discrete factored action space that induces a large combinatorial set of actions, thereby posing significant challenges. Existing approaches leverage the regular structure of the action space and resort to a linear decomposition of Q-functions, which avoids enumerating all combinations of factored actions. In this paper, we consider Q-functions defined over a lower dimensional projected subspace of the original action space, and study the condition for the unbiasedness of decomposed Q-functions using causal effect estimation from the no unobserved confounder setting in causal statistics. This leads to a general scheme which we call action decomposed reinforcement learning that uses the projected Q-functions to approximate the Q-function in standard model-free reinforcement learning algorithms. The proposed approach is shown to improve sample complexity in a model-based reinforcement learning setting. We demonstrate improvements in sample efficiency compared to state-of-the-art baselines in online continuous control environments and a real-world offline sepsis treatment environment."
  },
  {
    "title": "Phi-4-reasoning Technical Report",
    "url": "http://arxiv.org/abs/2504.21318v1",
    "arxiv_id": "2504.21318v1",
    "authors": [
      "Marah Abdin",
      "Sahaj Agarwal",
      "Ahmed Awadallah",
      "Vidhisha Balachandran",
      "Harkirat Behl",
      "Lingjiao Chen",
      "Gustavo de Rosa",
      "Suriya Gunasekar",
      "Mojan Javaheripi",
      "Neel Joshi",
      "Piero Kauffmann",
      "Yash Lara",
      "Caio C\u00e9sar Teodoro Mendes",
      "Arindam Mitra",
      "Besmira Nushi",
      "Dimitris Papailiopoulos",
      "Olli Saarikivi",
      "Shital Shah",
      "Vaishnavi Shrivastava",
      "Vibhav Vineet",
      "Yue Wu",
      "Safoora Yousefi",
      "Guoqing Zheng"
    ],
    "published": "2025-04-30T05:05:09+00:00",
    "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models."
  },
  {
    "title": "Covert Prompt Transmission for Secure Large Language Model Services",
    "url": "http://arxiv.org/abs/2504.21311v1",
    "arxiv_id": "2504.21311v1",
    "authors": [
      "Ruichen Zhang",
      "Yinqiu Liu",
      "Shunpu Tang",
      "Jiacheng Wang",
      "Dusit Niyato",
      "Geng Sun",
      "Yonghui Li",
      "Sumei Sun"
    ],
    "published": "2025-04-30T04:53:11+00:00",
    "summary": "This paper investigates covert prompt transmission for secure and efficient large language model (LLM) services over wireless networks. We formulate a latency minimization problem under fidelity and detectability constraints to ensure confidential and covert communication by jointly optimizing the transmit power and prompt compression ratio. To solve this problem, we first propose a prompt compression and encryption (PCAE) framework, performing surprisal-guided compression followed by lightweight permutation-based encryption. Specifically, PCAE employs a locally deployed small language model (SLM) to estimate token-level surprisal scores, selectively retaining semantically critical tokens while discarding redundant ones. This significantly reduces computational overhead and transmission duration. To further enhance covert wireless transmission, we then develop a group-based proximal policy optimization (GPPO) method that samples multiple candidate actions for each state, selecting the optimal one within each group and incorporating a Kullback-Leibler (KL) divergence penalty to improve policy stability and exploration. Simulation results show that PCAE achieves comparable LLM response fidelity to baseline methods while reducing preprocessing latency by over five orders of magnitude, enabling real-time edge deployment. We further validate PCAE effectiveness across diverse LLM backbones, including DeepSeek-32B, Qwen-32B, and their smaller variants. Moreover, GPPO reduces covert transmission latency by up to 38.6\\% compared to existing reinforcement learning strategies, with further analysis showing that increased transmit power provides additional latency benefits."
  },
  {
    "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models",
    "url": "http://arxiv.org/abs/2504.21299v1",
    "arxiv_id": "2504.21299v1",
    "authors": [
      "Zhiting Fan",
      "Ruizhe Chen",
      "Zuozhu Liu"
    ],
    "published": "2025-04-30T04:13:03+00:00",
    "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline."
  },
  {
    "title": "Base Models Beat Aligned Models at Randomness and Creativity",
    "url": "http://arxiv.org/abs/2505.00047v1",
    "arxiv_id": "2505.00047v1",
    "authors": [
      "Peter West",
      "Christopher Potts"
    ],
    "published": "2025-04-30T03:41:55+00:00",
    "summary": "Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate \"7\" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities."
  },
  {
    "title": "Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training",
    "url": "http://arxiv.org/abs/2504.21278v1",
    "arxiv_id": "2504.21278v1",
    "authors": [
      "Xuyan Ma",
      "Yawen Wang",
      "Junjie Wang",
      "Xiaofei Xie",
      "Boyu Wu",
      "Shoubin Li",
      "Fanjiang Xu",
      "Qing Wang"
    ],
    "published": "2025-04-30T03:14:50+00:00",
    "summary": "In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC\\_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC\\_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost."
  },
  {
    "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2504.21277v1",
    "arxiv_id": "2504.21277v1",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Cen Chen",
      "Jie Wang",
      "Zheming Yang",
      "Jian Xu",
      "Minghui Qiu"
    ],
    "published": "2025-04-30T03:14:28+00:00",
    "summary": "The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains a major challenge. This survey systematically reviews recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms--value-free and value-based methods--and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Furthermore, we provide an extensive overview of benchmark datasets, evaluation protocols, and existing limitations, and propose future research directions to address current bottlenecks such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to offer a comprehensive and structured guide to researchers interested in advancing RL-based reasoning in the multimodal era."
  },
  {
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math",
    "url": "http://arxiv.org/abs/2504.21233v1",
    "arxiv_id": "2504.21233v1",
    "authors": [
      "Haoran Xu",
      "Baolin Peng",
      "Hany Awadalla",
      "Dongdong Chen",
      "Yen-Chun Chen",
      "Mei Gao",
      "Young Jin Kim",
      "Yunsheng Li",
      "Liliang Ren",
      "Yelong Shen",
      "Shuohang Wang",
      "Weijian Xu",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "published": "2025-04-30T00:04:35+00:00",
    "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models."
  },
  {
    "title": "Kimina Lean Server: Technical Report",
    "url": "http://arxiv.org/abs/2504.21230v1",
    "arxiv_id": "2504.21230v1",
    "authors": [
      "Marco Dos Santos",
      "Haiming Wang",
      "Hugues de Saxc\u00e9",
      "Ran Wang",
      "Mantas Baksys",
      "Mert Unsal",
      "Junqi Liu",
      "Zhengying Liu",
      "Jia Li"
    ],
    "published": "2025-04-29T23:43:59+00:00",
    "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast and scalable interaction with Lean 4 via a unified REST API, designed as a simple verifier for reinforcement learning pipelines. Built on top of the Lean FRO's LeanREPL, it combines server-side parallelization by managing multiple Lean REPL processes in parallel, with an LRU caching strategy that reuses Lean imports across multiple requests. These features help reduce initialization overhead and allow large-scale batch processing of Lean code. The client-side interface allows users to submit batches of proofs and receive Lean feedback, including extracted tactics and tactic states via infotree processing. These features enable a high-performance, scalable workflow for both interaction and extraction of proofs, tactics, and tactic states. We open source our implementation on GitHub."
  },
  {
    "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
    "url": "http://arxiv.org/abs/2504.21228v1",
    "arxiv_id": "2504.21228v1",
    "authors": [
      "Rui Wang",
      "Junda Wu",
      "Yu Xia",
      "Tong Yu",
      "Ruiyi Zhang",
      "Ryan Rossi",
      "Lina Yao",
      "Julian McAuley"
    ],
    "published": "2025-04-29T23:42:21+00:00",
    "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems."
  },
  {
    "title": "Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions",
    "url": "http://arxiv.org/abs/2504.21164v1",
    "arxiv_id": "2504.21164v1",
    "authors": [
      "Bhavini Jeloka",
      "Yue Guan",
      "Panagiotis Tsiotras"
    ],
    "published": "2025-04-29T20:31:59+00:00",
    "summary": "State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as MADDPG and MAAC fail to scale in situations where the number of agents becomes large. Mean-field theory has shown encouraging results in modeling macroscopic agent behavior for teams with a large number of agents through a continuum approximation of the agent population and its interaction with the environment. In this work, we extend proximal policy optimization (PPO) to the mean-field domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization (MF-MAPPO), a novel algorithm that utilizes the effectiveness of the finite-population mean-field approximation in the context of zero-sum competitive multi-agent games between two teams. The proposed algorithm can be easily scaled to hundreds and thousands of agents in each team as shown through numerical experiments. In particular, the algorithm is applied to realistic applications such as large-scale offense-defense battlefield scenarios."
  },
  {
    "title": "How to Coordinate UAVs and UGVs for Efficient Mission Planning? Optimizing Energy-Constrained Cooperative Routing with a DRL Framework",
    "url": "http://arxiv.org/abs/2504.21111v1",
    "arxiv_id": "2504.21111v1",
    "authors": [
      "Md Safwan Mondal",
      "Subramanian Ramasamy",
      "Luca Russo",
      "James D. Humann",
      "James M. Dotterweich",
      "Pranav Bhounsule"
    ],
    "published": "2025-04-29T18:43:59+00:00",
    "summary": "Efficient mission planning for cooperative systems involving Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy constraints, scalability, and coordination challenges between agents. UAVs excel in rapidly covering large areas but are constrained by limited battery life, while UGVs, with their extended operational range and capability to serve as mobile recharging stations, are hindered by slower speeds. This heterogeneity makes coordination between UAVs and UGVs critical for achieving optimal mission outcomes. In this work, we propose a scalable deep reinforcement learning (DRL) framework to address the energy-constrained cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a set of task points in minimal time with UAVs relying on UGVs for recharging during the mission. The framework incorporates sortie-wise agent switching to efficiently manage multiple agents, by allocating task points and coordinating actions. Using an encoder-decoder transformer architecture, it optimizes routes and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive computational experiments demonstrate the framework's superior performance over heuristic methods and a DRL baseline, delivering significant improvements in solution quality and runtime efficiency across diverse scenarios. Generalization studies validate its robustness, while dynamic scenario highlights its adaptability to real-time changes with a case study. This work advances UAV-UGV cooperative routing by providing a scalable, efficient, and robust solution for multi-agent mission planning."
  },
  {
    "title": "Toward Efficient Exploration by Large Language Model Agents",
    "url": "http://arxiv.org/abs/2504.20997v1",
    "arxiv_id": "2504.20997v1",
    "authors": [
      "Dilip Arumugam",
      "Thomas L. Griffiths"
    ],
    "published": "2025-04-29T17:59:48+00:00",
    "summary": "A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration."
  },
  {
    "title": "XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search",
    "url": "http://arxiv.org/abs/2504.20969v1",
    "arxiv_id": "2504.20969v1",
    "authors": [
      "Yiting Zhang",
      "Shichen Li",
      "Elena Shrestha"
    ],
    "published": "2025-04-29T17:37:45+00:00",
    "summary": "Mechanical search (MS) in cluttered environments remains a significant challenge for autonomous manipulators, requiring long-horizon planning and robust state estimation under occlusions and partial observability. In this work, we introduce XPG-RL, a reinforcement learning framework that enables agents to efficiently perform MS tasks through explainable, priority-guided decision-making based on raw sensory inputs. XPG-RL integrates a task-driven action prioritization mechanism with a learned context-aware switching strategy that dynamically selects from a discrete set of action primitives such as target grasping, occlusion removal, and viewpoint adjustment. Within this strategy, a policy is optimized to output adaptive threshold values that govern the discrete selection among action primitives. The perception module fuses RGB-D inputs with semantic and geometric features to produce a structured scene representation for downstream decision-making. Extensive experiments in both simulation and real-world settings demonstrate that XPG-RL consistently outperforms baseline methods in task success rates and motion efficiency, achieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These results underscore the benefits of integrating domain knowledge with learnable decision-making policies for robust and efficient robotic manipulation."
  },
  {
    "title": "Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity",
    "url": "http://arxiv.org/abs/2504.20932v1",
    "arxiv_id": "2504.20932v1",
    "authors": [
      "Taisuke Kobayashi"
    ],
    "published": "2025-04-29T16:50:05+00:00",
    "summary": "Continual learning is the one of the most essential abilities for autonomous agents, which can incrementally learn daily-life skills. For this ultimate goal, a simple but powerful method, dark experience replay (DER), has been proposed recently. DER mitigates catastrophic forgetting, in which the skills acquired in the past are unintentionally forgotten, by stochastically storing the streaming data in a reservoir sampling (RS) buffer and by relearning them or retaining the past outputs for them. However, since DER considers multiple objectives, it will not function properly without appropriate weighting of them. In addition, the ability to retain past outputs inhibits learning if the past outputs are incorrect due to distribution shift or other effects. This is due to a tradeoff between memory consolidation and plasticity. The tradeoff is hidden even in the RS buffer, which gradually stops storing new data for new skills in it as data is continuously passed to it. To alleviate the tradeoff and achieve better balance, this paper proposes improvement strategies to each of DER and RS. Specifically, DER is improved with automatic adaptation of weights, block of replaying erroneous data, and correction of past outputs. RS is also improved with generalization of acceptance probability, stratification of plural buffers, and intentional omission of unnecessary data. These improvements are verified through multiple benchmarks including regression, classification, and reinforcement learning problems. As a result, the proposed methods achieve steady improvements in learning performance by balancing the memory consolidation and plasticity."
  },
  {
    "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification",
    "url": "http://arxiv.org/abs/2504.20930v1",
    "arxiv_id": "2504.20930v1",
    "authors": [
      "Ziqing Fan",
      "Cheng Liang",
      "Chaoyi Wu",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ],
    "published": "2025-04-29T16:48:23+00:00",
    "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs."
  },
  {
    "title": "Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR",
    "url": "http://arxiv.org/abs/2504.20927v1",
    "arxiv_id": "2504.20927v1",
    "authors": [
      "Shahbaz P Qadri Syed",
      "He Bai"
    ],
    "published": "2025-04-29T16:42:13+00:00",
    "summary": "Developing scalable and efficient reinforcement learning algorithms for cooperative multi-agent control has received significant attention over the past years. Existing literature has proposed inexact decompositions of local Q-functions based on empirical information structures between the agents. In this paper, we exploit inter-agent coupling information and propose a systematic approach to exactly decompose the local Q-function of each agent. We develop an approximate least square policy iteration algorithm based on the proposed decomposition and identify two architectures to learn the local Q-function for each agent. We establish that the worst-case sample complexity of the decomposition is equal to the centralized case and derive necessary and sufficient graphical conditions on the inter-agent couplings to achieve better sample efficiency. We demonstrate the improved sample efficiency and computational efficiency on numerical examples."
  },
  {
    "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
    "url": "http://arxiv.org/abs/2504.20924v1",
    "arxiv_id": "2504.20924v1",
    "authors": [
      "Beomjun Kim",
      "Kangyeon Kim",
      "Sunwoo Kim",
      "Heejin Ahn"
    ],
    "published": "2025-04-29T16:38:35+00:00",
    "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts.   We propose a novel AI safety framework that ensures AI systems comply with \\textbf{any user-defined constraint}, with \\textbf{any desired probability}, and across \\textbf{various domains}.   In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose \\textit{internal test data}, a supplementary set of safety-labeled data, and a \\textit{conservative testing} methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training.   We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms {for \\textbf{up to several order of magnitudes}} existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data."
  },
  {
    "title": "Automated Parking Trajectory Generation Using Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.21071v1",
    "arxiv_id": "2504.21071v1",
    "authors": [
      "Zheyu Zhang",
      "Yutong Luo",
      "Yongzhou Chen",
      "Haopeng Zhao",
      "Zhichao Ma",
      "Hao Liu"
    ],
    "published": "2025-04-29T15:25:34+00:00",
    "summary": "Autonomous parking is a key technology in modern autonomous driving systems, requiring high precision, strong adaptability, and efficiency in complex environments. This paper proposes a Deep Reinforcement Learning (DRL) framework based on the Soft Actor-Critic (SAC) algorithm to optimize autonomous parking tasks. SAC, an off-policy method with entropy regularization, is particularly well-suited for continuous action spaces, enabling fine-grained vehicle control. We model the parking task as a Markov Decision Process (MDP) and train an agent to maximize cumulative rewards while balancing exploration and exploitation through entropy maximization. The proposed system integrates multiple sensor inputs into a high-dimensional state space and leverages SAC's dual critic networks and policy network to achieve stable learning. Simulation results show that the SAC-based approach delivers high parking success rates, reduced maneuver times, and robust handling of dynamic obstacles, outperforming traditional rule-based methods and other DRL algorithms. This study demonstrates SAC's potential in autonomous parking and lays the foundation for real-world applications."
  },
  {
    "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints",
    "url": "http://arxiv.org/abs/2504.20834v1",
    "arxiv_id": "2504.20834v1",
    "authors": [
      "Alan Lee",
      "Harry Tong"
    ],
    "published": "2025-04-29T14:58:43+00:00",
    "summary": "We explore reinforcement learning (RL) techniques to enhance reasoning within targeted problem spaces in large language models (LLMs) under memory and compute constraints. Our focus is on critic-free methods compatible with LoRA fine-tuning on a single 40GB GPU, a common limitation in academic settings. We introduce S-GRPO, a memory-efficient variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching strategy for fine-grained credit assignment. Despite limited resources, when used to fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in multi-digit multiplication tasks, underscoring the potential of RL fine-tuning under hardware constraints. Additionally, we find that our full-token GRPO baseline under LoRA fine-tuning did not improve model performance (compared to base model) on either task, suggesting that our memory-efficient methods may act as a form of regularization that stabilizes training when only a small subset of parameters are updated."
  },
  {
    "title": "A Teacher-Student MPC-PPO Coupled Reinforcement Learning Framework for Winter Temperature Control of Solar Greenhouses in Northern China",
    "url": "http://arxiv.org/abs/2504.20815v1",
    "arxiv_id": "2504.20815v1",
    "authors": [
      "Jingxin Yu",
      "Lushun Ma",
      "Jinpeng Zhao",
      "Jianchao Ci",
      "Muhammad Abdul Munnaf",
      "Eldert van Henten",
      "Peter Groot Koerkamp",
      "Shuyi Peng",
      "Xiaoming Wei",
      "Congcong Sun"
    ],
    "published": "2025-04-29T14:32:40+00:00",
    "summary": "Solar greenhouses are crucial infrastructure of modern agricultural production in northern China. However, highly fluctuating temperature in winter season results in poor greenhouse temperature control, which affects crop growth and increases energy consumption. To tackle these challenges, an advanced control system that can efficiently optimize multiple objectives under dramatic climate conditions is essential. Therefore, this study propose a model predictive control-coupled proximal policy optimization (MPC-PPO) control framework. A teacher-student control framework is constructed in which the MPC generating high-quality control experiences to guide the PPO agent's learning process. An adaptive dynamic weighting mechanism is employed to balance the influence of MPC experiences during PPO training. Evaluation conducted in solar greenhouses across three provinces in northern China (Beijing, Hebei, and Shandong) demonstrates that: (1) the MPC-PPO method achieved the highest temperature control performance (96.31 on a 100-point scale), with a 5.46-point improvement compared to the non-experience integration baseline, when reduced standard deviation by nearly half and enhanced exploration efficiency; (2) the MPC-PPO method achieved a ventilation control reward of 99.19, optimizing ventilation window operations with intelligent time-differentiated strategies that reduced energy loss during non-optimal hours; (3) feature analysis reveals that historical window opening, air temperature, and historical temperature are the most influential features for effective control, i.e., SHAP values of 7.449, 4.905, and 4.747 respectively; and (4) cross-regional tests indicated that MPC-PPO performs best in all test regions, confirming generalization of the method."
  },
  {
    "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings",
    "url": "http://arxiv.org/abs/2504.20808v1",
    "arxiv_id": "2504.20808v1",
    "authors": [
      "Florian Vahl",
      "J\u00f6rn Griepenburg",
      "Jan Gutsche",
      "Jasper G\u00fcldenstein",
      "Jianwei Zhang"
    ],
    "published": "2025-04-29T14:21:08+00:00",
    "summary": "This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion"
  },
  {
    "title": "Q-Fusion: Diffusing Quantum Circuits",
    "url": "http://arxiv.org/abs/2504.20794v1",
    "arxiv_id": "2504.20794v1",
    "authors": [
      "Collin Beaudoin",
      "Swaroop Ghosh"
    ],
    "published": "2025-04-29T14:10:10+00:00",
    "summary": "Quantum computing holds great potential for solving socially relevant and computationally complex problems. Furthermore, quantum machine learning (QML) promises to rapidly improve our current machine learning capabilities. However, current noisy intermediate-scale quantum (NISQ) devices are constrained by limitations in the number of qubits and gate counts, which hinder their full capabilities. Furthermore, the design of quantum algorithms remains a laborious task, requiring significant domain expertise and time. Quantum Architecture Search (QAS) aims to streamline this process by automatically generating novel quantum circuits, reducing the need for manual intervention. In this paper, we propose a diffusion-based algorithm leveraging the LayerDAG framework to generate new quantum circuits. This method contrasts with other approaches that utilize large language models (LLMs), reinforcement learning (RL), variational autoencoders (VAE), and similar techniques. Our results demonstrate that the proposed model consistently generates 100% valid quantum circuit outputs."
  },
  {
    "title": "Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces",
    "url": "http://arxiv.org/abs/2504.20782v1",
    "arxiv_id": "2504.20782v1",
    "authors": [
      "Daniel Gaspar-Figueiredo",
      "Marta Fern\u00e1ndez-Diego",
      "Silvia Abrah\u00e3o",
      "Emilio Insfran"
    ],
    "published": "2025-04-29T14:00:22+00:00",
    "summary": "Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.   In this paper, we enhance a RL-based Adaptive User Interface adaption framework by incorporating personalized human feedback directly into the leaning process. Unlike prior approaches that rely on a single pre-trained RL model, our approach trains a unique RL agent for each user, allowing individuals to actively shape their personal RL agent's policy, potentially leading to more personalized and responsive UI adaptations. To evaluate this approach, we conducted an empirical study to assess the impact of integrating human feedback into the RL-based Adaptive User Interface adaption framework and its effect on User Experience (UX). The study involved 33 participants interacting with AUIs incorporating human feedback and non-adaptive user interfaces in two domains: an e-learning platform and a trip-planning application. The results suggest that incorporating human feedback into RL-driven adaptations significantly enhances UX, offering promising directions for advancing adaptive capabilities and user-centered design in AUIs."
  },
  {
    "title": "Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency",
    "url": "http://arxiv.org/abs/2504.20735v1",
    "arxiv_id": "2504.20735v1",
    "authors": [
      "Tariq Qayyum",
      "Asadullah Tariq",
      "Muhammad Ali",
      "Mohamed Adel Serhani",
      "Zouheir Trabelsi",
      "Maite L\u00f3pez-S\u00e1nchez"
    ],
    "published": "2025-04-29T13:20:02+00:00",
    "summary": "Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation systems, enabling vehicles to offload computational tasks to nearby roadside units (RSUs) and mobile edge computing (MEC) servers for real-time processing. However, the highly dynamic nature of VANETs introduces challenges, such as unpredictable network conditions, high latency, energy inefficiency, and task failure. This research addresses these issues by proposing a hybrid AI framework that integrates supervised learning, reinforcement learning, and Particle Swarm Optimization (PSO) for intelligent task offloading and resource allocation. The framework leverages supervised models for predicting optimal offloading strategies, reinforcement learning for adaptive decision-making, and PSO for optimizing latency and energy consumption. Extensive simulations demonstrate that the proposed framework achieves significant reductions in latency and energy usage while improving task success rates and network throughput. By offering an efficient, and scalable solution, this framework sets the foundation for enhancing real-time applications in dynamic vehicular environments."
  },
  {
    "title": "Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems",
    "url": "http://arxiv.org/abs/2504.20660v1",
    "arxiv_id": "2504.20660v1",
    "authors": [
      "Sahil Tomar",
      "Shamshe Alam",
      "Sandeep Kumar",
      "Amit Mathur"
    ],
    "published": "2025-04-29T11:36:08+00:00",
    "summary": "In this paper, a novel quantum classical hybrid framework is proposed that synergizes quantum with Classical Reinforcement Learning. By leveraging the inherent parallelism of quantum computing, the proposed approach generates robust Q tables and specialized turn cost estimations, which are then integrated with a classical Reinforcement Learning pipeline. The Classical Quantum fusion results in rapid convergence of training, reducing the training time significantly and improved adaptability in scenarios featuring static, dynamic, and moving obstacles. Simulator based evaluations demonstrate significant enhancements in path efficiency, trajectory smoothness, and mission success rates, underscoring the potential of framework for real time, autonomous navigation in complex and unpredictable environments. Furthermore, the proposed framework was tested beyond simulations on practical scenarios, including real world map data such as the IIT Delhi campus, reinforcing its potential for real time, autonomous navigation in complex and unpredictable environments."
  },
  {
    "title": "Independent Learning in Performative Markov Potential Games",
    "url": "http://arxiv.org/abs/2504.20593v1",
    "arxiv_id": "2504.20593v1",
    "authors": [
      "Rilind Sahitaj",
      "Paulius Sasnauskas",
      "Yi\u011fit Yal\u0131n",
      "Debmalya Mandal",
      "Goran Radanovi\u0107"
    ],
    "published": "2025-04-29T09:46:16+00:00",
    "summary": "Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. In this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings."
  },
  {
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
    "url": "http://arxiv.org/abs/2504.20571v1",
    "arxiv_id": "2504.20571v1",
    "authors": [
      "Yiping Wang",
      "Qing Yang",
      "Zhiyuan Zeng",
      "Liliang Ren",
      "Lucas Liu",
      "Baolin Peng",
      "Hao Cheng",
      "Xuehai He",
      "Kuan Wang",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Shuohang Wang",
      "Simon Shaolei Du",
      "Yelong Shen"
    ],
    "published": "2025-04-29T09:24:30+00:00",
    "summary": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR"
  },
  {
    "title": "Safe Bottom-Up Flexibility Provision from Distributed Energy Resources",
    "url": "http://arxiv.org/abs/2504.20529v1",
    "arxiv_id": "2504.20529v1",
    "authors": [
      "Costas Mylonas",
      "Emmanouel Varvarigos",
      "Georgios Tsaousoglou"
    ],
    "published": "2025-04-29T08:16:15+00:00",
    "summary": "Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop."
  },
  {
    "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations",
    "url": "http://arxiv.org/abs/2504.20520v1",
    "arxiv_id": "2504.20520v1",
    "authors": [
      "Haowen Sun",
      "Han Wang",
      "Chengzhong Ma",
      "Shaolong Zhang",
      "Jiawei Ye",
      "Xingyu Chen",
      "Xuguang Lan"
    ],
    "published": "2025-04-29T08:01:27+00:00",
    "summary": "Learning from few demonstrations to develop policies robust to variations in robot initial positions and object poses is a problem of significant practical interest in robotics. Compared to imitation learning, which often struggles to generalize from limited samples, reinforcement learning (RL) can autonomously explore to obtain robust behaviors. Training RL agents through direct interaction with the real world is often impractical and unsafe, while building simulation environments requires extensive manual effort, such as designing scenes and crafting task-specific reward functions. To address these challenges, we propose an integrated real-to-sim-to-real pipeline that constructs simulation environments based on expert demonstrations by identifying scene objects from images and retrieving their corresponding 3D models from existing libraries. We introduce a projection-based reward model for RL policy training that is supervised by a vision-language model (VLM) using human-guided object projection relationships as prompts, with the policy further fine-tuned using expert demonstrations. In general, our work focuses on the construction of simulation environments and RL-based policy training, ultimately enabling the deployment of reliable robotic control policies in real-world scenarios."
  },
  {
    "title": "A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.20464v1",
    "arxiv_id": "2504.20464v1",
    "authors": [
      "Jiahao Li",
      "Kaer Huang"
    ],
    "published": "2025-04-29T06:55:15+00:00",
    "summary": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured summary of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents."
  },
  {
    "title": "Reinforcement-learning-assisted control of four-roll mills: geometric symmetry and inertial effect",
    "url": "http://arxiv.org/abs/2504.20336v1",
    "arxiv_id": "2504.20336v1",
    "authors": [
      "X. Dai",
      "D. Xu",
      "M. Zhang",
      "Y. Yang"
    ],
    "published": "2025-04-29T01:01:14+00:00",
    "summary": "Embedding the intrinsic symmetry of a flow system in training its machine learning algorithms has become a significant trend in the recent surge of their application in fluid mechanics. This paper leverages the geometric symmetry of a four-roll mill (FRM) to enhance its training efficiency. Stabilizing and precisely controlling droplet trajectories in a FRM is challenging due to the unstable nature of the extensional flow with a saddle point. Extending the work of Vona & Lauga, this study applies Deep Reinforcement Learning (DRL) to effectively guide a displaced droplet to the center of the FRM. Through direct numerical simulations, we explore the applicability of DRL in controlling FRM flow with moderate inertial effects, i.e., Reynolds number $\\sim\\mathcal{O}(1)$, a nonlinear regime previously unexplored. The FRM's geometric symmetry allows control policies trained in one of the eight sub-quadrants to be extended to the entire domain, reducing training costs. Our results indicate that the DRL-based control method can successfully guide a displaced droplet to the target center with robust performance across various starting positions, even from substantially far distances. The work also highlights potential directions for future research, particularly focusing on efficiently addressing the delay effects in flow response caused by inertia. This study presents new advances in controlling droplet trajectories in more nonlinear and complex situations, with potential applications to other nonlinear flows. The geometric symmetry used in this cutting-edge reinforcement learning approach can also be applied to other control methods."
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey",
    "url": "http://arxiv.org/abs/2504.21048v1",
    "arxiv_id": "2504.21048v1",
    "authors": [
      "Mohamad A. Hady",
      "Siyi Hu",
      "Mahardhika Pratama",
      "Jimmy Cao",
      "Ryszard Kowalczyk"
    ],
    "published": "2025-04-29T00:18:31+00:00",
    "summary": "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions."
  },
  {
    "title": "AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.20187v1",
    "arxiv_id": "2504.20187v1",
    "authors": [
      "Weihao Sun",
      "Heeseung Bang",
      "Andreas A. Malikopoulos"
    ],
    "published": "2025-04-28T18:38:39+00:00",
    "summary": "In this paper, we present an adherence-aware reinforcement learning (RL) approach aimed at seeking optimal lane-changing recommendations within a semi-autonomous driving environment to enhance a single vehicle's travel efficiency. The problem is framed within a Markov decision process setting and is addressed through an adherence-aware deep Q network, which takes into account the partial compliance of human drivers with the recommended actions. This approach is evaluated within CARLA's driving environment under realistic scenarios."
  },
  {
    "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning",
    "url": "http://arxiv.org/abs/2504.20024v1",
    "arxiv_id": "2504.20024v1",
    "authors": [
      "Wufei Ma",
      "Yu-Cheng Chou",
      "Qihao Liu",
      "Xingrui Wang",
      "Celso de Melo",
      "Jieneng Chen",
      "Jianwen Xie",
      "Alan Yuille"
    ],
    "published": "2025-04-28T17:48:43+00:00",
    "summary": "Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning."
  },
  {
    "title": "Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions",
    "url": "http://arxiv.org/abs/2504.20004v1",
    "arxiv_id": "2504.20004v1",
    "authors": [
      "Jing Wang",
      "Yan Jin",
      "Hamid Taghavifar",
      "Fei Ding",
      "Chongfeng Wei"
    ],
    "published": "2025-04-28T17:24:04+00:00",
    "summary": "Since the emergence of autonomous driving technology, it has advanced rapidly over the past decade. It is becoming increasingly likely that autonomous vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the roads. Currently, safety and reliable decision-making remain significant challenges, particularly when AVs are navigating lane changes and interacting with surrounding HVs. Therefore, precise estimation of the intentions of surrounding HVs can assist AVs in making more reliable and safe lane change decision-making. This involves not only understanding their current behaviors but also predicting their future motions without any direct communication. However, distinguishing between the passing and yielding intentions of surrounding HVs still remains ambiguous. To address the challenge, we propose a social intention estimation algorithm rooted in Directed Acyclic Graph (DAG), coupled with a decision-making framework employing Deep Reinforcement Learning (DRL) algorithms. To evaluate the method's performance, the proposed framework can be tested and applied in a lane-changing scenario within a simulated environment. Furthermore, the experiment results demonstrate how our approach enhances the ability of AVs to navigate lane changes safely and efficiently on roads."
  },
  {
    "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets",
    "url": "http://arxiv.org/abs/2504.19981v1",
    "arxiv_id": "2504.19981v1",
    "authors": [
      "Adam Younsi",
      "Abdalgader Abubaker",
      "Mohamed El Amine Seddik",
      "Hakim Hacid",
      "Salem Lahlou"
    ],
    "published": "2025-04-28T16:56:41+00:00",
    "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs."
  },
  {
    "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics",
    "url": "http://arxiv.org/abs/2504.19938v1",
    "arxiv_id": "2504.19938v1",
    "authors": [
      "Yunfei Wan",
      "Jianheng Liu",
      "Jiarong Lin",
      "Fu Zhang"
    ],
    "published": "2025-04-28T16:09:25+00:00",
    "summary": "In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner."
  },
  {
    "title": "Automated decision-making for dynamic task assignment at scale",
    "url": "http://arxiv.org/abs/2504.19933v1",
    "arxiv_id": "2504.19933v1",
    "authors": [
      "Riccardo Lo Bianco",
      "Willem van Jaarsveld",
      "Jeroen Middelhuis",
      "Luca Begnardi",
      "Remco Dijkman"
    ],
    "published": "2025-04-28T16:08:35+00:00",
    "summary": "The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances."
  },
  {
    "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets",
    "url": "http://arxiv.org/abs/2504.19898v1",
    "arxiv_id": "2504.19898v1",
    "authors": [
      "Mingqian He",
      "Fei Zhao",
      "Chonggang Lu",
      "Ziyan Liu",
      "Yue Wang",
      "Haofu Qian"
    ],
    "published": "2025-04-28T15:30:58+00:00",
    "summary": "As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT \"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications."
  },
  {
    "title": "Optimizing the Charging of Open Quantum Batteries using Long Short-Term Memory-Driven Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.19840v1",
    "arxiv_id": "2504.19840v1",
    "authors": [
      "Shadab Zakavati",
      "Shahriar Salimi",
      "Behrouz Arash"
    ],
    "published": "2025-04-28T14:40:11+00:00",
    "summary": "Controlling the charging process of a quantum battery involves strategies to efficiently transfer, store, and retain energy, while mitigating decoherence, energy dissipation, and inefficiencies caused by surrounding interactions. We develop a model to study the charging process of a quantum battery in an open quantum setting, where the battery interacts with a charger and a structured reservoir. To overcome the limitations of static charging protocols, a reinforcement learning (RL) charging strategy is proposed, which utilizes the deep deterministic policy gradient algorithm alongside long short-term memory (LSTM) networks. The LSTM networks enable the RL model to capture temporal correlations driven by non-Markovian dynamics, facilitating a continuous, adaptive charging strategy. The RL protocols consistently outperform conventional fixed heuristic strategies by real-time controlling the driving field amplitude and coupling parameters. By penalizing battery-to-charger backflow in the reward function, the RL-optimized charging strategy promotes efficient unidirectional energy transfer from charger to battery, achieving higher and more stable extractable work. The proposed RL controller would provide a framework for designing efficient charging schemes in broader configurations and multi-cell quantum batteries."
  },
  {
    "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects",
    "url": "http://arxiv.org/abs/2504.19838v1",
    "arxiv_id": "2504.19838v1",
    "authors": [
      "Guangyi Liu",
      "Pengxiang Zhao",
      "Liang Liu",
      "Yaxuan Guo",
      "Han Xiao",
      "Weifeng Lin",
      "Yuxiang Chai",
      "Yue Han",
      "Shuai Ren",
      "Hao Wang",
      "Xiaoyu Liang",
      "Wenhao Wang",
      "Tianze Wu",
      "Linghao Li",
      "Hao Wang",
      "Guanjing Xiong",
      "Yong Liu",
      "Hongsheng Li"
    ],
    "published": "2025-04-28T14:39:25+00:00",
    "summary": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents."
  },
  {
    "title": "Reinforcement Learning-Based Heterogeneous Multi-Task Optimization in Semantic Broadcast Communications",
    "url": "http://arxiv.org/abs/2504.19806v1",
    "arxiv_id": "2504.19806v1",
    "authors": [
      "Zhilin Lu",
      "Rongpeng Li",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "published": "2025-04-28T13:59:01+00:00",
    "summary": "Semantic broadcast communications (Semantic BC) for image transmission have achieved significant performance gains for single-task scenarios. Nevertheless, extending these methods to multi-task scenarios remains challenging, as different tasks typically require distinct objective functions, leading to potential conflicts within the shared encoder. In this paper, we propose a tri-level reinforcement learning (RL)-based multi-task Semantic BC framework, termed SemanticBC-TriRL, which effectively resolves such conflicts and enables the simultaneous support of multiple downstream tasks at the receiver side, including image classification and content reconstruction tasks. Specifically, the proposed framework employs a bottom-up tri-level alternating learning strategy, formulated as a constrained multi-objective optimization problem. At the first level, task-specific decoders are locally optimized using supervised learning. At the second level, the shared encoder is updated via proximal policy optimization (PPO), guided by task-oriented rewards. At the third level, a multi-gradient aggregation-based task weighting module adaptively adjusts task priorities and steers the encoder optimization. Through this hierarchical learning process, the encoder and decoders are alternately trained, and the three levels are cohesively integrated via constrained learning objective. Besides, the convergence of SemanticBC-TriRL is also theoretically established. Extensive simulation results demonstrate the superior performance of the proposed framework across diverse channel conditions, particularly in low SNR regimes, and confirm its scalability with increasing numbers of receivers."
  },
  {
    "title": "Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control",
    "url": "http://arxiv.org/abs/2504.19715v1",
    "arxiv_id": "2504.19715v1",
    "authors": [
      "Heisei Yonezawa",
      "Ansei Yonezawa",
      "Itsuro Kajiwara"
    ],
    "published": "2025-04-28T12:09:07+00:00",
    "summary": "Complex mechanical systems such as vehicle powertrains are inherently subject to multiple nonlinearities and uncertainties arising from parametric variations. Modeling and calibration errors are therefore unavoidable, making the transfer of control systems from simulation to real-world systems a critical challenge. Traditional robust controls have limitations in handling certain types of nonlinearities and uncertainties, requiring a more practical approach capable of comprehensively compensating for these various constraints. This study proposes a new robust control approach using the framework of deep reinforcement learning (DRL). The key strategy lies in the synergy among domain randomization-based DRL, long short-term memory (LSTM)-based actor and critic networks, and model-based control (MBC). The problem setup is modeled via the latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an environment simulator is randomized during training to improve the robustness of the control system to real testing environments. The randomization increases training difficulties as well as conservativeness of the resultant control system; therefore, progress is assisted by concurrent use of a model-based controller based on a nominal system model. Compared to traditional DRL-based controls, the proposed controller design is smarter in that we can achieve a high level of generalization ability with a more compact neural network architecture and a smaller amount of training data. The proposed approach is verified via practical application to active damping for a complex powertrain system with nonlinearities and parametric variations. Comparative tests demonstrate the high robustness of the proposed approach."
  },
  {
    "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
    "url": "http://arxiv.org/abs/2504.19678v1",
    "arxiv_id": "2504.19678v1",
    "authors": [
      "Mohamed Amine Ferrag",
      "Norbert Tihanyi",
      "Merouane Debbah"
    ],
    "published": "2025-04-28T11:08:22+00:00",
    "summary": "Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols."
  },
  {
    "title": "Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey",
    "url": "http://arxiv.org/abs/2504.19660v1",
    "arxiv_id": "2504.19660v1",
    "authors": [
      "Yunting Xu",
      "Jiacheng Wang",
      "Ruichen Zhang",
      "Changyuan Zhao",
      "Dusit Niyato",
      "Jiawen Kang",
      "Zehui Xiong",
      "Bo Qian",
      "Haibo Zhou",
      "Shiwen Mao",
      "Abbas Jamalipour",
      "Xuemin Shen",
      "Dong In Kim"
    ],
    "published": "2025-04-28T10:20:04+00:00",
    "summary": "Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies."
  },
  {
    "title": "Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM",
    "url": "http://arxiv.org/abs/2504.19654v1",
    "arxiv_id": "2504.19654v1",
    "authors": [
      "Leon Davies",
      "Baihua Li",
      "Mohamad Saada",
      "Simon S\u00f8lvsten",
      "Qinggang Meng"
    ],
    "published": "2025-04-28T10:13:47+00:00",
    "summary": "SLAM (Simultaneous Localisation and Mapping) is a crucial component for robotic systems, providing a map of an environment, the current location and previous trajectory of a robot. While 3D LiDAR SLAM has received notable improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in large complex environments. Dynamic robotic motion coupled with inherent estimation based SLAM processes introduce noise and errors, degrading map quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and unclear. This is due to the fact that evidence based mapping represents maps according to uncertain observations. This is why OGMs are so popular in exploration or navigation tasks. However, this also limits OGMs' effectiveness for specific mapping based tasks such as floor plan creation in complex scenes. To address this, we propose our novel Transformation and Translation Occupancy Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation techniques from 3D SLAM to the world of 2D and mitigate errors to improve map quality using Generative Adversarial Networks (GANs). We introduce a novel data generation method via deep reinforcement learning (DRL) to build datasets large enough for training a GAN for SLAM error correction. We demonstrate our SLAM in real-time on data collected at Loughborough University. We also prove its generalisability on a variety of large complex environments on a collection of large scale well-known 2D occupancy maps. Our novel approach enables the creation of high quality OGMs in complex scenes, far surpassing the capabilities of current SLAM algorithms in terms of quality, accuracy and reliability."
  },
  {
    "title": "Fooling the Decoder: An Adversarial Attack on Quantum Error Correction",
    "url": "http://arxiv.org/abs/2504.19651v1",
    "arxiv_id": "2504.19651v1",
    "authors": [
      "Jerome Lenssen",
      "Alexandru Paler"
    ],
    "published": "2025-04-28T10:10:05+00:00",
    "summary": "Neural network decoders are becoming essential for achieving fault-tolerant quantum computations. However, their internal mechanisms are poorly understood, hindering our ability to ensure their reliability and security against adversarial attacks. Leading machine learning decoders utilize recurrent and transformer models (e.g., AlphaQubit), with reinforcement learning (RL) playing a key role in training advanced transformer models (e.g., DeepSeek R1). In this work, we target a basic RL surface code decoder (DeepQ) to create the first adversarial attack on quantum error correction. By applying state-of-the-art white-box methods, we uncover vulnerabilities in this decoder, demonstrating an attack that reduces the logical qubit lifetime in memory experiments by up to five orders of magnitude. We validate that this attack exploits a genuine weakness, as the decoder exhibits robustness against noise fluctuations, is largely unaffected by substituting the referee decoder, responsible for episode termination, with an MWPM decoder, and demonstrates fault tolerance at checkable code distances. This attack highlights the susceptibility of machine learning-based QEC and underscores the importance of further research into robust QEC methods."
  },
  {
    "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning",
    "url": "http://arxiv.org/abs/2504.19627v1",
    "arxiv_id": "2504.19627v1",
    "authors": [
      "Run Luo",
      "Renke Shan",
      "Longze Chen",
      "Ziqiang Liu",
      "Lu Wang",
      "Min Yang",
      "Xiaobo Xia"
    ],
    "published": "2025-04-28T09:39:07+00:00",
    "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM."
  },
  {
    "title": "Rulebook: bringing co-routines to reinforcement learning environments",
    "url": "http://arxiv.org/abs/2504.19625v1",
    "arxiv_id": "2504.19625v1",
    "authors": [
      "Massimo Fioravanti",
      "Samuele Pasini",
      "Giovanni Agosta"
    ],
    "published": "2025-04-28T09:34:34+00:00",
    "summary": "Reinforcement learning (RL) algorithms, due to their reliance on external systems to learn from, require digital environments (e.g., simulators) with very simple interfaces, which in turn constrain significantly the implementation of such environments. In particular, these environments are implemented either as separate processes or as state machines, leading to synchronization and communication overheads in the first case, and to unstructured programming in the second.   We propose a new domain-specific, co-routine-based, compiled language, called Rulebook, designed to automatically generate the state machine required to interact with machine learning (ML) algorithms and similar applications, with no performance overhead. Rulebook allows users to express programs without needing to be aware of the specific interface required by the ML components. By decoupling the execution model of the program from the syntactical encoding of the program, and thus without the need for manual state management, Rulebook allows to create larger and more sophisticated environments at a lower development cost."
  },
  {
    "title": "ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes",
    "url": "http://arxiv.org/abs/2504.19624v1",
    "arxiv_id": "2504.19624v1",
    "authors": [
      "Yizhe Zhang",
      "Jianping Li",
      "Xin Zhao",
      "Fuxun Liang",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "published": "2025-04-28T09:33:40+00:00",
    "summary": "Unexposed environments, such as lava tubes, mines, and tunnels, are among the most complex yet strategically significant domains for scientific exploration and infrastructure development. Accurate and real-time 3D meshing of these environments is essential for applications including automated structural assessment, robotic-assisted inspection, and safety monitoring. Implicit neural Signed Distance Fields (SDFs) have shown promising capabilities in online meshing; however, existing methods often suffer from large projection errors and rely on fixed reconstruction parameters, limiting their adaptability to complex and unstructured underground environments such as tunnels, caves, and lava tubes. To address these challenges, this paper proposes ARMOR, a scene-adaptive and reinforcement learning-based framework for real-time 3D meshing in unexposed environments. The proposed method was validated across more than 3,000 meters of underground environments, including engineered tunnels, natural caves, and lava tubes. Experimental results demonstrate that ARMOR achieves superior performance in real-time mesh reconstruction, reducing geometric error by 3.96\\% compared to state-of-the-art baselines, while maintaining real-time efficiency. The method exhibits improved robustness, accuracy, and adaptability, indicating its potential for advanced 3D monitoring and mapping in challenging unexposed scenarios. The project page can be found at: https://yizhezhang0418.github.io/armor.github.io/"
  },
  {
    "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training",
    "url": "http://arxiv.org/abs/2504.19599v1",
    "arxiv_id": "2504.19599v1",
    "authors": [
      "Kaichen Zhang",
      "Yuzhong Hong",
      "Junwei Bao",
      "Hongfei Jiang",
      "Yang Song",
      "Dingqian Hong",
      "Hui Xiong"
    ],
    "published": "2025-04-28T09:02:24+00:00",
    "summary": "Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. To address this challenge, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training."
  },
  {
    "title": "LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning",
    "url": "http://arxiv.org/abs/2504.19524v1",
    "arxiv_id": "2504.19524v1",
    "authors": [
      "Peijian Zeng",
      "Feiyan Pang",
      "Zhanbo Wang",
      "Aimin Yang"
    ],
    "published": "2025-04-28T06:52:35+00:00",
    "summary": "Industrial Anomaly Detection (IAD) is critical for ensuring product quality by identifying defects. Traditional methods such as feature embedding and reconstruction-based approaches require large datasets and struggle with scalability. Existing vision-language models (VLMs) and Multimodal Large Language Models (MLLMs) address some limitations but rely on mask annotations, leading to high implementation costs and false positives. Additionally, industrial datasets like MVTec-AD and VisA suffer from severe class imbalance, with defect samples constituting only 23.8% and 11.1% of total data respectively. To address these challenges, we propose a reward function that dynamically prioritizes rare defect patterns during training to handle class imbalance. We also introduce a mask-free reasoning framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms, enabling anomaly detection directly from raw images without annotated masks. This approach generates interpretable step-by-step explanations for defect localization. Our method achieves state-of-the-art performance, outperforming prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating mask dependency and reducing costs while providing explainable outputs, this work advances industrial anomaly detection and supports scalable quality control in manufacturing. Code to reproduce the experiment is available at https://github.com/LilaKen/LR-IAD."
  },
  {
    "title": "Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks",
    "url": "http://arxiv.org/abs/2504.19499v1",
    "arxiv_id": "2504.19499v1",
    "authors": [
      "Omid Semiari",
      "Hosein Nikopour",
      "Shilpa Talwar"
    ],
    "published": "2025-04-28T05:41:31+00:00",
    "summary": "Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic."
  },
  {
    "title": "An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination",
    "url": "http://arxiv.org/abs/2504.19480v1",
    "arxiv_id": "2504.19480v1",
    "authors": [
      "Dixiao Wei",
      "Peng Yi",
      "Jinlong Lei",
      "Yiguang Hong",
      "Yuchuan Du"
    ],
    "published": "2025-04-28T04:41:15+00:00",
    "summary": "Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\\% higher performance metrics in all scenarios."
  },
  {
    "title": "JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift",
    "url": "http://arxiv.org/abs/2504.19440v1",
    "arxiv_id": "2504.19440v1",
    "authors": [
      "Julien Piet",
      "Xiao Huang",
      "Dennis Jacob",
      "Annabella Chow",
      "Maha Alrashed",
      "Geng Zhao",
      "Zhanhao Hu",
      "Chawin Sitawarin",
      "Basel Alomair",
      "David Wagner"
    ],
    "published": "2025-04-28T03:01:51+00:00",
    "summary": "Safety and security remain critical concerns in AI deployment. Despite safety training through reinforcement learning with human feedback (RLHF) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release JailbreaksOverTime, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4% to 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach."
  },
  {
    "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets",
    "url": "http://arxiv.org/abs/2504.19394v1",
    "arxiv_id": "2504.19394v1",
    "authors": [
      "Toby Simonds"
    ],
    "published": "2025-04-27T23:59:39+00:00",
    "summary": "Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development."
  },
  {
    "title": "HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks",
    "url": "http://arxiv.org/abs/2504.19382v1",
    "arxiv_id": "2504.19382v1",
    "authors": [
      "Jonathan Gornet",
      "Yiannis Kantaros",
      "Bruno Sinopoli"
    ],
    "published": "2025-04-27T23:13:19+00:00",
    "summary": "We introduce Hyperparameter Controller (HyperController), a computationally efficient algorithm for hyperparameter optimization during training of reinforcement learning neural networks. HyperController optimizes hyperparameters quickly while also maintaining improvement of the reinforcement learning neural network, resulting in faster training and deployment. It achieves this by modeling the hyperparameter optimization problem as an unknown Linear Gaussian Dynamical System, which is a system with a state that linearly changes. It then learns an efficient representation of the hyperparameter objective function using the Kalman filter, which is the optimal one-step predictor for a Linear Gaussian Dynamical System. To demonstrate the performance of HyperController, it is applied as a hyperparameter optimizer during training of reinforcement learning neural networks on a variety of OpenAI Gymnasium environments. In four out of the five Gymnasium environments, HyperController achieves highest median reward during evaluation compared to other algorithms. The results exhibit the potential of HyperController for efficient and stable training of reinforcement learning neural networks."
  },
  {
    "title": "$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation",
    "url": "http://arxiv.org/abs/2504.19375v1",
    "arxiv_id": "2504.19375v1",
    "authors": [
      "Siddharth Chandak"
    ],
    "published": "2025-04-27T22:45:00+00:00",
    "summary": "Two-time-scale stochastic approximation is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. While several prior works have obtained a mean square error bound of $O(1/k)$ for linear two-time-scale iterations, the best known bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In this work, we obtain an improved bound of $O(1/k)$ for non-linear two-time-scale stochastic approximation. Our result applies to algorithms such as gradient descent-ascent and two-time-scale Lagrangian optimization. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence which decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation."
  },
  {
    "title": "Contextual Online Uncertainty-Aware Preference Learning for Human Feedback",
    "url": "http://arxiv.org/abs/2504.19342v1",
    "arxiv_id": "2504.19342v1",
    "authors": [
      "Nan Lu",
      "Ethan X. Fang",
      "Junwei Lu"
    ],
    "published": "2025-04-27T19:59:11+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a pivotal paradigm in artificial intelligence to align large models with human preferences. In this paper, we propose a novel statistical framework to simultaneously conduct the online decision-making and statistical inference on the optimal model using human preference data based on dynamic contextual information. Our approach introduces an efficient decision strategy that achieves both the optimal regret bound and the asymptotic distribution of the estimators. A key challenge in RLHF is handling the dependent online human preference outcomes with dynamic contexts. To address this, in the methodological aspect, we propose a two-stage algorithm starting with $\\epsilon$-greedy followed by exploitations; in the theoretical aspect, we tailor anti-concentration inequalities and matrix martingale concentration techniques to derive the uniform estimation rate and asymptotic normality of the estimators using dependent samples from both stages. Extensive simulation results demonstrate that our method outperforms state-of-the-art strategies. We apply the proposed framework to analyze the human preference data for ranking large language models on the Massive Multitask Language Understanding dataset, yielding insightful results on the performance of different large language models for medical anatomy knowledge."
  },
  {
    "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
    "url": "http://arxiv.org/abs/2504.19266v1",
    "arxiv_id": "2504.19266v1",
    "authors": [
      "Xiaofeng Jin",
      "Matteo Frosi",
      "Matteo Matteucci"
    ],
    "published": "2025-04-27T14:46:43+00:00",
    "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness."
  },
  {
    "title": "Deep Reinforcement Learning for Automated Web GUI Testing",
    "url": "http://arxiv.org/abs/2504.19237v1",
    "arxiv_id": "2504.19237v1",
    "authors": [
      "Zhiyu Gu",
      "Chenxu Liu",
      "Guoquan Wu",
      "Yifei Zhang",
      "ChenXi Yang",
      "Zheheng Liang",
      "Wei Chen",
      "Jun Wei"
    ],
    "published": "2025-04-27T13:42:30+00:00",
    "summary": "Automated GUI testing of web applications has always been considered a challenging task considering their large state space and complex interaction logic. Deep Reinforcement Learning (DRL) is a recent extension of Reinforcement Learning (RL), which takes advantage of the powerful learning capabilities of neural networks, making it suitable for complex exploration space. In this paper, leveraging the capability of deep reinforcement learning, we propose WebRLED, an effective approach for automated GUI testing of complex web applications. WebRLED has the following characteristics: (1) a grid-based action value learning technique, which can improve the efficiency of state space exploration; (2) a novel action discriminator which can be trained during the exploration to identify more actions; (3) an adaptive, curiosity-driven reward model, which considers the novelty of an explored state within an episode and global history, and can guide exploration continuously. We conduct a comprehensive evaluation of WebRLED on 12 open-source web applications and a field study of the top 50 most popular web applications in the world. The experimental results show that WebRLED achieves higher code/state coverage and failure detection rate compared to existing state-of-the-art (SOTA) techniques. Furthermore, WebRLED finds 695 unique failures in 50 real-world applications."
  },
  {
    "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors",
    "url": "http://arxiv.org/abs/2504.20106v1",
    "arxiv_id": "2504.20106v1",
    "authors": [
      "Ren-Wei Liang",
      "Chin-Ting Hsu",
      "Chan-Hung Yu",
      "Saransh Agrawal",
      "Shih-Cheng Huang",
      "Shang-Tse Chen",
      "Kuan-Hao Huang",
      "Shao-Hua Sun"
    ],
    "published": "2025-04-27T12:16:51+00:00",
    "summary": "Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment."
  },
  {
    "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
    "url": "http://arxiv.org/abs/2504.19162v1",
    "arxiv_id": "2504.19162v1",
    "authors": [
      "Jiaqi Chen",
      "Bang Zhang",
      "Ruotian Ma",
      "Peisong Wang",
      "Xiaodan Liang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Kwan-Yee K. Wong"
    ],
    "published": "2025-04-27T08:45:06+00:00",
    "summary": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a \"sneaky generator\" that deliberately produces erroneous steps designed to be difficult to detect, and a \"critic\" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models."
  },
  {
    "title": "BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.19142v1",
    "arxiv_id": "2504.19142v1",
    "authors": [
      "Chenhao Xu",
      "Chunyu Chen",
      "Jinglin Peng",
      "Jiannan Wang",
      "Jun Gao"
    ],
    "published": "2025-04-27T07:49:01+00:00",
    "summary": "Most large enterprises build predefined data pipelines and execute them periodically to process operational data using SQL queries for various tasks. A key issue in minimizing the overall makespan of these pipelines is the efficient scheduling of concurrent queries within the pipelines. Existing tools mainly rely on simple heuristic rules due to the difficulty of expressing the complex features and mutual influences of queries. The latest reinforcement learning (RL) based methods have the potential to capture these patterns from feedback, but it is non-trivial to apply them directly due to the large scheduling space, high sampling cost, and poor sample utilization.   Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler for Batch concurrent Queries via reinforcement learning. Specifically, BQSched designs an attention-based state representation to capture the complex query patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy optimization (PPO) algorithm, to fully exploit the rich signals of Individual Query completion in logs. Based on the RL framework above, BQSched further introduces three optimization strategies, including adaptive masking to prune the action space, scheduling gain-based query clustering to deal with large query sets, and an incremental simulator to reduce sampling cost. To our knowledge, BQSched is the first non-intrusive batch query scheduler via RL. Extensive experiments show that BQSched can significantly improve the efficiency and stability of batch query scheduling, while also achieving remarkable scalability and adaptability in both data and queries. For example, across all DBMSs and scales tested, BQSched reduces the overall makespan of batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with the commonly used heuristic strategy and the adapted RL-based scheduler, respectively."
  },
  {
    "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments",
    "url": "http://arxiv.org/abs/2504.19139v1",
    "arxiv_id": "2504.19139v1",
    "authors": [
      "Yun Qu",
      "Qi",
      "Wang",
      "Yixiu Mao",
      "Yiqin Lv",
      "Xiangyang Ji"
    ],
    "published": "2025-04-27T07:27:17+00:00",
    "summary": "Task robust adaptation is a long-standing pursuit in sequential decision-making. Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations. The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models are used to surrogate policy evaluation. This work characterizes the optimization pipeline of robust active task sampling as a Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios. Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making. Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios. Our project website is at https://thu-rllab.github.io/PDTS_project_page."
  },
  {
    "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation",
    "url": "http://arxiv.org/abs/2504.19024v1",
    "arxiv_id": "2504.19024v1",
    "authors": [
      "Jiabin Fan",
      "Guoqing Luo",
      "Michael Bowling",
      "Lili Mou"
    ],
    "published": "2025-04-26T21:21:04+00:00",
    "summary": "We propose a novel k-step return estimation method (called KETCHUP) for Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a K-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this K-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our K-step return induction offers a promising direction for enhancing RL-based KD in LLM research."
  },
  {
    "title": "Deep Reinforcement Learning for MIMO Communication with Low-Resolution ADCs",
    "url": "http://arxiv.org/abs/2504.18957v1",
    "arxiv_id": "2504.18957v1",
    "authors": [
      "Marian Temprana Alonso",
      "Dongsheng Luo",
      "Farhad Shirani"
    ],
    "published": "2025-04-26T15:42:01+00:00",
    "summary": "Multiple-input multiple-output (MIMO) wireless systems conventionally use high-resolution analog-to-digital converters (ADCs) at the receiver side to faithfully digitize received signals prior to digital signal processing. However, the power consumption of ADCs increases significantly as the bandwidth is increased, particularly in millimeter wave communications systems. A combination of two mitigating approaches has been considered in the literature: i) to use hybrid beamforming to reduce the number of ADCs, and ii) to use low-resolution ADCs to reduce per ADC power consumption. Lowering the number and resolution of the ADCs naturally reduces the communication rate of the system, leading to a tradeoff between ADC power consumption and communication rate. Prior works have shown that optimizing over the hybrid beamforming matrix and ADC thresholds may reduce the aforementioned rate-loss significantly. A key challenge is the complexity of optimization over all choices of beamforming matrices and threshold vectors. This work proposes a reinforcement learning (RL) architecture to perform the optimization. The proposed approach integrates deep neural network-based mutual information estimators for reward calculation with policy gradient methods for reinforcement learning. The approach is robust to dynamic channel statistics and noisy CSI estimates. It is shown theoretically that greedy RL methods converge to the globally optimal policy. Extensive empirical evaluations are provided demonstrating that the performance of the RL-based approach closely matches exhaustive search optimization across the solution space."
  },
  {
    "title": "Two-Agent DRL for Power Allocation and IRS Orientation in Dynamic NOMA-based OWC Networks",
    "url": "http://arxiv.org/abs/2504.18937v1",
    "arxiv_id": "2504.18937v1",
    "authors": [
      "Ahrar N. Hamad",
      "Ahmad Adnan Qidan",
      "Taisir E. H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "published": "2025-04-26T14:43:34+00:00",
    "summary": "Intelligent reflecting surfaces (IRSs) technology has been considered a promising solution in visible light communication (VLC) systems due to its potential to overcome the line-of-sight (LoS) blockage issue and enhance coverage. Moreover, integrating IRS with a downlink non-orthogonal multiple access (NOMA) transmission technique for multi-users is a smart solution to achieve a high sum rate and improve system performance. In this paper, a dynamic IRS-assisted NOMA-VLC system is modeled, and an optimization problem is formulated to maximize sum energy efficiency (SEE) and fairness among multiple mobile users under power allocation and IRS mirror orientation constraints. Due to the non-convex nature of the optimization problem and the non-linearity of the constraints, conventional optimization methods are impractical for real-time solutions. Therefore, a two-agent deep reinforcement learning (DRL) algorithm is designed for optimizing power allocation and IRS orientation based on centralized training with decentralized execution to obtain fast and real-time solutions in dynamic environments. The results show the superior performance of the proposed DRL algorithm compared to standard DRL algorithms typically used for resource allocation in wireless communication. The results also show that the proposed DRL algorithm achieves higher performance compared to deployments without IRS and with randomly oriented IRS elements."
  },
  {
    "title": "Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.18931v1",
    "arxiv_id": "2504.18931v1",
    "authors": [
      "Dianwei Chen",
      "Yaobang Gong",
      "Xianfeng Yang"
    ],
    "published": "2025-04-26T14:17:06+00:00",
    "summary": "Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS) are key to improving road safety, yet most existing implementations focus primarily on the vehicle ahead, neglecting the behavior of following vehicles. This shortfall often leads to chain reaction collisions in high speed, densely spaced traffic particularly when a middle vehicle suddenly brakes and trailing vehicles cannot respond in time. To address this critical gap, we propose a novel longitudinal control and collision avoidance algorithm that integrates adaptive cruising with emergency braking. Leveraging deep reinforcement learning, our method simultaneously accounts for both leading and following vehicles. Through a data preprocessing framework that calibrates real-world sensor data, we enhance the robustness and reliability of the training process, ensuring the learned policy can handle diverse driving conditions. In simulated high risk scenarios (e.g., emergency braking in dense traffic), the algorithm effectively prevents potential pile up collisions, even in situations involving heavy duty vehicles. Furthermore, in typical highway scenarios where three vehicles decelerate, the proposed DRL approach achieves a 99% success rate far surpassing the standard Federal Highway Administration speed concepts guide, which reaches only 36.77% success under the same conditions."
  },
  {
    "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning",
    "url": "http://arxiv.org/abs/2504.18904v1",
    "arxiv_id": "2504.18904v1",
    "authors": [
      "Haoran Geng",
      "Feishi Wang",
      "Songlin Wei",
      "Yuyang Li",
      "Bangjun Wang",
      "Boshi An",
      "Charlie Tianyue Cheng",
      "Haozhe Lou",
      "Peihao Li",
      "Yen-Jen Wang",
      "Yutong Liang",
      "Dylan Goetting",
      "Chaoyi Xu",
      "Haozhe Chen",
      "Yuxi Qian",
      "Yiran Geng",
      "Jiageng Mao",
      "Weikang Wan",
      "Mingtong Zhang",
      "Jiangran Lyu",
      "Siheng Zhao",
      "Jiazhao Zhang",
      "Jialiang Zhang",
      "Chengyang Zhao",
      "Haoran Lu",
      "Yufei Ding",
      "Ran Gong",
      "Yuran Wang",
      "Yuxuan Kuang",
      "Ruihai Wu",
      "Baoxiong Jia",
      "Carlo Sferrazza",
      "Hao Dong",
      "Siyuan Huang",
      "Yue Wang",
      "Jitendra Malik",
      "Pieter Abbeel"
    ],
    "published": "2025-04-26T12:31:04+00:00",
    "summary": "Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning."
  },
  {
    "title": "Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning",
    "url": "http://arxiv.org/abs/2504.18902v1",
    "arxiv_id": "2504.18902v1",
    "authors": [
      "Cyril Shih-Huan Hsu",
      "Anestis Dalgkitsis",
      "Chrysa Papagianni",
      "Paola Grosso"
    ],
    "published": "2025-04-26T12:18:57+00:00",
    "summary": "In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks."
  },
  {
    "title": "Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots",
    "url": "http://arxiv.org/abs/2504.18794v1",
    "arxiv_id": "2504.18794v1",
    "authors": [
      "Brendon Johnson",
      "Alfredo Weitzenfeld"
    ],
    "published": "2025-04-26T04:30:10+00:00",
    "summary": "Hierarchical reinforcement learning (HRL) is hypothesized to be able to take advantage of the inherent hierarchy in robot learning tasks with sparse reward schemes, in contrast to more traditional reinforcement learning algorithms. In this research, hierarchical reinforcement learning is evaluated and contrasted with standard reinforcement learning in complex navigation tasks. We evaluate unique characteristics of HRL, including their ability to create sub-goals and the termination function. We constructed experiments to test the differences between PPO and HRL, different ways of creating sub-goals, manual vs automatic sub-goal creation, and the effects of the frequency of termination on performance. These experiments highlight the advantages of HRL and how it achieves these advantages."
  },
  {
    "title": "ThinkFL: Self-Refining Failure Localization for Microservice Systems via Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2504.18776v1",
    "arxiv_id": "2504.18776v1",
    "authors": [
      "Lingzhe Zhang",
      "Yunpeng Zhai",
      "Tong Jia",
      "Chiming Duan",
      "Siyu Yu",
      "Jinyang Gao",
      "Bolin Ding",
      "Zhonghai Wu",
      "Ying Li"
    ],
    "published": "2025-04-26T03:08:30+00:00",
    "summary": "As modern microservice systems grow increasingly popular and complex-often consisting of hundreds or even thousands of fine-grained, interdependent components-they are becoming more susceptible to frequent and subtle failures. Ensuring system reliability therefore hinges on accurate and efficient failure localization. Traditional failure localization approaches based on small models lack the flexibility to adapt to diverse failure scenarios, while recent LLM-based methods suffer from two major limitations: they often rely on rigid invocation workflows that constrain the model's ability to dynamically explore optimal localization paths, and they require resource-intensive inference, making them cost-prohibitive for real-world deployment. To address these challenges, we explore the use of reinforcement fine-tuning to equip lightweight LLMs with reasoning and self-refinement capabilities, significantly improving the cost-effectiveness and adaptability of LLM-based failure localization. We begin with an empirical study to identify three key capabilities essential for accurate localization. Building on these insights, we propose a progressive multi-stage GRPO fine-tuning framework, which integrates a multi-factor failure localization grader and a recursion-of-thought actor module. The resulting model, ThinkFL, not only outperforms existing state-of-the-art LLMs and baseline methods in localization accuracy but also reduces end-to-end localization latency from minutes to seconds, demonstrating strong potential for real-world applications."
  },
  {
    "title": "Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance",
    "url": "http://arxiv.org/abs/2504.18766v1",
    "arxiv_id": "2504.18766v1",
    "authors": [
      "Wenjun Cao"
    ],
    "published": "2025-04-26T02:12:02+00:00",
    "summary": "Reinforcement learning (RL) suffers from severe sample inefficiency, especially during early training, requiring extensive environmental interactions to perform competently. Existing methods tend to solve this by incorporating prior knowledge, but introduce significant architectural and implementation complexity. We propose Dynamic Action Interpolation (DAI), a universal yet straightforward framework that interpolates expert and RL actions via a time-varying weight $\\alpha(t)$, integrating into any Actor-Critic algorithm with just a few lines of code and without auxiliary networks or additional losses. Our theoretical analysis shows that DAI reshapes state visitation distributions to accelerate value function learning while preserving convergence guarantees. Empirical evaluations across MuJoCo continuous control tasks demonstrate that DAI improves early-stage performance by over 160\\% on average and final performance by more than 50\\%, with the Humanoid task showing a 4$\\times$ improvement early on and a 2$\\times$ gain at convergence. These results challenge the assumption that complex architectural modifications are necessary for sample-efficient reinforcement learning."
  },
  {
    "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\\sqrt{T}$-Regret",
    "url": "http://arxiv.org/abs/2504.18657v1",
    "arxiv_id": "2504.18657v1",
    "authors": [
      "Benjamin Schiffer",
      "Lucas Janson"
    ],
    "published": "2025-04-25T19:22:57+00:00",
    "summary": "Understanding how to efficiently learn while adhering to safety constraints is essential for using online reinforcement learning in practical applications. However, proving rigorous regret bounds for safety-constrained reinforcement learning is difficult due to the complex interaction between safety, exploration, and exploitation. In this work, we seek to establish foundations for safety-constrained reinforcement learning by studying the canonical problem of controlling a one-dimensional linear dynamical system with unknown dynamics. We study the safety-constrained version of this problem, where the state must with high probability stay within a safe region, and we provide the first safe algorithm that achieves regret of $\\tilde{O}_T(\\sqrt{T})$. Furthermore, the regret is with respect to the baseline of truncated linear controllers, a natural baseline of non-linear controllers that are well-suited for safety-constrained linear systems. In addition to introducing this new baseline, we also prove several desirable continuity properties of the optimal controller in this baseline. In showing our main result, we prove that whenever the constraints impact the optimal controller, the non-linearity of our controller class leads to a faster rate of learning than in the unconstrained setting."
  },
  {
    "title": "Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion",
    "url": "http://arxiv.org/abs/2504.18631v1",
    "arxiv_id": "2504.18631v1",
    "authors": [
      "Dingxin Lu",
      "Shurui Wu",
      "Xinyi Huang"
    ],
    "published": "2025-04-25T18:15:59+00:00",
    "summary": "With the timely formation of personalized intervention plans based on high-dimensional heterogeneous time series information becoming an important challenge in the medical field today, electronic medical records, wearables, and other multi-source medical data are increasingly generated and diversified. In this work, we develop a system to generate personalized medical intervention strategies based on Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion. First, by incorporating relative policy constraints among the groups during policy gradient updates, we adaptively balance individual and group gains. To improve the robustness and interpretability of decision-making, a multi-layer neural network structure is employed to group-code patient characteristics. Second, for the rapid multi-modal fusion of multi-source heterogeneous time series, a multi-channel neural network combined with a self-attention mechanism is used for dynamic feature extraction. Key feature screening and aggregation are achieved through a differentiable gating network. Finally, a collaborative search process combining a genetic algorithm and Monte Carlo tree search is proposed to find the ideal intervention strategy, achieving global optimization. Experimental results show significant improvements in accuracy, coverage, and decision-making benefits compared with existing methods."
  },
  {
    "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks",
    "url": "http://arxiv.org/abs/2504.18519v1",
    "arxiv_id": "2504.18519v1",
    "authors": [
      "Han Zhang",
      "Hao Zhou",
      "Medhat Elsayed",
      "Majid Bavand",
      "Raimundas Gaigalas",
      "Yigit Ozcan",
      "Melike Erol-Kantarci"
    ],
    "published": "2025-04-25T17:40:35+00:00",
    "summary": "Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other models by parameter exchange. Meanwhile, such attacks can be hard to detect due to the dynamic wireless environment, especially considering local models can be heterogeneous with non-independent and identically distributed (non-IID) data. Therefore, it is critical to evaluate the effect of malicious attacks and develop advanced defense techniques for FL-enabled wireless networks. In this work, we introduce a federated deep reinforcement learning-based cell sleep control scenario that enhances the energy efficiency of the network. We propose multiple intelligent attacks targeting the learning-based approach and we propose defense methods to mitigate such attacks. In particular, we have designed two attack models, generative adversarial network (GAN)-enhanced model poisoning attack and regularization-based model poisoning attack. As a counteraction, we have proposed two defense schemes, autoencoder-based defense, and knowledge distillation (KD)-enabled defense. The autoencoder-based defense method leverages an autoencoder to identify the malicious participants and only aggregate the parameters of benign local models during the global aggregation, while KD-based defense protects the model from attacks by controlling the knowledge transferred between the global model and local models."
  },
  {
    "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning",
    "url": "http://arxiv.org/abs/2504.18458v1",
    "arxiv_id": "2504.18458v1",
    "authors": [
      "Wenyi Xiao",
      "Leilei Gan",
      "Weilong Dai",
      "Wanggui He",
      "Ziwei Huang",
      "Haoyuan Li",
      "Fangxun Shu",
      "Zhelun Yu",
      "Peng Zhang",
      "Hao Jiang",
      "Fei Wu"
    ],
    "published": "2025-04-25T16:11:23+00:00",
    "summary": "Recent advances in large vision-language models (LVLMs) have revealed an \\textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10\\% relative improvement compared to the base model, while reducing token usage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy."
  },
  {
    "title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation",
    "url": "http://arxiv.org/abs/2504.18453v1",
    "arxiv_id": "2504.18453v1",
    "authors": [
      "Peiyuan Jing",
      "Kinhei Lee",
      "Zhenxuan Zhang",
      "Huichi Zhou",
      "Zhengqing Yuan",
      "Zhifan Gao",
      "Lei Zhu",
      "Giorgos Papanastasiou",
      "Yingying Fang",
      "Guang Yang"
    ],
    "published": "2025-04-25T16:05:06+00:00",
    "summary": "Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports."
  },
  {
    "title": "Pushing the boundary on Natural Language Inference",
    "url": "http://arxiv.org/abs/2504.18376v1",
    "arxiv_id": "2504.18376v1",
    "authors": [
      "Pablo Miralles-Gonz\u00e1lez",
      "Javier Huertas-Tato",
      "Alejandro Mart\u00edn",
      "David Camacho"
    ],
    "published": "2025-04-25T14:20:57+00:00",
    "summary": "Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\\unicode{x2013}$or on all of them considering our replication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality."
  },
  {
    "title": "Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization",
    "url": "http://arxiv.org/abs/2504.18371v1",
    "arxiv_id": "2504.18371v1",
    "authors": [
      "Irshad A. Meer",
      "Bruno H\u00f6rmann",
      "Mustafa Ozger",
      "Fabien Geyer",
      "Alberto Viseras",
      "Dominic Schupke",
      "Cicek Cavdar"
    ],
    "published": "2025-04-25T14:11:51+00:00",
    "summary": "The integration of unmanned aerial vehicles (UAVs) into cellular networks presents significant mobility management challenges, primarily due to frequent handovers caused by probabilistic line-of-sight conditions with multiple ground base stations (BSs). To tackle these challenges, reinforcement learning (RL)-based methods, particularly deep Q-networks (DQN), have been employed to optimize handover decisions dynamically. However, a major drawback of these learning-based approaches is their black-box nature, which limits interpretability in the decision-making process. This paper introduces an explainable AI (XAI) framework that incorporates Shapley Additive Explanations (SHAP) to provide deeper insights into how various state parameters influence handover decisions in a DQN-based mobility management system. By quantifying the impact of key features such as reference signal received power (RSRP), reference signal received quality (RSRQ), buffer status, and UAV position, our approach enhances the interpretability and reliability of RL-based handover solutions. To validate and compare our framework, we utilize real-world network performance data collected from UAV flight trials. Simulation results show that our method provides intuitive explanations for policy decisions, effectively bridging the gap between AI-driven models and human decision-makers."
  },
  {
    "title": "Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps",
    "url": "http://arxiv.org/abs/2504.18300v1",
    "arxiv_id": "2504.18300v1",
    "authors": [
      "Simon Hakenes",
      "Tobias Glasmachers"
    ],
    "published": "2025-04-25T12:19:35+00:00",
    "summary": "This paper addresses the challenge of navigation in large, visually complex environments with sparse rewards. We propose a method that uses object-oriented macro actions grounded in a topological map, allowing a simple Deep Q-Network (DQN) to learn effective navigation policies. The agent builds a map by detecting objects from RGBD input and selecting discrete macro actions that correspond to navigating to these objects. This abstraction drastically reduces the complexity of the underlying reinforcement learning problem and enables generalization to unseen environments. We evaluate our approach in a photorealistic 3D simulation and show that it significantly outperforms a random baseline under both immediate and terminal reward conditions. Our results demonstrate that topological structure and macro-level abstraction can enable sample-efficient learning even from pixel data."
  },
  {
    "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing",
    "url": "http://arxiv.org/abs/2504.18253v1",
    "arxiv_id": "2504.18253v1",
    "authors": [
      "Amirhossein Zhalehmehrabi",
      "Daniele Meli",
      "Francesco Dal Santo",
      "Francesco Trotti",
      "Alessandro Farinelli"
    ],
    "published": "2025-04-25T10:56:56+00:00",
    "summary": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments."
  },
  {
    "title": "Aligning Language Models for Icelandic Legal Text Summarization",
    "url": "http://arxiv.org/abs/2504.18180v1",
    "arxiv_id": "2504.18180v1",
    "authors": [
      "\u00de\u00f3rir Hrafn Har\u00f0arson",
      "Hrafn Loftsson",
      "Stef\u00e1n \u00d3lafsson"
    ],
    "published": "2025-04-25T08:55:15+00:00",
    "summary": "The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain."
  },
  {
    "title": "Learning from Less: SINDy Surrogates in RL",
    "url": "http://arxiv.org/abs/2504.18113v1",
    "arxiv_id": "2504.18113v1",
    "authors": [
      "Aniket Dixit",
      "Muhammad Ibrahim Khan",
      "Faizan Ahmed",
      "James Brusey"
    ],
    "published": "2025-04-25T06:34:19+00:00",
    "summary": "This paper introduces an approach for developing surrogate environments in reinforcement learning (RL) using the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach through extensive experiments in OpenAI Gym environments, particularly Mountain Car and Lunar Lander. Our results show that SINDy-based surrogate models can accurately capture the underlying dynamics of these environments while reducing computational costs by 20-35%. With only 75 interactions for Mountain Car and 1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for LunarLander position. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in the original environments, exhibiting similar convergence patterns and final performance metrics. This work contributes to the field of model-based RL by providing an efficient method for generating accurate, interpretable surrogate environments."
  },
  {
    "title": "RL-Driven Data Generation for Robust Vision-Based Dexterous Grasping",
    "url": "http://arxiv.org/abs/2504.18084v1",
    "arxiv_id": "2504.18084v1",
    "authors": [
      "Atsushi Kanehira",
      "Naoki Wake",
      "Kazuhiro Sasabuchi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ],
    "published": "2025-04-25T05:21:19+00:00",
    "summary": "This work presents reinforcement learning (RL)-driven data augmentation to improve the generalization of vision-action (VA) models for dexterous grasping. While real-to-sim-to-real frameworks, where a few real demonstrations seed large-scale simulated data, have proven effective for VA models, applying them to dexterous settings remains challenging: obtaining stable multi-finger contacts is nontrivial across diverse object shapes. To address this, we leverage RL to generate contact-rich grasping data across varied geometries. In line with the real-to-sim-to-real paradigm, the grasp skill is formulated as a parameterized and tunable reference trajectory refined by a residual policy learned via RL. This modular design enables trajectory-level control that is both consistent with real demonstrations and adaptable to diverse object geometries. A vision-conditioned policy trained on simulation-augmented data demonstrates strong generalization to unseen objects, highlighting the potential of our approach to alleviate the data bottleneck in training VA models."
  },
  {
    "title": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control",
    "url": "http://arxiv.org/abs/2504.18062v1",
    "arxiv_id": "2504.18062v1",
    "authors": [
      "Lingyan Bao",
      "Sinwoong Yun",
      "Jemin Lee",
      "Tony Q. S. Quek"
    ],
    "published": "2025-04-25T04:18:23+00:00",
    "summary": "Recent advancements in large language models (LLMs) have led to a significant interest in deploying LLMempowered algorithms for wireless communication networks. Meanwhile, open radio access network (O-RAN) techniques offer unprecedented flexibility, with the non-real-time (non-RT) radio access network (RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT) RIC (near-RT RIC) components enabling intelligent resource management across different time scales. In this paper, we propose the LLM empowered hierarchical RIC (LLM-hRIC) framework to improve the collaboration between RICs. This framework integrates LLMs with reinforcement learning (RL) for efficient network resource management. In this framework, LLMs-empowered non-RT RICs provide strategic guidance and high-level policies based on environmental context. Concurrently, RL-empowered near-RT RICs perform low-latency tasks based on strategic guidance and local near-RT observation. We evaluate the LLM-hRIC framework in an integrated access and backhaul (IAB) network setting. Simulation results demonstrate that the proposed framework achieves superior performance. Finally, we discuss the key future challenges in applying LLMs to O-RAN."
  },
  {
    "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2504.18053v1",
    "arxiv_id": "2504.18053v1",
    "authors": [
      "Jianyu Liu",
      "Hangyu Guo",
      "Ranjie Duan",
      "Xingyuan Bu",
      "Yancheng He",
      "Shilong Li",
      "Hui Huang",
      "Jiaheng Liu",
      "Yucheng Wang",
      "Chenchen Jing",
      "Xingwei Qu",
      "Xiao Zhang",
      "Yingshui Tan",
      "Yanan Wu",
      "Jihao Gu",
      "Yangguang Li",
      "Jianke Zhu"
    ],
    "published": "2025-04-25T03:54:24+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \\textbf{DREAM} (\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety \\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM."
  },
  {
    "title": "Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation",
    "url": "http://arxiv.org/abs/2504.18010v1",
    "arxiv_id": "2504.18010v1",
    "authors": [
      "Zilin Huang",
      "Zihao Sheng",
      "Zhengyang Wan",
      "Yansong Qu",
      "Yuhao Luo",
      "Boyue Wang",
      "Pei Li",
      "Yen-Jung Chen",
      "Jiancong Chen",
      "Keke Long",
      "Jiayi Meng",
      "Yue Leng",
      "Sikai Chen"
    ],
    "published": "2025-04-25T01:33:26+00:00",
    "summary": "Recent advances in autonomous system simulation platforms have significantly enhanced the safe and scalable testing of driving policies. However, existing simulators do not yet fully meet the needs of future transportation research, particularly in modeling socially-aware driving agents and enabling effective human-AI collaboration. This paper introduces Sky-Drive, a novel distributed multi-agent simulation platform that addresses these limitations through four key innovations: (a) a distributed architecture for synchronized simulation across multiple terminals; (b) a multi-modal human-in-the-loop framework integrating diverse sensors to collect rich behavioral data; (c) a human-AI collaboration mechanism supporting continuous and adaptive knowledge exchange; and (d) a digital twin (DT) framework for constructing high-fidelity virtual replicas of real-world transportation environments. Sky-Drive supports diverse applications such as autonomous vehicle (AV)-vulnerable road user (VRU) interaction modeling, human-in-the-loop training, socially-aware reinforcement learning, personalized driving policy, and customized scenario generation. Future extensions will incorporate foundation models for context-aware decision support and hardware-in-the-loop (HIL) testing for real-world validation. By bridging scenario generation, data collection, algorithm training, and hardware integration, Sky-Drive has the potential to become a foundational platform for the next generation of socially-aware and human-centered autonomous transportation research. The demo video and code are available at:https://sky-lab-uw.github.io/Sky-Drive-website/"
  },
  {
    "title": "Do We Need Transformers to Play FPS Video Games?",
    "url": "http://arxiv.org/abs/2504.17891v1",
    "arxiv_id": "2504.17891v1",
    "authors": [
      "Karmanbir Batth",
      "Krish Sethi",
      "Aly Shariff",
      "Leo Shi",
      "Hetul Patel"
    ],
    "published": "2025-04-24T19:10:55+00:00",
    "summary": "In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transformers (DT) for offline reinforcement learning. DTQN leverages the sequential modelling capabilities of Transformers to enhance Q-learning in partially observable environments,while Decision Transformers repurpose sequence modelling techniques to enable offline agents to learn from past trajectories without direct interaction with the environment. We conclude that while Transformers might have performed well in Atari games, more traditional methods perform better than Transformer based method in both the settings in the VizDoom environment."
  },
  {
    "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures",
    "url": "http://arxiv.org/abs/2504.17857v1",
    "arxiv_id": "2504.17857v1",
    "authors": [
      "A. J Miller",
      "Fangzhou Yu",
      "Michael Brauckmann",
      "Farbod Farshidian"
    ],
    "published": "2025-04-24T18:01:36+00:00",
    "summary": "This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API."
  },
  {
    "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
    "url": "http://arxiv.org/abs/2504.17838v1",
    "arxiv_id": "2504.17838v1",
    "authors": [
      "Bernhard Jaeger",
      "Daniel Dauner",
      "Jens Bei\u00dfwenger",
      "Simon Gerstenecker",
      "Kashyap Chitta",
      "Andreas Geiger"
    ],
    "published": "2025-04-24T17:56:01+00:00",
    "summary": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v1",
    "arxiv_id": "2504.17771v1",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce \\ourmethod, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed ``IL+RL'' training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control",
    "url": "http://arxiv.org/abs/2504.17771v2",
    "arxiv_id": "2504.17771v2",
    "authors": [
      "Haochen Wang",
      "Zhiwei Shi",
      "Chengxi Zhu",
      "Yafei Qiao",
      "Cheng Zhang",
      "Fan Yang",
      "Pengjie Ren",
      "Lan Lu",
      "Dong Xuan"
    ],
    "published": "2025-04-24T17:46:29+00:00",
    "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce Hamlet, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed \"IL+RL\" training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/."
  },
  {
    "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence",
    "url": "http://arxiv.org/abs/2504.17703v1",
    "arxiv_id": "2504.17703v1",
    "authors": [
      "Edward Collins",
      "Michel Wang"
    ],
    "published": "2025-04-24T16:10:29+00:00",
    "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field of distributed machine learning, enabling multiple clients such as mobile devices, edge nodes, or organizations to collaboratively train a shared global model without the need to centralize sensitive data. This decentralized approach addresses growing concerns around data privacy, security, and regulatory compliance, making it particularly attractive in domains such as healthcare, finance, and smart IoT systems. This survey provides a concise yet comprehensive overview of Federated Learning, beginning with its core architecture and communication protocol. We discuss the standard FL lifecycle, including local training, model aggregation, and global updates. A particular emphasis is placed on key technical challenges such as handling non-IID (non-independent and identically distributed) data, mitigating system and hardware heterogeneity, reducing communication overhead, and ensuring privacy through mechanisms like differential privacy and secure aggregation. Furthermore, we examine emerging trends in FL research, including personalized FL, cross-device versus cross-silo settings, and integration with other paradigms such as reinforcement learning and quantum computing. We also highlight real-world applications and summarize benchmark datasets and evaluation metrics commonly used in FL research. Finally, we outline open research problems and future directions to guide the development of scalable, efficient, and trustworthy FL systems."
  },
  {
    "title": "Applied Sheaf Theory For Multi-agent Artificial Intelligence (Reinforcement Learning) Systems: A Prospectus",
    "url": "http://arxiv.org/abs/2504.17700v1",
    "arxiv_id": "2504.17700v1",
    "authors": [
      "Eric Schmid"
    ],
    "published": "2025-04-24T16:08:53+00:00",
    "summary": "This paper provides a pedagogical introduction to classical sheaf theory and sheaf cohomology, followed by a research prospectus exploring potential applications to multi-agent artificial intelligence systems. The first section offers a comprehensive overview of fundamental sheaf-theoretic concepts-presheaves, sheaves, stalks, and cohomology-aimed at researchers in computer science and AI who may not have extensive background in algebraic topology. The second section presents a detailed research prospectus that outlines a roadmap for developing sheaf-theoretic approaches to model and analyze complex systems of interacting agents. We propose that sheaf theory's inherent local-to-global perspective may provide valuable mathematical tools for reasoning about how local agent behaviors collectively determine emergent system properties. The third section contains a literature review connecting sheaf theory with existing research in multi-agent systems, reinforcement learning, and economic modeling. This paper does not present a completed model but rather lays theoretical groundwork and identifies promising research directions that could bridge abstract mathematics with practical AI applications, potentially revealing new approaches to coordination and emergence in multi-agent systems."
  },
  {
    "title": "SAPO-RL: Sequential Actuator Placement Optimization for Fuselage Assembly via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.17603v1",
    "arxiv_id": "2504.17603v1",
    "authors": [
      "Peng Ye",
      "Juan Du"
    ],
    "published": "2025-04-24T14:28:42+00:00",
    "summary": "Precise assembly of composite fuselages is critical for aircraft assembly to meet the ultra-high precision requirements. Due to dimensional variations, there is a gap when two fuselage assemble. In practice, actuators are required to adjust fuselage dimensions by applying forces to specific points on fuselage edge through pulling or pushing force actions. The positioning and force settings of these actuators significantly influence the efficiency of the shape adjustments. The current literature usually predetermines the fixed number of actuators, which is not optimal in terms of overall quality and corresponding actuator costs. However, optimal placement of actuators in terms of both locations and number is challenging due to compliant structures, complex material properties, and dimensional variabilities of incoming fuselages. To address these challenges, this paper introduces a reinforcement learning (RL) framework that enables sequential decision-making for actuator placement selection and optimal force computation. Specifically, our methodology employs the Dueling Double Deep Q-Learning (D3QN) algorithm to refine the decision-making capabilities of sequential actuator placements. The environment is meticulously crafted to enable sequential and incremental selection of an actuator based on system states. We formulate the actuator selection problem as a submodular function optimization problem, where the sub-modularity properties can be adopted to efficiently achieve near-optimal solutions. The proposed methodology has been comprehensively evaluated through numerical studies and comparison studies, demonstrating its effectiveness and outstanding performance in enhancing assembly precision with limited actuator numbers."
  },
  {
    "title": "Mitigating xApp conflicts for efficient network slicing in 6G O-RAN: a graph convolutional-based attention network approach",
    "url": "http://arxiv.org/abs/2504.17590v1",
    "arxiv_id": "2504.17590v1",
    "authors": [
      "Sihem Bakri",
      "Indrakshi Dey",
      "Harun Siljak",
      "Marco Ruffini",
      "Nicola Marchetti"
    ],
    "published": "2025-04-24T14:20:11+00:00",
    "summary": "O-RAN (Open-Radio Access Network) offers a flexible, open architecture for next-generation wireless networks. Network slicing within O-RAN allows network operators to create customized virtual networks, each tailored to meet the specific needs of a particular application or service. Efficiently managing these slices is crucial for future 6G networks. O-RAN introduces specialized software applications called xApps that manage different network functions. In network slicing, an xApp can be responsible for managing a separate network slice. To optimize resource allocation across numerous network slices, these xApps must coordinate. Traditional methods where all xApps communicate freely can lead to excessive overhead, hindering network performance. In this paper, we address the issue of xApp conflict mitigation by proposing an innovative Zero-Touch Management (ZTM) solution for radio resource management in O-RAN. Our approach leverages Multi-Agent Reinforcement Learning (MARL) to enable xApps to learn and optimize resource allocation without the need for constant manual intervention. We introduce a Graph Convolutional Network (GCN)-based attention mechanism to streamline communication among xApps, reducing overhead and improving overall system efficiency. Our results compare traditional MARL, where all xApps communicate, against our MARL GCN-based attention method. The findings demonstrate the superiority of our approach, especially as the number of xApps increases, ultimately providing a scalable and efficient solution for optimal network slicing management in O-RAN."
  },
  {
    "title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization",
    "url": "http://arxiv.org/abs/2504.17578v1",
    "arxiv_id": "2504.17578v1",
    "authors": [
      "Hongshu Guo",
      "Wenjie Qiu",
      "Zeyuan Ma",
      "Xinglin Zhang",
      "Jun Zhang",
      "Yue-Jiao Gong"
    ],
    "published": "2025-04-24T14:09:22+00:00",
    "summary": "Recent research in Cooperative Coevolution~(CC) have achieved promising progress in solving large-scale global optimization problems. However, existing CC paradigms have a primary limitation in that they require deep expertise for selecting or designing effective variable decomposition strategies. Inspired by advancements in Meta-Black-Box Optimization, this paper introduces LCC, a pioneering learning-based cooperative coevolution framework that dynamically schedules decomposition strategies during optimization processes. The decomposition strategy selector is parameterized through a neural network, which processes a meticulously crafted set of optimization status features to determine the optimal strategy for each optimization step. The network is trained via the Proximal Policy Optimization method in a reinforcement learning manner across a collection of representative problems, aiming to maximize the expected optimization performance. Extensive experimental results demonstrate that LCC not only offers certain advantages over state-of-the-art baselines in terms of optimization effectiveness and resource consumption, but it also exhibits promising transferability towards unseen problems."
  },
  {
    "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI",
    "url": "http://arxiv.org/abs/2504.17833v1",
    "arxiv_id": "2504.17833v1",
    "authors": [
      "Xiao Huang",
      "Zhengzhong Tu",
      "Xinyue Ye",
      "Michael Goodchild"
    ],
    "published": "2025-04-24T13:20:17+00:00",
    "summary": "Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's pivotal role in this transformation. While proprietary LLMs offer accessibility, they often limit the customization, interoperability, and transparency vital for specialized geospatial tasks. Conversely, open-source alternatives significantly advance Geographic Information Science (GIScience) by fostering greater adaptability, reproducibility, and community-driven innovation. Open frameworks empower researchers to tailor solutions, integrate cutting-edge methodologies (e.g., reinforcement learning, advanced spatial indexing), and align with FAIR principles. However, the growing reliance on any LLM necessitates careful consideration of security vulnerabilities, ethical risks, and robust governance for AI-generated geospatial outputs. Ongoing debates on accessibility, regulation, and misuse underscore the critical need for responsible AI development strategies. This paper argues that GIScience advances best not through a single model type, but by cultivating a diverse, interoperable ecosystem combining open-source foundations for innovation, bespoke geospatial models, and interdisciplinary collaboration. By critically evaluating the opportunities and challenges of open-source LLMs within the broader GeoAI landscape, this work contributes to a nuanced discourse on leveraging AI to effectively advance spatial research, policy, and decision-making in an equitable, sustainable, and scientifically rigorous manner."
  },
  {
    "title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks",
    "url": "http://arxiv.org/abs/2504.17526v1",
    "arxiv_id": "2504.17526v1",
    "authors": [
      "Yuelin Liu",
      "Haiyuan Li",
      "Xenofon Vasilakos",
      "Rasheed Hussain",
      "Dimitra Simeonidou"
    ],
    "published": "2025-04-24T13:12:12+00:00",
    "summary": "Future networks (including 6G) are poised to accelerate the realisation of Internet of Everything. However, it will result in a high demand for computing resources to support new services. Mobile Edge Computing (MEC) is a promising solution, enabling to offload computation-intensive tasks to nearby edge servers from the end-user devices, thereby reducing latency and energy consumption. However, relying solely on a single MEC server for task offloading can lead to uneven resource utilisation and suboptimal performance in complex scenarios. Additionally, traditional task offloading strategies specialise in centralised policy decisions, which unavoidably entail extreme transmission latency and reach computational bottleneck. To fill the gaps, we propose a latency and energy efficient Cooperative Task Offloading framework with Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent deep reinforcement learning to address these challenges. This approach fosters edge-edge cooperation and decreases the synchronous waiting time by performing asynchronous training, optimising task offloading, and resource allocation across distributed networks. The performance evaluation demonstrates that the proposed CTO-TP algorithm reduces up to 80% overall system latency and 87% energy consumption compared to the baseline schemes."
  },
  {
    "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.17490v1",
    "arxiv_id": "2504.17490v1",
    "authors": [
      "Mingqi Yuan",
      "Qi Wang",
      "Guozheng Ma",
      "Bo Li",
      "Xin Jin",
      "Yunbo Wang",
      "Xiaokang Yang",
      "Wenjun Zeng",
      "Dacheng Tao"
    ],
    "published": "2025-04-24T12:32:13+00:00",
    "summary": "Developing lifelong learning agents is crucial for artificial general intelligence. However, deep reinforcement learning (RL) systems often suffer from plasticity loss, where neural networks gradually lose their ability to adapt during training. Despite its significance, this field lacks unified benchmarks and evaluation protocols. We introduce Plasticine, the first open-source framework for benchmarking plasticity optimization in deep RL. Plasticine provides single-file implementations of over 13 mitigation methods, 10 evaluation metrics, and learning scenarios with increasing non-stationarity levels from standard to open-ended environments. This framework enables researchers to systematically quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics across different contexts. Our documentation, examples, and source code are available at https://github.com/RLE-Foundation/Plasticine."
  },
  {
    "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.17356v1",
    "arxiv_id": "2504.17356v1",
    "authors": [
      "Weiliang Zhang",
      "Xiaohan Huang",
      "Yi Du",
      "Ziyue Qiao",
      "Qingqing Long",
      "Zhen Meng",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "published": "2025-04-24T08:16:36+00:00",
    "summary": "Feature selection aims to preprocess the target dataset, find an optimal and most streamlined feature subset, and enhance the downstream machine learning task. Among filter, wrapper, and embedded-based approaches, the reinforcement learning (RL)-based subspace exploration strategy provides a novel objective optimization-directed perspective and promising performance. Nevertheless, even with improved performance, current reinforcement learning approaches face challenges similar to conventional methods when dealing with complex datasets. These challenges stem from the inefficient paradigm of using one agent per feature and the inherent complexities present in the datasets. This observation motivates us to investigate and address the above issue and propose a novel approach, namely HRLFS. Our methodology initially employs a Large Language Model (LLM)-based hybrid state extractor to capture each feature's mathematical and semantic characteristics. Based on this information, features are clustered, facilitating the construction of hierarchical agents for each cluster and sub-cluster. Extensive experiments demonstrate the efficiency, scalability, and robustness of our approach. Compared to contemporary or the one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML performance with iterative feature subspace exploration while accelerating total run time by reducing the number of agents involved."
  },
  {
    "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization",
    "url": "http://arxiv.org/abs/2504.17355v1",
    "arxiv_id": "2504.17355v1",
    "authors": [
      "Xiaohan Huang",
      "Dongjie Wang",
      "Zhiyuan Ning",
      "Ziyue Qiao",
      "Qingqing Long",
      "Haowei Zhu",
      "Yi Du",
      "Min Wu",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "published": "2025-04-24T08:16:13+00:00",
    "summary": "Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets."
  },
  {
    "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.17282v1",
    "arxiv_id": "2504.17282v1",
    "authors": [
      "Lynn Cherif",
      "Flemming Kondrup",
      "David Venuto",
      "Ankit Anand",
      "Doina Precup",
      "Khimya Khetarpal"
    ],
    "published": "2025-04-24T06:20:08+00:00",
    "summary": "Agents that can autonomously navigate the web through a graphical user interface (GUI) using a unified action space (e.g., mouse and keyboard actions) can require very large amounts of domain-specific expert demonstrations to achieve good performance. Low sample efficiency is often exacerbated in sparse-reward and large-action-space environments, such as a web GUI, where only a few actions are relevant in any given situation. In this work, we consider the low-data regime, with limited or no access to expert behavior. To enable sample-efficient learning, we explore the effect of constraining the action space through $\\textit{intent-based affordances}$ -- i.e., considering in any situation only the subset of actions that achieve a desired outcome. We propose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$, a method that leverages pre-trained vision-language models (VLMs) to generate code that determines affordable actions through implicit intent-completion functions and using a fully-automated program generation and verification pipeline. These programs are then used in-the-loop of a reinforcement learning agent to return a set of affordances given a pixel observation. By greatly reducing the number of actions that an agent must consider, we demonstrate on a wide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$ $\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent, $\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of tasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared with behavior cloning when a small number of expert demonstrations is available."
  },
  {
    "title": "Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot",
    "url": "http://arxiv.org/abs/2504.17249v1",
    "arxiv_id": "2504.17249v1",
    "authors": [
      "Yufeng Chi",
      "Qiayuan Liao",
      "Junfeng Long",
      "Xiaoyu Huang",
      "Sophia Shao",
      "Borivoje Nikolic",
      "Zhongyu Li",
      "Koushil Sreenath"
    ],
    "published": "2025-04-24T04:58:47+00:00",
    "summary": "Despite significant interest and advancements in humanoid robotics, most existing commercially available hardware remains high-cost, closed-source, and non-transparent within the robotics community. This lack of accessibility and customization hinders the growth of the field and the broader development of humanoid technologies. To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. The core of this design is a modular 3D-printed gearbox for the actuators and robot body. All components can be sourced from widely available e-commerce platforms and fabricated using standard desktop 3D printers, keeping the total hardware cost under $5,000 (based on U.S. market prices). The design emphasizes modularity and ease of fabrication. To address the inherent limitations of 3D-printed gearboxes, such as reduced strength and durability compared to metal alternatives, we adopted a cycloidal gear design, which provides an optimal form factor in this context. Extensive testing was conducted on the 3D-printed actuators to validate their durability and alleviate concerns about the reliability of plastic components. To demonstrate the capabilities of Berkeley Humanoid Lite, we conducted a series of experiments, including the development of a locomotion controller using reinforcement learning. These experiments successfully showcased zero-shot policy transfer from simulation to hardware, highlighting the platform's suitability for research validation. By fully open-sourcing the hardware design, embedded code, and training and deployment frameworks, we aim for Berkeley Humanoid Lite to serve as a pivotal step toward democratizing the development of humanoid robotics. All resources are available at https://lite.berkeley-humanoid.org."
  },
  {
    "title": "Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints",
    "url": "http://arxiv.org/abs/2504.17142v1",
    "arxiv_id": "2504.17142v1",
    "authors": [
      "Siddharth Nair",
      "Timothy F. Walsh",
      "Greg Pickrell",
      "Fabio Semperlotti"
    ],
    "published": "2025-04-23T23:20:44+00:00",
    "summary": "This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space."
  },
  {
    "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments",
    "url": "http://arxiv.org/abs/2504.17087v1",
    "arxiv_id": "2504.17087v1",
    "authors": [
      "Yuran Li",
      "Jama Hussein Mohamud",
      "Chongren Sun",
      "Di Wu",
      "Benoit Boulet"
    ],
    "published": "2025-04-23T20:32:12+00:00",
    "summary": "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support performance evaluation offers a more efficient alternative. However, most studies focus mainly on aligning LLMs' judgments with human preferences, overlooking the existence of biases and mistakes in human judgment. Furthermore, how to select suitable LLM judgments given multiple potential LLM responses remains underexplored. To address these two aforementioned issues, we propose a three-stage meta-judge selection pipeline: 1) developing a comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM agents to score judgments, and 3) applying a threshold to filter out low-scoring judgments. Compared to methods using a single LLM as both judge and meta-judge, our pipeline introduces multi-agent collaboration and a more comprehensive rubric. Experimental results on the JudgeBench dataset show about 15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over the single-agent baseline. Our work demonstrates the potential of LLMs as meta-judges and lays the foundation for future research on constructing preference datasets for LLM-as-a-judge reinforcement learning."
  },
  {
    "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs",
    "url": "http://arxiv.org/abs/2504.17006v1",
    "arxiv_id": "2504.17006v1",
    "authors": [
      "Jalal Arabneydi",
      "Saiful Islam",
      "Srijita Das",
      "Sai Krishna Gottipati",
      "William Duguay",
      "Cloderic Mars",
      "Matthew E. Taylor",
      "Matthew Guzdial",
      "Antoine Fagette",
      "Younes Zerouali"
    ],
    "published": "2025-04-23T18:00:08+00:00",
    "summary": "With the growing popularity of deep reinforcement learning (DRL), human-in-the-loop (HITL) approach has the potential to revolutionize the way we approach decision-making problems and create new opportunities for human-AI collaboration. In this article, we introduce a novel multi-layered hierarchical HITL DRL algorithm that comprises three types of learning: self learning, imitation learning and transfer learning. In addition, we consider three forms of human inputs: reward, action and demonstration. Furthermore, we discuss main challenges, trade-offs and advantages of HITL in solving complex problems and how human information can be integrated in the AI solution systematically. To verify our technical results, we present a real-world unmanned aerial vehicles (UAV) problem wherein a number of enemy drones attack a restricted area. The objective is to design a scalable HITL DRL algorithm for ally drones to neutralize the enemy drones before they reach the area. To this end, we first implement our solution using an award-winning open-source HITL software called Cogment. We then demonstrate several interesting results such as (a) HITL leads to faster training and higher performance, (b) advice acts as a guiding direction for gradient methods and lowers variance, and (c) the amount of advice should neither be too large nor too small to avoid over-training and under-training. Finally, we illustrate the role of human-AI cooperation in solving two real-world complex scenarios, i.e., overloaded and decoy attacks."
  },
  {
    "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models",
    "url": "http://arxiv.org/abs/2504.17004v1",
    "arxiv_id": "2504.17004v1",
    "authors": [
      "Amin Karbasi",
      "Omar Montasser",
      "John Sous",
      "Grigoris Velegkas"
    ],
    "published": "2025-04-23T18:00:07+00:00",
    "summary": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.   First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.   Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.   These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment."
  },
  {
    "title": "Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms",
    "url": "http://arxiv.org/abs/2504.16916v1",
    "arxiv_id": "2504.16916v1",
    "authors": [
      "Hsin-Jung Yang",
      "Mahsa Khosravi",
      "Benjamin Walt",
      "Girish Krishnan",
      "Soumik Sarkar"
    ],
    "published": "2025-04-23T17:41:55+00:00",
    "summary": "Soft continuum arms (SCAs) soft and deformable nature presents challenges in modeling and control due to their infinite degrees of freedom and non-linear behavior. This work introduces a reinforcement learning (RL)-based framework for visual servoing tasks on SCAs with zero-shot sim-to-real transfer capabilities, demonstrated on a single section pneumatic manipulator capable of bending and twisting. The framework decouples kinematics from mechanical properties using an RL kinematic controller for motion planning and a local controller for actuation refinement, leveraging minimal sensing with visual feedback. Trained entirely in simulation, the RL controller achieved a 99.8% success rate. When deployed on hardware, it achieved a 67% success rate in zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This approach offers a scalable solution for SCAs in 3D visual servoing, with potential for further refinement and expanded applications."
  },
  {
    "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion",
    "url": "http://arxiv.org/abs/2504.16875v1",
    "arxiv_id": "2504.16875v1",
    "authors": [
      "Julian Bedei",
      "Murray McBain",
      "Charles Robert Koch",
      "Jakob Andert",
      "David Gordon"
    ],
    "published": "2025-04-23T16:51:49+00:00",
    "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar."
  },
  {
    "title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
    "url": "http://arxiv.org/abs/2504.16855v1",
    "arxiv_id": "2504.16855v1",
    "authors": [
      "Zijing Shi",
      "Meng Fang",
      "Ling Chen"
    ],
    "published": "2025-04-23T16:23:15+00:00",
    "summary": "Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments."
  },
  {
    "title": "SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward",
    "url": "http://arxiv.org/abs/2504.16839v1",
    "arxiv_id": "2504.16839v1",
    "authors": [
      "Nicolas Jonason",
      "Luca Casini",
      "Bob L. T. Sturm"
    ],
    "published": "2025-04-23T16:02:25+00:00",
    "summary": "Recent work has proposed training machine learning models to predict aesthetic ratings for music audio. Our work explores whether such models can be used to finetune a symbolic music generation system with reinforcement learning, and what effect this has on the system outputs. To test this, we use group relative policy optimization to finetune a piano MIDI model with Meta Audiobox Aesthetics ratings of audio-rendered outputs as the reward. We find that this optimization has effects on multiple low-level features of the generated outputs, and improves the average subjective ratings in a preliminary listening study with $14$ participants. We also find that over-optimization dramatically reduces diversity of model outputs."
  },
  {
    "title": "Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation",
    "url": "http://arxiv.org/abs/2504.16782v1",
    "arxiv_id": "2504.16782v1",
    "authors": [
      "Tixiao Shan",
      "Abhinav Rajvanshi",
      "Niluthpol Mithun",
      "Han-Pang Chiu"
    ],
    "published": "2025-04-23T14:58:56+00:00",
    "summary": "We propose Graph2Nav, a real-time 3D object-relation graph generation framework, for autonomous navigation in the real world. Our framework fully generates and exploits both 3D objects and a rich set of semantic relationships among objects in a 3D layered scene graph, which is applicable to both indoor and outdoor scenes. It learns to generate 3D semantic relations among objects, by leveraging and advancing state-of-the-art 2D panoptic scene graph works into the 3D world via 3D semantic mapping techniques. This approach avoids previous training data constraints in learning 3D scene graphs directly from 3D data. We conduct experiments to validate the accuracy in locating 3D objects and labeling object-relations in our 3D scene graphs. We also evaluate the impact of Graph2Nav via integration with SayNav, a state-of-the-art planner based on large language models, on an unmanned ground robot to object search tasks in real environments. Our results demonstrate that modeling object relations in our scene graphs improves search efficiency in these navigation tasks."
  },
  {
    "title": "MEC Task Offloading in AIoT: A User-Centric DRL Model Splitting Inference Scheme",
    "url": "http://arxiv.org/abs/2504.16729v1",
    "arxiv_id": "2504.16729v1",
    "authors": [
      "Weixi Li",
      "Rongzuo Guo",
      "Yuning Wang",
      "Fangying Chen"
    ],
    "published": "2025-04-23T14:01:36+00:00",
    "summary": "With the rapid development of the Artificial Intelligence of Things (AIoT), mobile edge computing (MEC) becomes an essential technology underpinning AIoT applications. However, multi-angle resource constraints, multi-user task competition, and the complexity of task offloading decisions in dynamic MEC environments present new technical challenges. Therefore, a user-centric deep reinforcement learning (DRL) model splitting inference scheme is proposed to address the problem. This scheme combines model splitting inference technology and designs a UCMS_MADDPG-based offloading algorithm to realize efficient model splitting inference responses in the dynamic MEC environment with multi-angle resource constraints. Specifically, we formulate a joint optimization problem that integrates resource allocation, server selection, and task offloading, aiming to minimize the weighted sum of task execution delay and energy consumption. We also introduce a user-server co-selection algorithm to address the selection issue between users and servers. Furthermore, we design an algorithm centered on user pre-decision to coordinate the outputs of continuous and discrete hybrid decisions, and introduce a priority sampling mechanism based on reward-error trade-off to optimize the experience replay mechanism of the network. Simulation results show that the proposed UCMS_MADDPG-based offloading algorithm demonstrates superior overall performance compared with other benchmark algorithms in dynamic environments."
  },
  {
    "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation",
    "url": "http://arxiv.org/abs/2504.16693v1",
    "arxiv_id": "2504.16693v1",
    "authors": [
      "Wenxuan Li",
      "Hang Zhao",
      "Zhiyuan Yu",
      "Yu Du",
      "Qin Zou",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "published": "2025-04-23T13:27:07+00:00",
    "summary": "While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts."
  },
  {
    "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator",
    "url": "http://arxiv.org/abs/2504.16680v1",
    "arxiv_id": "2504.16680v1",
    "authors": [
      "Chenhao Li",
      "Andreas Krause",
      "Marco Hutter"
    ],
    "published": "2025-04-23T12:58:15+00:00",
    "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics."
  },
  {
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "url": "http://arxiv.org/abs/2504.16656v1",
    "arxiv_id": "2504.16656v1",
    "authors": [
      "Chris",
      "Yichen Wei",
      "Yi Peng",
      "Xiaokun Wang",
      "Weijie Qiu",
      "Wei Shen",
      "Tianyidan Xie",
      "Jiangbo Pei",
      "Jianhao Zhang",
      "Yunzhuo Hao",
      "Xuchen Song",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-04-23T12:24:10+00:00",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and 74.0 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B."
  },
  {
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "url": "http://arxiv.org/abs/2504.16656v2",
    "arxiv_id": "2504.16656v2",
    "authors": [
      "Chris",
      "Yichen Wei",
      "Yi Peng",
      "Xiaokun Wang",
      "Weijie Qiu",
      "Wei Shen",
      "Tianyidan Xie",
      "Jiangbo Pei",
      "Jianhao Zhang",
      "Yunzhuo Hao",
      "Xuchen Song",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-04-23T12:24:10+00:00",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that jointly leverages the Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization (GRPO), which harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we introduce the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in GRPO by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and 73.6 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B."
  },
  {
    "title": "Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models",
    "url": "http://arxiv.org/abs/2504.16635v1",
    "arxiv_id": "2504.16635v1",
    "authors": [
      "Fredy Pokou",
      "Jules Sadefo Kamdem",
      "Fran\u00e7ois Benhmad"
    ],
    "published": "2025-04-23T11:54:22+00:00",
    "summary": "In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management."
  },
  {
    "title": "HERB: Human-augmented Efficient Reinforcement learning for Bin-packing",
    "url": "http://arxiv.org/abs/2504.16595v1",
    "arxiv_id": "2504.16595v1",
    "authors": [
      "Gojko Perovic",
      "Nuno Ferreira Duarte",
      "Atabak Dehban",
      "Gon\u00e7alo Teixeira",
      "Egidio Falotico",
      "Jos\u00e9 Santos-Victor"
    ],
    "published": "2025-04-23T10:24:36+00:00",
    "summary": "Packing objects efficiently is a fundamental problem in logistics, warehouse automation, and robotics. While traditional packing solutions focus on geometric optimization, packing irregular, 3D objects presents significant challenges due to variations in shape and stability. Reinforcement Learning~(RL) has gained popularity in robotic packing tasks, but training purely from simulation can be inefficient and computationally expensive. In this work, we propose HERB, a human-augmented RL framework for packing irregular objects. We first leverage human demonstrations to learn the best sequence of objects to pack, incorporating latent factors such as space optimization, stability, and object relationships that are difficult to model explicitly. Next, we train a placement algorithm that uses visual information to determine the optimal object positioning inside a packing container. Our approach is validated through extensive performance evaluations, analyzing both packing efficiency and latency. Finally, we demonstrate the real-world feasibility of our method on a robotic system. Experimental results show that our method outperforms geometric and purely RL-based approaches by leveraging human intuition, improving both packing robustness and adaptability. This work highlights the potential of combining human expertise-driven RL to tackle complex real-world packing challenges in robotic systems."
  },
  {
    "title": "Algorithmic Pricing and Algorithmic Collusion",
    "url": "http://arxiv.org/abs/2504.16592v1",
    "arxiv_id": "2504.16592v1",
    "authors": [
      "Martin Bichler",
      "Julius Durmann",
      "Matthias Oberlechner"
    ],
    "published": "2025-04-23T10:18:57+00:00",
    "summary": "The rise of algorithmic pricing in online retail platforms has attracted significant interest in how autonomous software agents interact under competition. This article explores the potential emergence of algorithmic collusion - supra-competitive pricing outcomes that arise without explicit agreements - as a consequence of repeated interactions between learning agents. Most of the literature focuses on oligopoly pricing environments modeled as repeated Bertrand competitions, where firms use online learning algorithms to adapt prices over time. While experimental research has demonstrated that specific reinforcement learning algorithms can learn to maintain prices above competitive equilibrium levels in simulated environments, theoretical understanding of when and why such outcomes occur remains limited. This work highlights the interdisciplinary nature of this challenge, which connects computer science concepts of online learning with game-theoretical literature on equilibrium learning. We examine implications for the Business & Information Systems Engineering (BISE) community and identify specific research opportunities to address challenges of algorithmic competition in digital marketplaces."
  },
  {
    "title": "JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16591v1",
    "arxiv_id": "2504.16591v1",
    "authors": [
      "Tristan Kenneweg",
      "Philip Kenneweg",
      "Barbara Hammer"
    ],
    "published": "2025-04-23T10:16:12+00:00",
    "summary": "Joint-Embedding Predictive Architectures (JEPA) have recently become popular as promising architectures for self-supervised learning. Vision transformers have been trained using JEPA to produce embeddings from images and videos, which have been shown to be highly suitable for downstream tasks like classification and segmentation. In this paper, we show how to adapt the JEPA architecture to reinforcement learning from images. We discuss model collapse, show how to prevent it, and provide exemplary data on the classical Cart Pole task."
  },
  {
    "title": "Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows",
    "url": "http://arxiv.org/abs/2504.16588v1",
    "arxiv_id": "2504.16588v1",
    "authors": [
      "Defne E. Ozan",
      "Andrea N\u00f3voa",
      "Luca Magri"
    ],
    "published": "2025-04-23T10:12:53+00:00",
    "summary": "The goal of many applications in energy and transport sectors is to control turbulent flows. However, because of chaotic dynamics and high dimensionality, the control of turbulent flows is exceedingly difficult. Model-free reinforcement learning (RL) methods can discover optimal control policies by interacting with the environment, but they require full state information, which is often unavailable in experimental settings. We propose a data-assimilated model-based RL (DA-MBRL) framework for systems with partial observability and noisy measurements. Our framework employs a control-aware Echo State Network for data-driven prediction of the dynamics, and integrates data assimilation with an Ensemble Kalman Filter for real-time state estimation. An off-policy actor-critic algorithm is employed to learn optimal control strategies from state estimates. The framework is tested on the Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a spatiotemporally chaotic flow from noisy and partial measurements."
  },
  {
    "title": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression",
    "url": "http://arxiv.org/abs/2504.16574v1",
    "arxiv_id": "2504.16574v1",
    "authors": [
      "Lizhe Chen",
      "Binjia Zhou",
      "Yuyao Ge",
      "Jiayi Chen",
      "Shiguang NI"
    ],
    "published": "2025-04-23T09:53:01+00:00",
    "summary": "Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs."
  },
  {
    "title": "Private Federated Learning using Preference-Optimized Synthetic Data",
    "url": "http://arxiv.org/abs/2504.16438v1",
    "arxiv_id": "2504.16438v1",
    "authors": [
      "Charlie Hou",
      "Mei-Yu Wang",
      "Yige Zhu",
      "Daniel Lazar",
      "Giulia Fanti"
    ],
    "published": "2025-04-23T05:57:20+00:00",
    "summary": "In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri."
  },
  {
    "title": "Anytime Safe Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16417v1",
    "arxiv_id": "2504.16417v1",
    "authors": [
      "Pol Mestres",
      "Arnau Marzabal",
      "Jorge Cort\u00e9s"
    ],
    "published": "2025-04-23T04:51:31+00:00",
    "summary": "This paper considers the problem of solving constrained   reinforcement learning problems with anytime guarantees, meaning   that the algorithmic solution returns a safe policy regardless of   when it is terminated. Drawing inspiration from anytime constrained   optimization, we introduce Reinforcement Learning-based Safe   Gradient Flow (RL-SGF), an on-policy algorithm which employs   estimates of the value functions and their respective gradients   associated with the objective and safety constraints for the current   policy, and updates the policy parameters by solving a convex   quadratically constrained quadratic program. We show that if the   estimates are computed with a sufficiently large number of episodes   (for which we provide an explicit bound), safe policies are updated   to safe policies with a probability higher than a prescribed   tolerance. We also show that iterates asymptotically converge to a   neighborhood of a KKT point, whose size can be arbitrarily reduced   by refining the estimates of the value function and their gradients.   We illustrate the performance of RL-SGF in a navigation example."
  },
  {
    "title": "Natural Policy Gradient for Average Reward Non-Stationary RL",
    "url": "http://arxiv.org/abs/2504.16415v1",
    "arxiv_id": "2504.16415v1",
    "authors": [
      "Neharika Jali",
      "Eshika Pathak",
      "Pranay Sharma",
      "Guannan Qu",
      "Gauri Joshi"
    ],
    "published": "2025-04-23T04:37:26+00:00",
    "summary": "We consider the problem of non-stationary reinforcement learning (RL) in the infinite-horizon average-reward setting. We model it by a Markov Decision Process with time-varying rewards and transition probabilities, with a variation budget of $\\Delta_T$. Existing non-stationary RL algorithms focus on model-based and model-free value-based methods. Policy-based methods despite their flexibility in practice are not theoretically well understood in non-stationary RL. We propose and analyze the first model-free policy-based algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method with a restart based exploration for change and a novel interpretation of learning rates as adapting factors. Further, we present a bandit-over-RL based parameter-free algorithm BORL-NS-NAC that does not require prior knowledge of the variation budget $\\Delta_T$. We present a dynamic regret of $\\tilde{\\mathscr O}(|S|^{1/2}|A|^{1/2}\\Delta_T^{1/6}T^{5/6})$ for both algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of the state and action spaces. The regret analysis leverages a novel adaptation of the Lyapunov function analysis of NAC to dynamic environments and characterizes the effects of simultaneous updates in policy, value function estimate and changes in the environment."
  },
  {
    "title": "SplitReason: Learning To Offload Reasoning",
    "url": "http://arxiv.org/abs/2504.16379v1",
    "arxiv_id": "2504.16379v1",
    "authors": [
      "Yash Akhauri",
      "Anthony Fei",
      "Chi-Chih Chang",
      "Ahmed F. AbouElhamayed",
      "Yueying Li",
      "Mohamed S. Abdelfattah"
    ],
    "published": "2025-04-23T03:00:02+00:00",
    "summary": "Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs."
  },
  {
    "title": "Mining Software Repositories for Expert Recommendation",
    "url": "http://arxiv.org/abs/2504.16343v1",
    "arxiv_id": "2504.16343v1",
    "authors": [
      "Chad Marshall",
      "Andrew Barovic",
      "Armin Moin"
    ],
    "published": "2025-04-23T01:41:08+00:00",
    "summary": "We propose an automated approach to bug assignment to developers in large open-source software projects. This way, we assist human bug triagers who are in charge of finding the best developer with the right level of expertise in a particular area to be assigned to a newly reported issue. Our approach is based on the history of software development as documented in the issue tracking systems. We deploy BERTopic and techniques from TopicMiner. Our approach works based on the bug reports' features, such as the corresponding products and components, as well as their priority and severity levels. We sort developers based on their experience with specific combinations of new reports. The evaluation is performed using Top-k accuracy, and the results are compared with the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come from various Eclipse and Mozilla projects, such as JDT, Firefox, and Thunderbird."
  },
  {
    "title": "SLiM-Gym: Reinforcement Learning for Population Genetics",
    "url": "http://arxiv.org/abs/2504.16301v1",
    "arxiv_id": "2504.16301v1",
    "authors": [
      "Niko Zuppas",
      "Bryan C. Carstens"
    ],
    "published": "2025-04-22T22:41:35+00:00",
    "summary": "We introduce SLiM-Gym, a Python package for integrating reinforcement learning (RL) with forward-time population genetic simulations. Wright-Fisher evolutionary dynamics offer a tractable framework for modeling populations across discrete generations, yet applying RL to these systems requires a compatible training environment. SLiM-Gym connects the standardized RL interface provided by Gymnasium with the high-fidelity evolutionary simulations of SLiM, allowing agents to interact with evolving populations in real time. This framework enables the development and evaluation of RL-based strategies for understanding evolutionary processes."
  },
  {
    "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
    "url": "http://arxiv.org/abs/2504.16272v1",
    "arxiv_id": "2504.16272v1",
    "authors": [
      "Ryan Koo",
      "Ian Yang",
      "Vipul Raheja",
      "Mingyi Hong",
      "Kwang-Sung Jun",
      "Dongyeop Kang"
    ],
    "published": "2025-04-22T21:09:33+00:00",
    "summary": "Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward."
  },
  {
    "title": "Quantum reinforcement learning of classical rare dynamics: Enhancement by intrinsic Fourier features",
    "url": "http://arxiv.org/abs/2504.16258v1",
    "arxiv_id": "2504.16258v1",
    "authors": [
      "Alissa Wilms",
      "Laura Ohff",
      "Andrea Skolik",
      "Jens Eisert",
      "Sumeet Khatri",
      "David A. Reiss"
    ],
    "published": "2025-04-22T20:34:09+00:00",
    "summary": "Rare events are essential for understanding the behavior of non-equilibrium and industrial systems. It is of ongoing interest to develop methods for effectively searching for rare events. With the advent of quantum computing and its potential advantages over classical computing for applications like sampling certain probability distributions, the question arises whether quantum computers could also provide an advantage or inspire new methods for sampling the statistics of rare events. In this work, we propose a quantum reinforcement learning (QRL) method for studying rare dynamics, and we investigate their benefits over classical approaches based on neural networks. As a proof-of-concept example, we demonstrate that our QRL agents can learn and generate the rare dynamics of random walks, and we are able to explain this success as well as the different contributing factors to it via the intrinsic Fourier features of the parameterized quantum circuit. Furthermore, we show better learning behavior with fewer parameters compared to classical approaches. This is the first investigation of QRL applied to generating rare events and suggests that QRL is a promising method to study their dynamics and statistics."
  },
  {
    "title": "TTRL: Test-Time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.16084v1",
    "arxiv_id": "2504.16084v1",
    "authors": [
      "Yuxin Zuo",
      "Kaiyan Zhang",
      "Shang Qu",
      "Li Sheng",
      "Xuekai Zhu",
      "Biqing Qi",
      "Youbang Sun",
      "Ganqu Cui",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "published": "2025-04-22T17:59:56+00:00",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
  },
  {
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities",
    "url": "http://arxiv.org/abs/2504.16078v1",
    "arxiv_id": "2504.16078v1",
    "authors": [
      "Thomas Schmied",
      "J\u00f6rg Bornschein",
      "Jordi Grau-Moya",
      "Markus Wulfmeier",
      "Razvan Pascanu"
    ],
    "published": "2025-04-22T17:57:14+00:00",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various agentic applications. A key hypothesis is that LLMs, leveraging common sense and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently solve complex domains. However, LLM agents have been found to suffer from sub-optimal exploration and the knowing-doing gap, the inability to effectively act on knowledge present in the model. In this work, we systematically study why LLMs perform sub-optimally in decision-making scenarios. In particular, we closely examine three prevalent failure modes: greediness, frequency bias, and the knowing-doing gap. We propose mitigation of these shortcomings by fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales. Our experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making abilities of LLMs by increasing exploration and narrowing the knowing-doing gap. Finally, we study both classic exploration mechanisms, such as $\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and self-consistency, to enable more effective fine-tuning of LLMs for decision-making."
  },
  {
    "title": "Reinforcement Learning and Metaheuristics for Feynman Integral Reduction",
    "url": "http://arxiv.org/abs/2504.16045v1",
    "arxiv_id": "2504.16045v1",
    "authors": [
      "Mao Zeng"
    ],
    "published": "2025-04-22T17:14:05+00:00",
    "summary": "We propose new methods for optimizing the integration-by-parts (IBP) reduction of Feynman integrals, an important computational bottleneck in modern perturbative calculations in quantum field theory. Using the simple example of one-loop massive bubble integrals, we pose the problem of minimizing the number of arithmetic operations in reducing a target integral to master integrals via the Laporta algorithm. This is a nontrivial combinatorial optimization problem over the ordering of IBP equation generation (from pairs of seed integrals and IBP operators) and the ordering of integral elimination. Our first proposed method is reinforcement learning, which involves an agent interacting with an environment in a step-by-step manner and learning the best actions to take given an observation of the environment (in this case, the current state of the IBP reduction process). The second method is using metaheuristics, e.g. simulated annealing, to minimize the computational cost as a black-box function of numerical priority values that control the orderings. For large-scale problems, the number of free parameters can be compressed by using a small neural network to assign priority values. Remarkably, both methods lead to IBP reduction schemes that are more efficient than human-designed algorithms."
  },
  {
    "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer",
    "url": "http://arxiv.org/abs/2504.16020v1",
    "arxiv_id": "2504.16020v1",
    "authors": [
      "Soham Sane"
    ],
    "published": "2025-04-22T16:33:14+00:00",
    "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer addressing the memory overhead and hyperparameter complexity of adaptive methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2 gradient normalization followed by a smooth hyperbolic tangent transformation, $g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness parameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm formulation; (2) a formal non-convex convergence analysis guaranteeing stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN, TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent performance profile. While exhibiting instability in off-policy DQN, it provides enhanced training stability with competitive results in TD3 (requiring careful $\\alpha$ tuning) and achieves substantially superior performance in on-policy PPO. These results underscore the critical importance of empirical $\\alpha$ selection, revealing strong interactions between the optimizer's dynamics and the underlying RL algorithm. AlphaGrad presents a compelling alternative optimizer for memory-constrained scenarios and shows significant promise for on-policy learning regimes where its stability and efficiency advantages can be particularly impactful."
  },
  {
    "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer",
    "url": "http://arxiv.org/abs/2504.16020v2",
    "arxiv_id": "2504.16020v2",
    "authors": [
      "Soham Sane"
    ],
    "published": "2025-04-22T16:33:14+00:00",
    "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer addressing the memory overhead and hyperparameter complexity of adaptive methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2 gradient normalization followed by a smooth hyperbolic tangent transformation, $g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness parameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm formulation; (2) a formal non-convex convergence analysis guaranteeing stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN, TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent performance profile. While exhibiting instability in off-policy DQN, it provides enhanced training stability with competitive results in TD3 (requiring careful $\\alpha$ tuning) and achieves substantially superior performance in on-policy PPO. These results underscore the critical importance of empirical $\\alpha$ selection, revealing strong interactions between the optimizer's dynamics and the underlying RL algorithm. AlphaGrad presents a compelling alternative optimizer for memory-constrained scenarios and shows significant promise for on-policy learning regimes where its stability and efficiency advantages can be particularly impactful."
  },
  {
    "title": "The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information",
    "url": "http://arxiv.org/abs/2504.16010v1",
    "arxiv_id": "2504.16010v1",
    "authors": [
      "Tuong Manh Vu",
      "Ernesto Carrella",
      "Robert Axtell",
      "Omar A. Guerrero"
    ],
    "published": "2025-04-22T16:18:19+00:00",
    "summary": "We develop a model where firms determine the price at which they sell their differentiable goods, the volume that they produce, and the inputs (types and amounts) that they purchase from other firms. A steady-state production network emerges endogenously without resorting to assumptions such as equilibrium or perfect knowledge about production technologies. Through a simple version of reinforcement learning, firms with heterogeneous technologies cope with uncertainty and maximize profits. Due to this learning process, firms can adapt to shocks such as demand shifts, suppliers/clients closure, productivity changes, and production technology modifications; effectively reshaping the production network. To demonstrate the potential of this model, we analyze the upstream and downstream impact of demand and productivity shocks."
  },
  {
    "title": "Making Neural Networks More Suitable for Approximate Clifford+T Circuit Synthesis",
    "url": "http://arxiv.org/abs/2504.15990v1",
    "arxiv_id": "2504.15990v1",
    "authors": [
      "Mathias Weiden",
      "Justin Kalloor",
      "John Kubiatowicz",
      "Costin Iancu"
    ],
    "published": "2025-04-22T15:51:32+00:00",
    "summary": "Machine Learning with deep neural networks has transformed computational approaches to scientific and engineering problems. Central to many of these advancements are precisely tuned neural architectures that are tailored to the domains in which they are used. In this work, we develop deep learning techniques and architectural modifications that improve performance on reinforcement learning guided quantum circuit synthesis-the task of constructing a circuit that implements a given unitary matrix. First, we propose a global phase invariance operation which makes our architecture resilient to complex global phase shifts. Second, we demonstrate how augmenting data with small random unitary perturbations during training enables more robust learning. Finally, we show how encoding numerical data with techniques from image processing allow networks to better detect small but significant changes in data. Our work enables deep learning approaches to better synthesize quantum circuits that implement unitary matrices."
  },
  {
    "title": "Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems",
    "url": "http://arxiv.org/abs/2504.15984v1",
    "arxiv_id": "2504.15984v1",
    "authors": [
      "Lukas Gehrke",
      "Aleksandrs Koselevsk",
      "Marius Klug",
      "Klaus Gramann"
    ],
    "published": "2025-04-22T15:34:19+00:00",
    "summary": "Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion."
  },
  {
    "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15932v1",
    "arxiv_id": "2504.15932v1",
    "authors": [
      "Wang Lin",
      "Liyu Jia",
      "Wentao Hu",
      "Kaihang Pan",
      "Zhongqi Yue",
      "Wei Zhao",
      "Jingyuan Chen",
      "Fei Wu",
      "Hanwang Zhang"
    ],
    "published": "2025-04-22T14:20:59+00:00",
    "summary": "Despite recent progress in video generation, producing videos that adhere to physical laws remains a significant challenge. Traditional diffusion-based methods struggle to extrapolate to unseen physical conditions (eg, velocity) due to their reliance on data-driven approximations. To address this, we propose to integrate symbolic reasoning and reinforcement learning to enforce physical consistency in video generation. We first introduce the Diffusion Timestep Tokenizer (DDT), which learns discrete, recursive visual tokens by recovering visual attributes lost during the diffusion process. The recursive visual tokens enable symbolic reasoning by a large language model. Based on it, we propose the Phys-AR framework, which consists of two stages: The first stage uses supervised fine-tuning to transfer symbolic knowledge, while the second stage applies reinforcement learning to optimize the model's reasoning abilities through reward functions based on physical conditions. Our approach allows the model to dynamically adjust and improve the physical properties of generated videos, ensuring adherence to physical laws. Experimental results demonstrate that PhysAR can generate videos that are physically consistent."
  },
  {
    "title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation",
    "url": "http://arxiv.org/abs/2504.15930v1",
    "arxiv_id": "2504.15930v1",
    "authors": [
      "Yinmin Zhong",
      "Zili Zhang",
      "Xiaoniu Song",
      "Hanpeng Hu",
      "Chao Jin",
      "Bingyang Wu",
      "Nuo Chen",
      "Yukun Chen",
      "Yu Zhou",
      "Changyi Wan",
      "Hongyu Zhou",
      "Yimin Jiang",
      "Yibo Zhu",
      "Daxin Jiang"
    ],
    "published": "2025-04-22T14:19:06+00:00",
    "summary": "Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs). RL for LLMs involves two stages: generation and training. The LLM first generates samples online, which are then used to derive rewards for training. The conventional view holds that the colocated architecture, where the two stages share resources via temporal multiplexing, outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage. However, in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large-scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment.   StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation and achieves full overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling. Experiments show that StreamRL improves throughput by up to 2.66x compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33x in a heterogeneous, cross-datacenter setting."
  },
  {
    "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics",
    "url": "http://arxiv.org/abs/2504.15927v1",
    "arxiv_id": "2504.15927v1",
    "authors": [
      "Ling Cheng",
      "Jiashu Pu",
      "Ruicheng Liang",
      "Qian Shao",
      "Hezhe Qiao",
      "Feida Zhu"
    ],
    "published": "2025-04-22T14:17:15+00:00",
    "summary": "Semi-supervised community detection methods are widely used for identifying specific communities due to the label scarcity. Existing semi-supervised community detection methods typically involve two learning stages learning in both initial identification and subsequent adjustment, which often starts from an unreasonable community core candidate. Moreover, these methods encounter scalability issues because they depend on reinforcement learning and generative adversarial networks, leading to higher computational costs and restricting the selection of candidates. To address these limitations, we draw a parallel between crystallization kinetics and community detection to integrate the spontaneity of the annealing process into community detection. Specifically, we liken community detection to identifying a crystal subgrain (core) that expands into a complete grain (community) through a process similar to annealing. Based on this finding, we propose CLique ANNealing (CLANN), which applies kinetics concepts to community detection by integrating these principles into the optimization process to strengthen the consistency of the community core. Subsequently, a learning-free Transitive Annealer was employed to refine the first-stage candidates by merging neighboring cliques and repositioning the community core, enabling a spontaneous growth process that enhances scalability. Extensive experiments on \\textbf{43} different network settings demonstrate that CLANN outperforms state-of-the-art methods across multiple real-world datasets, showcasing its exceptional efficacy and efficiency in community detection."
  },
  {
    "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network",
    "url": "http://arxiv.org/abs/2504.15905v1",
    "arxiv_id": "2504.15905v1",
    "authors": [
      "Wenjing Xiao",
      "Chenglong Shi",
      "Miaojiang Chen",
      "Zhiquan Liu",
      "Min Chen",
      "H. Herbert Song"
    ],
    "published": "2025-04-22T13:45:13+00:00",
    "summary": "With the exponential growth of Internet of Things (IoT) devices, edge computing (EC) is gradually playing an important role in providing cost-effective services. However, existing approaches struggle to perform well in graph-structured scenarios where user data is correlated, such as traffic flow prediction and social relationship recommender systems. In particular, graph neural network (GNN)-based approaches lead to expensive server communication cost. To address this problem, we propose GraphEdge, an efficient GNN-based EC architecture. It considers the EC system of GNN tasks, where there are associations between users and it needs to take into account the task data of its neighbors when processing the tasks of a user. Specifically, the architecture first perceives the user topology and represents their data associations as a graph layout at each time step. Then the graph layout is optimized by calling our proposed hierarchical traversal graph cut algorithm (HiCut), which cuts the graph layout into multiple weakly associated subgraphs based on the aggregation characteristics of GNN, and the communication cost between different subgraphs during GNN inference is minimized. Finally, based on the optimized graph layout, our proposed deep reinforcement learning (DRL) based graph offloading algorithm (DRLGO) is executed to obtain the optimal offloading strategy for the tasks of users, the offloading strategy is subgraph-based, it tries to offload user tasks in a subgraph to the same edge server as possible while minimizing the task processing time and energy consumption of the EC system. Experimental results show the good effectiveness and dynamic adaptation of our proposed architecture and it also performs well even in dynamic scenarios."
  },
  {
    "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15900v1",
    "arxiv_id": "2504.15900v1",
    "authors": [
      "Cheng Wen",
      "Tingwei Guo",
      "Shuaijiang Zhao",
      "Wei Zou",
      "Xiangang Li"
    ],
    "published": "2025-04-22T13:41:26+00:00",
    "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding."
  },
  {
    "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation",
    "url": "http://arxiv.org/abs/2504.15876v1",
    "arxiv_id": "2504.15876v1",
    "authors": [
      "Qizhen Wu Lei Chen",
      "Kexin Liu",
      "Jinhu L\u00fc"
    ],
    "published": "2025-04-22T13:22:58+00:00",
    "summary": "In swarm robotics, confrontation scenarios, including strategic confrontations, require efficient decision-making that integrates discrete commands and continuous actions. Traditional task and motion planning methods separate decision-making into two layers, but their unidirectional structure fails to capture the interdependence between these layers, limiting adaptability in dynamic environments. Here, we propose a novel bidirectional approach based on hierarchical reinforcement learning, enabling dynamic interaction between the layers. This method effectively maps commands to task allocation and actions to path planning, while leveraging cross-training techniques to enhance learning across the hierarchical framework. Furthermore, we introduce a trajectory prediction model that bridges abstract task representations with actionable planning goals. In our experiments, it achieves over 80\\% in confrontation win rate and under 0.01 seconds in decision time, outperforming existing approaches. Demonstrations through large-scale tests and real-world robot experiments further emphasize the generalization capabilities and practical applicability of our method."
  },
  {
    "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation",
    "url": "http://arxiv.org/abs/2504.15876v2",
    "arxiv_id": "2504.15876v2",
    "authors": [
      "Qizhen Wu",
      "Lei Chen",
      "Kexin Liu",
      "Jinhu L\u00fc"
    ],
    "published": "2025-04-22T13:22:58+00:00",
    "summary": "In swarm robotics, confrontation scenarios, including strategic confrontations, require efficient decision-making that integrates discrete commands and continuous actions. Traditional task and motion planning methods separate decision-making into two layers, but their unidirectional structure fails to capture the interdependence between these layers, limiting adaptability in dynamic environments. Here, we propose a novel bidirectional approach based on hierarchical reinforcement learning, enabling dynamic interaction between the layers. This method effectively maps commands to task allocation and actions to path planning, while leveraging cross-training techniques to enhance learning across the hierarchical framework. Furthermore, we introduce a trajectory prediction model that bridges abstract task representations with actionable planning goals. In our experiments, it achieves over 80% in confrontation win rate and under 0.01 seconds in decision time, outperforming existing approaches. Demonstrations through large-scale tests and real-world robot experiments further emphasize the generalization capabilities and practical applicability of our method."
  },
  {
    "title": "Aerial Active STAR-RIS-assisted Satellite-Terrestrial Covert Communications",
    "url": "http://arxiv.org/abs/2504.16146v1",
    "arxiv_id": "2504.16146v1",
    "authors": [
      "Chuang Zhang",
      "Geng Sun",
      "Jiahui Li",
      "Jiacheng Wang",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Shiwen Mao",
      "Tony Q. S. Quek"
    ],
    "published": "2025-04-22T13:13:35+00:00",
    "summary": "An integration of satellites and terrestrial networks is crucial for enhancing performance of next generation communication systems. However, the networks are hindered by the long-distance path loss and security risks in dense urban environments. In this work, we propose a satellite-terrestrial covert communication system assisted by the aerial active simultaneous transmitting and reflecting reconfigurable intelligent surface (AASTAR-RIS) to improve the channel capacity while ensuring the transmission covertness. Specifically, we first derive the minimal detection error probability (DEP) under the worst condition that the Warden has perfect channel state information (CSI). Then, we formulate an AASTAR-RIS-assisted satellite-terrestrial covert communication optimization problem (ASCCOP) to maximize the sum of the fair channel capacity for all ground users while meeting the strict covert constraint, by jointly optimizing the trajectory and active beamforming of the AASTAR-RIS. Due to the challenges posed by the complex and high-dimensional state-action spaces as well as the need for efficient exploration in dynamic environments, we propose a generative deterministic policy gradient (GDPG) algorithm, which is a generative deep reinforcement learning (DRL) method to solve the ASCCOP. Concretely, the generative diffusion model (GDM) is utilized as the policy representation of the algorithm to enhance the exploration process by generating diverse and high-quality samples through a series of denoising steps. Moreover, we incorporate an action gradient mechanism to accomplish the policy improvement of the algorithm, which refines the better state-action pairs through the gradient ascent. Simulation results demonstrate that the proposed approach significantly outperforms important benchmarks."
  },
  {
    "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model",
    "url": "http://arxiv.org/abs/2504.15843v1",
    "arxiv_id": "2504.15843v1",
    "authors": [
      "Junshu Pan",
      "Wei Shen",
      "Shulin Huang",
      "Qiji Zhou",
      "Yue Zhang"
    ],
    "published": "2025-04-22T12:39:30+00:00",
    "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data."
  },
  {
    "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback",
    "url": "http://arxiv.org/abs/2504.15804v1",
    "arxiv_id": "2504.15804v1",
    "authors": [
      "Ning Wang",
      "Bingkun Yao",
      "Jie Zhou",
      "Yuchen Hu",
      "Xi Wang",
      "Nan Guan",
      "Zhe Jiang"
    ],
    "published": "2025-04-22T11:38:14+00:00",
    "summary": "Large language models (LLMs) have shown strong performance in Verilog generation from natural language description. However, ensuring the functional correctness of the generated code remains a significant challenge. This paper introduces a method that integrates verification insights from testbench into the training of Verilog generation LLMs, aligning the training with the fundamental goal of hardware design: functional correctness. The main obstacle in using LLMs for Verilog code generation is the lack of sufficient functional verification data, particularly testbenches paired with design specifications and code. To address this problem, we introduce an automatic testbench generation pipeline that decomposes the process and uses feedback from the Verilog compiler simulator (VCS) to reduce hallucination and ensure correctness. We then use the testbench to evaluate the generated codes and collect them for further training, where verification insights are introduced. Our method applies reinforcement learning (RL), specifically direct preference optimization (DPO), to align Verilog code generation with functional correctness by training preference pairs based on testbench outcomes. In evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2, and VerilogEval v2, our approach consistently outperforms state-of-the-art baselines in generating functionally correct Verilog code. We open source all training code, data, and models at https://anonymous.4open.science/r/VeriPrefer-E88B."
  },
  {
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents",
    "url": "http://arxiv.org/abs/2504.15785v1",
    "arxiv_id": "2504.15785v1",
    "authors": [
      "Siyu Zhou",
      "Tianyi Zhou",
      "Yijun Yang",
      "Guodong Long",
      "Deheng Ye",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "published": "2025-04-22T10:58:27+00:00",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free \"world alignment\" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations."
  },
  {
    "title": "Tina: Tiny Reasoning Models via LoRA",
    "url": "http://arxiv.org/abs/2504.15777v1",
    "arxiv_id": "2504.15777v1",
    "authors": [
      "Shangshang Wang",
      "Julian Asilis",
      "\u00d6mer Faruk Akg\u00fcl",
      "Enes Burak Bilgin",
      "Ollie Liu",
      "Willie Neiswanger"
    ],
    "published": "2025-04-22T10:38:00+00:00",
    "summary": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\\% reasoning performance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \\& checkpoints."
  },
  {
    "title": "CaRoSaC: A Reinforcement Learning-Based Kinematic Control of Cable-Driven Parallel Robots by Addressing Cable Sag through Simulation",
    "url": "http://arxiv.org/abs/2504.15740v1",
    "arxiv_id": "2504.15740v1",
    "authors": [
      "Rohit Dhakate",
      "Thomas Jantos",
      "Eren Allak",
      "Stephan Weiss",
      "Jan Steinbrener"
    ],
    "published": "2025-04-22T09:45:06+00:00",
    "summary": "This paper introduces the Cable Robot Simulation and Control (CaRoSaC) Framework, which integrates a simulation environment with a model-free reinforcement learning control methodology for suspended Cable-Driven Parallel Robots (CDPRs), accounting for cable sag. Our approach seeks to bridge the knowledge gap of the intricacies of CDPRs due to aspects such as cable sag and precision control necessities by establishing a simulation platform that captures the real-world behaviors of CDPRs, including the impacts of cable sag. The framework offers researchers and developers a tool to further develop estimation and control strategies within the simulation for understanding and predicting the performance nuances, especially in complex operations where cable sag can be significant. Using this simulation framework, we train a model-free control policy in Reinforcement Learning (RL). This approach is chosen for its capability to adaptively learn from the complex dynamics of CDPRs. The policy is trained to discern optimal cable control inputs, ensuring precise end-effector positioning. Unlike traditional feedback-based control methods, our RL control policy focuses on kinematic control and addresses the cable sag issues without being tethered to predefined mathematical models. We also demonstrate that our RL-based controller, coupled with the flexible cable simulation, significantly outperforms the classical kinematics approach, particularly in dynamic conditions and near the boundary regions of the workspace. The combined strength of the described simulation and control approach offers an effective solution in manipulating suspended CDPRs even at workspace boundary conditions where traditional approach fails, as proven from our experiments, ensuring that CDPRs function optimally in various applications while accounting for the often neglected but critical factor of cable sag."
  },
  {
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2504.15716v1",
    "arxiv_id": "2504.15716v1",
    "authors": [
      "Jie Zhu",
      "Qian Chen",
      "Huaixia Dou",
      "Junhui Li",
      "Lifan Guo",
      "Feng Chen",
      "Chi Zhang"
    ],
    "published": "2025-04-22T09:01:04+00:00",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications."
  },
  {
    "title": "Autonomous Control of Redundant Hydraulic Manipulator Using Reinforcement Learning with Action Feedback",
    "url": "http://arxiv.org/abs/2504.15714v1",
    "arxiv_id": "2504.15714v1",
    "authors": [
      "Rohit Dhakate",
      "Christian Brommer",
      "Christoph B\u00f6hm",
      "Stephan Weiss",
      "Jan Steinbrener"
    ],
    "published": "2025-04-22T08:55:28+00:00",
    "summary": "This article presents an entirely data-driven approach for autonomous control of redundant manipulators with hydraulic actuation. The approach only requires minimal system information, which is inherited from a simulation model. The non-linear hydraulic actuation dynamics are modeled using actuator networks from the data gathered during the manual operation of the manipulator to effectively emulate the real system in a simulation environment. A neural network control policy for autonomous control, based on end-effector (EE) position tracking is then learned using Reinforcement Learning (RL) with Ornstein-Uhlenbeck process noise (OUNoise) for efficient exploration. The RL agent also receives feedback based on supervised learning of the forward kinematics which facilitates selecting the best suitable action from exploration. The control policy directly provides the joint variables as outputs based on provided target EE position while taking into account the system dynamics. The joint variables are then mapped to the hydraulic valve commands, which are then fed to the system without further modifications. The proposed approach is implemented on a scaled hydraulic forwarder crane with three revolute and one prismatic joint to track the desired position of the EE in 3-Dimensional (3D) space. With the emulated dynamics and extensive learning in simulation, the results demonstrate the feasibility of deploying the learned controller directly on the real system."
  },
  {
    "title": "Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15679v1",
    "arxiv_id": "2504.15679v1",
    "authors": [
      "Brandon Panos",
      "Ivan Milic"
    ],
    "published": "2025-04-22T08:03:09+00:00",
    "summary": "We present a novel reinforcement learning (RL) approach for solving the classical 2-level atom non-LTE radiative transfer problem by framing it as a control task in which an RL agent learns a depth-dependent source function $S(\\tau)$ that self-consistently satisfies the equation of statistical equilibrium (SE). The agent's policy is optimized entirely via reward-based interactions with a radiative transfer engine, without explicit knowledge of the ground truth. This method bypasses the need for constructing approximate lambda operators ($\\Lambda^*$) common in accelerated iterative schemes. Additionally, it requires no extensive precomputed labeled datasets to extract a supervisory signal, and avoids backpropagating gradients through the complex RT solver itself. Finally, we show through experiment that a simple feedforward neural network trained greedily cannot solve for SE, possibly due to the moving target nature of the problem. Our $\\Lambda^*-\\text{Free}$ method offers potential advantages for complex scenarios (e.g., atmospheres with enhanced velocity fields, multi-dimensional geometries, or complex microphysics) where $\\Lambda^*$ construction or solver differentiability is challenging. Additionally, the agent can be incentivized to find more efficient policies by manipulating the discount factor, leading to a reprioritization of immediate rewards. If demonstrated to generalize past its training data, this RL framework could serve as an alternative or accelerated formalism to achieve SE. To the best of our knowledge, this study represents the first application of reinforcement learning in solar physics that directly solves for a fundamental physical constraint."
  },
  {
    "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers",
    "url": "http://arxiv.org/abs/2504.15634v1",
    "arxiv_id": "2504.15634v1",
    "authors": [
      "Peizheng Liu",
      "Hitoshi Iba"
    ],
    "published": "2025-04-22T06:53:36+00:00",
    "summary": "Transformer-based architectures have recently propelled advances in sequence modeling across domains, but their application to the hydrophobic-hydrophilic (H-P) model for protein folding remains relatively unexplored. In this work, we adapt a Deep Q-Network (DQN) integrated with attention mechanisms (Transformers) to address the 3D H-P protein folding problem. Our system formulates folding decisions as a self-avoiding walk in a reinforced environment, and employs a specialized reward function based on favorable hydrophobic interactions. To improve performance, the method incorporates validity check including symmetry-breaking constraints, dueling and double Q-learning, and prioritized replay to focus learning on critical transitions. Experimental evaluations on standard benchmark sequences demonstrate that our approach achieves several known best solutions for shorter sequences, and obtains near-optimal results for longer chains. This study underscores the promise of attention-based reinforcement learning for protein folding, and created a prototype of Transformer-based Q-network structure for 3-dimensional lattice models."
  },
  {
    "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization",
    "url": "http://arxiv.org/abs/2504.15619v1",
    "arxiv_id": "2504.15619v1",
    "authors": [
      "Jinda Lu",
      "Jinghan Li",
      "Yuan Gao",
      "Junkang Wu",
      "Jiancan Wu",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "published": "2025-04-22T06:19:38+00:00",
    "summary": "Preference alignment through Direct Preference Optimization (DPO) has demonstrated significant effectiveness in aligning multimodal large language models (MLLMs) with human preferences. However, existing methods focus primarily on language preferences while neglecting the critical visual context. In this paper, we propose an Adaptive Vision-enhanced Preference optimization (AdaViP) that addresses these limitations through two key innovations: (1) vision-based preference pair construction, which integrates multiple visual foundation models to strategically remove key visual elements from the image, enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference optimization that dynamically balances vision- and language-based preferences for more accurate alignment. Extensive evaluations across different benchmarks demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4% reductions in response-level and mentioned-level hallucination respectively on the Object HalBench, significantly outperforming current state-of-the-art methods."
  },
  {
    "title": "Research on Navigation Methods Based on LLMs",
    "url": "http://arxiv.org/abs/2504.15600v1",
    "arxiv_id": "2504.15600v1",
    "authors": [
      "Anlong Zhang",
      "Jianmin Ji"
    ],
    "published": "2025-04-22T05:40:59+00:00",
    "summary": "In recent years, the field of indoor navigation has witnessed groundbreaking advancements through the integration of Large Language Models (LLMs). Traditional navigation approaches relying on pre-built maps or reinforcement learning exhibit limitations such as poor generalization and limited adaptability to dynamic environments. In contrast, LLMs offer a novel paradigm for complex indoor navigation tasks by leveraging their exceptional semantic comprehension, reasoning capabilities, and zero-shot generalization properties. We propose an LLM-based navigation framework that leverages function calling capabilities, positioning the LLM as the central controller. Our methodology involves modular decomposition of conventional navigation functions into reusable LLM tools with expandable configurations. This is complemented by a systematically designed, transferable system prompt template and interaction workflow that can be easily adapted across different implementations. Experimental validation in PyBullet simulation environments across diverse scenarios demonstrates the substantial potential and effectiveness of our approach, particularly in achieving context-aware navigation through dynamic tool composition."
  },
  {
    "title": "Grasping Deformable Objects via Reinforcement Learning with Cross-Modal Attention to Visuo-Tactile Inputs",
    "url": "http://arxiv.org/abs/2504.15595v1",
    "arxiv_id": "2504.15595v1",
    "authors": [
      "Yonghyun Lee",
      "Sungeun Hong",
      "Min-gu Kim",
      "Gyeonghwan Kim",
      "Changjoo Nam"
    ],
    "published": "2025-04-22T05:22:31+00:00",
    "summary": "We consider the problem of grasping deformable objects with soft shells using a robotic gripper. Such objects have a center-of-mass that changes dynamically and are fragile so prone to burst. Thus, it is difficult for robots to generate appropriate control inputs not to drop or break the object while performing manipulation tasks. Multi-modal sensing data could help understand the grasping state through global information (e.g., shapes, pose) from visual data and local information around the contact (e.g., pressure) from tactile data. Although they have complementary information that can be beneficial to use together, fusing them is difficult owing to their different properties.   We propose a method based on deep reinforcement learning (DRL) that generates control inputs of a simple gripper from visuo-tactile sensing information. Our method employs a cross-modal attention module in the encoder network and trains it in a self-supervised manner using the loss function of the RL agent. With the multi-modal fusion, the proposed method can learn the representation for the DRL agent from the visuo-tactile sensory data. The experimental result shows that cross-modal attention is effective to outperform other early and late data fusion methods across different environments including unseen robot motions and objects."
  },
  {
    "title": "Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model",
    "url": "http://arxiv.org/abs/2504.15578v1",
    "arxiv_id": "2504.15578v1",
    "authors": [
      "Ian Mikesell",
      "Samuel Filgueira da Silva",
      "Mehmet Fatih Ozkan",
      "Faissal El Idrissi",
      "Prashanth Ramesh",
      "Marcello Canova"
    ],
    "published": "2025-04-22T04:25:50+00:00",
    "summary": "Accurately identifying the parameters of electrochemical models of li-ion battery (LiB) cells is a critical task for enhancing the fidelity and predictive ability. Traditional parameter identification methods often require extensive data collection experiments and lack adaptability in dynamic environments. This paper describes a Reinforcement Learning (RL) based approach that dynamically tailors the current profile applied to a LiB cell to optimize the parameters identifiability of the electrochemical model. The proposed framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup, which serves as a reliable testbed for evaluating the RL-based design strategy. The HIL validation confirms that the RL-based experimental design outperforms conventional test protocols used for parameter identification in terms of both reducing the modeling errors on a verification test and minimizing the duration of the experiment used for parameter identification."
  },
  {
    "title": "In-context Ranking Preference Optimization",
    "url": "http://arxiv.org/abs/2504.15477v1",
    "arxiv_id": "2504.15477v1",
    "authors": [
      "Junda Wu",
      "Rohan Surana",
      "Zhouhang Xie",
      "Yiran Shen",
      "Yu Xia",
      "Tong Yu",
      "Ryan A. Rossi",
      "Prithviraj Ammanabrolu",
      "Julian McAuley"
    ],
    "published": "2025-04-21T23:06:12+00:00",
    "summary": "Recent developments in Direct Preference Optimization (DPO) allow large language models (LLMs) to function as implicit ranking models by maximizing the margin between preferred and non-preferred responses. In practice, user feedback on such lists typically involves identifying a few relevant items in context rather than providing detailed pairwise comparisons for every possible item pair. Moreover, many complex information retrieval tasks, such as conversational agents and summarization systems, critically depend on ranking the highest-quality outputs at the top, emphasizing the need to support natural and flexible forms of user feedback. To address the challenge of limited and sparse pairwise feedback in the in-context setting, we propose an In-context Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs based on ranking lists constructed during inference. To further capture flexible forms of feedback, IRPO extends the DPO objective by incorporating both the relevance of items and their positions in the list. Modeling these aspects jointly is non-trivial, as ranking metrics are inherently discrete and non-differentiable, making direct optimization difficult. To overcome this, IRPO introduces a differentiable objective based on positional aggregation of pairwise item preferences, enabling effective gradient-based optimization of discrete ranking metrics. We further provide theoretical insights showing that IRPO (i) automatically emphasizes items with greater disagreement between the model and the reference ranking, and (ii) links its gradient to an importance sampling estimator, yielding an unbiased estimator with reduced variance. Empirical results show IRPO outperforms standard DPO approaches in ranking performance, highlighting its effectiveness in aligning LLMs with direct in-context ranking preferences."
  },
  {
    "title": "LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.15472v1",
    "arxiv_id": "2504.15472v1",
    "authors": [
      "Pingcheng Jian",
      "Xiao Wei",
      "Yanbaihui Liu",
      "Samuel A. Moore",
      "Michael M. Zavlanos",
      "Boyuan Chen"
    ],
    "published": "2025-04-21T22:46:29+00:00",
    "summary": "We introduce Large Language Model-Assisted Preference Prediction (LAPP), a novel framework for robot learning that enables efficient, customizable, and expressive behavior acquisition with minimum human effort. Unlike prior approaches that rely heavily on reward engineering, human demonstrations, motion capture, or expensive pairwise preference labels, LAPP leverages large language models (LLMs) to automatically generate preference labels from raw state-action trajectories collected during reinforcement learning (RL). These labels are used to train an online preference predictor, which in turn guides the policy optimization process toward satisfying high-level behavioral specifications provided by humans. Our key technical contribution is the integration of LLMs into the RL feedback loop through trajectory-level preference prediction, enabling robots to acquire complex skills including subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a diverse set of quadruped locomotion and dexterous manipulation tasks and show that it achieves efficient learning, higher final performance, faster adaptation, and precise control of high-level behaviors. Notably, LAPP enables robots to master highly dynamic and expressive tasks such as quadruped backflips, which remain out of reach for standard LLM-generated or handcrafted rewards. Our results highlight LAPP as a promising direction for scalable preference-driven robot learning."
  },
  {
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "url": "http://arxiv.org/abs/2504.15466v1",
    "arxiv_id": "2504.15466v1",
    "authors": [
      "Jiayi Pan",
      "Xiuyu Li",
      "Long Lian",
      "Charlie Snell",
      "Yifei Zhou",
      "Adam Yala",
      "Trevor Darrell",
      "Kurt Keutzer",
      "Alane Suhr"
    ],
    "published": "2025-04-21T22:29:02+00:00",
    "summary": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation."
  },
  {
    "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL",
    "url": "http://arxiv.org/abs/2504.15425v1",
    "arxiv_id": "2504.15425v1",
    "authors": [
      "Songyuan Zhang",
      "Oswin So",
      "Mitchell Black",
      "Zachary Serlin",
      "Chuchu Fan"
    ],
    "published": "2025-04-21T20:34:55+00:00",
    "summary": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods."
  },
  {
    "title": "Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking",
    "url": "http://arxiv.org/abs/2504.15414v1",
    "arxiv_id": "2504.15414v1",
    "authors": [
      "Dylan Khor",
      "Bowen Weng"
    ],
    "published": "2025-04-21T19:48:05+00:00",
    "summary": "Learning-based approaches, particularly reinforcement learning (RL), have become widely used for developing control policies for autonomous agents, such as locomotion policies for legged robots. RL training typically maximizes a predefined reward (or minimizes a corresponding cost/loss) by iteratively optimizing policies within a simulator. Starting from a randomly initialized policy, the empirical expected reward follows a trajectory with an overall increasing trend. While some policies become temporarily stuck in local optima, a well-defined training process generally converges to a reward level with noisy oscillations. However, selecting a policy for real-world deployment is rarely an analytical decision (i.e., simply choosing the one with the highest reward) and is instead often performed through trial and error. To improve sim-to-real transfer, most research focuses on the pre-convergence stage, employing techniques such as domain randomization, multi-fidelity training, adversarial training, and architectural innovations. However, these methods do not eliminate the inevitable convergence trajectory and noisy oscillations of rewards, leading to heuristic policy selection or cherry-picking. This paper addresses the post-convergence sim-to-real transfer problem by introducing a worst-case performance transference optimization approach, formulated as a convex quadratic-constrained linear programming problem. Extensive experiments demonstrate its effectiveness in transferring RL-based locomotion policies from simulation to real-world laboratory tests."
  },
  {
    "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models",
    "url": "http://arxiv.org/abs/2504.15279v1",
    "arxiv_id": "2504.15279v1",
    "authors": [
      "Weiye Xu",
      "Jiahao Wang",
      "Weiyun Wang",
      "Zhe Chen",
      "Wengang Zhou",
      "Aijun Yang",
      "Lewei Lu",
      "Houqiang Li",
      "Xiaohua Wang",
      "Xizhou Zhu",
      "Wenhai Wang",
      "Jifeng Dai",
      "Jinguo Zhu"
    ],
    "published": "2025-04-21T17:59:53+00:00",
    "summary": "Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress."
  },
  {
    "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning",
    "url": "http://arxiv.org/abs/2504.15275v1",
    "arxiv_id": "2504.15275v1",
    "authors": [
      "Jie Cheng",
      "Ruixi Qiao",
      "Lijun Li",
      "Chao Guo",
      "Junle Wang",
      "Gang Xiong",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "published": "2025-04-21T17:59:02+00:00",
    "summary": "Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at https://github.com/CJReinforce/PURE."
  },
  {
    "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
    "url": "http://arxiv.org/abs/2504.15257v1",
    "arxiv_id": "2504.15257v1",
    "authors": [
      "Hongcheng Gao",
      "Yue Liu",
      "Yufei He",
      "Longxu Dou",
      "Chao Du",
      "Zhijie Deng",
      "Bryan Hooi",
      "Min Lin",
      "Tianyu Pang"
    ],
    "published": "2025-04-21T17:35:42+00:00",
    "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner."
  },
  {
    "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.15241v1",
    "arxiv_id": "2504.15241v1",
    "authors": [
      "Yahan Yang",
      "Soham Dan",
      "Shuo Li",
      "Dan Roth",
      "Insup Lee"
    ],
    "published": "2025-04-21T17:15:06+00:00",
    "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation."
  },
  {
    "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
    "url": "http://arxiv.org/abs/2504.15217v1",
    "arxiv_id": "2504.15217v1",
    "authors": [
      "Yatong Bai",
      "Jonah Casebeer",
      "Somayeh Sojoudi",
      "Nicholas J. Bryan"
    ],
    "published": "2025-04-21T16:41:40+00:00",
    "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with a broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and a set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of a different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct a positive demonstration set and a negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including a custom music aesthetics model, CLAP score, Vendi diversity, and Frechet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits a new approach to designing and optimizing reward functions for improving human-perceived quality. Sound examples at https://ml-dragon.github.io/web."
  },
  {
    "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs",
    "url": "http://arxiv.org/abs/2504.15210v1",
    "arxiv_id": "2504.15210v1",
    "authors": [
      "Marina Sakharova",
      "Abhinav Anand",
      "Mira Mezini"
    ],
    "published": "2025-04-21T16:29:07+00:00",
    "summary": "Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark."
  },
  {
    "title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution",
    "url": "http://arxiv.org/abs/2504.15176v1",
    "arxiv_id": "2504.15176v1",
    "authors": [
      "Miaomiao Cai",
      "Simiao Li",
      "Wei Li",
      "Xudong Huang",
      "Hanting Chen",
      "Jie Hu",
      "Yunhe Wang"
    ],
    "published": "2025-04-21T15:35:48+00:00",
    "summary": "Recent advances in diffusion models have improved Real-World Image Super-Resolution (Real-ISR), but existing methods lack human feedback integration, risking misalignment with human preference and may leading to artifacts, hallucinations and harmful content generation. To this end, we are the first to introduce human preference alignment into Real-ISR, a technique that has been successfully applied in Large Language Models and Text-to-Image tasks to effectively enhance the alignment of generated outputs with human preferences. Specifically, we introduce Direct Preference Optimization (DPO) into Real-ISR to achieve alignment, where DPO serves as a general alignment technique that directly learns from the human preference dataset. Nevertheless, unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR are difficult to reconcile with the image-level preferences of DPO, which can lead to the DPO being overly sensitive to local anomalies, leading to reduced generation quality. To resolve this dichotomy, we propose Direct Semantic Preference Optimization (DSPO) to align instance-level human preferences by incorporating semantic guidance, which is through two strategies: (a) semantic instance alignment strategy, implementing instance-level alignment to ensure fine-grained perceptual consistency, and (b) user description feedback strategy, mitigating hallucinations through semantic textual feedback on instance-level images. As a plug-and-play solution, DSPO proves highly effective in both one-step and multi-step SR frameworks."
  },
  {
    "title": "Beyond Binary Opinions: A Deep Reinforcement Learning-Based Approach to Uncertainty-Aware Competitive Influence Maximization",
    "url": "http://arxiv.org/abs/2504.15131v1",
    "arxiv_id": "2504.15131v1",
    "authors": [
      "Qi Zhang",
      "Dian Chen",
      "Lance M. Kaplan",
      "Audun J\u00f8sang",
      "Dong Hyun Jeong",
      "Feng Chen",
      "Jin-Hee Cho"
    ],
    "published": "2025-04-21T14:26:04+00:00",
    "summary": "The Competitive Influence Maximization (CIM) problem involves multiple entities competing for influence in online social networks (OSNs). While Deep Reinforcement Learning (DRL) has shown promise, existing methods often assume users' opinions are binary and ignore their behavior and prior knowledge. We propose DRIM, a multi-dimensional uncertainty-aware DRL-based CIM framework that leverages Subjective Logic (SL) to model uncertainty in user opinions, preferences, and DRL decision-making. DRIM introduces an Uncertainty-based Opinion Model (UOM) for a more realistic representation of user uncertainty and optimizes seed selection for propagating true information while countering false information. In addition, it quantifies uncertainty in balancing exploration and exploitation. Results show that UOM significantly enhances true information spread and maintains influence against advanced false information strategies. DRIM-based CIM schemes outperform state-of-the-art methods by up to 57% and 88% in influence while being up to 48% and 77% faster. Sensitivity analysis indicates that higher network observability and greater information propagation boost performance, while high network activity mitigates the effect of users' initial biases."
  },
  {
    "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment",
    "url": "http://arxiv.org/abs/2504.15129v1",
    "arxiv_id": "2504.15129v1",
    "authors": [
      "Kangyao Huang",
      "Hao Wang",
      "Yu Luo",
      "Jingyu Chen",
      "Jintao Chen",
      "Xiangkui Zhang",
      "Xiangyang Ji",
      "Huaping Liu"
    ],
    "published": "2025-04-21T14:25:23+00:00",
    "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor environments is an exciting task. Quadrotors operating in real-world environments by learning-based methods encounter several challenges: a large amount of simulator generated data required for training, strict demands for real-time processing onboard, and the sim-to-real gap caused by dynamic and noisy conditions. Current works have made a great breakthrough in applying learning-based methods to end-to-end control of quadrotors, but rarely mention the infrastructure system training from scratch and deploying to reality, which makes it difficult to reproduce methods and applications. To bridge this gap, we propose a platform that enables the seamless transfer of end-to-end deep reinforcement learning (DRL) policies. We integrate the training environment, flight dynamics control, DRL algorithms, the MAVROS middleware stack, and hardware into a comprehensive workflow and architecture that enables quadrotors' policies to be trained from scratch to real-world deployment in several minutes. Our platform provides rich types of environments including hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and planning in unknown environments, as a physical experiment benchmark. Through extensive empirical validation, we demonstrate the efficiency of proposed sim-to-real platform, and robust outdoor flight performance under real-world perturbations. Details can be found from our website https://emnavi.tech/AirGym/."
  },
  {
    "title": "Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions",
    "url": "http://arxiv.org/abs/2504.15327v1",
    "arxiv_id": "2504.15327v1",
    "authors": [
      "Tianliang Yao",
      "Bo Lu",
      "Markus Kowarschik",
      "Yixuan Yuan",
      "Hubin Zhao",
      "Sebastien Ourselin",
      "Kaspar Althoefer",
      "Junbo Ge",
      "Peng Qi"
    ],
    "published": "2025-04-21T13:49:30+00:00",
    "summary": "Endovascular procedures have revolutionized the treatment of vascular diseases thanks to minimally invasive solutions that significantly reduce patient recovery time and enhance clinical outcomes. However, the precision and dexterity required during these procedures poses considerable challenges for interventionists. Robotic systems have emerged offering transformative solutions, addressing issues such as operator fatigue, radiation exposure, and the inherent limitations of human precision. The integration of Embodied Intelligence (EI) into these systems signifies a paradigm shift, enabling robots to navigate complex vascular networks and adapt to dynamic physiological conditions. Data-driven approaches, advanced computer vision, medical image analysis, and machine learning techniques, are at the forefront of this evolution. These methods augment procedural intelligence by facilitating real-time vessel segmentation, device tracking, and anatomical landmark detection. Reinforcement learning and imitation learning further refine navigation strategies and replicate experts' techniques. This review systematically examines the integration of EI principles into robotic technologies, in relation to endovascular procedures. We discuss recent advancements in intelligent perception and data-driven control, and their practical applications in robot-assisted endovascular procedures. By critically evaluating current limitations and emerging opportunities, this review establishes a framework for future developments, emphasizing the potential for greater autonomy and improved clinical outcomes. Emerging trends and specific areas of research, such as federated learning for medical data sharing, explainable AI for clinical decision support, and advanced human-robot collaboration paradigms, are also explored, offering insights into the future direction of this rapidly evolving field."
  },
  {
    "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN",
    "url": "http://arxiv.org/abs/2504.15099v1",
    "arxiv_id": "2504.15099v1",
    "authors": [
      "Lin Wang",
      "Xiancheng Wang",
      "Rui Wang",
      "Zhibo Zhang",
      "Minghang Zhao"
    ],
    "published": "2025-04-21T13:41:09+00:00",
    "summary": "Up to now, the training processes of typical Generative Adversarial Networks (GANs) are still particularly sensitive to data properties and hyperparameters, which may lead to severe oscillations, difficulties in convergence, or even failures to converge, especially when the overall variances of the training sets are large. These phenomena are often attributed to the training characteristics of such networks. Aiming at the problem, this paper develops a new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which employs reinforcement learning in the training process of GANs to make training easier. Specifically, this paper allows the training step size to be controlled by an agent to improve training stability, and makes the training process more intelligent with variable learning rates, making GANs less sensitive to step size. Experiments have been conducted on three benchmark datasets to verify the effectiveness of the developed FSCO."
  },
  {
    "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
    "url": "http://arxiv.org/abs/2504.15077v1",
    "arxiv_id": "2504.15077v1",
    "authors": [
      "Simone Papicchio",
      "Simone Rossi",
      "Luca Cagliero",
      "Paolo Papotti"
    ],
    "published": "2025-04-21T13:05:26+00:00",
    "summary": "Large Language Models (LLMs) have shown impressive capabilities in transforming natural language questions about relational databases into SQL queries. Despite recent improvements, small LLMs struggle to handle questions involving multiple tables and complex SQL patterns under a Zero-Shot Learning (ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge deficits in pretrained models but falls short while dealing with queries involving multi-hop reasoning. To bridge this gap, different LLM training strategies to reinforce reasoning capabilities have been proposed, ranging from leveraging a thinking process within ZSL, including reasoning traces in SFT, or adopt Reinforcement Learning (RL) strategies. However, the influence of reasoning on Text2SQL performance is still largely unexplored. This paper investigates to what extent LLM reasoning capabilities influence their Text2SQL performance on four benchmark datasets. To this end, it considers the following LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT, with and without task-specific reasoning traces; (3) RL, leveraging execution accuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that combines SFT and RL. The results show that general-purpose reasoning under ZSL proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit from SFT with reasoning much more than larger ones, bridging the gap of their (weaker) model pretraining. RL is generally beneficial across all tested models and datasets, particularly when SQL queries involve multi-hop reasoning and multiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks to a strategic balance between generality of the reasoning process and optimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5 model performs on par with 100+ Billion ones on the Bird dataset."
  },
  {
    "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing",
    "url": "http://arxiv.org/abs/2504.15049v1",
    "arxiv_id": "2504.15049v1",
    "authors": [
      "Mohamed el amine Boudjoghra",
      "Ivan Laptev",
      "Angela Dai"
    ],
    "published": "2025-04-21T12:12:43+00:00",
    "summary": "With the fast pace of 3D capture technology and resulting abundance of 3D data, effective 3D scene editing becomes essential for a variety of graphics applications. In this work we present ScanEdit, an instruction-driven method for functional editing of complex, real-world 3D scans. To model large and interdependent sets of ob- jectswe propose a hierarchically-guided approach. Given a 3D scan decomposed into its object instances, we first construct a hierarchical scene graph representation to enable effective, tractable editing. We then leverage reason- ing capabilities of Large Language Models (LLMs) and translate high-level language instructions into actionable commands applied hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based guidance with ex- plicit physical constraints and generates realistic scenes where object arrangements obey both physics and common sense. In our extensive experimental evaluation ScanEdit outperforms state of the art and demonstrates excellent re- sults for a variety of real-world scenes and input instruc- tions."
  },
  {
    "title": "Energy-Efficient UAV-Mounted RIS for IoT: A Hybrid Energy Harvesting and DRL Approach",
    "url": "http://arxiv.org/abs/2504.15043v1",
    "arxiv_id": "2504.15043v1",
    "authors": [
      "Mahmoud M. Salim",
      "Khaled M. Rabie",
      "Ali H. Muqaibel"
    ],
    "published": "2025-04-21T11:54:40+00:00",
    "summary": "Many future Internet of Things (IoT) applications are expected to rely heavily on reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicles (UAVs). However, the endurance of such systems is constrained by the limited onboard energy, where frequent recharging or battery replacements are required. This consequently disrupts continuous operation and may be impractical in disaster scenarios. To address this challenge, we explore a dual energy harvesting (EH) framework that integrates time-switching (TS), power-splitting (PS), and element-splitting (ES) EH protocols for radio frequency energy, along with solar energy as a renewable source. First, we present the proposed system architecture and EH operating protocols, introducing the proposed hybrid ES-TS-PS EH strategy to extend UAV-mounted RIS endurance. Next, we outline key application scenarios and the associated design challenges. After that, a deep reinforcement learning-based framework is introduced to maximize the EH efficiency by jointly optimizing UAV trajectory, RIS phase shifts, and EH strategies. The framework considers dual EH, hardware impairments, and channel state information imperfections to reflect real-world deployment conditions. The optimization problem is formulated as a Markov decision process and solved using an enhanced deep deterministic policy gradient algorithm, incorporating clipped double Q-learning and softmax-based Q-value estimation for improved stability and efficiency. The results demonstrate significant performance gains compared to the considered baseline approaches. Finally, possible challenges and open research directions are presented, highlighting the transformative potential of energy-efficient UAV-mounted RIS networks for IoT systems."
  },
  {
    "title": "Energy-Efficient Irregular RIS-aided UAV-Assisted Optimization: A Deep Reinforcement Learning Approach",
    "url": "http://arxiv.org/abs/2504.15031v1",
    "arxiv_id": "2504.15031v1",
    "authors": [
      "Mahmoud M. Salim",
      "Khaled M. Rabie",
      "Ali H. Muqaibel"
    ],
    "published": "2025-04-21T11:40:33+00:00",
    "summary": "Reconfigurable intelligent surfaces (RISs) enhance unmanned aerial vehicles (UAV)-assisted communication by extending coverage, improving efficiency, and enabling adaptive beamforming. This paper investigates a multiple-input single-output system where a base station (BS) communicates with multiple single-antenna users through a UAV-assisted RIS, dynamically adapting to user mobility to maintain seamless connectivity. To extend UAV-RIS operational time, we propose a hybrid energy-harvesting resource allocation (HERA) strategy that leverages the irregular RIS ON/OFF capability while adapting to BS-RIS and RIS-user channels. The HERA strategy dynamically allocates resources by integrating non-linear radio frequency energy harvesting (EH) based on the time-switching (TS) approach and renewable energy as a complementary source. A non-convex mixed-integer nonlinear programming problem is formulated to maximize EH efficiency while satisfying quality-of-service, power, and energy constraints under channel state information and hardware impairments. The optimization jointly considers BS transmit power, RIS phase shifts, TS factor, and RIS element selection as decision variables. To solve this problem, we introduce the energy-efficient deep deterministic policy gradient (EE-DDPG) algorithm. This deep reinforcement learning (DRL)-based approach integrates action clipping and softmax-weighted Q-value estimation to mitigate estimation errors. Simulation results demonstrate that the proposed HERA method significantly improves EH efficiency, reaching up to 81.5\\% and 73.2\\% in single-user and multi-user scenarios, respectively, contributing to extended UAV operational time. Additionally, the proposed EE-DDPG model outperforms existing DRL algorithms while maintaining practical computational complexity."
  },
  {
    "title": "Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14989v1",
    "arxiv_id": "2504.14989v1",
    "authors": [
      "Dongjie Zhu",
      "Zhuo Yang",
      "Tianhang Wu",
      "Luzhou Ge",
      "Xuesong Li",
      "Qi Liu",
      "Xiang Li"
    ],
    "published": "2025-04-21T09:38:38+00:00",
    "summary": "Advancing the dynamic loco-manipulation capabilities of quadruped robots in complex terrains is crucial for performing diverse tasks. Specifically, dynamic ball manipulation in rugged environments presents two key challenges. The first is coordinating distinct motion modalities to integrate terrain traversal and ball control seamlessly. The second is overcoming sparse rewards in end-to-end deep reinforcement learning, which impedes efficient policy convergence. To address these challenges, we propose a hierarchical reinforcement learning framework. A high-level policy, informed by proprioceptive data and ball position, adaptively switches between pre-trained low-level skills such as ball dribbling and rough terrain navigation. We further propose Dynamic Skill-Focused Policy Optimization to suppress gradients from inactive skills and enhance critical skill learning. Both simulation and real-world experiments validate that our methods outperform baseline approaches in dynamic ball manipulation across rugged terrains, highlighting its effectiveness in challenging environments. Videos are on our website: dribble-hrl.github.io."
  },
  {
    "title": "Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling",
    "url": "http://arxiv.org/abs/2504.14946v1",
    "arxiv_id": "2504.14946v1",
    "authors": [
      "Tin Ping Chan",
      "Yunlong Cheng",
      "Yizhan Zhu",
      "Xiaofeng Gao",
      "Guihai Chen"
    ],
    "published": "2025-04-21T08:09:40+00:00",
    "summary": "As cloud computing continues to evolve, the adoption of multi-NUMA (Non-Uniform Memory Access) architecture by cloud service providers has introduced new challenges in virtual machine (VM) scheduling. To address these challenges and more accurately reflect the complexities faced by modern cloud environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM (DVAMP). We formally define both offline and online versions of DVAMP as mixed-integer linear programming problems, providing a rigorous mathematical foundation for analysis. A tight performance bound for greedy online algorithms is derived, offering insights into the worst-case optimality gap as a function of the number of physical machines and VM lifetime variability. To address the challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture for Multi-NUMA Environments), a novel deep reinforcement learning approach that exploits the problem's inherent symmetries. SPANE produces invariant results under arbitrary permutations of physical machine states, enhancing learning efficiency and solution quality. Extensive experiments conducted on the Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines, reducing average VM wait time by 45%. Our work contributes to the field of cloud resource management by providing both theoretical insights and practical solutions for VM scheduling in multi-NUMA environments, addressing a critical gap in the literature and offering improved performance for real-world cloud systems."
  },
  {
    "title": "Learning to Reason under Off-Policy Guidance",
    "url": "http://arxiv.org/abs/2504.14945v1",
    "arxiv_id": "2504.14945v1",
    "authors": [
      "Jianhao Yan",
      "Yafu Li",
      "Zican Hu",
      "Zhi Wang",
      "Ganqu Cui",
      "Xiaoye Qu",
      "Yu Cheng",
      "Yue Zhang"
    ],
    "published": "2025-04-21T08:09:13+00:00",
    "summary": "Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance."
  },
  {
    "title": "Learning to Reason under Off-Policy Guidance",
    "url": "http://arxiv.org/abs/2504.14945v2",
    "arxiv_id": "2504.14945v2",
    "authors": [
      "Jianhao Yan",
      "Yafu Li",
      "Zican Hu",
      "Zhi Wang",
      "Ganqu Cui",
      "Xiaoye Qu",
      "Yu Cheng",
      "Yue Zhang"
    ],
    "published": "2025-04-21T08:09:13+00:00",
    "summary": "Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance."
  },
  {
    "title": "Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative System for Underwater Tasks in Extreme Sea Conditions",
    "url": "http://arxiv.org/abs/2504.14894v1",
    "arxiv_id": "2504.14894v1",
    "authors": [
      "Jingzehua Xu",
      "Guanwen Xie",
      "Jiwei Tang",
      "Yimian Ding",
      "Weiyi Liu",
      "Shuai Zhang",
      "Yi Li"
    ],
    "published": "2025-04-21T06:47:46+00:00",
    "summary": "This paper develops a novel unmanned surface vehicle (USV)-autonomous underwater vehicle (AUV) collaborative system designed to enhance underwater task performance in extreme sea conditions. The system integrates a dual strategy: (1) high-precision multi-AUV localization enabled by Fisher information matrix-optimized USV path planning, and (2) reinforcement learning-based cooperative planning and control method for multi-AUV task execution. Extensive experimental evaluations in the underwater data collection task demonstrate the system's operational feasibility, with quantitative results showing significant performance improvements over baseline methods. The proposed system exhibits robust coordination capabilities between USV and AUVs while maintaining stability in extreme sea conditions. To facilitate reproducibility and community advancement, we provide an open-source simulation toolkit available at: https://github.com/360ZMEM/USV-AUV-colab ."
  },
  {
    "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14870v1",
    "arxiv_id": "2504.14870v1",
    "authors": [
      "Hongru Wang",
      "Cheng Qian",
      "Wanjun Zhong",
      "Xiusi Chen",
      "Jiahao Qiu",
      "Shijue Huang",
      "Bowen Jin",
      "Mengdi Wang",
      "Kam-Fai Wong",
      "Heng Ji"
    ],
    "published": "2025-04-21T05:40:05+00:00",
    "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR."
  },
  {
    "title": "Establishing Reliability Metrics for Reward Models in Large Language Models",
    "url": "http://arxiv.org/abs/2504.14838v1",
    "arxiv_id": "2504.14838v1",
    "authors": [
      "Yizhou Chen",
      "Yawen Liu",
      "Xuesi Wang",
      "Qingtao Yu",
      "Guangda Huzhang",
      "Anxiang Zeng",
      "Han Yu",
      "Zhiming Zhou"
    ],
    "published": "2025-04-21T03:39:33+00:00",
    "summary": "The reward model (RM) that represents human preferences plays a crucial role in optimizing the outputs of large language models (LLMs), e.g., through reinforcement learning from human feedback (RLHF) or rejection sampling. However, a long challenge for RM is its uncertain reliability, i.e., LLM outputs with higher rewards may not align with actual human preferences. Currently, there is a lack of a convincing metric to quantify the reliability of RMs. To bridge this gap, we propose the \\textit{\\underline{R}eliable at \\underline{$\\eta$}} (RETA) metric, which directly measures the reliability of an RM by evaluating the average quality (scored by an oracle) of the top $\\eta$ quantile responses assessed by an RM. On top of RETA, we present an integrated benchmarking pipeline that allows anyone to evaluate their own RM without incurring additional Oracle labeling costs. Extensive experimental studies demonstrate the superior stability of RETA metric, providing solid evaluations of the reliability of various publicly available and proprietary RMs. When dealing with an unreliable RM, we can use the RETA metric to identify the optimal quantile from which to select the responses."
  },
  {
    "title": "An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV: A Value of Information Perspective",
    "url": "http://arxiv.org/abs/2504.14824v1",
    "arxiv_id": "2504.14824v1",
    "authors": [
      "Wei Wang",
      "Nan Cheng",
      "Conghao Zhou",
      "Haixia Peng",
      "Haibo Zhou",
      "Zhou Su",
      "Xuemin",
      "Shen"
    ],
    "published": "2025-04-21T03:00:04+00:00",
    "summary": "The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information\" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks."
  },
  {
    "title": "Accelerating Visual Reinforcement Learning with Separate Primitive Policy for Peg-in-Hole Tasks",
    "url": "http://arxiv.org/abs/2504.14820v1",
    "arxiv_id": "2504.14820v1",
    "authors": [
      "Zichun Xu",
      "Zhaomin Wang",
      "Yuntao Li",
      "Lei Zhuang",
      "Zhiyuan Zhao",
      "Guocai Yang",
      "Jingdong Zhao"
    ],
    "published": "2025-04-21T02:53:17+00:00",
    "summary": "For peg-in-hole tasks, humans rely on binocular visual perception to locate the peg above the hole surface and then proceed with insertion. This paper draws insights from this behavior to enable agents to learn efficient assembly strategies through visual reinforcement learning. Hence, we propose a Separate Primitive Policy (S2P) to simultaneously learn how to derive location and insertion actions. S2P is compatible with model-free reinforcement learning algorithms. Ten insertion tasks featuring different polygons are developed as benchmarks for evaluations. Simulation experiments show that S2P can boost the sample efficiency and success rate even with force constraints. Real-world experiments are also performed to verify the feasibility of S2P. Ablations are finally given to discuss the generalizability of S2P and some factors that affect its performance."
  },
  {
    "title": "Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment",
    "url": "http://arxiv.org/abs/2504.14805v1",
    "arxiv_id": "2504.14805v1",
    "authors": [
      "Jinwoo Choi",
      "Seung-Woo Seo"
    ],
    "published": "2025-04-21T02:11:39+00:00",
    "summary": "Reinforcement learning (RL) has made significant progress in various domains, but scaling it to long-horizon tasks with complex decision-making remains challenging. Skill learning attempts to address this by abstracting actions into higher-level behaviors. However, current approaches often fail to recognize semantically similar behaviors as the same skill and use fixed skill lengths, limiting flexibility and generalization. To address this, we propose Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines skill representation and learning. DCSL introduces three key ideas: state-transition based skill representation, skill similarity function learning, and dynamic skill length adjustment. By focusing on state transitions and leveraging contrastive learning, DCSL effectively captures the semantic context of behaviors and adapts skill lengths to match the appropriate temporal extent of behaviors. Our approach enables more flexible and adaptive skill extraction, particularly in complex or noisy datasets, and demonstrates competitive performance compared to existing methods in task completion and efficiency."
  },
  {
    "title": "PPO-EPO: Energy and Performance Optimization for O-RAN Using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14749v1",
    "arxiv_id": "2504.14749v1",
    "authors": [
      "Rawlings Ntassah",
      "Gian Michele Dell'Aera",
      "Fabrizio Granelli"
    ],
    "published": "2025-04-20T21:42:25+00:00",
    "summary": "Energy consumption in mobile communication networks has become a significant challenge due to its direct impact on Capital Expenditure (CAPEX) and Operational Expenditure (OPEX). The introduction of Open RAN (O-RAN) enables telecommunication providers to leverage network intelligence to optimize energy efficiency while maintaining Quality of Service (QoS). One promising approach involves traffic-aware cell shutdown strategies, where underutilized cells are selectively deactivated without compromising overall network performance. However, achieving this balance requires precise traffic steering mechanisms that account for throughput performance, power efficiency, and network interference constraints.   This work proposes a reinforcement learning (RL) model based on the Proximal Policy Optimization (PPO) algorithm to optimize traffic steering and energy efficiency. The objective is to maximize energy efficiency and performance gains while strategically shutting down underutilized cells. The proposed RL model learns adaptive policies to make optimal shutdown decisions by considering throughput degradation constraints, interference thresholds, and PRB utilization balance. Experimental validation using TeraVM Viavi RIC tester data demonstrates that our method significantly improves the network's energy efficiency and downlink throughput."
  },
  {
    "title": "Interference-Aware PMI selection for MIMO systems in an O-RAN scenario",
    "url": "http://arxiv.org/abs/2504.14745v1",
    "arxiv_id": "2504.14745v1",
    "authors": [
      "Rawlings Ntassah",
      "Gian Michele Dell'Aera",
      "Fabrizio Granelli"
    ],
    "published": "2025-04-20T21:33:07+00:00",
    "summary": "The optimization of Precoding Matrix Indicators (PMIs) is crucial for enhancing the performance of 5G networks, particularly in dense deployments where inter-cell interference is a significant challenge. Some approaches have leveraged AI/ML techniques for beamforming and beam selection, however, these methods often overlook the multi-objective nature of PMI selection, which requires balancing spectral efficiency (SE) and interference reduction. This paper proposes an interference-aware PMI selection method using an Advantage Actor-Critic (A2C) reinforcement learning model, designed for deployment within an O-RAN framework as an xApp. The proposed model prioritizes user equipment (UE) based on a novel strategy and adjusts PMI values accordingly, with interference management and efficient resource utilization. Experimental results in an O-RAN environment demonstrate the approach's effectiveness in improving network performance metrics, including SE and interference mitigation."
  },
  {
    "title": "Reinforcement Learning from Multi-level and Episodic Human Feedback",
    "url": "http://arxiv.org/abs/2504.14732v1",
    "arxiv_id": "2504.14732v1",
    "authors": [
      "Muhammad Qasim Elahi",
      "Somtochukwu Oguchienti",
      "Maheed H. Ahmed",
      "Mahsa Ghasemi"
    ],
    "published": "2025-04-20T20:09:19+00:00",
    "summary": "Designing an effective reward function has long been a challenge in reinforcement learning, particularly for complex tasks in unstructured environments. To address this, various learning paradigms have emerged that leverage different forms of human input to specify or refine the reward function. Reinforcement learning from human feedback is a prominent approach that utilizes human comparative feedback, expressed as a preference for one behavior over another, to tackle this problem. In contrast to comparative feedback, we explore multi-level human feedback, which is provided in the form of a score at the end of each episode. This type of feedback offers more coarse but informative signals about the underlying reward function than binary feedback. Additionally, it can handle non-Markovian rewards, as it is based on the evaluation of an entire episode. We propose an algorithm to efficiently learn both the reward function and the optimal policy from this form of feedback. Moreover, we show that the proposed algorithm achieves sublinear regret and demonstrate its empirical effectiveness through extensive simulations."
  },
  {
    "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation",
    "url": "http://arxiv.org/abs/2504.14716v1",
    "arxiv_id": "2504.14716v1",
    "authors": [
      "Tuhina Tripathi",
      "Manya Wadhwa",
      "Greg Durrett",
      "Scott Niekum"
    ],
    "published": "2025-04-20T19:05:59+00:00",
    "summary": "Large Language Models (LLMs) are widely used as proxies for human labelers in both training (Reinforcement Learning from AI Feedback) and large-scale response evaluation (LLM-as-a-judge). Alignment and evaluation are critical components in the development of reliable LLMs, and the choice of feedback protocol plays a central role in both but remains understudied. In this work, we show that the choice of feedback protocol (absolute scores versus relative preferences) can significantly affect evaluation reliability and induce systematic biases. In particular, we show that pairwise evaluation protocols are more vulnerable to distracted evaluation. Generator models can exploit spurious attributes (or distractor features) favored by the LLM judge, resulting in inflated scores for lower-quality outputs and misleading training signals. We find that absolute scoring is more robust to such manipulation, producing judgments that better reflect response quality and are less influenced by distractor features. Our results demonstrate that generator models can flip preferences by embedding distractor features, skewing LLM-as-a-judge comparisons and leading to inaccurate conclusions about model quality in benchmark evaluations. Pairwise preferences flip in about 35% of the cases, compared to only 9% for absolute scores. We offer recommendations for choosing feedback protocols based on dataset characteristics and evaluation objectives."
  },
  {
    "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline",
    "url": "http://arxiv.org/abs/2504.14709v1",
    "arxiv_id": "2504.14709v1",
    "authors": [
      "Hui Zhou",
      "Shaoshuai Shi",
      "Hongsheng Li"
    ],
    "published": "2025-04-20T18:51:26+00:00",
    "summary": "Machine learning (ML)-based planners have recently gained significant attention. They offer advantages over traditional optimization-based planning algorithms. These advantages include fewer manually selected parameters and faster development. Within ML-based planning, imitation learning (IL) is a common algorithm. It primarily learns driving policies directly from supervised trajectory data. While IL has demonstrated strong performance on many open-loop benchmarks, it remains challenging to determine if the learned policy truly understands fundamental driving principles, rather than simply extrapolating from the ego-vehicle's initial state. Several studies have identified this limitation and proposed algorithms to address it. However, these methods often use original datasets for evaluation. In these datasets, future trajectories are heavily dependent on initial conditions. Furthermore, IL often overfits to the most common scenarios. It struggles to generalize to rare or unseen situations.   To address these challenges, this work proposes: 1) a novel closed-loop simulator supporting both imitation and reinforcement learning, 2) a causal benchmark derived from the Waymo Open Dataset to rigorously assess the impact of the copycat problem, and 3) a novel framework integrating imitation learning and reinforcement learning to overcome the limitations of purely imitative approaches. The code for this work will be released soon."
  },
  {
    "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14645v1",
    "arxiv_id": "2504.14645v1",
    "authors": [
      "Philipp Altmann",
      "C\u00e9line Davignon",
      "Maximilian Zorn",
      "Fabian Ritz",
      "Claudia Linnhoff-Popien",
      "Thomas Gabor"
    ],
    "published": "2025-04-20T15:01:19+00:00",
    "summary": "We employ an evolutionary optimization framework that perturbs initial states to generate informative and diverse policy demonstrations. A joint surrogate fitness function guides the optimization by combining local diversity, behavioral certainty, and global population diversity. To assess demonstration quality, we apply a set of evaluation metrics, including the reward-based optimality gap, fidelity interquartile means (IQMs), fitness composition analysis, and trajectory visualizations. Hyperparameter sensitivity is also examined to better understand the dynamics of trajectory optimization. Our findings demonstrate that optimizing trajectory selection via surrogate fitness metrics significantly improves interpretability of RL policies in both discrete and continuous environments. In gridworld domains, evaluations reveal significantly enhanced demonstration fidelities compared to random and ablated baselines. In continuous control, the proposed framework offers valuable insights, particularly for early-stage policies, while fidelity-based optimization proves more effective for mature policies. By refining and systematically analyzing surrogate fitness functions, this study advances the interpretability of RL models. The proposed improvements provide deeper insights into RL decision-making, benefiting applications in safety-critical and explainability-focused domains."
  },
  {
    "title": "Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension",
    "url": "http://arxiv.org/abs/2504.14642v1",
    "arxiv_id": "2504.14642v1",
    "authors": [
      "Lin Li",
      "Wei Chen",
      "Jiahui Li",
      "Long Chen"
    ],
    "published": "2025-04-20T14:50:49+00:00",
    "summary": "Recent advances in multi-modal large language models (MLLMs) have significantly improved object-level grounding and region captioning, but remain limited in visual relation understanding (\\eg, scene graph generation), particularly in modeling \\textit{N}-ary relationships that identify multiple semantic roles among an action event. Such a lack of \\textit{semantic dependencies} modeling among multi-entities leads to unreliable outputs, intensifying MLLMs' hallucinations and over-reliance on language priors. To this end, we propose Relation-R1, the first unified relational comprehension framework that explicitly integrates cognitive chain-of-thought (CoT)-guided Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we first establish foundational reasoning capabilities via SFT, enforcing structured outputs with thinking processes. Then, GRPO is utilized to refine these outputs via multi-reward optimization, prioritizing visual-semantic grounding over language-induced biases, thereby improving generalization capability. Extensive experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1 achieves state-of-the-art performance in both binary and \\textit{N}-ary relation understanding."
  },
  {
    "title": "AlphaZero-Edu: Making AlphaZero Accessible to Everyone",
    "url": "http://arxiv.org/abs/2504.14636v1",
    "arxiv_id": "2504.14636v1",
    "authors": [
      "Binjie Guo",
      "Hanyu Zheng",
      "Guowei Su",
      "Ru Zhang",
      "Haohan Jiang",
      "Xurong Lin",
      "Hongyan Wei",
      "Aisheng Mo",
      "Jie Li",
      "Zhiyuan Qian",
      "Zhuhao Zhang",
      "Xiaoyuan Cheng"
    ],
    "published": "2025-04-20T14:29:39+00:00",
    "summary": "Recent years have witnessed significant progress in reinforcement learning, especially with Zero-like paradigms, which have greatly boosted the generalization and reasoning abilities of large-scale language models. Nevertheless, existing frameworks are often plagued by high implementation complexity and poor reproducibility. To tackle these challenges, we present AlphaZero-Edu, a lightweight, education-focused implementation built upon the mathematical framework of AlphaZero. It boasts a modular architecture that disentangles key components, enabling transparent visualization of the algorithmic processes. Additionally, it is optimized for resource-efficient training on a single NVIDIA RTX 3090 GPU and features highly parallelized self-play data generation, achieving a 3.2-fold speedup with 8 processes. In Gomoku matches, the framework has demonstrated exceptional performance, achieving a consistently high win rate against human opponents. AlphaZero-Edu has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu, providing an accessible and practical benchmark for both academic research and industrial applications."
  },
  {
    "title": "Generative Auto-Bidding with Value-Guided Explorations",
    "url": "http://arxiv.org/abs/2504.14587v1",
    "arxiv_id": "2504.14587v1",
    "authors": [
      "Jingtong Gao",
      "Yewen Li",
      "Shuai Mao",
      "Peng Jiang",
      "Nan Jiang",
      "Yejing Wang",
      "Qingpeng Cai",
      "Fei Pan",
      "Peng Jiang",
      "Kun Gai",
      "Bo An",
      "Xiangyu Zhao"
    ],
    "published": "2025-04-20T12:28:49+00:00",
    "summary": "Auto-bidding, with its strong capability to optimize bidding decisions within dynamic and competitive online environments, has become a pivotal strategy for advertising platforms. Existing approaches typically employ rule-based strategies or Reinforcement Learning (RL) techniques. However, rule-based strategies lack the flexibility to adapt to time-varying market conditions, and RL-based methods struggle to capture essential historical dependencies and observations within Markov Decision Process (MDP) frameworks. Furthermore, these approaches often face challenges in ensuring strategy adaptability across diverse advertising objectives. Additionally, as offline training methods are increasingly adopted to facilitate the deployment and maintenance of stable online strategies, the issues of documented behavioral patterns and behavioral collapse resulting from training on fixed offline datasets become increasingly significant. To address these limitations, this paper introduces a novel offline Generative Auto-bidding framework with Value-Guided Explorations (GAVE). GAVE accommodates various advertising objectives through a score-based Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration mechanism with an RTG-based evaluation method to explore novel actions while ensuring stability-preserving updates. A learnable value function is also designed to guide the direction of action exploration and mitigate Out-of-Distribution (OOD) problems. Experimental results on two offline datasets and real-world deployments demonstrate that GAVE outperforms state-of-the-art baselines in both offline evaluations and online A/B tests. The implementation code is publicly available to facilitate reproducibility and further research."
  },
  {
    "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
    "url": "http://arxiv.org/abs/2504.14556v1",
    "arxiv_id": "2504.14556v1",
    "authors": [
      "Yousef Emami",
      "Hao Gao",
      "SeyedSina Nabavirazani",
      "Luis Almeida"
    ],
    "published": "2025-04-20T10:05:07+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various private and commercial applications, e.g. traffic control, package delivery, and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sample efficiency, which conflict with the urgency of emergencies such as SAR operations. This paper proposes In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as an alternative to DRL in emergencies. The UAV collects and transmits logged sensory data, to an LLM, to generate a task description in natural language, from which it obtains a data collection schedule to be executed by the UAV. The system continuously adapts by adding feedback to task descriptions and utilizing feedback for future decisions. This method is tested against jailbreaking attacks, where task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative packet loss by approximately 56\\%. ICLDC presents a promising direction for intelligent scheduling and control in UAV-assisted data collection."
  },
  {
    "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey",
    "url": "http://arxiv.org/abs/2504.14520v1",
    "arxiv_id": "2504.14520v1",
    "authors": [
      "Ahsan Bilal",
      "Muhammad Ahmed Mohsin",
      "Muhammad Umer",
      "Muhammad Awais Khan Bangash",
      "Muhammad Ali Jamshed"
    ],
    "published": "2025-04-20T07:34:26+00:00",
    "summary": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed."
  },
  {
    "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering",
    "url": "http://arxiv.org/abs/2504.14493v1",
    "arxiv_id": "2504.14493v1",
    "authors": [
      "Xinyu Wang",
      "Jijun Chi",
      "Zhenghan Tai",
      "Tung Sum Thomas Kwok",
      "Muzhi Li",
      "Zhuhong Li",
      "Hailin He",
      "Yuchen Hua",
      "Peng Lu",
      "Suyuchen Wang",
      "Yihong Wu",
      "Jerry Huang",
      "Ling Zhou"
    ],
    "published": "2025-04-20T04:58:14+00:00",
    "summary": "Leveraging large language models in real-world settings often entails a need to utilize domain-specific data and tools in order to follow the complex regulations that need to be followed for acceptable use. Within financial sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation (RAG) systems to address complex compliance requirements in financial document workflows. However, existing solutions struggle to account for the inherent heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of regulatory standards used in financial filings, leading to compromised accuracy in critical information extraction. We propose the FinSage framework as a solution, utilizing a multi-aspect RAG framework tailored for regulatory compliance analysis in multi-modal financial documents. FinSage introduces three innovative components: (1) a multi-modal pre-processing pipeline that unifies diverse data formats and generates chunk-level metadata summaries, (2) a multi-path sparse-dense retrieval system augmented with query expansion (HyDE) and metadata-aware semantic search, and (3) a domain-specialized re-ranking module fine-tuned via Direct Preference Optimization (DPO) to prioritize compliance-critical content. Extensive experiments demonstrate that FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions derived from surpasses the best baseline method on the FinanceBench question answering datasets by 24.06% in accuracy. Moreover, FinSage has been successfully deployed as financial question-answering agent in online meetings, where it has already served more than 1,200 people."
  },
  {
    "title": "SG-Reg: Generalizable and Efficient Scene Graph Registration",
    "url": "http://arxiv.org/abs/2504.14440v1",
    "arxiv_id": "2504.14440v1",
    "authors": [
      "Chuhao Liu",
      "Zhijian Qiao",
      "Jieqi Shi",
      "Ke Wang",
      "Peize Liu",
      "Shaojie Shen"
    ],
    "published": "2025-04-20T01:22:40+00:00",
    "summary": "This paper addresses the challenges of registering two rigid semantic scene graphs, an essential capability when an autonomous agent needs to register its map against a remote agent, or against a prior map. The hand-crafted descriptors in classical semantic-aided registration, or the ground-truth annotation reliance in learning-based scene graph registration, impede their application in practical real-world environments. To address the challenges, we design a scene graph network to encode multiple modalities of semantic nodes: open-set semantic feature, local topology with spatial awareness, and shape feature. These modalities are fused to create compact semantic node features. The matching layers then search for correspondences in a coarse-to-fine manner. In the back-end, we employ a robust pose estimator to decide transformation according to the correspondences. We manage to maintain a sparse and hierarchical scene representation. Our approach demands fewer GPU resources and fewer communication bandwidth in multi-agent tasks. Moreover, we design a new data generation approach using vision foundation models and a semantic mapping module to reconstruct semantic scene graphs. It differs significantly from previous works, which rely on ground-truth semantic annotations to generate data. We validate our method in a two-agent SLAM benchmark. It significantly outperforms the hand-crafted baseline in terms of registration success rate. Compared to visual loop closure networks, our method achieves a slightly higher registration recall while requiring only 52 KB of communication bandwidth for each query frame. Code available at: \\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}."
  },
  {
    "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling",
    "url": "http://arxiv.org/abs/2504.14439v1",
    "arxiv_id": "2504.14439v1",
    "authors": [
      "Avinandan Bose",
      "Zhihan Xiong",
      "Yuejie Chi",
      "Simon Shaolei Du",
      "Lin Xiao",
      "Maryam Fazel"
    ],
    "published": "2025-04-20T01:16:24+00:00",
    "summary": "Personalizing large language models (LLMs) to accommodate diverse user preferences is essential for enhancing alignment and user satisfaction. Traditional reinforcement learning from human feedback (RLHF) approaches often rely on monolithic value representations, limiting their ability to adapt to individual preferences. We introduce a novel framework that leverages low-rank preference modeling to efficiently learn and generalize user-specific reward functions. By representing reward functions in a low-dimensional subspace and modeling individual preferences as weighted combinations of shared basis functions, our approach avoids rigid user categorization while enabling scalability and few-shot adaptation. We validate our method on multiple preference datasets, demonstrating superior generalization to unseen users and improved accuracy in preference prediction tasks."
  },
  {
    "title": "Application of Deep Reinforcement Learning for Intrusion Detection in Internet of Things: A Systematic Review",
    "url": "http://arxiv.org/abs/2504.14436v1",
    "arxiv_id": "2504.14436v1",
    "authors": [
      "Saeid Jamshidia",
      "Amin Nikanjama",
      "Kawser Wazed Nafia",
      "Foutse Khomha",
      "Rasoul Rastab"
    ],
    "published": "2025-04-20T00:55:58+00:00",
    "summary": "The Internet of Things (IoT) has significantly expanded the digital landscape, interconnecting an unprecedented array of devices, from home appliances to industrial equipment. This growth enhances functionality, e.g., automation, remote monitoring, and control, and introduces substantial security challenges, especially in defending these devices against cyber threats. Intrusion Detection Systems (IDS) are crucial for securing IoT; however, traditional IDS often struggle to adapt to IoT networks' dynamic and evolving nature and threat patterns. A potential solution is using Deep Reinforcement Learning (DRL) to enhance IDS adaptability, enabling them to learn from and react to their operational environment dynamically. This systematic review examines the application of DRL to enhance IDS in IoT settings, covering research from the past ten years. This review underscores the state-of-the-art DRL techniques employed to improve adaptive threat detection and real-time security across IoT domains by analyzing various studies. Our findings demonstrate that DRL significantly enhances IDS capabilities by enabling systems to learn and adapt from their operational environment. This adaptability allows IDS to improve threat detection accuracy and minimize false positives, making it more effective in identifying genuine threats while reducing unnecessary alerts. Additionally, this systematic review identifies critical research gaps and future research directions, emphasizing the necessity for more diverse datasets, enhanced reproducibility, and improved integration with emerging IoT technologies. This review aims to foster the development of dynamic and adaptive IDS solutions essential for protecting IoT networks against sophisticated cyber threats."
  },
  {
    "title": "Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14422v1",
    "arxiv_id": "2504.14422v1",
    "authors": [
      "Paul Fischer",
      "Sebastian Kaltenbach",
      "Sergey Litvinov",
      "Sauro Succi",
      "Petros Koumoutsakos"
    ],
    "published": "2025-04-19T23:31:29+00:00",
    "summary": "The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone."
  },
  {
    "title": "Unlearning Works Better Than You Think: Local Reinforcement-Based Selection of Auxiliary Objectives",
    "url": "http://arxiv.org/abs/2504.14418v1",
    "arxiv_id": "2504.14418v1",
    "authors": [
      "Abderrahim Bendahi",
      "Adrien Fradin",
      "Matthieu Lerasle"
    ],
    "published": "2025-04-19T23:00:24+00:00",
    "summary": "We introduce Local Reinforcement-Based Selection of Auxiliary Objectives (LRSAO), a novel approach that selects auxiliary objectives using reinforcement learning (RL) to support the optimization process of an evolutionary algorithm (EA) as in EA+RL framework and furthermore incorporates the ability to unlearn previously used objectives. By modifying the reward mechanism to penalize moves that do no increase the fitness value and relying on the local auxiliary objectives, LRSAO dynamically adapts its selection strategy to optimize performance according to the landscape and unlearn previous objectives when necessary.   We analyze and evaluate LRSAO on the black-box complexity version of the non-monotonic Jump function, with gap parameter $\\ell$, where each auxiliary objective is beneficial at specific stages of optimization. The Jump function is hard to optimize for evolutionary-based algorithms and the best-known complexity for reinforcement-based selection on Jump was $O(n^2 \\log(n) / \\ell)$. Our approach improves over this result to achieve a complexity of $\\Theta(n^2 / \\ell^2 + n \\log(n))$ resulting in a significant improvement, which demonstrates the efficiency and adaptability of LRSAO, highlighting its potential to outperform traditional methods in complex optimization scenarios."
  },
  {
    "title": "Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment",
    "url": "http://arxiv.org/abs/2504.14412v1",
    "arxiv_id": "2504.14412v1",
    "authors": [
      "Benjamin M. Peter",
      "Mert Korkali"
    ],
    "published": "2025-04-19T21:59:05+00:00",
    "summary": "The increasingly challenging task of maintaining power grid security requires innovative solutions. Novel approaches using reinforcement learning (RL) agents have been proposed to help grid operators navigate the massive decision space and nonlinear behavior of these complex networks. However, applying RL to power grid security assessment, specifically for combinatorially troublesome contingency analysis problems, has proven difficult to scale. The integration of quantum computing into these RL frameworks helps scale by improving computational efficiency and boosting agent proficiency by leveraging quantum advantages in action exploration and model-based interdependence. To demonstrate a proof-of-concept use of quantum computing for RL agent training and simulation, we propose a hybrid agent that runs on quantum hardware using IBM's Qiskit Runtime. We also provide detailed insight into the construction of parameterized quantum circuits (PQCs) for generating relevant quantum output. This agent's proficiency at maintaining grid stability is demonstrated relative to a benchmark model without quantum enhancement using N-k contingency analysis. Additionally, we offer a comparative assessment of the training procedures for RL models integrated with a quantum backend."
  },
  {
    "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay",
    "url": "http://arxiv.org/abs/2504.14363v1",
    "arxiv_id": "2504.14363v1",
    "authors": [
      "Shihan Dou",
      "Muling Wu",
      "Jingwen Xu",
      "Rui Zheng",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "published": "2025-04-19T17:40:04+00:00",
    "summary": "Reinforcement learning (RL) has increasingly become a pivotal technique in the post-training of large language models (LLMs). The effective exploration of the output space is essential for the success of RL. We observe that for complex problems, during the early stages of training, the model exhibits strong exploratory capabilities and can identify promising solution ideas. However, its limited capability at this stage prevents it from successfully solving these problems. The early suppression of these potentially valuable solution ideas by the policy gradient hinders the model's ability to revisit and re-explore these ideas later. Consequently, although the LLM's capabilities improve in the later stages of training, it still struggles to effectively address these complex problems. To address this exploration issue, we propose a novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL), which introduces a dynamic replay mechanism throughout the training process. RRL enables the model to revisit promising states identified in the early stages, thereby improving its efficiency and effectiveness in exploration. To evaluate the effectiveness of RRL, we conduct extensive experiments on complex reasoning tasks, including mathematical reasoning and code generation, and general dialogue tasks. The results indicate that RRL maintains high exploration efficiency throughout the training period, significantly enhancing the effectiveness of RL in optimizing LLMs for complicated reasoning tasks. Moreover, it also improves the performance of RLHF, making the model both safer and more helpful."
  },
  {
    "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2504.14290v1",
    "arxiv_id": "2504.14290v1",
    "authors": [
      "Shouwei Ruan",
      "Zhenyu Wu",
      "Yao Huang",
      "Ruochen Zhang",
      "Yitong Sun",
      "Caixin Kang",
      "Xingxing Wei"
    ],
    "published": "2025-04-19T13:26:46+00:00",
    "summary": "Ensuring the safety of generated content remains a fundamental challenge for Text-to-Image (T2I) generation. Existing studies either fail to guarantee complete safety under potentially harmful concepts or struggle to balance safety with generation quality. To address these issues, we propose Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework for safety alignment in T2I models. SC-DPO integrates safety constraints into the general human preference calibration, aiming to maximize the likelihood of generating human-preferred samples while minimizing the safety cost of the generated outputs. In SC-DPO, we introduce a safety cost model to accurately quantify harmful levels for images, and train it effectively using the proposed contrastive learning and cost anchoring objectives. To apply SC-DPO for effective T2I safety alignment, we constructed SCP-10K, a safety-constrained preference dataset containing rich harmful concepts, which blends safety-constrained preference pairs under both harmful and clean instructions, further mitigating the trade-off between safety and sample quality. Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO, promoting the model's learning of difficult preference pair samples. Extensive experiments demonstrate that SC-DPO outperforms existing methods, effectively defending against various NSFW content while maintaining optimal sample quality and human preference alignment. Additionally, SC-DPO exhibits resilience against adversarial prompts designed to generate harmful content."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v1",
    "arxiv_id": "2504.14286v1",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, dedicating to offer valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM",
    "url": "http://arxiv.org/abs/2504.14286v2",
    "arxiv_id": "2504.14286v2",
    "authors": [
      "Xiaojiang Zhang",
      "Jinghui Wang",
      "Zifei Cheng",
      "Wenhao Zhuang",
      "Zheng Lin",
      "Minglei Zhang",
      "Shaojie Wang",
      "Yinghan Cui",
      "Chao Wang",
      "Junyi Peng",
      "Shimiao Jiang",
      "Shiqi Kuang",
      "Shouyu Yin",
      "Chaohang Wen",
      "Haotian Zhang",
      "Bin Chen",
      "Bing Yu"
    ],
    "published": "2025-04-19T13:06:03+00:00",
    "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and DeepSeek's R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps required by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, offering valuable insights into scaling LLM reasoning capabilities across diverse tasks."
  },
  {
    "title": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning",
    "url": "http://arxiv.org/abs/2504.14268v1",
    "arxiv_id": "2504.14268v1",
    "authors": [
      "Xinye Chen"
    ],
    "published": "2025-04-19T11:35:03+00:00",
    "summary": "This paper presents a novel reinforcement learning (RL) framework for dynamically optimizing numerical precision in the preconditioned conjugate gradient (CG) method. By modeling precision selection as a Markov Decision Process (MDP), we employ Q-learning to adaptively assign precision levels to key operations, striking an optimal balance between computational efficiency and numerical accuracy, while ensuring stability through double-precision scalar computations and residual computing. In practice, the algorithm is trained on a set of data and subsequently performs inference for precision selection on out-of-sample data, without requiring re-analysis or retraining for new datasets. This enables the method to adapt seamlessly to new problem instances without the computational overhead of recalibration. Our results demonstrate the effectiveness of RL in enhancing solver's performance, marking the first application of RL to mixed-precision numerical methods. The findings highlight the approach's practical advantages, robustness, and scalability, providing valuable insights into its integration with iterative solvers and paving the way for AI-driven advancements in scientific computing."
  },
  {
    "title": "Generative emulation of chaotic dynamics with coherent prior",
    "url": "http://arxiv.org/abs/2504.14264v1",
    "arxiv_id": "2504.14264v1",
    "authors": [
      "Juan Nathaniel",
      "Pierre Gentine"
    ],
    "published": "2025-04-19T11:14:40+00:00",
    "summary": "Data-driven emulation of nonlinear dynamics is challenging due to long-range skill decay that often produces physically unrealistic outputs. Recent advances in generative modeling aim to address these issues by providing uncertainty quantification and correction. However, the quality of generated simulation remains heavily dependent on the choice of conditioning priors. In this work, we present an efficient generative framework for dynamics emulation, unifying principles of turbulence with diffusion-based modeling: Cohesion. Specifically, our method estimates large-scale coherent structure of the underlying dynamics as guidance during the denoising process, where small-scale fluctuation in the flow is then resolved. These coherent priors are efficiently approximated using reduced-order models, such as deep Koopman operators, that allow for rapid generation of long prior sequences while maintaining stability over extended forecasting horizon. With this gain, we can reframe forecasting as trajectory planning, a common task in reinforcement learning, where conditional denoising is performed once over entire sequences, minimizing the computational cost of autoregressive-based generative methods. Empirical evaluations on chaotic systems of increasing complexity, including Kolmogorov flow, shallow water equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion superior long-range forecasting skill that can efficiently generate physically-consistent simulations, even in the presence of partially-observed guidance."
  },
  {
    "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners",
    "url": "http://arxiv.org/abs/2504.14239v1",
    "arxiv_id": "2504.14239v1",
    "authors": [
      "Yuhang Liu",
      "Pengxiang Li",
      "Congkai Xie",
      "Xavier Hu",
      "Xiaotian Han",
      "Shengyu Zhang",
      "Hongxia Yang",
      "Fei Wu"
    ],
    "published": "2025-04-19T09:25:55+00:00",
    "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1."
  },
  {
    "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward",
    "url": "http://arxiv.org/abs/2504.14177v1",
    "arxiv_id": "2504.14177v1",
    "authors": [
      "Li He",
      "He Zhao",
      "Stephen Wan",
      "Dadong Wang",
      "Lina Yao",
      "Tongliang Liu"
    ],
    "published": "2025-04-19T04:44:32+00:00",
    "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines."
  },
  {
    "title": "Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility",
    "url": "http://arxiv.org/abs/2504.14103v1",
    "arxiv_id": "2504.14103v1",
    "authors": [
      "Merve Atasever",
      "Ali Okhovat",
      "Azhang Nazaripouya",
      "John Nisbet",
      "Omer Kurkutlu",
      "Jyotirmoy V. Deshmukh",
      "Yasemin Ozkan Aydin"
    ],
    "published": "2025-04-18T23:08:48+00:00",
    "summary": "Among vertebrates, salamanders, with their unique ability to transition between walking and swimming gaits, highlight the role of spinal mobility in locomotion. A flexible spine enables undulation of the body through a wavelike motion along the spine, aiding navigation over uneven terrains and obstacles. Yet environmental uncertainties, such as surface irregularities and variations in friction, can significantly disrupt body-limb coordination and cause discrepancies between predictions from mathematical models and real-world outcomes. Addressing this challenge requires the development of sophisticated control strategies capable of dynamically adapting to uncertain conditions while maintaining efficient locomotion. Deep reinforcement learning (DRL) offers a promising framework for handling non-deterministic environments and enabling robotic systems to adapt effectively and perform robustly under challenging conditions. In this study, we comparatively examine learning-based control strategies and biologically inspired gait design methods on a salamander-like robot."
  },
  {
    "title": "Personalizing Exposure Therapy via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.14095v1",
    "arxiv_id": "2504.14095v1",
    "authors": [
      "Athar Mahmoudi-Nejad",
      "Matthew Guzdial",
      "Pierre Boulanger"
    ],
    "published": "2025-04-18T22:21:41+00:00",
    "summary": "Personalized therapy, in which a therapeutic practice is adapted to an individual patient, can lead to improved health outcomes. Typically, this is accomplished by relying on a therapist's training and intuition along with feedback from a patient. However, this requires the therapist to become an expert on any technological components, such as in the case of Virtual Reality Exposure Therapy (VRET). While there exist approaches to automatically adapt therapeutic content to a patient, they generally rely on hand-authored, pre-defined rules, which may not generalize to all individuals. In this paper, we propose an approach to automatically adapt therapeutic content to patients based on physiological measures. We implement our approach in the context of virtual reality arachnophobia exposure therapy, and rely on experience-driven procedural content generation via reinforcement learning (EDPCGRL) to generate virtual spiders to match an individual patient. Through a human subject study, we demonstrate that our system significantly outperforms a more common rules-based method, highlighting its potential for enhancing personalized therapeutic interventions."
  },
  {
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
    "url": "http://arxiv.org/abs/2504.13837v1",
    "arxiv_id": "2504.13837v1",
    "authors": [
      "Yang Yue",
      "Zhiqi Chen",
      "Rui Lu",
      "Andrew Zhao",
      "Zhaokai Wang",
      "Yang Yue",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2025-04-18T17:59:56+00:00",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io"
  },
  {
    "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13818v1",
    "arxiv_id": "2504.13818v1",
    "authors": [
      "Yixuan Even Xu",
      "Yash Savani",
      "Fei Fang",
      "Zico Kolter"
    ],
    "published": "2025-04-18T17:49:55+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark."
  },
  {
    "title": "Bake Two Cakes with One Oven: RL for Defusing Popularity Bias and Cold-start in Third-Party Library Recommendations",
    "url": "http://arxiv.org/abs/2504.13772v1",
    "arxiv_id": "2504.13772v1",
    "authors": [
      "Minh Hoang Vuong",
      "Anh M. T. Bui",
      "Phuong T. Nguyen",
      "Davide Di Ruscio"
    ],
    "published": "2025-04-18T16:17:20+00:00",
    "summary": "Third-party libraries (TPLs) have become an integral part of modern software development, enhancing developer productivity and accelerating time-to-market. However, identifying suitable candidates from a rapidly growing and continuously evolving collection of TPLs remains a challenging task. TPL recommender systems have been studied, offering a promising solution to address this issue. They typically rely on collaborative filtering (CF) that exploits a two-dimensional project-library matrix (user-item in general context of recommendation) when making recommendations. We have noticed that CF-based approaches often encounter two challenges: (i) a tendency to recommend popular items more frequently, making them even more dominant, a phenomenon known as popularity bias, and (ii) difficulty in generating recommendations for new users or items due to limited user-item interactions, commonly referred to as the cold-start problem. In this paper, we propose a reinforcement learning (RL)-based approach to address popularity bias and the cold-start problem in TPL recommendation. Our method comprises three key components. First, we utilize a graph convolution network (GCN)-based embedding model to learn user preferences and user-item interactions, allowing us to capture complex relationships within interaction subgraphs and effectively represent new user/item embeddings. Second, we introduce an aggregation operator to generate a representative embedding from user and item embeddings, which is then used to model cold-start users. Finally, we adopt a model-based RL framework for TPL recommendation, where popularity bias is mitigated through a carefully designed reward function and a rarity-based replay buffer partitioning strategy. The results demonstrated that our proposed approach outperforms state-of-the-art models in cold-start scenarios while effectively mitigating the impact of popularity bias."
  },
  {
    "title": "A Reinforcement Learning Method to Factual and Counterfactual Explanations for Session-based Recommendation",
    "url": "http://arxiv.org/abs/2504.13632v1",
    "arxiv_id": "2504.13632v1",
    "authors": [
      "Han Zhou",
      "Hui Fang",
      "Zhu Sun",
      "Wentao Hu"
    ],
    "published": "2025-04-18T11:13:40+00:00",
    "summary": "Session-based Recommendation (SR) systems have recently achieved considerable success, yet their complex, \"black box\" nature often obscures why certain recommendations are made. Existing explanation methods struggle to pinpoint truly influential factors, as they frequently depend on static user profiles or fail to grasp the intricate dynamics within user sessions. In response, we introduce FCESR (Factual and Counterfactual Explanations for Session-based Recommendation), a novel framework designed to illuminate SR model predictions by emphasizing both the sufficiency (factual) and necessity (counterfactual) of recommended items. By recasting explanation generation as a combinatorial optimization challenge and leveraging reinforcement learning, our method uncovers the minimal yet critical sequence of items influencing recommendations. Moreover, recognizing the intrinsic value of robust explanations, we innovatively utilize these factual and counterfactual insights within a contrastive learning paradigm, employing them as high-quality positive and negative samples to fine-tune and significantly enhance SR accuracy. Extensive qualitative and quantitative evaluations across diverse datasets and multiple SR architectures confirm that our framework not only boosts recommendation accuracy but also markedly elevates the quality and interpretability of explanations, thereby paving the way for more transparent and trustworthy recommendation systems."
  },
  {
    "title": "Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13619v1",
    "arxiv_id": "2504.13619v1",
    "authors": [
      "Rohan P. Singh",
      "Mitsuharu Morisawa",
      "Mehdi Benallegue",
      "Zhaoming Xie",
      "Fumio Kanehiro"
    ],
    "published": "2025-04-18T10:49:07+00:00",
    "summary": "For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only proprioceptive feedback. We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment. Further, we argue that the robustness of a bipedal walking policy can be improved if the robot is allowed to exhibit aperiodic motion with variable stepping frequency. We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity. Through simulation experiments, we show the effectiveness of this policy specifically for walking over challenging terrains by controlling swing and stance durations. The code for training and evaluation is available online at https://github.com/rohanpsingh/LearningHumanoidWalking. Demo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q."
  },
  {
    "title": "Compile Scene Graphs with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13617v1",
    "arxiv_id": "2504.13617v1",
    "authors": [
      "Zuyao Chen",
      "Jinlin Wu",
      "Zhen Lei",
      "Marc Pollefeys",
      "Chang Wen Chen"
    ],
    "published": "2025-04-18T10:46:22+00:00",
    "summary": "Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rate--unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at https://github.com/gpt4vision/R1-SGG."
  },
  {
    "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling",
    "url": "http://arxiv.org/abs/2504.13592v1",
    "arxiv_id": "2504.13592v1",
    "authors": [
      "Zihao Feng",
      "Xiaoxue Wang",
      "Ziwei Bai",
      "Donghang Su",
      "Bowen Wu",
      "Qun Yu",
      "Baoxun Wang"
    ],
    "published": "2025-04-18T09:52:12+00:00",
    "summary": "Intent detection, a critical component in task-oriented dialogue (TOD) systems, faces significant challenges in adapting to the rapid influx of integrable tools with complex interrelationships. Existing approaches, such as zero-shot reformulations and LLM-based dynamic recognition, struggle with performance degradation when encountering unseen intents, leading to erroneous task routing. To enhance the model's generalization performance on unseen tasks, we employ Reinforcement Learning (RL) combined with a Reward-based Curriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO) training in intent detection tasks. Experiments demonstrate that RL-trained models substantially outperform supervised fine-tuning (SFT) baselines in generalization. Besides, the introduction of the RCS, significantly bolsters the effectiveness of RL in intent detection by focusing the model on challenging cases during training. Moreover, incorporating Chain-of-Thought (COT) processes in RL notably improves generalization in complex intent detection tasks, underscoring the importance of thought in challenging scenarios. This work advances the generalization of intent detection tasks, offering practical insights for deploying adaptable dialogue systems."
  },
  {
    "title": "Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots",
    "url": "http://arxiv.org/abs/2504.13582v1",
    "arxiv_id": "2504.13582v1",
    "authors": [
      "Zongyuan Chen",
      "Yan Xia",
      "Jiayuan Liu",
      "Jijia Liu",
      "Wenhao Tang",
      "Jiayu Chen",
      "Feng Gao",
      "Longfei Ma",
      "Hongen Liao",
      "Yu Wang",
      "Chao Yu",
      "Boyu Zhang",
      "Fei Xing"
    ],
    "published": "2025-04-18T09:34:56+00:00",
    "summary": "Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications."
  },
  {
    "title": "Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13554v1",
    "arxiv_id": "2504.13554v1",
    "authors": [
      "Xin Tang",
      "Qian Chen",
      "Wenjie Weng",
      "Chao Jin",
      "Zhang Liu",
      "Jiacheng Wang",
      "Geng Sun",
      "Xiaohuan Li",
      "Dusit Niyato"
    ],
    "published": "2025-04-18T08:44:06+00:00",
    "summary": "Artificial Intelligence (AI)-driven convolutional neural networks enhance rescue, inspection, and surveillance tasks performed by low-altitude uncrewed aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown environments. However, their high computational demands often exceed a single UAV's capacity, leading to system instability, further exacerbated by the limited and dynamic resources of GCNs. To address these challenges, this paper proposes a novel cooperation framework involving UAVs, ground-embedded robots (GERs), and high-altitude platforms (HAPs), which enable resource pooling through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide computing services for UAV offloaded tasks. Specifically, we formulate the multi-objective optimization problem of task assignment and exploration optimization in UAVs as a dynamic long-term optimization problem. Our objective is to minimize task completion time and energy consumption while ensuring system stability over time. To achieve this, we first employ the Lyapunov optimization technique to transform the original problem, with stability constraints, into a per-slot deterministic problem. We then propose an algorithm named HG-MADDPG, which combines the Hungarian algorithm with a generative diffusion model (GDM)-based multi-agent deep deterministic policy gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a method for exploration area selection, enhancing UAV efficiency in interacting with the environment. We then innovatively integrate the GDM and multi-agent deep deterministic policy gradient (MADDPG) to optimize task assignment decisions, such as task offloading and resource allocation. Simulation results demonstrate the effectiveness of the proposed approach, with significant improvements in task offloading efficiency, latency reduction, and system stability compared to baseline methods."
  },
  {
    "title": "SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents",
    "url": "http://arxiv.org/abs/2504.13541v1",
    "arxiv_id": "2504.13541v1",
    "authors": [
      "Avaneesh Devkota",
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "published": "2025-04-18T08:12:59+00:00",
    "summary": "The ability to train intelligent autonomous agents (such as mobile robots) on multiple tasks is crucial for adapting to dynamic real-world environments. However, state-of-the-art reinforcement learning (RL) methods only excel in single-task settings, and still struggle to generalize across multiple tasks due to task interference. Moreover, real-world environments also demand the agents to have data stream processing capabilities. Toward this, a state-of-the-art work employs Spiking Neural Networks (SNNs) to improve multi-task learning by exploiting temporal information in data stream, while enabling lowpower/energy event-based operations. However, it relies on fixed context/task-switching intervals during its training, hence limiting the scalability and effectiveness of multi-task learning. To address these limitations, we propose SwitchMT, a novel adaptive task-switching methodology for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT employs the following key ideas: (1) a Deep Spiking Q-Network with active dendrites and dueling structure, that utilizes task-specific context signals to create specialized sub-networks; and (2) an adaptive task-switching policy that leverages both rewards and internal dynamics of the network parameters. Experimental results demonstrate that SwitchMT achieves superior performance in multi-task learning compared to state-of-the-art methods. It achieves competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6, and Enduro: 355.2) compared to the state-of-the-art, showing its better generalized learning capability. These results highlight the effectiveness of our SwitchMT methodology in addressing task interference while enabling multi-task learning automation through adaptive task switching, thereby paving the way for more efficient generalist agents with scalable multi-task learning capabilities."
  },
  {
    "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning",
    "url": "http://arxiv.org/abs/2504.13500v1",
    "arxiv_id": "2504.13500v1",
    "authors": [
      "Jianing Wang",
      "Jin Jiang",
      "Yang Liu",
      "Mengdi Zhang",
      "Xunliang Cai"
    ],
    "published": "2025-04-18T06:42:30+00:00",
    "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent reasoning steps, similar to people sometimes pausing to think about what mistakes may occur and how to avoid them, rather than relying solely on trial and error. Specifically, we define a prejudge node in the rationale, which represents a reasoning step, with at least one step that follows the prejudge node that has no paths toward the correct answer. To synthesize the prejudge reasoning process, we present an automated reasoning framework with a dynamic tree-searching strategy. This framework requires only one LLM to perform answer judging, response critiquing, prejudge generation, and thought completion. Furthermore, we develop a two-phase training mechanism with supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance the reasoning capabilities of LLMs. Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs. Code and data is released at https://github.com/wjn1996/Prejudge-Before-Think."
  },
  {
    "title": "Improving Sequential Recommenders through Counterfactual Augmentation of System Exposure",
    "url": "http://arxiv.org/abs/2504.13482v1",
    "arxiv_id": "2504.13482v1",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiyuan Yang",
      "Zuming Yan",
      "Zihan Wang",
      "Liu Yang",
      "Pengjie Ren",
      "Zhumin Chen",
      "Maarten de Rijke",
      "Xin Xin"
    ],
    "published": "2025-04-18T05:46:27+00:00",
    "summary": "In sequential recommendation (SR), system exposure refers to items that are exposed to the user. Typically, only a few of the exposed items would be interacted with by the user. Although SR has achieved great success in predicting future user interests, existing SR methods still fail to fully exploit system exposure data. Most methods only model items that have been interacted with, while the large volume of exposed but non-interacted items is overlooked. Even methods that consider the whole system exposure typically train the recommender using only the logged historical system exposure, without exploring unseen user interests.   In this paper, we propose counterfactual augmentation over system exposure for sequential recommendation (CaseRec). To better model historical system exposure, CaseRec introduces reinforcement learning to account for different exposure rewards. CaseRec uses a decision transformer-based sequential model to take an exposure sequence as input and assigns different rewards according to the user feedback. To further explore unseen user interests, CaseRec proposes to perform counterfactual augmentation, where exposed original items are replaced with counterfactual items. Then, a transformer-based user simulator is proposed to predict the user feedback reward for the augmented items. Augmentation, together with the user simulator, constructs counterfactual exposure sequences to uncover new user interests. Finally, CaseRec jointly uses the logged exposure sequences with the counterfactual exposure sequences to train a decision transformer-based sequential model for generating recommendation. Experiments on three real-world benchmarks show the effectiveness of CaseRec. Our code is available at https://github.com/ZiqiZhao1/CaseRec."
  },
  {
    "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs",
    "url": "http://arxiv.org/abs/2504.13471v1",
    "arxiv_id": "2504.13471v1",
    "authors": [
      "Jiliang Ni",
      "Jiachen Pu",
      "Zhongyi Yang",
      "Kun Zhou",
      "Hui Wang",
      "Xiaoliang Xiao",
      "Dakui Wang",
      "Xin Li",
      "Jingfeng Luo",
      "Conggang Hu"
    ],
    "published": "2025-04-18T05:25:22+00:00",
    "summary": "In recent years, Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) pipelines, improving performance and generalization. This has spurred their integration into various systems. Many NLP systems, including ours, employ a \"one-stage\" pipeline directly incorporating LLMs. While effective, this approach incurs substantial costs and latency due to the need for large model parameters to achieve satisfactory outcomes. This paper introduces a three-stage cost-efficient end-to-end LLM deployment pipeline-including prototyping, knowledge transfer, and model compression-to tackle the cost-performance dilemma in LLM-based frameworks. Our approach yields a super tiny model optimized for cost and performance in online systems, simplifying the system architecture. Initially, by transforming complex tasks into a function call-based LLM-driven pipeline, an optimal performance prototype system is constructed to produce high-quality data as a teacher model. The second stage combine techniques like rejection fine-tuning, reinforcement learning and knowledge distillation to transfer knowledge to a smaller 0.5B student model, delivering effective performance at minimal cost. The final stage applies quantization and pruning to extremely compress model to 0.4B, achieving ultra-low latency and cost. The framework's modular design and cross-domain capabilities suggest potential applicability in other NLP areas."
  },
  {
    "title": "Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks",
    "url": "http://arxiv.org/abs/2504.13424v1",
    "arxiv_id": "2504.13424v1",
    "authors": [
      "Yang Shen",
      "Shuqi Chai",
      "Bing Li",
      "Xiaodong Luo",
      "Qingjiang Shi",
      "Rongqing Zhang"
    ],
    "published": "2025-04-18T02:49:45+00:00",
    "summary": "In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance."
  },
  {
    "title": "An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13368v1",
    "arxiv_id": "2504.13368v1",
    "authors": [
      "Haoran Xu",
      "Shuozhe Li",
      "Harshit Sikchi",
      "Scott Niekum",
      "Amy Zhang"
    ],
    "published": "2025-04-17T22:21:35+00:00",
    "summary": "We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and then performing discriminator-weighted behavior cloning gives strong results on various types of datasets. That optimal discriminator weight is quite similar to the learned visitation distribution ratio in Dual-RL, however, we find that current Dual-RL methods do not correctly estimate that ratio. In IDRL, we propose a correction method to iteratively approach the optimal visitation distribution ratio in the offline dataset given no addtional expert dataset. During each iteration, IDRL removes zero-weight suboptimal transitions using the learned ratio from the previous iteration and runs Dual-RL on the remaining subdataset. This can be seen as replacing the behavior visitation distribution with the optimized visitation distribution from the previous iteration, which theoretically gives a curriculum of improved visitation distribution ratios that are closer to the optimal discriminator weight. We verify the effectiveness of IDRL on various kinds of offline datasets, including D4RL datasets and more realistic corrupted demonstrations. IDRL beats strong Primal-RL and Dual-RL baselines in terms of both performance and stability, on all datasets."
  },
  {
    "title": "On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management",
    "url": "http://arxiv.org/abs/2504.13314v1",
    "arxiv_id": "2504.13314v1",
    "authors": [
      "Timothy Tjhay",
      "Ricardo J. Bessa",
      "Jose Paulos"
    ],
    "published": "2025-04-17T20:01:48+00:00",
    "summary": "The European Union's Artificial Intelligence (AI) Act defines robustness, resilience, and security requirements for high-risk sectors but lacks detailed methodologies for assessment. This paper introduces a novel framework for quantitatively evaluating the robustness and resilience of reinforcement learning agents in congestion management. Using the AI-friendly digital environment Grid2Op, perturbation agents simulate natural and adversarial disruptions by perturbing the input of AI systems without altering the actual state of the environment, enabling the assessment of AI performance under various scenarios. Robustness is measured through stability and reward impact metrics, while resilience quantifies recovery from performance degradation. The results demonstrate the framework's effectiveness in identifying vulnerabilities and improving AI robustness and resilience for critical applications."
  },
  {
    "title": "Energy-Based Reward Models for Robust Language Model Alignment",
    "url": "http://arxiv.org/abs/2504.13134v1",
    "arxiv_id": "2504.13134v1",
    "authors": [
      "Anamika Lochab",
      "Ruqi Zhang"
    ],
    "published": "2025-04-17T17:47:15+00:00",
    "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM."
  },
  {
    "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
    "url": "http://arxiv.org/abs/2504.13125v1",
    "arxiv_id": "2504.13125v1",
    "authors": [
      "Varun Rao",
      "Youran Sun",
      "Mahendra Kumar",
      "Tejas Mutneja",
      "Agastya Mukherjee",
      "Haizhao Yang"
    ],
    "published": "2025-04-17T17:42:02+00:00",
    "summary": "This paper investigates the application of large language models (LLMs) to financial tasks. We fine-tuned foundation models using the Open FinLLM Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed techniques including supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) to enhance their financial capabilities. The fine-tuned models demonstrated substantial performance gains across a wide range of financial tasks. Moreover, we measured the data scaling law in the financial domain. Our work demonstrates the potential of large language models (LLMs) in financial applications."
  },
  {
    "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training",
    "url": "http://arxiv.org/abs/2504.13123v1",
    "arxiv_id": "2504.13123v1",
    "authors": [
      "Xinsong Zhang",
      "Yarong Zeng",
      "Xinting Huang",
      "Hu Hu",
      "Runquan Xie",
      "Han Hu",
      "Zhanhui Kang"
    ],
    "published": "2025-04-17T17:40:06+00:00",
    "summary": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for multimodal large language models heavily rely on high-quality image-text pairs. As models and data scales grow exponentially, the availability of such meticulously curated data has become increasingly scarce and saturated, thereby severely limiting further advancements in this domain. This study investigates scalable caption generation techniques for vision-language model pre-training and demonstrates that large-scale low-hallucination synthetic captions can serve dual purposes: 1) acting as a viable alternative to real-world data for pre-training paradigms and 2) achieving superior performance enhancement when integrated into vision-language models through empirical validation. This paper presents three key contributions: 1) a novel pipeline for generating high-quality, low-hallucination, and knowledge-rich synthetic captions. Our continuous DPO methodology yields remarkable results in reducing hallucinations. Specifically, the non-hallucination caption rate on a held-out test set increases from 48.2% to 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals that our synthetic captions confer superior pre-training advantages over their counterparts. Across 35 vision language tasks, the model trained with our data achieves a significant performance gain of at least 6.2% compared to alt-text pairs and other previous work. Meanwhile, it also offers considerable support in the text-to-image domain. With our dataset, the FID score is reduced by 17.1 on a real-world validation benchmark and 13.3 on the MSCOCO validation benchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and knowledge-intensive synthetic caption dataset."
  },
  {
    "title": "Recursive Deep Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.13241v1",
    "arxiv_id": "2504.13241v1",
    "authors": [
      "Paul Ghanem",
      "Michael Potter",
      "Owen Howell",
      "Pau Closas",
      "Alireza Ramezani",
      "Deniz Erdogmus",
      "Robert Platt",
      "Tales Imbiriba"
    ],
    "published": "2025-04-17T17:39:35+00:00",
    "summary": "Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms."
  },
  {
    "title": "SkyReels-V2: Infinite-length Film Generative Model",
    "url": "http://arxiv.org/abs/2504.13074v1",
    "arxiv_id": "2504.13074v1",
    "authors": [
      "Guibin Chen",
      "Dixuan Lin",
      "Jiangping Yang",
      "Chunze Lin",
      "Juncheng Zhu",
      "Mingyuan Fan",
      "Hao Zhang",
      "Sheng Chen",
      "Zheng Chen",
      "Chengchen Ma",
      "Weiming Xiong",
      "Wei Wang",
      "Nuo Pang",
      "Kang Kang",
      "Zhiheng Xu",
      "Yuzhe Jin",
      "Yupeng Liang",
      "Yubing Song",
      "Peng Zhao",
      "Boyuan Xu",
      "Di Qiu",
      "Debang Li",
      "Zhengcong Fei",
      "Yang Li",
      "Yahui Zhou"
    ],
    "published": "2025-04-17T16:37:27+00:00",
    "summary": "Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2."
  },
  {
    "title": "SkyReels-V2: Infinite-length Film Generative Model",
    "url": "http://arxiv.org/abs/2504.13074v2",
    "arxiv_id": "2504.13074v2",
    "authors": [
      "Guibin Chen",
      "Dixuan Lin",
      "Jiangping Yang",
      "Chunze Lin",
      "Juncheng Zhu",
      "Mingyuan Fan",
      "Hao Zhang",
      "Sheng Chen",
      "Zheng Chen",
      "Chengchen Ma",
      "Weiming Xiong",
      "Wei Wang",
      "Nuo Pang",
      "Kang Kang",
      "Zhiheng Xu",
      "Yuzhe Jin",
      "Yupeng Liang",
      "Yubing Song",
      "Peng Zhao",
      "Boyuan Xu",
      "Di Qiu",
      "Debang Li",
      "Zhengcong Fei",
      "Yang Li",
      "Yahui Zhou"
    ],
    "published": "2025-04-17T16:37:27+00:00",
    "summary": "Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at https://github.com/SkyworkAI/SkyReels-V2."
  },
  {
    "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "url": "http://arxiv.org/abs/2504.13055v1",
    "arxiv_id": "2504.13055v1",
    "authors": [
      "Xiangyan Liu",
      "Jinjie Ni",
      "Zijian Wu",
      "Chao Du",
      "Longxu Dou",
      "Haonan Wang",
      "Tianyu Pang",
      "Michael Qizhe Shieh"
    ],
    "published": "2025-04-17T16:10:13+00:00",
    "summary": "Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance."
  },
  {
    "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning",
    "url": "http://arxiv.org/abs/2504.13032v1",
    "arxiv_id": "2504.13032v1",
    "authors": [
      "Zheng Wang",
      "Shu Xian Teo",
      "Jun Jie Chew",
      "Wei Shi"
    ],
    "published": "2025-04-17T15:41:39+00:00",
    "summary": "Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought-action-observation (TAO) process to enhance LLM performance, but these approaches are often constrained by the LLMs' limited knowledge of complex tasks. Retrieval-augmented generation (RAG) offers new opportunities by leveraging external databases to ground generation in retrieved information. In this paper, we identify two key challenges (enlargability and transferability) in applying RAG to task planning. We propose InstructRAG, a novel solution within a multi-agent meta-reinforcement learning framework, to address these challenges. InstructRAG includes a graph to organize past instruction paths (sequences of correct actions), an RL-Agent with Reinforcement Learning to expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to improve task generalization for transferability. The two agents are trained end-to-end to optimize overall planning performance. Our experiments on four widely used task planning datasets demonstrate that InstructRAG significantly enhances performance and adapts efficiently to new tasks, achieving up to a 19.2% improvement over the best existing approach."
  },
  {
    "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?",
    "url": "http://arxiv.org/abs/2504.12961v1",
    "arxiv_id": "2504.12961v1",
    "authors": [
      "Zhouyang Jiang",
      "Bin Zhang",
      "Airong Wei",
      "Zhiwei Xu"
    ],
    "published": "2025-04-17T14:07:11+00:00",
    "summary": "Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \\textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios."
  },
  {
    "title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs",
    "url": "http://arxiv.org/abs/2504.12949v1",
    "arxiv_id": "2504.12949v1",
    "authors": [
      "Zhenao Song"
    ],
    "published": "2025-04-17T13:50:55+00:00",
    "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs). However, their performance heavily relies on the strategy used to select training points. Conventional adaptive sampling methods, such as residual-based refinement, often require multi-round sampling and repeated retraining of PINNs, leading to computational inefficiency due to redundant points and costly gradient computations-particularly in high-dimensional or high-order derivative scenarios. To address these limitations, we propose RL-PINNs, a reinforcement learning(RL)-driven adaptive sampling framework that enables efficient training with only a single round of sampling. Our approach formulates adaptive sampling as a Markov decision process, where an RL agent dynamically selects optimal training points by maximizing a long-term utility metric. Critically, we replace gradient-dependent residual metrics with a computationally efficient function variation as the reward signal, eliminating the overhead of derivative calculations. Furthermore, we employ a delayed reward mechanism to prioritize long-term training stability over short-term gains. Extensive experiments across diverse PDE benchmarks, including low-regular, nonlinear, high-dimensional, and high-order problems, demonstrate that RL-PINNs significantly outperforms existing residual-driven adaptive methods in accuracy. Notably, RL-PINNs achieve this with negligible sampling overhead, making them scalable to high-dimensional and high-order problems."
  },
  {
    "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models",
    "url": "http://arxiv.org/abs/2504.12833v1",
    "arxiv_id": "2504.12833v1",
    "authors": [
      "Elior Benarous",
      "Yilun Du",
      "Heng Yang"
    ],
    "published": "2025-04-17T10:46:39+00:00",
    "summary": "We present a novel approach to training specialized instruction-based image-editing diffusion models, addressing key challenges in structural preservation with input images and semantic alignment with user prompts. We introduce an online reinforcement learning framework that aligns the diffusion model with human preferences without relying on extensive human annotations or curating a large dataset. Our method significantly improves the realism and alignment with instructions in two ways. First, the proposed models achieve precise and structurally coherent modifications in complex scenes while maintaining high fidelity in instruction-irrelevant areas. Second, they capture fine nuances in the desired edit by leveraging a visual prompt, enabling detailed control over visual edits without lengthy textual prompts. This approach simplifies users' efforts to achieve highly specific edits, requiring only 5 reference images depicting a certain concept for training. Experimental results demonstrate that our models can perform intricate edits in complex scenes, after just 10 training steps. Finally, we showcase the versatility of our method by applying it to robotics, where enhancing the visual realism of simulated environments through targeted sim-to-real image edits improves their utility as proxies for real-world settings."
  },
  {
    "title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis",
    "url": "http://arxiv.org/abs/2504.12777v1",
    "arxiv_id": "2504.12777v1",
    "authors": [
      "James Rudd-Jones",
      "Mirco Musolesi",
      "Mar\u00eda P\u00e9rez-Ortiz"
    ],
    "published": "2025-04-17T09:18:04+00:00",
    "summary": "Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research."
  },
  {
    "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks",
    "url": "http://arxiv.org/abs/2504.12764v1",
    "arxiv_id": "2504.12764v1",
    "authors": [
      "Hao Xu",
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chao Zhang",
      "Suyuchen Wang",
      "Qixin Zhang",
      "Joao Monteiro",
      "Qiuzhuang Sun",
      "Tianshu Yu"
    ],
    "published": "2025-04-17T09:01:16+00:00",
    "summary": "In this paper, we presented GraphOmni, a comprehensive benchmark framework for systematically evaluating the graph reasoning capabilities of LLMs. By analyzing critical dimensions, including graph types, serialization formats, and prompt schemes, we provided extensive insights into the strengths and limitations of current LLMs. Our empirical findings emphasize that no single serialization or prompting strategy consistently outperforms others. Motivated by these insights, we propose a reinforcement learning-based approach that dynamically selects the best serialization-prompt pairings, resulting in significant accuracy improvements. GraphOmni's modular and extensible design establishes a robust foundation for future research, facilitating advancements toward general-purpose graph reasoning models."
  },
  {
    "title": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection",
    "url": "http://arxiv.org/abs/2504.12749v1",
    "arxiv_id": "2504.12749v1",
    "authors": [
      "Weijia Li",
      "Guanglei Chu",
      "Jiong Chen",
      "Guo-Sen Xie",
      "Caifeng Shan",
      "Fang Zhao"
    ],
    "published": "2025-04-17T08:41:23+00:00",
    "summary": "Recent advances in industrial anomaly detection have highlighted the need for deeper logical anomaly analysis, where unexpected relationships among objects, counts, and spatial configurations must be identified and explained. Existing approaches often rely on large-scale external reasoning modules or elaborate pipeline designs, hindering practical deployment and interpretability. To address these limitations, we introduce a new task, Reasoning Logical Anomaly Detection (RLAD), which extends traditional anomaly detection by incorporating logical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny multimodal language model built on Qwen2.5-VL 3B. Our approach leverages a two-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for fine-grained visual understanding, followed by Group Relative Policy Optimization (GRPO) to refine logical anomaly detection and enforce coherent, human-readable reasoning. Crucially, reward signals are derived from both the detection accuracy and the structural quality of the outputs, obviating the need for building chain of thought (CoT) reasoning data. Experiments on the MVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller, matches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further excels in producing concise and interpretable rationales. This unified design reduces reliance on large models and complex pipelines, while offering transparent and interpretable insights into logical anomaly detection. Code and data will be released."
  },
  {
    "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination",
    "url": "http://arxiv.org/abs/2504.12714v1",
    "arxiv_id": "2504.12714v1",
    "authors": [
      "Kunal Jha",
      "Wilka Carvalho",
      "Yancheng Liang",
      "Simon S. Du",
      "Max Kleiman-Weiner",
      "Natasha Jaques"
    ],
    "published": "2025-04-17T07:41:25+00:00",
    "summary": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a distribution of environments with a single partner enables learning general cooperative skills that support ZSC with many new partners on many new problems. We introduce two Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called Cross-Environment Cooperation (CEC), and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data."
  },
  {
    "title": "Embodied Neuromorphic Control Applied on a 7-DOF Robotic Manipulator",
    "url": "http://arxiv.org/abs/2504.12702v1",
    "arxiv_id": "2504.12702v1",
    "authors": [
      "Ziqi Wang",
      "Jingyue Zhao",
      "Jichao Yang",
      "Yaohua Wang",
      "Xun Xiao",
      "Yuan Li",
      "Chao Xiao",
      "Lei Wang"
    ],
    "published": "2025-04-17T07:13:37+00:00",
    "summary": "The development of artificial intelligence towards real-time interaction with the environment is a key aspect of embodied intelligence and robotics. Inverse dynamics is a fundamental robotics problem, which maps from joint space to torque space of robotic systems. Traditional methods for solving it rely on direct physical modeling of robots which is difficult or even impossible due to nonlinearity and external disturbance. Recently, data-based model-learning algorithms are adopted to address this issue. However, they often require manual parameter tuning and high computational costs. Neuromorphic computing is inherently suitable to process spatiotemporal features in robot motion control at extremely low costs. However, current research is still in its infancy: existing works control only low-degree-of-freedom systems and lack performance quantification and comparison. In this paper, we propose a neuromorphic control framework to control 7 degree-of-freedom robotic manipulators. We use Spiking Neural Network to leverage the spatiotemporal continuity of the motion data to improve control accuracy, and eliminate manual parameters tuning. We validated the algorithm on two robotic platforms, which reduces torque prediction error by at least 60% and performs a target position tracking task successfully. This work advances embodied neuromorphic control by one step forward from proof of concept to applications in complex real-world tasks."
  },
  {
    "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.12680v1",
    "arxiv_id": "2504.12680v1",
    "authors": [
      "Baining Zhao",
      "Ziyou Wang",
      "Jianjie Fang",
      "Chen Gao",
      "Fanhang Man",
      "Jinqiang Cui",
      "Xin Wang",
      "Xinlei Chen",
      "Yong Li",
      "Wenwu Zhu"
    ],
    "published": "2025-04-17T06:16:11+00:00",
    "summary": "Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training."
  },
  {
    "title": "Autonomous Drone for Dynamic Smoke Plume Tracking",
    "url": "http://arxiv.org/abs/2504.12664v1",
    "arxiv_id": "2504.12664v1",
    "authors": [
      "Srijan Kumar Pal",
      "Shashank Sharma",
      "Nikil Krishnakumar",
      "Jiarong Hong"
    ],
    "published": "2025-04-17T05:50:15+00:00",
    "summary": "This paper presents a novel autonomous drone-based smoke plume tracking system capable of navigating and tracking plumes in highly unsteady atmospheric conditions. The system integrates advanced hardware and software and a comprehensive simulation environment to ensure robust performance in controlled and real-world settings. The quadrotor, equipped with a high-resolution imaging system and an advanced onboard computing unit, performs precise maneuvers while accurately detecting and tracking dynamic smoke plumes under fluctuating conditions. Our software implements a two-phase flight operation, i.e., descending into the smoke plume upon detection and continuously monitoring the smoke movement during in-plume tracking. Leveraging Proportional Integral-Derivative (PID) control and a Proximal Policy Optimization based Deep Reinforcement Learning (DRL) controller enables adaptation to plume dynamics. Unreal Engine simulation evaluates performance under various smoke-wind scenarios, from steady flow to complex, unsteady fluctuations, showing that while the PID controller performs adequately in simpler scenarios, the DRL-based controller excels in more challenging environments. Field tests corroborate these findings. This system opens new possibilities for drone-based monitoring in areas like wildfire management and air quality assessment. The successful integration of DRL for real-time decision-making advances autonomous drone control for dynamic environments."
  },
  {
    "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
    "url": "http://arxiv.org/abs/2504.12609v1",
    "arxiv_id": "2504.12609v1",
    "authors": [
      "Tyler Ga Wei Lum",
      "Olivia Y. Lee",
      "C. Karen Liu",
      "Jeannette Bohg"
    ],
    "published": "2025-04-17T03:15:20+00:00",
    "summary": "Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels from videos and morphological differences between robot and human hands. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the human-robot embodiment gap without relying on wearables, teleoperation, or large-scale data collection typically necessary for imitation learning methods. From the demonstration, we extract two task-specific components: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward function, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. We found that these two components are highly effective for learning the desired task, eliminating the need for task-specific reward shaping and tuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop trajectory replay by 55% and imitation learning with data augmentation by 68% across grasping, non-prehensile manipulation, and multi-step tasks. Project Site: https://human2sim2robot.github.io"
  },
  {
    "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
    "url": "http://arxiv.org/abs/2504.12606v1",
    "arxiv_id": "2504.12606v1",
    "authors": [
      "Changsheng Lv",
      "Mengshi Qi",
      "Zijian Fu",
      "Huadong Ma"
    ],
    "published": "2025-04-17T03:09:22+00:00",
    "summary": "In this paper, we introduce a novel method named Robo-SGG, i.e., Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation. Compared to the existing SGG setting, the robust scene graph generation aims to perform inference on a diverse range of corrupted images, with the core challenge being the domain shift between the clean and corrupted images. Existing SGG methods suffer from degraded performance due to compromised visual features e.g., corruption interference or occlusions. To obtain robust visual features, we exploit the layout information, which is domain-invariant, to enhance the efficacy of existing SGG methods on corrupted images. Specifically, we employ Instance Normalization(IN) to filter out the domain-specific feature and recover the unchangeable structural features, i.e., the positional and semantic relationships among objects by the proposed Layout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder (LEE) that augments the existing object and predicate encoders within the SGG framework, enriching the robust positional and semantic features of objects and predicates. Note that our proposed Robo-SGG module is designed as a plug-and-play component, which can be easily integrated into any baseline SGG model. Extensive experiments demonstrate that by integrating the state-of-the-art method into our proposed Robo-SGG, we achieve relative improvements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet tasks on the VG-C dataset, respectively, and achieve new state-of-the-art performance in corruption scene graph generation benchmark (VG-C and GQA-C). We will release our source code and model."
  },
  {
    "title": "Evolutionary Policy Optimization",
    "url": "http://arxiv.org/abs/2504.12568v1",
    "arxiv_id": "2504.12568v1",
    "authors": [
      "Zelal Su \"Lain\" Mustafaoglu",
      "Keshav Pingali",
      "Risto Miikkulainen"
    ],
    "published": "2025-04-17T01:33:06+00:00",
    "summary": "A key challenge in reinforcement learning (RL) is managing the exploration-exploitation trade-off without sacrificing sample efficiency. Policy gradient (PG) methods excel in exploitation through fine-grained, gradient-based optimization but often struggle with exploration due to their focus on local search. In contrast, evolutionary computation (EC) methods excel in global exploration, but lack mechanisms for exploitation. To address these limitations, this paper proposes Evolutionary Policy Optimization (EPO), a hybrid algorithm that integrates neuroevolution with policy gradient methods for policy optimization. EPO leverages the exploration capabilities of EC and the exploitation strengths of PG, offering an efficient solution to the exploration-exploitation dilemma in RL. EPO is evaluated on the Atari Pong and Breakout benchmarks. Experimental results show that EPO improves both policy quality and sample efficiency compared to standard PG and EC methods, making it effective for tasks that require both exploration and local optimization."
  },
  {
    "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
    "url": "http://arxiv.org/abs/2504.12557v1",
    "arxiv_id": "2504.12557v1",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "published": "2025-04-17T01:11:08+00:00",
    "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks."
  },
  {
    "title": "Evaluating the Diversity and Quality of LLM Generated Content",
    "url": "http://arxiv.org/abs/2504.12522v1",
    "arxiv_id": "2504.12522v1",
    "authors": [
      "Alexander Shypula",
      "Shuo Li",
      "Botong Zhang",
      "Vishakh Padmakumar",
      "Kayo Yin",
      "Osbert Bastani"
    ],
    "published": "2025-04-16T23:02:23+00:00",
    "summary": "Recent work suggests that preference-tuning techniques--including Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO, as well as alternatives like DPO--reduce diversity, creating a dilemma given that such models are widely deployed in applications requiring diverse outputs. To address this, we introduce a framework for measuring effective semantic diversity--diversity among outputs that meet quality thresholds--which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: although preference-tuned models--especially those trained via RL--exhibit reduced lexical and syntactic diversity, they produce greater effective semantic diversity than SFT or base models, not from increasing diversity among high-quality outputs, but from generating more high-quality outputs overall. We discover that preference tuning reduces syntactic diversity while preserving semantic diversity--revealing a distinction between diversity in form and diversity in content that traditional metrics often overlook. Our analysis further shows that smaller models are consistently more parameter-efficient at generating unique content within a fixed sampling budget, offering insights into the relationship between model scaling and diversity. These findings have important implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation."
  },
  {
    "title": "Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2504.12501v1",
    "arxiv_id": "2504.12501v1",
    "authors": [
      "Nathan Lambert"
    ],
    "published": "2025-04-16T21:36:46+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF -- both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. The core of the book details every optimization stage in using RLHF, from starting with instruction tuning to training a reward model and finally all of rejection sampling, reinforcement learning, and direct alignment algorithms. The book concludes with advanced topics -- understudied research questions in synthetic data and evaluation -- and open questions for the field."
  },
  {
    "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.12216v1",
    "arxiv_id": "2504.12216v1",
    "authors": [
      "Siyan Zhao",
      "Devaansh Gupta",
      "Qinqing Zheng",
      "Aditya Grover"
    ],
    "published": "2025-04-16T16:08:45+00:00",
    "summary": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM."
  },
  {
    "title": "Generalized Visual Relation Detection with Diffusion Models",
    "url": "http://arxiv.org/abs/2504.12100v1",
    "arxiv_id": "2504.12100v1",
    "authors": [
      "Kaifeng Gao",
      "Siqi Chen",
      "Hanwang Zhang",
      "Jun Xiao",
      "Yueting Zhuang",
      "Qianru Sun"
    ],
    "published": "2025-04-16T14:03:24+00:00",
    "summary": "Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image. Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations. Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., ``ride'' can be depicted as ``race'' and ``sit on'', from the sports and spatial position views, respectively. To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD. We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence. During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention. After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities. Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets. To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning. Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD."
  },
  {
    "title": "Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework",
    "url": "http://arxiv.org/abs/2504.12090v1",
    "arxiv_id": "2504.12090v1",
    "authors": [
      "Jack Preuveneers",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "published": "2025-04-16T13:53:42+00:00",
    "summary": "We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights."
  },
  {
    "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization",
    "url": "http://arxiv.org/abs/2504.12083v1",
    "arxiv_id": "2504.12083v1",
    "authors": [
      "Pritam Sarkar",
      "Ali Etemad"
    ],
    "published": "2025-04-16T13:43:56+00:00",
    "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose a self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains a training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), a novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, short- and long-video understanding, and fine-grained temporal reasoning."
  },
  {
    "title": "pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild",
    "url": "http://arxiv.org/abs/2504.12045v1",
    "arxiv_id": "2504.12045v1",
    "authors": [
      "Jonas Myhre Schi\u00f8tt",
      "Viktor Sebastian Petersen",
      "Dimitrios P. Papadopoulos"
    ],
    "published": "2025-04-16T13:01:44+00:00",
    "summary": "Computer vision models have seen increased usage in sports, and reinforcement learning (RL) is famous for beating humans in strategic games such as Chess and Go. In this paper, we are interested in building upon these advances and examining the game of classic 8-ball pool. We introduce pix2pockets, a foundation for an RL-assisted pool coach. Given a single image of a pool table, we first aim to detect the table and the balls and then propose the optimal shot suggestion. For the first task, we build a dataset with 195 diverse images where we manually annotate all balls and table dots, leading to 5748 object segmentation masks. For the second task, we build a standardized RL environment that allows easy development and benchmarking of any RL algorithm. Our object detection model yields an AP50 of 91.2 while our ball location pipeline obtains an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set a baseline for the shot suggestion task and we show that all of them fail to pocket all balls without making a foul move. We also present a simple baseline that achieves a per-shot success rate of 94.7% and clears a full game in a single turn 30% of the time."
  },
  {
    "title": "Evolutionary Reinforcement Learning for Interpretable Decision-Making in Supply Chain Management",
    "url": "http://arxiv.org/abs/2504.12023v1",
    "arxiv_id": "2504.12023v1",
    "authors": [
      "Stefano Genetti",
      "Alberto Longobardi",
      "Giovanni Iacca"
    ],
    "published": "2025-04-16T12:28:35+00:00",
    "summary": "In the context of Industry 4.0, Supply Chain Management (SCM) faces challenges in adopting advanced optimization techniques due to the \"black-box\" nature of most AI-based solutions, which causes reluctance among company stakeholders. To overcome this issue, in this work, we employ an Interpretable Artificial Intelligence (IAI) approach that combines evolutionary computation with Reinforcement Learning (RL) to generate interpretable decision-making policies in the form of decision trees. This IAI solution is embedded within a simulation-based optimization framework specifically designed to handle the inherent uncertainties and stochastic behaviors of modern supply chains. To our knowledge, this marks the first attempt to combine IAI with simulation-based optimization for decision-making in SCM. The methodology is tested on two supply chain optimization problems, one fictional and one from the real world, and its performance is compared against widely used optimization and RL algorithms. The results reveal that the interpretable approach delivers competitive, and sometimes better, performance, challenging the prevailing notion that there must be a trade-off between interpretability and optimization efficiency. Additionally, the developed framework demonstrates strong potential for industrial applications, offering seamless integration with various Python-based algorithms."
  },
  {
    "title": "Control of Rayleigh-B\u00e9nard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime",
    "url": "http://arxiv.org/abs/2504.12000v1",
    "arxiv_id": "2504.12000v1",
    "authors": [
      "Thorben Markmann",
      "Michiel Straat",
      "Sebastian Peitz",
      "Barbara Hammer"
    ],
    "published": "2025-04-16T11:51:59+00:00",
    "summary": "Data-driven flow control has significant potential for industry, energy systems, and climate science. In this work, we study the effectiveness of Reinforcement Learning (RL) for reducing convective heat transfer in the 2D Rayleigh-B\\'enard Convection (RBC) system under increasing turbulence. We investigate the generalizability of control across varying initial conditions and turbulence levels and introduce a reward shaping technique to accelerate the training. RL agents trained via single-agent Proximal Policy Optimization (PPO) are compared to linear proportional derivative (PD) controllers from classical control theory. The RL agents reduced convection, measured by the Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly turbulent settings, clearly outperforming PD control in all settings. The agents showed strong generalization performance across different initial conditions and to a significant extent, generalized to higher degrees of turbulence. The reward shaping improved sample efficiency and consistently stabilized the Nusselt Number to higher turbulence levels."
  },
  {
    "title": "A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
    "url": "http://arxiv.org/abs/2504.11997v1",
    "arxiv_id": "2504.11997v1",
    "authors": [
      "Kihyuk Hong",
      "Ambuj Tewari"
    ],
    "published": "2025-04-16T11:47:41+00:00",
    "summary": "We study reinforcement learning in infinite-horizon average-reward settings with linear MDPs. Previous work addresses this problem by approximating the average-reward setting by discounted setting and employing a value iteration-based algorithm that uses clipping to constrain the span of the value function for improved statistical efficiency. However, the clipping procedure requires computing the minimum of the value function over the entire state space, which is prohibitive since the state space in linear MDP setting can be large or even infinite. In this paper, we introduce a value iteration method with efficient clipping operation that only requires computing the minimum of value functions over the set of states visited by the algorithm. Our algorithm enjoys the same regret bound as the previous work while being computationally efficient, with computational complexity that is independent of the size of the state space."
  },
  {
    "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
    "url": "http://arxiv.org/abs/2504.11967v1",
    "arxiv_id": "2504.11967v1",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Sanjian Zhang",
      "Guangyu Chen",
      "Yuzhi Hu",
      "Masumi Yano",
      "Jingdong Sun",
      "Siyu Huang",
      "Feng Liu",
      "Qi Dai",
      "Zhi-Qi Cheng"
    ],
    "published": "2025-04-16T10:58:33+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs."
  },
  {
    "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
    "url": "http://arxiv.org/abs/2504.11967v2",
    "arxiv_id": "2504.11967v2",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Sanjian Zhang",
      "Guangyu Chen",
      "Yuzhi Hu",
      "Masumi Yano",
      "Jingdong Sun",
      "Siyu Huang",
      "Feng Liu",
      "Qi Dai",
      "Zhi-Qi Cheng"
    ],
    "published": "2025-04-16T10:58:33+00:00",
    "summary": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs."
  },
  {
    "title": "R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors",
    "url": "http://arxiv.org/abs/2504.11946v1",
    "arxiv_id": "2504.11946v1",
    "authors": [
      "Haoyang Wang",
      "Liming Liu",
      "Peiheng Wang",
      "Junlin Hao",
      "Jiangkai Wu",
      "Xinggong Zhang"
    ],
    "published": "2025-04-16T10:23:59+00:00",
    "summary": "Mesh reconstruction from multi-view images is a fundamental problem in computer vision, but its performance degrades significantly under sparse-view conditions, especially in unseen regions where no ground-truth observations are available. While recent advances in diffusion models have demonstrated strong capabilities in synthesizing novel views from limited inputs, their outputs often suffer from visual artifacts and lack 3D consistency, posing challenges for reliable mesh optimization. In this paper, we propose a novel framework that leverages diffusion models to enhance sparse-view mesh reconstruction in a principled and reliable manner. To address the instability of diffusion outputs, we propose a Consensus Diffusion Module that filters unreliable generations via interquartile range (IQR) analysis and performs variance-aware image fusion to produce robust pseudo-supervision. Building on this, we design an online reinforcement learning strategy based on the Upper Confidence Bound (UCB) to adaptively select the most informative viewpoints for enhancement, guided by diffusion loss. Finally, the fused images are used to jointly supervise a NeRF-based model alongside sparse-view ground truth, ensuring consistency across both geometry and appearance. Extensive experiments demonstrate that our method achieves significant improvements in both geometric quality and rendering quality."
  },
  {
    "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11944v1",
    "arxiv_id": "2504.11944v1",
    "authors": [
      "Xuyang Chen",
      "Guojian Wang",
      "Keyu Yan",
      "Lin Zhao"
    ],
    "published": "2025-04-16T10:23:44+00:00",
    "summary": "Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable. In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. It offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy. As a result, VIPO achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks."
  },
  {
    "title": "AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection",
    "url": "http://arxiv.org/abs/2504.11914v1",
    "arxiv_id": "2504.11914v1",
    "authors": [
      "Yuhao Chao",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ],
    "published": "2025-04-16T09:48:41+00:00",
    "summary": "Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data."
  },
  {
    "title": "A Graph-Based Reinforcement Learning Approach with Frontier Potential Based Reward for Safe Cluttered Environment Exploration",
    "url": "http://arxiv.org/abs/2504.11907v1",
    "arxiv_id": "2504.11907v1",
    "authors": [
      "Gabriele Calzolari",
      "Vidya Sumathy",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "published": "2025-04-16T09:31:14+00:00",
    "summary": "Autonomous exploration of cluttered environments requires efficient exploration strategies that guarantee safety against potential collisions with unknown random obstacles. This paper presents a novel approach combining a graph neural network-based exploration greedy policy with a safety shield to ensure safe navigation goal selection. The network is trained using reinforcement learning and the proximal policy optimization algorithm to maximize exploration efficiency while reducing the safety shield interventions. However, if the policy selects an infeasible action, the safety shield intervenes to choose the best feasible alternative, ensuring system consistency. Moreover, this paper proposes a reward function that includes a potential field based on the agent's proximity to unexplored regions and the expected information gain from reaching them. Overall, the approach investigated in this paper merges the benefits of the adaptability of reinforcement learning-driven exploration policies and the guarantee ensured by explicit safety mechanisms. Extensive evaluations in simulated environments demonstrate that the approach enables efficient and safe exploration in cluttered environments."
  },
  {
    "title": "Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization",
    "url": "http://arxiv.org/abs/2504.11874v1",
    "arxiv_id": "2504.11874v1",
    "authors": [
      "Ruoyu Sun",
      "Angelos Stefanidis",
      "Zhengyong Jiang",
      "Jionglong Su"
    ],
    "published": "2025-04-16T08:51:09+00:00",
    "summary": "Typical deep reinforcement learning (DRL) agents for dynamic portfolio optimization learn the factors influencing portfolio return and risk by analyzing the output values of the reward function while adjusting portfolio weights within the training environment. However, it faces a major limitation where it is difficult for investors to intervene in the training based on different levels of risk aversion towards each portfolio asset. This difficulty arises from another limitation: existing DRL agents may not develop a thorough understanding of the factors responsible for the portfolio return and risk by only learning from the output of the reward function. As a result, the strategy for determining the target portfolio weights is entirely dependent on the DRL agents themselves. To address these limitations, we propose a reward factor matrix for elucidating the return and risk of each asset in the portfolio. Additionally, we propose a novel learning system named Factor-MCLS using a multi-critic framework that facilitates learning of the reward factor matrix. In this way, our DRL-based learning system can effectively learn the factors influencing portfolio return and risk. Moreover, based on the critic networks within the multi-critic framework, we develop a risk constraint term in the training objective function of the policy function. This risk constraint term allows investors to intervene in the training of the DRL agent according to their individual levels of risk aversion towards the portfolio assets."
  },
  {
    "title": "Steerable rolling of a 1-DoF robot using an internal pendulum",
    "url": "http://arxiv.org/abs/2504.11748v1",
    "arxiv_id": "2504.11748v1",
    "authors": [
      "Christopher Y. Xu",
      "Jack Yan",
      "Kathleen Lum",
      "Justin K. Yim"
    ],
    "published": "2025-04-16T03:59:30+00:00",
    "summary": "We present ROCK (Rolling One-motor Controlled rocK), a 1 degree-of-freedom robot consisting of a round shell and an internal pendulum. An uneven shell surface enables steering by using only the movement of the pendulum, allowing for mechanically simple designs that may be feasible to scale to large quantities or small sizes. We train a control policy using reinforcement learning in simulation and deploy it onto the robot to complete a rectangular trajectory."
  },
  {
    "title": "Hardware Co-Designed Optimal Control for Programmable Atomic Quantum Processors via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11737v1",
    "arxiv_id": "2504.11737v1",
    "authors": [
      "Qian Ding",
      "Dirk Englund"
    ],
    "published": "2025-04-16T03:30:40+00:00",
    "summary": "Developing scalable, fault-tolerant atomic quantum processors requires precise control over large arrays of optical beams. This remains a major challenge due to inherent imperfections in classical control hardware, such as inter-channel crosstalk and beam leakage. In this work, we introduce a hardware co-designed intelligent quantum control framework to address these limitations. We construct a mathematical model of the photonic control hardware, integrate it into the quantum optimal control (QOC) framework, and apply reinforcement learning (RL) techniques to discover optimal control strategies. We demonstrate that the proposed framework enables robust, high-fidelity parallel single-qubit gate operations under realistic control conditions, where each atom is individually addressed by an optical beam. Specifically, we implement and benchmark three optimization strategies: a classical hybrid Self-Adaptive Differential Evolution-Adam (SADE-Adam) optimizer, a conventional RL approach based on Proximal Policy Optimization (PPO), and a novel end-to-end differentiable RL method. Using SADE-Adam as a baseline, we find that while PPO performance degrades as system complexity increases, the end-to-end differentiable RL consistently achieves gate fidelities above 99.9$\\%$, exhibits faster convergence, and maintains robustness under varied channel crosstalk strength and randomized dynamic control imperfections."
  },
  {
    "title": "EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos",
    "url": "http://arxiv.org/abs/2504.11732v1",
    "arxiv_id": "2504.11732v1",
    "authors": [
      "Jilan Xu",
      "Yifei Huang",
      "Baoqi Pei",
      "Junlin Hou",
      "Qingqiu Li",
      "Guo Chen",
      "Yuejie Zhang",
      "Rui Feng",
      "Weidi Xie"
    ],
    "published": "2025-04-16T03:12:39+00:00",
    "summary": "Generating videos in the first-person perspective has broad application prospects in the field of augmented reality and embodied intelligence. In this work, we explore the cross-view video prediction task, where given an exo-centric video, the first frame of the corresponding ego-centric video, and textual instructions, the goal is to generate futur frames of the ego-centric video. Inspired by the notion that hand-object interactions (HOI) in ego-centric videos represent the primary intentions and actions of the current actor, we present EgoExo-Gen that explicitly models the hand-object dynamics for cross-view video prediction. EgoExo-Gen consists of two stages. First, we design a cross-view HOI mask prediction model that anticipates the HOI masks in future ego-frames by modeling the spatio-temporal ego-exo correspondence. Next, we employ a video diffusion model to predict future ego-frames using the first ego-frame and textual instructions, while incorporating the HOI masks as structural guidance to enhance prediction quality. To facilitate training, we develop an automated pipeline to generate pseudo HOI masks for both ego- and exo-videos by exploiting vision foundation models. Extensive experiments demonstrate that our proposed EgoExo-Gen achieves better prediction performance compared to previous video prediction models on the Ego-Exo4D and H2O benchmark datasets, with the HOI masks significantly improving the generation of hands and interactive objects in the ego-centric videos."
  },
  {
    "title": "Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids",
    "url": "http://arxiv.org/abs/2504.11650v1",
    "arxiv_id": "2504.11650v1",
    "authors": [
      "Shengyuan Yan",
      "Farzad Vazinram",
      "Zeynab Kaseb",
      "Lindsay Spoor",
      "Jochen Stiasny",
      "Betul Mamudi",
      "Amirhossein Heydarian Ardakani",
      "Ugochukwu Orji",
      "Pedro P. Vergara",
      "Yu Xiang",
      "Jerry Guo"
    ],
    "published": "2025-04-15T22:37:55+00:00",
    "summary": "Power flow (PF) calculations are fundamental to power system analysis to ensure stable and reliable grid operation. The Newton-Raphson (NR) method is commonly used for PF analysis due to its rapid convergence when initialized properly. However, as power grids operate closer to their capacity limits, ill-conditioned cases and convergence issues pose significant challenges. This work, therefore, addresses these challenges by proposing strategies to improve NR initialization, hence minimizing iterations and avoiding divergence. We explore three approaches: (i) an analytical method that estimates the basin of attraction using mathematical bounds on voltages, (ii) Two data-driven models leveraging supervised learning or physics-informed neural networks (PINNs) to predict optimal initial guesses, and (iii) a reinforcement learning (RL) approach that incrementally adjusts voltages to accelerate convergence. These methods are tested on benchmark systems. This research is particularly relevant for modern power systems, where high penetration of renewables and decentralized generation require robust and scalable PF solutions. In experiments, all three proposed methods demonstrate a strong ability to provide an initial guess for Newton-Raphson method to converge with fewer steps. The findings provide a pathway for more efficient real-time grid operations, which, in turn, support the transition toward smarter and more resilient electricity networks."
  },
  {
    "title": "Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling",
    "url": "http://arxiv.org/abs/2504.11645v1",
    "arxiv_id": "2504.11645v1",
    "authors": [
      "Feng Zhu",
      "Aritra Mitra",
      "Robert W. Heath"
    ],
    "published": "2025-04-15T22:13:55+00:00",
    "summary": "Motivated by collaborative reinforcement learning (RL) and optimization with time-correlated data, we study a generic federated stochastic approximation problem involving $M$ agents, where each agent is characterized by an agent-specific (potentially nonlinear) local operator. The goal is for the agents to communicate intermittently via a server to find the root of the average of the agents' local operators. The generality of our setting stems from allowing for (i) Markovian data at each agent and (ii) heterogeneity in the roots of the agents' local operators. The limited recent work that has accounted for both these features in a federated setting fails to guarantee convergence to the desired point or to show any benefit of collaboration; furthermore, they rely on projection steps in their algorithms to guarantee bounded iterates. Our work overcomes each of these limitations. We develop a novel algorithm titled \\texttt{FedHSA}, and prove that it guarantees convergence to the correct point, while enjoying an $M$-fold linear speedup in sample-complexity due to collaboration. To our knowledge, \\emph{this is the first finite-time result of its kind}, and establishing it (without relying on a projection step) entails a fairly intricate argument that accounts for the interplay between complex temporal correlations due to Markovian sampling, multiple local steps to save communication, and the drift-effects induced by heterogeneous local operators. Our results have implications for a broad class of heterogeneous federated RL problems (e.g., policy evaluation and control) with function approximation, where the agents' Markov decision processes can differ in their probability transition kernels and reward functions."
  },
  {
    "title": "Dueling Deep Reinforcement Learning for Financial Time Series",
    "url": "http://arxiv.org/abs/2504.11601v1",
    "arxiv_id": "2504.11601v1",
    "authors": [
      "Bruno Giorgio"
    ],
    "published": "2025-04-15T20:30:34+00:00",
    "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for solving decision-making problems in dynamic environments. In this research, we explore the application of Double DQN (DDQN) and Dueling Network Architectures, to financial trading tasks using historical SP500 index data. Our focus is training agents capable of optimizing trading strategies while accounting for practical constraints such as transaction costs. The study evaluates the model performance across scenarios with and without commissions, highlighting the impact of cost-sensitive environments on reward dynamics. Despite computational limitations and the inherent complexity of financial time series data, the agent successfully learned meaningful trading policies. The findings confirm that RL agents, even when trained on limited datasets, can outperform random strategies by leveraging advanced architectures such as DDQN and Dueling Networks. However, significant challenges persist, particularly with a sub-optimal policy due to the complexity of data source."
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Decentralized Reservoir Management via Murmuration Intelligence",
    "url": "http://arxiv.org/abs/2504.11569v1",
    "arxiv_id": "2504.11569v1",
    "authors": [
      "Heming Fu",
      "Guojun Xiong",
      "Jian Li",
      "Shan Lin"
    ],
    "published": "2025-04-15T19:21:35+00:00",
    "summary": "Conventional centralized water management systems face critical limitations from computational complexity and uncertainty propagation. We present MurmuRL, a novel decentralized framework inspired by starling murmurations intelligence, integrating bio-inspired alignment, separation, and cohesion rules with multi-agent reinforcement learning. MurmuRL enables individual reservoirs to make autonomous local decisions while achieving emergent global coordination. Experiments on grid networks demonstrate that MurmuRL achieves 8.8% higher final performance while using 27% less computing overhead compared to centralized approaches. Notably, strategic diversity scales super-linearly with system size, exhibiting sophisticated coordination patterns and enhanced resilience during extreme events. MurmuRL offers a scalable solution for managing complex water systems by leveraging principles of natural collective behavior."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v1",
    "arxiv_id": "2504.11536v1",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
    "url": "http://arxiv.org/abs/2504.11536v2",
    "arxiv_id": "2504.11536v2",
    "authors": [
      "Jiazhan Feng",
      "Shijue Huang",
      "Xingwei Qu",
      "Ge Zhang",
      "Yujia Qin",
      "Baoquan Zhong",
      "Chengquan Jiang",
      "Jinxin Chi",
      "Wanjun Zhong"
    ],
    "published": "2025-04-15T18:10:22+00:00",
    "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems."
  },
  {
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning",
    "url": "http://arxiv.org/abs/2504.11456v1",
    "arxiv_id": "2504.11456v1",
    "authors": [
      "Zhiwei He",
      "Tian Liang",
      "Jiahao Xu",
      "Qiuzhi Liu",
      "Xingyu Chen",
      "Yue Wang",
      "Linfeng Song",
      "Dian Yu",
      "Zhenwen Liang",
      "Wenxuan Wang",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-04-15T17:59:51+00:00",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath."
  },
  {
    "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL",
    "url": "http://arxiv.org/abs/2504.11455v1",
    "arxiv_id": "2504.11455v1",
    "authors": [
      "Junke Wang",
      "Zhi Tian",
      "Xun Wang",
      "Xinyu Zhang",
      "Weilin Huang",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "published": "2025-04-15T17:59:46+00:00",
    "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR."
  },
  {
    "title": "A Clean Slate for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11453v1",
    "arxiv_id": "2504.11453v1",
    "authors": [
      "Matthew Thomas Jackson",
      "Uljad Berdica",
      "Jarek Liesen",
      "Shimon Whiteson",
      "Jakob Nicolaus Foerster"
    ],
    "published": "2025-04-15T17:59:05+00:00",
    "summary": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches within a single, comprehensive hyperparameter space, enabling algorithm development in a shared hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral."
  },
  {
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion",
    "url": "http://arxiv.org/abs/2504.11447v1",
    "arxiv_id": "2504.11447v1",
    "authors": [
      "An Zhaol",
      "Shengyuan Zhang",
      "Ling Yang",
      "Zejian Li",
      "Jiale Wu",
      "Haoran Xu",
      "AnYang Wei",
      "Perry Pengyun GU Lingyun Sun"
    ],
    "published": "2025-04-15T17:57:13+00:00",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO."
  },
  {
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion",
    "url": "http://arxiv.org/abs/2504.11447v2",
    "arxiv_id": "2504.11447v2",
    "authors": [
      "An Zhao",
      "Shengyuan Zhang",
      "Ling Yang",
      "Zejian Li",
      "Jiale Wu",
      "Haoran Xu",
      "AnYang Wei",
      "Perry Pengyun GU",
      "Lingyun Sun"
    ],
    "published": "2025-04-15T17:57:13+00:00",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO."
  },
  {
    "title": "Embodied World Models Emerge from Navigational Task in Open-Ended Environments",
    "url": "http://arxiv.org/abs/2504.11419v1",
    "arxiv_id": "2504.11419v1",
    "authors": [
      "Li Jin",
      "Liu Jia"
    ],
    "published": "2025-04-15T17:35:13+00:00",
    "summary": "Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agent's neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios."
  },
  {
    "title": "Measures of Variability for Risk-averse Policy Gradient",
    "url": "http://arxiv.org/abs/2504.11412v1",
    "arxiv_id": "2504.11412v1",
    "authors": [
      "Yudong Luo",
      "Yangchen Pan",
      "Jiaqi Tan",
      "Pascal Poupart"
    ],
    "published": "2025-04-15T17:28:15+00:00",
    "summary": "Risk-averse reinforcement learning (RARL) is critical for decision-making under uncertainty, which is especially valuable in high-stake applications. However, most existing works focus on risk measures, e.g., conditional value-at-risk (CVaR), while measures of variability remain underexplored. In this paper, we comprehensively study nine common measures of variability, namely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation, Standard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and Semi_Standard Deviation. Among them, four metrics have not been previously studied in RARL. We derive policy gradient formulas for these unstudied metrics, improve gradient estimation for Gini Deviation, analyze their gradient properties, and incorporate them with the REINFORCE and PPO frameworks to penalize the dispersion of returns.   Our empirical study reveals that variance-based metrics lead to unstable policy updates. In contrast, CVaR Deviation and Gini Deviation show consistent performance across different randomness and evaluation domains, achieving high returns while effectively learning risk-averse policies. Mean Deviation and Semi_Standard Deviation are also competitive across different scenarios. This work provides a comprehensive overview of variability measures in RARL, offering practical insights for risk-aware decision-making and guiding future research on risk metrics and RARL algorithms."
  },
  {
    "title": "Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11354v1",
    "arxiv_id": "2504.11354v1",
    "authors": [
      "Haiming Wang",
      "Mert Unsal",
      "Xiaohan Lin",
      "Mantas Baksys",
      "Junqi Liu",
      "Marco Dos Santos",
      "Flood Sung",
      "Marina Vinyes",
      "Zhenzhe Ying",
      "Zekai Zhu",
      "Jianqiao Lu",
      "Hugues de Saxc\u00e9",
      "Bolton Bailey",
      "Chendong Song",
      "Chenjun Xiao",
      "Dehao Zhang",
      "Ebony Zhang",
      "Frederick Pu",
      "Han Zhu",
      "Jiawei Liu",
      "Jonas Bayer",
      "Julien Michel",
      "Longhui Yu",
      "L\u00e9o Dreyfus-Schmidt",
      "Lewis Tunstall",
      "Luigi Pagani",
      "Moreira Machado",
      "Pauline Bourigault",
      "Ran Wang",
      "Stanislas Polu",
      "Thibaut Barroyer",
      "Wen-Ding Li",
      "Yazhe Niu",
      "Yann Fleureau",
      "Yangyang Hu",
      "Zhouliang Yu",
      "Zihan Wang",
      "Zhilin Yang",
      "Zhengying Liu",
      "Jia Li"
    ],
    "published": "2025-04-15T16:23:44+00:00",
    "summary": "We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \\textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover"
  },
  {
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
    "url": "http://arxiv.org/abs/2504.11343v1",
    "arxiv_id": "2504.11343v1",
    "authors": [
      "Wei Xiong",
      "Jiarui Yao",
      "Yuhui Xu",
      "Bo Pang",
      "Lei Wang",
      "Doyen Sahoo",
      "Junnan Li",
      "Nan Jiang",
      "Tong Zhang",
      "Caiming Xiong",
      "Hanze Dong"
    ],
    "published": "2025-04-15T16:15:02+00:00",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training."
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets",
    "url": "http://arxiv.org/abs/2504.11258v1",
    "arxiv_id": "2504.11258v1",
    "authors": [
      "Liam Welsh",
      "Udit Grover",
      "Sebastian Jaimungal"
    ],
    "published": "2025-04-15T14:56:42+00:00",
    "summary": "Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments."
  },
  {
    "title": "A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes",
    "url": "http://arxiv.org/abs/2504.11250v1",
    "arxiv_id": "2504.11250v1",
    "authors": [
      "Jeroen Middelhuis",
      "Zaharah Bukhsh",
      "Ivo Adan",
      "Remco Dijkman"
    ],
    "published": "2025-04-15T14:46:58+00:00",
    "summary": "Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the mean cycle time. Maximizing our reward function guarantees that the objective function is minimized without requiring extensive reward engineering. The results show that our method consistently learns the optimal policy in all six evaluated business processes, outperforming the state-of-the-art algorithm that can only learn the optimal policy in two of the evaluated processes."
  },
  {
    "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks",
    "url": "http://arxiv.org/abs/2504.11247v1",
    "arxiv_id": "2504.11247v1",
    "authors": [
      "Fikrican \u00d6zg\u00fcr",
      "Ren\u00e9 Zurbr\u00fcgg",
      "Suryansh Kumar"
    ],
    "published": "2025-04-15T14:45:51+00:00",
    "summary": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art algorithm for achieving sample-efficient multi-goal reinforcement learning (RL) in robotic manipulation tasks with binary rewards. HER facilitates learning from failed attempts by replaying trajectories with redefined goals. However, it relies on a heuristic-based replay method that lacks a principled framework. To address this limitation, we introduce a novel replay strategy, \"Next-Future\", which focuses on rewarding single-step transitions. This approach significantly enhances sample efficiency and accuracy in learning multi-goal Markov decision processes (MDPs), particularly under stringent accuracy requirements -- a critical aspect for performing complex and precise robotic-arm tasks. We demonstrate the efficacy of our method by highlighting how single-step learning enables improved value approximation within the multi-goal RL framework. The performance of the proposed replay strategy is evaluated across eight challenging robotic manipulation tasks, using ten random seeds for training. Our results indicate substantial improvements in sample efficiency for seven out of eight tasks and higher success rates in six tasks. Furthermore, real-world experiments validate the practical feasibility of the learned policies, demonstrating the potential of \"Next-Future\" in solving complex robotic-arm tasks."
  },
  {
    "title": "Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay",
    "url": "http://arxiv.org/abs/2504.11118v1",
    "arxiv_id": "2504.11118v1",
    "authors": [
      "Henrik Krauss",
      "Takehisa Yairi"
    ],
    "published": "2025-04-15T12:07:14+00:00",
    "summary": "This study introduces a novel method for revealing human covert attention patterns using gameplay data alone, utilizing offline attention techniques from reinforcement learning (RL). We propose the contextualized, task-relevant (CTR) attention network, which generates attention maps from both human and RL agent gameplay in Atari environments. These maps are sparse yet retain the necessary information for the current player's decision making. We compare the CTR-derived attention maps with a temporally integrated overt attention (TIOA) model based on eye-tracking data, serving as a point of comparison and discussion. Visual inspection reveals distinct attention patterns: human CTR maps focus on the player and rather nearby opponents, occasionally shifting between stronger focus and broader views - sometimes even attending to empty space ahead. In contrast, agent maps maintain a consistent broad focus on most objects, including distant ones and the player. Quantitative analysis further demonstrates that human CTR maps align more closely with TIOA than agent maps do. Our findings indicate that the CTR attention network can effectively reveal human covert attention patterns from gameplay alone, without the need for additional data like brain activity recordings. This work contributes to understanding human-agent attention differences and enables the development of RL agents augmented with human covert attention."
  },
  {
    "title": "Breaking the Dimensional Barrier: A Pontryagin-Guided Direct Policy Optimization for Continuous-Time Multi-Asset Portfolio",
    "url": "http://arxiv.org/abs/2504.11116v1",
    "arxiv_id": "2504.11116v1",
    "authors": [
      "Jeonggyu Huh",
      "Jaegi Jeon",
      "Hyeng Keun Koo"
    ],
    "published": "2025-04-15T12:03:14+00:00",
    "summary": "Solving large-scale, continuous-time portfolio optimization problems involving numerous assets and state-dependent dynamics has long been challenged by the curse of dimensionality. Traditional dynamic programming and PDE-based methods, while rigorous, typically become computationally intractable beyond a small number of state variables (often limited to ~3-6 in prior numerical studies). To overcome this critical barrier, we introduce the \\emph{Pontryagin-Guided Direct Policy Optimization} (PG-DPO) framework. PG-DPO leverages Pontryagin's Maximum Principle to directly guide neural network policies via backpropagation-through-time, naturally incorporating exogenous state processes without requiring dense state grids. Crucially, our computationally efficient ``Two-Stage'' variant exploits rapidly stabilizing costate estimates derived from BPTT, converting them into near-optimal closed-form Pontryagin controls after only a short warm-up, significantly reducing training overhead. This enables a breakthrough in scalability: numerical experiments demonstrate that PG-DPO successfully tackles problems with dimensions previously considered far out of reach, optimizing portfolios with up to 50 assets and 10 state variables. The framework delivers near-optimal policies, offering a practical and powerful alternative for high-dimensional continuous-time portfolio choice."
  },
  {
    "title": "Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs",
    "url": "http://arxiv.org/abs/2504.11511v1",
    "arxiv_id": "2504.11511v1",
    "authors": [
      "Flint Xiaofeng Fan",
      "Cheston Tan",
      "Roger Wattenhofer",
      "Yew-Soon Ong"
    ],
    "published": "2025-04-15T10:45:55+00:00",
    "summary": "The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems."
  },
  {
    "title": "Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models",
    "url": "http://arxiv.org/abs/2504.11054v1",
    "arxiv_id": "2504.11054v1",
    "authors": [
      "Andrea Tirinzoni",
      "Ahmed Touati",
      "Jesse Farebrother",
      "Mateusz Guzek",
      "Anssi Kanervisto",
      "Yingchen Xu",
      "Alessandro Lazaric",
      "Matteo Pirotta"
    ],
    "published": "2025-04-15T10:41:11+00:00",
    "summary": "Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior datasets. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines."
  },
  {
    "title": "A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications",
    "url": "http://arxiv.org/abs/2504.11026v1",
    "arxiv_id": "2504.11026v1",
    "authors": [
      "Alexandru Vasilache",
      "Jona Scholz",
      "Vincent Schilling",
      "Sven Nitzsche",
      "Florian Kaelber",
      "Johannes Korsch",
      "Juergen Becker"
    ],
    "published": "2025-04-15T09:50:03+00:00",
    "summary": "Spiking Neural Networks (SNNs) offer promising energy efficiency advantages, particularly when processing sparse spike trains. However, their incompatibility with traditional datasets, which consist of batches of input vectors rather than spike trains, necessitates the development of efficient encoding methods. This paper introduces a novel, open-source PyTorch-compatible Python framework for spike encoding, designed for neuromorphic applications in machine learning and reinforcement learning. The framework supports a range of encoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward (SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well as specialized encoding strategies covering population coding and reinforcement learning scenarios. Furthermore, we investigate the performance trade-offs of each method on embedded hardware using C/C++ implementations, considering energy consumption, computation time, spike sparsity, and reconstruction accuracy. Our findings indicate that SF typically achieves the lowest reconstruction error and offers the highest energy efficiency and fastest encoding speed, achieving the second-best spike sparsity. At the same time, other methods demonstrate particular strengths depending on the signal characteristics. This framework and the accompanying empirical analysis provide valuable resources for selecting optimal encoding strategies for energy-efficient SNN applications."
  },
  {
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "url": "http://arxiv.org/abs/2504.11001v1",
    "arxiv_id": "2504.11001v1",
    "authors": [
      "Alan Dao",
      "Thinh Le"
    ],
    "published": "2025-04-15T09:18:21+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient."
  },
  {
    "title": "AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent",
    "url": "http://arxiv.org/abs/2504.10978v1",
    "arxiv_id": "2504.10978v1",
    "authors": [
      "Pu Wang",
      "Zhihua Zhang",
      "Dianjie Lu",
      "Guijuan Zhang",
      "Youshan Zhang",
      "Zhuoran Zheng"
    ],
    "published": "2025-04-15T08:39:35+00:00",
    "summary": "Since human and environmental factors interfere, captured polyp images usually suffer from issues such as dim lighting, blur, and overexposure, which pose challenges for downstream polyp segmentation tasks. To address the challenges of noise-induced degradation in polyp images, we present AgentPolyp, a novel framework integrating CLIP-based semantic guidance and dynamic image enhancement with a lightweight neural network for segmentation. The agent first evaluates image quality using CLIP-driven semantic analysis (e.g., identifying ``low-contrast polyps with vascular textures\") and adapts reinforcement learning strategies to dynamically apply multi-modal enhancement operations (e.g., denoising, contrast adjustment). A quality assessment feedback loop optimizes pixel-level enhancement and segmentation focus in a collaborative manner, ensuring robust preprocessing before neural network segmentation. This modular architecture supports plug-and-play extensions for various enhancement algorithms and segmentation networks, meeting deployment requirements for endoscopic devices."
  },
  {
    "title": "Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.11506v1",
    "arxiv_id": "2504.11506v1",
    "authors": [
      "Hongliang Lu",
      "Shuqi Shen",
      "Junjie Yang",
      "Chao Lu",
      "Xinhu Zheng",
      "Hai Yang"
    ],
    "published": "2025-04-15T08:22:35+00:00",
    "summary": "More than the adherence to specific traffic regulations, driving culture touches upon a more implicit part - an informal, conventional, collective behavioral pattern followed by drivers - that varies across countries, regions, and even cities. Such cultural divergence has become one of the biggest challenges in deploying autonomous vehicles (AVs) across diverse regions today. The current emergence of data-driven methods has shown a potential solution to enable culture-compatible driving through learning from data, but what if some underdeveloped regions cannot provide sufficient local data to inform driving culture? This issue is particularly significant for a broader global AV market. Here, we propose a cross-cultural deployment scheme for AVs, called data-light inverse reinforcement learning, designed to re-calibrate culture-specific AVs and assimilate them into other cultures. First, we report the divergence in driving cultures through a comprehensive comparative analysis of naturalistic driving datasets on highways from three countries: Germany, China, and the USA. Then, we demonstrate the effectiveness of our scheme by testing the expeditious cross-cultural deployment across these three countries, with cumulative testing mileage of over 56084 km. The performance is particularly advantageous when cross-cultural deployment is carried out without affluent local data. Results show that we can reduce the dependence on local data by a margin of 98.67% at best. This study is expected to bring a broader, fairer AV global market, particularly in those regions that lack enough local data to develop culture-compatible AVs."
  },
  {
    "title": "Efficient Reasoning Models: A Survey",
    "url": "http://arxiv.org/abs/2504.10903v1",
    "arxiv_id": "2504.10903v1",
    "authors": [
      "Sicheng Feng",
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "published": "2025-04-15T06:28:00+00:00",
    "summary": "Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this \"slow-thinking\" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository."
  },
  {
    "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control",
    "url": "http://arxiv.org/abs/2504.10831v1",
    "arxiv_id": "2504.10831v1",
    "authors": [
      "Hyojun Ahn",
      "Seungcheol Oh",
      "Gyu Seon Kim",
      "Soyi Jung",
      "Soohyun Park",
      "Joongheon Kim"
    ],
    "published": "2025-04-15T03:21:08+00:00",
    "summary": "This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics."
  },
  {
    "title": "AdapCsiNet: Environment-Adaptive CSI Feedback via Scene Graph-Aided Deep Learning",
    "url": "http://arxiv.org/abs/2504.10798v1",
    "arxiv_id": "2504.10798v1",
    "authors": [
      "Jiayi Liu",
      "Jiajia Guo",
      "Yiming Cui",
      "Chao-Kai Wen",
      "Shi Jin"
    ],
    "published": "2025-04-15T01:51:15+00:00",
    "summary": "Accurate channel state information (CSI) is critical for realizing the full potential of multiple-antenna wireless communication systems. While deep learning (DL)-based CSI feedback methods have shown promise in reducing feedback overhead, their generalization capability across varying propagation environments remains limited due to their data-driven nature. Existing solutions based on online training improve adaptability but impose significant overhead in terms of data collection and computational resources. In this work, we propose AdapCsiNet, an environment-adaptive DL-based CSI feedback framework that eliminates the need for online training. By integrating environmental information -- represented as a scene graph -- into a hypernetwork-guided CSI reconstruction process, AdapCsiNet dynamically adapts to diverse channel conditions. A two-step training strategy is introduced to ensure baseline reconstruction performance and effective environment-aware adaptation. Simulation results demonstrate that AdapCsiNet achieves up to 46.4% improvement in CSI reconstruction accuracy and matches the performance of online learning methods without incurring additional runtime overhead."
  },
  {
    "title": "Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.10677v1",
    "arxiv_id": "2504.10677v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-04-14T19:57:03+00:00",
    "summary": "In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination."
  },
  {
    "title": "Better Estimation of the KL Divergence Between Language Models",
    "url": "http://arxiv.org/abs/2504.10637v1",
    "arxiv_id": "2504.10637v1",
    "authors": [
      "Afra Amini",
      "Tim Vieira",
      "Ryan Cotterell"
    ],
    "published": "2025-04-14T18:40:02+00:00",
    "summary": "Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient."
  },
  {
    "title": "Weight Ensembling Improves Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2504.10478v1",
    "arxiv_id": "2504.10478v1",
    "authors": [
      "Xingyu Dang",
      "Christina Baek",
      "Kaiyue Wen",
      "Zico Kolter",
      "Aditi Raghunathan"
    ],
    "published": "2025-04-14T17:59:07+00:00",
    "summary": "We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance."
  },
  {
    "title": "Weight Ensembling Improves Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2504.10478v2",
    "arxiv_id": "2504.10478v2",
    "authors": [
      "Xingyu Dang",
      "Christina Baek",
      "Kaiyue Wen",
      "Zico Kolter",
      "Aditi Raghunathan"
    ],
    "published": "2025-04-14T17:59:07+00:00",
    "summary": "We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance."
  },
  {
    "title": "Co-optimizing Physical Reconfiguration Parameters and Controllers for an Origami-inspired Reconfigurable Manipulator",
    "url": "http://arxiv.org/abs/2504.10474v1",
    "arxiv_id": "2504.10474v1",
    "authors": [
      "Zhe Chen",
      "Li Chen",
      "Hao Zhang",
      "Jianguo Zhao"
    ],
    "published": "2025-04-14T17:56:38+00:00",
    "summary": "Reconfigurable robots that can change their physical configuration post-fabrication have demonstrate their potential in adapting to different environments or tasks. However, it is challenging to determine how to optimally adjust reconfigurable parameters for a given task, especially when the controller depends on the robot's configuration. In this paper, we address this problem using a tendon-driven reconfigurable manipulator composed of multiple serially connected origami-inspired modules as an example. Under tendon actuation, these modules can achieve different shapes and motions, governed by joint stiffnesses (reconfiguration parameters) and the tendon displacements (control inputs). We leverage recent advances in co-optimization of design and control for robotic system to treat reconfiguration parameters as design variables and optimize them using reinforcement learning techniques. We first establish a forward model based on the minimum potential energy method to predict the shape of the manipulator under tendon actuations. Using the forward model as the environment dynamics, we then co-optimize the control policy (on the tendon displacements) and joint stiffnesses of the modules for goal reaching tasks while ensuring collision avoidance. Through co-optimization, we obtain optimized joint stiffness and the corresponding optimal control policy to enable the manipulator to accomplish the task that would be infeasible with fixed reconfiguration parameters (i.e., fixed joint stiffness). We envision the co-optimization framework can be extended to other reconfigurable robotic systems, enabling them to optimally adapt their configuration and behavior for diverse tasks and environments."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v1",
    "arxiv_id": "2504.10458v1",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
    "url": "http://arxiv.org/abs/2504.10458v2",
    "arxiv_id": "2504.10458v2",
    "authors": [
      "Xiaobo Xia",
      "Run Luo"
    ],
    "published": "2025-04-14T17:45:54+00:00",
    "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks."
  },
  {
    "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos",
    "url": "http://arxiv.org/abs/2504.10358v1",
    "arxiv_id": "2504.10358v1",
    "authors": [
      "Rui Chen",
      "Lei Sun",
      "Jing Tang",
      "Geng Li",
      "Xiangxiang Chu"
    ],
    "published": "2025-04-14T16:07:16+00:00",
    "summary": "Recent advances in video generation have posed great challenges in the assessment of AI-generated content, particularly with the emergence of increasingly sophisticated models. The various inconsistencies and defects observed in such videos are inherently complex, making overall scoring notoriously difficult. In this paper, we emphasize the critical importance of integrating fine-grained reasoning into video evaluation, and we propose $\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation framework that first automatically generates $\\textbf{F}$ine-grained $\\textbf{E}$ntity-level questions, and then answers those questions by a $\\textbf{R}$easoning model with scores, which can be subsequently weighted summed to an overall score for different applications. Specifically, we leverage LLMs to derive entity-level questions across five distinct perspectives, which (i) often focus on some specific entities of the content, thereby making answering or scoring much easier by MLLMs, and (ii) are more interpretable. Then we construct a FingER dataset, consisting of approximately 3.3k videos and corresponding 60k fine-grained QA annotations, each with detailed reasons. Based on that, we further investigate various training protocols to best incentivize the reasoning capability of MLLMs for correct answer prediction. Extensive experiments demonstrate that a reasoning model trained using Group Relative Policy Optimization (GRPO) with a cold-start strategy achieves the best performance. Notably, our model surpasses existing methods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on MonetBench with only 3.3k training videos, which is at most one-tenth of the training samples utilized by other methods. Our code and dataset will be released soon."
  },
  {
    "title": "The Communication and Computation Trade-off in Wireless Semantic Communications",
    "url": "http://arxiv.org/abs/2504.10357v1",
    "arxiv_id": "2504.10357v1",
    "authors": [
      "Xuyang Chen",
      "Chong Huang",
      "Gaojie Chen",
      "Daquan Feng",
      "Pei Xiao"
    ],
    "published": "2025-04-14T16:06:20+00:00",
    "summary": "Semantic communications have emerged as a crucial research direction for future wireless communication networks. However, as wireless systems become increasingly complex, the demands for computation and communication resources in semantic communications continue to grow rapidly. This paper investigates the trade-off between computation and communication in wireless semantic communications, taking into consideration transmission task delay and performance constraints within the semantic communication framework. We propose a novel tradeoff metric to analyze the balance between computation and communication in semantic transmissions and employ the deep reinforcement learning (DRL) algorithm to minimize this metric, thereby reducing the cost associated with balancing computation and communication. Through simulations, we analyze the tradeoff between computation and communication and demonstrate the effectiveness of optimizing this trade-off metric."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v1",
    "arxiv_id": "2504.10337v1",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "Heimdall: test-time scaling on the generative verification",
    "url": "http://arxiv.org/abs/2504.10337v2",
    "arxiv_id": "2504.10337v2",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "published": "2025-04-14T15:46:33+00:00",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath."
  },
  {
    "title": "InstructEngine: Instruction-driven Text-to-Image Alignment",
    "url": "http://arxiv.org/abs/2504.10329v1",
    "arxiv_id": "2504.10329v1",
    "authors": [
      "Xingyu Lu",
      "Yuhang Hu",
      "YiFan Zhang",
      "Kaiyu Jiang",
      "Changyi Liu",
      "Tianke Zhang",
      "Jinpeng Wang",
      "Bin Wen",
      "Chun Yuan",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang"
    ],
    "published": "2025-04-14T15:36:28+00:00",
    "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been extensively utilized for preference alignment of text-to-image models. Existing methods face certain limitations in terms of both data and algorithm. For training data, most approaches rely on manual annotated preference data, either by directly fine-tuning the generators or by training reward models to provide training signals. However, the high annotation cost makes them difficult to scale up, the reward model consumes extra computation and cannot guarantee accuracy. From an algorithmic perspective, most methods neglect the value of text and only take the image feedback as a comparative signal, which is inefficient and sparse. To alleviate these drawbacks, we propose the InstructEngine framework. Regarding annotation cost, we first construct a taxonomy for text-to-image generation, then develop an automated data construction pipeline based on it. Leveraging advanced large multimodal models and human-defined rules, we generate 25K text-image preference pairs. Finally, we introduce cross-validation alignment method, which refines data efficiency by organizing semantically analogous samples into mutually comparable pairs. Evaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and SDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art baselines, with ablation study confirming the benefits of InstructEngine's all components. A win rate of over 50% in human reviews also proves that InstructEngine better aligns with human preferences."
  },
  {
    "title": "Vision based driving agent for race car simulation environments",
    "url": "http://arxiv.org/abs/2504.10266v1",
    "arxiv_id": "2504.10266v1",
    "authors": [
      "Gergely B\u00e1ri",
      "L\u00e1szl\u00f3 Palkovics"
    ],
    "published": "2025-04-14T14:29:37+00:00",
    "summary": "In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential."
  },
  {
    "title": "Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins",
    "url": "http://arxiv.org/abs/2504.10248v1",
    "arxiv_id": "2504.10248v1",
    "authors": [
      "Collins O. Ogbodo",
      "Timothy J. Rogers",
      "Mattia Dal Borgo",
      "David J. Wagg"
    ],
    "published": "2025-04-14T14:11:00+00:00",
    "summary": "This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins."
  },
  {
    "title": "Deep Reasoning Translation via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10187v1",
    "arxiv_id": "2504.10187v1",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "published": "2025-04-14T12:40:39+00:00",
    "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown promising performance in various complex tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation and taking cultural differences into account. This task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning. Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought process. Given the source sentences, the reward model teaches the deep translation model how to think and free-translate them during reinforcement learning. In this way, training DeepTrans does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning baselines as well as baselines that are fine-tuned with synthesized data. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation."
  },
  {
    "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10160v1",
    "arxiv_id": "2504.10160v1",
    "authors": [
      "Zhaopeng Feng",
      "Shaosheng Cao",
      "Jiahan Ren",
      "Jiayuan Su",
      "Ruizhe Chen",
      "Yan Zhang",
      "Zhe Xu",
      "Yao Hu",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "published": "2025-04-14T12:14:18+00:00",
    "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective in enhancing the reasoning abilities of large language models (LLMs), particularly for tasks with verifiable solutions such as mathematics and coding. However, applying this idea to machine translation (MT), where outputs are flexibly formatted and difficult to automatically evaluate with explicit rules, remains underexplored. In this work, we introduce MT-R1-Zero, the first open-source adaptation of the R1-Zero RL framework for MT without supervised fine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to guide LLMs towards improved translation quality via emergent reasoning. On the WMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive performance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points. Meanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across all metrics, placing it on par with advanced proprietary models such as GPT-4o and Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves state-of-the-art scores on semantic metrics. Moreover, our work exhibits strong generalization capabilities on out-of-distribution MT tasks, robustly supporting multilingual and low-resource settings. Extensive analysis of model behavior across different initializations and reward metrics offers pioneering insight into the critical role of reward design, LLM adaptability, training dynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT. Our code is available at https://github.com/fzp0424/MT-R1-Zero."
  },
  {
    "title": "A Human-Sensitive Controller: Adapting to Human Ergonomics and Physical Constraints via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10102v1",
    "arxiv_id": "2504.10102v1",
    "authors": [
      "Vitor Martins",
      "Sara M. Cerqueira",
      "Mercedes Balcells",
      "Elazer R Edelman",
      "Cristina P. Santos"
    ],
    "published": "2025-04-14T11:09:43+00:00",
    "summary": "Work-Related Musculoskeletal Disorders continue to be a major challenge in industrial environments, leading to reduced workforce participation, increased healthcare costs, and long-term disability. This study introduces a human-sensitive robotic system aimed at reintegrating individuals with a history of musculoskeletal disorders into standard job roles, while simultaneously optimizing ergonomic conditions for the broader workforce. This research leverages reinforcement learning to develop a human-aware control strategy for collaborative robots, focusing on optimizing ergonomic conditions and preventing pain during task execution. Two RL approaches, Q-Learning and Deep Q-Network (DQN), were implemented and tested to personalize control strategies based on individual user characteristics. Although experimental results revealed a simulation-to-real gap, a fine-tuning phase successfully adapted the policies to real-world conditions. DQN outperformed Q-Learning by completing tasks faster while maintaining zero pain risk and safe ergonomic levels. The structured testing protocol confirmed the system's adaptability to diverse human anthropometries, underscoring the potential of RL-driven cobots to enable safer, more inclusive workplaces."
  },
  {
    "title": "Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.10071v1",
    "arxiv_id": "2504.10071v1",
    "authors": [
      "Tien Pham",
      "Angelo Cangelosi"
    ],
    "published": "2025-04-14T10:18:34+00:00",
    "summary": "Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extractor (IFE) architecture, aimed at generating an accurate attention mask to illustrate both \"what\" and \"where\" the agent concentrates on in the spatial domain. Our design incorporates a Human-Understandable Encoding module to generate a fully interpretable attention mask, followed by an Agent-Friendly Encoding module to enhance the agent's learning efficiency. These two components together form the Interpretable Feature Extractor for vision-based deep reinforcement learning to enable the model's interpretability. The resulting attention mask is consistent, highly understandable by humans, accurate in spatial dimension, and effectively highlights important objects or locations in visual input. The Interpretable Feature Extractor is integrated into the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI games to show the effectiveness of the proposed approach on Spatial Preservation, Interpretability, and Data-efficiency. Finally, we showcase the versatility of our approach by incorporating the IFE into the Asynchronous Advantage Actor-Critic Model."
  },
  {
    "title": "Undermining Federated Learning Accuracy in EdgeIoT via Variational Graph Auto-Encoders",
    "url": "http://arxiv.org/abs/2504.10067v1",
    "arxiv_id": "2504.10067v1",
    "authors": [
      "Kai Li",
      "Shuyan Hu",
      "Bochun Wu",
      "Sai Zou",
      "Wei Ni",
      "Falko Dressler"
    ],
    "published": "2025-04-14T10:09:38+00:00",
    "summary": "EdgeIoT represents an approach that brings together mobile edge computing with Internet of Things (IoT) devices, allowing for data processing close to the data source. Sending source data to a server is bandwidth-intensive and may compromise privacy. Instead, federated learning allows each device to upload a shared machine-learning model update with locally processed data. However, this technique, which depends on aggregating model updates from various IoT devices, is vulnerable to attacks from malicious entities that may inject harmful data into the learning process. This paper introduces a new attack method targeting federated learning in EdgeIoT, known as data-independent model manipulation attack. This attack does not rely on training data from the IoT devices but instead uses an adversarial variational graph auto-encoder (AV-GAE) to create malicious model updates by analyzing benign model updates intercepted during communication. AV-GAE identifies and exploits structural relationships between benign models and their training data features. By manipulating these structural correlations, the attack maximizes the training loss of the federated learning system, compromising its overall effectiveness."
  },
  {
    "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
    "url": "http://arxiv.org/abs/2504.10045v1",
    "arxiv_id": "2504.10045v1",
    "authors": [
      "Xiao Zhu",
      "Chenmien Tan",
      "Pinzhen Chen",
      "Rico Sennrich",
      "Yanlin Zhang",
      "Hanxu Hu"
    ],
    "published": "2025-04-14T09:51:09+00:00",
    "summary": "Reward models (RMs) play a crucial role in Reinforcement Learning from Human Feedback by serving as proxies for human preferences in aligning large language models. In this paper, we identify a model preference bias in RMs, where they systematically assign disproportionately high scores to responses from certain policy models. This bias distorts ranking evaluations and leads to unfair judgments. To address this issue, we propose a calibration method named CHatbot Arena calibrated Reward Modeling (CHARM) that leverages Elo scores from the Chatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a Mismatch Degree metric to measure this preference bias. Our approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings. By mitigating model preference bias, our method provides a generalizable and efficient solution for building fairer and more reliable reward models."
  },
  {
    "title": "Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making",
    "url": "http://arxiv.org/abs/2504.10031v1",
    "arxiv_id": "2504.10031v1",
    "authors": [
      "Arthur Vandervoort",
      "Miguel Costa",
      "Morten W. Petersen",
      "Martin Drews",
      "Sonja Haustein",
      "Karyn Morrissey",
      "Francisco C. Pereira"
    ],
    "published": "2025-04-14T09:34:12+00:00",
    "summary": "Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run."
  },
  {
    "title": "Improving Controller Generalization with Dimensionless Markov Decision Processes",
    "url": "http://arxiv.org/abs/2504.10006v1",
    "arxiv_id": "2504.10006v1",
    "authors": [
      "Valentin Charvet",
      "Sebastian Stein",
      "Roderick Murray-Smith"
    ],
    "published": "2025-04-14T09:08:53+00:00",
    "summary": "Controllers trained with Reinforcement Learning tend to be very specialized and thus generalize poorly when their testing environment differs from their training one. We propose a Model-Based approach to increase generalization where both world model and policy are trained in a dimensionless state-action space. To do so, we introduce the Dimensionless Markov Decision Process ($\\Pi$-MDP): an extension of Contextual-MDPs in which state and action spaces are non-dimensionalized with the Buckingham-$\\Pi$ theorem. This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics. We provide a generic framework for this approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context."
  },
  {
    "title": "FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions",
    "url": "http://arxiv.org/abs/2504.10002v1",
    "arxiv_id": "2504.10002v1",
    "authors": [
      "Daniel Marta",
      "Simon Holk",
      "Miguel Vasco",
      "Jens Lundell",
      "Timon Homberger",
      "Finn Busch",
      "Olov Andersson",
      "Danica Kragic",
      "Iolanda Leite"
    ],
    "published": "2025-04-14T09:04:14+00:00",
    "summary": "Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks."
  },
  {
    "title": "GenTe: Generative Real-world Terrains for General Legged Robot Locomotion Control",
    "url": "http://arxiv.org/abs/2504.09997v1",
    "arxiv_id": "2504.09997v1",
    "authors": [
      "Hanwen Wan",
      "Mengkang Li",
      "Donghao Wu",
      "Yebin Zhong",
      "Yixuan Deng",
      "Zhenglong Sun",
      "Xiaoqiang Ji"
    ],
    "published": "2025-04-14T09:01:44+00:00",
    "summary": "Developing bipedal robots capable of traversing diverse real-world terrains presents a fundamental robotics challenge, as existing methods using predefined height maps and static environments fail to address the complexity of unstructured landscapes. To bridge this gap, we propose GenTe, a framework for generating physically realistic and adaptable terrains to train generalizable locomotion policies. GenTe constructs an atomic terrain library that includes both geometric and physical terrains, enabling curriculum training for reinforcement learning-based locomotion policies. By leveraging function-calling techniques and reasoning capabilities of Vision-Language Models (VLMs), GenTe generates complex, contextually relevant terrains from textual and graphical inputs. The framework introduces realistic force modeling for terrain interactions, capturing effects such as soil sinkage and hydrodynamic resistance. To the best of our knowledge, GenTe is the first framework that systemically generates simulation environments for legged robot locomotion control. Additionally, we introduce a benchmark of 100 generated terrains. Experiments demonstrate improved generalization and robustness in bipedal robot locomotion."
  },
  {
    "title": "Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss",
    "url": "http://arxiv.org/abs/2504.09929v1",
    "arxiv_id": "2504.09929v1",
    "authors": [
      "Ukjo Hwang",
      "Songnam Hong"
    ],
    "published": "2025-04-14T06:41:15+00:00",
    "summary": "Overestimation is a fundamental characteristic of model-free reinforcement learning (MF-RL), arising from the principles of temporal difference learning and the approximation of the Q-function. To address this challenge, we propose a novel moderate target in the Q-function update, formulated as a convex optimization of an overestimated Q-function and its lower bound. Our primary contribution lies in the efficient estimation of this lower bound through the lower expectile of the Q-value distribution conditioned on a state. Notably, our moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL algorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor Critic (SAC). Experimental results validate the effectiveness of our moderate target in mitigating overestimation bias in DDPG, SAC, and distributional RL algorithms."
  },
  {
    "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models",
    "url": "http://arxiv.org/abs/2504.09910v1",
    "arxiv_id": "2504.09910v1",
    "authors": [
      "Yujing Wang",
      "Hainan Zhang",
      "Liang Pang",
      "Yongxin Tong",
      "Binghui Guo",
      "Hongwei Zheng",
      "Zhiming Zheng"
    ],
    "published": "2025-04-14T06:10:31+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively erasing private information from retrieved documents is a key challenge for RAG. Unlike traditional text anonymization, RAG should consider: (1) the inherent multi-document reasoning may face de-anonymization attacks; (2) private knowledge varies by scenarios, so users should be allowed to customize which information to erase; (3) preserving sufficient publicly available knowledge for generation tasks. This paper introduces the privacy erasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which effectively removes user-defined private knowledge from documents while preserving sufficient public knowledge for generation. Specifically, we first construct a global knowledge graph to identify potential knowledge across documents, aiming to defend against de-anonymization attacks. Then we randomly split it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite the retrieved documents excluding private triples. Finally, PPO algorithm optimizes the rewriting model to minimize private triples and maximize public triples retention. Experiments on four QA datasets demonstrate that Eraser4RAG achieves superior erase performance than GPT-4o."
  },
  {
    "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",
    "url": "http://arxiv.org/abs/2504.09848v1",
    "arxiv_id": "2504.09848v1",
    "authors": [
      "Jie Feng",
      "Jinwei Zeng",
      "Qingyue Long",
      "Hongyi Chen",
      "Jie Zhao",
      "Yanxin Xi",
      "Zhilun Zhou",
      "Yuan Yuan",
      "Shengyuan Wang",
      "Qingbin Zeng",
      "Songwei Li",
      "Yunke Zhang",
      "Yuming Lin",
      "Tong Li",
      "Jingtao Ding",
      "Chen Gao",
      "Fengli Xu",
      "Yong Li"
    ],
    "published": "2025-04-14T03:38:31+00:00",
    "summary": "Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies."
  },
  {
    "title": "PreCi: Pretraining and Continual Improvement of Humanoid Locomotion via Model-Assumption-Based Regularization",
    "url": "http://arxiv.org/abs/2504.09833v1",
    "arxiv_id": "2504.09833v1",
    "authors": [
      "Hyunyoung Jung",
      "Zhaoyuan Gu",
      "Ye Zhao",
      "Hae-Won Park",
      "Sehoon Ha"
    ],
    "published": "2025-04-14T03:02:02+00:00",
    "summary": "Humanoid locomotion is a challenging task due to its inherent complexity and high-dimensional dynamics, as well as the need to adapt to diverse and unpredictable environments. In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands. Our framework consists of three key components: pre-training through imitation of the model-based controller, fine-tuning via reinforcement learning, and model-assumption-based regularization (MAR) during fine-tuning. In particular, MAR aligns the policy with actions from the model-based controller only in states where the model assumption holds to prevent catastrophic forgetting. We evaluate the proposed framework through comprehensive simulation tests and hardware experiments on a full-size humanoid robot, Digit, demonstrating a forward speed of 1.5 m/s and robust locomotion across diverse terrains, including slippery, sloped, uneven, and sandy terrains."
  },
  {
    "title": "Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand",
    "url": "http://arxiv.org/abs/2504.09831v1",
    "arxiv_id": "2504.09831v1",
    "authors": [
      "Korel Gundem",
      "Zhengling Qi"
    ],
    "published": "2025-04-14T02:57:51+00:00",
    "summary": "In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https://github.com/gundemkorel/Inventory_Pricing_Control"
  },
  {
    "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
    "url": "http://arxiv.org/abs/2504.09710v1",
    "arxiv_id": "2504.09710v1",
    "authors": [
      "Zhenting Wang",
      "Guofeng Cui",
      "Kun Wan",
      "Wentian Zhao"
    ],
    "published": "2025-04-13T20:10:27+00:00",
    "summary": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP."
  },
  {
    "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models",
    "url": "http://arxiv.org/abs/2504.09696v1",
    "arxiv_id": "2504.09696v1",
    "authors": [
      "Jixiao Zhang",
      "Chunsheng Zuo"
    ],
    "published": "2025-04-13T19:07:45+00:00",
    "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy Optimization (GRPO) have significantly improved the performance of language models on mathematical reasoning tasks. However, current GRPO implementations encounter critical challenges, including reward sparsity due to binary accuracy metrics, limited incentives for conciseness, and insufficient focus on complex reasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of novel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD introduces (1) a length-dependent accuracy reward to encourage concise and precise solutions, (2) an explicit penalty mechanism for incorrect answers to sharpen decision boundaries, and (3) a difficulty-aware advantage reweighting strategy that amplifies learning signals for challenging problems. Furthermore, we systematically examine the impact of model scale and supervised fine-tuning (SFT) strategies, demonstrating that larger-scale base models and carefully curated datasets significantly enhance reinforcement learning effectiveness. Extensive empirical evaluations and ablation studies confirm that GRPO-LEAD substantially mitigates previous shortcomings, resulting in language models that produce more concise, accurate, and robust reasoning across diverse mathematical tasks."
  },
  {
    "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
    "url": "http://arxiv.org/abs/2504.09643v1",
    "arxiv_id": "2504.09643v1",
    "authors": [
      "Nikita Sorokin",
      "Ivan Sedykh",
      "Valentin Malykh"
    ],
    "published": "2025-04-13T16:34:17+00:00",
    "summary": "Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language."
  },
  {
    "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
    "url": "http://arxiv.org/abs/2504.09641v1",
    "arxiv_id": "2504.09641v1",
    "authors": [
      "Xingjian Zhang",
      "Siwei Wen",
      "Wenjun Wu",
      "Lei Huang"
    ],
    "published": "2025-04-13T16:32:49+00:00",
    "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of \"aha moments\". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1."
  },
  {
    "title": "Slow Thinking for Sequential Recommendation",
    "url": "http://arxiv.org/abs/2504.09627v1",
    "arxiv_id": "2504.09627v1",
    "authors": [
      "Junjie Zhang",
      "Beichen Zhang",
      "Wenqi Sun",
      "Hongyu Lu",
      "Wayne Xin Zhao",
      "Yu Chen",
      "Ji-Rong Wen"
    ],
    "published": "2025-04-13T15:53:30+00:00",
    "summary": "To develop effective sequential recommender systems, numerous methods have been proposed to model historical user behaviors. Despite the effectiveness, these methods share the same fast thinking paradigm. That is, for making recommendations, these methods typically encodes user historical interactions to obtain user representations and directly match these representations with candidate item representations. However, due to the limited capacity of traditional lightweight recommendation models, this one-step inference paradigm often leads to suboptimal performance. To tackle this issue, we present a novel slow thinking recommendation model, named STREAM-Rec. Our approach is capable of analyzing historical user behavior, generating a multi-step, deliberative reasoning process, and ultimately delivering personalized recommendations. In particular, we focus on two key challenges: (1) identifying the suitable reasoning patterns in recommender systems, and (2) exploring how to effectively stimulate the reasoning capabilities of traditional recommenders. To this end, we introduce a three-stage training framework. In the first stage, the model is pretrained on large-scale user behavior data to learn behavior patterns and capture long-range dependencies. In the second stage, we design an iterative inference algorithm to annotate suitable reasoning traces by progressively refining the model predictions. This annotated data is then used to fine-tune the model. Finally, in the third stage, we apply reinforcement learning to further enhance the model generalization ability. Extensive experiments validate the effectiveness of our proposed method."
  },
  {
    "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps",
    "url": "http://arxiv.org/abs/2504.09608v1",
    "arxiv_id": "2504.09608v1",
    "authors": [
      "Xingke Song",
      "Xiaoying Yang",
      "Chenglin Yao",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Xin Chen",
      "Xudong Jiang"
    ],
    "published": "2025-04-13T14:56:41+00:00",
    "summary": "Solving jigsaw puzzles has been extensively studied. While most existing models focus on solving either small-scale puzzles or puzzles with no gap between fragments, solving large-scale puzzles with gaps presents distinctive challenges in both image understanding and combinatorial optimization. To tackle these challenges, we propose a framework of Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of swapping actions for solving the puzzles. Specifically, to tackle the challenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception Network (MPPN) with a shared encoder is designed, where multiple puzzlet heads comprehensively perceive the local assembly status, and a discriminator head provides a global assessment of the puzzle. To explore the large swapping action space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent is designed, where an actor recommends a set of suitable swapping actions from a large action space based on the perceived puzzle status, a critic updates the actor using the estimated rewards and the puzzle status, and an evaluator coupled with evolutionary strategies evolves the actions aligning with the historical assembly experience. The proposed ERL-MPP is comprehensively evaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with large-scale puzzles. It significantly outperforms all state-of-the-art models on both datasets."
  },
  {
    "title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation",
    "url": "http://arxiv.org/abs/2504.09587v1",
    "arxiv_id": "2504.09587v1",
    "authors": [
      "Haotian Xu",
      "Yue Hu",
      "Chen Gao",
      "Zhengqiu Zhu",
      "Yong Zhao",
      "Yong Li",
      "Quanjun Yin"
    ],
    "published": "2025-04-13T14:12:42+00:00",
    "summary": "Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation."
  },
  {
    "title": "Kongzi: A Historical Large Language Model with Fact Enhancement",
    "url": "http://arxiv.org/abs/2504.09488v1",
    "arxiv_id": "2504.09488v1",
    "authors": [
      "Jiashu Yang",
      "Ningning Wang",
      "Yian Zhao",
      "Chaoran Feng",
      "Junjia Du",
      "Hao Pang",
      "Zhirui Fang",
      "Xuxin Cheng"
    ],
    "published": "2025-04-13T09:01:05+00:00",
    "summary": "The capabilities of the latest large language models (LLMs) have been extended from pure natural language understanding to complex reasoning tasks. However, current reasoning models often exhibit factual inaccuracies in longer reasoning chains, which poses challenges for historical reasoning and limits the potential of LLMs in complex, knowledge-intensive tasks. Historical studies require not only the accurate presentation of factual information but also the ability to establish cross-temporal correlations and derive coherent conclusions from fragmentary and often ambiguous sources. To address these challenges, we propose Kongzi, a large language model specifically designed for historical analysis. Through the integration of curated, high-quality historical data and a novel fact-reinforcement learning strategy, Kongzi demonstrates strong factual alignment and sophisticated reasoning depth. Extensive experiments on tasks such as historical question answering and narrative generation demonstrate that Kongzi outperforms existing models in both factual accuracy and reasoning depth. By effectively addressing the unique challenges inherent in historical texts, Kongzi sets a new standard for the development of accurate and reliable LLMs in professional domains."
  },
  {
    "title": "A highly maneuverable flying squirrel drone with controllable foldable wings",
    "url": "http://arxiv.org/abs/2504.09478v1",
    "arxiv_id": "2504.09478v1",
    "authors": [
      "Jun-Gill Kang",
      "Dohyeon Lee",
      "Soohee Han"
    ],
    "published": "2025-04-13T08:15:28+00:00",
    "summary": "Typical drones with multi rotors are generally less maneuverable due to unidirectional thrust, which may be unfavorable to agile flight in very narrow and confined spaces. This paper suggests a new bio-inspired drone that is empowered with high maneuverability in a lightweight and easy-to-carry way. The proposed flying squirrel inspired drone has controllable foldable wings to cover a wider range of flight attitudes and provide more maneuverable flight capability with stable tracking performance. The wings of a drone are fabricated with silicone membranes and sophisticatedly controlled by reinforcement learning based on human-demonstrated data. Specially, such learning based wing control serves to capture even the complex aerodynamics that are often impossible to model mathematically. It is shown through experiment that the proposed flying squirrel drone intentionally induces aerodynamic drag and hence provides the desired additional repulsive force even under saturated mechanical thrust. This work is very meaningful in demonstrating the potential of biomimicry and machine learning for realizing an animal-like agile drone."
  },
  {
    "title": "Evolutionary Defense: Advancing Moving Target Strategies with Bio-Inspired Reinforcement Learning to Secure Misconfigured Software Applications",
    "url": "http://arxiv.org/abs/2504.09465v1",
    "arxiv_id": "2504.09465v1",
    "authors": [
      "Niloofar Heidarikohol",
      "Shuvalaxmi Dass",
      "Akbar Siami Namin"
    ],
    "published": "2025-04-13T07:39:01+00:00",
    "summary": "Improper configurations in software systems often create vulnerabilities, leaving them open to exploitation. Static architectures exacerbate this issue by allowing misconfigurations to persist, providing adversaries with opportunities to exploit them during attacks. To address this challenge, a dynamic proactive defense strategy known as Moving Target Defense (MTD) can be applied. MTD continually changes the attack surface of the system, thwarting potential threats. In the previous research, we developed a proof of concept for a single-player MTD game model called RL-MTD, which utilizes Reinforcement Learning (RL) to generate dynamic secure configurations. While the model exhibited satisfactory performance in generating secure configurations, it grappled with an unoptimized and sparse search space, leading to performance issues. To tackle this obstacle, this paper addresses the search space optimization problem by leveraging two bio-inspired search algorithms: Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). Additionally, we extend our base RL-MTD model by integrating these algorithms, resulting in the creation of PSO-RL andGA-RL. We compare the performance of three models: base RL-MTD, GA-RL, and PSO-RL, across four misconfigured SUTs in terms of generating the most secure configuration. Results show that the optimal search space derived from both GA-RL and PSO-RL significantly enhances the performance of the base RL-MTD model compared to the version without optimized search space. While both GA-RL and PSO-RL demonstrate effective search capabilities, PSO-RL slightly outperforms GA-RL for most SUTs. Overall, both algorithms excel in seeking an optimal search space which in turn improves the performance of the model in generating optimal secure configuration."
  },
  {
    "title": "Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
    "url": "http://arxiv.org/abs/2504.09427v1",
    "arxiv_id": "2504.09427v1",
    "authors": [
      "Moirangthem Tiken Singh"
    ],
    "published": "2025-04-13T04:21:11+00:00",
    "summary": "Fault classification in industrial machinery is vital for enhancing reliability and reducing downtime, yet it remains challenging due to the variability of vibration patterns across diverse operating conditions. This study introduces a novel graph-based framework for fault classification, converting time-series vibration data from machinery operating at varying horsepower levels into a graph representation. We utilize Shannon's entropy to determine the optimal window size for data segmentation, ensuring each segment captures significant temporal patterns, and employ Dynamic Time Warping (DTW) to define graph edges based on segment similarity. A Graph Auto Encoder (GAE) with a deep graph transformer encoder, decoder, and ensemble classifier is developed to learn latent graph representations and classify faults across various categories. The GAE's performance is evaluated on the Case Western Reserve University (CWRU) dataset, with cross-dataset generalization assessed on the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on the CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN, GRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for Bi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99 vs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that datasets with amplified vibration patterns and diverse fault dynamics enhance generalization. This framework provides a robust solution for fault diagnosis under varying conditions, offering insights into dataset impacts on model performance."
  },
  {
    "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment",
    "url": "http://arxiv.org/abs/2504.09420v1",
    "arxiv_id": "2504.09420v1",
    "authors": [
      "Yutao Mou",
      "Yuxiao Luo",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "published": "2025-04-13T03:36:06+00:00",
    "summary": "Current safety alignment techniques for large language models (LLMs) face two key challenges: (1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of benign instructions. Our preliminary investigation reveals semantic overlap between jailbreak/harmful queries and normal prompts in embedding space, suggesting that more effective safety alignment requires a deeper semantic understanding. This motivates us to incorporate safety-policy-driven reasoning into the alignment process. To this end, we propose the Safety-oriented Reasoning Optimization Framework (SaRO), which consists of two stages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize long-chain reasoning through supervised fine-tuning, and (2) Safety-oriented Reasoning Process Optimization (SRPO) that promotes safety reflection via direct preference optimization (DPO). Extensive experiments demonstrate the superiority of SaRO over traditional alignment methods."
  },
  {
    "title": "Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
    "url": "http://arxiv.org/abs/2504.09415v1",
    "arxiv_id": "2504.09415v1",
    "authors": [
      "Gengcan Chen",
      "Donghong Cai",
      "Zahid Khan",
      "Jawad Ahmad",
      "Wadii Boulila"
    ],
    "published": "2025-04-13T03:09:47+00:00",
    "summary": "In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods."
  },
  {
    "title": "Scalable Motion In-betweening via Diffusion and Physics-Based Character Adaptation",
    "url": "http://arxiv.org/abs/2504.09413v1",
    "arxiv_id": "2504.09413v1",
    "authors": [
      "Jia Qin"
    ],
    "published": "2025-04-13T03:04:25+00:00",
    "summary": "We propose a two-stage framework for motion in-betweening that combines diffusion-based motion generation with physics-based character adaptation. In Stage 1, a character-agnostic diffusion model synthesizes transitions from sparse keyframes on a canonical skeleton, allowing the same model to generalize across diverse characters. In Stage 2, a reinforcement learning-based controller adapts the canonical motion to the target character's morphology and dynamics, correcting artifacts and enhancing stylistic realism. This design supports scalable motion generation across characters with diverse skeletons without retraining the entire model. Experiments on standard benchmarks and stylized characters demonstrate that our method produces physically plausible, style-consistent motions under sparse and long-range constraints."
  },
  {
    "title": "Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes",
    "url": "http://arxiv.org/abs/2504.09396v1",
    "arxiv_id": "2504.09396v1",
    "authors": [
      "Stella C. Dong",
      "James R. Finlay"
    ],
    "published": "2025-04-13T01:43:25+00:00",
    "summary": "This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.   The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty."
  },
  {
    "title": "Context-Aware Adaptive Sampling for Intelligent Data Acquisition Systems Using DQN",
    "url": "http://arxiv.org/abs/2504.09344v1",
    "arxiv_id": "2504.09344v1",
    "authors": [
      "Weiqiang Huang",
      "Juecen Zhan",
      "Yumeng Sun",
      "Xu Han",
      "Tai An",
      "Nan Jiang"
    ],
    "published": "2025-04-12T21:11:56+00:00",
    "summary": "Multi-sensor systems are widely used in the Internet of Things, environmental monitoring, and intelligent manufacturing. However, traditional fixed-frequency sampling strategies often lead to severe data redundancy, high energy consumption, and limited adaptability, failing to meet the dynamic sensing needs of complex environments. To address these issues, this paper proposes a DQN-based multi-sensor adaptive sampling optimization method. By leveraging a reinforcement learning framework to learn the optimal sampling strategy, the method balances data quality, energy consumption, and redundancy. We first model the multi-sensor sampling task as a Markov Decision Process (MDP), then employ a Deep Q-Network to optimize the sampling policy. Experiments on the Intel Lab Data dataset confirm that, compared with fixed-frequency sampling, threshold-triggered sampling, and other reinforcement learning approaches, DQN significantly improves data quality while lowering average energy consumption and redundancy rates. Moreover, in heterogeneous multi-sensor environments, DQN-based adaptive sampling shows enhanced robustness, maintaining superior data collection performance even in the presence of interference factors. These findings demonstrate that DQN-based adaptive sampling can enhance overall data acquisition efficiency in multi-sensor systems, providing a new solution for efficient and intelligent sensing."
  },
  {
    "title": "Towards Optimal Differentially Private Regret Bounds in Linear MDPs",
    "url": "http://arxiv.org/abs/2504.09339v1",
    "arxiv_id": "2504.09339v1",
    "authors": [
      "Sharan Sahu"
    ],
    "published": "2025-04-12T20:51:51+00:00",
    "summary": "We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $\\phi(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to minimax optimal regret $\\widetilde{O}(HD\\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{4.5} d^{7/6} K^{1/2} / \\epsilon)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting."
  },
  {
    "title": "Efficient Implementation of Reinforcement Learning over Homomorphic Encryption",
    "url": "http://arxiv.org/abs/2504.09335v1",
    "arxiv_id": "2504.09335v1",
    "authors": [
      "Jihoon Suh",
      "Takashi Tanaka"
    ],
    "published": "2025-04-12T20:34:26+00:00",
    "summary": "We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify control policy synthesis into model-based, simulator-driven, and data-driven approaches and examine their implementation over fully homomorphic encryption (FHE) for privacy enhancements. A key challenge arises from comparison operations (min or max) in standard reinforcement learning algorithms, which are difficult to execute over encrypted data. This observation motivates our focus on Relative-Entropy-regularized reinforcement learning (RL) problems, which simplifies encrypted evaluation of synthesis algorithms due to their comparison-free structures. We demonstrate how linearly solvable value iteration, path integral control, and Z-learning can be readily implemented over FHE. We conduct a case study of our approach through numerical simulations of encrypted Z-learning in a grid world environment using the CKKS encryption scheme, showing convergence with acceptable approximation error. Our work suggests the potential for secure and efficient cloud-based reinforcement learning."
  },
  {
    "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks",
    "url": "http://arxiv.org/abs/2504.09258v1",
    "arxiv_id": "2504.09258v1",
    "authors": [
      "Jianyu Wu",
      "Hao Yang",
      "Xinhua Zeng",
      "Guibing He",
      "Zhiyu Chen",
      "Zihui Li",
      "Xiaochuan Zhang",
      "Yangyang Ma",
      "Run Fang",
      "Yang Liu"
    ],
    "published": "2025-04-12T15:32:16+00:00",
    "summary": "The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the reliability of clinical decisions. To address the weak reasoning abilities and lack of supervised processes in pathological VLMs, we have innovatively proposed PathVLM-R1, a visual language model designed specifically for pathological images. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its performance for pathological tasks through meticulously designed post-training strategies. Firstly, we conduct supervised fine-tuning guided by pathological data to imbue the model with foundational pathological knowledge, forming a new pathological base model. Subsequently, we introduce Group Relative Policy Optimization (GRPO) and propose a dual reward-driven reinforcement learning optimization, ensuring strict constraint on logical supervision of the reasoning process and accuracy of results via cross-modal process reward and outcome accuracy reward. In the pathological image question-answering tasks, the testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy compared to baseline methods, and it demonstrated superior performance compared to the Qwen2.5-VL-32B version despite having a significantly smaller parameter size. Furthermore, in out-domain data evaluation involving four medical imaging modalities: Computed Tomography (CT), dermoscopy, fundus photography, and Optical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance improved by an average of 17.3% compared to traditional SFT methods. These results clearly indicate that PathVLM-R1 not only enhances accuracy but also possesses broad applicability and expansion potential."
  },
  {
    "title": "Development of a PPO-Reinforcement Learned Walking Tripedal Soft-Legged Robot using SOFA",
    "url": "http://arxiv.org/abs/2504.09242v1",
    "arxiv_id": "2504.09242v1",
    "authors": [
      "Yomna Mokhtar",
      "Tarek Shohdy",
      "Abdallah A. Hassan",
      "Mostafa Eshra",
      "Omar Elmenawy",
      "Osama Khalil",
      "Haitham El-Hussieny"
    ],
    "published": "2025-04-12T14:46:51+00:00",
    "summary": "Rigid robots were extensively researched, whereas soft robotics remains an underexplored field. Utilizing soft-legged robots in performing tasks as a replacement for human beings is an important stride to take, especially under harsh and hazardous conditions over rough terrain environments. For the demand to teach any robot how to behave in different scenarios, a real-time physical and visual simulation is essential. When it comes to soft robots specifically, a simulation framework is still an arduous problem that needs to be disclosed. Using the simulation open framework architecture (SOFA) is an advantageous step. However, neither SOFA's manual nor prior public SOFA projects show its maximum capabilities the users can reach. So, we resolved this by establishing customized settings and handling the framework components appropriately. Settling on perfect, fine-tuned SOFA parameters has stimulated our motivation towards implementing the state-of-the-art (SOTA) reinforcement learning (RL) method of proximal policy optimization (PPO). The final representation is a well-defined, ready-to-deploy walking, tripedal, soft-legged robot based on PPO-RL in a SOFA environment. Robot navigation performance is a key metric to be considered for measuring the success resolution. Although in the simulated soft robots case, an 82\\% success rate in reaching a single goal is a groundbreaking output, we pushed the boundaries to further steps by evaluating the progress under assigning a sequence of goals. While trailing the platform steps, outperforming discovery has been observed with an accumulative squared error deviation of 19 mm. The full code is publicly available at \\href{https://github.com/tarekshohdy/PPO_SOFA_Soft_Legged_Robot.git}{github.com/tarekshohdy/PPO$\\textunderscore$SOFA$\\textunderscore$Soft$\\textunderscore$Legged$\\textunderscore$ Robot.git}"
  },
  {
    "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
    "url": "http://arxiv.org/abs/2504.09192v1",
    "arxiv_id": "2504.09192v1",
    "authors": [
      "Zhiyong Wang"
    ],
    "published": "2025-04-12T12:17:20+00:00",
    "summary": "The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Online learning methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable online learning algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits."
  },
  {
    "title": "Feature-Aware Malicious Output Detection and Mitigation",
    "url": "http://arxiv.org/abs/2504.09191v1",
    "arxiv_id": "2504.09191v1",
    "authors": [
      "Weilong Dong",
      "Peiguang Li",
      "Yu Tian",
      "Xinyi Zeng",
      "Fengdi Li",
      "Sirui Wang"
    ],
    "published": "2025-04-12T12:12:51+00:00",
    "summary": "The rapid advancement of large language models (LLMs) has brought significant benefits to various domains while introducing substantial risks. Despite being fine-tuned through reinforcement learning, LLMs lack the capability to discern malicious content, limiting their defense against jailbreak. To address these safety concerns, we propose a feature-aware method for harmful response rejection (FMM), which detects the presence of malicious features within the model's feature space and adaptively adjusts the model's rejection mechanism. By employing a simple discriminator, we detect potential malicious traits during the decoding phase. Upon detecting features indicative of toxic tokens, FMM regenerates the current token. By employing activation patching, an additional rejection vector is incorporated during the subsequent token generation, steering the model towards a refusal response. Experimental results demonstrate the effectiveness of our approach across multiple language models and diverse attack techniques, while crucially maintaining the models' standard generation capabilities."
  },
  {
    "title": "RSLAQ -- A Robust SLA-driven 6G O-RAN QoS xApp using deep reinforcement learning",
    "url": "http://arxiv.org/abs/2504.09187v1",
    "arxiv_id": "2504.09187v1",
    "authors": [
      "Noe M. Yungaicela-Naula",
      "Vishal Sharma",
      "Sandra Scott-Hayward"
    ],
    "published": "2025-04-12T12:00:54+00:00",
    "summary": "The evolution of 6G envisions a wide range of applications and services characterized by highly differentiated and stringent Quality of Service (QoS) requirements. Open Radio Access Network (O-RAN) technology has emerged as a transformative approach that enables intelligent software-defined management of the RAN. A cornerstone of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the deployment of intelligent applications (xApps and rApps) near the radio unit. In this context, QoS management through O-RAN has been explored using network slice and machine learning (ML) techniques. Although prior studies have demonstrated the ability to optimize RAN resource allocation and prioritize slices effectively, they have not considered the critical integration of Service Level Agreements (SLAs) into the ML learning process. This omission can lead to suboptimal resource utilization and, in many cases, service outages when target Key Performance Indicators (KPIs) are not met. This work introduces RSLAQ, an innovative xApp designed to ensure robust QoS management for RAN slicing while incorporating SLAs directly into its operational framework. RSLAQ translates operator policies into actionable configurations, guiding resource distribution and scheduling for RAN slices. Using deep reinforcement learning (DRL), RSLAQ dynamically monitors RAN performance metrics and computes optimal actions, embedding SLA constraints to mitigate conflicts and prevent outages. Extensive system-level simulations validate the efficacy of the proposed solution, demonstrating its ability to optimize resource allocation, improve SLA adherence, and maintain operational reliability (>95%) in challenging scenarios."
  },
  {
    "title": "Large Language Models integration in Smart Grids",
    "url": "http://arxiv.org/abs/2504.09059v1",
    "arxiv_id": "2504.09059v1",
    "authors": [
      "Seyyedreza Madani",
      "Ahmadreza Tavasoli",
      "Zahra Khoshtarash Astaneh",
      "Pierre-Olivier Pineau"
    ],
    "published": "2025-04-12T03:29:30+00:00",
    "summary": "Large Language Models (LLMs) are changing the way we operate our society and will undoubtedly impact power systems as well - but how exactly? By integrating various data streams - including real-time grid data, market dynamics, and consumer behaviors - LLMs have the potential to make power system operations more adaptive, enhance proactive security measures, and deliver personalized energy services. This paper provides a comprehensive analysis of 30 real-world applications across eight key categories: Grid Operations and Management, Energy Markets and Trading, Personalized Energy Management and Customer Engagement, Grid Planning and Education, Grid Security and Compliance, Advanced Data Analysis and Knowledge Discovery, Emerging Applications and Societal Impact, and LLM-Enhanced Reinforcement Learning. Critical technical hurdles, such as data privacy and model reliability, are examined, along with possible solutions. Ultimately, this review illustrates how LLMs can significantly contribute to building more resilient, efficient, and sustainable energy infrastructures, underscoring the necessity of their responsible and equitable deployment."
  },
  {
    "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
    "url": "http://arxiv.org/abs/2504.09037v1",
    "arxiv_id": "2504.09037v1",
    "authors": [
      "Zixuan Ke",
      "Fangkai Jiao",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Austin Xu",
      "Do Xuan Long",
      "Minzhi Li",
      "Chengwei Qin",
      "Peifeng Wang",
      "Silvio Savarese",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "published": "2025-04-12T01:27:49+00:00",
    "summary": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ..."
  },
  {
    "title": "InterQ: A DQN Framework for Optimal Intermittent Control",
    "url": "http://arxiv.org/abs/2504.09035v1",
    "arxiv_id": "2504.09035v1",
    "authors": [
      "Shubham Aggarwal",
      "Dipankar Maity",
      "Tamer Ba\u015far"
    ],
    "published": "2025-04-12T01:18:53+00:00",
    "summary": "In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https://github.com/AC-sh/InterQ."
  },
  {
    "title": "Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
    "url": "http://arxiv.org/abs/2504.08704v1",
    "arxiv_id": "2504.08704v1",
    "authors": [
      "Vinal Asodia",
      "Zhenhua Feng",
      "Saber Fallah"
    ],
    "published": "2025-04-11T17:11:21+00:00",
    "summary": "Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values."
  },
  {
    "title": "Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics",
    "url": "http://arxiv.org/abs/2504.08686v1",
    "arxiv_id": "2504.08686v1",
    "authors": [
      "Alessia Loi",
      "Loona Macabre",
      "J\u00e9r\u00e9my Fersula",
      "Keivan Amini",
      "Leo Cazenille",
      "Fabien Caura",
      "Alexandre Guerre",
      "St\u00e9phane Gourichon",
      "Olivier Dauchot",
      "Nicolas Bredeche"
    ],
    "published": "2025-04-11T16:47:59+00:00",
    "summary": "This paper describes the Pogobot, an open-source and open-hardware platform specifically designed for research involving swarm robotics. Pogobot features vibration-based locomotion, infrared communication, and an array of sensors in a cost-effective package (approx. 250~euros/unit). The platform's modular design, comprehensive API, and extensible architecture facilitate the implementation of swarm intelligence algorithms and distributed online reinforcement learning algorithms. Pogobots offer an accessible alternative to existing platforms while providing advanced capabilities including directional communication between units. More than 200 Pogobots are already being used on a daily basis at Sorbonne Universit\\'e and PSL to study self-organizing systems, programmable active matter, discrete reaction-diffusion-advection systems as well as models of social learning and evolution."
  },
  {
    "title": "Reinforcement Learning-Driven Plant-Wide Refinery Planning Using Model Decomposition",
    "url": "http://arxiv.org/abs/2504.08642v1",
    "arxiv_id": "2504.08642v1",
    "authors": [
      "Zhouchang Li",
      "Runze Lin",
      "Hongye Su",
      "Lei Xie"
    ],
    "published": "2025-04-11T15:42:49+00:00",
    "summary": "In the era of smart manufacturing and Industry 4.0, the refining industry is evolving towards large-scale integration and flexible production systems. In response to these new demands, this paper presents a novel optimization framework for plant-wide refinery planning, integrating model decomposition with deep reinforcement learning. The approach decomposes the complex large scale refinery optimization problem into manageable submodels, improving computational efficiency while preserving accuracy. A reinforcement learning-based pricing mechanism is introduced to generate pricing strategies for intermediate products, facilitating better coordination between submodels and enabling rapid responses to market changes. Three industrial case studies, covering both single-period and multi-period planning, demonstrate significant improvements in computational efficiency while ensuring refinery profitability."
  },
  {
    "title": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
    "url": "http://arxiv.org/abs/2504.08604v1",
    "arxiv_id": "2504.08604v1",
    "authors": [
      "Youwei Yu",
      "Lantao Liu"
    ],
    "published": "2025-04-11T15:12:12+00:00",
    "summary": "Deep reinforcement learning can seamlessly transfer agile locomotion and navigation skills from the simulator to real world. However, bridging the sim-to-real gap with domain randomization or adversarial methods often demands expert physics knowledge to ensure policy robustness. Even so, cutting-edge simulators may fall short of capturing every real-world detail, and the reconstructed environment may introduce errors due to various perception uncertainties. To address these challenges, we propose Neural Fidelity Calibration (NFC), a novel framework that employs conditional score-based diffusion models to calibrate simulator physical coefficients and residual fidelity domains online during robot execution. Specifically, the residual fidelity reflects the simulation model shift relative to the real-world dynamics and captures the uncertainty of the perceived environment, enabling us to sample realistic environments under the inferred distribution for policy fine-tuning. Our framework is informative and adaptive in three key ways: (a) we fine-tune the pretrained policy only under anomalous scenarios, (b) we build sequential NFC online with the pretrained NFC's proposal prior, reducing the diffusion model's training burden, and (c) when NFC uncertainty is high and may degrade policy improvement, we leverage optimistic exploration to enable hallucinated policy optimization. Our framework achieves superior simulator calibration precision compared to state-of-the-art methods across diverse robots with high-dimensional parametric spaces. We study the critical contribution of residual fidelity to policy improvement in simulation and real-world experiments. Notably, our approach demonstrates robust robot navigation under challenging real-world conditions, such as a broken wheel axle on snowy surfaces."
  },
  {
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.08600v1",
    "arxiv_id": "2504.08600v1",
    "authors": [
      "Peixian Ma",
      "Xialie Zhuang",
      "Chengjin Xu",
      "Xuhui Jiang",
      "Ran Chen",
      "Jian Guo"
    ],
    "published": "2025-04-11T15:01:30+00:00",
    "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model."
  },
  {
    "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction",
    "url": "http://arxiv.org/abs/2504.08590v1",
    "arxiv_id": "2504.08590v1",
    "authors": [
      "Nicola Horst",
      "Davide Mazzaccara",
      "Antonia Schmidt",
      "Michael Sullivan",
      "Filippo Moment\u00e8",
      "Luca Franceschetti",
      "Philipp Sadler",
      "Sherzod Hakimov",
      "Alberto Testoni",
      "Raffaella Bernardi",
      "Raquel Fern\u00e1ndez",
      "Alexander Koller",
      "Oliver Lemon",
      "David Schlangen",
      "Mario Giulianelli",
      "Alessandro Suglia"
    ],
    "published": "2025-04-11T14:49:33+00:00",
    "summary": "Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for \"alignment\" (with a reward model judging the quality of instruction following attempts) and for improving \"reasoning\" (process- and outcome-based verifiers judging reasoning steps). In this paper, we explore to what extent synthetic interaction in what we call Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can provide a learning signal, and how this signal can be used. We introduce an environment for producing such interaction data (with the help of a Large Language Model as counterpart to the learner model), both offline and online. We investigate the effects of supervised fine-tuning on this data, as well as reinforcement learning setups such as DPO, and GRPO; showing that all of these approaches achieve some improvements in in-domain games, but only GRPO demonstrates the ability to generalise to out-of-domain games as well as retain competitive performance in reference-based tasks. We release the framework and the baseline training setups in the hope that this can foster research in this promising new direction."
  },
  {
    "title": "Slicing the Gaussian Mixture Wasserstein Distance",
    "url": "http://arxiv.org/abs/2504.08544v1",
    "arxiv_id": "2504.08544v1",
    "authors": [
      "Moritz Piening",
      "Robert Beinert"
    ],
    "published": "2025-04-11T13:57:09+00:00",
    "summary": "Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization"
  },
  {
    "title": "Discriminator-Free Direct Preference Optimization for Video Diffusion",
    "url": "http://arxiv.org/abs/2504.08542v1",
    "arxiv_id": "2504.08542v1",
    "authors": [
      "Haoran Cheng",
      "Qide Dong",
      "Liang Peng",
      "Zhizhou Sha",
      "Weiguo Feng",
      "Jinghui Xie",
      "Zhao Song",
      "Shilei Wen",
      "Xiaofei He",
      "Boxi Wu"
    ],
    "published": "2025-04-11T13:55:48+00:00",
    "summary": "Direct Preference Optimization (DPO), which aligns models with human preferences through win/lose data pairs, has achieved remarkable success in language and image generation. However, applying DPO to video diffusion models faces critical challenges: (1) Data inefficiency. Generating thousands of videos per DPO iteration incurs prohibitive costs; (2) Evaluation uncertainty. Human annotations suffer from subjective bias, and automated discriminators fail to detect subtle temporal artifacts like flickering or motion incoherence. To address these, we propose a discriminator-free video DPO framework that: (1) Uses original real videos as win cases and their edited versions (e.g., reversed, shuffled, or noise-corrupted clips) as lose cases; (2) Trains video diffusion models to distinguish and avoid artifacts introduced by editing. This approach eliminates the need for costly synthetic video comparisons, provides unambiguous quality signals, and enables unlimited training data expansion through simple editing operations. We theoretically prove the framework's effectiveness even when real videos and model-generated videos follow different distributions. Experiments on CogVideoX demonstrate the efficiency of the proposed method."
  },
  {
    "title": "Diffusion Models for Robotic Manipulation: A Survey",
    "url": "http://arxiv.org/abs/2504.08438v1",
    "arxiv_id": "2504.08438v1",
    "authors": [
      "Rosa Wolf",
      "Yitian Shi",
      "Sheng Liu",
      "Rania Rayyes"
    ],
    "published": "2025-04-11T11:01:11+00:00",
    "summary": "Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods."
  },
  {
    "title": "Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability",
    "url": "http://arxiv.org/abs/2504.08417v1",
    "arxiv_id": "2504.08417v1",
    "authors": [
      "Paul J. Pritz",
      "Kin K. Leung"
    ],
    "published": "2025-04-11T10:21:58+00:00",
    "summary": "Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability."
  },
  {
    "title": "Scalable Conflict-free Decision Making with Photons",
    "url": "http://arxiv.org/abs/2504.08331v1",
    "arxiv_id": "2504.08331v1",
    "authors": [
      "Kohei Konaka",
      "Andr\u00e9 R\u00f6hm",
      "Takatomo Mihana",
      "Ryoichi Horisaki"
    ],
    "published": "2025-04-11T07:54:45+00:00",
    "summary": "Quantum optics utilizes the unique properties of light for computation or communication. In this work, we explore its ability to solve certain reinforcement learning tasks, with a particular view towards the scalability of the approach. Our method utilizes the Orbital Angular Momentum (OAM) of photons to solve the Competitive Multi-Armed Bandit (CMAB) problem while maximizing rewards. In particular, we encode each player's preferences in the OAM amplitudes, while the phases are optimized to avoid conflicts. We find that the proposed system is capable of solving the CMAB problem with a scalable number of options and demonstrates improved performance over existing techniques. As an example of a system with simple rules for solving complex tasks, our OAM-based method adds to the repertoire of functionality of quantum optics."
  },
  {
    "title": "DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search",
    "url": "http://arxiv.org/abs/2504.08310v1",
    "arxiv_id": "2504.08310v1",
    "authors": [
      "Shubing Xie",
      "Aritra Sarkar",
      "Sebastian Feld"
    ],
    "published": "2025-04-11T07:23:14+00:00",
    "summary": "Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.   This work addresses the challenge of explainability in quantum architecture search (QAS) by introducing a novel genetic programming-based decompiler framework for reverse-engineering high-level quantum algorithms from low-level circuit representations. The proposed approach, implemented in the open-source tool DeQompile, employs program synthesis techniques, including symbolic regression and abstract syntax tree manipulation, to distill interpretable Qiskit algorithms from quantum assembly language. Validation of benchmark algorithms demonstrates the efficacy of our tool. By integrating the decompiler with online learning frameworks, this research potentiates explainable QAS by fostering the development of generalizable and provable quantum algorithms."
  },
  {
    "title": "DSM: Building A Diverse Semantic Map for 3D Visual Grounding",
    "url": "http://arxiv.org/abs/2504.08307v1",
    "arxiv_id": "2504.08307v1",
    "authors": [
      "Qinghongbing Xie",
      "Zijian Liang",
      "Long Zeng"
    ],
    "published": "2025-04-11T07:18:42+00:00",
    "summary": "In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scene information through geometric and visual information, overlooking the extraction of diverse semantic information from the scene and the understanding of rich implicit semantic attributes, such as appearance, physics, and affordance. The 3D scene graph, which combines geometry and language, is an ideal representation method for environmental perception and is an effective carrier for language models in 3D Visual Grounding tasks. To address these issues, we propose a diverse semantic map construction method specifically designed for robotic agents performing 3D Visual Grounding tasks. This method leverages VLMs to capture the latent semantic attributes and relations of objects within the scene and creates a Diverse Semantic Map (DSM) through a geometry sliding-window map construction strategy. We enhance the understanding of grounding information based on DSM and introduce a novel approach named DSM-Grounding. Experimental results show that our method outperforms current approaches in tasks like semantic segmentation and 3D Visual Grounding, particularly excelling in overall metrics compared to the state-of-the-art. In addition, we have deployed this method on robots to validate its effectiveness in navigation and grasping tasks."
  },
  {
    "title": "Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion",
    "url": "http://arxiv.org/abs/2504.08246v1",
    "arxiv_id": "2504.08246v1",
    "authors": [
      "Jaeyong Shin",
      "Woohyun Cha",
      "Donghyeon Kim",
      "Junhyeok Cha",
      "Jaeheung Park"
    ],
    "published": "2025-04-11T04:12:15+00:00",
    "summary": "Reinforcement learning (RL) has shown great potential in training agile and adaptable controllers for legged robots, enabling them to learn complex locomotion behaviors directly from experience. However, policies trained in simulation often fail to transfer to real-world robots due to unrealistic assumptions such as infinite actuator bandwidth and the absence of torque limits. These conditions allow policies to rely on abrupt, high-frequency torque changes, which are infeasible for real actuators with finite bandwidth.   Traditional methods address this issue by penalizing aggressive motions through regularization rewards, such as joint velocities, accelerations, and energy consumption, but they require extensive hyperparameter tuning. Alternatively, Lipschitz-Constrained Policies (LCP) enforce finite bandwidth action control by penalizing policy gradients, but their reliance on gradient calculations introduces significant GPU memory overhead. To overcome this limitation, this work proposes Spectral Normalization (SN) as an efficient replacement for enforcing Lipschitz continuity. By constraining the spectral norm of network weights, SN effectively limits high-frequency policy fluctuations while significantly reducing GPU memory usage. Experimental evaluations in both simulation and real-world humanoid robot show that SN achieves performance comparable to gradient penalty methods while enabling more efficient parallel training."
  },
  {
    "title": "Deep Distributional Learning with Non-crossing Quantile Network",
    "url": "http://arxiv.org/abs/2504.08215v1",
    "arxiv_id": "2504.08215v1",
    "authors": [
      "Guohao Shen",
      "Runpeng Dai",
      "Guojun Wu",
      "Shikai Luo",
      "Chengchun Shi",
      "Hongtu Zhu"
    ],
    "published": "2025-04-11T02:46:39+00:00",
    "summary": "In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ network-based deep distributional learning framework is highly adaptable, applicable to a wide range of applications, from classical non-parametric quantile regression to more advanced tasks such as causal effect estimation and distributional reinforcement learning (RL). We also develop a comprehensive theoretical foundation for the deep NQ estimator and its application to distributional RL, providing an in-depth analysis that demonstrates its effectiveness across these domains. Our experimental results further highlight the robustness and versatility of the NQ network."
  },
  {
    "title": "Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges",
    "url": "http://arxiv.org/abs/2504.08210v1",
    "arxiv_id": "2504.08210v1",
    "authors": [
      "Erica van der Sar",
      "Alessandro Zocca",
      "Sandjai Bhulai"
    ],
    "published": "2025-04-11T02:27:30+00:00",
    "summary": "Power grid operation is becoming increasingly complex due to the rising integration of renewable energy sources and the need for more adaptive control strategies. Reinforcement Learning (RL) has emerged as a promising approach to power network control (PNC), offering the potential to enhance decision-making in dynamic and uncertain environments. The Learning To Run a Power Network (L2RPN) competitions have played a key role in accelerating research by providing standardized benchmarks and problem formulations, leading to rapid advancements in RL-based methods. This survey provides a comprehensive and structured overview of RL applications for power grid topology optimization, categorizing existing techniques, highlighting key design choices, and identifying gaps in current research. Additionally, we present a comparative numerical study evaluating the impact of commonly applied RL-based methods, offering insights into their practical effectiveness. By consolidating existing research and outlining open challenges, this survey aims to provide a foundation for future advancements in RL-driven power grid optimization."
  },
  {
    "title": "Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation",
    "url": "http://arxiv.org/abs/2504.08195v1",
    "arxiv_id": "2504.08195v1",
    "authors": [
      "Michael Elrod",
      "Niloufar Mehrabi",
      "Rahul Amin",
      "Manveen Kaur",
      "Long Cheng",
      "Jim Martin",
      "Abolfazl Razi"
    ],
    "published": "2025-04-11T01:46:18+00:00",
    "summary": "Mission planning for a fleet of cooperative autonomous drones in applications that involve serving distributed target points, such as disaster response, environmental monitoring, and surveillance, is challenging, especially under partial observability, limited communication range, and uncertain environments. Traditional path-planning algorithms struggle in these scenarios, particularly when prior information is not available. To address these challenges, we propose a novel framework that integrates Graph Neural Networks (GNNs), Deep Reinforcement Learning (DRL), and transformer-based mechanisms for enhanced multi-agent coordination and collective task execution. Our approach leverages GNNs to model agent-agent and agent-goal interactions through adaptive graph construction, enabling efficient information aggregation and decision-making under constrained communication. A transformer-based message-passing mechanism, augmented with edge-feature-enhanced attention, captures complex interaction patterns, while a Double Deep Q-Network (Double DQN) with prioritized experience replay optimizes agent policies in partially observable environments. This integration is carefully designed to address specific requirements of multi-agent navigation, such as scalability, adaptability, and efficient task execution. Experimental results demonstrate superior performance, with 90% service provisioning and 100% grid coverage (node discovery), while reducing the average steps per episode to 200, compared to 600 for benchmark methods such as particle swarm optimization (PSO), greedy algorithms and DQN."
  },
  {
    "title": "Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention",
    "url": "http://arxiv.org/abs/2504.08183v1",
    "arxiv_id": "2504.08183v1",
    "authors": [
      "Qiuwu Sha",
      "Tengda Tang",
      "Xinyu Du",
      "Jie Liu",
      "Yixian Wang",
      "Yuan Sheng"
    ],
    "published": "2025-04-11T00:53:53+00:00",
    "summary": "This study proposes a credit card fraud detection method based on Heterogeneous Graph Neural Network (HGNN) to address fraud in complex transaction networks. Unlike traditional machine learning methods that rely solely on numerical features of transaction records, this approach constructs heterogeneous transaction graphs. These graphs incorporate multiple node types, including users, merchants, and transactions. By leveraging graph neural networks, the model captures higher-order transaction relationships. A Graph Attention Mechanism is employed to dynamically assign weights to different transaction relationships. Additionally, a Temporal Decay Mechanism is integrated to enhance the model's sensitivity to time-related fraud patterns. To address the scarcity of fraudulent transaction samples, this study applies SMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the model's ability to identify fraudulent transactions. Experimental results demonstrate that the proposed method outperforms existing GNN models, including GCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model achieves notable improvements in both accuracy and OC-ROC. Future research may explore the integration of dynamic graph neural networks and reinforcement learning. Such advancements could enhance the real-time adaptability of fraud detection systems and provide more intelligent solutions for financial risk control."
  },
  {
    "title": "Rethinking the Foundations for Continual Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.08161v1",
    "arxiv_id": "2504.08161v1",
    "authors": [
      "Michael Bowling",
      "Esraa Elelimy"
    ],
    "published": "2025-04-10T23:05:56+00:00",
    "summary": "Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations."
  },
  {
    "title": "Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing",
    "url": "http://arxiv.org/abs/2504.08134v1",
    "arxiv_id": "2504.08134v1",
    "authors": [
      "Minrui Xu",
      "Dusit Niyato",
      "Jiawen Kang",
      "Zehui Xiong",
      "Mingzhe Chen",
      "Dong In Kim",
      "Xuemin",
      "Shen"
    ],
    "published": "2025-04-10T21:26:35+00:00",
    "summary": "Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines."
  },
  {
    "title": "RL-based Control of UAS Subject to Significant Disturbance",
    "url": "http://arxiv.org/abs/2504.08114v1",
    "arxiv_id": "2504.08114v1",
    "authors": [
      "Kousheek Chakraborty",
      "Thijs Hof",
      "Ayham Alharbat",
      "Abeje Mersha"
    ],
    "published": "2025-04-10T20:25:14+00:00",
    "summary": "This paper proposes a Reinforcement Learning (RL)-based control framework for position and attitude control of an Unmanned Aerial System (UAS) subjected to significant disturbance that can be associated with an uncertain trigger signal. The proposed method learns the relationship between the trigger signal and disturbance force, enabling the system to anticipate and counteract the impending disturbances before they occur. We train and evaluate three policies: a baseline policy trained without exposure to the disturbance, a reactive policy trained with the disturbance but without the trigger signal, and a predictive policy that incorporates the trigger signal as an observation and is exposed to the disturbance during training. Our simulation results show that the predictive policy outperforms the other policies by minimizing position deviations through a proactive correction maneuver. This work highlights the potential of integrating predictive cues into RL frameworks to improve UAS performance."
  },
  {
    "title": "Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit Schemes",
    "url": "http://arxiv.org/abs/2504.08074v1",
    "arxiv_id": "2504.08074v1",
    "authors": [
      "Xiaoyi Wu",
      "Ravi Seshadri",
      "Filipe Rodrigues",
      "Carlos Lima Azevedo"
    ],
    "published": "2025-04-10T19:04:28+00:00",
    "summary": "Tradable credit schemes (TCS) are an increasingly studied alternative to congestion pricing, given their revenue neutrality and ability to address issues of equity through the initial credit allocation. Modeling TCS to aid future design and implementation is associated with challenges involving user and market behaviors, demand-supply dynamics, and control mechanisms. In this paper, we focus on the latter and address the day-to-day dynamic tolling problem under TCS, which is formulated as a discrete-time Markov Decision Process and solved using reinforcement learning (RL) algorithms. Our results indicate that RL algorithms achieve travel times and social welfare comparable to the Bayesian optimization benchmark, with generalization across varying capacities and demand levels. We further assess the robustness of RL under different hyperparameters and apply regularization techniques to mitigate action oscillation, which generates practical tolling strategies that are transferable under day-to-day demand and supply variability. Finally, we discuss potential challenges such as scaling to large networks, and show how transfer learning can be leveraged to improve computational efficiency and facilitate the practical deployment of RL-based TCS solutions."
  },
  {
    "title": "MM-IFEngine: Towards Multimodal Instruction Following",
    "url": "http://arxiv.org/abs/2504.07957v1",
    "arxiv_id": "2504.07957v1",
    "authors": [
      "Shengyuan Ding",
      "Shenxi Wu",
      "Xiangyu Zhao",
      "Yuhang Zang",
      "Haodong Duan",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "published": "2025-04-10T17:59:12+00:00",
    "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval (+12.3$\\%$). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine."
  },
  {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07954v1",
    "arxiv_id": "2504.07954v1",
    "authors": [
      "En Yu",
      "Kangheng Lin",
      "Liang Zhao",
      "Jisheng Yin",
      "Yana Wei",
      "Yuang Peng",
      "Haoran Wei",
      "Jianjian Sun",
      "Chunrui Han",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Daxin Jiang",
      "Jingyu Wang",
      "Wenbing Tao"
    ],
    "published": "2025-04-10T17:58:27+00:00",
    "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning."
  },
  {
    "title": "Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
    "url": "http://arxiv.org/abs/2504.07912v1",
    "arxiv_id": "2504.07912v1",
    "authors": [
      "Rosie Zhao",
      "Alexandru Meterez",
      "Sham Kakade",
      "Cengiz Pehlevan",
      "Samy Jelassi",
      "Eran Malach"
    ],
    "published": "2025-04-10T17:15:53+00:00",
    "summary": "Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior."
  },
  {
    "title": "Fast Adaptation with Behavioral Foundation Models",
    "url": "http://arxiv.org/abs/2504.07896v1",
    "arxiv_id": "2504.07896v1",
    "authors": [
      "Harshit Sikchi",
      "Andrea Tirinzoni",
      "Ahmed Touati",
      "Yingchen Xu",
      "Anssi Kanervisto",
      "Scott Niekum",
      "Amy Zhang",
      "Alessandro Lazaric",
      "Matteo Pirotta"
    ],
    "published": "2025-04-10T16:14:17+00:00",
    "summary": "Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines."
  },
  {
    "title": "SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos",
    "url": "http://arxiv.org/abs/2504.07867v1",
    "arxiv_id": "2504.07867v1",
    "authors": [
      "Joshua Li",
      "Fernando Jose Pena Cantu",
      "Emily Yu",
      "Alexander Wong",
      "Yuchen Cui",
      "Yuhao Chen"
    ],
    "published": "2025-04-10T15:43:10+00:00",
    "summary": "Video Scene Graph Generation (VidSGG) is an important topic in understanding dynamic kitchen environments. Current models for VidSGG require extensive training to produce scene graphs. Recently, Vision Language Models (VLM) and Vision Foundation Models (VFM) have demonstrated impressive zero-shot capabilities in a variety of tasks. However, VLMs like Gemini struggle with the dynamics for VidSGG, failing to maintain stable object identities across frames. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline that combines SAM2's temporal tracking with Gemini's semantic understanding. SAM2 also improves upon Gemini's object grounding by producing more accurate bounding boxes. In our method, we first prompt Gemini to generate a frame-level scene graph. Then, we employ a matching algorithm to map each object in the scene graph with a SAM2-generated or SAM2-propagated mask, producing a temporally-consistent scene graph in dynamic environments. Finally, we repeat this process again in each of the following frames. We empirically demonstrate that SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and EPIC-KITCHENS-100 datasets."
  },
  {
    "title": "2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization",
    "url": "http://arxiv.org/abs/2504.07856v1",
    "arxiv_id": "2504.07856v1",
    "authors": [
      "Mengyang Li",
      "Zhong Zhang"
    ],
    "published": "2025-04-10T15:32:00+00:00",
    "summary": "Aligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization."
  },
  {
    "title": "Genetic Programming with Reinforcement Learning Trained Transformer for Real-World Dynamic Scheduling Problems",
    "url": "http://arxiv.org/abs/2504.07779v1",
    "arxiv_id": "2504.07779v1",
    "authors": [
      "Xian Chen",
      "Rong Qu",
      "Jing Dong",
      "Ruibin Bai",
      "Yaochu Jin"
    ],
    "published": "2025-04-10T14:18:22+00:00",
    "summary": "Dynamic scheduling in real-world environments often struggles to adapt to unforeseen disruptions, making traditional static scheduling methods and human-designed heuristics inadequate. This paper introduces an innovative approach that combines Genetic Programming (GP) with a Transformer trained through Reinforcement Learning (GPRT), specifically designed to tackle the complexities of dynamic scheduling scenarios. GPRT leverages the Transformer to refine heuristics generated by GP while also seeding and guiding the evolution of GP. This dual functionality enhances the adaptability and effectiveness of the scheduling heuristics, enabling them to better respond to the dynamic nature of real-world tasks. The efficacy of this integrated approach is demonstrated through a practical application in container terminal truck scheduling, where the GPRT method outperforms traditional GP, standalone Transformer methods, and other state-of-the-art competitors. The key contribution of this research is the development of the GPRT method, which showcases a novel combination of GP and Reinforcement Learning (RL) to produce robust and efficient scheduling solutions. Importantly, GPRT is not limited to container port truck scheduling; it offers a versatile framework applicable to various dynamic scheduling challenges. Its practicality, coupled with its interpretability and ease of modification, makes it a valuable tool for diverse real-world scenarios."
  },
  {
    "title": "Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks",
    "url": "http://arxiv.org/abs/2504.07741v1",
    "arxiv_id": "2504.07741v1",
    "authors": [
      "Marius Kurz",
      "Andrea Beck",
      "Benjamin Sanderse"
    ],
    "published": "2025-04-10T13:37:54+00:00",
    "summary": "This work proposes a novel methodology for turbulence modeling in Large Eddy Simulation (LES) based on Graph Neural Networks (GNNs), which embeds the discrete rotational, reflectional and translational symmetries of the Navier-Stokes equations into the model architecture. In addition, suitable invariant input and output spaces are derived that allow the GNN models to be embedded seamlessly into the LES framework to obtain a symmetry-preserving simulation setup. The suitability of the proposed approach is investigated for two canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent channel flow. For both cases, GNN models are trained successfully in actual simulations using Reinforcement Learning (RL) to ensure that the models are consistent with the underlying LES formulation and discretization. It is demonstrated for the HIT case that the resulting GNN-based LES scheme recovers rotational and reflectional equivariance up to machine precision in actual simulations. At the same time, the stability and accuracy remain on par with non-symmetry-preserving machine learning models that fail to obey these properties. The same modeling strategy translates well to turbulent channel flow, where the GNN model successfully learns the more complex flow physics and is able to recover the turbulent statistics and Reynolds stresses. It is shown that the GNN model learns a zonal modeling strategy with distinct behaviors in the near-wall and outer regions. The proposed approach thus demonstrates the potential of GNNs for turbulence modeling, especially in the context of LES and RL."
  },
  {
    "title": "Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability",
    "url": "http://arxiv.org/abs/2504.07722v1",
    "arxiv_id": "2504.07722v1",
    "authors": [
      "MaryLena Bleile"
    ],
    "published": "2025-04-10T13:15:52+00:00",
    "summary": "Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference."
  },
  {
    "title": "Sim-to-Real Transfer in Reinforcement Learning for Maneuver Control of a Variable-Pitch MAV",
    "url": "http://arxiv.org/abs/2504.07694v1",
    "arxiv_id": "2504.07694v1",
    "authors": [
      "Zhikun Wang",
      "Shiyu Zhao"
    ],
    "published": "2025-04-10T12:27:59+00:00",
    "summary": "Reinforcement learning (RL) algorithms can enable high-maneuverability in unmanned aerial vehicles (MAVs), but transferring them from simulation to real-world use is challenging. Variable-pitch propeller (VPP) MAVs offer greater agility, yet their complex dynamics complicate the sim-to-real transfer. This paper introduces a novel RL framework to overcome these challenges, enabling VPP MAVs to perform advanced aerial maneuvers in real-world settings. Our approach includes real-to-sim transfer techniques-such as system identification, domain randomization, and curriculum learning to create robust training simulations and a sim-to-real transfer strategy combining a cascade control system with a fast-response low-level controller for reliable deployment. Results demonstrate the effectiveness of this framework in achieving zero-shot deployment, enabling MAVs to perform complex maneuvers such as flips and wall-backtracking."
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v1",
    "arxiv_id": "2504.07615v1",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
    "url": "http://arxiv.org/abs/2504.07615v2",
    "arxiv_id": "2504.07615v2",
    "authors": [
      "Haozhan Shen",
      "Peng Liu",
      "Jingcheng Li",
      "Chunxin Fang",
      "Yibo Ma",
      "Jiajia Liao",
      "Qiaoli Shen",
      "Zilun Zhang",
      "Kangjia Zhao",
      "Qianqian Zhang",
      "Ruochen Xu",
      "Tiancheng Zhao"
    ],
    "published": "2025-04-10T10:05:15+00:00",
    "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the \"OD aha moment\", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1"
  },
  {
    "title": "Boosting Universal LLM Reward Design through the Heuristic Reward Observation Space Evolution",
    "url": "http://arxiv.org/abs/2504.07596v1",
    "arxiv_id": "2504.07596v1",
    "authors": [
      "Zen Kit Heng",
      "Zimeng Zhao",
      "Tianhao Wu",
      "Yuanfei Wang",
      "Mingdong Wu",
      "Yangang Wang",
      "Hao Dong"
    ],
    "published": "2025-04-10T09:48:56+00:00",
    "summary": "Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward."
  },
  {
    "title": "Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution",
    "url": "http://arxiv.org/abs/2504.07596v2",
    "arxiv_id": "2504.07596v2",
    "authors": [
      "Zen Kit Heng",
      "Zimeng Zhao",
      "Tianhao Wu",
      "Yuanfei Wang",
      "Mingdong Wu",
      "Yangang Wang",
      "Hao Dong"
    ],
    "published": "2025-04-10T09:48:56+00:00",
    "summary": "Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward."
  },
  {
    "title": "Strategic learning for disturbance rejection in multi-agent systems: Nash and Minmax in graphical games",
    "url": "http://arxiv.org/abs/2504.07547v1",
    "arxiv_id": "2504.07547v1",
    "authors": [
      "Xinyang Wang",
      "Martin Guay",
      "Shimin Wang",
      "Hongwei Zhang"
    ],
    "published": "2025-04-10T08:22:33+00:00",
    "summary": "This article investigates the optimal control problem with disturbance rejection for discrete-time multi-agent systems under cooperative and non-cooperative graphical games frameworks. Given the practical challenges of obtaining accurate models, Q-function-based policy iteration methods are proposed to seek the Nash equilibrium solution for the cooperative graphical game and the distributed minmax solution for the non-cooperative graphical game. To implement these methods online, two reinforcement learning frameworks are developed, an actor-disturber-critic structure for the cooperative graphical game and an actor-adversary-disturber-critic structure for the non-cooperative graphical game. The stability of the proposed methods is rigorously analyzed, and simulation results are provided to illustrate the effectiveness of the proposed methods."
  },
  {
    "title": "Deep Learning Based Service Composition in Integrated Aerial-Terrestrial Networks",
    "url": "http://arxiv.org/abs/2504.07528v1",
    "arxiv_id": "2504.07528v1",
    "authors": [
      "Mohammad Farhoudi",
      "Masoud Shokrnezhad",
      "Somayeh Kianpisheh",
      "Tarik Taleb"
    ],
    "published": "2025-04-10T07:52:00+00:00",
    "summary": "The explosive growth of user devices and emerging applications is driving unprecedented traffic demands, accompanied by stringent Quality of Service (QoS) requirements. Addressing these challenges necessitates innovative service orchestration methods capable of seamless integration across the edge-cloud continuum. Terrestrial network-based service orchestration methods struggle to deliver timely responses to growing traffic demands or support users with poor or lack of access to terrestrial infrastructure. Exploiting both aerial and terrestrial resources in service composition increases coverage and facilitates the use of full computing and communication potentials. This paper proposes a service placement and composition mechanism for integrated aerial-terrestrial networks over the edge-cloud continuum while considering the dynamic nature of the network. The service function placement and service orchestration are modeled in an optimization framework. Considering the dynamicity, the Aerial Base Station (ABS) trajectory might not be deterministic, and their mobility pattern might not be known as assumed knowledge. Also, service requests can traverse through access nodes due to users' mobility. By incorporating predictive algorithms, including Deep Reinforcement Learning (DRL) approaches, the proposed method predicts ABS locations and service requests. Subsequently, a heuristic isomorphic graph matching approach is proposed to enable efficient, latency-aware service orchestration. Simulation results demonstrate the efficiency of the proposed prediction and service composition schemes in terms of accuracy, cost optimization, scalability, and responsiveness, ensuring timely and reliable service delivery under diverse network conditions."
  },
  {
    "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure",
    "url": "http://arxiv.org/abs/2504.07527v1",
    "arxiv_id": "2504.07527v1",
    "authors": [
      "Junjie Zhang",
      "Rushuai Yang",
      "Shunyu Liu",
      "Ting-En Lin",
      "Fei Huang",
      "Yi Chen",
      "Yongbin Li",
      "Dacheng Tao"
    ],
    "published": "2025-04-10T07:50:03+00:00",
    "summary": "In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models."
  },
  {
    "title": "Learning Joint Source-Channel Encoding in IRS-assisted Multi-User Semantic Communications",
    "url": "http://arxiv.org/abs/2504.07498v1",
    "arxiv_id": "2504.07498v1",
    "authors": [
      "Haidong Wang",
      "Songhan Zhao",
      "Lanhua Li",
      "Bo Gu",
      "Jing Xu",
      "Shimin Gong",
      "Jiawen Kang"
    ],
    "published": "2025-04-10T06:53:48+00:00",
    "summary": "In this paper, we investigate a joint source-channel encoding (JSCE) scheme in an intelligent reflecting surface (IRS)-assisted multi-user semantic communication system. Semantic encoding not only compresses redundant information, but also enhances information orthogonality in a semantic feature space. Meanwhile, the IRS can adjust the spatial orthogonality, enabling concurrent multi-user semantic communication in densely deployed wireless networks to improve spectrum efficiency. We aim to maximize the users' semantic throughput by jointly optimizing the users' scheduling, the IRS's passive beamforming, and the semantic encoding strategies. To tackle this non-convex problem, we propose an explainable deep neural network-driven deep reinforcement learning (XD-DRL) framework. Specifically, we employ a deep neural network (DNN) to serve as a joint source-channel semantic encoder, enabling transmitters to extract semantic features from raw images. By leveraging structural similarity, we assign some DNN weight coefficients as the IRS's phase shifts, allowing simultaneous optimization of IRS's passive beamforming and DNN training. Given the IRS's passive beamforming and semantic encoding strategies, user scheduling is optimized using the DRL method. Numerical results validate that our JSCE scheme achieves superior semantic throughput compared to the conventional schemes and efficiently reduces the semantic encoder's mode size in multi-user scenarios."
  },
  {
    "title": "Kimi-VL Technical Report",
    "url": "http://arxiv.org/abs/2504.07491v1",
    "arxiv_id": "2504.07491v1",
    "authors": [
      "Kimi Team",
      "Angang Du",
      "Bohong Yin",
      "Bowei Xing",
      "Bowen Qu",
      "Bowen Wang",
      "Cheng Chen",
      "Chenlin Zhang",
      "Chenzhuang Du",
      "Chu Wei",
      "Congcong Wang",
      "Dehao Zhang",
      "Dikang Du",
      "Dongliang Wang",
      "Enming Yuan",
      "Enzhe Lu",
      "Fang Li",
      "Flood Sung",
      "Guangda Wei",
      "Guokun Lai",
      "Han Zhu",
      "Hao Ding",
      "Hao Hu",
      "Hao Yang",
      "Hao Zhang",
      "Haoning Wu",
      "Haotian Yao",
      "Haoyu Lu",
      "Heng Wang",
      "Hongcheng Gao",
      "Huabin Zheng",
      "Jiaming Li",
      "Jianlin Su",
      "Jianzhou Wang",
      "Jiaqi Deng",
      "Jiezhong Qiu",
      "Jin Xie",
      "Jinhong Wang",
      "Jingyuan Liu",
      "Junjie Yan",
      "Kun Ouyang",
      "Liang Chen",
      "Lin Sui",
      "Longhui Yu",
      "Mengfan Dong",
      "Mengnan Dong",
      "Nuo Xu",
      "Pengyu Cheng",
      "Qizheng Gu",
      "Runjie Zhou",
      "Shaowei Liu",
      "Sihan Cao",
      "Tao Yu",
      "Tianhui Song",
      "Tongtong Bai",
      "Wei Song",
      "Weiran He",
      "Weixiao Huang",
      "Weixin Xu",
      "Xiaokun Yuan",
      "Xingcheng Yao",
      "Xingzhe Wu",
      "Xinxing Zu",
      "Xinyu Zhou",
      "Xinyuan Wang",
      "Y. Charles",
      "Yan Zhong",
      "Yang Li",
      "Yangyang Hu",
      "Yanru Chen",
      "Yejie Wang",
      "Yibo Liu",
      "Yibo Miao",
      "Yidao Qin",
      "Yimin Chen",
      "Yiping Bao",
      "Yiqin Wang",
      "Yongsheng Kang",
      "Yuanxin Liu",
      "Yulun Du",
      "Yuxin Wu",
      "Yuzhi Wang",
      "Yuzi Yan",
      "Zaida Zhou",
      "Zhaowei Li",
      "Zhejun Jiang",
      "Zheng Zhang",
      "Zhilin Yang",
      "Zhiqi Huang",
      "Zihao Huang",
      "Zijia Zhao",
      "Ziwei Chen"
    ],
    "published": "2025-04-10T06:48:26+00:00",
    "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL."
  },
  {
    "title": "Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games",
    "url": "http://arxiv.org/abs/2504.07425v1",
    "arxiv_id": "2504.07425v1",
    "authors": [
      "Shouren Wang",
      "Zehua Jiang",
      "Fernando Sliva",
      "Sam Earle",
      "Julian Togelius"
    ],
    "published": "2025-04-10T03:38:06+00:00",
    "summary": "Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres. However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players. To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II. The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents. In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents. In addition, we investigate and model several key factors that affect the enjoyability of the opponent. The experiments demonstrate improvements from 64. 36% to 156. 36% in the execution of advanced skills over baseline methods. The trained agents also exhibit distinct game-playing styles. Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system."
  },
  {
    "title": "State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements",
    "url": "http://arxiv.org/abs/2504.07393v1",
    "arxiv_id": "2504.07393v1",
    "authors": [
      "Wonjin Song",
      "Feng Bao"
    ],
    "published": "2025-04-10T02:20:45+00:00",
    "summary": "Reliable state estimation is essential for autonomous systems operating in complex, noisy environments. Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains. Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance. In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements. By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty. Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering. Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise."
  },
  {
    "title": "PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning",
    "url": "http://arxiv.org/abs/2504.07383v1",
    "arxiv_id": "2504.07383v1",
    "authors": [
      "Vahid Eghbal Akhlaghi",
      "Reza Zandehshahvar",
      "Pascal Van Hentenryck"
    ],
    "published": "2025-04-10T02:04:29+00:00",
    "summary": "This paper considers how to fuse Machine Learning (ML) and optimization to solve large-scale Supply Chain Planning (SCP) optimization problems. These problems can be formulated as MIP models which feature both integer (non-binary) and continuous variables, as well as flow balance and capacity constraints. This raises fundamental challenges for existing integrations of ML and optimization that have focused on binary MIPs and graph problems. To address these, the paper proposes PROPEL, a new framework that combines optimization with both supervised and Deep Reinforcement Learning (DRL) to reduce the size of search space significantly. PROPEL uses supervised learning, not to predict the values of all integer variables, but to identify the variables that are fixed to zero in the optimal solution, leveraging the structure of SCP applications. PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed to improve solution quality when the supervised learning step does not produce a solution with the desired optimality tolerance. PROPEL has been applied to industrial supply chain planning optimizations with millions of variables. The computational results show dramatic improvements in solution times and quality, including a 60% reduction in primal integral and an 88% primal gap reduction, and improvement factors of up to 13.57 and 15.92, respectively."
  },
  {
    "title": "Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments",
    "url": "http://arxiv.org/abs/2504.07283v1",
    "arxiv_id": "2504.07283v1",
    "authors": [
      "Licheng Luo",
      "Mingyu Cai"
    ],
    "published": "2025-04-09T21:19:51+00:00",
    "summary": "Deep Reinforcement Learning (DRL) has emerged as a powerful model-free paradigm for learning optimal policies. However, in real-world navigation tasks, DRL methods often suffer from insufficient exploration, particularly in cluttered environments with sparse rewards or complex dynamics under system disturbances. To address this challenge, we bridge general graph-based motion planning with DRL, enabling agents to explore cluttered spaces more effectively and achieve desired navigation performance. Specifically, we design a dense reward function grounded in a graph structure that spans the entire state space. This graph provides rich guidance, steering the agent toward optimal strategies. We validate our approach in challenging environments, demonstrating substantial improvements in exploration efficiency and task success rates. The project website is available at: https://plen1lune.github.io/overcome_exploration/"
  },
  {
    "title": "Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
    "url": "http://arxiv.org/abs/2504.07273v1",
    "arxiv_id": "2504.07273v1",
    "authors": [
      "Michael K\u00f6lle",
      "Alexander Feist",
      "Jonas Stein",
      "Sebastian W\u00f6lckert",
      "Claudia Linnhoff-Popien"
    ],
    "published": "2025-04-09T21:00:41+00:00",
    "summary": "In recent years, neural networks (NNs) have driven significant advances in machine learning. However, as tasks grow more complex, NNs often require large numbers of trainable parameters, which increases computational and energy demands. Variational quantum circuits (VQCs) offer a promising alternative: they leverage quantum mechanics to capture intricate relationships and typically need fewer parameters. In this work, we evaluate NNs and VQCs on simple supervised and reinforcement learning tasks, examining models with different parameter sizes. We simulate VQCs and execute selected parts of the training process on real quantum hardware to approximate actual training times. Our results show that VQCs can match NNs in performance while using significantly fewer parameters, despite longer training durations. As quantum technology and algorithms advance, and VQC architectures improve, we posit that VQCs could become advantageous for certain machine learning tasks."
  },
  {
    "title": "Better Decisions through the Right Causal World Model",
    "url": "http://arxiv.org/abs/2504.07257v1",
    "arxiv_id": "2504.07257v1",
    "authors": [
      "Elisabeth Dillies",
      "Quentin Delfosse",
      "Jannis Bl\u00fcml",
      "Raban Emunds",
      "Florian Peter Busch",
      "Kristian Kersting"
    ],
    "published": "2025-04-09T20:29:13+00:00",
    "summary": "Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability.   By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning."
  },
  {
    "title": "Reinforcement Learning Dynamics of Network Vaccination and Hysteresis: A Double-Edged Sword for Addressing Vaccine Hesitancy",
    "url": "http://arxiv.org/abs/2504.07254v1",
    "arxiv_id": "2504.07254v1",
    "authors": [
      "Atticus McWhorter",
      "Feng Fu"
    ],
    "published": "2025-04-09T20:15:41+00:00",
    "summary": "Mass vaccination remains a long-lasting challenge for disease control and prevention with upticks in vaccine hesitancy worldwide. Here, we introduce an experience-based learning (Q-learning) dynamics model of vaccination behavior in social networks, where agents choose whether or not to vaccinate given environmental feedbacks from their local neighborhood. We focus on how bounded rationality of individuals impacts decision-making of irrational agents in networks. Additionally, we observe hysteresis behavior and bistability with respect to vaccination cost and the Q-learning hyperparameters such as discount rate. Our results offer insight into the complexities of Q-learning and particularly how foresightedness of individuals will help mitigate - or conversely deteriorate, therefore acting as a double-edged sword - collective action problems in important contexts like vaccination. We also find a diversification of uptake choices, with individuals evolving into complete opt-in vs. complete opt-out. Our results have real-world implications for targeting the persistence of vaccine hesitancy using an interdisciplinary computational social science approach integrating social networks, game theory, and learning dynamics."
  },
  {
    "title": "Kernpiler: Compiler Optimization for Quantum Hamiltonian Simulation with Partial Trotterization",
    "url": "http://arxiv.org/abs/2504.07214v1",
    "arxiv_id": "2504.07214v1",
    "authors": [
      "Ethan Decker",
      "Lucas Goetz",
      "Evan McKinney",
      "Erik Gustafson",
      "Junyu Zhou",
      "Yuhao Liu",
      "Alex K. Jones",
      "Ang Li",
      "Alexander Schuckert",
      "Samuel Stein",
      "Eleanor Crane",
      "Gushu Li"
    ],
    "published": "2025-04-09T18:41:31+00:00",
    "summary": "Quantum computing promises transformative impacts in simulating Hamiltonian dynamics, essential for studying physical systems inaccessible by classical computing. However, existing compilation techniques for Hamiltonian simulation, in particular the commonly used Trotter formulas struggle to provide gate counts feasible on current quantum computers for beyond-classical simulations. We propose partial Trotterization, where sets of non-commuting Hamiltonian terms are directly compiled allowing for less error per Trotter step and therefore a reduction of Trotter steps overall. Furthermore, a suite of novel optimizations are introduced which complement the new partial Trotterization technique, including reinforcement learning for complex unitary decompositions and high level Hamiltonian analysis for unitary reduction. We demonstrate with numerical simulations across spin and fermionic Hamiltonians that compared to state of the art methods such as Qiskit's Rustiq and Qiskit's Paulievolutiongate, our novel compiler presents up to 10x gate and depth count reductions."
  },
  {
    "title": "Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.07095v1",
    "arxiv_id": "2504.07095v1",
    "authors": [
      "Chenjie Hao",
      "Weyl Lu",
      "Yifan Xu",
      "Yubei Chen"
    ],
    "published": "2025-04-09T17:59:32+00:00",
    "summary": "An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems."
  },
  {
    "title": "AssistanceZero: Scalably Solving Assistance Games",
    "url": "http://arxiv.org/abs/2504.07091v1",
    "arxiv_id": "2504.07091v1",
    "authors": [
      "Cassidy Laidlaw",
      "Eli Bronstein",
      "Timothy Guo",
      "Dylan Feng",
      "Lukas Berglund",
      "Justin Svegliato",
      "Stuart Russell",
      "Anca Dragan"
    ],
    "published": "2025-04-09T17:59:03+00:00",
    "summary": "Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Our code and models are available at https://github.com/cassidylaidlaw/minecraft-building-assistance-game."
  },
  {
    "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility",
    "url": "http://arxiv.org/abs/2504.07086v1",
    "arxiv_id": "2504.07086v1",
    "authors": [
      "Andreas Hochlehnert",
      "Hardik Bhatnagar",
      "Vishaal Udandarao",
      "Samuel Albanie",
      "Ameya Prabhu",
      "Matthias Bethge"
    ],
    "published": "2025-04-09T17:58:17+00:00",
    "summary": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work."
  },
  {
    "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning",
    "url": "http://arxiv.org/abs/2504.07052v1",
    "arxiv_id": "2504.07052v1",
    "authors": [
      "Tian Qin",
      "David Alvarez-Melis",
      "Samy Jelassi",
      "Eran Malach"
    ],
    "published": "2025-04-09T17:12:49+00:00",
    "summary": "Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling--especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage \"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm."
  },
  {
    "title": "Free Random Projection for In-Context Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.06983v1",
    "arxiv_id": "2504.06983v1",
    "authors": [
      "Tomohiro Hayase",
      "Beno\u00eet Collins",
      "Nakamasa Inoue"
    ],
    "published": "2025-04-09T15:38:50+00:00",
    "summary": "Hierarchical inductive biases are hypothesized to promote generalizable policies in reinforcement learning, as demonstrated by explicit hyperbolic latent representations and architectures. Therefore, a more flexible approach is to have these biases emerge naturally from the algorithm. We introduce Free Random Projection, an input mapping grounded in free probability theory that constructs random orthogonal matrices where hierarchical structure arises inherently. The free random projection integrates seamlessly into existing in-context reinforcement learning frameworks by encoding hierarchical organization within the input space without requiring explicit architectural modifications. Empirical results on multi-environment benchmarks show that free random projection consistently outperforms the standard random projection, leading to improvements in generalization. Furthermore, analyses within linearly solvable Markov decision processes and investigations of the spectrum of kernel random matrices reveal the theoretical underpinnings of free random projection's enhanced performance, highlighting its capacity for effective adaptation in hierarchically structured state spaces."
  },
  {
    "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2504.06958v1",
    "arxiv_id": "2504.06958v1",
    "authors": [
      "Xinhao Li",
      "Ziang Yan",
      "Desen Meng",
      "Lu Dong",
      "Xiangyu Zeng",
      "Yinan He",
      "Yali Wang",
      "Yu Qiao",
      "Yi Wang",
      "Limin Wang"
    ],
    "published": "2025-04-09T15:09:27+00:00",
    "summary": "Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs."
  },
  {
    "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning",
    "url": "http://arxiv.org/abs/2504.06958v2",
    "arxiv_id": "2504.06958v2",
    "authors": [
      "Xinhao Li",
      "Ziang Yan",
      "Desen Meng",
      "Lu Dong",
      "Xiangyu Zeng",
      "Yinan He",
      "Yali Wang",
      "Yu Qiao",
      "Yi Wang",
      "Limin Wang"
    ],
    "published": "2025-04-09T15:09:27+00:00",
    "summary": "Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs."
  },
  {
    "title": "Regret Bounds for Robust Online Decision Making",
    "url": "http://arxiv.org/abs/2504.06820v1",
    "arxiv_id": "2504.06820v1",
    "authors": [
      "Alexander Appel",
      "Vanessa Kosoy"
    ],
    "published": "2025-04-09T12:25:00+00:00",
    "summary": "We propose a framework which generalizes \"decision making with structured observations\" by allowing robust (i.e. multivalued) models. In this framework, each model associates each decision with a convex set of probability distributions over outcomes. Nature can choose distributions out of this set in an arbitrary (adversarial) manner, that can be nonoblivious and depend on past history. The resulting framework offers much greater generality than classical bandits and reinforcement learning, since the realizability assumption becomes much weaker and more realistic. We then derive a theory of regret bounds for this framework. Although our lower and upper bounds are not tight, they are sufficient to fully characterize power-law learnability. We demonstrate this theory in two special cases: robust linear bandits and tabular robust online reinforcement learning. In both cases, we derive regret bounds that improve state-of-the-art (except that we do not address computational efficiency)."
  },
  {
    "title": "Learning global control of underactuated systems with Model-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.06721v1",
    "arxiv_id": "2504.06721v1",
    "authors": [
      "Niccol\u00f2 Turcato",
      "Marco Cal\u00ec",
      "Alberto Dalla Libera",
      "Giulio Giacomuzzo",
      "Ruggero Carli",
      "Diego Romeres"
    ],
    "published": "2025-04-09T09:20:37+00:00",
    "summary": "This short paper describes our proposed solution for the third edition of the \"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed Monte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL algorithm recognized for its exceptional data efficiency across various low-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta pendulum systems. MC-PILCO optimizes a system dynamics model using interaction data, enabling policy refinement through simulation rather than direct system data optimization. This approach has proven highly effective in physical systems, offering greater data efficiency than Model-Free (MF) alternatives. Notably, MC-PILCO has previously won the first two editions of this competition, demonstrating its robustness in both simulated and real-world environments. Besides briefly reviewing the algorithm, we discuss the most critical aspects of the MC-PILCO implementation in the tasks at hand: learning a global policy for the pendubot and acrobot systems."
  },
  {
    "title": "SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination",
    "url": "http://arxiv.org/abs/2504.06684v1",
    "arxiv_id": "2504.06684v1",
    "authors": [
      "Delin Zhao",
      "Yanbo Shan",
      "Chang Liu",
      "Shenghang Lin",
      "Yingxin Shou",
      "Bin Xu"
    ],
    "published": "2025-04-09T08:41:57+00:00",
    "summary": "Multi-Agent Reinforcement Learning is widely used for multi-robot coordination, where simple graphs typically model pairwise interactions. However, such representations fail to capture higher-order collaborations, limiting effectiveness in complex tasks. While hypergraph-based approaches enhance cooperation, existing methods often generate arbitrary hypergraph structures and lack adaptability to environmental uncertainties. To address these challenges, we propose the Skewness-Driven Hypergraph Network (SDHN), which employs stochastic Bernoulli hyperedges to explicitly model higher-order multi-robot interactions. By introducing a skewness loss, SDHN promotes an efficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots to prioritize localized synchronization while still adhering to the overall information, similar to human coordination. Extensive experiments on Moving Agents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness, demonstrating superior performance over state-of-the-art baselines."
  },
  {
    "title": "Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.06683v1",
    "arxiv_id": "2504.06683v1",
    "authors": [
      "Llewyn Salt",
      "Marcus Gallagher"
    ],
    "published": "2025-04-09T08:41:27+00:00",
    "summary": "Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning."
  },
  {
    "title": "Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making",
    "url": "http://arxiv.org/abs/2504.06670v1",
    "arxiv_id": "2504.06670v1",
    "authors": [
      "Kaifeng Wang",
      "Yinsong Chen",
      "Qi Liu",
      "Xueyuan Li",
      "Xin Gao"
    ],
    "published": "2025-04-09T08:13:14+00:00",
    "summary": "In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters."
  },
  {
    "title": "RAMBO: RL-augmented Model-based Optimal Control for Whole-body Loco-manipulation",
    "url": "http://arxiv.org/abs/2504.06662v1",
    "arxiv_id": "2504.06662v1",
    "authors": [
      "Jin Cheng",
      "Dongho Kang",
      "Gabriele Fadini",
      "Guanya Shi",
      "Stelian Coros"
    ],
    "published": "2025-04-09T07:53:09+00:00",
    "summary": "Loco-manipulation -- coordinated locomotion and physical interaction with objects -- remains a major challenge for legged robots due to the need for both accurate force interaction and robustness to unmodeled dynamics. While model-based controllers provide interpretable dynamics-level planning and optimization, they are limited by model inaccuracies and computational cost. In contrast, learning-based methods offer robustness while struggling with precise modulation of interaction forces. We introduce RAMBO -- RL-Augmented Model-Based Optimal Control -- a hybrid framework that integrates model-based reaction force optimization using a simplified dynamics model and a feedback policy trained with reinforcement learning. The model-based module generates feedforward torques by solving a quadratic program, while the policy provides feedback residuals to enhance robustness in control execution. We validate our framework on a quadruped robot across a diverse set of real-world loco-manipulation tasks -- such as pushing a shopping cart, balancing a plate, and holding soft objects -- in both quadrupedal and bipedal walking. Our experiments demonstrate that RAMBO enables precise manipulation while achieving robust and dynamic locomotion, surpassing the performance of policies trained with end-to-end scheme. In addition, our method enables flexible trade-off between end-effector tracking accuracy with compliance."
  },
  {
    "title": "Domain-Conditioned Scene Graphs for State-Grounded Task Planning",
    "url": "http://arxiv.org/abs/2504.06661v1",
    "arxiv_id": "2504.06661v1",
    "authors": [
      "Jonas Herzog",
      "Jiangpin Liu",
      "Yue Wang"
    ],
    "published": "2025-04-09T07:51:46+00:00",
    "summary": "Recent robotic task planning frameworks have integrated large multimodal models (LMMs) such as GPT-4V. To address grounding issues of such models, it has been suggested to split the pipeline into perceptional state grounding and subsequent state-based planning. As we show in this work, the state grounding ability of LMM-based approaches is still limited by weaknesses in granular, structured, domain-specific scene understanding. To address this shortcoming, we develop a more structured state grounding framework that features a domain-conditioned scene graph as its scene representation. We show that such representation is actionable in nature as it is directly mappable to a symbolic state in classical planning languages such as PDDL. We provide an instantiation of our state grounding framework where the domain-conditioned scene graph generation is implemented with a lightweight vision-language approach that classifies domain-specific predicates on top of domain-relevant object detections. Evaluated across three domains, our approach achieves significantly higher state estimation accuracy and task planning success rates compared to the previous LMM-based approaches."
  },
  {
    "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
    "url": "http://arxiv.org/abs/2504.06659v1",
    "arxiv_id": "2504.06659v1",
    "authors": [
      "Xiaohua Feng",
      "Yuyuan Li",
      "Huwei Ji",
      "Jiaming Zhang",
      "Li Zhang",
      "Tianyu Du",
      "Chaochao Chen"
    ],
    "published": "2025-04-09T07:49:08+00:00",
    "summary": "Despite advances in Preference Alignment (PA) for Large Language Models (LLMs), mainstream methods like Reinforcement Learning with Human Feedback (RLHF) face notable challenges. These approaches require high-quality datasets of positive preference examples, which are costly to obtain and computationally intensive due to training instability, limiting their use in low-resource scenarios. LLM unlearning technique presents a promising alternative, by directly removing the influence of negative examples. However, current research has primarily focused on empirical validation, lacking systematic quantitative analysis. To bridge this gap, we propose a framework to explore the relationship between PA and LLM unlearning. Specifically, we introduce a bi-level optimization-based method to quantify the impact of unlearning specific negative examples on PA performance. Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples. Building on this insight, we pose a crucial question: how can we optimally select and weight negative examples for unlearning to maximize PA performance? To answer this, we propose a framework called Unlearning to Align (U2A), which leverages bi-level optimization to efficiently select and unlearn examples for optimal PA performance. We validate the proposed method through extensive experiments, with results confirming its effectiveness."
  },
  {
    "title": "Human-like compositional learning of visually-grounded concepts using synthetic environments",
    "url": "http://arxiv.org/abs/2504.06618v1",
    "arxiv_id": "2504.06618v1",
    "authors": [
      "Zijun Lin",
      "M Ganesh Kumar",
      "Cheston Tan"
    ],
    "published": "2025-04-09T06:33:28+00:00",
    "summary": "The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence. While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial and error. To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction. These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both. The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to \"some blue cubes below the red sphere\". We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts. Second, we show that curriculum learning, a strategy humans employ, enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts. Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations. Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency."
  },
  {
    "title": "Wanting to be Understood",
    "url": "http://arxiv.org/abs/2504.06611v1",
    "arxiv_id": "2504.06611v1",
    "authors": [
      "Chrisantha Fernando",
      "Dylan Banarse",
      "Simon Osindero"
    ],
    "published": "2025-04-09T06:15:24+00:00",
    "summary": "This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand \\textit{and to be understood} even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other."
  },
  {
    "title": "Wanting to be Understood",
    "url": "http://arxiv.org/abs/2504.06611v2",
    "arxiv_id": "2504.06611v2",
    "authors": [
      "Chrisantha Fernando",
      "Dylan Banarse",
      "Simon Osindero"
    ],
    "published": "2025-04-09T06:15:24+00:00",
    "summary": "This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other."
  },
  {
    "title": "Do Reasoning Models Show Better Verbalized Calibration?",
    "url": "http://arxiv.org/abs/2504.06564v1",
    "arxiv_id": "2504.06564v1",
    "authors": [
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Leyang Cui",
      "Rob Voigt"
    ],
    "published": "2025-04-09T03:58:19+00:00",
    "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs."
  },
  {
    "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion",
    "url": "http://arxiv.org/abs/2504.06562v1",
    "arxiv_id": "2504.06562v1",
    "authors": [
      "Longguang Zhong",
      "Fanqi Wan",
      "Ziyi Yang",
      "Guosheng Liang",
      "Tianyuan Shi",
      "Xiaojun Quan"
    ],
    "published": "2025-04-09T03:51:53+00:00",
    "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating the knowledge and capabilities of multiple structurally diverse models. However, existing approaches often rely solely on selecting the best output for each prompt from source models, which underutilizes their full potential due to limited source knowledge and results in sparse optimization signals. To address this limitation, we propose FuseRL, a novel two-stage framework comprising FuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT establishes a robust initialization by integrating the strengths of heterogeneous source models through weighted supervised fine-tuning (SFT) on diverse outputs for each prompt. FusePO optimizes weighted preferences based on the outputs of multiple source models to enable superior alignment performance. Extensive experiments demonstrate the effectiveness of our framework across various preference alignment methods, including RLOO, DPO, and SimPO. Using Llama-3.1-8B-Instruct as the target model, our approach achieves state-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard benchmarks. Further analysis suggests that FuseSFT regularizes the training process to reduce overfitting, while FusePO introduces dense and diverse signals for preference optimization."
  },
  {
    "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
    "url": "http://arxiv.org/abs/2504.06553v1",
    "arxiv_id": "2504.06553v1",
    "authors": [
      "Yun Chang",
      "Leonor Fermoselle",
      "Duy Ta",
      "Bernadette Bucher",
      "Luca Carlone",
      "Jiuguang Wang"
    ],
    "published": "2025-04-09T03:22:52+00:00",
    "summary": "While recent work in scene reconstruction and understanding has made strides in grounding natural language to physical 3D environments, it is still challenging to ground abstract, high-level instructions to a 3D scene. High-level instructions might not explicitly invoke semantic elements in the scene, and even the process of breaking a high-level task into a set of more concrete subtasks, a process called hierarchical task analysis, is environment-dependent. In this work, we propose ASHiTA, the first framework that generates a task hierarchy grounded to a 3D scene graph by breaking down high-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted hierarchical task analysis, to generate the task breakdown, with task-driven 3D scene graph construction to generate a suitable representation of the environment. Our experiments show that ASHiTA performs significantly better than LLM baselines in breaking down high-level tasks into environment-dependent subtasks and is additionally able to achieve grounding performance comparable to state-of-the-art methods."
  },
  {
    "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
    "url": "http://arxiv.org/abs/2504.06553v2",
    "arxiv_id": "2504.06553v2",
    "authors": [
      "Yun Chang",
      "Leonor Fermoselle",
      "Duy Ta",
      "Bernadette Bucher",
      "Luca Carlone",
      "Jiuguang Wang"
    ],
    "published": "2025-04-09T03:22:52+00:00",
    "summary": "While recent work in scene reconstruction and understanding has made strides in grounding natural language to physical 3D environments, it is still challenging to ground abstract, high-level instructions to a 3D scene. High-level instructions might not explicitly invoke semantic elements in the scene, and even the process of breaking a high-level task into a set of more concrete subtasks, a process called hierarchical task analysis, is environment-dependent. In this work, we propose ASHiTA, the first framework that generates a task hierarchy grounded to a 3D scene graph by breaking down high-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted hierarchical task analysis, to generate the task breakdown, with task-driven 3D scene graph construction to generate a suitable representation of the environment. Our experiments show that ASHiTA performs significantly better than LLM baselines in breaking down high-level tasks into environment-dependent subtasks and is additionally able to achieve grounding performance comparable to state-of-the-art methods."
  },
  {
    "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
    "url": "http://arxiv.org/abs/2504.06514v1",
    "arxiv_id": "2504.06514v1",
    "authors": [
      "Chenrui Fan",
      "Ming Li",
      "Lichao Sun",
      "Tianyi Zhou"
    ],
    "published": "2025-04-09T01:25:27+00:00",
    "summary": "We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem."
  },
  {
    "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
    "url": "http://arxiv.org/abs/2504.06386v1",
    "arxiv_id": "2504.06386v1",
    "authors": [
      "Jacques Cloete",
      "Nikolaus Vertovec",
      "Alessandro Abate"
    ],
    "published": "2025-04-08T19:09:07+00:00",
    "summary": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates."
  },
  {
    "title": "An Information-Geometric Approach to Artificial Curiosity",
    "url": "http://arxiv.org/abs/2504.06355v1",
    "arxiv_id": "2504.06355v1",
    "authors": [
      "Alexander Nedergaard",
      "Pablo A. Morales"
    ],
    "published": "2025-04-08T18:04:15+00:00",
    "summary": "Learning in environments with sparse rewards remains a fundamental challenge in reinforcement learning. Artificial curiosity addresses this limitation through intrinsic rewards to guide exploration, however, the precise formulation of these rewards has remained elusive. Ideally, such rewards should depend on the agent's information about the environment, remaining agnostic to the representation of the information -- an invariance central to information geometry. Leveraging information geometry, we show that invariance under congruent Markov morphisms and the agent-environment interaction, uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy. Additional geometrically motivated restrictions effectively limits the candidates to those determined by a real parameter that governs the occupancy space geometry. Remarkably, special values of this parameter are found to correspond to count-based and maximum entropy exploration, revealing a geometric exploration-exploitation trade-off. This framework provides important constraints to the engineering of intrinsic reward while integrating foundational exploration methods into a single, cohesive model."
  },
  {
    "title": "Adversarial Training of Reward Models",
    "url": "http://arxiv.org/abs/2504.06141v1",
    "arxiv_id": "2504.06141v1",
    "authors": [
      "Alexander Bukharin",
      "Haifeng Qian",
      "Shengyang Sun",
      "Adithya Renduchintala",
      "Soumye Singhal",
      "Zhilin Wang",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Tuo Zhao"
    ],
    "published": "2025-04-08T15:38:25+00:00",
    "summary": "Reward modeling has emerged as a promising approach for the scalable alignment of language models. However, contemporary reward models (RMs) often lack robustness, awarding high rewards to low-quality, out-of-distribution (OOD) samples. This can lead to reward hacking, where policies exploit unintended shortcuts to maximize rewards, undermining alignment. To address this challenge, we introduce Adv-RM, a novel adversarial training framework that automatically identifies adversarial examples -- responses that receive high rewards from the target RM but are OOD and of low quality. By leveraging reinforcement learning, Adv-RM trains a policy to generate adversarial examples that reliably expose vulnerabilities in large state-of-the-art reward models such as Nemotron 340B RM. Incorporating these adversarial examples into the reward training process improves the robustness of RMs, mitigating reward hacking and enhancing downstream performance in RLHF. We demonstrate that Adv-RM significantly outperforms conventional RM training, increasing stability and enabling more effective RLHF training in both synthetic and real-data settings."
  },
  {
    "title": "A Multimedia Analytics Model for the Foundation Model Era",
    "url": "http://arxiv.org/abs/2504.06138v1",
    "arxiv_id": "2504.06138v1",
    "authors": [
      "Marcel Worring",
      "Jan Zah\u00e1lka",
      "Stef van den Elzen",
      "Maximilian Fischer",
      "Daniel Keim"
    ],
    "published": "2025-04-08T15:35:59+00:00",
    "summary": "The rapid advances in Foundation Models and agentic Artificial Intelligence are transforming multimedia analytics by enabling richer, more sophisticated interactions between humans and analytical systems. Existing conceptual models for visual and multimedia analytics, however, do not adequately capture the complexity introduced by these powerful AI paradigms. To bridge this gap, we propose a comprehensive multimedia analytics model specifically designed for the foundation model era. Building upon established frameworks from visual analytics, multimedia analytics, knowledge generation, analytic task definition, mixed-initiative guidance, and human-in-the-loop reinforcement learning, our model emphasizes integrated human-AI teaming based on visual analytics agents from both technical and conceptual perspectives. Central to the model is a seamless, yet explicitly separable, interaction channel between expert users and semi-autonomous analytical processes, ensuring continuous alignment between user intent and AI behavior. The model addresses practical challenges in sensitive domains such as intelligence analysis, investigative journalism, and other fields handling complex, high-stakes data. We illustrate through detailed case studies how our model facilitates deeper understanding and targeted improvement of multimedia analytics solutions. By explicitly capturing how expert users can optimally interact with and guide AI-powered multimedia analytics systems, our conceptual framework sets a clear direction for system design, comparison, and future research."
  },
  {
    "title": "A Multimedia Analytics Model for the Foundation Model Era",
    "url": "http://arxiv.org/abs/2504.06138v2",
    "arxiv_id": "2504.06138v2",
    "authors": [
      "Marcel Worring",
      "Jan Zah\u00e1lka",
      "Stef van den Elzen",
      "Maximilian T. Fischer",
      "Daniel A. Keim"
    ],
    "published": "2025-04-08T15:35:59+00:00",
    "summary": "The rapid advances in Foundation Models and agentic Artificial Intelligence are transforming multimedia analytics by enabling richer, more sophisticated interactions between humans and analytical systems. Existing conceptual models for visual and multimedia analytics, however, do not adequately capture the complexity introduced by these powerful AI paradigms. To bridge this gap, we propose a comprehensive multimedia analytics model specifically designed for the foundation model era. Building upon established frameworks from visual analytics, multimedia analytics, knowledge generation, analytic task definition, mixed-initiative guidance, and human-in-the-loop reinforcement learning, our model emphasizes integrated human-AI teaming based on visual analytics agents from both technical and conceptual perspectives. Central to the model is a seamless, yet explicitly separable, interaction channel between expert users and semi-autonomous analytical processes, ensuring continuous alignment between user intent and AI behavior. The model addresses practical challenges in sensitive domains such as intelligence analysis, investigative journalism, and other fields handling complex, high-stakes data. We illustrate through detailed case studies how our model facilitates deeper understanding and targeted improvement of multimedia analytics solutions. By explicitly capturing how expert users can optimally interact with and guide AI-powered multimedia analytics systems, our conceptual framework sets a clear direction for system design, comparison, and future research."
  },
  {
    "title": "Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms",
    "url": "http://arxiv.org/abs/2504.06126v1",
    "arxiv_id": "2504.06126v1",
    "authors": [
      "Ido Greenberg",
      "Piotr Sielski",
      "Hugo Linsenmaier",
      "Rajesh Gandham",
      "Shie Mannor",
      "Alex Fender",
      "Gal Chechik",
      "Eli Meirom"
    ],
    "published": "2025-04-08T15:21:01+00:00",
    "summary": "Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson Problem and are a fundamental NP-hard challenge in combinatorial optimization. Solving VRP in real-time at large scale has become critical in numerous applications, from growing markets like last-mile delivery to emerging use-cases like interactive logistics planning. Such applications involve solving similar problem instances repeatedly, yet current state-of-the-art solvers treat each instance on its own without leveraging previous examples. We introduce a novel optimization framework that uses a reinforcement learning agent - trained on prior instances - to quickly generate initial solutions, which are then further optimized by genetic algorithms. Our framework, Evolutionary Algorithm with Reinforcement Learning Initialization (EARLI), consistently outperforms current state-of-the-art solvers across various time scales. For example, EARLI handles vehicle routing with 500 locations within 1s, 10x faster than current solvers for the same solution quality, enabling applications like real-time and interactive routing. EARLI can generalize to new data, as demonstrated on real e-commerce delivery data of a previously unseen city. Our hybrid framework presents a new way to combine reinforcement learning and genetic algorithms, paving the road for closer interdisciplinary collaboration between AI and optimization communities towards real-time optimization in diverse domains."
  },
  {
    "title": "Robo-taxi Fleet Coordination at Scale via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.06125v1",
    "arxiv_id": "2504.06125v1",
    "authors": [
      "Luigi Tresca",
      "Carolin Schmidt",
      "James Harrison",
      "Filipe Rodrigues",
      "Gioele Zardini",
      "Daniele Gammelli",
      "Marco Pavone"
    ],
    "published": "2025-04-08T15:19:41+00:00",
    "summary": "Fleets of robo-taxis offering on-demand transportation services, commonly known as Autonomous Mobility-on-Demand (AMoD) systems, hold significant promise for societal benefits, such as reducing pollution, energy consumption, and urban congestion. However, orchestrating these systems at scale remains a critical challenge, with existing coordination algorithms often failing to exploit the systems' full potential. This work introduces a novel decision-making framework that unites mathematical modeling with data-driven techniques. In particular, we present the AMoD coordination problem through the lens of reinforcement learning and propose a graph network-based framework that exploits the main strengths of graph representation learning, reinforcement learning, and classical operations research tools. Extensive evaluations across diverse simulation fidelities and scenarios demonstrate the flexibility of our approach, achieving superior system performance, computational efficiency, and generalizability compared to prior methods. Finally, motivated by the need to democratize research efforts in this area, we release publicly available benchmarks, datasets, and simulators for network-level coordination alongside an open-source codebase designed to provide accessible simulation platforms and establish a standardized validation process for comparing methodologies. Code available at: https://github.com/StanfordASL/RL4AMOD"
  },
  {
    "title": "Robo-taxi Fleet Coordination at Scale via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.06125v2",
    "arxiv_id": "2504.06125v2",
    "authors": [
      "Luigi Tresca",
      "Carolin Schmidt",
      "James Harrison",
      "Filipe Rodrigues",
      "Gioele Zardini",
      "Daniele Gammelli",
      "Marco Pavone"
    ],
    "published": "2025-04-08T15:19:41+00:00",
    "summary": "Fleets of robo-taxis offering on-demand transportation services, commonly known as Autonomous Mobility-on-Demand (AMoD) systems, hold significant promise for societal benefits, such as reducing pollution, energy consumption, and urban congestion. However, orchestrating these systems at scale remains a critical challenge, with existing coordination algorithms often failing to exploit the systems' full potential. This work introduces a novel decision-making framework that unites mathematical modeling with data-driven techniques. In particular, we present the AMoD coordination problem through the lens of reinforcement learning and propose a graph network-based framework that exploits the main strengths of graph representation learning, reinforcement learning, and classical operations research tools. Extensive evaluations across diverse simulation fidelities and scenarios demonstrate the flexibility of our approach, achieving superior system performance, computational efficiency, and generalizability compared to prior methods. Finally, motivated by the need to democratize research efforts in this area, we release publicly available benchmarks, datasets, and simulators for network-level coordination alongside an open-source codebase designed to provide accessible simulation platforms and establish a standardized validation process for comparing methodologies. Code available at: https://github.com/StanfordASL/RL4AMOD"
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v1",
    "arxiv_id": "2504.06122v1",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
    "url": "http://arxiv.org/abs/2504.06122v2",
    "arxiv_id": "2504.06122v2",
    "authors": [
      "Jingyuan Zhang",
      "Qi Wang",
      "Xingguang Ji",
      "Yahui Liu",
      "Yang Yue",
      "Fuzheng Zhang",
      "Di Zhang",
      "Guorui Zhou",
      "Kun Gai"
    ],
    "published": "2025-04-08T15:15:26+00:00",
    "summary": "Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details."
  },
  {
    "title": "Trust-Region Twisted Policy Improvement",
    "url": "http://arxiv.org/abs/2504.06048v1",
    "arxiv_id": "2504.06048v1",
    "authors": [
      "Joery A. de Vries",
      "Jinke He",
      "Yaniv Oren",
      "Matthijs T. J. Spaan"
    ],
    "published": "2025-04-08T13:47:07+00:00",
    "summary": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL). However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem. Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically for RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation. This leads to our Trust-Region Twisted SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains."
  },
  {
    "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF",
    "url": "http://arxiv.org/abs/2504.06020v1",
    "arxiv_id": "2504.06020v1",
    "authors": [
      "Liyuan Mao",
      "Haoran Xu",
      "Amy Zhang",
      "Weinan Zhang",
      "Chenjia Bai"
    ],
    "published": "2025-04-08T13:26:07+00:00",
    "summary": "A generalizable reward model is crucial in Reinforcement Learning from Human Feedback (RLHF) as it enables correctly evaluating unseen prompt-response pairs. However, existing reward models lack this ability, as they are typically trained by increasing the reward gap between chosen and rejected responses, while overlooking the prompts that the responses are conditioned on. Consequently, when the trained reward model is evaluated on prompt-response pairs that lie outside the data distribution, neglecting the effect of prompts may result in poor generalization of the reward model. To address this issue, we decompose the reward value into two independent components: prompt-free reward and prompt-related reward. Prompt-free reward represents the evaluation that is determined only by responses, while the prompt-related reward reflects the reward that derives from both the prompt and the response. We extract these two components from an information-theoretic perspective, which requires no extra models. Subsequently, we propose a new reward learning algorithm by prioritizing data samples based on their prompt-free reward values. Through toy examples, we demonstrate that the extracted prompt-free and prompt-related rewards effectively characterize two parts of the reward model. Further, standard evaluations show that our method improves both the alignment performance and the generalization capability of the reward model."
  },
  {
    "title": "A Geometric-Aware Perspective and Beyond: Hybrid Quantum-Classical Machine Learning Methods",
    "url": "http://arxiv.org/abs/2504.06328v1",
    "arxiv_id": "2504.06328v1",
    "authors": [
      "Azadeh Alavia",
      "Hossein Akhoundib",
      "Fatemeh Kouchmeshkib",
      "Mojtaba Mahmoodianc",
      "Sanduni Jayasinghec",
      "Yongli Rena",
      "Abdolrahman Alavi"
    ],
    "published": "2025-04-08T13:24:55+00:00",
    "summary": "Geometric Machine Learning (GML) has shown that respecting non-Euclidean geometry in data spaces can significantly improve performance over naive Euclidean assumptions. In parallel, Quantum Machine Learning (QML) has emerged as a promising paradigm that leverages superposition, entanglement, and interference within quantum state manifolds for learning tasks. This paper offers a unifying perspective by casting QML as a specialized yet more expressive branch of GML. We argue that quantum states, whether pure or mixed, reside on curved manifolds (e.g., projective Hilbert spaces or density-operator manifolds), mirroring how covariance matrices inhabit the manifold of symmetric positive definite (SPD) matrices or how image sets occupy Grassmann manifolds. However, QML also benefits from purely quantum properties, such as entanglement-induced curvature, that can yield richer kernel structures and more nuanced data embeddings.   We illustrate these ideas with published and newly discussed results, including hybrid classical -quantum pipelines for diabetic foot ulcer classification and structural health monitoring. Despite near-term hardware limitations that constrain purely quantum solutions, hybrid architectures already demonstrate tangible benefits by combining classical manifold-based feature extraction with quantum embeddings. We present a detailed mathematical treatment of the geometrical underpinnings of quantum states, emphasizing parallels to classical Riemannian geometry and manifold-based optimization. Finally, we outline open research challenges and future directions, including Quantum Large Language Models (LLMs), quantum reinforcement learning, and emerging hardware approaches, demonstrating how synergizing GML and QML principles can unlock the next generation of machine intelligence."
  },
  {
    "title": "Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models",
    "url": "http://arxiv.org/abs/2504.05978v1",
    "arxiv_id": "2504.05978v1",
    "authors": [
      "J. S. van Hulst",
      "W. P. M. H. Heemels",
      "D. J. Antunes"
    ],
    "published": "2025-04-08T12:33:38+00:00",
    "summary": "Reinforcement learning (RL) is a powerful tool for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We propose using prior model knowledge to guide the exploration process to speed up this learning process. This model knowledge comes in the form of a model set to which the true transition kernel and reward function belong. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also show that we obtain finite-time convergence to the optimal policy under additional assumptions. We demonstrate the effectiveness of the proposed exploration strategy in a simulation study. The results indicate that the proposed method can significantly speed up the learning process in reinforcement learning."
  },
  {
    "title": "AEGIS: Human Attention-based Explainable Guidance for Intelligent Vehicle Systems",
    "url": "http://arxiv.org/abs/2504.05950v1",
    "arxiv_id": "2504.05950v1",
    "authors": [
      "Zhuoli Zhuang",
      "Cheng-You Lu",
      "Yu-Cheng Fred Chang",
      "Yu-Kai Wang",
      "Thomas Do",
      "Chin-Teng Lin"
    ],
    "published": "2025-04-08T12:04:52+00:00",
    "summary": "Improving decision-making capabilities in Autonomous Intelligent Vehicles (AIVs) has been a heated topic in recent years. Despite advancements, training machines to capture regions of interest for comprehensive scene understanding, like human perception and reasoning, remains a significant challenge. This study introduces a novel framework, Human Attention-based Explainable Guidance for Intelligent Vehicle Systems (AEGIS). AEGIS utilizes human attention, converted from eye-tracking, to guide reinforcement learning (RL) models to identify critical regions of interest for decision-making. AEGIS uses a pre-trained human attention model to guide RL models to identify critical regions of interest for decision-making. By collecting 1.2 million frames from 20 participants across six scenarios, AEGIS pre-trains a model to predict human attention patterns."
  },
  {
    "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
    "url": "http://arxiv.org/abs/2504.05946v1",
    "arxiv_id": "2504.05946v1",
    "authors": [
      "Ruixiang Wu",
      "Jiahao Ai",
      "Tongxin Li"
    ],
    "published": "2025-04-08T11:59:00+00:00",
    "summary": "Model Predictive Control~(MPC) is a powerful control strategy widely utilized in domains like energy management, building control, and autonomous systems. However, its effectiveness in real-world settings is challenged by the need to incorporate context-specific predictions and expert instructions, which traditional MPC often neglects. We propose \\IMPC, a novel framework that addresses this gap by integrating real-time human instructions through a Large Language Model~(LLM) to produce context-aware predictions for MPC. Our method employs a Language-to-Distribution~(L2D) module to translate contextual information into predictive disturbance trajectories, which are then incorporated into the MPC optimization. Unlike existing context-aware and language-based MPC models, \\IMPC enables dynamic human-LLM interaction and fine-tunes the L2D module in a closed loop with theoretical performance guarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics when optimized via advanced fine-tuning methods such as Direct Preference Optimization~(DPO) using a tailored loss function."
  },
  {
    "title": "Deep RL-based Autonomous Navigation of Micro Aerial Vehicles (MAVs) in a complex GPS-denied Indoor Environment",
    "url": "http://arxiv.org/abs/2504.05918v1",
    "arxiv_id": "2504.05918v1",
    "authors": [
      "Amit Kumar Singh",
      "Prasanth Kumar Duba",
      "P. Rajalakshmi"
    ],
    "published": "2025-04-08T11:14:37+00:00",
    "summary": "The Autonomy of Unmanned Aerial Vehicles (UAVs) in indoor environments poses significant challenges due to the lack of reliable GPS signals in enclosed spaces such as warehouses, factories, and indoor facilities. Micro Aerial Vehicles (MAVs) are preferred for navigating in these complex, GPS-denied scenarios because of their agility, low power consumption, and limited computational capabilities. In this paper, we propose a Reinforcement Learning based Deep-Proximal Policy Optimization (D-PPO) algorithm to enhance realtime navigation through improving the computation efficiency. The end-to-end network is trained in 3D realistic meta-environments created using the Unreal Engine. With these trained meta-weights, the MAV system underwent extensive experimental trials in real-world indoor environments. The results indicate that the proposed method reduces computational latency by 91\\% during training period without significant degradation in performance. The algorithm was tested on a DJI Tello drone, yielding similar results."
  },
  {
    "title": "Learning strategies for optimised fitness in a model of cyclic dominance",
    "url": "http://arxiv.org/abs/2504.05886v1",
    "arxiv_id": "2504.05886v1",
    "authors": [
      "Honghao Yu",
      "Robert L. Jack"
    ],
    "published": "2025-04-08T10:22:25+00:00",
    "summary": "A major problem in evolutionary biology is how species learn and adapt under the constraint of environmental conditions and competition of other species. Models of cyclic dominance provide simplified settings in which such questions can be addressed using methods from theoretical physics. We investigate how a privileged (\"smart\") species optimises its population by adopting advantageous strategies in one such model. We use a reinforcement learning algorithm, which successfully identifies optimal strategies based on a survival-of-the-weakest effect, including directional incentives to avoid predators. We also characterise the steady-state behaviour of the system in the presence of the smart species and compare with the symmetric case where all species are equivalent."
  },
  {
    "title": "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
    "url": "http://arxiv.org/abs/2504.05840v1",
    "arxiv_id": "2504.05840v1",
    "authors": [
      "Dolton Fernandes",
      "Pramod Kaushik",
      "Harsh Shukla",
      "Bapi Raju Surampudi"
    ],
    "published": "2025-04-08T09:21:39+00:00",
    "summary": "Traditional Reinforcement Learning (RL) algorithms assume the distribution of the data to be uniform or mostly uniform. However, this is not the case with most real-world applications like autonomous driving or in nature where animals roam. Some experiences are encountered frequently, and most of the remaining experiences occur rarely; the resulting distribution is called Zipfian. Taking inspiration from the theory of complementary learning systems, an architecture for learning from Zipfian distributions is proposed where important long tail trajectories are discovered in an unsupervised manner. The proposal comprises an episodic memory buffer containing a prioritised memory module to ensure important rare trajectories are kept longer to address the Zipfian problem, which needs credit assignment to happen in a sample efficient manner. The experiences are then reinstated from episodic memory and given weighted importance forming the trajectory to be executed. Notably, the proposed architecture is modular, can be incorporated in any RL architecture and yields improved performance in multiple Zipfian tasks over traditional architectures. Our method outperforms IMPALA by a significant margin on all three tasks and all three evaluation metrics (Zipfian, Uniform, and Rare Accuracy) and also gives improvements on most Atari environments that are considered challenging"
  },
  {
    "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization",
    "url": "http://arxiv.org/abs/2504.05812v1",
    "arxiv_id": "2504.05812v1",
    "authors": [
      "Qingyang Zhang",
      "Haitao Wu",
      "Changqing Zhang",
      "Peilin Zhao",
      "Yatao Bian"
    ],
    "published": "2025-04-08T08:48:51+00:00",
    "summary": "While large language models (LLMs) have demonstrated exceptional capabilities in challenging tasks such as mathematical reasoning, existing methods to enhance reasoning ability predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data after pre-training. However, these approaches critically depend on external supervisions--such as human labelled reasoning traces, verified golden answers, or pre-trained reward models--which limits scalability and practical applicability. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. EMPO does not require any supervised information for incentivizing reasoning capabilities (i.e., neither verifiable reasoning traces, problems with golden answers, nor additional pre-trained reward models). By continuously minimizing the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, EMPO enables purely self-supervised evolution of reasoning capabilities with strong flexibility and practicality. Our experiments demonstrate competitive performance of EMPO on both mathematical reasoning and free-form commonsense reasoning tasks. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness accuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA."
  },
  {
    "title": "PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning",
    "url": "http://arxiv.org/abs/2504.05810v1",
    "arxiv_id": "2504.05810v1",
    "authors": [
      "Xinpeng Ding",
      "Kui Zhang",
      "Jinahua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Xiaomeng Li"
    ],
    "published": "2025-04-08T08:41:41+00:00",
    "summary": "Direct Preference Optimization (DPO) helps reduce hallucinations in Video Multimodal Large Language Models (VLLMs), but its reliance on offline preference data limits adaptability and fails to capture true video-response misalignment. We propose Video Direct Preference Optimization (VDPO), an online preference learning framework that eliminates the need for preference annotation by leveraging video augmentations to generate rejected samples while keeping responses fixed. However, selecting effective augmentations is non-trivial, as some clips may be semantically identical to the original under specific prompts, leading to false rejections and disrupting alignment. To address this, we introduce Prompt-aware Multi-instance Learning VDPO (PaMi-VDPO), which selects augmentations based on prompt context. Instead of a single rejection, we construct a candidate set of augmented clips and apply a close-to-far selection strategy, initially ensuring all clips are semantically relevant while then prioritizing the most prompt-aware distinct clip. This allows the model to better capture meaningful visual differences, mitigating hallucinations, while avoiding false rejections, and improving alignment. PaMi-VDPOseamlessly integrates into existing VLLMs without additional parameters, GPT-4/human supervision. With only 10k SFT data, it improves the base model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining stable performance on general video benchmarks."
  },
  {
    "title": "xMTF: A Formula-Free Model for Reinforcement-Learning-Based Multi-Task Fusion in Recommender Systems",
    "url": "http://arxiv.org/abs/2504.05669v1",
    "arxiv_id": "2504.05669v1",
    "authors": [
      "Yang Cao",
      "Changhao Zhang",
      "Xiaoshuang Chen",
      "Kaiqiao Zhan",
      "Ben Wang"
    ],
    "published": "2025-04-08T04:28:22+00:00",
    "summary": "Recommender systems need to optimize various types of user feedback, e.g., clicks, likes, and shares. A typical recommender system handling multiple types of feedback has two components: a multi-task learning (MTL) module, predicting feedback such as click-through rate and like rate; and a multi-task fusion (MTF) module, integrating these predictions into a single score for item ranking. MTF is essential for ensuring user satisfaction, as it directly influences recommendation outcomes. Recently, reinforcement learning (RL) has been applied to MTF tasks to improve long-term user satisfaction. However, existing RL-based MTF methods are formula-based methods, which only adjust limited coefficients within pre-defined formulas. The pre-defined formulas restrict the RL search space and become a bottleneck for MTF. To overcome this, we propose a formula-free MTF framework. We demonstrate that any suitable fusion function can be expressed as a composition of single-variable monotonic functions, as per the Sprecher Representation Theorem. Leveraging this, we introduce a novel learnable monotonic fusion cell (MFC) to replace pre-defined formulas. We call this new MFC-based model eXtreme MTF (xMTF). Furthermore, we employ a two-stage hybrid (TSH) learning strategy to train xMTF effectively. By expanding the MTF search space, xMTF outperforms existing methods in extensive offline and online experiments."
  },
  {
    "title": "To Start Up a Start-Up$-$Embedding Strategic Demand Development in Operational On-Demand Fulfillment via Reinforcement Learning with Information Shaping",
    "url": "http://arxiv.org/abs/2504.05633v1",
    "arxiv_id": "2504.05633v1",
    "authors": [
      "Xinwei Chen",
      "Marlin W. Ulmer",
      "Barrett W. Thomas"
    ],
    "published": "2025-04-08T03:25:37+00:00",
    "summary": "The last few years have witnessed rapid growth in the on-demand delivery market, with many start-ups entering the field. However, not all of these start-ups have succeeded due to various reasons, among others, not being able to establish a large enough customer base. In this paper, we address this problem that many on-demand transportation start-ups face: how to establish themselves in a new market. When starting, such companies often have limited fleet resources to serve demand across a city. Depending on the use of the fleet, varying service quality is observed in different areas of the city, and in turn, the service quality impacts the respective growth of demand in each area. Thus, operational fulfillment decisions drive the longer-term demand development. To integrate strategic demand development into real-time fulfillment operations, we propose a two-step approach. First, we derive analytical insights into optimal allocation decisions for a stylized problem. Second, we use these insights to shape the training data of a reinforcement learning strategy for operational real-time fulfillment. Our experiments demonstrate that combining operational efficiency with long-term strategic planning is highly advantageous. Further, we show that the careful shaping of training data is essential for the successful development of demand."
  },
  {
    "title": "PTRL: Prior Transfer Deep Reinforcement Learning for Legged Robots Locomotion",
    "url": "http://arxiv.org/abs/2504.05629v1",
    "arxiv_id": "2504.05629v1",
    "authors": [
      "Haodong Huang",
      "Shilong Sun",
      "Zida Zhao",
      "Hailin Huang",
      "Changqing Shen",
      "Wenfu Xu"
    ],
    "published": "2025-04-08T03:11:43+00:00",
    "summary": "In the field of legged robot motion control, reinforcement learning (RL) holds great promise but faces two major challenges: high computational cost for training individual robots and poor generalization of trained models. To address these problems, this paper proposes a novel framework called Prior Transfer Reinforcement Learning (PTRL), which improves both training efficiency and model transferability across different robots. Drawing inspiration from model transfer techniques in deep learning, PTRL introduces a fine-tuning mechanism that selectively freezes layers of the policy network during transfer, making it the first to apply such a method in RL. The framework consists of three stages: pre-training on a source robot using the Proximal Policy Optimization (PPO) algorithm, transferring the learned policy to a target robot, and fine-tuning with partial network freezing. Extensive experiments on various robot platforms confirm that this approach significantly reduces training time while maintaining or even improving performance. Moreover, the study quantitatively analyzes how the ratio of frozen layers affects transfer results, providing valuable insights into optimizing the process. The experimental outcomes show that PTRL achieves better walking control performance and demonstrates strong generalization and adaptability, offering a promising solution for efficient and scalable RL-based control of legged robots."
  },
  {
    "title": "Stratified Expert Cloning with Adaptive Selection for User Retention in Large-Scale Recommender Systems",
    "url": "http://arxiv.org/abs/2504.05628v1",
    "arxiv_id": "2504.05628v1",
    "authors": [
      "Chengzhi Lin",
      "Annan Xie",
      "Shuchang Liu",
      "Wuhong Wang",
      "Chuyuan Wang",
      "Yongqi Liu"
    ],
    "published": "2025-04-08T03:10:42+00:00",
    "summary": "User retention has emerged as a critical challenge in large-scale recommender systems, significantly impacting the long-term success of online platforms. Existing methods often focus on short-term engagement metrics, failing to capture the complex dynamics of user preferences and behaviors over extended periods. While reinforcement learning (RL) approaches have shown promise in optimizing long-term rewards, they face difficulties in credit assignment, sample efficiency, and exploration when applied to the user retention problem. In this work, we propose Stratified Expert Cloning (SEC), a novel imitation learning framework that effectively leverages abundant logged data from high-retention users to learn robust recommendation policies. SEC introduces three key innovations: 1) a multi-level expert stratification strategy that captures the nuances in expert user behaviors at different retention levels; 2) an adaptive expert selection mechanism that dynamically assigns users to the most suitable policy based on their current state and historical retention level; and 3) an action entropy regularization technique that promotes recommendation diversity and mitigates the risk of policy collapse. Through extensive offline experiments and online A/B tests on two major video platforms, Kuaishou and Kuaishou Lite, with hundreds of millions of daily active users, we demonstrate SEC's significant improvements over state-of-the-art methods in user retention. The results demonstrate significant improvements in user retention, with cumulative lifts of 0.098\\% and 0.122\\% in active days on Kuaishou and Kuaishou Lite respectively, additionally bringing tens of thousands of daily active users to each platform."
  },
  {
    "title": "ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs",
    "url": "http://arxiv.org/abs/2504.05605v1",
    "arxiv_id": "2504.05605v1",
    "authors": [
      "Gejian Zhao",
      "Hanzhou Wu",
      "Xinpeng Zhang",
      "Athanasios V. Vasilakos"
    ],
    "published": "2025-04-08T01:36:16+00:00",
    "summary": "Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning tasks, but it also introduces new security issues. In this work, we present ShadowCoT, a novel backdoor attack framework that targets the internal reasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks, ShadowCoT directly manipulates the model's cognitive reasoning path, enabling it to hijack multi-step reasoning chains and produce logically coherent but adversarial outcomes. By conditioning on internal reasoning states, ShadowCoT learns to recognize and selectively disrupt key reasoning steps, effectively mounting a self-reflective cognitive attack within the target model. Our approach introduces a lightweight yet effective multi-stage injection pipeline, which selectively rewires attention pathways and perturbs intermediate representations with minimal parameter overhead (only 0.15% updated). ShadowCoT further leverages reinforcement learning and reasoning chain pollution (RCP) to autonomously synthesize stealthy adversarial CoTs that remain undetectable to advanced defenses. Extensive experiments across diverse reasoning benchmarks and LLMs show that ShadowCoT consistently achieves high Attack Success Rate (94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance. These results reveal an emergent class of cognition-level threats and highlight the urgent need for defenses beyond shallow surface-level consistency."
  },
  {
    "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought",
    "url": "http://arxiv.org/abs/2504.05599v1",
    "arxiv_id": "2504.05599v1",
    "authors": [
      "Yi Peng",
      "Chris",
      "Xiaokun Wang",
      "Yichen Wei",
      "Jiangbo Pei",
      "Weijie Qiu",
      "Ai Jian",
      "Yunzhuo Hao",
      "Jiachun Pan",
      "Tianyidan Xie",
      "Li Ge",
      "Rongxian Zhuang",
      "Xuchen Song",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "published": "2025-04-08T01:19:20+00:00",
    "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility."
  },
  {
    "title": "Multi-fidelity Reinforcement Learning Control for Complex Dynamical Systems",
    "url": "http://arxiv.org/abs/2504.05588v1",
    "arxiv_id": "2504.05588v1",
    "authors": [
      "Luning Sun",
      "Xin-Yang Liu",
      "Siyan Zhao",
      "Aditya Grover",
      "Jian-Xun Wang",
      "Jayaraman J. Thiagarajan"
    ],
    "published": "2025-04-08T00:50:15+00:00",
    "summary": "Controlling instabilities in complex dynamical systems is challenging in scientific and engineering applications. Deep reinforcement learning (DRL) has seen promising results for applications in different scientific applications. The many-query nature of control tasks requires multiple interactions with real environments of the underlying physics. However, it is usually sparse to collect from the experiments or expensive to simulate for complex dynamics. Alternatively, controlling surrogate modeling could mitigate the computational cost issue. However, a fast and accurate learning-based model by offline training makes it very hard to get accurate pointwise dynamics when the dynamics are chaotic. To bridge this gap, the current work proposes a multi-fidelity reinforcement learning (MFRL) framework that leverages differentiable hybrid models for control tasks, where a physics-based hybrid model is corrected by limited high-fidelity data. We also proposed a spectrum-based reward function for RL learning. The effect of the proposed framework is demonstrated on two complex dynamics in physics. The statistics of the MFRL control result match that computed from many-query evaluations of the high-fidelity environments and outperform other SOTA baselines."
  },
  {
    "title": "TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05585v1",
    "arxiv_id": "2504.05585v1",
    "authors": [
      "Yuxuan Li",
      "Ning Yang",
      "Stephen Xia"
    ],
    "published": "2025-04-08T00:48:29+00:00",
    "summary": "Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden \"trap states\" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness."
  },
  {
    "title": "Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control",
    "url": "http://arxiv.org/abs/2504.05553v1",
    "arxiv_id": "2504.05553v1",
    "authors": [
      "Yongjie Fu",
      "Lingyun Zhong",
      "Zifan Li",
      "Xuan Di"
    ],
    "published": "2025-04-07T23:02:59+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems."
  },
  {
    "title": "Deep Reinforcement Learning Algorithms for Option Hedging",
    "url": "http://arxiv.org/abs/2504.05521v1",
    "arxiv_id": "2504.05521v1",
    "authors": [
      "Andrei Neagu",
      "Fr\u00e9d\u00e9ric Godin",
      "Leila Kosseim"
    ],
    "published": "2025-04-07T21:32:14+00:00",
    "summary": "Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment."
  },
  {
    "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.05520v1",
    "arxiv_id": "2504.05520v1",
    "authors": [
      "Taiwei Shi",
      "Yiyang Wu",
      "Linxin Song",
      "Tianyi Zhou",
      "Jieyu Zhao"
    ],
    "published": "2025-04-07T21:31:31+00:00",
    "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework."
  },
  {
    "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering",
    "url": "http://arxiv.org/abs/2504.05463v1",
    "arxiv_id": "2504.05463v1",
    "authors": [
      "Sofian Chaybouti",
      "Walid Bousselham",
      "Moritz Wolter",
      "Hilde Kuehne"
    ],
    "published": "2025-04-07T19:54:04+00:00",
    "summary": "Video-Question-Answering (VideoQA) comprises the capturing of complex visual relation changes over time, remaining a challenge even for advanced Video Language Models (VLM), i.a., because of the need to represent the visual content to a reasonably sized input for those models. To address this problem, we propose   RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed to capture visual relation information by encoding them into structured, decomposed representations. Specifically, inspired by spatiotemporal scene graphs, we propose to encode video sequences as sets of relation triplets in the form of (\\textit{subject-predicate-object}) over time via their language embeddings. To this end, we extract explicit relations from video captions and introduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a Q-Former architecture to align an unordered set of video-derived queries with corresponding text-based relation descriptions. At inference, the resulting Q-former produces an efficient token representation that can serve as input to a VLM for VideoQA.   We evaluate the proposed framework on five challenging benchmarks: NeXT-QA, Intent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video representation is able to outperform global alignment-based CLS or patch token representations and achieves competitive results against state-of-the-art models, particularly on tasks requiring temporal reasoning and relation comprehension. The code and models will be publicly released."
  },
  {
    "title": "TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2504.05407v1",
    "arxiv_id": "2504.05407v1",
    "authors": [
      "Yazan Youssef",
      "Paulo Ricardo Marques de Araujo",
      "Aboelmagd Noureldin",
      "Sidney Givigi"
    ],
    "published": "2025-04-07T18:23:13+00:00",
    "summary": "Efficient scheduling remains a critical challenge in various domains, requiring solutions to complex NP-hard optimization problems to achieve optimal resource allocation and maximize productivity. In this paper, we introduce a framework called Transformer-Based Task Scheduling System (TRATSS), designed to address the intricacies of single agent scheduling in graph-based environments. By integrating the latest advancements in reinforcement learning and transformer architecture, TRATSS provides a novel system that outputs optimized task scheduling decisions while dynamically adapting to evolving task requirements and resource availability. Leveraging the self-attention mechanism in transformers, TRATSS effectively captures complex task dependencies, thereby providing solutions with enhanced resource utilization and task completion efficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's effectiveness in providing high-quality solutions to scheduling problems that involve multiple action profiles."
  },
  {
    "title": "The Role of Environment Access in Agnostic Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05405v1",
    "arxiv_id": "2504.05405v1",
    "authors": [
      "Akshay Krishnamurthy",
      "Gene Li",
      "Ayush Sekhari"
    ],
    "published": "2025-04-07T18:19:56+00:00",
    "summary": "We study Reinforcement Learning (RL) in environments with large state spaces, where function approximation is required for sample-efficient learning. Departing from a long history of prior work, we consider the weakest possible form of function approximation, called agnostic policy learning, where the learner seeks to find the best policy in a given class $\\Pi$, with no guarantee that $\\Pi$ contains an optimal policy for the underlying task. Although it is known that sample-efficient agnostic policy learning is not possible in the standard online RL setting without further assumptions, we investigate the extent to which this can be overcome with stronger forms of access to the environment. Specifically, we show that: 1. Agnostic policy learning remains statistically intractable when given access to a local simulator, from which one can reset to any previously seen state. This result holds even when the policy class is realizable, and stands in contrast to a positive result of [MFR24] showing that value-based learning under realizability is tractable with local simulator access. 2. Agnostic policy learning remains statistically intractable when given online access to a reset distribution with good coverage properties over the state space (the so-called $\\mu$-reset setting). We also study stronger forms of function approximation for policy learning, showing that PSDP [BKSN03] and CPI [KL02] provably fail in the absence of policy completeness. 3. On a positive note, agnostic policy learning is statistically tractable for Block MDPs with access to both of the above reset models. We establish this via a new algorithm that carefully constructs a policy emulator: a tabular MDP with a small state space that approximates the value functions of all policies $\\pi \\in \\Pi$. These values are approximated without any explicit value function class."
  },
  {
    "title": "Physics-informed Modularized Neural Network for Advanced Building Control by Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05397v1",
    "arxiv_id": "2504.05397v1",
    "authors": [
      "Zixin Jiang",
      "Xuezheng Wang",
      "Bing Dong"
    ],
    "published": "2025-04-07T18:07:54+00:00",
    "summary": "Physics-informed machine learning (PIML) provides a promising solution for building energy modeling and can serve as a virtual environment to enable reinforcement learning (RL) agents to interact and learn. However, challenges remain in efficiently integrating physics priors, evaluating the effectiveness of physics constraints, balancing model accuracy and physics consistency, and enabling real-world implementation. To address these gaps, this study introduces a Physics-Informed Modularized Neural Network (PI-ModNN), which incorporates physics priors through a physics-informed model structure, loss functions, and hard constraints. A new evaluation metric called \"temperature response violation\" is developed to quantify the physical consistency of data-driven building dynamic models under varying control inputs and training data sizes. Additionally, a physics prior evaluation framework based on rule importance is proposed to assess the contribution of each individual physics prior, offering guidance on selecting appropriate PIML techniques. Results indicate that incorporating physical priors does not always improve model performance; inappropriate priors may decrease model accuracy and consistency. However, hard constraints are effective in enforcing model consistency. Furthermore, we present a general workflow for developing control-oriented PIML models and integrating them with deep reinforcement learning (DRL). Following this framework, a case study implementing DRL in an office space over three months demonstrates potential energy savings of 31.4%. Finally, we provide a general guideline for integrating data-driven models with advanced building control through a four-step evaluation framework, paving the way for reliable and scalable deployment of advanced building controls."
  },
  {
    "title": "Interactive Explanations for Reinforcement-Learning Agents",
    "url": "http://arxiv.org/abs/2504.05393v1",
    "arxiv_id": "2504.05393v1",
    "authors": [
      "Yotam Amitai",
      "Ofra Amir",
      "Guy Avni"
    ],
    "published": "2025-04-07T18:00:50+00:00",
    "summary": "As reinforcement learning methods increasingly amass accomplishments, the need for comprehending their solutions becomes more crucial. Most explainable reinforcement learning (XRL) methods generate a static explanation depicting their developers' intuition of what should be explained and how. In contrast, literature from the social sciences proposes that meaningful explanations are structured as a dialog between the explainer and the explainee, suggesting a more active role for the user and her communication with the agent. In this paper, we present ASQ-IT -- an interactive explanation system that presents video clips of the agent acting in its environment based on queries given by the user that describe temporal properties of behaviors of interest. Our approach is based on formal methods: queries in ASQ-IT's user interface map to a fragment of Linear Temporal Logic over finite traces (LTLf), which we developed, and our algorithm for query processing is based on automata theory. User studies show that end-users can understand and formulate queries in ASQ-IT and that using ASQ-IT assists users in identifying faulty agent behaviors."
  },
  {
    "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception",
    "url": "http://arxiv.org/abs/2504.05287v1",
    "arxiv_id": "2504.05287v1",
    "authors": [
      "Hui Zhang",
      "Zijian Wu",
      "Linyi Huang",
      "Sammy Christen",
      "Jie Song"
    ],
    "published": "2025-04-07T17:38:19+00:00",
    "summary": "Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/"
  },
  {
    "title": "Concise Reasoning via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05185v1",
    "arxiv_id": "2504.05185v1",
    "authors": [
      "Mehdi Fatemi",
      "Banafsheh Rafiee",
      "Mingjie Tang",
      "Kartik Talamadupula"
    ],
    "published": "2025-04-07T15:35:54+00:00",
    "summary": "Despite significant advancements in large language models (LLMs), a major drawback of reasoning models is their enormous token usage, which increases computational cost, resource requirements, and response time. In this work, we revisit the core principles of reinforcement learning (RL) and, through mathematical analysis, demonstrate that the tendency to generate lengthy responses arises inherently from RL-based optimization during training. This finding questions the prevailing assumption that longer responses inherently improve reasoning accuracy. Instead, we uncover a natural correlation between conciseness and accuracy that has been largely overlooked. Moreover, we show that introducing a secondary phase of RL post-training, using a small set of problems and limited resources, can significantly reduce a model's chain of thought while maintaining or even enhancing accuracy. Finally, we validate our conclusions through extensive experimental results."
  },
  {
    "title": "Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
    "url": "http://arxiv.org/abs/2504.05181v1",
    "arxiv_id": "2504.05181v1",
    "authors": [
      "Kidist Amde Mekonnen",
      "Yubao Tang",
      "Maarten de Rijke"
    ],
    "published": "2025-04-07T15:27:37+00:00",
    "summary": "Generative information retrieval (GenIR) is a promising neural retrieval paradigm that formulates document retrieval as a document identifier (docid) generation task, allowing for end-to-end optimization toward a unified global retrieval objective. However, existing GenIR models suffer from token-level misalignment, where models trained to predict the next token often fail to capture document-level relevance effectively. While reinforcement learning-based methods, such as reinforcement learning from relevance feedback (RLRF), aim to address this misalignment through reward modeling, they introduce significant complexity, requiring the optimization of an auxiliary reward function followed by reinforcement fine-tuning, which is computationally expensive and often unstable. To address these challenges, we propose direct document relevance optimization (DDRO), which aligns token-level docid generation with document-level relevance estimation through direct optimization via pairwise ranking, eliminating the need for explicit reward modeling and reinforcement learning. Experimental results on benchmark datasets, including MS MARCO document and Natural Questions, show that DDRO outperforms reinforcement learning-based methods, achieving a 7.4% improvement in MRR@10 for MS MARCO and a 19.9% improvement for Natural Questions. These findings highlight DDRO's potential to enhance retrieval effectiveness with a simplified optimization approach. By framing alignment as a direct optimization problem, DDRO simplifies the ranking optimization pipeline of GenIR models while offering a viable alternative to reinforcement learning-based methods."
  },
  {
    "title": "RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
    "url": "http://arxiv.org/abs/2504.05167v1",
    "arxiv_id": "2504.05167v1",
    "authors": [
      "Mingcan Wang",
      "Junchang Xin",
      "Luxuan Qu",
      "Qi Chen",
      "Zhiqiong Wang"
    ],
    "published": "2025-04-07T15:11:51+00:00",
    "summary": "The score-based structure learning of Bayesian network (BN) is an effective way to learn BN models, which are regarded as some of the most compelling probabilistic graphical models in the field of representation and reasoning under uncertainty. However, the search space of structure learning grows super-exponentially as the number of variables increases, which makes BN structure learning an NP-hard problem, as well as a combination optimization problem (COP). Despite the successes of many heuristic methods on it, the results of the structure learning of BN are usually unsatisfactory. Inspired by Q-learning, in this paper, a Bayesian network structure learning algorithm via reinforcement learning-based (RL-based) search strategy is proposed, namely RLBayes. The method borrows the idea of RL and tends to record and guide the learning process by a dynamically maintained Q-table. By creating and maintaining the dynamic Q-table, RLBayes achieve storing the unlimited search space within limited space, thereby achieving the structure learning of BN via Q-learning. Not only is it theoretically proved that RLBayes can converge to the global optimal BN structure, but also it is experimentally proved that RLBayes has a better effect than almost all other heuristic search algorithms."
  },
  {
    "title": "A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
    "url": "http://arxiv.org/abs/2504.05150v1",
    "arxiv_id": "2504.05150v1",
    "authors": [
      "Leonardo Kanashiro Felizardo",
      "Edoardo Fadda",
      "Paolo Brandimarte",
      "Emilio Del-Moral-Hernandez",
      "Mari\u00e1 Cristina Vasconcelos Nascimento"
    ],
    "published": "2025-04-07T14:56:43+00:00",
    "summary": "This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a novel variation of the leading deep reinforcement learning method, Proximal Policy Optimization (PPO). The PDPPO state transition process is divided into two steps: a deterministic step resulting in the post-decision state and a stochastic step leading to the next state. Our approach incorporates post-decision states and dual critics to reduce the problem's dimensionality and enhance the accuracy of value function estimation. Lot-sizing is a mixed integer programming problem for which we exemplify such dynamics. The objective of lot-sizing is to optimize production, delivery fulfillment, and inventory levels in uncertain demand and cost parameters. This paper evaluates the performance of PDPPO across various environments and configurations. Notably, PDPPO with a dual critic architecture achieves nearly double the maximum reward of vanilla PPO in specific scenarios, requiring fewer episode iterations and demonstrating faster and more consistent learning across different initializations. On average, PDPPO outperforms PPO in environments with a stochastic component in the state transition. These results support the benefits of using a post-decision state. Integrating this post-decision state in the value function approximation leads to more informed and efficient learning in high-dimensional and stochastic environments."
  },
  {
    "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks",
    "url": "http://arxiv.org/abs/2504.05118v1",
    "arxiv_id": "2504.05118v1",
    "authors": [
      "YuYue",
      "Yufeng Yuan",
      "Qiying Yu",
      "Xiaochen Zuo",
      "Ruofei Zhu",
      "Wenyuan Xu",
      "Jiaze Chen",
      "Chengyi Wang",
      "TianTian Fan",
      "Zhengyin Du",
      "Xiangpeng Wei",
      "Gaohong Liu",
      "Juncai Liu",
      "Lingjun Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Ru Zhang",
      "Xin Liu",
      "Mingxuan Wang",
      "Yonghui Wu",
      "Lin Yan"
    ],
    "published": "2025-04-07T14:21:11+00:00",
    "summary": "We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks."
  },
  {
    "title": "VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks",
    "url": "http://arxiv.org/abs/2504.05118v2",
    "arxiv_id": "2504.05118v2",
    "authors": [
      "Yu Yue",
      "Yufeng Yuan",
      "Qiying Yu",
      "Xiaochen Zuo",
      "Ruofei Zhu",
      "Wenyuan Xu",
      "Jiaze Chen",
      "Chengyi Wang",
      "TianTian Fan",
      "Zhengyin Du",
      "Xiangpeng Wei",
      "Xiangyu Yu",
      "Gaohong Liu",
      "Juncai Liu",
      "Lingjun Liu",
      "Haibin Lin",
      "Zhiqi Lin",
      "Bole Ma",
      "Chi Zhang",
      "Mofan Zhang",
      "Wang Zhang",
      "Hang Zhu",
      "Ru Zhang",
      "Xin Liu",
      "Mingxuan Wang",
      "Yonghui Wu",
      "Lin Yan"
    ],
    "published": "2025-04-07T14:21:11+00:00",
    "summary": "We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of $\\mathbf{60.4}$. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks."
  },
  {
    "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05108v1",
    "arxiv_id": "2504.05108v1",
    "authors": [
      "Anja Surina",
      "Amin Mansouri",
      "Lars Quaedvlieg",
      "Amal Seddas",
      "Maryna Viazovska",
      "Emmanuel Abbe",
      "Caglar Gulcehre"
    ],
    "published": "2025-04-07T14:14:15+00:00",
    "summary": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design."
  },
  {
    "title": "Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation",
    "url": "http://arxiv.org/abs/2504.05045v1",
    "arxiv_id": "2504.05045v1",
    "authors": [
      "Huilin Yin",
      "Zhikun Yang",
      "Daniel Watzenig"
    ],
    "published": "2025-04-07T13:14:45+00:00",
    "summary": "Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency."
  },
  {
    "title": "Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation",
    "url": "http://arxiv.org/abs/2504.05045v2",
    "arxiv_id": "2504.05045v2",
    "authors": [
      "Huilin Yin",
      "Zhikun Yang",
      "Daniel Watzenig"
    ],
    "published": "2025-04-07T13:14:45+00:00",
    "summary": "Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency."
  },
  {
    "title": "Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.05018v1",
    "arxiv_id": "2504.05018v1",
    "authors": [
      "Bibek Poudel",
      "Xuan Wang",
      "Weizi Li",
      "Lei Zhu",
      "Kevin Heaslip"
    ],
    "published": "2025-04-07T12:41:58+00:00",
    "summary": "Reinforcement learning (RL) holds significant promise for adaptive traffic signal control. While existing RL-based methods demonstrate effectiveness in reducing vehicular congestion, their predominant focus on vehicle-centric optimization leaves pedestrian mobility needs and safety challenges unaddressed. In this paper, we present a deep RL framework for adaptive control of eight traffic signals along a real-world urban corridor, jointly optimizing both pedestrian and vehicular efficiency. Our single-agent policy is trained using real-world pedestrian and vehicle demand data derived from Wi-Fi logs and video analysis. The results demonstrate significant performance improvements over traditional fixed-time signals, reducing average wait times per pedestrian and per vehicle by up to 67% and 52%, respectively, while simultaneously decreasing total accumulated wait times for both groups by up to 67% and 53%. Additionally, our results demonstrate generalization capabilities across varying traffic demands, including conditions entirely unseen during training, validating RL's potential for developing transportation systems that serve all road users."
  },
  {
    "title": "Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds",
    "url": "http://arxiv.org/abs/2504.04973v1",
    "arxiv_id": "2504.04973v1",
    "authors": [
      "Qian Zuo",
      "Fengxiang He"
    ],
    "published": "2025-04-07T11:58:19+00:00",
    "summary": "This paper studies constrained Markov decision processes (CMDPs) with constraints against stochastic thresholds, aiming at safety of reinforcement learning in unknown and uncertain environments. We leverage a Growing-Window estimator sampling from interactions with the uncertain and dynamic environment to estimate the thresholds, based on which we design Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual algorithm for multiple constraints against stochastic thresholds. SPOT enables reinforcement learning under both pessimistic and optimistic threshold settings. We prove that our algorithm achieves sublinear regret and constraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$ episodes. The theoretical guarantees show that our algorithm achieves performance comparable to that of an approach relying on fixed and clear thresholds. To the best of our knowledge, SPOT is the first reinforcement learning algorithm that realises theoretical guaranteed performance in an uncertain environment where even thresholds are unknown."
  },
  {
    "title": "A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization",
    "url": "http://arxiv.org/abs/2504.04950v1",
    "arxiv_id": "2504.04950v1",
    "authors": [
      "Wenyuan Xu",
      "Xiaochen Zuo",
      "Chao Xin",
      "Yu Yue",
      "Lin Yan",
      "Yonghui Wu"
    ],
    "published": "2025-04-07T11:34:48+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior."
  },
  {
    "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
    "url": "http://arxiv.org/abs/2504.04918v1",
    "arxiv_id": "2504.04918v1",
    "authors": [
      "Xue Zhang"
    ],
    "published": "2025-04-07T11:01:25+00:00",
    "summary": "As language models continue to grow larger, the cost of acquiring high-quality training data has increased significantly. Collecting human feedback is both expensive and time-consuming, and manual labels can be noisy, leading to an imbalance between helpfulness and harmfulness. Constitutional AI, introduced by Anthropic in December 2022, uses AI to provide feedback to another AI, greatly reducing the need for human labeling. However, the original implementation was designed for a model with around 52 billion parameters, and there is limited information on how well Constitutional AI performs with smaller models, such as LLaMA 3-8B. In this paper, we replicated the Constitutional AI workflow using the smaller LLaMA 3-8B model. Our results show that Constitutional AI can effectively increase the harmlessness of the model, reducing the Attack Success Rate in MT-Bench by 40.8%. However, similar to the original study, increasing harmlessness comes at the cost of helpfulness. The helpfulness metrics, which are an average of the Turn 1 and Turn 2 scores, dropped by 9.8% compared to the baseline. Additionally, we observed clear signs of model collapse in the final DPO-CAI model, indicating that smaller models may struggle with self-improvement due to insufficient output quality, making effective fine-tuning more challenging. Our study suggests that, like reasoning and math ability, self-improvement is an emergent property."
  },
  {
    "title": "Age-of-information minimization under energy harvesting and non-stationary environment",
    "url": "http://arxiv.org/abs/2504.04916v1",
    "arxiv_id": "2504.04916v1",
    "authors": [
      "Akanksha Jaiswal",
      "Arpan Chattopadhyay"
    ],
    "published": "2025-04-07T10:53:53+00:00",
    "summary": "This work focuses on minimizing the age of information for multiple energy harvesting sources that sample data and transmit it to a sink node. At each time, the central scheduler selects one of the sources to probe the quality of its channel to the sink node, and then the assessed channel quality is utilized to determine whether a source will sample and send the packet. For a single source case, we assume that the probed channel quality is known at each time instant, model the problem of AoI minimization as a Markov decision process, and prove the optimal sampling policy threshold structure. We then use this threshold structure and propose an AEC-SW-UCRL2 algorithm to handle unknown and time varying energy harvesting rate and channel statistics, motivated by the popular SWUCRL2 algorithm for non stationary reinforcement learning. This algorithm is applicable when an upper bound is available for the total variation of each of these quantities over a time horizon. Furthermore, in situations where these variation budgets are not accessible, we introduce the AEC-BORL algorithm, motivated by the well known BORL algorithm. For the multiple source case, we demonstrate that the AoI minimization problem can be formulated as a constrained MDP, which can be relaxed using a Lagrange multiplier and decoupled into sub problems across source nodes. We also derive Whittle index based source scheduling policy for probing and an optimal threshold policy for source sampling. We next leverage this Whittle index and threshold structure to develop the WIT-SW-UCRL2 algorithm for unknown time varying energy harvesting rates and channel statistics under their respective variation budgets. Moreover, we also proposed a Whittle index and threshold based bandit over reinforcement learning (WIT-BORL) algorithm for unknown variation budgets. Finally, we numerically demonstrate the efficacy of our algorithms."
  },
  {
    "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
    "url": "http://arxiv.org/abs/2504.04858v1",
    "arxiv_id": "2504.04858v1",
    "authors": [
      "Roie Kazoom",
      "Raz Lapid",
      "Moshe Sipper",
      "Ofer Hadar"
    ],
    "published": "2025-04-07T09:14:47+00:00",
    "summary": "Adversarial patch attacks pose a major threat to vision systems by embedding localized perturbations that mislead deep models. Traditional defense methods often require retraining or fine-tuning, making them impractical for real-world deployment. We propose a training-free Visual Retrieval-Augmented Generation (VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial patch detection. By retrieving visually similar patches and images that resemble stored attacks in a continuously expanding database, VRAG performs generative reasoning to identify diverse attack types, all without additional training or fine-tuning. We extensively evaluate open-source large-scale VLMs, including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO model achieves up to 95 percent classification accuracy, setting a new state-of-the-art for open-source adversarial patch detection. Gemini-2.0 attains the highest overall accuracy, 98 percent, but remains closed-source. Experimental results demonstrate VRAG's effectiveness in identifying a variety of adversarial patches with minimal human annotation, paving the way for robust, practical defenses against evolving adversarial patch attacks."
  },
  {
    "title": "Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors",
    "url": "http://arxiv.org/abs/2504.04785v1",
    "arxiv_id": "2504.04785v1",
    "authors": [
      "Fan Nie",
      "Lan Feng",
      "Haotian Ye",
      "Weixin Liang",
      "Pan Lu",
      "Huaxiu Yao",
      "Alexandre Alahi",
      "James Zou"
    ],
    "published": "2025-04-07T07:27:31+00:00",
    "summary": "Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models."
  },
  {
    "title": "Playing Non-Embedded Card-Based Games with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.04783v1",
    "arxiv_id": "2504.04783v1",
    "authors": [
      "Tianyang Wu",
      "Lipeng Wan",
      "Yuhang Wang",
      "Qiang Wan",
      "Xuguang Lan"
    ],
    "published": "2025-04-07T07:26:02+00:00",
    "summary": "Significant progress has been made in AI for games, including board games, MOBA, and RTS games. However, complex agents are typically developed in an embedded manner, directly accessing game state information, unlike human players who rely on noisy visual data, leading to unfair competition. Developing complex non-embedded agents remains challenging, especially in card-based RTS games with complex features and large state spaces. We propose a non-embedded offline reinforcement learning training strategy using visual inputs to achieve real-time autonomous gameplay in the RTS game Clash Royale. Due to the lack of a object detection dataset for this game, we designed an efficient generative object detection dataset for training. We extract features using state-of-the-art object detection and optical character recognition models. Our method enables real-time image acquisition, perception feature fusion, decision-making, and control on mobile devices, successfully defeating built-in AI opponents. All code is open-sourced at https://github.com/wty-yy/katacr."
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning for Multiple Anesthetics Collaborative Control",
    "url": "http://arxiv.org/abs/2504.04765v1",
    "arxiv_id": "2504.04765v1",
    "authors": [
      "Huijie Li",
      "Yide Yu",
      "Si Shi",
      "Anmin Hu",
      "Jian Huo",
      "Wei Lin",
      "Chaoran Wu",
      "Wuman Luo"
    ],
    "published": "2025-04-07T06:36:24+00:00",
    "summary": "Automated control of personalized multiple anesthetics in clinical Total Intravenous Anesthesia (TIVA) is crucial yet challenging. Current systems, including target-controlled infusion (TCI) and closed-loop systems, either rely on relatively static pharmacokinetic/pharmacodynamic (PK/PD) models or focus on single anesthetic control, limiting personalization and collaborative control. To address these issues, we propose a novel framework, Value Decomposition Multi-Agent Deep Reinforcement Learning (VD-MADRL). VD-MADRL optimizes the collaboration between two anesthetics propofol (Agent I) and remifentanil (Agent II). And It uses a Markov Game (MG) to identify optimal actions among heterogeneous agents. We employ various value function decomposition methods to resolve the credit allocation problem and enhance collaborative control. We also introduce a multivariate environment model based on random forest (RF) for anesthesia state simulation. Additionally, a data resampling and alignment technique ensures synchronized trajectory data. Our experiments on general and thoracic surgery datasets show that VD-MADRL performs better than human experience. It improves dose precision and keeps anesthesia states stable, providing great clinical value."
  },
  {
    "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images",
    "url": "http://arxiv.org/abs/2504.04753v1",
    "arxiv_id": "2504.04753v1",
    "authors": [
      "Cheng Chen",
      "Jiacheng Wei",
      "Tianrun Chen",
      "Chi Zhang",
      "Xiaofeng Yang",
      "Shangzhan Zhang",
      "Bingchen Yang",
      "Chuan-Sheng Foo",
      "Guosheng Lin",
      "Qixing Huang",
      "Fayao Liu"
    ],
    "published": "2025-04-07T06:01:35+00:00",
    "summary": "Creating CAD digital twins from the physical world is crucial for manufacturing, design, and simulation. However, current methods typically rely on costly 3D scanning with labor-intensive post-processing. To provide a user-friendly design process, we explore the problem of reverse engineering from unconstrained real-world CAD images that can be easily captured by users of all experiences. However, the scarcity of real-world CAD data poses challenges in directly training such models. To tackle these challenges, we propose CADCrafter, an image-to-parametric CAD model generation framework that trains solely on synthetic textureless CAD data while testing on real-world images. To bridge the significant representation disparity between images and parametric CAD models, we introduce a geometry encoder to accurately capture diverse geometric features. Moreover, the texture-invariant properties of the geometric features can also facilitate the generalization to real-world scenarios. Since compiling CAD parameter sequences into explicit CAD models is a non-differentiable process, the network training inherently lacks explicit geometric supervision. To impose geometric validity constraints, we employ direct preference optimization (DPO) to fine-tune our model with the automatic code checker feedback on CAD sequence quality. Furthermore, we collected a real-world dataset, comprised of multi-view images and corresponding CAD command sequence pairs, to evaluate our method. Experimental results demonstrate that our approach can robustly handle real unconstrained CAD images, and even generalize to unseen general objects."
  },
  {
    "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
    "url": "http://arxiv.org/abs/2504.04744v1",
    "arxiv_id": "2504.04744v1",
    "authors": [
      "He Zhu",
      "Quyu Kong",
      "Kechun Xu",
      "Xunlong Xia",
      "Bing Deng",
      "Jieping Ye",
      "Rong Xiong",
      "Yue Wang"
    ],
    "published": "2025-04-07T05:38:23+00:00",
    "summary": "Grounding 3D object affordance is a task that locates objects in 3D space where they can be manipulated, which links perception and action for embodied intelligence. For example, for an intelligent robot, it is necessary to accurately ground the affordance of an object and grasp it according to human instructions. In this paper, we introduce a novel task that grounds 3D object affordance based on language instructions, visual observations and interactions, which is inspired by cognitive science. We collect an Affordance Grounding dataset with Points, Images and Language instructions (AGPIL) to support the proposed task. In the 3D physical world, due to observation orientation, object rotation, or spatial occlusion, we can only get a partial observation of the object. So this dataset includes affordance estimations of objects from full-view, partial-view, and rotation-view perspectives. To accomplish this task, we propose LMAffordance3D, the first multi-modal, language-guided 3D affordance grounding network, which applies a vision-language model to fuse 2D and 3D spatial features with semantic features. Comprehensive experiments on AGPIL demonstrate the effectiveness and superiority of our method on this task, even in unseen experimental settings. Our project is available at https://sites.google.com/view/lmaffordance3d."
  },
  {
    "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
    "url": "http://arxiv.org/abs/2504.04736v1",
    "arxiv_id": "2504.04736v1",
    "authors": [
      "Anna Goldie",
      "Azalia Mirhoseini",
      "Hao Zhou",
      "Irene Cai",
      "Christopher D. Manning"
    ],
    "published": "2025-04-07T05:20:58+00:00",
    "summary": "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%."
  },
  {
    "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
    "url": "http://arxiv.org/abs/2504.04717v1",
    "arxiv_id": "2504.04717v1",
    "authors": [
      "Yubo Li",
      "Xiaobin Shen",
      "Xinyu Yao",
      "Xueying Ding",
      "Yidi Miao",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "published": "2025-04-07T04:00:08+00:00",
    "summary": "Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
  },
  {
    "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
    "url": "http://arxiv.org/abs/2504.04717v2",
    "arxiv_id": "2504.04717v2",
    "authors": [
      "Yubo Li",
      "Xiaobin Shen",
      "Xinyu Yao",
      "Xueying Ding",
      "Yidi Miao",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "published": "2025-04-07T04:00:08+00:00",
    "summary": "Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs."
  },
  {
    "title": "Generalising from Self-Produced Data: Model Training Beyond Human Constraints",
    "url": "http://arxiv.org/abs/2504.04711v1",
    "arxiv_id": "2504.04711v1",
    "authors": [
      "Alfath Daryl Alhajir",
      "Jennifer Dodgson",
      "Joseph Lim",
      "Truong Ma Phi",
      "Julian Peh",
      "Akira Rafhael Janson Pattirane",
      "Lokesh Poovaragan"
    ],
    "published": "2025-04-07T03:48:02+00:00",
    "summary": "Current large language models (LLMs) are constrained by human-derived training data and limited by a single level of abstraction that impedes definitive truth judgments. This paper introduces a novel framework in which AI models autonomously generate and validate new knowledge through direct interaction with their environment. Central to this approach is an unbounded, ungamable numeric reward - such as annexed disk space or follower count - that guides learning without requiring human benchmarks. AI agents iteratively generate strategies and executable code to maximize this metric, with successful outcomes forming the basis for self-retraining and incremental generalisation. To mitigate model collapse and the warm start problem, the framework emphasizes empirical validation over textual similarity and supports fine-tuning via GRPO. The system architecture employs modular agents for environment analysis, strategy generation, and code synthesis, enabling scalable experimentation. This work outlines a pathway toward self-improving AI systems capable of advancing beyond human-imposed constraints toward autonomous general intelligence."
  },
  {
    "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation",
    "url": "http://arxiv.org/abs/2504.04699v1",
    "arxiv_id": "2504.04699v1",
    "authors": [
      "Martin Weyssow",
      "Chengran Yang",
      "Junkai Chen",
      "Yikun Li",
      "Huihui Huang",
      "Ratnadira Widyasari",
      "Han Wei Ang",
      "Frank Liauw",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "published": "2025-04-07T03:04:16+00:00",
    "summary": "Large language models (LLMs) have shown promising performance in software vulnerability detection (SVD), yet their reasoning capabilities remain unreliable. Existing approaches relying on chain-of-thought (CoT) struggle to provide relevant and actionable security assessments. Additionally, effective SVD requires not only generating coherent reasoning but also differentiating between well-founded and misleading yet plausible security assessments, an aspect overlooked in prior work. To this end, we introduce R2Vul, a novel approach that distills structured reasoning into small LLMs using reinforcement learning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce structured, security-aware reasoning that is actionable and reliable while explicitly learning to distinguish valid assessments from misleading ones. We evaluate R2Vul across five languages against SAST tools, CoT, instruction tuning, and classification-based baselines. Our results show that R2Vul with structured reasoning distillation enables a 1.5B student LLM to rival larger models while improving generalization to out-of-distribution vulnerabilities. Beyond model improvements, we contribute a large-scale, multilingual preference dataset featuring structured reasoning to support future research in SVD."
  },
  {
    "title": "Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.04691v1",
    "arxiv_id": "2504.04691v1",
    "authors": [
      "Songyang Liu",
      "Muyang Fan",
      "Weizi Li",
      "Jing Du",
      "Shuai Li"
    ],
    "published": "2025-04-07T02:52:39+00:00",
    "summary": "Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies."
  },
  {
    "title": "HypRL: Reinforcement Learning of Control Policies for Hyperproperties",
    "url": "http://arxiv.org/abs/2504.04675v1",
    "arxiv_id": "2504.04675v1",
    "authors": [
      "Tzu-Han Hsu",
      "Arshia Rafieioskouei",
      "Borzoo Bonakdarpour"
    ],
    "published": "2025-04-07T01:58:36+00:00",
    "summary": "We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP)."
  },
  {
    "title": "HypRL: Reinforcement Learning of Control Policies for Hyperproperties",
    "url": "http://arxiv.org/abs/2504.04675v2",
    "arxiv_id": "2504.04675v2",
    "authors": [
      "Tzu-Han Hsu",
      "Arshia Rafieioskouei",
      "Borzoo Bonakdarpour"
    ],
    "published": "2025-04-07T01:58:36+00:00",
    "summary": "We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP)."
  },
  {
    "title": "ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation Tool using Large Language Models and Reinforcement Learning with Human Feedback",
    "url": "http://arxiv.org/abs/2504.04657v1",
    "arxiv_id": "2504.04657v1",
    "authors": [
      "Tasnia Rahman",
      "Sathish A. P. Kumar",
      "Sumit Jha",
      "Arvind Ramanathan"
    ],
    "published": "2025-04-07T01:11:22+00:00",
    "summary": "Automated Program Repair tools are developed for generating feedback and suggesting a repair method for erroneous code. State of the art (SOTA) code repair methods rely on data-driven approaches and often fail to deliver solution for complicated programming questions. To interpret the natural language of unprecedented programming problems, using Large Language Models (LLMs) for code-feedback generation is crucial. LLMs generate more comprehensible feedback than compiler-generated error messages, and Reinforcement Learning with Human Feedback (RLHF) further enhances quality by integrating human-in-the-loop which helps novice students to lean programming from scratch interactively. We are applying RLHF fine-tuning technique for an expected Socratic response such as a question with hint to solve the programming issue. We are proposing code feedback generation tool by fine-tuning LLM with RLHF, Automated Code Evaluation with RLHF (ACE-RLHF), combining two open-source LLM models with two different SOTA optimization techniques. The quality of feedback is evaluated on two benchmark datasets containing basic and competition-level programming questions where the later is proposed by us. We achieved 2-5% higher accuracy than RL-free SOTA techniques using Llama-3-7B-Proximal-policy optimization in automated evaluation and similar or slightly higher accuracy compared to reward model-free RL with AI Feedback (RLAIF). We achieved almost 40% higher accuracy with GPT-3.5 Best-of-n optimization while performing manual evaluation."
  },
  {
    "title": "Joint Optimization of Handoff and Video Rate in LEO Satellite Networks",
    "url": "http://arxiv.org/abs/2504.04586v1",
    "arxiv_id": "2504.04586v1",
    "authors": [
      "Kyoungjun Park",
      "Zhiyuan He",
      "Cheng Luo",
      "Yi Xu",
      "Lili Qiu",
      "Changhan Ge",
      "Muhammad Muaz",
      "Yuqing Yang"
    ],
    "published": "2025-04-06T18:58:22+00:00",
    "summary": "Low Earth Orbit (LEO) satellite communication presents a promising solution for delivering Internet access to users in remote regions. Given that video content is expected to dominate network traffic in LEO satellite systems, this study presents a new video-aware mobility management framework specifically designed for such networks. By combining simulation models with real-world datasets, we highlight the critical role of handoff strategies and throughput prediction algorithms in both single-user and multi-user video streaming scenarios. Building on these insights, we introduce a suite of innovative algorithms that jointly determine satellite selection and video bitrate to enhance users' quality of experience (QoE). Initially, we design model predictive control (MPC) and reinforcement learning (RL) based methods for individual users, then extend the approach to manage multiple users sharing a satellite. Notably, we incorporate centralized training with distributed inference in our RL design to develop distributed policies informed by a global view. The effectiveness of our approach is validated through trace-driven simulations and testbed experiments."
  },
  {
    "title": "Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning",
    "url": "http://arxiv.org/abs/2504.04524v1",
    "arxiv_id": "2504.04524v1",
    "authors": [
      "Xuerui Su",
      "Shufang Xie",
      "Guoqing Liu",
      "Yingce Xia",
      "Renqian Luo",
      "Peiran Jin",
      "Zhiming Ma",
      "Yue Wang",
      "Zun Wang",
      "Yuting Liu"
    ],
    "published": "2025-04-06T15:48:26+00:00",
    "summary": "Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) haven't yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git."
  },
  {
    "title": "AI2STOW: End-to-End Deep Reinforcement Learning to Construct Master Stowage Plans under Demand Uncertainty",
    "url": "http://arxiv.org/abs/2504.04469v1",
    "arxiv_id": "2504.04469v1",
    "authors": [
      "Jaike Van Twiller",
      "Djordje Grbic",
      "Rune M\u00f8ller Jensen"
    ],
    "published": "2025-04-06T12:45:25+00:00",
    "summary": "The worldwide economy and environmental sustainability depend on eff icient and reliable supply chains, in which container shipping plays a crucial role as an environmentally friendly mode of transport. Liner shipping companies seek to improve operational efficiency by solving the stowage planning problem. Due to many complex combinatorial aspects, stowage planning is challenging and often decomposed into two NP-hard subproblems: master and slot planning. This article proposes AI2STOW, an end-to-end deep reinforcement learning model with feasibility projection and an action mask to create master plans under demand uncertainty with global objectives and constraints, including paired block stowage patterms. Our experimental results demonstrate that AI2STOW outperforms baseline methods from reinforcement learning and stochastic programming in objective performance and computational efficiency, based on simulated instances reflecting the scale of realistic vessels and operational planning horizons."
  },
  {
    "title": "DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication",
    "url": "http://arxiv.org/abs/2504.04438v1",
    "arxiv_id": "2504.04438v1",
    "authors": [
      "Wang Zhang",
      "Chenguang Liu",
      "Yue Pi",
      "Yong Zhang",
      "Hairong Huang",
      "Baoquan Rao",
      "Yulong Ding",
      "Shuanghua Yang",
      "Jie Jiang"
    ],
    "published": "2025-04-06T10:33:08+00:00",
    "summary": "The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing."
  },
  {
    "title": "Deliberate Planning of 3D Bin Packing on Packing Configuration Trees",
    "url": "http://arxiv.org/abs/2504.04421v1",
    "arxiv_id": "2504.04421v1",
    "authors": [
      "Hang Zhao",
      "Juzhan Xu",
      "Kexiong Yu",
      "Ruizhen Hu",
      "Chenyang Zhu",
      "Kai Xu"
    ],
    "published": "2025-04-06T09:07:10+00:00",
    "summary": "Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation, packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. We further discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of BPP setting. A recursive packing method is proposed to decompose large-scale packing into smaller sub-trees while a spatial ensemble mechanism integrates local solutions into global. For different BPP variations with additional decision variables, such as lookahead, buffering, and offline packing, we propose a unified planning framework enabling out-of-the-box problem solving. Extensive evaluations demonstrate that our method outperforms existing online BPP baselines and is versatile in incorporating various practical constraints. The planning process excels across large-scale problems and diverse problem variations. We develop a real-world packing robot for industrial warehousing, with careful designs accounting for constrained placement and transportation stability. Our packing robot operates reliably and efficiently on unprotected pallets at 10 seconds per box. It achieves averagely 19 boxes per pallet with 57.4% space utilization for relatively large-size boxes."
  },
  {
    "title": "Human-Level Competitive Pok\u00e9mon via Scalable Offline Reinforcement Learning with Transformers",
    "url": "http://arxiv.org/abs/2504.04395v1",
    "arxiv_id": "2504.04395v1",
    "authors": [
      "Jake Grigsby",
      "Yuqi Xie",
      "Justin Sasek",
      "Steven Zheng",
      "Yuke Zhu"
    ],
    "published": "2025-04-06T07:35:15+00:00",
    "summary": "Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pok\\'emon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players."
  },
  {
    "title": "Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks",
    "url": "http://arxiv.org/abs/2504.04366v1",
    "arxiv_id": "2504.04366v1",
    "authors": [
      "Sergey Pastukhov"
    ],
    "published": "2025-04-06T05:30:21+00:00",
    "summary": "We introduce a novel hierarchical reinforcement learning (HRL) framework that performs top-down recursive planning via learned subgoals, successfully applied to the complex combinatorial puzzle game Sokoban. Our approach constructs a six-level policy hierarchy, where each higher-level policy generates subgoals for the level below. All subgoals and policies are learned end-to-end from scratch, without any domain knowledge. Our results show that the agent can generate long action sequences from a single high-level call. While prior work has explored 2-3 level hierarchies and subgoal-based planning heuristics, we demonstrate that deep recursive goal decomposition can emerge purely from learning, and that such hierarchies can scale effectively to hard puzzle domains."
  },
  {
    "title": "Economic Battery Storage Dispatch with Deep Reinforcement Learning from Rule-Based Demonstrations",
    "url": "http://arxiv.org/abs/2504.04326v1",
    "arxiv_id": "2504.04326v1",
    "authors": [
      "Manuel Sage",
      "Martin Staniszewski",
      "Yaoyao Fiona Zhao"
    ],
    "published": "2025-04-06T02:16:42+00:00",
    "summary": "The application of deep reinforcement learning algorithms to economic battery dispatch problems has significantly increased recently. However, optimizing battery dispatch over long horizons can be challenging due to delayed rewards. In our experiments we observe poor performance of popular actor-critic algorithms when trained on yearly episodes with hourly resolution. To address this, we propose an approach extending soft actor-critic (SAC) with learning from demonstrations. The special feature of our approach is that, due to the absence of expert demonstrations, the demonstration data is generated through simple, rule-based policies. We conduct a case study on a grid-connected microgrid and use if-then-else statements based on the wholesale price of electricity to collect demonstrations. These are stored in a separate replay buffer and sampled with linearly decaying probability along with the agent's own experiences. Despite these minimal modifications and the imperfections in the demonstration data, the results show a drastic performance improvement regarding both sample efficiency and final rewards. We further show that the proposed method reliably outperforms the demonstrator and is robust to the choice of rule, as long as the rule is sufficient to guide early training into the right direction."
  },
  {
    "title": "Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement",
    "url": "http://arxiv.org/abs/2504.04300v1",
    "arxiv_id": "2504.04300v1",
    "authors": [
      "Anastasis Kratsios",
      "Xiaofei Shi",
      "Qiang Sun",
      "Zhanhao Zhang"
    ],
    "published": "2025-04-05T23:29:46+00:00",
    "summary": "We present a general computational framework for solving continuous-time financial market equilibria under minimal modeling assumptions while incorporating realistic financial frictions, such as trading costs, and supporting multiple interacting agents. Inspired by generative adversarial networks (GANs), our approach employs a novel generative deep reinforcement learning framework with a decoupling feedback system embedded in the adversarial training loop, which we term as the \\emph{reinforcement link}. This architecture stabilizes the training dynamics by incorporating feedback from the discriminator. Our theoretically guided feedback mechanism enables the decoupling of the equilibrium system, overcoming challenges that hinder conventional numerical algorithms. Experimentally, our algorithm not only learns but also provides testable predictions on how asset returns and volatilities emerge from the endogenous trading behavior of market participants, where traditional analytical methods fall short. The design of our model is further supported by an approximation guarantee."
  },
  {
    "title": "ORCA: An Open-Source, Reliable, Cost-Effective, Anthropomorphic Robotic Hand for Uninterrupted Dexterous Task Learning",
    "url": "http://arxiv.org/abs/2504.04259v1",
    "arxiv_id": "2504.04259v1",
    "authors": [
      "Clemens C. Christoph",
      "Maximilian Eberlein",
      "Filippos Katsimalis",
      "Arturo Roberti",
      "Aristotelis Sympetheros",
      "Michel R. Vogt",
      "Davide Liconti",
      "Chenyu Yang",
      "Barnabas Gavin Cangan",
      "Ronan J. Hinchet",
      "Robert K. Katzschmann"
    ],
    "published": "2025-04-05T19:34:34+00:00",
    "summary": "General-purpose robots should possess humanlike dexterity and agility to perform tasks with the same versatility as us. A human-like form factor further enables the use of vast datasets of human-hand interactions. However, the primary bottleneck in dexterous manipulation lies not only in software but arguably even more in hardware. Robotic hands that approach human capabilities are often prohibitively expensive, bulky, or require enterprise-level maintenance, limiting their accessibility for broader research and practical applications. What if the research community could get started with reliable dexterous hands within a day? We present the open-source ORCA hand, a reliable and anthropomorphic 17-DoF tendon-driven robotic hand with integrated tactile sensors, fully assembled in less than eight hours and built for a material cost below 2,000 CHF. We showcase ORCA's key design features such as popping joints, auto-calibration, and tensioning systems that significantly reduce complexity while increasing reliability, accuracy, and robustness. We benchmark the ORCA hand across a variety of tasks, ranging from teleoperation and imitation learning to zero-shot sim-to-real reinforcement learning. Furthermore, we demonstrate its durability, withstanding more than 10,000 continuous operation cycles - equivalent to approximately 20 hours - without hardware failure, the only constraint being the duration of the experiment itself. All design files, source code, and documentation will be available at https://www.orcahand.com/."
  },
  {
    "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
    "url": "http://arxiv.org/abs/2504.04191v1",
    "arxiv_id": "2504.04191v1",
    "authors": [
      "Jieming Cui",
      "Tengyu Liu",
      "Ziyu Meng",
      "Jiale Yu",
      "Ran Song",
      "Wei Zhang",
      "Yixin Zhu",
      "Siyuan Huang"
    ],
    "published": "2025-04-05T14:44:47+00:00",
    "summary": "Learning open-vocabulary physical skills for simulated agents presents a significant challenge in artificial intelligence. Current reinforcement learning approaches face critical limitations: manually designed rewards lack scalability across diverse tasks, while demonstration-based methods struggle to generalize beyond their training distribution. We introduce GROVE, a generalized reward framework that enables open-vocabulary physical skill learning without manual engineering or task-specific demonstrations. Our key insight is that Large Language Models(LLMs) and Vision Language Models(VLMs) provide complementary guidance -- LLMs generate precise physical constraints capturing task requirements, while VLMs evaluate motion semantics and naturalness. Through an iterative design process, VLM-based feedback continuously refines LLM-generated constraints, creating a self-improving reward system. To bridge the domain gap between simulation and natural images, we develop Pose2CLIP, a lightweight mapper that efficiently projects agent poses directly into semantic feature space without computationally expensive rendering. Extensive experiments across diverse embodiments and learning paradigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion naturalness and 25.7% better task completion scores while training 8.4x faster than previous methods. These results establish a new foundation for scalable physical skill acquisition in simulated environments."
  },
  {
    "title": "CutQAS: Topology-aware quantum circuit cutting via reinforcement learning",
    "url": "http://arxiv.org/abs/2504.04167v1",
    "arxiv_id": "2504.04167v1",
    "authors": [
      "Abhishek Sadhu",
      "Aritra Sarkar",
      "Akash Kundu"
    ],
    "published": "2025-04-05T13:13:50+00:00",
    "summary": "Simulating molecular systems on quantum processors has the potential to surpass classical methods in computational resource efficiency. The limited qubit connectivity, small processor size, and short coherence times of near-term quantum hardware constrain the applicability of quantum algorithms like QPE and VQE. Quantum circuit cutting mitigates these constraints by partitioning large circuits into smaller subcircuits, enabling execution on resource-limited devices. However, finding optimal circuit partitions remains a significant challenge, affecting both computational efficiency and accuracy. To address these limitations, in this article, we propose CutQAS, a novel framework that integrates quantum circuit cutting with quantum architecture search (QAS) to enhance quantum chemistry simulations. Our framework employs a multi-step reinforcement learning (RL) agent to optimize circuit configurations. First, an RL agent explores all possible topologies to identify an optimal circuit structure. Subsequently, a second RL agent refines the selected topology by determining optimal circuit cuts, ensuring efficient execution on constrained hardware. Through numerical simulations, we demonstrate the effectiveness of our method in improving simulation accuracy and resource efficiency. This approach presents a scalable solution for quantum chemistry applications, offering a systematic pathway to overcoming hardware constraints in near-term quantum computing."
  },
  {
    "title": "MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.04164v1",
    "arxiv_id": "2504.04164v1",
    "authors": [
      "Shiguang Sun",
      "Hanbo Zhang",
      "Zeyang Liu",
      "Xinrui Yang",
      "Lipeng Wan",
      "Bing Yan",
      "Xingyu Chen",
      "Xuguang Lan"
    ],
    "published": "2025-04-05T12:57:31+00:00",
    "summary": "Existing visual model-based reinforcement learning (MBRL) algorithms with observation reconstruction often suffer from information conflicts, making it difficult to learn compact representations and hence result in less robust policies, especially in the presence of task-irrelevant visual distractions. In this paper, we first reveal that the information conflicts in current visual MBRL algorithms stem from visual representation learning and latent dynamics modeling with an information-theoretic perspective. Based on this finding, we present a new algorithm to resolve information conflicts for visual MBRL, named MInCo, which mitigates information conflicts by leveraging negative-free contrastive learning, aiding in learning invariant representation and robust policies despite noisy observations. To prevent the dominance of visual representation learning, we introduce time-varying reweighting to bias the learning towards dynamics modeling as training proceeds. We evaluate our method on several robotic control tasks with dynamic background distractions. Our experiments demonstrate that MInCo learns invariant representations against background noise and consistently outperforms current state-of-the-art visual MBRL methods. Code is available at https://github.com/ShiguangSun/minco."
  },
  {
    "title": "OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics",
    "url": "http://arxiv.org/abs/2504.04160v1",
    "arxiv_id": "2504.04160v1",
    "authors": [
      "Alexandre Oliveira",
      "Katarina Dyreby",
      "Francisco Caldas",
      "Cl\u00e1udia Soares"
    ],
    "published": "2025-04-05T12:44:21+00:00",
    "summary": "The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations."
  },
  {
    "title": "A General Peg-in-Hole Assembly Policy Based on Domain Randomized Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.04148v1",
    "arxiv_id": "2504.04148v1",
    "authors": [
      "Xinyu Liu",
      "Aljaz Kramberger",
      "Leon Bodenhagen"
    ],
    "published": "2025-04-05T11:39:53+00:00",
    "summary": "Generalization is important for peg-in-hole assembly, a fundamental industrial operation, to adapt to dynamic industrial scenarios and enhance manufacturing efficiency. While prior work has enhanced generalization ability for pose variations, spatial generalization to six degrees of freedom (6-DOF) is less researched, limiting application in real-world scenarios. This paper addresses this limitation by developing a general policy GenPiH using Proximal Policy Optimization(PPO) and dynamic simulation with domain randomization. The policy learning experiment demonstrates the policy's generalization ability with nearly 100\\% success insertion across over eight thousand unique hole poses in parallel environments, and sim-to-real validation on a UR10e robot confirms the policy's performance through direct trajectory execution without task-specific tuning."
  },
  {
    "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models",
    "url": "http://arxiv.org/abs/2504.04042v1",
    "arxiv_id": "2504.04042v1",
    "authors": [
      "Kepu Zhang",
      "Weijie Yu",
      "Zhongxiang Sun",
      "Jun Xu"
    ],
    "published": "2025-04-05T03:34:51+00:00",
    "summary": "Syllogistic reasoning is a fundamental aspect of legal decision-making, enabling logical conclusions by connecting general legal principles with specific case facts. Although existing large language models (LLMs) can generate responses to legal questions, they fail to perform explicit syllogistic reasoning, often producing implicit and unstructured answers that lack explainability and trustworthiness. To address this limitation, we propose SyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic legal reasoning. SyLeR integrates a tree-structured hierarchical retrieval mechanism to effectively combine relevant legal statutes and precedent cases, forming comprehensive major premises. This is followed by a two-stage fine-tuning process: supervised fine-tuning warm-up establishes a foundational understanding of syllogistic reasoning, while reinforcement learning with a structure-aware reward mechanism refines the ability of the model to generate diverse logically sound and well-structured reasoning paths. We conducted extensive experiments across various dimensions, including in-domain and cross-domain user groups (legal laypersons and practitioners), multiple languages (Chinese and French), and different LLM backbones (legal-specific and open-domain LLMs). The results show that SyLeR significantly improves response accuracy and consistently delivers explicit, explainable, and trustworthy legal reasoning."
  },
  {
    "title": "ADAPT: Actively Discovering and Adapting to Preferences for any Task",
    "url": "http://arxiv.org/abs/2504.04040v1",
    "arxiv_id": "2504.04040v1",
    "authors": [
      "Maithili Patel",
      "Xavier Puig",
      "Ruta Desai",
      "Roozbeh Mottaghi",
      "Sonia Chernova",
      "Joanne Truong",
      "Akshara Rai"
    ],
    "published": "2025-04-05T03:16:22+00:00",
    "summary": "Assistive agents should be able to perform under-specified long-horizon tasks while respecting user preferences. We introduce Actively Discovering and Adapting to Preferences for any Task (ADAPT) -- a benchmark designed to evaluate agents' ability to adhere to user preferences across various household tasks through active questioning. Next, we propose Reflection-DPO, a novel training approach for adapting large language models (LLMs) to the task of active questioning. Reflection-DPO finetunes a 'student' LLM to follow the actions of a privileged 'teacher' LLM, and optionally ask a question to gather necessary information to better predict the teacher action. We find that prior approaches that use state-of-the-art LLMs fail to sufficiently follow user preferences in ADAPT due to insufficient questioning and poor adherence to elicited preferences. In contrast, Reflection-DPO achieves a higher rate of satisfying user preferences, outperforming a zero-shot chain-of-thought baseline by 6.1% on unseen users."
  },
  {
    "title": "Rethinking Reflection in Pre-Training",
    "url": "http://arxiv.org/abs/2504.04022v1",
    "arxiv_id": "2504.04022v1",
    "authors": [
      "Essential AI",
      ":",
      "Darsh J Shah",
      "Peter Rushton",
      "Somanshu Singla",
      "Mohit Parmar",
      "Kurt Smith",
      "Yash Vanjani",
      "Ashish Vaswani",
      "Adarsh Chaluvaraju",
      "Andrew Hojel",
      "Andrew Ma",
      "Anil Thomas",
      "Anthony Polloreno",
      "Ashish Tanwer",
      "Burhan Drak Sibai",
      "Divya S Mansingka",
      "Divya Shivaprasad",
      "Ishaan Shah",
      "Karl Stratos",
      "Khoi Nguyen",
      "Michael Callahan",
      "Michael Pust",
      "Mrinal Iyer",
      "Philip Monk",
      "Platon Mazarakis",
      "Ritvik Kapila",
      "Saurabh Srivastava",
      "Tim Romanski"
    ],
    "published": "2025-04-05T02:24:07+00:00",
    "summary": "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks."
  },
  {
    "title": "Improving Offline Mixed-Criticality Scheduling with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.03994v1",
    "arxiv_id": "2504.03994v1",
    "authors": [
      "Muhammad El-Mahdy",
      "Nourhan Sakr",
      "Rodrigo Carrasco"
    ],
    "published": "2025-04-04T23:28:48+00:00",
    "summary": "This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.   Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios."
  },
  {
    "title": "Improving Mixed-Criticality Scheduling with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.03994v2",
    "arxiv_id": "2504.03994v2",
    "authors": [
      "Muhammad El-Mahdy",
      "Nourhan Sakr",
      "Rodrigo Carrasco"
    ],
    "published": "2025-04-04T23:28:48+00:00",
    "summary": "This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.   Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios."
  },
  {
    "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models",
    "url": "http://arxiv.org/abs/2504.03991v1",
    "arxiv_id": "2504.03991v1",
    "authors": [
      "Siddharth Srikanth",
      "Varun Bhatt",
      "Boshen Zhang",
      "Werner Hager",
      "Charles Michael Lewis",
      "Katia P. Sycara",
      "Aaquib Tabrez",
      "Stefanos Nikolaidis"
    ],
    "published": "2025-04-04T23:09:40+00:00",
    "summary": "Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration."
  },
  {
    "title": "I Can Hear You Coming: RF Sensing for Uncooperative Satellite Evasion",
    "url": "http://arxiv.org/abs/2504.03983v1",
    "arxiv_id": "2504.03983v1",
    "authors": [
      "Cameron Mehlman",
      "Gregory Falco"
    ],
    "published": "2025-04-04T22:54:27+00:00",
    "summary": "Uncooperative satellite engagements with nation-state actors prompts the need for enhanced maneuverability and agility on-orbit. However, robust, autonomous and rapid adversary avoidance capabilities for the space environment is seldom studied. Further, the capability constrained nature of many space vehicles does not afford robust space situational awareness capabilities that can inform maneuvers. We present a \"Cat & Mouse\" system for training optimal adversary avoidance algorithms using Reinforcement Learning (RL). We propose the novel approach of utilizing intercepted radio frequency communication and dynamic spacecraft state as multi-modal input that could inform paths for a mouse to outmaneuver the cat satellite. Given the current ubiquitous use of RF communications, our proposed system can be applicable to a diverse array of satellites. In addition to providing a comprehensive framework for an RL architecture capable of training performant and adaptive adversary avoidance policies, we also explore several optimization based methods for adversarial avoidance on real-world data obtained from the Space Surveillance Network (SSN) to analyze the benefits and limitations of different avoidance methods."
  },
  {
    "title": "Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization",
    "url": "http://arxiv.org/abs/2504.03961v1",
    "arxiv_id": "2504.03961v1",
    "authors": [
      "Mario Rico Ibanez",
      "Azim Akhtarshenas",
      "David Lopez-Perez",
      "Giovanni Geraci"
    ],
    "published": "2025-04-04T22:06:01+00:00",
    "summary": "Unmanned aerial vehicle (UAV)-based base stations offer a promising solution in emergencies where the rapid deployment of cutting-edge networks is crucial for maximizing life-saving potential. Optimizing the strategic positioning of these UAVs is essential for enhancing communication efficiency. This paper introduces an automated reinforcement learning approach that enables UAVs to dynamically interact with their environment and determine optimal configurations. By leveraging the radio signal sensing capabilities of communication networks, our method provides a more realistic perspective, utilizing state-of-the-art algorithm -- proximal policy optimization -- to learn and generalize positioning strategies across diverse user equipment (UE) movement patterns. We evaluate our approach across various UE mobility scenarios, including static, random, linear, circular, and mixed hotspot movements. The numerical results demonstrate the algorithm's adaptability and effectiveness in maintaining comprehensive coverage across all movement patterns."
  },
  {
    "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking",
    "url": "http://arxiv.org/abs/2504.03947v1",
    "arxiv_id": "2504.03947v1",
    "authors": [
      "Chris Samarinas",
      "Hamed Zamani"
    ],
    "published": "2025-04-04T21:27:48+00:00",
    "summary": "We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems."
  },
  {
    "title": "An Exploration-free Method for a Linear Stochastic Bandit Driven by a Linear Gaussian Dynamical System",
    "url": "http://arxiv.org/abs/2504.03926v1",
    "arxiv_id": "2504.03926v1",
    "authors": [
      "Jonathan Gornet",
      "Yilin Mo",
      "Bruno Sinopoli"
    ],
    "published": "2025-04-04T20:46:35+00:00",
    "summary": "In stochastic multi-armed bandits, a major problem the learner faces is the trade-off between exploration and exploitation. Recently, exploration-free methods -- methods that commit to the action predicted to return the highest reward -- have been studied from the perspective of linear bandits. In this paper, we introduce a linear bandit setting where the reward is the output of a linear Gaussian dynamical system. Motivated by a problem encountered in hyperparameter optimization for reinforcement learning, where the number of actions is much higher than the number of training iterations, we propose Kalman filter Observability Dependent Exploration (KODE), an exploration-free method that utilizes the Kalman filter predictions to select actions. Our major contribution of this work is our analysis of the performance of the proposed method, which is dependent on the observability properties of the underlying linear Gaussian dynamical system. We evaluate KODE via two different metrics: regret, which is the cumulative expected difference between the highest possible reward and the reward sampled by KODE, and action alignment, which measures how closely KODE's chosen action aligns with the linear Gaussian dynamical system's state variable. To provide intuition on the performance, we prove that KODE implicitly encourages the learner to explore actions depending on the observability of the linear Gaussian dynamical system. This method is compared to several well-known stochastic multi-armed bandit algorithms to validate our theoretical results."
  },
  {
    "title": "Align to Structure: Aligning Large Language Models with Structural Information",
    "url": "http://arxiv.org/abs/2504.03622v1",
    "arxiv_id": "2504.03622v1",
    "authors": [
      "Zae Myung Kim",
      "Anand Ramachandran",
      "Farideh Tavazoee",
      "Joo-Kyung Kim",
      "Oleg Rokhlenko",
      "Dongyeop Kang"
    ],
    "published": "2025-04-04T17:40:04+00:00",
    "summary": "Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align."
  },
  {
    "title": "Optimization of a Triangular Delaunay Mesh Generator using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.03610v1",
    "arxiv_id": "2504.03610v1",
    "authors": [
      "Will Thacher",
      "Per-Olof Persson",
      "Yulong Pan"
    ],
    "published": "2025-04-04T17:30:50+00:00",
    "summary": "In this work we introduce a triangular Delaunay mesh generator that can be trained using reinforcement learning to maximize a given mesh quality metric. Our mesh generator consists of a graph neural network that distributes and modifies vertices, and a standard Delaunay algorithm to triangulate the vertices. We explore various design choices and evaluate our mesh generator on various tasks including mesh generation, mesh improvement, and producing variable resolution meshes. The learned mesh generator outputs meshes that are comparable to those produced by Triangle and DistMesh, two popular Delaunay-based mesh generators."
  },
  {
    "title": "Dexterous Manipulation through Imitation Learning: A Survey",
    "url": "http://arxiv.org/abs/2504.03515v1",
    "arxiv_id": "2504.03515v1",
    "authors": [
      "Shan An",
      "Ziyu Meng",
      "Chao Tang",
      "Yuning Zhou",
      "Tengyu Liu",
      "Fangqiang Ding",
      "Shufang Zhang",
      "Yao Mu",
      "Ran Song",
      "Wei Zhang",
      "Zeng-Guang Hou",
      "Hong Zhang"
    ],
    "published": "2025-04-04T15:14:38+00:00",
    "summary": "Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high-dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning (IL), details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain."
  },
  {
    "title": "Learning Dual-Arm Coordination for Grasping Large Flat Objects",
    "url": "http://arxiv.org/abs/2504.03500v1",
    "arxiv_id": "2504.03500v1",
    "authors": [
      "Yongliang Wang",
      "Hamidreza Kasaei"
    ],
    "published": "2025-04-04T14:55:46+00:00",
    "summary": "Grasping large flat objects, such as books or keyboards lying horizontally, presents significant challenges for single-arm robotic systems, often requiring extra actions like pushing objects against walls or moving them to the edge of a surface to facilitate grasping. In contrast, dual-arm manipulation, inspired by human dexterity, offers a more refined solution by directly coordinating both arms to lift and grasp the object without the need for complex repositioning. In this paper, we propose a model-free deep reinforcement learning (DRL) framework to enable dual-arm coordination for grasping large flat objects. We utilize a large-scale grasp pose detection model as a backbone to extract high-dimensional features from input images, which are then used as the state representation in a reinforcement learning (RL) model. A CNN-based Proximal Policy Optimization (PPO) algorithm with shared Actor-Critic layers is employed to learn coordinated dual-arm grasp actions. The system is trained and tested in Isaac Gym and deployed to real robots. Experimental results demonstrate that our policy can effectively grasp large flat objects without requiring additional maneuvers. Furthermore, the policy exhibits strong generalization capabilities, successfully handling unseen objects. Importantly, it can be directly transferred to real robots without fine-tuning, consistently outperforming baseline methods."
  },
  {
    "title": "Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and Graph Neural Networks",
    "url": "http://arxiv.org/abs/2504.03429v1",
    "arxiv_id": "2504.03429v1",
    "authors": [
      "Alexander Mattick",
      "Maniraman Periyasamy",
      "Christian Ufrecht",
      "Abhishek Y. Dubey",
      "Christopher Mutschler",
      "Axel Plinge",
      "Daniel D. Scherer"
    ],
    "published": "2025-04-04T13:19:08+00:00",
    "summary": "Quantum computing is currently strongly limited by the impact of noise, in particular introduced by the application of two-qubit gates. For this reason, reducing the number of two-qubit gates is of paramount importance on noisy intermediate-scale quantum hardware. To advance towards more reliable quantum computing, we introduce a framework based on ZX calculus, graph-neural networks and reinforcement learning for quantum circuit optimization. By combining reinforcement learning and tree search, our method addresses the challenge of selecting optimal sequences of ZX calculus rewrite rules. Instead of relying on existing heuristic rules for minimizing circuits, our method trains a novel reinforcement learning policy that directly operates on ZX-graphs, therefore allowing us to search through the space of all possible circuit transformations to find a circuit significantly minimizing the number of CNOT gates. This way we can scale beyond hard-coded rules towards discovering arbitrary optimization rules. We demonstrate our method's competetiveness with state-of-the-art circuit optimizers and generalization capabilities on large sets of diverse random circuits."
  },
  {
    "title": "DML-RAM: Deep Multimodal Learning Framework for Robotic Arm Manipulation using Pre-trained Models",
    "url": "http://arxiv.org/abs/2504.03423v1",
    "arxiv_id": "2504.03423v1",
    "authors": [
      "Sathish Kumar",
      "Swaroop Damodaran",
      "Naveen Kumar Kuruba",
      "Sumit Jha",
      "Arvind Ramanathan"
    ],
    "published": "2025-04-04T13:11:43+00:00",
    "summary": "This paper presents a novel deep learning framework for robotic arm manipulation that integrates multimodal inputs using a late-fusion strategy. Unlike traditional end-to-end or reinforcement learning approaches, our method processes image sequences with pre-trained models and robot state data with machine learning algorithms, fusing their outputs to predict continuous action values for control. Evaluated on BridgeData V2 and Kuka datasets, the best configuration (VGG16 + Random Forest) achieved MSEs of 0.0021 and 0.0028, respectively, demonstrating strong predictive performance and robustness. The framework supports modularity, interpretability, and real-time decision-making, aligning with the goals of adaptive, human-in-the-loop cyber-physical systems."
  },
  {
    "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
    "url": "http://arxiv.org/abs/2504.03420v1",
    "arxiv_id": "2504.03420v1",
    "authors": [
      "Gianluca Maselli",
      "Vieri Giuliano Santucci"
    ],
    "published": "2025-04-04T13:06:23+00:00",
    "summary": "Dealing with environments with sparse rewards has always been crucial for systems developed to operate in autonomous open-ended learning settings. Intrinsic Motivations could be an effective way to help Deep Reinforcement Learning algorithms learn in such scenarios. In fact, intrinsic reward signals, such as novelty or curiosity, are generally adopted to improve exploration when extrinsic rewards are delayed or absent. Building on previous works, we tackle the problem of learning policies in the presence of sparse rewards by proposing a two-level architecture that alternates an ''intrinsically driven'' phase of exploration and autonomous sub-goal generation, to a phase of sparse reward, goal-directed policy learning. The idea is to build several small networks, each one specialized on a particular sub-path, and use them as starting points for future exploration without the need to further explore from scratch previously learnt paths. Two versions of the system have been trained and tested in the Gym SuperMarioBros environment without considering any additional extrinsic reward. The results show the validity of our approach and the importance of autonomously segment the environment to generate an efficient path towards the final goal."
  },
  {
    "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.03380v1",
    "arxiv_id": "2504.03380v1",
    "authors": [
      "Sanghwan Bae",
      "Jiwoo Hong",
      "Min Young Lee",
      "Hanbyul Kim",
      "JeongYeon Nam",
      "Donghyun Kwak"
    ],
    "published": "2025-04-04T11:52:05+00:00",
    "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set."
  },
  {
    "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
    "url": "http://arxiv.org/abs/2504.03272v1",
    "arxiv_id": "2504.03272v1",
    "authors": [
      "Enguerrand Prebet",
      "Samuel Teuber",
      "Andr\u00e9 Platzer"
    ],
    "published": "2025-04-04T08:43:31+00:00",
    "summary": "This article presents a formal model and formal safety proofs for the ABZ'25 case study in differential dynamic logic (dL). The case study considers an autonomous car driving on a highway avoiding collisions with neighbouring cars. Using KeYmaera X's dL implementation, we prove absence of collision on an infinite time horizon which ensures that safety is preserved independently of trip length. The safety guarantees hold for time-varying reaction time and brake force. Our dL model considers the single lane scenario with cars ahead or behind. We demonstrate that dL with its tools is a rigorous foundation for runtime monitoring, shielding, and neural network verification. Doing so sheds light on inconsistencies between the provided specification and simulation environment highway-env of the ABZ'25 study. We attempt to fix these inconsistencies and uncover numerous counterexamples which also indicate issues in the provided reinforcement learning environment."
  },
  {
    "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
    "url": "http://arxiv.org/abs/2504.03206v1",
    "arxiv_id": "2504.03206v1",
    "authors": [
      "Yanming Wan",
      "Jiaxing Wu",
      "Marwa Abdulhai",
      "Lior Shani",
      "Natasha Jaques"
    ],
    "published": "2025-04-04T06:35:02+00:00",
    "summary": "Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them."
  },
  {
    "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
    "url": "http://arxiv.org/abs/2504.03185v1",
    "arxiv_id": "2504.03185v1",
    "authors": [
      "Jaymari Chua",
      "Chen Wang",
      "Lina Yao"
    ],
    "published": "2025-04-04T05:26:28+00:00",
    "summary": "Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings."
  },
  {
    "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.03164v1",
    "arxiv_id": "2504.03164v1",
    "authors": [
      "Kexin Tian",
      "Jingrui Mao",
      "Yunlong Zhang",
      "Jiwan Jiang",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "published": "2025-04-04T04:43:10+00:00",
    "summary": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning."
  },
  {
    "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
    "url": "http://arxiv.org/abs/2504.03164v2",
    "arxiv_id": "2504.03164v2",
    "authors": [
      "Kexin Tian",
      "Jingrui Mao",
      "Yunlong Zhang",
      "Jiwan Jiang",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "published": "2025-04-04T04:43:10+00:00",
    "summary": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning-key capabilities for autonomous driving-still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose NuScenes-SpatialQA, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning."
  },
  {
    "title": "Enhanced Penalty-based Bidirectional Reinforcement Learning Algorithms",
    "url": "http://arxiv.org/abs/2504.03163v1",
    "arxiv_id": "2504.03163v1",
    "authors": [
      "Sai Gana Sandeep Pula",
      "Sathish A. P. Kumar",
      "Sumit Jha",
      "Arvind Ramanathan"
    ],
    "published": "2025-04-04T04:43:07+00:00",
    "summary": "This research focuses on enhancing reinforcement learning (RL) algorithms by integrating penalty functions to guide agents in avoiding unwanted actions while optimizing rewards. The goal is to improve the learning process by ensuring that agents learn not only suitable actions but also which actions to avoid. Additionally, we reintroduce a bidirectional learning approach that enables agents to learn from both initial and terminal states, thereby improving speed and robustness in complex environments. Our proposed Penalty-Based Bidirectional methodology is tested against Mani skill benchmark environments, demonstrating an optimality improvement of success rate of approximately 4% compared to existing RL implementations. The findings indicate that this integrated strategy enhances policy learning, adaptability, and overall performance in challenging scenarios"
  },
  {
    "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
    "url": "http://arxiv.org/abs/2504.03160v1",
    "arxiv_id": "2504.03160v1",
    "authors": [
      "Yuxiang Zheng",
      "Dayuan Fu",
      "Xiangkun Hu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Pengrui Lu",
      "Pengfei Liu"
    ],
    "published": "2025-04-04T04:41:28+00:00",
    "summary": "Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
  },
  {
    "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
    "url": "http://arxiv.org/abs/2504.03160v2",
    "arxiv_id": "2504.03160v2",
    "authors": [
      "Yuxiang Zheng",
      "Dayuan Fu",
      "Xiangkun Hu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Pengrui Lu",
      "Pengfei Liu"
    ],
    "published": "2025-04-04T04:41:28+00:00",
    "summary": "Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
  },
  {
    "title": "MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories",
    "url": "http://arxiv.org/abs/2504.03153v1",
    "arxiv_id": "2504.03153v1",
    "authors": [
      "Natalie Tirabassi",
      "Sathish A. P. Kumar",
      "Sumit Jha",
      "Arvind Ramanathan"
    ],
    "published": "2025-04-04T04:15:52+00:00",
    "summary": "We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments."
  },
  {
    "title": "Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards",
    "url": "http://arxiv.org/abs/2504.03040v1",
    "arxiv_id": "2504.03040v1",
    "authors": [
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "published": "2025-04-03T21:35:22+00:00",
    "summary": "Safe Reinforcement Learning (Safe RL) aims to train an RL agent to maximize its performance in real-world environments while adhering to safety constraints, as exceeding safety violation limits can result in severe consequences. In this paper, we propose a novel safe RL approach called Safety Modulated Policy Optimization (SMPO), which enables safe policy function learning within the standard policy optimization framework through safety modulated rewards. In particular, we consider safety violation costs as feedback from the RL environments that are parallel to the standard awards, and introduce a Q-cost function as safety critic to estimate expected future cumulative costs. Then we propose to modulate the rewards using a cost-aware weighting function, which is carefully designed to ensure the safety limits based on the estimation of the safety critic, while maximizing the expected rewards. The policy function and the safety critic are simultaneously learned through gradient descent during online interactions with the environment. We conduct experiments using multiple RL environments and the experimental results demonstrate that our method outperforms several classic and state-of-the-art comparison methods in terms of overall safe RL performance."
  },
  {
    "title": "Low Rank Factorizations are Indirect Encodings for Deep Neuroevolution",
    "url": "http://arxiv.org/abs/2504.03037v1",
    "arxiv_id": "2504.03037v1",
    "authors": [
      "Jack Garbus",
      "Jordan Pollack"
    ],
    "published": "2025-04-03T21:31:48+00:00",
    "summary": "Deep neuroevolution is a highly scalable alternative to reinforcement learning due to its unique ability to encode network updates in a small number of bytes. Recent insights from traditional deep learning indicate high-dimensional models possess intrinsic, low-rank structure. In this work, we introduce low-rank, factorized neuroevolution: an indirect encoding through which we can search a small space of low-rank factors that enforce underlying structure across a network's weights. We compare our approach with non-factorized networks of similar and smaller size to understand how much performance can be attributed to the smaller search space. We evaluate our method on a language modeling task using transformers, as well as continuous and discrete vision-based reinforcement learning tasks. Our study shows that low-rank, factorized neuroevolution outperforms or is competitive with non-factorized neuroevolution, performing notably well on language modeling. Our results also suggest deleterious factorized mutations have a stronger negative impact on performance than deleterious non-factorized mutations, which significantly reduces the runtime on environments with early termination for bad performers. More broadly, these results show how we can use insights from backpropgation-based methods to enhance neuroevolution"
  },
  {
    "title": "Deep Reinforcement Learning via Object-Centric Attention",
    "url": "http://arxiv.org/abs/2504.03024v1",
    "arxiv_id": "2504.03024v1",
    "authors": [
      "Jannis Bl\u00fcml",
      "Cedric Derstroff",
      "Bjarne Gregori",
      "Elisabeth Dillies",
      "Quentin Delfosse",
      "Kristian Kersting"
    ],
    "published": "2025-04-03T20:48:27+00:00",
    "summary": "Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. Contrary to deep agents, no single object-centric architecture can be applied to any environment. Inspired by principles of cognitive science and Occam's Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information. Specifically, OCCAM takes advantage of the object-centric inductive bias. Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines."
  },
  {
    "title": "Anomaly Detection in Time Series Data Using Reinforcement Learning, Variational Autoencoder, and Active Learning",
    "url": "http://arxiv.org/abs/2504.02999v1",
    "arxiv_id": "2504.02999v1",
    "authors": [
      "Bahareh Golchin",
      "Banafsheh Rekabdar"
    ],
    "published": "2025-04-03T19:41:52+00:00",
    "summary": "A novel approach to detecting anomalies in time series data is presented in this paper. This approach is pivotal in domains such as data centers, sensor networks, and finance. Traditional methods often struggle with manual parameter tuning and cannot adapt to new anomaly types. Our method overcomes these limitations by integrating Deep Reinforcement Learning (DRL) with a Variational Autoencoder (VAE) and Active Learning. By incorporating a Long Short-Term Memory (LSTM) network, our approach models sequential data and its dependencies effectively, allowing for the detection of new anomaly classes with minimal labeled data. Our innovative DRL- VAE and Active Learning combination significantly improves existing methods, as shown by our evaluations on real-world datasets, enhancing anomaly detection techniques and advancing time series analysis."
  },
  {
    "title": "Improving log-based anomaly detection through learned adaptive filter",
    "url": "http://arxiv.org/abs/2504.02994v1",
    "arxiv_id": "2504.02994v1",
    "authors": [
      "Yiyuan Xiong",
      "Shaofeng Cai"
    ],
    "published": "2025-04-03T19:31:24+00:00",
    "summary": "Log messages record important system runtime information and are useful for detecting anomalous behaviors and managing modern software systems. Many supervised and unsupervised learning methods have been proposed recently for log-based anomaly detection. State-of-the-art unsupervised methods predict the next log event given a log sequence and apply fixed configurations that use the same filter condition (i.e. k, the top k predicted log events will be regarded as normal next events) which leads to inferior performance in the detection stage because it sets one fixed k for all log sequences, which ignores the dynamic nature and variance in different log sequences. Recently, deep reinforcement learning (DRL) are widely applied to make intelligent decisions in a dynamic environment. In this work, we contend that it is necessary to apply adaptive filters for different log sequences. To achieve this, we propose a novel approach based on DRL to construct a learned adaptive filter and apply different normal/abnormal filter thresholds for different log sequences. We define the Markov Decision Process (MDP) and formulate the learned adaptive filter as a problem that can be solved by DRL. We evaluate the learned adaptive filter on two state-of-the-art log-based anomaly detection unsupervised approaches DeepLog and LogAnomaly in two datasets HDFS and BGL. Extensive experiments show that our approach outperforms the fixed configurations and achieves significantly better performance in log-based anomaly detection."
  },
  {
    "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02949v1",
    "arxiv_id": "2504.02949v1",
    "authors": [
      "Xianwei Zhuang",
      "Yuxin Xie",
      "Yufan Deng",
      "Dongchao Yang",
      "Liming Liang",
      "Jinghan Ru",
      "Yuguo Yin",
      "Yuexian Zou"
    ],
    "published": "2025-04-03T18:06:28+00:00",
    "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1."
  },
  {
    "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization",
    "url": "http://arxiv.org/abs/2504.02725v1",
    "arxiv_id": "2504.02725v1",
    "authors": [
      "Kehua Feng",
      "Keyan Ding",
      "Jing Yu",
      "Menghan Li",
      "Yuhao Wang",
      "Tong Xu",
      "Xinda Wang",
      "Qiang Zhang",
      "Huajun Chen"
    ],
    "published": "2025-04-03T16:07:38+00:00",
    "summary": "Recent advancements in large language models (LLMs) have accelerated progress toward artificial general intelligence, yet their potential to generate harmful content poses critical safety challenges. Existing alignment methods often struggle to cover diverse safety scenarios and remain vulnerable to adversarial attacks. In this work, we propose Ex-Ante Reasoning Preference Optimization (ERPO), a novel safety alignment framework that equips LLMs with explicit preemptive reasoning through Chain-of-Thought and provides clear evidence for safety judgments by embedding predefined safety rules. Specifically, our approach consists of three stages: first, equipping the model with Ex-Ante reasoning through supervised fine-tuning (SFT) using a constructed reasoning module; second, enhancing safety, usefulness, and efficiency via Direct Preference Optimization (DPO); and third, mitigating inference latency with a length-controlled iterative preference optimization strategy. Experiments on multiple open-source LLMs demonstrate that ERPO significantly enhances safety performance while maintaining response efficiency."
  },
  {
    "title": "A Numerically Efficient Method to Enhance Model Predictive Control Performance with a Reinforcement Learning Policy",
    "url": "http://arxiv.org/abs/2504.02710v1",
    "arxiv_id": "2504.02710v1",
    "authors": [
      "Andrea Ghezzi",
      "Rudolf Reiter",
      "Katrin Baumg\u00e4rtner",
      "Alberto Bemporad",
      "Moritz Diehl"
    ],
    "published": "2025-04-03T15:50:47+00:00",
    "summary": "We propose a novel approach for combining model predictive control (MPC) with reinforcement learning (RL) to reduce online computation while achieving high closed-loop tracking performance and constraint satisfaction. This method, called Policy-Enhanced Partial Tightening (PEPT), approximates the optimal value function through a Riccati recursion around a state-control trajectory obtained by evaluating the RL policy. The result is a convex quadratic terminal cost that can be seamlessly integrated into the MPC formulation. The proposed controller is tested in simulations on a trajectory tracking problem for a quadcopter with nonlinear dynamics and bounded state and control. The results highlight PEPT's effectiveness, outperforming both pure RL policies and several MPC variations. Compared to pure RL, PEPT achieves 1000 times lower constraint violation cost with only twice the feedback time. Against the best MPC-based policy, PEPT reduces constraint violations by 2 to 5 times and runs nearly 3 times faster while maintaining similar tracking performance. The code is open-source at www.github.com/aghezz1/pept."
  },
  {
    "title": "Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL",
    "url": "http://arxiv.org/abs/2504.02688v1",
    "arxiv_id": "2504.02688v1",
    "authors": [
      "Achilles Kiwanuka Machumilane",
      "Alberto Gotta",
      "Pietro Cassar\u00e0"
    ],
    "published": "2025-04-03T15:28:04+00:00",
    "summary": "Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted next-generation wireless networks is critical for mobility management and ensuring UAV safety and ubiquitous connectivity, especially in dense urban environments with street canyons and tall buildings. Traditional statistical and model-based techniques have been successfully used for path optimization in communication networks. However, when dynamic channel propagation characteristics such as line-of-sight (LOS), interference, handover, and signal-to-interference and noise ratio (SINR) are included in path optimization, statistical and model-based path planning solutions become obsolete since they cannot adapt to the dynamic and time-varying wireless channels, especially in the mmWave bands. In this paper, we propose a novel model-free actor-critic deep reinforcement learning (AC-DRL) framework for path optimization in UAV-assisted 5G mmWave wireless networks, which combines four important aspects of UAV communication: \\textit{flight time, handover, connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to a gNB to determine the optimal path to a desired destination in the shortest possible time with minimal gNB handover, while maintaining connectivity and the highest possible SINR. We train our model with data from a powerful ray tracing tool called Wireless InSite, which uses 3D images of the propagation environment and provides data that closely resembles the real propagation environment. The simulation results show that our system has superior performance in tracking high SINR compared to other selected RL algorithms."
  },
  {
    "title": "Integrating Human Knowledge Through Action Masking in Reinforcement Learning for Operations Research",
    "url": "http://arxiv.org/abs/2504.02662v1",
    "arxiv_id": "2504.02662v1",
    "authors": [
      "Mirko Stappert",
      "Bernhard Lutz",
      "Niklas Goby",
      "Dirk Neumann"
    ],
    "published": "2025-04-03T15:00:04+00:00",
    "summary": "Reinforcement learning (RL) provides a powerful method to address problems in operations research. However, its real-world application often fails due to a lack of user acceptance and trust. A possible remedy is to provide managers with the possibility of altering the RL policy by incorporating human expert knowledge. In this study, we analyze the benefits and caveats of including human knowledge via action masking. While action masking has so far been used to exclude invalid actions, its ability to integrate human expertise remains underexplored. Human knowledge is often encapsulated in heuristics, which suggest reasonable, near-optimal actions in certain situations. Enforcing such actions should hence increase trust among the human workforce to rely on the model's decisions. Yet, a strict enforcement of heuristic actions may also restrict the policy from exploring superior actions, thereby leading to overall lower performance. We analyze the effects of action masking based on three problems with different characteristics, namely, paint shop scheduling, peak load management, and inventory management. Our findings demonstrate that incorporating human knowledge through action masking can achieve substantial improvements over policies trained without action masking. In addition, we find that action masking is crucial for learning effective policies in constrained action spaces, where certain actions can only be performed a limited number of times. Finally, we highlight the potential for suboptimal outcomes when action masks are overly restrictive."
  },
  {
    "title": "SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02654v1",
    "arxiv_id": "2504.02654v1",
    "authors": [
      "Ivo Amador",
      "Nina Gierasimczuk"
    ],
    "published": "2025-04-03T14:51:11+00:00",
    "summary": "We propose a learning architecture that allows symbolic control and guidance in reinforcement learning with deep neural networks. We introduce SymDQN, a novel modular approach that augments the existing Dueling Deep Q-Networks (DuelDQN) architecture with modules based on the neuro-symbolic framework of Logic Tensor Networks (LTNs). The modules guide action policy learning and allow reinforcement learning agents to display behaviour consistent with reasoning about the environment. Our experiment is an ablation study performed on the modules. It is conducted in a reinforcement learning environment of a 5x5 grid navigated by an agent that encounters various shapes, each associated with a given reward. The underlying DuelDQN attempts to learn the optimal behaviour of the agent in this environment, while the modules facilitate shape recognition and reward prediction. We show that our architecture significantly improves learning, both in terms of performance and the precision of the agent. The modularity of SymDQN allows reflecting on the intricacies and complexities of combining neural and symbolic approaches in reinforcement learning."
  },
  {
    "title": "Solving the Paint Shop Problem with Flexible Management of Multi-Lane Buffers Using Reinforcement Learning and Action Masking",
    "url": "http://arxiv.org/abs/2504.02644v1",
    "arxiv_id": "2504.02644v1",
    "authors": [
      "Mirko Stappert",
      "Bernhard Lutz",
      "Janis Brammer",
      "Dirk Neumann"
    ],
    "published": "2025-04-03T14:37:40+00:00",
    "summary": "In the paint shop problem, an unordered incoming sequence of cars assigned to different colors has to be reshuffled with the objective of minimizing the number of color changes. To reshuffle the incoming sequence, manufacturers can employ a first-in-first-out multi-lane buffer system allowing store and retrieve operations. So far, prior studies primarily focused on simple decision heuristics like greedy or simplified problem variants that do not allow full flexibility when performing store and retrieve operations. In this study, we propose a reinforcement learning approach to minimize color changes for the flexible problem variant, where store and retrieve operations can be performed in an arbitrary order. After proving that greedy retrieval is optimal, we incorporate this finding into the model using action masking. Our evaluation, based on 170 problem instances with 2-8 buffer lanes and 5-15 colors, shows that our approach reduces color changes compared to existing methods by considerable margins depending on the problem size. Furthermore, we demonstrate the robustness of our approach towards different buffer sizes and imbalanced color distributions."
  },
  {
    "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "url": "http://arxiv.org/abs/2504.02605v1",
    "arxiv_id": "2504.02605v1",
    "authors": [
      "Daoguang Zan",
      "Zhirong Huang",
      "Wei Liu",
      "Hanwu Chen",
      "Linhao Zhang",
      "Shulin Xin",
      "Lu Chen",
      "Qi Liu",
      "Xiaojian Zhong",
      "Aoyan Li",
      "Siyao Liu",
      "Yongsheng Xiao",
      "Liangqiang Chen",
      "Yuyu Zhang",
      "Jing Su",
      "Tianyu Liu",
      "Rui Long",
      "Kai Shen",
      "Liang Xiang"
    ],
    "published": "2025-04-03T14:06:17+00:00",
    "summary": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI."
  },
  {
    "title": "Regulating Spatial Fairness in a Tripartite Micromobility Sharing System via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02597v1",
    "arxiv_id": "2504.02597v1",
    "authors": [
      "Matteo Cederle",
      "Marco Fabris",
      "Gian Antonio Susto"
    ],
    "published": "2025-04-03T13:59:29+00:00",
    "summary": "In the growing field of Shared Micromobility Systems, which holds great potential for shaping urban transportation, fairness-oriented approaches remain largely unexplored. This work addresses such a gap by investigating the balance between performance optimization and algorithmic fairness in Shared Micromobility Services using Reinforcement Learning. Our methodology achieves equitable outcomes, measured by the Gini index, across central, peripheral, and remote station categories. By strategically rebalancing vehicle distribution, it maximizes operator performance while upholding fairness principles. The efficacy of our approach is validated through a case study using synthetic data."
  },
  {
    "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning",
    "url": "http://arxiv.org/abs/2504.02590v1",
    "arxiv_id": "2504.02590v1",
    "authors": [
      "Kepu Zhang",
      "Guofu Xie",
      "Weijie Yu",
      "Mingyue Xu",
      "Xu Tang",
      "Yaxin Li",
      "Jun Xu"
    ],
    "published": "2025-04-03T13:54:53+00:00",
    "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying them to real-world scenarios, as it directly affects the credibility of the LLM. While existing legal LLMs can perform general judicial question answering, their legal mathematical reasoning capabilities have not been trained. Open-domain reasoning models, though able to generate detailed calculation steps, do not follow the reasoning logic required for legal scenarios. Additionally, there is currently a lack of legal mathematical reasoning datasets to help validate and enhance LLMs' reasoning abilities in legal contexts. To address these issues, we propose the first Chinese legal Mathematical Reasoning Dataset, LexNum, which includes three common legal mathematical reasoning scenarios: economic compensation, work injury compensation, and traffic accident compensation. Based on LexNum, we tested the performance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a reinforcement learning algorithm guided by legal procedural awareness to train LLMs, enhancing their mathematical reasoning abilities in legal scenarios. Experiments on tasks in the three legal scenarios show that the performance of existing legal LLMs and reasoning models in legal mathematical reasoning tasks is unsatisfactory. LexPam can enhance the LLM's ability in these tasks."
  },
  {
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme",
    "url": "http://arxiv.org/abs/2504.02587v1",
    "arxiv_id": "2504.02587v1",
    "authors": [
      "Yan Ma",
      "Steffi Chern",
      "Xuyang Shen",
      "Yiran Zhong",
      "Pengfei Liu"
    ],
    "published": "2025-04-03T13:53:28+00:00",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research."
  },
  {
    "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme",
    "url": "http://arxiv.org/abs/2504.02587v2",
    "arxiv_id": "2504.02587v2",
    "authors": [
      "Yan Ma",
      "Steffi Chern",
      "Xuyang Shen",
      "Yiran Zhong",
      "Pengfei Liu"
    ],
    "published": "2025-04-03T13:53:28+00:00",
    "summary": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research."
  },
  {
    "title": "MAD: A Magnitude And Direction Policy Parametrization for Stability Constrained Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02565v1",
    "arxiv_id": "2504.02565v1",
    "authors": [
      "Luca Furieri",
      "Sucheth Shenoy",
      "Danilo Saccani",
      "Andrea Martin",
      "Giancarlo Ferrari Trecate"
    ],
    "published": "2025-04-03T13:26:26+00:00",
    "summary": "We introduce magnitude and direction (MAD) policies, a policy parameterization for reinforcement learning (RL) that preserves Lp closed-loop stability for nonlinear dynamical systems. Although complete in their ability to describe all stabilizing controllers, methods based on nonlinear Youla and system-level synthesis are significantly affected by the difficulty of parameterizing Lp-stable operators. In contrast, MAD policies introduce explicit feedback on state-dependent features - a key element behind the success of RL pipelines - without compromising closed-loop stability. This is achieved by describing the magnitude of the control input with a disturbance-feedback Lp-stable operator, while selecting its direction based on state-dependent features through a universal function approximator. We further characterize the robust stability properties of MAD policies under model mismatch. Unlike existing disturbance-feedback policy parameterizations, MAD policies introduce state-feedback components compatible with model-free RL pipelines, ensuring closed-loop stability without requiring model information beyond open-loop stability. Numerical experiments show that MAD policies trained with deep deterministic policy gradient (DDPG) methods generalize to unseen scenarios, matching the performance of standard neural network policies while guaranteeing closed-loop stability by design."
  },
  {
    "title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning",
    "url": "http://arxiv.org/abs/2504.02546v1",
    "arxiv_id": "2504.02546v1",
    "authors": [
      "Xiangxiang Chu",
      "Hailang Huang",
      "Xiao Zhang",
      "Fei Wei",
      "Yong Wang"
    ],
    "published": "2025-04-03T12:53:41+00:00",
    "summary": "Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG."
  },
  {
    "title": "Probabilistic Pontryagin's Maximum Principle for Continuous-Time Model-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02543v1",
    "arxiv_id": "2504.02543v1",
    "authors": [
      "David Leeftink",
      "\u00c7a\u011fatay Y\u0131ld\u0131z",
      "Steffen Ridderbusch",
      "Max Hinne",
      "Marcel van Gerven"
    ],
    "published": "2025-04-03T12:51:20+00:00",
    "summary": "Without exact knowledge of the true system dynamics, optimal control of non-linear continuous-time systems requires careful treatment of epistemic uncertainty. In this work, we propose a probabilistic extension to Pontryagin's maximum principle by minimizing the mean Hamiltonian with respect to epistemic uncertainty. We show minimization of the mean Hamiltonian is a necessary optimality condition when optimizing the mean cost, and propose a multiple shooting numerical method scalable to large-scale probabilistic dynamical models, including ensemble neural ordinary differential equations. Comparisons against state-of-the-art methods in online and offline model-based reinforcement learning tasks show that our probabilistic Hamiltonian formulation leads to reduced trial costs in offline settings and achieves competitive performance in online scenarios. By bridging optimal control and reinforcement learning, our approach offers a principled and practical framework for controlling uncertain systems with learned dynamics."
  },
  {
    "title": "Probabilistic Pontryagin's Maximum Principle for Continuous-Time Model-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02543v2",
    "arxiv_id": "2504.02543v2",
    "authors": [
      "David Leeftink",
      "\u00c7a\u011fatay Y\u0131ld\u0131z",
      "Steffen Ridderbusch",
      "Max Hinne",
      "Marcel van Gerven"
    ],
    "published": "2025-04-03T12:51:20+00:00",
    "summary": "Without exact knowledge of the true system dynamics, optimal control of non-linear continuous-time systems requires careful treatment of epistemic uncertainty. In this work, we propose a probabilistic extension to Pontryagin's maximum principle by minimizing the mean Hamiltonian with respect to epistemic uncertainty. We show minimization of the mean Hamiltonian is a necessary optimality condition when optimizing the mean cost, and propose a multiple shooting numerical method scalable to large-scale probabilistic dynamical models, including ensemble neural ordinary differential equations. Comparisons against state-of-the-art methods in online and offline model-based reinforcement learning tasks show that our probabilistic Hamiltonian formulation leads to reduced trial costs in offline settings and achieves competitive performance in online scenarios. By bridging optimal control and reinforcement learning, our approach offers a principled and practical framework for controlling uncertain systems with learned dynamics."
  },
  {
    "title": "Inference-Time Scaling for Generalist Reward Modeling",
    "url": "http://arxiv.org/abs/2504.02495v1",
    "arxiv_id": "2504.02495v1",
    "authors": [
      "Zijun Liu",
      "Peiyi Wang",
      "Runxin Xu",
      "Shirong Ma",
      "Chong Ruan",
      "Peng Li",
      "Yang Liu",
      "Yu Wu"
    ],
    "published": "2025-04-03T11:19:49+00:00",
    "summary": "Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced."
  },
  {
    "title": "Inference-Time Scaling for Generalist Reward Modeling",
    "url": "http://arxiv.org/abs/2504.02495v2",
    "arxiv_id": "2504.02495v2",
    "authors": [
      "Zijun Liu",
      "Peiyi Wang",
      "Runxin Xu",
      "Shirong Ma",
      "Chong Ruan",
      "Peng Li",
      "Yang Liu",
      "Yu Wu"
    ],
    "published": "2025-04-03T11:19:49+00:00",
    "summary": "Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced."
  },
  {
    "title": "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets",
    "url": "http://arxiv.org/abs/2504.02479v1",
    "arxiv_id": "2504.02479v1",
    "authors": [
      "Stefano Covone",
      "Italo Napolitano",
      "Francesco De Lellis",
      "Mario di Bernardo"
    ],
    "published": "2025-04-03T10:56:57+00:00",
    "summary": "We propose a decentralized reinforcement learning solution for multi-agent shepherding of non-cohesive targets using policy-gradient methods. Our architecture integrates target-selection with target-driving through Proximal Policy Optimization, overcoming discrete-action constraints of previous Deep Q-Network approaches and enabling smoother agent trajectories. This model-free framework effectively solves the shepherding problem without prior dynamics knowledge. Experiments demonstrate our method's effectiveness and scalability with increased target numbers and limited sensing capabilities."
  },
  {
    "title": "CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles",
    "url": "http://arxiv.org/abs/2504.02450v1",
    "arxiv_id": "2504.02450v1",
    "authors": [
      "Jingyi Wang",
      "Duanfeng Chu",
      "Zejian Deng",
      "Liping Lu"
    ],
    "published": "2025-04-03T10:15:19+00:00",
    "summary": "To address the current challenges of low intelligence and simplistic vehicle behavior modeling in autonomous driving simulation scenarios, this paper proposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles (CHARMS). The model can reason about the behavior of other vehicles like a human driver and respond with different decision-making styles, thereby improving the intelligence and diversity of the surrounding vehicles in the driving scenario. By introducing the Level-k behavioral game theory, the paper models the decision-making process of human drivers and employs deep reinforcement learning to train the models with diverse decision styles, simulating different reasoning approaches and behavioral characteristics. Building on the Poisson cognitive hierarchy theory, this paper also presents a novel driving scenario generation method. The method controls the proportion of vehicles with different driving styles in the scenario using Poisson and binomial distributions, thus generating controllable and diverse driving environments. Experimental results demonstrate that CHARMS not only exhibits superior decision-making capabilities as ego vehicles, but also generates more complex and diverse driving scenarios as surrounding vehicles. We will release code for CHARMS at https://github.com/WUTAD-Wjy/CHARMS."
  },
  {
    "title": "On learning racing policies with reinforcement learning",
    "url": "http://arxiv.org/abs/2504.02420v1",
    "arxiv_id": "2504.02420v1",
    "authors": [
      "Grzegorz Czechmanowski",
      "Jan W\u0119grzynowski",
      "Piotr Kicki",
      "Krzysztof Walas"
    ],
    "published": "2025-04-03T09:21:48+00:00",
    "summary": "Fully autonomous vehicles promise enhanced safety and efficiency. However, ensuring reliable operation in challenging corner cases requires control algorithms capable of performing at the vehicle limits. We address this requirement by considering the task of autonomous racing and propose solving it by learning a racing policy using Reinforcement Learning (RL). Our approach leverages domain randomization, actuator dynamics modeling, and policy architecture design to enable reliable and safe zero-shot deployment on a real platform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a state-of-the-art Model Predictive Control (MPC), but, to the best of our knowledge, also represents the first instance of an RL policy outperforming expert human drivers in RC racing. This work identifies the key factors driving this performance improvement, providing critical insights for the design of robust RL-based control strategies for autonomous vehicles."
  },
  {
    "title": "F5R-TTS: Improving Flow Matching based Text-to-Speech with Group Relative Policy Optimization",
    "url": "http://arxiv.org/abs/2504.02407v1",
    "arxiv_id": "2504.02407v1",
    "authors": [
      "Xiaohui Sun",
      "Ruitong Xiao",
      "Jianye Mo",
      "Bowen Wu",
      "Qun Yu",
      "Baoxun Wang"
    ],
    "published": "2025-04-03T08:57:15+00:00",
    "summary": "We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (relatively 29.5\\% WER reduction) and speaker similarity (relatively 4.6\\% SIM score increase) compared to conventional flow-matching based TTS systems. Audio samples are available at https://frontierlabs.github.io/F5R."
  },
  {
    "title": "Reinforcement Learning for Solving the Pricing Problem in Column Generation: Applications to Vehicle Routing",
    "url": "http://arxiv.org/abs/2504.02383v1",
    "arxiv_id": "2504.02383v1",
    "authors": [
      "Abdo Abouelrous",
      "Laurens Bliek",
      "Adriana F. Gabor",
      "Yaoxin Wu",
      "Yingqian Zhang"
    ],
    "published": "2025-04-03T08:22:19+00:00",
    "summary": "In this paper, we address the problem of Column Generation (CG) using Reinforcement Learning (RL). Specifically, we use a RL model based on the attention-mechanism architecture to find the columns with most negative reduced cost in the Pricing Problem (PP). Unlike previous Machine Learning (ML) applications for CG, our model deploys an end-to-end mechanism as it independently solves the pricing problem without the help of any heuristic. We consider a variant of Vehicle Routing Problem (VRP) as a case study for our method. Through a set of experiments where our method is compared against a Dynamic Programming (DP)-based heuristic for solving the PP, we show that our method solves the linear relaxation up to a reasonable objective gap within 9% in significantly shorter running times, up to over 300 times faster for instances with 100 customers."
  },
  {
    "title": "An Efficient Reservation Protocol for Medium Access: When Tree Splitting Meets Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.02376v1",
    "arxiv_id": "2504.02376v1",
    "authors": [
      "Yutao Chen",
      "Wei Chen"
    ],
    "published": "2025-04-03T08:10:25+00:00",
    "summary": "As an enhanced version of massive machine-type communication in 5G, massive communication has emerged as one of the six usage scenarios anticipated for 6G, owing to its potential in industrial internet-of-things and smart metering. Driven by the need for random multiple-access (RMA) in massive communication, as well as, next-generation Wi-Fi, medium access control has attracted considerable recent attention. Holding the promise of attaining bandwidth-efficient collision resolution, multiaccess reservation no doubt plays a central role in RMA, e.g., the distributed coordination function (DCF) in IEEE 802.11. In this paper, we are interested in maximizing the bandwidth efficiency of reservation protocols for RMA under quality-of-service constraints. Particularly, we present a tree splitting based reservation scheme, in which the attempting probability is dynamically optimized by partially observable Markov decision process or reinforcement learning (RL). The RL-empowered tree-splitting algorithm guarantees that all these terminals with backlogged packets at the beginning of a contention cycle can be scheduled, thereby providing a first-in-first-out service. More importantly, it substantially reduces the reservation bandwidth determined by the communication complexity of DCF, through judiciously conceived coding and interaction for exchanging information required by distributed ordering. Simulations demonstrate that the proposed algorithm outperforms the CSMA/CA based DCF in IEEE 802.11."
  },
  {
    "title": "Improving turbulence control through explainable deep learning",
    "url": "http://arxiv.org/abs/2504.02354v1",
    "arxiv_id": "2504.02354v1",
    "authors": [
      "Miguel Beneitez",
      "Andres Cremades",
      "Luca Guastoni",
      "Ricardo Vinuesa"
    ],
    "published": "2025-04-03T07:42:47+00:00",
    "summary": "Turbulent-flow control aims to develop strategies that effectively manipulate fluid systems, such as the reduction of drag in transportation and enhancing energy efficiency, both critical steps towards reducing global CO$_2$ emissions. Deep reinforcement learning (DRL) offers novel tools to discover flow-control strategies, which we combine with our knowledge of the physics of turbulence. We integrate explainable deep learning (XDL) to objectively identify the coherent structures containing the most informative regions in the flow, with a DRL model trained to reduce them. The trained model targets the most relevant regions in the flow to sustain turbulence and produces a drag reduction which is higher than that of a model specifically trained to reduce the drag, while using only half its power consumption. Moreover, the XDL model results in a better drag reduction than other models focusing on specific classically identified coherent structures. This demonstrates that combining DRL with XDL can produce causal control strategies that precisely target the most influential features of turbulence. By directly addressing the core mechanisms that sustain turbulence, our approach offers a powerful pathway towards its efficient control, which is a long-standing challenge in physics with profound implications for energy systems, climate modeling and aerodynamics."
  },
  {
    "title": "Improving turbulence control through explainable deep learning",
    "url": "http://arxiv.org/abs/2504.02354v2",
    "arxiv_id": "2504.02354v2",
    "authors": [
      "Miguel Beneitez",
      "Andres Cremades",
      "Luca Guastoni",
      "Ricardo Vinuesa"
    ],
    "published": "2025-04-03T07:42:47+00:00",
    "summary": "Turbulent-flow control aims to develop strategies that effectively manipulate fluid systems, such as the reduction of drag in transportation and enhancing energy efficiency, both critical steps towards reducing global CO$_2$ emissions. Deep reinforcement learning (DRL) offers novel tools to discover flow-control strategies, which we combine with our knowledge of the physics of turbulence. We integrate explainable deep learning (XDL) to objectively identify the coherent structures containing the most informative regions in the flow, with a DRL model trained to reduce them. The trained model targets the most relevant regions in the flow to sustain turbulence and produces a drag reduction which is higher than that of a model specifically trained to reduce the drag, while using only half its power consumption. Moreover, the XDL model results in a better drag reduction than other models focusing on specific classically identified coherent structures. This demonstrates that combining DRL with XDL can produce causal control strategies that precisely target the most influential features of turbulence. By directly addressing the core mechanisms that sustain turbulence, our approach offers a powerful pathway towards its efficient control, which is a long-standing challenge in physics with profound implications for energy systems, climate modeling and aerodynamics."
  },
  {
    "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models",
    "url": "http://arxiv.org/abs/2504.02327v1",
    "arxiv_id": "2504.02327v1",
    "authors": [
      "Weibin Liao",
      "Xin Gao",
      "Tianyu Jia",
      "Rihong Qiu",
      "Yifan Zhu",
      "Yang Lin",
      "Xu Chu",
      "Junfeng Zhao",
      "Yasha Wang"
    ],
    "published": "2025-04-03T06:59:44+00:00",
    "summary": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling seamless interaction with databases. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable performance in this domain. However, existing NL2SQL methods predominantly rely on closed-source LLMs leveraging prompt engineering, while open-source models typically require fine-tuning to acquire domain-specific knowledge. Despite these efforts, open-source LLMs struggle with complex NL2SQL tasks due to the indirect expression of user query objectives and the semantic gap between user queries and database schemas. Inspired by the application of reinforcement learning in mathematical problem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT (Learning NL2SQL with AST-guided Task Decomposition), a novel framework that improves the performance of open-source LLMs on complex NL2SQL tasks through task decomposition and reinforcement learning. LearNAT introduces three key components: (1) a Decomposition Synthesis Procedure that leverages Abstract Syntax Trees (ASTs) to guide efficient search and pruning strategies for task decomposition, (2) Margin-aware Reinforcement Learning, which employs fine-grained step-level optimization via DPO with AST margins, and (3) Adaptive Demonstration Reasoning, a mechanism for dynamically selecting relevant examples to enhance decomposition capabilities. Extensive experiments on two benchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a 7B-parameter open-source LLM to achieve performance comparable to GPT-4, while offering improved efficiency and accessibility."
  },
  {
    "title": "Parallel Market Environments for FinRL Contests",
    "url": "http://arxiv.org/abs/2504.02281v1",
    "arxiv_id": "2504.02281v1",
    "authors": [
      "Keyi Wang",
      "Kairong Xiao",
      "Xiao-Yang Liu Yanglet"
    ],
    "published": "2025-04-03T05:08:04+00:00",
    "summary": "Reinforcement learning has shown great potential in finance. We have organized the FinRL Contests 2023-2025 featuring different financial tasks. Large language models have a strong capability to process financial texts. Integrating LLM-generated signals into FinRL is a new task, enabling agents to use both structured market data and unstructured financial text. To address the sampling bottleneck during training, we introduce GPU-based parallel market environments to improve sampling speed. In this paper, we summarize the parallel market environments used in FinRL Contests 2023-2025. Two new environments incorporate LLM-generated signals and support massively parallel simulation. Contestants utilize these environments to train agents for stock and cryptocurrency trading tasks."
  },
  {
    "title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models",
    "url": "http://arxiv.org/abs/2504.02273v1",
    "arxiv_id": "2504.02273v1",
    "authors": [
      "Hung Le",
      "Dai Do",
      "Dung Nguyen",
      "Svetha Venkatesh"
    ],
    "published": "2025-04-03T04:46:17+00:00",
    "summary": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these successes have been largely demonstrated on large-scale models with billions of parameters, where a strong pretraining foundation ensures effective initial exploration. In contrast, RL remains challenging for tiny LLMs with 1 billion parameters or fewer because they lack the necessary pretraining strength to explore effectively, often leading to suboptimal reasoning patterns. This work introduces a novel intrinsic motivation approach that leverages episodic memory to address this challenge, improving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven learning, our method leverages successful reasoning patterns stored in memory while allowing for controlled exploration to generate novel responses. Intrinsic rewards are computed efficiently using a kNN-based episodic memory, allowing the model to discover new reasoning strategies while quickly adapting to effective past solutions. Experiments on fine-tuning GSM8K and AI-MO datasets demonstrate that our approach significantly enhances smaller LLMs' sample efficiency and generalization capability, making RL-based reasoning improvements more accessible in low-resource settings."
  },
  {
    "title": "Adapting World Models with Latent-State Dynamics Residuals",
    "url": "http://arxiv.org/abs/2504.02252v1",
    "arxiv_id": "2504.02252v1",
    "authors": [
      "JB Lanier",
      "Kyungmin Kim",
      "Armin Karamzade",
      "Yifei Liu",
      "Ankita Sinha",
      "Kat He",
      "Davide Corsi",
      "Roy Fox"
    ],
    "published": "2025-04-03T03:41:30+00:00",
    "summary": "Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail."
  },
  {
    "title": "Learning and Improving Backgammon Strategy",
    "url": "http://arxiv.org/abs/2504.02221v1",
    "arxiv_id": "2504.02221v1",
    "authors": [
      "Gregory R. Galperin"
    ],
    "published": "2025-04-03T02:27:22+00:00",
    "summary": "A novel approach to learning is presented, combining features of on-line and off-line methods to achieve considerable performance in the task of learning a backgammon value function in a process that exploits the processing power of parallel supercomputers. The off-line methods comprise a set of techniques for parallelizing neural network training and $TD(\\lambda)$ reinforcement learning; here Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line policy improvement technique which applies resources to the decision points encountered during the search of the game tree to further augment the learned value function estimate. A level of play roughly as good as, or possibly better than, the current champion human and computer backgammon players has been achieved in a short period of learning."
  },
  {
    "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment",
    "url": "http://arxiv.org/abs/2504.02193v1",
    "arxiv_id": "2504.02193v1",
    "authors": [
      "Yifan Wang",
      "Runjin Chen",
      "Bolian Li",
      "David Cho",
      "Yihe Deng",
      "Ruqi Zhang",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Ananth Grama",
      "Junyuan Hong"
    ],
    "published": "2025-04-03T00:36:40+00:00",
    "summary": "Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings."
  },
  {
    "title": "FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets",
    "url": "http://arxiv.org/abs/2504.02174v1",
    "arxiv_id": "2504.02174v1",
    "authors": [
      "Rushi Jayeshkumar Babaria",
      "Minzhao Lyu",
      "Gustavo Batista",
      "Vijay Sivaraman"
    ],
    "published": "2025-04-02T23:17:14+00:00",
    "summary": "Network traffic classification is of great importance for network operators in their daily routines, such as analyzing the usage patterns of multimedia applications and optimizing network configurations. Internet service providers (ISPs) that operate high-speed links expect network flow classifiers to accurately classify flows early, using the minimal number of necessary initial packets per flow. These classifiers must also be robust to packet sequence disorders in candidate flows and capable of detecting unseen flow types that are not within the existing classification scope, which are not well achieved by existing methods. In this paper, we develop FastFlow, a time-series flow classification method that accurately classifies network flows as one of the known types or the unknown type, which dynamically selects the minimal number of packets to balance accuracy and efficiency. Toward the objectives, we first develop a flow representation process that converts packet streams at both per-packet and per-slot granularity for precise packet statistics with robustness to packet sequence disorders. Second, we develop a sequential decision-based classification model that leverages LSTM architecture trained with reinforcement learning. Our model makes dynamic decisions on the minimal number of time-series data points per flow for the confident classification as one of the known flow types or an unknown one. We evaluated our method on public datasets and demonstrated its superior performance in early and accurate flow classification. Deployment insights on the classification of over 22.9 million flows across seven application types and 33 content providers in a campus network over one week are discussed, showing that FastFlow requires an average of only 8.37 packets and 0.5 seconds to classify the application type of a flow with over 91% accuracy and over 96% accuracy for the content providers."
  },
  {
    "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning",
    "url": "http://arxiv.org/abs/2504.02161v1",
    "arxiv_id": "2504.02161v1",
    "authors": [
      "Zhen Meng",
      "Kan Chen",
      "Xiangmin Xu",
      "Erwin Jose Lopez Pulgarin",
      "Emma Li",
      "Philip G. Zhao",
      "David Flynn"
    ],
    "published": "2025-04-02T22:20:48+00:00",
    "summary": "Active 3D scene representation is pivotal in modern robotics applications, including remote inspection, manipulation, and telepresence. Traditional methods primarily optimize geometric fidelity or rendering accuracy, but often overlook operator-specific objectives, such as safety-critical coverage or task-driven viewpoints. This limitation leads to suboptimal viewpoint selection, particularly in constrained environments such as nuclear decommissioning. To bridge this gap, we introduce a novel framework that integrates expert operator preferences into the active 3D scene representation pipeline. Specifically, we employ Reinforcement Learning from Human Feedback (RLHF) to guide robotic path planning, reshaping the reward function based on expert input. To capture operator-specific priorities, we conduct interactive choice experiments that evaluate user preferences in 3D scene representation. We validate our framework using a UR3e robotic arm for reactor tile inspection in a nuclear decommissioning scenario. Compared to baseline methods, our approach enhances scene representation while optimizing trajectory efficiency. The RLHF-based policy consistently outperforms random selection, prioritizing task-critical details. By unifying explicit 3D geometric modeling with implicit human-in-the-loop optimization, this work establishes a foundation for adaptive, safety-critical robotic perception systems, paving the way for enhanced automation in nuclear decommissioning, remote maintenance, and other high-risk environments."
  },
  {
    "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
    "url": "http://arxiv.org/abs/2504.01943v1",
    "arxiv_id": "2504.01943v1",
    "authors": [
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Somshubra Majumdar",
      "Aleksander Ficek",
      "Siddhartha Jain",
      "Jocelyn Huang",
      "Vahid Noroozi",
      "Boris Ginsburg"
    ],
    "published": "2025-04-02T17:50:31+00:00",
    "summary": "Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community."
  },
  {
    "title": "Overcoming Deceptiveness in Fitness Optimization with Unsupervised Quality-Diversity",
    "url": "http://arxiv.org/abs/2504.01915v1",
    "arxiv_id": "2504.01915v1",
    "authors": [
      "Lisa Coiffard",
      "Paul Templier",
      "Antoine Cully"
    ],
    "published": "2025-04-02T17:18:21+00:00",
    "summary": "Policy optimization seeks the best solution to a control problem according to an objective or fitness function, serving as a fundamental field of engineering and research with applications in robotics. Traditional optimization methods like reinforcement learning and evolutionary algorithms struggle with deceptive fitness landscapes, where following immediate improvements leads to suboptimal solutions. Quality-diversity (QD) algorithms offer a promising approach by maintaining diverse intermediate solutions as stepping stones for escaping local optima. However, QD algorithms require domain expertise to define hand-crafted features, limiting their applicability where characterizing solution diversity remains unclear. In this paper, we show that unsupervised QD algorithms - specifically the AURORA framework, which learns features from sensory data - efficiently solve deceptive optimization problems without domain expertise. By enhancing AURORA with contrastive learning and periodic extinction events, we propose AURORA-XCon, which outperforms all traditional optimization baselines and matches, in some cases even improving by up to 34%, the best QD baseline with domain-specific hand-crafted features. This work establishes a novel application of unsupervised QD algorithms, shifting their focus from discovering novel solutions toward traditional optimization and expanding their potential to domains where defining feature spaces poses challenges."
  },
  {
    "title": "Overcoming Deceptiveness in Fitness Optimization with Unsupervised Quality-Diversity",
    "url": "http://arxiv.org/abs/2504.01915v2",
    "arxiv_id": "2504.01915v2",
    "authors": [
      "Lisa Coiffard",
      "Paul Templier",
      "Antoine Cully"
    ],
    "published": "2025-04-02T17:18:21+00:00",
    "summary": "Policy optimization seeks the best solution to a control problem according to an objective or fitness function, serving as a fundamental field of engineering and research with applications in robotics. Traditional optimization methods like reinforcement learning and evolutionary algorithms struggle with deceptive fitness landscapes, where following immediate improvements leads to suboptimal solutions. Quality-diversity (QD) algorithms offer a promising approach by maintaining diverse intermediate solutions as stepping stones for escaping local optima. However, QD algorithms require domain expertise to define hand-crafted features, limiting their applicability where characterizing solution diversity remains unclear. In this paper, we show that unsupervised QD algorithms - specifically the AURORA framework, which learns features from sensory data - efficiently solve deceptive optimization problems without domain expertise. By enhancing AURORA with contrastive learning and periodic extinction events, we propose AURORA-XCon, which outperforms all traditional optimization baselines and matches, in some cases even improving by up to 34%, the best QD baseline with domain-specific hand-crafted features. This work establishes a novel application of unsupervised QD algorithms, shifting their focus from discovering novel solutions toward traditional optimization and expanding their potential to domains where defining feature spaces poses challenges."
  },
  {
    "title": "GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning",
    "url": "http://arxiv.org/abs/2504.01886v1",
    "arxiv_id": "2504.01886v1",
    "authors": [
      "Yanzhou Su",
      "Tianbin Li",
      "Jiyao Liu",
      "Chenglong Ma",
      "Junzhi Ning",
      "Cheng Tang",
      "Sibo Ju",
      "Jin Ye",
      "Pengcheng Chen",
      "Ming Hu",
      "Shixiang Tang",
      "Lihao Liu",
      "Bin Fu",
      "Wenqi Shao",
      "Xiaowei Hu",
      "Xiangwen Liao",
      "Yuanfeng Ji",
      "Junjun He"
    ],
    "published": "2025-04-02T16:43:16+00:00",
    "summary": "Recent advances in general medical AI have made significant strides, but existing models often lack the reasoning capabilities needed for complex medical decision-making. This paper presents GMAI-VL-R1, a multimodal medical reasoning model enhanced by reinforcement learning (RL) to improve its reasoning abilities. Through iterative training, GMAI-VL-R1 optimizes decision-making, significantly boosting diagnostic accuracy and clinical support. We also develop a reasoning data synthesis method, generating step-by-step reasoning data via rejection sampling, which further enhances the model's generalization. Experimental results show that after RL training, GMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question answering. While the model demonstrates basic memorization with supervised fine-tuning, RL is crucial for true generalization. Our work establishes new evaluation benchmarks and paves the way for future advancements in medical reasoning models. Code, data, and model will be released at \\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}."
  },
  {
    "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01871v1",
    "arxiv_id": "2504.01871v1",
    "authors": [
      "Thomas Bush",
      "Stephen Chung",
      "Usman Anwar",
      "Adri\u00e0 Garriga-Alonso",
      "David Krueger"
    ],
    "published": "2025-04-02T16:24:23+00:00",
    "summary": "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL"
  },
  {
    "title": "Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning",
    "url": "http://arxiv.org/abs/2504.01805v1",
    "arxiv_id": "2504.01805v1",
    "authors": [
      "Kun Ouyang"
    ],
    "published": "2025-04-02T15:12:17+00:00",
    "summary": "Enhancing the spatial reasoning capabilities of Multi-modal Large Language Models (MLLMs) for video understanding is crucial yet challenging. We present Spatial-R1, a targeted approach involving two key contributions: the curation of SR, a new video spatial reasoning dataset from ScanNet with automatically generated QA pairs across seven task types, and the application of Task-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By training the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1 significantly advances performance on the VSI-Bench benchmark, achieving a 7.4\\% gain over the baseline and outperforming strong contemporary models. This work validates the effectiveness of specialized data curation and optimization techniques for improving complex spatial reasoning in video MLLMs."
  },
  {
    "title": "Learning with Imperfect Models: When Multi-step Prediction Mitigates Compounding Error",
    "url": "http://arxiv.org/abs/2504.01766v1",
    "arxiv_id": "2504.01766v1",
    "authors": [
      "Anne Somalwar",
      "Bruce D. Lee",
      "George J. Pappas",
      "Nikolai Matni"
    ],
    "published": "2025-04-02T14:18:52+00:00",
    "summary": "Compounding error, where small prediction mistakes accumulate over time, presents a major challenge in learning-based control. For example, this issue often limits the performance of model-based reinforcement learning and imitation learning. One common approach to mitigate compounding error is to train multi-step predictors directly, rather than relying on autoregressive rollout of a single-step model. However, it is not well understood when the benefits of multi-step prediction outweigh the added complexity of learning a more complicated model. In this work, we provide a rigorous analysis of this trade-off in the context of linear dynamical systems. We show that when the model class is well-specified and accurately captures the system dynamics, single-step models achieve lower asymptotic prediction error. On the other hand, when the model class is misspecified due to partial observability, direct multi-step predictors can significantly reduce bias and thus outperform single-step approaches. These theoretical results are supported by numerical experiments, wherein we also (a) empirically evaluate an intermediate strategy which trains a single-step model using a multi-step loss and (b) evaluate performance of single step and multi-step predictors in a closed loop control setting."
  },
  {
    "title": "Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01719v1",
    "arxiv_id": "2504.01719v1",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Yao Li",
      "Xiaoyang Tan"
    ],
    "published": "2025-04-02T13:27:44+00:00",
    "summary": "We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage \\textit{distribution shift} while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with {\\it distribution shift} by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data."
  },
  {
    "title": "Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for Offline Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01719v2",
    "arxiv_id": "2504.01719v2",
    "authors": [
      "Ke Jiang",
      "Wen Jiang",
      "Yao Li",
      "Xiaoyang Tan"
    ],
    "published": "2025-04-02T13:27:44+00:00",
    "summary": "We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. Under such circumstance, the learned policy must be safe enough to manage distribution shift while maintaining sufficient flexibility to deal with non-expert (bad) demonstrations from offline data.To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy, hence reducing the negative impact of those bad demonstrations.To be specific, a new conservative reward mechanism is developed to deal with distribution shift by evaluating actions according to whether their outcomes meet safety requirements - remaining within the state support area, rather than solely depending on the actions' likelihood based on offline data.Besides theoretical justification, we provide empirical evidence on widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data."
  },
  {
    "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs",
    "url": "http://arxiv.org/abs/2504.01698v1",
    "arxiv_id": "2504.01698v1",
    "authors": [
      "Yi-Long Lu",
      "Chunhui Zhang",
      "Jiajun Song",
      "Lifeng Fan",
      "Wei Wang"
    ],
    "published": "2025-04-02T12:58:42+00:00",
    "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during the post-training phase of large language models (LLMs), have significantly enhanced their capabilities in structured reasoning tasks such as mathematics and logical inference. However, the effectiveness of RL in social reasoning, particularly in Theory of Mind (ToM), the ability to infer others' mental states, remains largely unexplored. In this study, we demonstrate that RL methods effectively unlock ToM reasoning capabilities even in small-scale LLMs (0.5B to 7B parameters). Using a modest dataset comprising 3200 questions across diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on the Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite significantly fewer parameters. While smaller models ($\\leq$3B parameters) suffer from reasoning collapse, larger models (7B parameters) maintain stable performance through consistent belief tracking. Additionally, our RL-based models demonstrate robust generalization to higher-order, out-of-distribution ToM problems, novel textual presentations, and previously unseen datasets. These findings highlight RL's potential to enhance social cognitive reasoning, bridging the gap between structured problem-solving and nuanced social inference in LLMs."
  },
  {
    "title": "A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown L\u00e9vy Process Dynamics",
    "url": "http://arxiv.org/abs/2504.01482v1",
    "arxiv_id": "2504.01482v1",
    "authors": [
      "Qihao Ye",
      "Xiaochuan Tian",
      "Yuhua Zhu"
    ],
    "published": "2025-04-02T08:37:14+00:00",
    "summary": "This paper develops a model-based framework for continuous-time policy evaluation (CTPE) in reinforcement learning, incorporating both Brownian and L\\'evy noise to model stochastic dynamics influenced by rare and extreme events. Our approach formulates the policy evaluation problem as solving a partial integro-differential equation (PIDE) for the value function with unknown coefficients. A key challenge in this setting is accurately recovering the unknown coefficients in the stochastic dynamics, particularly when driven by L\\'evy processes with heavy tail effects. To address this, we propose a robust numerical approach that effectively handles both unbiased and censored trajectory datasets. This method combines maximum likelihood estimation with an iterative tail correction mechanism, improving the stability and accuracy of coefficient recovery. Additionally, we establish a theoretical bound for the policy evaluation error based on coefficient recovery error. Through numerical experiments, we demonstrate the effectiveness and robustness of our method in recovering heavy-tailed L\\'evy dynamics and verify the theoretical error analysis in policy evaluation."
  },
  {
    "title": "Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01459v1",
    "arxiv_id": "2504.01459v1",
    "authors": [
      "Llewyn Salt",
      "Marcus Gallagher"
    ],
    "published": "2025-04-02T08:15:16+00:00",
    "summary": "Reinforcement learning (RL) -- algorithms that teach artificial agents to interact with environments by maximising reward signals -- has achieved significant success in recent years. These successes have been facilitated by advances in algorithms (e.g., deep Q-learning, deep deterministic policy gradients, proximal policy optimisation, trust region policy optimisation, and soft actor-critic) and specialised computational resources such as GPUs and TPUs. One promising research direction involves introducing goals to allow multimodal policies, commonly through hierarchical or curriculum reinforcement learning. These methods systematically decompose complex behaviours into simpler sub-tasks, analogous to how humans progressively learn skills (e.g. we learn to run before we walk, or we learn arithmetic before calculus). However, fully automating goal creation remains an open challenge. We present a novel probabilistic curriculum learning algorithm to suggest goals for reinforcement learning agents in continuous control and navigation tasks."
  },
  {
    "title": "Deep Graph Reinforcement Learning for UAV-Enabled Multi-User Secure Communications",
    "url": "http://arxiv.org/abs/2504.01446v1",
    "arxiv_id": "2504.01446v1",
    "authors": [
      "Xiao Tang",
      "Kexin Zhao",
      "Chao Shen",
      "Qinghe Du",
      "Yichen Wang",
      "Dusit Niyato",
      "Zhu Han"
    ],
    "published": "2025-04-02T07:57:33+00:00",
    "summary": "While unmanned aerial vehicles (UAVs) with flexible mobility are envisioned to enhance physical layer security in wireless communications, the efficient security design that adapts to such high network dynamics is rather challenging. The conventional approaches extended from optimization perspectives are usually quite involved, especially when jointly considering factors in different scales such as deployment and transmission in UAV-related scenarios. In this paper, we address the UAV-enabled multi-user secure communications by proposing a deep graph reinforcement learning framework. Specifically, we reinterpret the security beamforming as a graph neural network (GNN) learning task, where mutual interference among users is managed through the message-passing mechanism. Then, the UAV deployment is obtained through soft actor-critic reinforcement learning, where the GNN-based security beamforming is exploited to guide the deployment strategy update. Simulation results demonstrate that the proposed approach achieves near-optimal security performance and significantly enhances the efficiency of strategy determination. Moreover, the deep graph reinforcement learning framework offers a scalable solution, adaptable to various network scenarios and configurations, establishing a robust basis for information security in UAV-enabled communications."
  },
  {
    "title": "De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning",
    "url": "http://arxiv.org/abs/2504.01389v1",
    "arxiv_id": "2504.01389v1",
    "authors": [
      "Junyu Hou"
    ],
    "published": "2025-04-02T06:00:21+00:00",
    "summary": "De novo molecular design has extensive applications in drug discovery and materials science. The vast chemical space renders direct molecular searches computationally prohibitive, while traditional experimental screening is both time- and labor-intensive. Efficient molecular generation and screening methods are therefore essential for accelerating drug discovery and reducing costs. Although reinforcement learning (RL) has been applied to optimize molecular properties via reward mechanisms, its practical utility is limited by issues in training efficiency, convergence, and stability. To address these challenges, we adopt Direct Preference Optimization (DPO) from NLP, which uses molecular score-based sample pairs to maximize the likelihood difference between high- and low-quality molecules, effectively guiding the model toward better compounds. Moreover, integrating curriculum learning further boosts training efficiency and accelerates convergence. A systematic evaluation of the proposed method on the GuacaMol Benchmark yielded excellent scores. For instance, the method achieved a score of 0.883 on the Perindopril MPO task, representing a 6\\% improvement over competing models. And subsequent target protein binding experiments confirmed its practical efficacy. These results demonstrate the strong potential of DPO for molecular design tasks and highlight its effectiveness as a robust and efficient solution for data-driven drug discovery."
  },
  {
    "title": "Inverse RL Scene Dynamics Learning for Nonlinear Predictive Control in Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2504.01336v1",
    "arxiv_id": "2504.01336v1",
    "authors": [
      "Sorin Grigorescu",
      "Mihai Zaha"
    ],
    "published": "2025-04-02T03:46:37+00:00",
    "summary": "This paper introduces the Deep Learning-based Nonlinear Model Predictive Controller with Scene Dynamics (DL-NMPC-SD) method for autonomous navigation. DL-NMPC-SD uses an a-priori nominal vehicle model in combination with a scene dynamics model learned from temporal range sensing information. The scene dynamics model is responsible for estimating the desired vehicle trajectory, as well as to adjust the true system model used by the underlying model predictive controller. We propose to encode the scene dynamics model within the layers of a deep neural network, which acts as a nonlinear approximator for the high order state-space of the operating conditions. The model is learned based on temporal sequences of range sensing observations and system states, both integrated by an Augmented Memory component. We use Inverse Reinforcement Learning and the Bellman optimality principle to train our learning controller with a modified version of the Deep Q-Learning algorithm, enabling us to estimate the desired state trajectory as an optimal action-value function. We have evaluated DL-NMPC-SD against the baseline Dynamic Window Approach (DWA), as well as against two state-of-the-art End2End and reinforcement learning methods, respectively. The performance has been measured in three experiments: i) in our GridSim virtual environment, ii) on indoor and outdoor navigation tasks using our RovisLab AMTU (Autonomous Mobile Test Unit) platform and iii) on a full scale autonomous test vehicle driving on public roads."
  },
  {
    "title": "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.01296v1",
    "arxiv_id": "2504.01296v1",
    "authors": [
      "Bairu Hou",
      "Yang Zhang",
      "Jiabao Ji",
      "Yujian Liu",
      "Kaizhi Qian",
      "Jacob Andreas",
      "Shiyu Chang"
    ],
    "published": "2025-04-02T01:59:26+00:00",
    "summary": "We present ThinkPrune, a simple yet effective method for pruning the thinking length for long-thinking LLMs, which has been found to often produce inefficient and redundant thinking processes. Existing preliminary explorations of reducing thinking length primarily focus on forcing the thinking process to early exit, rather than adapting the LLM to optimize and consolidate the thinking process, and therefore the length-performance tradeoff observed so far is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that continuously trains the long-thinking LLMs via reinforcement learning (RL) with an added token limit, beyond which any unfinished thoughts and answers will be discarded, resulting in a zero reward. To further preserve model performance, we introduce an iterative length pruning approach, where multiple rounds of RL are conducted, each with an increasingly more stringent token limit. We observed that ThinkPrune results in a remarkable performance-length tradeoff -- on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B can be reduced by half with only 2% drop in performance. We also observed that after pruning, the LLMs can bypass unnecessary steps while keeping the core reasoning process complete. Code is available at https://github.com/UCSB-NLP-Chang/ThinkPrune."
  },
  {
    "title": "Value Iteration for Learning Concurrently Executable Robotic Control Tasks",
    "url": "http://arxiv.org/abs/2504.01174v1",
    "arxiv_id": "2504.01174v1",
    "authors": [
      "Sheikh A. Tahmid",
      "Gennaro Notomista"
    ],
    "published": "2025-04-01T20:31:36+00:00",
    "summary": "Many modern robotic systems such as multi-robot systems and manipulators exhibit redundancy, a property owing to which they are capable of executing multiple tasks. This work proposes a novel method, based on the Reinforcement Learning (RL) paradigm, to train redundant robots to be able to execute multiple tasks concurrently. Our approach differs from typical multi-objective RL methods insofar as the learned tasks can be combined and executed in possibly time-varying prioritized stacks. We do so by first defining a notion of task independence between learned value functions. We then use our definition of task independence to propose a cost functional that encourages a policy, based on an approximated value function, to accomplish its control objective while minimally interfering with the execution of higher priority tasks. This allows us to train a set of control policies that can be executed simultaneously. We also introduce a version of fitted value iteration to learn to approximate our proposed cost functional efficiently. We demonstrate our approach on several scenarios and robotic systems."
  },
  {
    "title": "Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER",
    "url": "http://arxiv.org/abs/2504.01165v1",
    "arxiv_id": "2504.01165v1",
    "authors": [
      "Rui Zong",
      "Martin Liang",
      "Yuntian Fang",
      "Ke Wang",
      "Xiaoshuai Chen",
      "Wei Chen",
      "Petar Kormushev"
    ],
    "published": "2025-04-01T20:04:49+00:00",
    "summary": "Knee-less bipedal robots like SLIDER have the advantage of ultra-lightweight legs and improved walking energy efficiency compared to traditional humanoid robots. In this paper, we firstly introduce an improved hardware design of the bipedal robot SLIDER with new line-feet and more optimized mass distribution which enables higher locomotion speeds. Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER. The eHZD method is then used to generate a library of gaits with varying reference velocities in an offline way. Thirdly, a Guided Deep Reinforcement Learning (DRL) algorithm is proposed to use the pre-generated library to create walking control policies in real-time. This approach allows us to combine the advantages of both HZD (for generating stable gaits with a full-dynamics model) and DRL (for real-time adaptive gait generation). The experimental results show that this approach achieves 150% higher walking velocity than the previous MPC-based approach."
  },
  {
    "title": "Active Learning Design: Modeling Force Output for Axisymmetric Soft Pneumatic Actuators",
    "url": "http://arxiv.org/abs/2504.01156v1",
    "arxiv_id": "2504.01156v1",
    "authors": [
      "Gregory M. Campbell",
      "Gentian Muhaxheri",
      "Leonardo Ferreira Guilhoto",
      "Christian D. Santangelo",
      "Paris Perdikaris",
      "James Pikul",
      "Mark Yim"
    ],
    "published": "2025-04-01T19:43:00+00:00",
    "summary": "Soft pneumatic actuators (SPA) made from elastomeric materials can provide large strain and large force. The behavior of locally strain-restricted hyperelastic materials under inflation has been investigated thoroughly for shape reconfiguration, but requires further investigation for trajectories involving external force. In this work we model force-pressure-height relationships for a concentrically strain-limited class of soft pneumatic actuators and demonstrate the use of this model to design SPA response for object lifting. We predict relationships under different loadings by solving energy minimization equations and verify this theory by using an automated test rig to collect rich data for n=22 Ecoflex 00-30 membranes. We collect this data using an active learning pipeline to efficiently model the design space. We show that this learned material model outperforms the theory-based model and naive curve-fitting approaches. We use our model to optimize membrane design for different lift tasks and compare this performance to other designs. These contributions represent a step towards understanding the natural response for this class of actuator and embodying intelligent lifts in a single-pressure input actuator system."
  },
  {
    "title": "HomeEmergency -- Using Audio to Find and Respond to Emergencies in the Home",
    "url": "http://arxiv.org/abs/2504.01089v1",
    "arxiv_id": "2504.01089v1",
    "authors": [
      "James F. Mullen Jr",
      "Dhruva Kumar",
      "Xuewei Qi",
      "Rajasimman Madhivanan",
      "Arnie Sen",
      "Dinesh Manocha",
      "Richard Kim"
    ],
    "published": "2025-04-01T18:07:25+00:00",
    "summary": "In the United States alone accidental home deaths exceed 128,000 per year. Our work aims to enable home robots who respond to emergency scenarios in the home, preventing injuries and deaths. We introduce a new dataset of household emergencies based in the ThreeDWorld simulator. Each scenario in our dataset begins with an instantaneous or periodic sound which may or may not be an emergency. The agent must navigate the multi-room home scene using prior observations, alongside audio signals and images from the simulator, to determine if there is an emergency or not.   In addition to our new dataset, we present a modular approach for localizing and identifying potential home emergencies. Underpinning our approach is a novel probabilistic dynamic scene graph (P-DSG), where our key insight is that graph nodes corresponding to agents can be represented with a probabilistic edge. This edge, when refined using Bayesian inference, enables efficient and effective localization of agents in the scene. We also utilize multi-modal vision-language models (VLMs) as a component in our approach, determining object traits (e.g. flammability) and identifying emergencies. We present a demonstration of our method completing a real-world version of our task on a consumer robot, showing the transferability of both our task and our method. Our dataset will be released to the public upon this papers publication."
  },
  {
    "title": "MPCritic: A plug-and-play MPC architecture for reinforcement learning",
    "url": "http://arxiv.org/abs/2504.01086v1",
    "arxiv_id": "2504.01086v1",
    "authors": [
      "Nathan P. Lawrence",
      "Thomas Banker",
      "Ali Mesbah"
    ],
    "published": "2025-04-01T18:07:07+00:00",
    "summary": "The reinforcement learning (RL) and model predictive control (MPC) communities have developed vast ecosystems of theoretical approaches and computational tools for solving optimal control problems. Given their conceptual similarities but differing strengths, there has been increasing interest in synergizing RL and MPC. However, existing approaches tend to be limited for various reasons, including computational cost of MPC in an RL algorithm and software hurdles towards seamless integration of MPC and RL tools. These challenges often result in the use of \"simple\" MPC schemes or RL algorithms, neglecting the state-of-the-art in both areas. This paper presents MPCritic, a machine learning-friendly architecture that interfaces seamlessly with MPC tools. MPCritic utilizes the loss landscape defined by a parameterized MPC problem, focusing on \"soft\" optimization over batched training steps; thereby updating the MPC parameters while avoiding costly minimization and parametric sensitivities. Since the MPC structure is preserved during training, an MPC agent can be readily used for online deployment, where robust constraint satisfaction is paramount. We demonstrate the versatility of MPCritic, in terms of MPC architectures and RL algorithms that it can accommodate, on classic control benchmarks."
  },
  {
    "title": "A Parametric Model for Near-Optimal Online Synthesis with Robust Reach-Avoid Guarantees",
    "url": "http://arxiv.org/abs/2504.01006v1",
    "arxiv_id": "2504.01006v1",
    "authors": [
      "Mario Gleirscher",
      "Philip H\u00f6nnecke"
    ],
    "published": "2025-04-01T17:45:50+00:00",
    "summary": "Objective: To obtain explainable guarantees in the online synthesis of optimal controllers for high-integrity cyber-physical systems, we re-investigate the use of exhaustive search as an alternative to reinforcement learning. Approach: We model an application scenario as a hybrid game automaton, enabling the synthesis of robustly correct and near-optimal controllers online without prior training. For modal synthesis, we employ discretised games solved via scope-adaptive and step-pre-shielded discrete dynamic programming. Evaluation: In a simulation-based experiment, we apply our approach to an autonomous aerial vehicle scenario. Contribution: We propose a parametric system model and a parametric online synthesis."
  },
  {
    "title": "Resource Allocation for RIS-Assisted CoMP-NOMA Networks using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00975v1",
    "arxiv_id": "2504.00975v1",
    "authors": [
      "Muhammad Umer",
      "Muhammad Ahmed Mohsin",
      "Huma Ghafoor",
      "Syed Ali Hassan"
    ],
    "published": "2025-04-01T17:14:01+00:00",
    "summary": "This thesis delves into the forefront of wireless communication by exploring the synergistic integration of three transformative technologies: STAR-RIS, CoMP, and NOMA. Driven by the ever-increasing demand for higher data rates, improved spectral efficiency, and expanded coverage in the evolving landscape of 6G development, this research investigates the potential of these technologies to revolutionize future wireless networks.   The thesis analyzes the performance gains achievable through strategic deployment of STAR-RIS, focusing on mitigating inter-cell interference, enhancing signal strength, and extending coverage to cell-edge users. Resource sharing strategies for STAR-RIS elements are explored, optimizing both transmission and reflection functionalities. Analytical frameworks are developed to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks under realistic channel conditions, deriving key performance metrics such as ergodic rates and outage probabilities. Additionally, the research delves into energy-efficient design approaches for CoMP-NOMA networks incorporating RIS, proposing novel RIS configurations and optimization algorithms to achieve a balance between performance and energy consumption. Furthermore, the application of Deep Reinforcement Learning (DRL) techniques for intelligent and adaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored, aiming to maximize network sum rate while meeting user quality of service requirements. Through a comprehensive investigation of these technologies and their synergistic potential, this thesis contributes valuable insights into the future of wireless communication, paving the way for the development of more efficient, reliable, and sustainable networks capable of meeting the demands of our increasingly connected world."
  },
  {
    "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00907v2",
    "arxiv_id": "2504.00907v2",
    "authors": [
      "Ram Ramrakhya",
      "Matthew Chang",
      "Xavier Puig",
      "Ruta Desai",
      "Zsolt Kira",
      "Roozbeh Mottaghi"
    ],
    "published": "2025-04-01T15:41:50+00:00",
    "summary": "Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin ($19.1$-$40.3\\%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL."
  },
  {
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "url": "http://arxiv.org/abs/2504.00883v1",
    "arxiv_id": "2504.00883v1",
    "authors": [
      "Zhenyi Liao",
      "Qingsong Xie",
      "Yanhao Zhang",
      "Zijian Kong",
      "Haonan Lu",
      "Zhenyu Yang",
      "Zhijie Deng"
    ],
    "published": "2025-04-01T15:11:11+00:00",
    "summary": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon."
  },
  {
    "title": "PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks",
    "url": "http://arxiv.org/abs/2504.00844v1",
    "arxiv_id": "2504.00844v1",
    "authors": [
      "Abdelrahman Elskhawy",
      "Mengze Li",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "published": "2025-04-01T14:29:51+00:00",
    "summary": "In Scene Graphs Generation (SGG) one extracts structured representation from visual inputs in the form of objects nodes and predicates connecting them. This facilitates image-based understanding and reasoning for various downstream tasks. Although fully supervised SGG approaches showed steady performance improvements, they suffer from a severe training bias. This is caused by the availability of only small subsets of curated data and exhibits long-tail predicate distribution issues with a lack of predicate diversity adversely affecting downstream tasks. To overcome this, we introduce PRISM-0, a framework for zero-shot open-vocabulary SGG that bootstraps foundation models in a bottom-up approach to capture the whole spectrum of diverse, open-vocabulary predicate prediction. Detected object pairs are filtered and passed to a Vision Language Model (VLM) that generates descriptive captions. These are used to prompt an LLM to generate fine-andcoarse-grained predicates for the pair. The predicates are then validated using a VQA model to provide a final SGG. With the modular and dataset-independent PRISM-0, we can enrich existing SG datasets such as Visual Genome (VG). Experiments illustrate that PRIMS-0 generates semantically meaningful graphs that improve downstream tasks such as Image Captioning and Sentence-to-Graph Retrieval with a performance on par to the best fully supervised methods."
  },
  {
    "title": "How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study",
    "url": "http://arxiv.org/abs/2504.00829v1",
    "arxiv_id": "2504.00829v1",
    "authors": [
      "Yunjie Ji",
      "Sitong Zhao",
      "Xiaoyu Tian",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-04-01T14:18:38+00:00",
    "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) with efficiency and scalability remains a fundamental challenge in artificial intelligence research. This paper presents a rigorous experimental investigation into how difficulty-aware staged reinforcement learning (RL) strategies can substantially improve LLM reasoning performance. Through systematic analysis, we demonstrate that strategically selecting training data according to well-defined difficulty levels markedly enhances RL optimization. Moreover, we introduce a staged training methodology, progressively exposing models to increasingly challenging tasks, further amplifying reasoning capabilities. Our findings reveal significant cross-domain benefits when simultaneously training models on mathematical reasoning and code generation tasks. Notably, our proposed approach enables a 1.5B parameter model to achieve an accuracy of 42.3\\% on the AIME-2024 benchmark, 89.5\\% on the MATH-500 benchmark. These results underscore the efficacy of our method in advancing the reasoning proficiency of LLMs. We will open-source our datasets on GitHub and Hugging Face."
  },
  {
    "title": "Visual Environment-Interactive Planning for Embodied Complex-Question Answering",
    "url": "http://arxiv.org/abs/2504.00775v1",
    "arxiv_id": "2504.00775v1",
    "authors": [
      "Ning Lan",
      "Baoshan Ou",
      "Xuemei Xie",
      "Guangming Shi"
    ],
    "published": "2025-04-01T13:26:28+00:00",
    "summary": "This study focuses on Embodied Complex-Question Answering task, which means the embodied robot need to understand human questions with intricate structures and abstract semantics. The core of this task lies in making appropriate plans based on the perception of the visual environment. Existing methods often generate plans in a once-for-all manner, i.e., one-step planning. Such approach rely on large models, without sufficient understanding of the environment. Considering multi-step planning, the framework for formulating plans in a sequential manner is proposed in this paper. To ensure the ability of our framework to tackle complex questions, we create a structured semantic space, where hierarchical visual perception and chain expression of the question essence can achieve iterative interaction. This space makes sequential task planning possible. Within the framework, we first parse human natural language based on a visual hierarchical scene graph, which can clarify the intention of the question. Then, we incorporate external rules to make a plan for current step, weakening the reliance on large models. Every plan is generated based on feedback from visual perception, with multiple rounds of interaction until an answer is obtained. This approach enables continuous feedback and adjustment, allowing the robot to optimize its action strategy. To test our framework, we contribute a new dataset with more complex questions. Experimental results demonstrate that our approach performs excellently and stably on complex tasks. And also, the feasibility of our approach in real-world scenarios has been established, indicating its practical applicability."
  },
  {
    "title": "Reinforcement learning for robust dynamic metabolic control",
    "url": "http://arxiv.org/abs/2504.00735v1",
    "arxiv_id": "2504.00735v1",
    "authors": [
      "Sebasti\u00e1n Espinel-R\u00edos",
      "River Walser",
      "Dongda Zhang"
    ],
    "published": "2025-04-01T12:47:05+00:00",
    "summary": "Dynamic metabolic control can enhance bioprocess flexibility and expand the available optimization degrees of freedom via real-time modulation of metabolic enzyme expression. This allows target metabolic fluxes to be dynamically tuned throughout the process. However, identifying optimal dynamic control policies is challenging due to the presence of potential metabolic burden, cytotoxic effects, and the generally high-dimensional solution space, making exhaustive experimentation impractical. Here, we propose an approach based on reinforcement learning to derive optimal dynamic metabolic control policies by allowing an agent or controller to interact with a surrogate dynamic model $\\textit{in silico}$. To incorporate and test robustness, we apply domain randomization, enabling the controller to generalize across system uncertainties. Our approach provides an alternative to conventional model-based control such as model predictive control, which requires differentiating the models with respect to decision variables; an often impractical task when dealing with complex stochastic, nonlinear, stiff, or piecewise-defined dynamics. In contrast, our approach only requires forward integration, making the task computationally much simpler with off-the-shelf solvers. We demonstrate our approach with a case study on the dynamic control of acetyl-CoA carboxylase in $\\textit{Escherichia coli}$ for fatty acid biosynthesis. The derived dynamic metabolic control policies outperform static control, achieving up to 40 % higher titers while remaining robust under uncertainty."
  },
  {
    "title": "Immersive Explainability: Visualizing Robot Navigation Decisions through XAI Semantic Scene Projections in Virtual Reality",
    "url": "http://arxiv.org/abs/2504.00682v1",
    "arxiv_id": "2504.00682v1",
    "authors": [
      "Jorge de Heuvel",
      "Sebastian M\u00fcller",
      "Marlene Wessels",
      "Aftab Akhtar",
      "Christian Bauckhage",
      "Maren Bennewitz"
    ],
    "published": "2025-04-01T11:52:39+00:00",
    "summary": "End-to-end robot policies achieve high performance through neural networks trained via reinforcement learning (RL). Yet, their black box nature and abstract reasoning pose challenges for human-robot interaction (HRI), because humans may experience difficulty in understanding and predicting the robot's navigation decisions, hindering trust development. We present a virtual reality (VR) interface that visualizes explainable AI (XAI) outputs and the robot's lidar perception to support intuitive interpretation of RL-based navigation behavior. By visually highlighting objects based on their attribution scores, the interface grounds abstract policy explanations in the scene context. This XAI visualization bridges the gap between obscure numerical XAI attribution scores and a human-centric semantic level of explanation. A within-subjects study with 24 participants evaluated the effectiveness of our interface for four visualization conditions combining XAI and lidar. Participants ranked scene objects across navigation scenarios based on their importance to the robot, followed by a questionnaire assessing subjective understanding and predictability. Results show that semantic projection of attributions significantly enhances non-expert users' objective understanding and subjective awareness of robot behavior. In addition, lidar visualization further improves perceived predictability, underscoring the value of integrating XAI and sensor for transparent, trustworthy HRI."
  },
  {
    "title": "Probabilistically safe and efficient model-based Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00626v1",
    "arxiv_id": "2504.00626v1",
    "authors": [
      "Filippo Airaldi",
      "Bart De Schutter",
      "Azita Dabiri"
    ],
    "published": "2025-04-01T10:24:54+00:00",
    "summary": "This paper proposes tackling safety-critical stochastic Reinforcement Learning (RL) tasks with a samplebased, model-based approach. At the core of the method lies a Model Predictive Control (MPC) scheme that acts as function approximation, providing a model-based predictive control policy. To ensure safety, a probabilistic Control Barrier Function (CBF) is integrated into the MPC controller. A sample-based approach with guarantees is employed to approximate the effects of stochasticies in the optimal control formulation and to guarantee the probabilistic CBF condition. A learnable terminal cost formulation is included in the MPC objective to counterbalance the additional computational burden due to sampling. An RL algorithm is deployed to learn both the terminal cost and the CBF constraint. Results from our numerical experiment on a constrained LTI problem corroborate the effectiveness of the proposed methodology in reducing computation time while preserving control performance and safety."
  },
  {
    "title": "Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs",
    "url": "http://arxiv.org/abs/2504.00614v1",
    "arxiv_id": "2504.00614v1",
    "authors": [
      "Sotaro Katayama",
      "Yuta Koda",
      "Norio Nagatsuka",
      "Masaya Kinoshita"
    ],
    "published": "2025-04-01T10:11:55+00:00",
    "summary": "Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains. Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillation to enhance bipedal locomotion learning over rough terrain. We validate our approach through hardware experiments on a miniature-sized humanoid EVAL-03 over a variety of environments. The experimental results demonstrate that our method improves rapid stabilization capabilities over non-rigid surfaces and sudden environmental transitions."
  },
  {
    "title": "Efficient and Sustainable Task Offloading in UAV-Assisted MEC Systems via Meta Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00453v1",
    "arxiv_id": "2504.00453v1",
    "authors": [
      "Maryam Farajzadeh Dehkordi",
      "Bijan Jabbari"
    ],
    "published": "2025-04-01T06:18:37+00:00",
    "summary": "Integrated into existing Mobile Edge Computing (MEC) systems, Unmanned Aerial Vehicles (UAVs) serve as a cornerstone in meeting the stringent requirements of future Internet of Things (IoT) networks. The current endeavor studies an MEC system, in which a computationally-empowered UAV, wirelessly linked to a cloud server, is destined for task offloading in uplink transmission of IoT devices. The performance of this system is studied by formulating a resource allocation problem, which aims to maximize the long-term computed task efficiency, while ensuring the stability of task buffers at the IoT devices, UAV and cloud. The problem jointly optimizes the uplink transmit power of IoT devices and their offloading decisions, the trajectory of the UAV and computing power at all transceivers. Regarding the non-convex and stochastic nature of the problem, we devise a multi-step solution approach. Initially, by invoking the fractional programming and Lyapunov theory, we transform the long-term optimization problem into an equivalent per-time-slot form. Subsequently, we recast the reformulated problem as a Markov Decision Process (MDP), which reflects the network dynamics. The MDP model, eventually, serves for training a Meta Twin Delayed Deep Deterministic Policy Gradient (MTD3) agent, in charge of adaptive resource allocation with respect to the MEC system variations derived from the mobility of the UAV and IoT devices. Simulations reveal the dominance of our proposed resource allocation approach over its Deep Reinforcement Learning (DRL)-powered counterparts, increasing computed task efficiency and reducing task buffer lengths."
  },
  {
    "title": "Hawkeye:Efficient Reasoning with Model Collaboration",
    "url": "http://arxiv.org/abs/2504.00424v1",
    "arxiv_id": "2504.00424v1",
    "authors": [
      "Jianshu She",
      "Zhuohao Li",
      "Zhemin Huang",
      "Qi Li",
      "Peiran Xu",
      "Haonan Li",
      "Qirong Ho"
    ],
    "published": "2025-04-01T05:09:04+00:00",
    "summary": "Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to the generation of excessive intermediate reasoning tokens, which introduce semantic redundancy and overly detailed reasoning steps. Moreover, computational expense and latency are significant concerns, as the cost scales with the number of output tokens, including those intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel post-training and inference framework where a large model produces concise CoT instructions to guide a smaller model in response generation. HAWKEYE quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able to expand responses while reducing token usage and computational cost significantly. Our evaluation shows that HAWKEYE can achieve comparable response quality using only 35% of the full CoTs, while improving clarity, coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can accelerate end-to-end reasoning by up to 3.4x on complex math tasks while reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the models will be available soon."
  },
  {
    "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding",
    "url": "http://arxiv.org/abs/2504.00409v1",
    "arxiv_id": "2504.00409v1",
    "authors": [
      "Mohanakrishnan Hariharan"
    ],
    "published": "2025-04-01T04:12:04+00:00",
    "summary": "Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding."
  },
  {
    "title": "VNJPTranslate: A comprehensive pipeline for Vietnamese-Japanese translation",
    "url": "http://arxiv.org/abs/2504.00339v1",
    "arxiv_id": "2504.00339v1",
    "authors": [
      "Hoang Hai Phan",
      "Nguyen Duc Minh Vu",
      "Nam Dang Phuong"
    ],
    "published": "2025-04-01T01:38:25+00:00",
    "summary": "Neural Machine Translation (NMT) driven by Transformer architectures has advanced significantly, yet faces challenges with low-resource language pairs like Vietnamese-Japanese (Vi-Ja). Issues include sparse parallel data and handling linguistic/cultural nuances. Recent progress in Large Language Models (LLMs) with strong reasoning, often refined via Reinforcement Learning (RL), enables high-quality synthetic data generation. We introduce VNJPTranslate, a pipeline designed to systematically address the Vi-Ja translation task. It features a targeted data augmentation strategy using advanced LLMs with Chain-of-Thought prompting for challenging segments identified via corpus analysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth with QLoRA) on a capable, low-parameter autoregressive model (specifically, a fine-tuned version of the 1.8B parameter Sailor model, which is based on the Qwen architecture) to create a practical and high-performing translation system. This integrated approach aims to improve Vi-Ja translation quality significantly over existing baselines."
  },
  {
    "title": "Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies",
    "url": "http://arxiv.org/abs/2504.00280v1",
    "arxiv_id": "2504.00280v1",
    "authors": [
      "Gunbir Singh Baveja"
    ],
    "published": "2025-03-31T23:00:07+00:00",
    "summary": "This paper investigates the application of Diffusion Policy in non-stationary, vision-based RL settings, specifically targeting environments where task dynamics and objectives evolve over time. Our work is grounded in practical challenges encountered in dynamic real-world scenarios such as robotics assembly lines and autonomous navigation, where agents must adapt control strategies from high-dimensional visual inputs. We apply Diffusion Policy -- which leverages iterative stochastic denoising to refine latent action representations-to benchmark environments including Procgen and PointMaze. Our experiments demonstrate that, despite increased computational demands, Diffusion Policy consistently outperforms standard RL methods such as PPO and DQN, achieving higher mean and maximum rewards with reduced variability. These findings underscore the approach's capability to generate coherent, contextually relevant action sequences in continuously shifting conditions, while also highlighting areas for further improvement in handling extreme non-stationarity."
  },
  {
    "title": "Rack Position Optimization in Large-Scale Heterogeneous Data Centers",
    "url": "http://arxiv.org/abs/2504.00277v1",
    "arxiv_id": "2504.00277v1",
    "authors": [
      "Chang-Lin Chen",
      "Jiayu Chen",
      "Tian Lan",
      "Zhaoxia Zhao",
      "Hongbo Dong",
      "Vaneet Aggarwal"
    ],
    "published": "2025-03-31T22:55:37+00:00",
    "summary": "As rapidly growing AI computational demands accelerate the need for new hardware installation and maintenance, this work explores optimal data center resource management by balancing operational efficiency with fault tolerance through strategic rack positioning considering diverse resources and locations. Traditional mixed-integer programming (MIP) approaches often struggle with scalability, while heuristic methods may result in significant sub-optimality. To address these issues, this paper presents a novel two-tier optimization framework using a high-level deep reinforcement learning (DRL) model to guide a low-level gradient-based heuristic for local search. The high-level DRL agent employs Leader Reward for optimal rack type ordering, and the low-level heuristic efficiently maps racks to positions, minimizing movement counts and ensuring fault-tolerant resource distribution. This approach allows scalability to over 100,000 positions and 100 rack types. Our method outperformed the gradient-based heuristic by 7\\% on average and the MIP solver by over 30\\% in objective value. It achieved a 100\\% success rate versus MIP's 97.5\\% (within a 20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes (i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which showed performance variability under time constraints and high penalties, our algorithm consistently delivered stable, efficient results - an essential feature for large-scale data center management."
  },
  {
    "title": "Nuclear Microreactor Control with Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2504.00156v1",
    "arxiv_id": "2504.00156v1",
    "authors": [
      "Leo Tunkle",
      "Kamal Abdulraheem",
      "Linyu Lin",
      "Majdi I. Radaideh"
    ],
    "published": "2025-03-31T19:11:19+00:00",
    "summary": "The economic feasibility of nuclear microreactors will depend on minimizing operating costs through advancements in autonomous control, especially when these microreactors are operating alongside other types of energy systems (e.g., renewable energy). This study explores the application of deep reinforcement learning (RL) for real-time drum control in microreactors, exploring performance in regard to load-following scenarios. By leveraging a point kinetics model with thermal and xenon feedback, we first establish a baseline using a single-output RL agent, then compare it against a traditional proportional-integral-derivative (PID) controller. This study demonstrates that RL controllers, including both single- and multi-agent RL (MARL) frameworks, can achieve similar or even superior load-following performance as traditional PID control across a range of load-following scenarios. In short transients, the RL agent was able to reduce the tracking error rate in comparison to PID. Over extended 300-minute load-following scenarios in which xenon feedback becomes a dominant factor, PID maintained better accuracy, but RL still remained within a 1% error margin despite being trained only on short-duration scenarios. This highlights RL's strong ability to generalize and extrapolate to longer, more complex transients, affording substantial reductions in training costs and reduced overfitting. Furthermore, when control was extended to multiple drums, MARL enabled independent drum control as well as maintained reactor symmetry constraints without sacrificing performance -- an objective that standard single-agent RL could not learn. We also found that, as increasing levels of Gaussian noise were added to the power measurements, the RL controllers were able to maintain lower error rates than PID, and to do so with less control effort."
  },
  {
    "title": "Topological Symmetry Breaking in Antagonistic Dynamics",
    "url": "http://arxiv.org/abs/2504.00144v1",
    "arxiv_id": "2504.00144v1",
    "authors": [
      "Giulio Iannelli",
      "Pablo Villegas",
      "Tommaso Gili",
      "Andrea Gabrielli"
    ],
    "published": "2025-03-31T18:52:11+00:00",
    "summary": "A dynamic concordia discors, a finely tuned equilibrium between opposing forces, is hypothesized to drive historical transformations. Similarly, a precise interplay of excitation and inhibition, the 80:20 ratio, is at the basis of the normal functionality of neural systems. In artificial neural networks, reinforcement learning allows for fine-tuning internal signed connections, optimizing adaptive responses to complex stimuli, and ensuring robust performance. At present, engineered structures of competing components are, however, largely unexplored, particularly because their emergent phases are closely linked with frustration mechanisms in the hosting network. In this context, the spin glass theory has shown how an apparently uncontrollable non-ergodic chaotic behavior arises from the complex interplay of competing interactions and frustration among units, leading to multiple metastable states preventing the system from exploring all accessible configurations over time. Here, we tackle the problem of disentangling topology and dynamics in systems with antagonistic interactions. We make use of the signed Laplacian operator to demonstrate how fundamental topological defects in lattices and networks percolate, shaping the geometrical arena and complex energy landscape of the system. This unveils novel, highly robust multistable phases and establishes deep connections with spin glasses when thermal noise is considered, providing a natural topological and algebraic description of their still-unknown set of pure states at zero temperature."
  },
  {
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
    "url": "http://arxiv.org/abs/2503.24376v1",
    "arxiv_id": "2503.24376v1",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Rui Wang",
      "Yixiao Ge",
      "Lu Qiu",
      "Ying Shan",
      "Xihui Liu"
    ],
    "published": "2025-03-31T17:55:23+00:00",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals."
  },
  {
    "title": "Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24296v1",
    "arxiv_id": "2503.24296v1",
    "authors": [
      "Yubo Zhang",
      "Pedro Botelho",
      "Trevor Gordon",
      "Gil Zussman",
      "Igor Kadota"
    ],
    "published": "2025-03-31T16:42:11+00:00",
    "summary": "We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average."
  },
  {
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "url": "http://arxiv.org/abs/2503.24290v1",
    "arxiv_id": "2503.24290v1",
    "authors": [
      "Jingcheng Hu",
      "Yinmin Zhang",
      "Qi Han",
      "Daxin Jiang",
      "Xiangyu Zhang",
      "Heung-Yeung Shum"
    ],
    "published": "2025-03-31T16:36:05+00:00",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes."
  },
  {
    "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24289v1",
    "arxiv_id": "2503.24289v1",
    "authors": [
      "Jiacheng Lin",
      "Tian Wang",
      "Kun Qian"
    ],
    "published": "2025-03-31T16:36:00+00:00",
    "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting."
  },
  {
    "title": "Moving Edge for On-Demand Edge Computing: An Uncertainty-aware Approach",
    "url": "http://arxiv.org/abs/2503.24214v1",
    "arxiv_id": "2503.24214v1",
    "authors": [
      "Fangtong Zhou",
      "Ruozhou Yu"
    ],
    "published": "2025-03-31T15:32:05+00:00",
    "summary": "We study an edge demand response problem where, based on historical edge workload demands, an edge provider needs to dispatch moving computing units, e.g. truck-carried modular data centers, in response to emerging hotspots within service area. The goal of edge provider is to maximize the expected revenue brought by serving congested users with satisfactory performance, while minimizing the costs of moving units and the potential service-level agreement violation penalty for interrupted services. The challenge is to make robust predictions for future demands, as well as optimized moving unit dispatching decisions. We propose a learning-based, uncertain-aware moving unit scheduling framework, URANUS, to address this problem. Our framework novelly combines Bayesian deep learning and distributionally robust approximation to make predictions that are robust to data, model and distributional uncertainties in deep learning-based prediction models. Based on the robust prediction outputs, we further propose an efficient planning algorithm to optimize moving unit scheduling in an online manner. Simulation experiments show that URANUS can significantly improve robustness in decision making, and achieve superior performance compared to state-of-the-art reinforcement learning, uncertainty-agnostic learning-based methods, and other baselines."
  },
  {
    "title": "Ride-Sourcing Vehicle Rebalancing with Service Accessibility Guarantees via Constrained Mean-Field Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24183v1",
    "arxiv_id": "2503.24183v1",
    "authors": [
      "Matej Jusup",
      "Kenan Zhang",
      "Zhiyuan Hu",
      "Barna P\u00e1sztor",
      "Andreas Krause",
      "Francesco Corman"
    ],
    "published": "2025-03-31T15:00:11+00:00",
    "summary": "The rapid expansion of ride-sourcing services such as Uber, Lyft, and Didi Chuxing has fundamentally reshaped urban transportation by offering flexible, on-demand mobility via mobile applications. Despite their convenience, these platforms confront significant operational challenges, particularly vehicle rebalancing - the strategic repositioning of thousands of vehicles to address spatiotemporal mismatches in supply and demand. Inadequate rebalancing results in prolonged rider waiting times, inefficient vehicle utilization, and inequitable distribution of services, leading to disparities in driver availability and income.   To tackle these complexities, we introduce scalable continuous-state mean-field control (MFC) and reinforcement learning (MFRL) models that explicitly represent each vehicle's precise location and employ continuous repositioning actions guided by the distribution of other vehicles. To ensure equitable service distribution, an accessibility constraint is integrated within our optimal control formulation, balancing operational efficiency with equitable access to the service across geographic regions. Our approach acknowledges realistic conditions, including inherent stochasticity in transitions, the simultaneous occurrence of vehicle-rider matching, vehicles' rebalancing and cruising, and variability in rider behaviors. Crucially, we relax the traditional mean-field assumption of equal supply-demand volume, better reflecting practical scenarios. Extensive empirical evaluation using real-world data-driven simulation of Shenzhen demonstrates the real-time efficiency and robustness of our approach at the scale of tens of thousands of vehicles.   The code is available at https://github.com/mjusup1501/mf-vehicle-rebalancing."
  },
  {
    "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings",
    "url": "http://arxiv.org/abs/2503.24150v1",
    "arxiv_id": "2503.24150v1",
    "authors": [
      "Kailas Vodrahalli",
      "Wei Wei",
      "James Zou"
    ],
    "published": "2025-03-31T14:35:48+00:00",
    "summary": "Recent advances in generative AI have been driven by alignment techniques such as reinforcement learning from human feedback (RLHF). RLHF and related techniques typically involve constructing a dataset of binary or ranked choice human preferences and subsequently fine-tuning models to align with these preferences. This paper shifts the focus to understanding the preferences encoded in such datasets and identifying common human preferences. We find that a small subset of 21 preference categories (selected from a set of nearly 5,000 distinct preferences) captures >89% of preference variation across individuals. This small set of preferences is analogous to a canonical basis of human preferences, similar to established findings that characterize human variation in psychology or facial recognition studies. Through both synthetic and empirical evaluations, we confirm that our low-rank, canonical set of human preferences generalizes across the entire dataset and within specific topics. We further demonstrate our preference basis' utility in model evaluation, where our preference categories offer deeper insights into model alignment, and in model training, where we show that fine-tuning on preference-defined subsets successfully aligns the model accordingly."
  },
  {
    "title": "Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral Vessels in Mechanical Thrombectomy",
    "url": "http://arxiv.org/abs/2503.24140v1",
    "arxiv_id": "2503.24140v1",
    "authors": [
      "Harry Robertshaw",
      "Benjamin Jackson",
      "Jiaheng Wang",
      "Hadi Sadati",
      "Lennart Karstensen",
      "Alejandro Granados",
      "Thomas C Booth"
    ],
    "published": "2025-03-31T14:25:46+00:00",
    "summary": "Purpose: Autonomous systems in mechanical thrombectomy (MT) hold promise for reducing procedure times, minimizing radiation exposure, and enhancing patient safety. However, current reinforcement learning (RL) methods only reach the carotid arteries, are not generalizable to other patient vasculatures, and do not consider safety. We propose a safe dual-device RL algorithm that can navigate beyond the carotid arteries to cerebral vessels.   Methods: We used the Simulation Open Framework Architecture to represent the intricacies of cerebral vessels, and a modified Soft Actor-Critic RL algorithm to learn, for the first time, the navigation of micro-catheters and micro-guidewires. We incorporate patient safety metrics into our reward function by integrating guidewire tip forces. Inverse RL is used with demonstrator data on 12 patient-specific vascular cases.   Results: Our simulation demonstrates successful autonomous navigation within unseen cerebral vessels, achieving a 96% success rate, 7.0s procedure time, and 0.24 N mean forces, well below the proposed 1.5 N vessel rupture threshold.   Conclusion: To the best of our knowledge, our proposed autonomous system for MT two-device navigation reaches cerebral vessels, considers safety, and is generalizable to unseen patient-specific cases for the first time. We envisage future work will extend the validation to vasculatures of different complexity and on in vitro models. While our contributions pave the way towards deploying agents in clinical settings, safety and trustworthiness will be crucial elements to consider when proposing new methodology."
  },
  {
    "title": "Level the Level: Balancing Game Levels for Asymmetric Player Archetypes With Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24099v1",
    "arxiv_id": "2503.24099v1",
    "authors": [
      "Florian Rupp",
      "Kai Eckert"
    ],
    "published": "2025-03-31T13:55:04+00:00",
    "summary": "Balancing games, especially those with asymmetric multiplayer content, requires significant manual effort and extensive human playtesting during development. For this reason, this work focuses on generating balanced levels tailored to asymmetric player archetypes, where the disparity in abilities is balanced entirely through the level design. For instance, while one archetype may have an advantage over another, both should have an equal chance of winning. We therefore conceptualize game balancing as a procedural content generation problem and build on and extend a recently introduced method that uses reinforcement learning to balance tile-based game levels. We evaluate the method on four different player archetypes and demonstrate its ability to balance a larger proportion of levels compared to two baseline approaches. Furthermore, our results indicate that as the disparity between player archetypes increases, the required number of training steps grows, while the model's accuracy in achieving balance decreases."
  },
  {
    "title": "HACTS: a Human-As-Copilot Teleoperation System for Robot Learning",
    "url": "http://arxiv.org/abs/2503.24070v1",
    "arxiv_id": "2503.24070v1",
    "authors": [
      "Zhiyuan Xu",
      "Yinuo Zhao",
      "Kun Wu",
      "Ning Liu",
      "Junjie Ji",
      "Zhengping Che",
      "Chi Harold Liu",
      "Jian Tang"
    ],
    "published": "2025-03-31T13:28:13+00:00",
    "summary": "Teleoperation is essential for autonomous robot learning, especially in manipulation tasks that require human demonstrations or corrections. However, most existing systems only offer unilateral robot control and lack the ability to synchronize the robot's status with the teleoperation hardware, preventing real-time, flexible intervention. In this work, we introduce HACTS (Human-As-Copilot Teleoperation System), a novel system that establishes bilateral, real-time joint synchronization between a robot arm and teleoperation hardware. This simple yet effective feedback mechanism, akin to a steering wheel in autonomous vehicles, enables the human copilot to intervene seamlessly while collecting action-correction data for future learning. Implemented using 3D-printed components and low-cost, off-the-shelf motors, HACTS is both accessible and scalable. Our experiments show that HACTS significantly enhances performance in imitation learning (IL) and reinforcement learning (RL) tasks, boosting IL recovery capabilities and data efficiency, and facilitating human-in-the-loop RL. HACTS paves the way for more effective and interactive human-robot collaboration and data-collection, advancing the capabilities of robot manipulation."
  },
  {
    "title": "Impact of Amplitude and Phase Damping Noise on Quantum Reinforcement Learning: Challenges and Opportunities",
    "url": "http://arxiv.org/abs/2503.24069v1",
    "arxiv_id": "2503.24069v1",
    "authors": [
      "Mar\u00eda Laura Olivera-Atencio",
      "Lucas Lamata",
      "Jes\u00fas Casado-Pascual"
    ],
    "published": "2025-03-31T13:27:30+00:00",
    "summary": "Quantum machine learning (QML) is an emerging field with significant potential, yet it remains highly susceptible to noise, which poses a major challenge to its practical implementation. While various noise mitigation strategies have been proposed to enhance algorithmic performance, the impact of noise is not fully understood. In this work, we investigate the effects of amplitude and phase damping noise on a quantum reinforcement learning algorithm. Through analytical and numerical analysis, we assess how these noise sources influence the learning process and overall performance. Our findings contribute to a deeper understanding of the role of noise in quantum learning algorithms and suggest that, rather than being purely detrimental, unavoidable noise may present opportunities to enhance QML processes."
  },
  {
    "title": "A Reactive Framework for Whole-Body Motion Planning of Mobile Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic Programmi",
    "url": "http://arxiv.org/abs/2503.23975v1",
    "arxiv_id": "2503.23975v1",
    "authors": [
      "Chenyu Zhang",
      "Shiying Sun",
      "Kuan Liu",
      "Chuanbao Zhou",
      "Xiaoguang Zhao",
      "Min Tan",
      "Yanlong Huang"
    ],
    "published": "2025-03-31T11:37:02+00:00",
    "summary": "As an important branch of embodied artificial intelligence, mobile manipulators are increasingly applied in intelligent services, but their redundant degrees of freedom also limit efficient motion planning in cluttered environments. To address this issue, this paper proposes a hybrid learning and optimization framework for reactive whole-body motion planning of mobile manipulators. We develop the Bayesian distributional soft actor-critic (Bayes-DSAC) algorithm to improve the quality of value estimation and the convergence performance of the learning. Additionally, we introduce a quadratic programming method constrained by the signed distance field to enhance the safety of the obstacle avoidance motion. We conduct experiments and make comparison with standard benchmark. The experimental results verify that our proposed framework significantly improves the efficiency of reactive whole-body motion planning, reduces the planning time, and improves the success rate of motion planning. Additionally, the proposed reinforcement learning method ensures a rapid learning process in the whole-body planning task. The novel framework allows mobile manipulators to adapt to complex environments more safely and efficiently."
  },
  {
    "title": "Noise-based reward-modulated learning",
    "url": "http://arxiv.org/abs/2503.23972v1",
    "arxiv_id": "2503.23972v1",
    "authors": [
      "Jes\u00fas Garc\u00eda Fern\u00e1ndez",
      "Nasir Ahmad",
      "Marcel van Gerven"
    ],
    "published": "2025-03-31T11:35:23+00:00",
    "summary": "Recent advances in reinforcement learning (RL) have led to significant improvements in task performance. However, training neural networks in an RL regime is typically achieved in combination with backpropagation, limiting their applicability in resource-constrained environments or when using non-differentiable neural networks. While noise-based alternatives like reward-modulated Hebbian learning (RMHL) have been proposed, their performance has remained limited, especially in scenarios with delayed rewards, which require retrospective credit assignment over time. Here, we derive a novel noise-based learning rule that addresses these challenges. Our approach combines directional derivative theory with Hebbian-like updates to enable efficient, gradient-free learning in RL. It features stochastic noisy neurons which can approximate gradients, and produces local synaptic updates modulated by a global reward signal. Drawing on concepts from neuroscience, our method uses reward prediction error as its optimization target to generate increasingly advantageous behavior, and incorporates an eligibility trace to facilitate temporal credit assignment in environments with delayed rewards. Its formulation relies on local information alone, making it compatible with implementations in neuromorphic hardware. Experimental validation shows that our approach significantly outperforms RMHL and is competitive with BP-based baselines, highlighting the promise of noise-based, biologically inspired learning for low-power and real-time applications."
  },
  {
    "title": "MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented Experience Replay for Robot Navigation",
    "url": "http://arxiv.org/abs/2503.23908v1",
    "arxiv_id": "2503.23908v1",
    "authors": [
      "Shanze Wang",
      "Mingao Tan",
      "Zhibo Yang",
      "Biao Huang",
      "Xiaoyu Shen",
      "Hailong Huang",
      "Wei Zhang"
    ],
    "published": "2025-03-31T09:58:28+00:00",
    "summary": "Deep Reinforcement Learning (DRL) based navigation methods have demonstrated promising results for mobile robots, but suffer from limited action flexibility in confined spaces. Conventional DRL approaches predominantly learn forward-motion policies, causing robots to become trapped in complex environments where backward maneuvers are necessary for recovery. This paper presents MAER-Nav (Mirror-Augmented Experience Replay for Robot Navigation), a novel framework that enables bidirectional motion learning without requiring explicit failure-driven hindsight experience replay or reward function modifications. Our approach integrates a mirror-augmented experience replay mechanism with curriculum learning to generate synthetic backward navigation experiences from successful trajectories. Experimental results in both simulation and real-world environments demonstrate that MAER-Nav significantly outperforms state-of-the-art methods while maintaining strong forward navigation capabilities. The framework effectively bridges the gap between the comprehensive action space utilization of traditional planning methods and the environmental adaptability of learning-based approaches, enabling robust navigation in scenarios where conventional DRL methods consistently fail."
  },
  {
    "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO",
    "url": "http://arxiv.org/abs/2503.23905v1",
    "arxiv_id": "2503.23905v1",
    "authors": [
      "Qihan Huang",
      "Long Chan",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jingyuan Chen",
      "Chang Yao",
      "Jie Song"
    ],
    "published": "2025-03-31T09:54:55+00:00",
    "summary": "MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at https://github.com/hqhQAQ/Hint-GRPO."
  },
  {
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "url": "http://arxiv.org/abs/2503.23829v1",
    "arxiv_id": "2503.23829v1",
    "authors": [
      "Yi Su",
      "Dian Yu",
      "Linfeng Song",
      "Juntao Li",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Min Zhang",
      "Dong Yu"
    ],
    "published": "2025-03-31T08:22:49+00:00",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels."
  },
  {
    "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains",
    "url": "http://arxiv.org/abs/2503.23829v2",
    "arxiv_id": "2503.23829v2",
    "authors": [
      "Yi Su",
      "Dian Yu",
      "Linfeng Song",
      "Juntao Li",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Min Zhang",
      "Dong Yu"
    ],
    "published": "2025-03-31T08:22:49+00:00",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated significant success in enhancing mathematical reasoning and coding performance of large language models (LLMs), especially when structured reference answers are accessible for verification. However, its extension to broader, less structured domains remains unexplored. In this work, we investigate the effectiveness and scalability of RLVR across diverse real-world domains including medicine, chemistry, psychology, economics, and education, where structured reference answers are typically unavailable. We reveal that binary verification judgments on broad-domain tasks exhibit high consistency across various LLMs provided expert-written reference answers exist. Motivated by this finding, we utilize a generative scoring technique that yields soft, model-based reward signals to overcome limitations posed by binary verifications, especially in free-form, unstructured answer scenarios. We further demonstrate the feasibility of training cross-domain generative reward models using relatively small (7B) LLMs without the need for extensive domain-specific annotation. Through comprehensive experiments, our RLVR framework establishes clear performance gains, significantly outperforming state-of-the-art open-source aligned models such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach notably enhances the robustness, flexibility, and scalability of RLVR, representing a substantial step towards practical reinforcement learning applications in complex, noisy-label scenarios."
  },
  {
    "title": "Accelerating High-Efficiency Organic Photovoltaic Discovery via Pretrained Graph Neural Networks and Generative Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23766v1",
    "arxiv_id": "2503.23766v1",
    "authors": [
      "Jiangjie Qiu",
      "Hou Hei Lam",
      "Xiuyuan Hu",
      "Wentao Li",
      "Siwei Fu",
      "Fankun Zeng",
      "Hao Zhang",
      "Xiaonan Wang"
    ],
    "published": "2025-03-31T06:31:15+00:00",
    "summary": "Organic photovoltaic (OPV) materials offer a promising avenue toward cost-effective solar energy utilization. However, optimizing donor-acceptor (D-A) combinations to achieve high power conversion efficiency (PCE) remains a significant challenge. In this work, we propose a framework that integrates large-scale pretraining of graph neural networks (GNNs) with a GPT-2 (Generative Pretrained Transformer 2)-based reinforcement learning (RL) strategy to design OPV molecules with potentially high PCE. This approach produces candidate molecules with predicted efficiencies approaching 21\\%, although further experimental validation is required. Moreover, we conducted a preliminary fragment-level analysis to identify structural motifs recognized by the RL model that may contribute to enhanced PCE, thus providing design guidelines for the broader research community. To facilitate continued discovery, we are building the largest open-source OPV dataset to date, expected to include nearly 3,000 donor-acceptor pairs. Finally, we discuss plans to collaborate with experimental teams on synthesizing and characterizing AI-designed molecules, which will provide new data to refine and improve our predictive and generative models."
  },
  {
    "title": "The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning",
    "url": "http://arxiv.org/abs/2503.23679v1",
    "arxiv_id": "2503.23679v1",
    "authors": [
      "Mingkai Tian",
      "Guorong Li",
      "Yuankai Qi",
      "Amin Beheshti",
      "Javen Qinfeng Shi",
      "Anton van den Hengel",
      "Qingming Huang"
    ],
    "published": "2025-03-31T03:00:19+00:00",
    "summary": "Zero-shot video captioning requires that a model generate high-quality captions without human-annotated video-text pairs for training. State-of-the-art approaches to the problem leverage CLIP to extract visual-relevant textual prompts to guide language models in generating captions. These methods tend to focus on one key aspect of the scene and build a caption that ignores the rest of the visual input. To address this issue, and generate more accurate and complete captions, we propose a novel progressive multi-granularity textual prompting strategy for zero-shot video captioning. Our approach constructs three distinct memory banks, encompassing noun phrases, scene graphs of noun phrases, and entire sentences. Moreover, we introduce a category-aware retrieval mechanism that models the distribution of natural language surrounding the specific topics in question. Extensive experiments demonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4% improvements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX benchmarks compared to existing state-of-the-art."
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning for Optimized Multi-UAV Coverage and Power-Efficient UE Connectivity",
    "url": "http://arxiv.org/abs/2503.23669v1",
    "arxiv_id": "2503.23669v1",
    "authors": [
      "Xuli Cai",
      "Poonam Lohan",
      "Burak Kantarci"
    ],
    "published": "2025-03-31T02:26:23+00:00",
    "summary": "In critical situations such as natural disasters, network outages, battlefield communication, or large-scale public events, Unmanned Aerial Vehicles (UAVs) offer a promising approach to maximize wireless coverage for affected users in the shortest possible time. In this paper, we propose a novel framework where multiple UAVs are deployed with the objective to maximize the number of served user equipment (UEs) while ensuring a predefined data rate threshold. UEs are initially clustered using a K-means algorithm, and UAVs are optimally positioned based on the UEs' spatial distribution. To optimize power allocation and mitigate inter-cluster interference, we employ the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm, considering both LOS and NLOS fading. Simulation results demonstrate that our method significantly enhances UEs coverage and outperforms Deep Q-Network (DQN) and equal power distribution methods, improving their UE coverage by up to 2.07 times and 8.84 times, respectively."
  },
  {
    "title": "Dynamic Operating System Scheduling Using Double DQN: A Reinforcement Learning Approach to Task Optimization",
    "url": "http://arxiv.org/abs/2503.23659v1",
    "arxiv_id": "2503.23659v1",
    "authors": [
      "Xiaoxuan Sun",
      "Yifei Duan",
      "Yingnan Deng",
      "Fan Guo",
      "Guohui Cai",
      "Yuting Peng"
    ],
    "published": "2025-03-31T01:48:21+00:00",
    "summary": "In this paper, an operating system scheduling algorithm based on Double DQN (Double Deep Q network) is proposed, and its performance under different task types and system loads is verified by experiments. Compared with the traditional scheduling algorithm, the algorithm based on Double DQN can dynamically adjust the task priority and resource allocation strategy, thus improving the task completion efficiency, system throughput, and response speed. The experimental results show that the Double DQN algorithm has high scheduling performance under light load, medium load and heavy load scenarios, especially when dealing with I/O intensive tasks, and can effectively reduce task completion time and system response time. In addition, the algorithm also shows high optimization ability in resource utilization and can intelligently adjust resource allocation according to the system state, avoiding resource waste and excessive load. Future studies will further explore the application of the algorithm in more complex systems, especially scheduling optimization in cloud computing and large-scale distributed environments, combining factors such as network latency and energy efficiency to improve the overall performance and adaptability of the algorithm."
  },
  {
    "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective",
    "url": "http://arxiv.org/abs/2503.23650v1",
    "arxiv_id": "2503.23650v1",
    "authors": [
      "Zhuoren Li",
      "Guizhe Jin",
      "Ran Yu",
      "Zhiwen Chen",
      "Nan Li",
      "Wei Han",
      "Lu Xiong",
      "Bo Leng",
      "Jia Hu",
      "Ilya Kolmanovsky",
      "Dimitar Filev"
    ],
    "published": "2025-03-31T01:31:14+00:00",
    "summary": "Reinforcement learning (RL), with its ability to explore and optimize policies in complex, dynamic decision-making tasks, has emerged as a promising approach to addressing motion planning (MoP) challenges in autonomous driving (AD). Despite rapid advancements in RL and AD, a systematic description and interpretation of the RL design process tailored to diverse driving tasks remains underdeveloped. This survey provides a comprehensive review of RL-based MoP for AD, focusing on lessons from task-specific perspectives. We first outline the fundamentals of RL methodologies, and then survey their applications in MoP, analyzing scenario-specific features and task requirements to shed light on their influence on RL design choices. Building on this analysis, we summarize key design experiences, extract insights from various driving task applications, and provide guidance for future implementations. Additionally, we examine the frontier challenges in RL-based MoP, review recent efforts to addresse these challenges, and propose strategies for overcoming unresolved issues."
  },
  {
    "title": "A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control",
    "url": "http://arxiv.org/abs/2503.23626v1",
    "arxiv_id": "2503.23626v1",
    "authors": [
      "Anirudh Satheesh",
      "Keenan Powell"
    ],
    "published": "2025-03-30T23:29:48+00:00",
    "summary": "Traffic congestion in modern cities is exacerbated by the limitations of traditional fixed-time traffic signal systems, which fail to adapt to dynamic traffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have emerged as a solution by dynamically adjusting signal timing based on real-time traffic conditions. However, the main limitation of such methods is that they are not transferable to environments under real-world constraints, such as balancing efficiency, minimizing collisions, and ensuring fairness across intersections. In this paper, we view the ATSC problem as a constrained multi-agent reinforcement learning (MARL) problem and propose a novel algorithm named Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE) to produce effective traffic signal control policies. Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment. We also introduce three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which penalize traffic policies that do not conform to real-world scenarios. Our experimental results on three real-world datasets demonstrate that MAPPO-LCE outperforms three baseline MARL algorithms by across all environments and traffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by 13.10%). Our results show that constrained MARL is a valuable tool for traffic planners to deploy scalable and efficient ATSC methods in real-world traffic networks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE."
  },
  {
    "title": "An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23615v1",
    "arxiv_id": "2503.23615v1",
    "authors": [
      "Julien Soul\u00e9",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Th\u00e9ron"
    ],
    "published": "2025-03-30T22:43:01+00:00",
    "summary": "Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents."
  },
  {
    "title": "Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models",
    "url": "http://arxiv.org/abs/2503.23487v1",
    "arxiv_id": "2503.23487v1",
    "authors": [
      "Irtaza Khalid",
      "Amir Masoud Nourollah",
      "Steven Schockaert"
    ],
    "published": "2025-03-30T15:41:55+00:00",
    "summary": "Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is still known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond problem solving in mathematics and programming, where finding genuine out-of-distribution problems can be difficult. In this paper, we focus on tasks that require systematic reasoning about relational compositions, especially for qualitative spatial and temporal reasoning. These tasks allow us to control the difficulty of problem instances, and measure in a precise way to what extent models can generalise. We find that that the considered LLMs and LRMs overall perform poorly overall, albeit better than random chance."
  },
  {
    "title": "A Systematic Decade Review of Trip Route Planning with Travel Time Estimation based on User Preferences and Behavior",
    "url": "http://arxiv.org/abs/2503.23486v1",
    "arxiv_id": "2503.23486v1",
    "authors": [
      "Nikil Jayasuriya",
      "Deshan Sumanathilaka"
    ],
    "published": "2025-03-30T15:41:44+00:00",
    "summary": "This paper systematically explores the advancements in adaptive trip route planning and travel time estimation (TTE) through Artificial Intelligence (AI). With the increasing complexity of urban transportation systems, traditional navigation methods often struggle to accommodate dynamic user preferences, real-time traffic conditions, and scalability requirements. This study explores the contributions of established AI techniques, including Machine Learning (ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside emerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI, and Federated Learning. In addition to highlighting these innovations, the paper identifies critical challenges such as ethical concerns, computational scalability, and effective data integration, which must be addressed to advance the field. The paper concludes with recommendations for leveraging AI to build efficient, transparent, and sustainable navigation systems."
  },
  {
    "title": "Handling Delay in Real-Time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23478v1",
    "arxiv_id": "2503.23478v1",
    "authors": [
      "Ivan Anokhin",
      "Rishav Rishav",
      "Matthew Riemer",
      "Stephen Chung",
      "Irina Rish",
      "Samira Ebrahimi Kahou"
    ],
    "published": "2025-03-30T15:30:27+00:00",
    "summary": "Real-time reinforcement learning (RL) introduces several challenges. First, policies are constrained to a fixed number of actions per second due to hardware limitations. Second, the environment may change while the network is still computing an action, leading to observational delay. The first issue can partly be addressed with pipelining, leading to higher throughput and potentially better policies. However, the second issue remains: if each neuron operates in parallel with an execution time of $\\tau$, an $N$-layer feed-forward network experiences observation delay of $\\tau N$. Reducing the number of layers can decrease this delay, but at the cost of the network's expressivity. In this work, we explore the trade-off between minimizing delay and network's expressivity. We present a theoretically motivated solution that leverages temporal skip connections combined with history-augmented observations. We evaluate several architectures and show that those incorporating temporal skip connections achieve strong performance across various neuron execution times, reinforcement learning algorithms, and environments, including four Mujoco tasks and all MinAtar games. Moreover, we demonstrate parallel neuron computation can accelerate inference by 6-350% on standard hardware. Our investigation into temporal skip connections and parallel computations paves the way for more efficient RL agents in real-time setting."
  },
  {
    "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach",
    "url": "http://arxiv.org/abs/2503.23459v1",
    "arxiv_id": "2503.23459v1",
    "authors": [
      "Chenglong Lu",
      "Shen Liang",
      "Xuewei Wang",
      "Wei Wang"
    ],
    "published": "2025-03-30T14:34:28+00:00",
    "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically with the number of tokens, calling for effective token pruning policies. Most existing policies are handcrafted, lacking adaptivity to varying inputs. Moreover, they fail to consider the sequential nature of token pruning across multiple layers. In this work, for the first time (as far as we know), we exploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy. Formulating token pruning as a sequential decision-making problem, we model it as a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO) where each agent makes an individualized pruning decision for a single token. We also develop reward functions that enable simultaneous collaboration and competition of these agents to balance efficiency and accuracy. On the well-known ImageNet-1k dataset, our method improves the inference speed by up to 44% while incurring only a negligible accuracy drop of 0.4%. The source code is available at https://github.com/daashuai/rl4evit."
  },
  {
    "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
    "url": "http://arxiv.org/abs/2503.23427v1",
    "arxiv_id": "2503.23427v1",
    "authors": [
      "Wenhan Liu",
      "Xinyu Ma",
      "Yutao Zhu",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Zhicheng Dou"
    ],
    "published": "2025-03-30T13:00:52+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \\textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\\%) while achieving even better effectiveness compared to using only the LLM listwise reranker."
  },
  {
    "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
    "url": "http://arxiv.org/abs/2503.23427v2",
    "arxiv_id": "2503.23427v2",
    "authors": [
      "Wenhan Liu",
      "Xinyu Ma",
      "Yutao Zhu",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Zhicheng Dou"
    ],
    "published": "2025-03-30T13:00:52+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \\textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\\%) while achieving even better effectiveness compared to using only the LLM listwise reranker."
  },
  {
    "title": "ToRL: Scaling Tool-Integrated RL",
    "url": "http://arxiv.org/abs/2503.23383v1",
    "arxiv_id": "2503.23383v1",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "published": "2025-03-30T10:16:25+00:00",
    "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for training large language models (LLMs) to autonomously use computational tools via reinforcement learning. Unlike supervised fine-tuning, ToRL allows models to explore and discover optimal strategies for tool use. Experiments with Qwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\% accuracy on AIME~24, surpassing reinforcement learning without tool integration by 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%. Further analysis reveals emergent behaviors such as strategic tool invocation, self-regulation of ineffective code, and dynamic adaptation between computational and analytical reasoning, all arising purely through reward-driven learning."
  },
  {
    "title": "FeRG-LLM : Feature Engineering by Reason Generation Large Language Models",
    "url": "http://arxiv.org/abs/2503.23371v1",
    "arxiv_id": "2503.23371v1",
    "authors": [
      "Jeonghyun Ko",
      "Gyeongyun Park",
      "Donghoon Lee",
      "Kyunam Lee"
    ],
    "published": "2025-03-30T09:07:21+00:00",
    "summary": "One of the key tasks in machine learning for tabular data is feature engineering. Although it is vital for improving the performance of models, it demands considerable human expertise and deep domain knowledge, making it labor-intensive endeavor. To address this issue, we propose a novel framework, \\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason \\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large language model designed to automatically perform feature engineering at an 8-billion-parameter scale. We have constructed two-stage conversational dialogues that enable language models to analyze machine learning tasks and discovering new features, exhibiting their Chain-of-Thought (CoT) capabilities. We use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct Preference Optimization (DPO) to receive feedback improving quality of new features and the model's performance. Our experiments show that FeRG-LLM performs comparably to or better than Llama 3.1 70B on most datasets, while using fewer resources and achieving reduced inference time. It outperforms other studies in classification tasks and performs well in regression tasks. Moreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API costs when generating features, it can be deployed locally, addressing security concerns."
  },
  {
    "title": "Reinforcement Learning for Active Matter",
    "url": "http://arxiv.org/abs/2503.23308v1",
    "arxiv_id": "2503.23308v1",
    "authors": [
      "Wenjie Cai",
      "Gongyi Wang",
      "Yu Zhang",
      "Xiang Qu",
      "Zihan Huang"
    ],
    "published": "2025-03-30T04:27:17+00:00",
    "summary": "Active matter refers to systems composed of self-propelled entities that consume energy to produce motion, exhibiting complex non-equilibrium dynamics that challenge traditional models. With the rapid advancements in machine learning, reinforcement learning (RL) has emerged as a promising framework for addressing the complexities of active matter. This review systematically introduces the integration of RL for guiding and controlling active matter systems, focusing on two key aspects: optimal motion strategies for individual active particles and the regulation of collective dynamics in active swarms. We discuss the use of RL to optimize the navigation, foraging, and locomotion strategies for individual active particles. In addition, the application of RL in regulating collective behaviors is also examined, emphasizing its role in facilitating the self-organization and goal-directed control of active swarms. This investigation offers valuable insights into how RL can advance the understanding, manipulation, and control of active matter, paving the way for future developments in fields such as biological systems, robotics, and medical science."
  },
  {
    "title": "SalesRLAgent: A Reinforcement Learning Approach for Real-Time Sales Conversion Prediction and Optimization",
    "url": "http://arxiv.org/abs/2503.23303v1",
    "arxiv_id": "2503.23303v1",
    "authors": [
      "Nandakishor M"
    ],
    "published": "2025-03-30T03:56:26+00:00",
    "summary": "Current approaches to sales conversation analysis and conversion prediction typically rely on Large Language Models (LLMs) combined with basic retrieval augmented generation (RAG). These systems, while capable of answering questions, fail to accurately predict conversion probability or provide strategic guidance in real time. In this paper, we present SalesRLAgent, a novel framework leveraging specialized reinforcement learning to predict conversion probability throughout sales conversations. Unlike systems from Kapa.ai, Mendable, Inkeep, and others that primarily use off-the-shelf LLMs for content generation, our approach treats conversion prediction as a sequential decision problem, training on synthetic data generated using GPT-4O to develop a specialized probability estimation model. Our system incorporates Azure OpenAI embeddings (3072 dimensions), turn-by-turn state tracking, and meta-learning capabilities to understand its own knowledge boundaries. Evaluations demonstrate that SalesRLAgent achieves 96.7% accuracy in conversion prediction, outperforming LLM-only approaches by 34.7% while offering significantly faster inference (85ms vs 3450ms for GPT-4). Furthermore, integration with existing sales platforms shows a 43.2% increase in conversion rates when representatives utilize our system's real-time guidance. SalesRLAgent represents a fundamental shift from content generation to strategic sales intelligence, providing moment-by-moment conversion probability estimation with actionable insights for sales professionals."
  },
  {
    "title": "Efficient Twin Migration in Vehicular Metaverses: Multi-Agent Split Deep Reinforcement Learning with Spatio-Temporal Trajectory Generation",
    "url": "http://arxiv.org/abs/2503.23290v1",
    "arxiv_id": "2503.23290v1",
    "authors": [
      "Junlong Chen",
      "Jiawen Kang",
      "Minrui Xu",
      "Fan Wu",
      "Hongliang Zhang",
      "Huawei Huang",
      "Dusit Niyato",
      "Shiwen Mao"
    ],
    "published": "2025-03-30T03:00:01+00:00",
    "summary": "Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users' immersive experiences in vehicular metaverses."
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Graph Discovery in D2D-Enabled Federated Learning",
    "url": "http://arxiv.org/abs/2503.23218v1",
    "arxiv_id": "2503.23218v1",
    "authors": [
      "Satyavrat Wagle",
      "Anindya Bijoy Das",
      "David J. Love",
      "Christopher G. Brinton"
    ],
    "published": "2025-03-29T20:42:26+00:00",
    "summary": "Augmenting federated learning (FL) with device-to-device (D2D) communications can help improve convergence speed and reduce model bias through local information exchange. However, data privacy concerns, trust constraints between devices, and unreliable wireless channels each pose challenges in finding an effective yet resource efficient D2D graph structure. In this paper, we develop a decentralized reinforcement learning (RL) method for D2D graph discovery that promotes communication of impactful datapoints over reliable links for multiple learning paradigms, while following both data and device-specific trust constraints. An independent RL agent at each device trains a policy to predict the impact of incoming links in a decentralized manner without exposure of local data or significant communication overhead. For supervised settings, the D2D graph aims to improve device-specific label diversity without compromising system-level performance. For semi-supervised settings, we enable this by incorporating distributed label propagation. For unsupervised settings, we develop a variation-based diversity metric which estimates data diversity in terms of occupied latent space. Numerical experiments on five widely used datasets confirm that the data diversity improvements induced by our method increase convergence speed by up to 3 times while reducing energy consumption by up to 5 times. They also show that our method is resilient to stragglers and changes in the aggregation interval. Finally, we show that our method offers scalability benefits for larger system sizes without increases in relative overhead, and adaptability to various downstream FL architectures and to dynamic wireless environments."
  },
  {
    "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
    "url": "http://arxiv.org/abs/2503.23157v1",
    "arxiv_id": "2503.23157v1",
    "authors": [
      "Mohammadreza Pourreza",
      "Shayan Talaei",
      "Ruoxi Sun",
      "Xingchen Wan",
      "Hailong Li",
      "Azalia Mirhoseini",
      "Amin Saberi",
      "Sercan \"O. Arik"
    ],
    "published": "2025-03-29T17:29:30+00:00",
    "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks."
  },
  {
    "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
    "url": "http://arxiv.org/abs/2503.23157v2",
    "arxiv_id": "2503.23157v2",
    "authors": [
      "Mohammadreza Pourreza",
      "Shayan Talaei",
      "Ruoxi Sun",
      "Xingchen Wan",
      "Hailong Li",
      "Azalia Mirhoseini",
      "Amin Saberi",
      "Sercan \"O. Arik"
    ],
    "published": "2025-03-29T17:29:30+00:00",
    "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks."
  },
  {
    "title": "Dexterous Non-Prehensile Manipulation for Ungraspable Object via Extrinsic Dexterity",
    "url": "http://arxiv.org/abs/2503.23120v1",
    "arxiv_id": "2503.23120v1",
    "authors": [
      "Yuhan Wang",
      "Yu Li",
      "Yaodong Yang",
      "Yuanpei Chen"
    ],
    "published": "2025-03-29T15:21:41+00:00",
    "summary": "Objects with large base areas become ungraspable when they exceed the end-effector's maximum aperture. Existing approaches address this limitation through extrinsic dexterity, which exploits environmental features for non-prehensile manipulation. While grippers have shown some success in this domain, dexterous hands offer superior flexibility and manipulation capabilities that enable richer environmental interactions, though they present greater control challenges. Here we present ExDex, a dexterous arm-hand system that leverages reinforcement learning to enable non-prehensile manipulation for grasping ungraspable objects. Our system learns two strategic manipulation sequences: relocating objects from table centers to edges for direct grasping, or to walls where extrinsic dexterity enables grasping through environmental interaction. We validate our approach through extensive experiments with dozens of diverse household objects, demonstrating both superior performance and generalization capabilities with novel objects. Furthermore, we successfully transfer the learned policies from simulation to a real-world robot system without additional training, further demonstrating its applicability in real-world scenarios. Project website: https://tangty11.github.io/ExDex/."
  },
  {
    "title": "RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations",
    "url": "http://arxiv.org/abs/2503.23101v1",
    "arxiv_id": "2503.23101v1",
    "authors": [
      "Enrico Marchesini",
      "Benjamin Donnot",
      "Constance Crozier",
      "Ian Dytham",
      "Christian Merz",
      "Lars Schewe",
      "Nico Westerbeck",
      "Cathy Wu",
      "Antoine Marot",
      "Priya L. Donti"
    ],
    "published": "2025-03-29T14:39:17+00:00",
    "summary": "Reinforcement learning (RL) can transform power grid operations by providing adaptive and scalable controllers essential for grid decarbonization. However, existing methods struggle with the complex dynamics, aleatoric uncertainty, long-horizon goals, and hard physical constraints that occur in real-world systems. This paper presents RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on a power simulation framework developed by RTE France, RL2Grid standardizes tasks, state and action spaces, and reward structures within a unified interface for a systematic evaluation and comparison of RL approaches. Moreover, we integrate real control heuristics and safety constraints informed by the operators' expertise to ensure RL2Grid aligns with grid operation requirements. We benchmark popular RL baselines on the grid control tasks represented within RL2Grid, establishing reference performance metrics. Our results and discussion highlight the challenges that power grids pose for RL methods, emphasizing the need for novel algorithms capable of handling real-world physical systems."
  },
  {
    "title": "Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22958v1",
    "arxiv_id": "2503.22958v1",
    "authors": [
      "Supriyo Maji",
      "Linran Zhao",
      "Souradip Poddar",
      "David Z. Pan"
    ],
    "published": "2025-03-29T03:13:56+00:00",
    "summary": "Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing."
  },
  {
    "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning",
    "url": "http://arxiv.org/abs/2503.22948v1",
    "arxiv_id": "2503.22948v1",
    "authors": [
      "Tianyang Xu",
      "Xiaoze Liu",
      "Feijie Wu",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "published": "2025-03-29T02:33:26+00:00",
    "summary": "Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications."
  },
  {
    "title": "Adaptive Interactive Navigation of Quadruped Robots using Large Language Models",
    "url": "http://arxiv.org/abs/2503.22942v1",
    "arxiv_id": "2503.22942v1",
    "authors": [
      "Kangjie Zhou",
      "Yao Mu",
      "Haoyang Song",
      "Yi Zeng",
      "Pengying Wu",
      "Han Gao",
      "Chang Liu"
    ],
    "published": "2025-03-29T02:17:52+00:00",
    "summary": "Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within free space, struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this gap, we propose an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to reach originally unavailable goals. Specifically, we present a primitive tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning method featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan modification in unknown environments. Comprehensive simulations and experiments have demonstrated our method's effectiveness and adaptivity in diverse scenarios. The supplementary video is available at page: https://youtu.be/W5ttPnSap2g."
  },
  {
    "title": "Predictive Traffic Rule Compliance using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22925v1",
    "arxiv_id": "2503.22925v1",
    "authors": [
      "Yanliang Huang",
      "Sebastian Mair",
      "Zhuoqi Zeng",
      "Amr Alanwar",
      "Matthias Althoff"
    ],
    "published": "2025-03-29T01:04:08+00:00",
    "summary": "Autonomous vehicle path planning has reached a stage where safety and regulatory compliance are crucial. This paper presents a new approach that integrates a motion planner with a deep reinforcement learning model to predict potential traffic rule violations. In this setup, the predictions of the critic directly affect the cost function of the motion planner, guiding the choices of the trajectory. We incorporate key interstate rules from the German Road Traffic Regulation into a rule book and use a graph-based state representation to handle complex traffic information. Our main innovation is replacing the standard actor network in an actor-critic setup with a motion planning module, which ensures both predictable trajectory generation and prevention of long-term rule violations. Experiments on an open German highway dataset show that the model can predict and prevent traffic rule violations beyond the planning horizon, significantly increasing safety in challenging traffic conditions."
  },
  {
    "title": "Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models",
    "url": "http://arxiv.org/abs/2503.22886v1",
    "arxiv_id": "2503.22886v1",
    "authors": [
      "Ron Vainshtein",
      "Zohar Rimon",
      "Shie Mannor",
      "Chen Tessler"
    ],
    "published": "2025-03-28T21:28:13+00:00",
    "summary": "Recent advancements in imitation learning have led to transformer-based behavior foundation models (BFMs) that enable multi-modal, human-like control for humanoid agents. While excelling at zero-shot generation of robust behaviors, BFMs often require meticulous prompt engineering for specific tasks, potentially yielding suboptimal results. We introduce \"Task Tokens\", a method to effectively tailor BFMs to specific tasks while preserving their flexibility. Our approach leverages the transformer architecture of BFMs to learn a new task-specific encoder through reinforcement learning, keeping the original BFM frozen. This allows incorporation of user-defined priors, balancing reward design and prompt engineering. By training a task encoder to map observations to tokens, used as additional BFM inputs, we guide performance improvement while maintaining the model's diverse control characteristics. We demonstrate Task Tokens' efficacy across various tasks, including out-of-distribution scenarios, and show their compatibility with other prompting modalities. Our results suggest that Task Tokens offer a promising approach for adapting BFMs to specific control tasks while retaining their generalization capabilities."
  },
  {
    "title": "Markov Potential Game Construction and Multi-Agent Reinforcement Learning with Applications to Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.22867v1",
    "arxiv_id": "2503.22867v1",
    "authors": [
      "Huiwen Yan",
      "Mushuang Liu"
    ],
    "published": "2025-03-28T20:49:47+00:00",
    "summary": "Markov games (MGs) serve as the mathematical foundation for multi-agent reinforcement learning (MARL), enabling self-interested agents to learn their optimal policies while interacting with others in a shared environment. However, due to the complexities of an MG problem, seeking (Markov perfect) Nash equilibrium (NE) is often very challenging for a general-sum MG. Markov potential games (MPGs), which are a special class of MGs, have appealing properties such as guaranteed existence of pure NEs and guaranteed convergence of gradient play algorithms, thereby leading to desirable properties for many MARL algorithms in their NE-seeking processes. However, the question of how to construct MPGs has been open. This paper provides sufficient conditions on the reward design and on the Markov decision process (MDP), under which an MG is an MPG. Numerical results on autonomous driving applications are reported."
  },
  {
    "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22679v1",
    "arxiv_id": "2503.22679v1",
    "authors": [
      "Weiqi Li",
      "Xuanyu Zhang",
      "Shijie Zhao",
      "Yabin Zhang",
      "Junlin Li",
      "Li Zhang",
      "Jian Zhang"
    ],
    "published": "2025-03-28T17:59:54+00:00",
    "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at https://github.com/lwq20020127/Q-Insight."
  },
  {
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "url": "http://arxiv.org/abs/2503.22677v1",
    "arxiv_id": "2503.22677v1",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "published": "2025-03-28T17:59:53+00:00",
    "summary": "Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs."
  },
  {
    "title": "Reinforcement Learning for Machine Learning Model Deployment: Evaluating Multi-Armed Bandits in ML Ops Environments",
    "url": "http://arxiv.org/abs/2503.22595v1",
    "arxiv_id": "2503.22595v1",
    "authors": [
      "S. Aaron McClendon",
      "Vishaal Venkatesh",
      "Juan Morinelli"
    ],
    "published": "2025-03-28T16:42:21+00:00",
    "summary": "In modern ML Ops environments, model deployment is a critical process that traditionally relies on static heuristics such as validation error comparisons and A/B testing. However, these methods require human intervention to adapt to real-world deployment challenges, such as model drift or unexpected performance degradation. We investigate whether reinforcement learning, specifically multi-armed bandit (MAB) algorithms, can dynamically manage model deployment decisions more effectively. Our approach enables more adaptive production environments by continuously evaluating deployed models and rolling back underperforming ones in real-time. We test six model selection strategies across two real-world datasets and find that RL based approaches match or exceed traditional methods in performance. Our findings suggest that reinforcement learning (RL)-based model management can improve automation, reduce reliance on manual interventions, and mitigate risks associated with post-deployment model failures."
  },
  {
    "title": "On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations",
    "url": "http://arxiv.org/abs/2503.22575v1",
    "arxiv_id": "2503.22575v1",
    "authors": [
      "Rajdeep Singh Hundal",
      "Yan Xiao",
      "Xiaochun Cao",
      "Jin Song Dong",
      "Manuel Rigger"
    ],
    "published": "2025-03-28T16:25:06+00:00",
    "summary": "Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used."
  },
  {
    "title": "Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games",
    "url": "http://arxiv.org/abs/2503.22779v1",
    "arxiv_id": "2503.22779v1",
    "authors": [
      "Junkai Hu",
      "Li Xia"
    ],
    "published": "2025-03-28T16:21:05+00:00",
    "summary": "We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted."
  },
  {
    "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments",
    "url": "http://arxiv.org/abs/2503.22496v1",
    "arxiv_id": "2503.22496v1",
    "authors": [
      "Luke Rowe",
      "Roger Girgis",
      "Anthony Gosselin",
      "Liam Paull",
      "Christopher Pal",
      "Felix Heide"
    ],
    "published": "2025-03-28T15:03:41+00:00",
    "summary": "We introduce Scenario Dreamer, a fully data-driven generative simulator for autonomous vehicle planning that generates both the initial traffic scene - comprising a lane graph and agent bounding boxes - and closed-loop agent behaviours. Existing methods for generating driving simulation environments encode the initial traffic scene as a rasterized image and, as such, require parameter-heavy networks that perform unnecessary computation due to many empty pixels in the rasterized scene. Moreover, we find that existing methods that employ rule-based agent behaviours lack diversity and realism. Scenario Dreamer instead employs a novel vectorized latent diffusion model for initial scene generation that directly operates on the vectorized scene elements and an autoregressive Transformer for data-driven agent behaviour simulation. Scenario Dreamer additionally supports scene extrapolation via diffusion inpainting, enabling the generation of unbounded simulation environments. Extensive experiments show that Scenario Dreamer outperforms existing generative simulators in realism and efficiency: the vectorized scene-generation base model achieves superior generation quality with around 2x fewer parameters, 6x lower generation latency, and 10x fewer GPU training hours compared to the strongest baseline. We confirm its practical utility by showing that reinforcement learning planning agents are more challenged in Scenario Dreamer environments than traditional non-generative simulation environments, especially on long and adversarial driving environments."
  },
  {
    "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model",
    "url": "http://arxiv.org/abs/2503.22480v1",
    "arxiv_id": "2503.22480v1",
    "authors": [
      "Wangtao Sun",
      "Xiang Cheng",
      "Xing Yu",
      "Haotian Xu",
      "Zhao Yang",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ],
    "published": "2025-03-28T14:39:52+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed uncertain reward model to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data. In this paper, we propose the Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model. PURM learns reward distributions directly from preference data and quantifies per-sample uncertainty via the average overlap area between reward distributions. To mitigate reward hacking, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. We propose a lightweight and easy-to-use implementation of PURM. Experiments demonstrate that PURM significantly delays the onset of reward hacking while improving final reward performance, outperforming baseline methods in both stability and effectiveness."
  },
  {
    "title": "Control of Humanoid Robots with Parallel Mechanisms using Kinematic Actuation Models",
    "url": "http://arxiv.org/abs/2503.22459v1",
    "arxiv_id": "2503.22459v1",
    "authors": [
      "Victor Lutz",
      "Ludovic de Matte\u00efs",
      "Virgile Batto",
      "Nicolas Mansard"
    ],
    "published": "2025-03-28T14:13:00+00:00",
    "summary": "Inspired by the mechanical design of Cassie, several recently released humanoid robots are using actuator configuration in which the motor is displaced from the joint location to optimize the leg inertia. This in turn induces a non linearity in the reduction ratio of the transmission which is often neglected when computing the robot motion (e.g. by trajectory optimization or reinforcement learning) and only accounted for at control time. This paper proposes an analytical method to efficiently handle this non-linearity. Using this actuation model, we demonstrate that we can leverage the dynamic abilities of the non-linear transmission while only modeling the inertia of the main serial chain of the leg, without approximating the motor capabilities nor the joint range. Based on analytical inverse kinematics, our method does not need any numerical routines dedicated to the closed-kinematics actuation, hence leading to very efficient computations. Our study focuses on two mechanisms widely used in recent humanoid robots; the four bar knee linkage as well as a parallel 2 DoF ankle mechanism. We integrate these models inside optimization based (DDP) and learning (PPO) control approaches. A comparison of our model against a simplified model that completely neglects closed chains is then shown in simulation."
  },
  {
    "title": "Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning",
    "url": "http://arxiv.org/abs/2503.22456v1",
    "arxiv_id": "2503.22456v1",
    "authors": [
      "Abdullah Vanlioglu"
    ],
    "published": "2025-03-28T14:07:51+00:00",
    "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies."
  },
  {
    "title": "Reinforcement learning for efficient and robust multi-setpoint and multi-trajectory tracking in bioprocesses",
    "url": "http://arxiv.org/abs/2503.22409v1",
    "arxiv_id": "2503.22409v1",
    "authors": [
      "Sebasti\u00e1n Espinel-R\u00edos",
      "Jos\u00e9 L. Avalos",
      "Ehecatl Antonio del Rio Chanona",
      "Dongda Zhang"
    ],
    "published": "2025-03-28T13:19:02+00:00",
    "summary": "Efficient and robust bioprocess control is essential for maximizing performance and adaptability in advanced biotechnological systems. In this work, we present a reinforcement-learning framework for multi-setpoint and multi-trajectory tracking. Tracking multiple setpoints and time-varying trajectories in reinforcement learning is challenging due to the complexity of balancing multiple objectives, a difficulty further exacerbated by system uncertainties such as uncertain initial conditions and stochastic dynamics. This challenge is relevant, e.g., in bioprocesses involving microbial consortia, where precise control over population compositions is required. We introduce a novel return function based on multiplicative reciprocal saturation functions, which explicitly couples reward gains to the simultaneous satisfaction of multiple references. Through a case study involving light-mediated cybergenetic growth control in microbial consortia, we demonstrate via computational experiments that our approach achieves faster convergence, improved stability, and superior control compliance compared to conventional quadratic-cost-based return functions. Moreover, our method enables tuning of the saturation function's parameters, shaping the learning process and policy updates. By incorporating system uncertainties, our framework also demonstrates robustness, a key requirement in industrial bioprocessing. Overall, this work advances reinforcement-learning-based control strategies in bioprocess engineering, with implications in the broader field of process and systems engineering."
  },
  {
    "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
    "url": "http://arxiv.org/abs/2503.22402v1",
    "arxiv_id": "2503.22402v1",
    "authors": [
      "Yizhang Zhu",
      "Runzhi Jiang",
      "Boyan Li",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "published": "2025-03-28T13:11:27+00:00",
    "summary": "Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costs--often overlooked--stand as the \"elephant in the room\" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2x boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL."
  },
  {
    "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
    "url": "http://arxiv.org/abs/2503.22342v1",
    "arxiv_id": "2503.22342v1",
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Yuan Xie",
      "Rongrong Ji"
    ],
    "published": "2025-03-28T11:30:05+00:00",
    "summary": "This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO). GRPO, while effective, incurs high training costs due to the need for sampling multiple completions for each question. Our experiment and theoretical analysis reveals that the number of completions impacts model accuracy yet increases training time multiplicatively, and not all completions contribute equally to policy training -- their contribution depends on their relative advantage. To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates. Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency. Experimental results demonstrate that CPPO achieves up to $8.32\\times$ speedup on GSM8K and $3.51\\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO. We release our code at https://github.com/lzhxmu/CPPO."
  },
  {
    "title": "FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation",
    "url": "http://arxiv.org/abs/2503.22249v1",
    "arxiv_id": "2503.22249v1",
    "authors": [
      "Xianqi Zhang",
      "Hongliang Wei",
      "Wenrui Wang",
      "Xingtao Wang",
      "Xiaopeng Fan",
      "Debin Zhao"
    ],
    "published": "2025-03-28T09:02:32+00:00",
    "summary": "Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance."
  },
  {
    "title": "CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.22248v1",
    "arxiv_id": "2503.22248v1",
    "authors": [
      "Xinwei Gao",
      "Arambam James Singh",
      "Gangadhar Royyuru",
      "Michael Yuhas",
      "Arvind Easwaran"
    ],
    "published": "2025-03-28T08:59:02+00:00",
    "summary": "Lane keeping in autonomous driving systems requires scenario-specific weight tuning for different objectives. We formulate lane-keeping as a constrained reinforcement learning problem, where weight coefficients are automatically learned along with the policy, eliminating the need for scenario-specific tuning. Empirically, our approach outperforms traditional RL in efficiency and reliability. Additionally, real-world demonstrations validate its practical value for real-world autonomous driving."
  },
  {
    "title": "Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch",
    "url": "http://arxiv.org/abs/2503.22244v1",
    "arxiv_id": "2503.22244v1",
    "authors": [
      "Weizhen Wang",
      "Jianping He",
      "Xiaoming Duan"
    ],
    "published": "2025-03-28T08:52:41+00:00",
    "summary": "Policy gradient methods are one of the most successful methods for solving challenging reinforcement learning problems. However, despite their empirical successes, many SOTA policy gradient algorithms for discounted problems deviate from the theoretical policy gradient theorem due to the existence of a distribution mismatch. In this work, we analyze the impact of this mismatch on the policy gradient methods. Specifically, we first show that in the case of tabular parameterizations, the methods under the mismatch remain globally optimal. Then, we extend this analysis to more general parameterizations by leveraging the theory of biased stochastic gradient descent. Our findings offer new insights into the robustness of policy gradient methods as well as the gap between theoretical foundations and practical implementations."
  },
  {
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2503.22230v1",
    "arxiv_id": "2503.22230v1",
    "authors": [
      "Wei Shen",
      "Guanlin Liu",
      "Zheng Wu",
      "Ruofei Zhu",
      "Qingping Yang",
      "Chao Xin",
      "Yu Yue",
      "Lin Yan"
    ],
    "published": "2025-03-28T08:26:41+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF."
  },
  {
    "title": "Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps",
    "url": "http://arxiv.org/abs/2503.22162v1",
    "arxiv_id": "2503.22162v1",
    "authors": [
      "Ning Liu",
      "Sen Shen",
      "Xiangrui Kong",
      "Hongtao Zhang",
      "Thomas Br\u00e4unl"
    ],
    "published": "2025-03-28T05:57:23+00:00",
    "summary": "Multi-Agent Pathfinding is used in areas including multi-robot formations, warehouse logistics, and intelligent vehicles. However, many environments are incomplete or frequently change, making it difficult for standard centralized planning or pure reinforcement learning to maintain both global solution quality and local flexibility. This paper introduces a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning, using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. We evaluate the framework in the discrete POGEMA environment and compare it with baseline methods. Experimental outcomes indicate that the proposed framework substantially improves success rate, collision rate, and path efficiency. The model is further tested on the EyeSim platform, where it maintains feasible Pathfinding under frequent changes and large-scale robot deployments."
  },
  {
    "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
    "url": "http://arxiv.org/abs/2503.22137v1",
    "arxiv_id": "2503.22137v1",
    "authors": [
      "Syrine Belakaria",
      "Joshua Kazdan",
      "Charles Marx",
      "Chris Cundy",
      "Willie Neiswanger",
      "Sanmi Koyejo",
      "Barbara E. Engelhardt",
      "Stefano Ermon"
    ],
    "published": "2025-03-28T04:22:53+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of the training and alignment pipeline for large language models (LLMs). Recent advances, such as direct preference optimization (DPO), have simplified the preference learning step. However, collecting preference data remains a challenging and costly process, often requiring expert annotation. This cost can be mitigated by carefully selecting the data points presented for annotation. In this work, we propose an active learning approach to efficiently select prompt and preference pairs using a risk assessment strategy based on the Sharpe Ratio. To address the challenge of unknown preferences prior to annotation, our method evaluates the gradients of all potential preference annotations to assess their impact on model updates. These gradient-based evaluations enable risk assessment of data points regardless of the annotation outcome. By leveraging the DPO loss derivations, we derive a closed-form expression for computing these Sharpe ratios on a per-tuple basis, ensuring our approach remains both tractable and computationally efficient. We also introduce two variants of our method, each making different assumptions about prior information. Experimental results demonstrate that our method outperforms the baseline by up to 5% in win rates against the chosen completion with limited human preference data across several language models and real-world datasets."
  },
  {
    "title": "Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for Contact-Rich Robotic Tasks",
    "url": "http://arxiv.org/abs/2503.21989v1",
    "arxiv_id": "2503.21989v1",
    "authors": [
      "Heng Zhang",
      "Gokhan Solak",
      "Arash Ajoudani"
    ],
    "published": "2025-03-27T21:11:32+00:00",
    "summary": "Ensuring safety in reinforcement learning (RL)-based robotic systems is a critical challenge, especially in contact-rich tasks within unstructured environments. While the state-of-the-art safe RL approaches mitigate risks through safe exploration or high-level recovery mechanisms, they often overlook low-level execution safety, where reflexive responses to potential hazards are crucial. Similarly, variable impedance control (VIC) enhances safety by adjusting the robot's mechanical response, yet lacks a systematic way to adapt parameters, such as stiffness and damping throughout the task. In this paper, we propose Bresa, a Bio-inspired Reflexive Hierarchical Safe RL method inspired by biological reflexes. Our method decouples task learning from safety learning, incorporating a safety critic network that evaluates action risks and operates at a higher frequency than the task solver. Unlike existing recovery-based methods, our safety critic functions at a low-level control layer, allowing real-time intervention when unsafe conditions arise. The task-solving RL policy, running at a lower frequency, focuses on high-level planning (decision-making), while the safety critic ensures instantaneous safety corrections. We validate Bresa on multiple tasks including a contact-rich robotic task, demonstrating its reflexive ability to enhance safety, and adaptability in unforeseen dynamic environments. Our results show that Bresa outperforms the baseline, providing a robust and reflexive safety mechanism that bridges the gap between high-level planning and low-level execution. Real-world experiments and supplementary material are available at project website https://jack-sherman01.github.io/Bresa."
  },
  {
    "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
    "url": "http://arxiv.org/abs/2503.21983v1",
    "arxiv_id": "2503.21983v1",
    "authors": [
      "Abed Kareem Musaffar",
      "Anand Gokhale",
      "Sirui Zeng",
      "Rasta Tadayon",
      "Xifeng Yan",
      "Ambuj Singh",
      "Francesco Bullo"
    ],
    "published": "2025-03-27T21:01:02+00:00",
    "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies."
  },
  {
    "title": "Pretrained Bayesian Non-parametric Knowledge Prior in Robotic Long-Horizon Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21975v1",
    "arxiv_id": "2503.21975v1",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Kejia Chen",
      "Yansong Wu",
      "Liding Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "published": "2025-03-27T20:43:36+00:00",
    "summary": "Reinforcement learning (RL) methods typically learn new tasks from scratch, often disregarding prior knowledge that could accelerate the learning process. While some methods incorporate previously learned skills, they usually rely on a fixed structure, such as a single Gaussian distribution, to define skill priors. This rigid assumption can restrict the diversity and flexibility of skills, particularly in complex, long-horizon tasks. In this work, we introduce a method that models potential primitive skill motions as having non-parametric properties with an unknown number of underlying features. We utilize a Bayesian non-parametric model, specifically Dirichlet Process Mixtures, enhanced with birth and merge heuristics, to pre-train a skill prior that effectively captures the diverse nature of skills. Additionally, the learned skills are explicitly trackable within the prior space, enhancing interpretability and control. By integrating this flexible skill prior into an RL framework, our approach surpasses existing methods in long-horizon manipulation tasks, enabling more efficient skill transfer and task success in complex environments. Our findings show that a richer, non-parametric representation of skill priors significantly improves both the learning and execution of challenging robotic tasks. All data, code, and videos are available at https://ghiara.github.io/HELIOS/."
  },
  {
    "title": "Data-Agnostic Robotic Long-Horizon Manipulation with Vision-Language-Guided Closed-Loop Feedback",
    "url": "http://arxiv.org/abs/2503.21969v1",
    "arxiv_id": "2503.21969v1",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Haihui Ye",
      "Yirui Zhou",
      "Shengqiang Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "published": "2025-03-27T20:32:58+00:00",
    "summary": "Recent advances in language-conditioned robotic manipulation have leveraged imitation and reinforcement learning to enable robots to execute tasks from human commands. However, these methods often suffer from limited generalization, adaptability, and the lack of large-scale specialized datasets, unlike data-rich domains such as computer vision, making long-horizon task execution challenging. To address these gaps, we introduce DAHLIA, a data-agnostic framework for language-conditioned long-horizon robotic manipulation, leveraging large language models (LLMs) for real-time task planning and execution. DAHLIA employs a dual-tunnel architecture, where an LLM-powered planner collaborates with co-planners to decompose tasks and generate executable plans, while a reporter LLM provides closed-loop feedback, enabling adaptive re-planning and ensuring task recovery from potential failures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning and temporal abstraction for efficient action execution, enhancing traceability and robustness. Our framework demonstrates state-of-the-art performance across diverse long-horizon tasks, achieving strong generalization in both simulated and real-world scenarios. Videos and code are available at https://ghiara.github.io/DAHLIA/."
  },
  {
    "title": "Safety Verification and Optimization in Industrial Drive Systems",
    "url": "http://arxiv.org/abs/2503.21965v1",
    "arxiv_id": "2503.21965v1",
    "authors": [
      "Imran Riaz Hasrat",
      "Eun-Young Kang",
      "Christian Uldal Graulund"
    ],
    "published": "2025-03-27T20:27:19+00:00",
    "summary": "Safety and reliability are crucial in industrial drive systems, where hazardous failures can have severe consequences. Detecting and mitigating dangerous faults on time is challenging due to the stochastic and unpredictable nature of fault occurrences, which can lead to limited diagnostic efficiency and compromise safety. This paper optimizes the safety and diagnostic performance of a real-world industrial Basic Drive Module(BDM) using Uppaal Stratego. We model the functional safety architecture of the BDM with timed automata and formally verify its key functional and safety requirements through model checking to eliminate unwanted behaviors. Considering the formally verified correct model as a baseline, we leverage the reinforcement learning facility in Uppaal Stratego to optimize the safe failure fraction to the 90 % threshold, improving fault detection ability. The promising results highlight strong potential for broader safety applications in industrial automation."
  },
  {
    "title": "Reward Design for Reinforcement Learning Agents",
    "url": "http://arxiv.org/abs/2503.21949v1",
    "arxiv_id": "2503.21949v1",
    "authors": [
      "Rati Devidze"
    ],
    "published": "2025-03-27T19:48:02+00:00",
    "summary": "Reward functions are central in reinforcement learning (RL), guiding agents towards optimal decision-making. The complexity of RL tasks requires meticulously designed reward functions that effectively drive learning while avoiding unintended consequences. Effective reward design aims to provide signals that accelerate the agent's convergence to optimal behavior. Crafting rewards that align with task objectives, foster desired behaviors, and prevent undesirable actions is inherently challenging. This thesis delves into the critical role of reward signals in RL, highlighting their impact on the agent's behavior and learning dynamics and addressing challenges such as delayed, ambiguous, or intricate rewards. In this thesis work, we tackle different aspects of reward shaping. First, we address the problem of designing informative and interpretable reward signals from a teacher's/expert's perspective (teacher-driven). Here, the expert, equipped with the optimal policy and the corresponding value function, designs reward signals that expedite the agent's convergence to optimal behavior. Second, we build on this teacher-driven approach by introducing a novel method for adaptive interpretable reward design. In this scenario, the expert tailors the rewards based on the learner's current policy, ensuring alignment and optimal progression. Third, we propose a meta-learning approach, enabling the agent to self-design its reward signals online without expert input (agent-driven). This self-driven method considers the agent's learning and exploration to establish a self-improving feedback loop."
  },
  {
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "url": "http://arxiv.org/abs/2503.21776v1",
    "arxiv_id": "2503.21776v1",
    "authors": [
      "Kaituo Feng",
      "Kaixiong Gong",
      "Bohao Li",
      "Zonghao Guo",
      "Yibing Wang",
      "Tianshuo Peng",
      "Benyou Wang",
      "Xiangyu Yue"
    ],
    "published": "2025-03-27T17:59:51+00:00",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released."
  },
  {
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
    "url": "http://arxiv.org/abs/2503.21860v1",
    "arxiv_id": "2503.21860v1",
    "authors": [
      "Kailin Li",
      "Puhao Li",
      "Tengyu Liu",
      "Yuyang Li",
      "Siyuan Huang"
    ],
    "published": "2025-03-27T17:50:30+00:00",
    "summary": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments."
  },
  {
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
    "url": "http://arxiv.org/abs/2503.21729v1",
    "arxiv_id": "2503.21729v1",
    "authors": [
      "Zhicheng Lee",
      "Shulin Cao",
      "Jinxin Liu",
      "Jiajie Zhang",
      "Weichuan Liu",
      "Xiaoyin Che",
      "Lei Hou",
      "Juanzi Li"
    ],
    "published": "2025-03-27T17:44:18+00:00",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG)."
  },
  {
    "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
    "url": "http://arxiv.org/abs/2503.21720v1",
    "arxiv_id": "2503.21720v1",
    "authors": [
      "Souradip Chakraborty",
      "Sujay Bhatt",
      "Udari Madhushani Sehwag",
      "Soumya Suvra Ghosal",
      "Jiahao Qiu",
      "Mengdi Wang",
      "Dinesh Manocha",
      "Furong Huang",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "published": "2025-03-27T17:34:25+00:00",
    "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."
  },
  {
    "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21683v1",
    "arxiv_id": "2503.21683v1",
    "authors": [
      "Hui Wang"
    ],
    "published": "2025-03-27T16:52:25+00:00",
    "summary": "In recent years, large language models (LLMs) have shown significant advancements in natural language processing (NLP), with strong capa-bilities in generation, comprehension, and rea-soning. These models have found applications in education, intelligent decision-making, and gaming. However, effectively utilizing LLMs for strategic planning and decision-making in the game of Gomoku remains a challenge. This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to understand and apply Gomoku strat-egies and logic to make rational decisions. The research methods include enabling the model to \"read the board,\" \"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while en-hancing its abilities through self-play and rein-forcement learning. The results demonstrate that this approach significantly improves the se-lection of move positions, resolves the issue of generating illegal positions, and reduces pro-cess time through parallel position evaluation. After extensive self-play training, the model's Gomoku-playing capabilities have been notably enhanced."
  },
  {
    "title": "A tale of two goals: leveraging sequentiality in multi-goal scenarios",
    "url": "http://arxiv.org/abs/2503.21677v1",
    "arxiv_id": "2503.21677v1",
    "authors": [
      "Olivier Serris",
      "St\u00e9phane Doncieux",
      "Olivier Sigaud"
    ],
    "published": "2025-03-27T16:47:46+00:00",
    "summary": "Several hierarchical reinforcement learning methods leverage planning to create a graph or sequences of intermediate goals, guiding a lower-level goal-conditioned (GC) policy to reach some final goals. The low-level policy is typically conditioned on the current goal, with the aim of reaching it as quickly as possible. However, this approach can fail when an intermediate goal can be reached in multiple ways, some of which may make it impossible to continue toward subsequent goals. To address this issue, we introduce two instances of Markov Decision Process (MDP) where the optimization objective favors policies that not only reach the current goal but also subsequent ones. In the first, the agent is conditioned on both the current and final goals, while in the second, it is conditioned on the next two goals in the sequence. We conduct a series of experiments on navigation and pole-balancing tasks in which sequences of intermediate goals are given. By evaluating policies trained with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that, in most cases, conditioning on the next two goals improves stability and sample efficiency over other approaches."
  },
  {
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21620v1",
    "arxiv_id": "2503.21620v1",
    "authors": [
      "Zhengxi Lu",
      "Yuxiang Chai",
      "Yaxuan Guo",
      "Xi Yin",
      "Liang Liu",
      "Hao Wang",
      "Guanjing Xiong",
      "Hongsheng Li"
    ],
    "published": "2025-03-27T15:39:30+00:00",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain."
  },
  {
    "title": "A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols",
    "url": "http://arxiv.org/abs/2503.21601v1",
    "arxiv_id": "2503.21601v1",
    "authors": [
      "Johannes Voigt",
      "Peter Jiacheng Gu",
      "Peter Rost"
    ],
    "published": "2025-03-27T15:20:59+00:00",
    "summary": "The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility. This can lead to frequent radio link failures and reduced data rates. In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol. Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate. Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method."
  },
  {
    "title": "DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker Availability Windows",
    "url": "http://arxiv.org/abs/2503.21458v1",
    "arxiv_id": "2503.21458v1",
    "authors": [
      "Jinwen Chen",
      "Jiannan Guo",
      "Dazhuo Qiu",
      "Yawen Li",
      "Guanhua Ye",
      "Yan Zhao",
      "Kai Zheng"
    ],
    "published": "2025-03-27T12:46:12+00:00",
    "summary": "With the rapid advancement of mobile networks and the widespread use of mobile devices, spatial crowdsourcing, which involves assigning location-based tasks to mobile workers, has gained significant attention. However, most existing research focuses on task assignment at the current moment, overlooking the fluctuating demand and supply between tasks and workers over time. To address this issue, we introduce an adaptive task assignment problem, which aims to maximize the number of assigned tasks by dynamically adjusting task assignments in response to changing demand and supply. We develop a spatial crowdsourcing framework, namely demand-based adaptive task assignment with dynamic worker availability windows, which consists of two components including task demand prediction and task assignment. In the first component, we construct a graph adjacency matrix representing the demand dependency relationships in different regions and employ a multivariate time series learning approach to predict future task demands. In the task assignment component, we adjust tasks to workers based on these predictions, worker availability windows, and the current task assignments, where each worker has an availability window that indicates the time periods they are available for task assignments. To reduce the search space of task assignments and be efficient, we propose a worker dependency separation approach based on graph partition and a task value function with reinforcement learning. Experiments on real data demonstrate that our proposals are both effective and efficient."
  },
  {
    "title": "On Learning-Based Traffic Monitoring With a Swarm of Drones",
    "url": "http://arxiv.org/abs/2503.21433v1",
    "arxiv_id": "2503.21433v1",
    "authors": [
      "Marko Maljkovic",
      "Nikolas Geroliminis"
    ],
    "published": "2025-03-27T12:18:49+00:00",
    "summary": "Efficient traffic monitoring is crucial for managing urban transportation networks, especially under congested and dynamically changing traffic conditions. Drones offer a scalable and cost-effective alternative to fixed sensor networks. However, deploying fleets of low-cost drones for traffic monitoring poses challenges in adaptability, scalability, and real-time operation. To address these issues, we propose a learning-based framework for decentralized traffic monitoring with drone swarms, targeting the uneven and unpredictable distribution of monitoring needs across urban areas. Our approach introduces a semi-decentralized reinforcement learning model, which trains a single Q-function using the collective experience of the swarm. This model supports full scalability, flexible deployment, and, when hardware allows, the online adaptation of each drone's action-selection mechanism. We first train and evaluate the model in a synthetic traffic environment, followed by a case study using real traffic data from Shenzhen, China, to validate its performance and demonstrate its potential for real-world applications in complex urban monitoring tasks."
  },
  {
    "title": "AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control",
    "url": "http://arxiv.org/abs/2503.21401v1",
    "arxiv_id": "2503.21401v1",
    "authors": [
      "Tianyu Xu",
      "Yaoyu Cheng",
      "Pinxi Shen",
      "Lin Zhao",
      "Electrical",
      "Computer Engineering",
      "National University of Singapore",
      "Singapore",
      "Mechanical Engineering",
      "National University of Singapore",
      "Singapore"
    ],
    "published": "2025-03-27T11:47:20+00:00",
    "summary": "Quadrupedal robots can learn versatile locomotion skills but remain vulnerable when one or more joints lose power. In contrast, dogs and cats can adopt limping gaits when injured, demonstrating their remarkable ability to adapt to physical conditions. Inspired by such adaptability, this paper presents Action Learner (AcL), a novel teacher-student reinforcement learning framework that enables quadrupeds to autonomously adapt their gait for stable walking under multiple joint faults. Unlike conventional teacher-student approaches that enforce strict imitation, AcL leverages teacher policies to generate style rewards, guiding the student policy without requiring precise replication. We train multiple teacher policies, each corresponding to a different fault condition, and subsequently distill them into a single student policy with an encoder-decoder architecture. While prior works primarily address single-joint faults, AcL enables quadrupeds to walk with up to four faulty joints across one or two legs, autonomously switching between different limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot under single- and double-joint faults, demonstrating fault-tolerant, stable walking, smooth gait transitions between normal and lamb gaits, and robustness against external disturbances."
  },
  {
    "title": "AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control",
    "url": "http://arxiv.org/abs/2503.21401v2",
    "arxiv_id": "2503.21401v2",
    "authors": [
      "Tianyu Xu",
      "Yaoyu Cheng",
      "Pinxi Shen",
      "Lin Zhao"
    ],
    "published": "2025-03-27T11:47:20+00:00",
    "summary": "Quadrupedal robots can learn versatile locomotion skills but remain vulnerable when one or more joints lose power. In contrast, dogs and cats can adopt limping gaits when injured, demonstrating their remarkable ability to adapt to physical conditions. Inspired by such adaptability, this paper presents Action Learner (AcL), a novel teacher-student reinforcement learning framework that enables quadrupeds to autonomously adapt their gait for stable walking under multiple joint faults. Unlike conventional teacher-student approaches that enforce strict imitation, AcL leverages teacher policies to generate style rewards, guiding the student policy without requiring precise replication. We train multiple teacher policies, each corresponding to a different fault condition, and subsequently distill them into a single student policy with an encoder-decoder architecture. While prior works primarily address single-joint faults, AcL enables quadrupeds to walk with up to four faulty joints across one or two legs, autonomously switching between different limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot under single- and double-joint faults, demonstrating fault-tolerant, stable walking, smooth gait transitions between normal and lamb gaits, and robustness against external disturbances."
  },
  {
    "title": "Controlling Large Language Model with Latent Actions",
    "url": "http://arxiv.org/abs/2503.21383v1",
    "arxiv_id": "2503.21383v1",
    "authors": [
      "Chengxing Jia",
      "Ziniu Li",
      "Pengyuan Wang",
      "Yi-Chen Li",
      "Zhenyu Hou",
      "Yuxiao Dong",
      "Yang Yu"
    ],
    "published": "2025-03-27T11:25:22+00:00",
    "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA), a framework that integrates a latent action space into pre-trained LLMs. We apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, CoLA's latent action enables greater semantic diversity in text generation. For enhancing downstream tasks, we show that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, CoLA with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, CoLA reduces computation time by half in tasks involving enhanced thinking prompts for LLMs by RL. These results highlight CoLA's potential to advance RL-based adaptation of LLMs for downstream applications."
  },
  {
    "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.21257v1",
    "arxiv_id": "2503.21257v1",
    "authors": [
      "Yongxu Wang",
      "Weiyun Yi",
      "Xinhao Kong",
      "Wanting Li"
    ],
    "published": "2025-03-27T08:28:22+00:00",
    "summary": "With the rapid development of embodied intelligence, leveraging large-scale human data for high-level imitation learning on humanoid robots has become a focal point of interest in both academia and industry. However, applying humanoid robots to precision operation domains remains challenging due to the complexities they face in perception and control processes, the long-standing physical differences in morphology and actuation mechanisms between humanoid robots and humans, and the lack of task-relevant features obtained from egocentric vision. To address the issue of covariate shift in imitation learning, this paper proposes an imitation learning algorithm tailored for humanoid robots. By focusing on the primary task objectives, filtering out background information, and incorporating channel feature fusion with spatial attention mechanisms, the proposed algorithm suppresses environmental disturbances and utilizes a dynamic weight update strategy to significantly improve the success rate of humanoid robots in accomplishing target tasks. Experimental results demonstrate that the proposed method exhibits robustness and scalability across various typical task scenarios, providing new ideas and approaches for autonomous learning and control in humanoid robots. The project will be open-sourced on GitHub."
  },
  {
    "title": "Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection",
    "url": "http://arxiv.org/abs/2503.21099v1",
    "arxiv_id": "2503.21099v1",
    "authors": [
      "Yun Zhu",
      "Le Hui",
      "Hang Yang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ],
    "published": "2025-03-27T02:37:05+00:00",
    "summary": "Both indoor and outdoor scene perceptions are essential for embodied intelligence. However, current sparse supervised 3D object detection methods focus solely on outdoor scenes without considering indoor settings. To this end, we propose a unified sparse supervised 3D object detection method for both indoor and outdoor scenes through learning class prototypes to effectively utilize unlabeled objects. Specifically, we first propose a prototype-based object mining module that converts the unlabeled object mining into a matching problem between class prototypes and unlabeled features. By using optimal transport matching results, we assign prototype labels to high-confidence features, thereby achieving the mining of unlabeled objects. We then present a multi-label cooperative refinement module to effectively recover missed detections through pseudo label quality control and prototype label cooperation. Experiments show that our method achieves state-of-the-art performance under the one object per scene sparse supervised setting across indoor and outdoor datasets. With only one labeled object per scene, our method achieves about 78%, 90%, and 96% performance compared to the fully supervised detector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the scalability of our method. Code is available at https://github.com/zyrant/CPDet3D."
  },
  {
    "title": "Robust Quantum Control using Reinforcement Learning from Demonstration",
    "url": "http://arxiv.org/abs/2503.21085v1",
    "arxiv_id": "2503.21085v1",
    "authors": [
      "Shengyong Li",
      "Yidian Fan",
      "Xiang Li",
      "Xinhui Ruan",
      "Qianchuan Zhao",
      "Zhihui Peng",
      "Re-Bing Wu",
      "Jing Zhang",
      "Pengtao Song"
    ],
    "published": "2025-03-27T02:01:28+00:00",
    "summary": "Quantum control requires high-precision and robust control pulses to ensure optimal system performance. However, control sequences generated with a system model may suffer from model bias, leading to low fidelity. While model-free reinforcement learning (RL) methods have been developed to avoid such biases, training an RL agent from scratch can be time-consuming, often taking hours to gather enough samples for convergence. This challenge has hindered the broad application of RL techniques to larger and more complex quantum control issues, limiting their adaptability. In this work, we use Reinforcement Learning from Demonstration (RLfD) to leverage the control sequences generated with system models and further optimize them with RL to avoid model bias. By avoiding learning from scratch and starting with reasonable control pulse shapes, this approach can increase sample efficiency by reducing the number of samples, which can significantly reduce the training time. Thus, this method can effectively handle pulse shapes that are discretized into more than 1000 pieces without compromising final fidelity. We have simulated the preparation of several high-fidelity non-classical states using the RLfD method. We also find that the training process is more stable when using RLfD. In addition, this method is suitable for fast gate calibration using reinforcement learning."
  },
  {
    "title": "World Model Agents with Change-Based Intrinsic Motivation",
    "url": "http://arxiv.org/abs/2503.21047v1",
    "arxiv_id": "2503.21047v1",
    "authors": [
      "Jeremias Ferrao",
      "Rafael Cunha"
    ],
    "published": "2025-03-26T23:40:03+00:00",
    "summary": "Sparse reward environments pose a significant challenge for reinforcement learning due to the scarcity of feedback. Intrinsic motivation and transfer learning have emerged as promising strategies to address this issue. Change Based Exploration Transfer (CBET), a technique that combines these two approaches for model-free algorithms, has shown potential in addressing sparse feedback but its effectiveness with modern algorithms remains understudied. This paper provides an adaptation of CBET for world model algorithms like DreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both with and without CBET, in the sparse reward environments of Crafter and Minigrid. Our tabula rasa results highlight the possibility of CBET improving DreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in Minigrid with CBET further reducing returns. In the same vein, our transfer learning experiments show that pre-training DreamerV3 with intrinsic rewards does not immediately lead to a policy that maximizes extrinsic rewards in Minigrid. Overall, our results suggest that CBET provides a positive impact on DreamerV3 in more complex environments like Crafter but may be detrimental in environments like Minigrid. In the latter case, the behaviours promoted by CBET in DreamerV3 may not align with the task objectives of the environment, leading to reduced returns and suboptimal policies."
  },
  {
    "title": "AllReduce Scheduling with Hierarchical Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21013v1",
    "arxiv_id": "2503.21013v1",
    "authors": [
      "Yufan Wei",
      "Mickel Liu",
      "Wenfei Wu"
    ],
    "published": "2025-03-26T22:01:49+00:00",
    "summary": "AllReduce is a technique in distributed computing which saw use in many critical applications of deep learning. Existing methods of AllReduce scheduling oftentimes lack flexibility due to being topology-specific or relying on extensive handcrafted designs that require domain-specific knowledge. In this work, we aim to alleviate this inflexibility by proposing a deep-reinforcement-learning (DRL)-based pipeline that can generate AllReduce scheduling for various network topologies without topology-specific design features. The flow scheduling module of this pipeline consists of two hierarchically-structured DRL policies that work cooperatively to find optimal scheduling. We showcase the performance of our method compared to the baseline methods on three topologies: BCube, DCell, and Jellyfish. Finally, we contributed a Python-based simulation environment simulating AllReduce scheduling on these network topologies."
  },
  {
    "title": "Multi-head Reward Aggregation Guided by Entropy",
    "url": "http://arxiv.org/abs/2503.20995v1",
    "arxiv_id": "2503.20995v1",
    "authors": [
      "Xiaomin Li",
      "Xupeng Chen",
      "Jingxuan Fan",
      "Eric Hanchen Jiang",
      "Mingye Gao"
    ],
    "published": "2025-03-26T21:16:48+00:00",
    "summary": "Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, we introduce ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, we demonstrate that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying our entropy-based penalization. Through extensive experiments on RewardBench safety tasks, our method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. Our proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling."
  },
  {
    "title": "Reinforcement Learning for Efficient Toxicity Detection in Competitive Online Video Games",
    "url": "http://arxiv.org/abs/2503.20968v1",
    "arxiv_id": "2503.20968v1",
    "authors": [
      "Jacob Morrier",
      "Rafal Kocielnik",
      "R. Michael Alvarez"
    ],
    "published": "2025-03-26T20:13:30+00:00",
    "summary": "Online platforms take proactive measures to detect and address undesirable behavior, aiming to focus these resource-intensive efforts where such behavior is most prevalent. This article considers the problem of efficient sampling for toxicity detection in competitive online video games. To make optimal monitoring decisions, video game service operators need estimates of the likelihood of toxic behavior. If no model is available for these predictions, one must be estimated in real time. To close this gap, we propose a contextual bandit algorithm that makes monitoring decisions based on a small set of variables that, according to domain expertise, are associated with toxic behavior. This algorithm balances exploration and exploitation to optimize long-term outcomes and is deliberately designed for easy deployment in production. Using data from the popular first-person action game Call of Duty: Modern Warfare III, we show that our algorithm consistently outperforms baseline algorithms that rely solely on players' past behavior. This finding has substantive implications for the nature of toxicity. It also illustrates how domain expertise can be harnessed to help video game service operators identify and mitigate toxicity, ultimately fostering a safer and more enjoyable gaming experience."
  },
  {
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "url": "http://arxiv.org/abs/2503.20783v1",
    "arxiv_id": "2503.20783v1",
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "published": "2025-03-26T17:59:14+00:00",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero."
  },
  {
    "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
    "url": "http://arxiv.org/abs/2503.20752v2",
    "arxiv_id": "2503.20752v2",
    "authors": [
      "Huajie Tan",
      "Yuheng Ji",
      "Xiaoshuai Hao",
      "Minglan Lin",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "published": "2025-03-26T17:38:06+00:00",
    "summary": "Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the model's ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation. Experimental results demonstrate Reasoning-RFT's three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. Project website: https://tanhuajie.github.io/ReasonRFT"
  },
  {
    "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
    "url": "http://arxiv.org/abs/2503.20688v1",
    "arxiv_id": "2503.20688v1",
    "authors": [
      "Eloy Anguiano Batanero",
      "\u00c1ngela Fern\u00e1ndez",
      "\u00c1lvaro Barbero"
    ],
    "published": "2025-03-26T16:20:30+00:00",
    "summary": "The increasing complexity of power grid management, driven by the emergence of prosumers and the demand for cleaner energy solutions, has needed innovative approaches to ensure stability and efficiency. This paper presents a novel approach within the model-free framework of reinforcement learning, aimed at optimizing power network operations without prior expert knowledge. We introduce a masked topological action space, enabling agents to explore diverse strategies for cost reduction while maintaining reliable service using the state logic as a guide for choosing proper actions. Through extensive experimentation across 20 different scenarios in a simulated 5-substation environment, we demonstrate that our approach achieves a consistent reduction in power losses, while ensuring grid stability against potential blackouts. The results underscore the effectiveness of combining dynamic observation formalization with opponent-based training, showing a viable way for autonomous management solutions in modern energy systems or even for building a foundational model for this field."
  },
  {
    "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
    "url": "http://arxiv.org/abs/2503.20685v2",
    "arxiv_id": "2503.20685v2",
    "authors": [
      "Yuhao Huang",
      "Ao Chang",
      "Haoran Dou",
      "Xing Tao",
      "Xinrui Zhou",
      "Yan Cao",
      "Ruobing Huang",
      "Alejandro F Frangi",
      "Lingyun Bao",
      "Xin Yang",
      "Dong Ni"
    ],
    "published": "2025-03-26T16:20:02+00:00",
    "summary": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms."
  },
  {
    "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
    "url": "http://arxiv.org/abs/2503.20641v1",
    "arxiv_id": "2503.20641v1",
    "authors": [
      "Han Wu",
      "Yuxuan Yao",
      "Shuqi Liu",
      "Zehua Liu",
      "Xiaojin Fu",
      "Xiongwei Han",
      "Xing Li",
      "Hui-Ling Zhen",
      "Tao Zhong",
      "Mingxuan Yuan"
    ],
    "published": "2025-03-26T15:34:37+00:00",
    "summary": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging."
  },
  {
    "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
    "url": "http://arxiv.org/abs/2503.20844v1",
    "arxiv_id": "2503.20844v1",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui",
      "Yue Gao"
    ],
    "published": "2025-03-26T15:08:58+00:00",
    "summary": "Deep reinforcement learning (DRL) has emerged as a promising approach for robotic control, but its realworld deployment remains challenging due to its vulnerability to environmental perturbations. Existing white-box adversarial attack methods, adapted from supervised learning, fail to effectively target DRL agents as they overlook temporal dynamics and indiscriminately perturb all state dimensions, limiting their impact on long-term rewards. To address these challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR) Attack, a white-box attack method that combines DRL with a gradient-based soft masking mechanism to dynamically identify critical state dimensions and optimize adversarial policies. AGMR selectively allocates perturbations to the most impactful state features and incorporates a dynamic adjustment mechanism to balance exploration and exploitation during training. Extensive experiments demonstrate that AGMR outperforms state-of-the-art adversarial attack methods in degrading the performance of the victim agent and enhances the victim agent's robustness through adversarial defense mechanisms."
  },
  {
    "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20613v1",
    "arxiv_id": "2503.20613v1",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui"
    ],
    "published": "2025-03-26T15:00:07+00:00",
    "summary": "Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and state-specific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks."
  },
  {
    "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models",
    "url": "http://arxiv.org/abs/2503.20576v1",
    "arxiv_id": "2503.20576v1",
    "authors": [
      "Siyuan Guo",
      "Huiwu Liu",
      "Xiaolong Chen",
      "Yuming Xie",
      "Liang Zhang",
      "Tao Han",
      "Hechang Chen",
      "Yi Chang",
      "Jun Wang"
    ],
    "published": "2025-03-26T14:23:59+00:00",
    "summary": "In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs."
  },
  {
    "title": "TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion",
    "url": "http://arxiv.org/abs/2503.20839v1",
    "arxiv_id": "2503.20839v1",
    "authors": [
      "Amr Mousa",
      "Neil Karavis",
      "Michele Caprio",
      "Wei Pan",
      "Richard Allmendinger"
    ],
    "published": "2025-03-26T12:49:26+00:00",
    "summary": "Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed using the teacher-student paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between the privileged teacher and the proprioceptive-only student, covariate shift due to behavioral cloning, and lack of deployable adaptation lead to poor generalization in real-world scenarios. We propose Teacher-Aligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self-supervised contrastive learning to bridge this gap. By aligning representations to a privileged teacher in simulation via contrastive objectives, our student policy learns structured latent spaces and exhibits robust generalization to Out-of-Distribution (OOD) scenarios, surpassing the fully privileged \"Teacher\". Results showed accelerated training by 2x compared to state-of-the-art baselines to achieve peak performance. OOD scenarios showed better generalization by 40 percent on average compared to existing methods. Additionally, TAR transitions seamlessly into learning during deployment without requiring privileged states, setting a new benchmark in sample-efficient, adaptive locomotion and enabling continual fine-tuning in real-world scenarios. Open-source code and videos are available at https://ammousa.github.io/TARLoco/."
  },
  {
    "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems",
    "url": "http://arxiv.org/abs/2503.20507v1",
    "arxiv_id": "2503.20507v1",
    "authors": [
      "Rakesh Nadig",
      "Vamanan Arulchelvan",
      "Rahul Bera",
      "Taha Shahroodi",
      "Gagandeep Singh",
      "Mohammad Sadrosadati",
      "Jisung Park",
      "Onur Mutlu"
    ],
    "published": "2025-03-26T12:47:52+00:00",
    "summary": "Hybrid storage systems (HSS) combine multiple storage devices with diverse characteristics to achieve high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which rearranges stored data across the devices to sustain high HSS performance. Prior works focus on improving only data placement or only data migration in HSS, which leads to sub-optimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique for HSS that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two light-weight autonomous RL agents, a data-placement agent and a data-migration agent, which adapt their policies for the current workload and HSS configuration, and coordinate with each other to improve overall HSS performance. We evaluate Harmonia on a real HSS with up to four heterogeneous storage devices with diverse characteristics. Our evaluation using 17 data-intensive workloads on performance-optimized (cost-optimized) HSS with two storage devices shows that, on average, Harmonia (1) outperforms the best-performing prior approach by 49.5% (31.7%), (2) bridges the performance gap between the best-performing prior work and Oracle by 64.2% (64.3%). On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%). Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB for both RL agents together). We plan to open-source Harmonia's implementation to aid future research on HSS."
  },
  {
    "title": "Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement Learning for Connected Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.20462v1",
    "arxiv_id": "2503.20462v1",
    "authors": [
      "Ruoqi Wen",
      "Rongpeng Li",
      "Xing Xu",
      "Zhifeng Zhao"
    ],
    "published": "2025-03-26T11:49:02+00:00",
    "summary": "Deep Reinforcement Learning (DRL) holds significant promise for achieving human-like Autonomous Vehicle (AV) capabilities, but suffers from low sample efficiency and challenges in reward design. Model-Based Reinforcement Learning (MBRL) offers improved sample efficiency and generalizability compared to Model-Free Reinforcement Learning (MFRL) in various multi-agent decision-making scenarios. Nevertheless, MBRL faces critical difficulties in estimating uncertainty during the model learning phase, thereby limiting its scalability and applicability in real-world scenarios. Additionally, most Connected Autonomous Vehicle (CAV) studies focus on single-agent decision-making, while existing multi-agent MBRL solutions lack computationally tractable algorithms with Probably Approximately Correct (PAC) guarantees, an essential factor for ensuring policy reliability with limited training data. To address these challenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-Based Reinforcement Learning framework for CAVs, incorporating a max-min optimization approach to enhance robustness and decision-making. To mitigate the inherent subjectivity of uncertainty estimation in MBRL and avoid incurring catastrophic failures in AV, MA-PMBRL employs a pessimistic optimization framework combined with Projected Gradient Descent (PGD) for both model and policy learning. MA-PMBRL also employs general function approximations under partial dataset coverage to enhance learning efficiency and system-level performance. By bounding the suboptimality of the resulting policy under mild theoretical assumptions, we successfully establish PAC guarantees for MA-PMBRL, demonstrating that the proposed framework represents a significant step toward scalable, efficient, and reliable multi-agent decision-making for CAVs."
  },
  {
    "title": "The Crucial Role of Problem Formulation in Real-World Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20442v1",
    "arxiv_id": "2503.20442v1",
    "authors": [
      "Georg Sch\u00e4fer",
      "Tatjana Krau",
      "Jakob Rehrl",
      "Stefan Huber",
      "Simon Hirlaender"
    ],
    "published": "2025-03-26T11:17:44+00:00",
    "summary": "Reinforcement Learning (RL) offers promising solutions for control tasks in industrial cyber-physical systems (ICPSs), yet its real-world adoption remains limited. This paper demonstrates how seemingly small but well-designed modifications to the RL problem formulation can substantially improve performance, stability, and sample efficiency. We identify and investigate key elements of RL problem formulation and show that these enhance both learning speed and final policy quality. Our experiments use a one-degree-of-freedom (1-DoF) helicopter testbed, the Quanser Aero~2, which features non-linear dynamics representative of many industrial settings. In simulation, the proposed problem design principles yield more reliable and efficient training, and we further validate these results by training the agent directly on physical hardware. The encouraging real-world outcomes highlight the potential of RL for ICPS, especially when careful attention is paid to the design principles of problem formulation. Overall, our study underscores the crucial role of thoughtful problem formulation in bridging the gap between RL research and the demands of real-world industrial systems."
  },
  {
    "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
    "url": "http://arxiv.org/abs/2503.20425v1",
    "arxiv_id": "2503.20425v1",
    "authors": [
      "Kevin Alcedo",
      "Pedro U. Lima",
      "Rachid Alami"
    ],
    "published": "2025-03-26T10:59:08+00:00",
    "summary": "Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings."
  },
  {
    "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
    "url": "http://arxiv.org/abs/2503.20285v1",
    "arxiv_id": "2503.20285v1",
    "authors": [
      "Hongye Cao",
      "Fan Feng",
      "Jing Huo",
      "Shangdong Yang",
      "Meng Fang",
      "Tianpei Yang",
      "Yang Gao"
    ],
    "published": "2025-03-26T07:24:34+00:00",
    "summary": "Model-based offline Reinforcement Learning (RL) constructs environment models from offline datasets to perform conservative policy optimization. Existing approaches focus on learning state transitions through ensemble models, rollouting conservative estimation to mitigate extrapolation errors. However, the static data makes it challenging to develop a robust policy, and offline agents cannot access the environment to gather new data. To address these challenges, we introduce Model-based Offline Reinforcement learning with AdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon rollout by employing adversaria data augmentation to execute alternating sampling with ensemble models to enrich training data. Specifically, this adversarial process dynamically selects ensemble models against policy for biased sampling, mitigating the optimistic estimation of fixed models, thus robustly expanding the training data for policy optimization. Moreover, a differential factor is integrated into the adversarial process for regularization, ensuring error minimization in extrapolations. This data-augmented optimization adapts to diverse offline tasks without rollout horizon tuning, showing remarkable applicability. Extensive experiments on D4RL benchmark demonstrate that MORAL outperforms other model-based offline RL methods in terms of policy learning and sample efficiency."
  },
  {
    "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
    "url": "http://arxiv.org/abs/2503.20208v1",
    "arxiv_id": "2503.20208v1",
    "authors": [
      "Liangzhi Shi",
      "Yulin Liu",
      "Lingqi Zeng",
      "Bo Ai",
      "Zhengdong Hong",
      "Hao Su"
    ],
    "published": "2025-03-26T04:05:50+00:00",
    "summary": "How can robots learn dexterous grasping skills efficiently and apply them adaptively based on user instructions? This work tackles two key challenges: efficient skill acquisition from limited human demonstrations and context-driven skill selection. We introduce AdaDexGrasp, a framework that learns a library of grasping skills from a single human demonstration per skill and selects the most suitable one using a vision-language model (VLM). To improve sample efficiency, we propose a trajectory following reward that guides reinforcement learning (RL) toward states close to a human demonstration while allowing flexibility in exploration. To learn beyond the single demonstration, we employ curriculum learning, progressively increasing object pose variations to enhance robustness. At deployment, a VLM retrieves the appropriate skill based on user instructions, bridging low-level learned skills with high-level intent. We evaluate AdaDexGrasp in both simulation and real-world settings, showing that our approach significantly improves RL efficiency and enables learning human-like grasp strategies across varied object configurations. Finally, we demonstrate zero-shot transfer of our learned policies to a real-world PSYONIC Ability Hand, with a 90% success rate across objects, significantly outperforming the baseline."
  },
  {
    "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control",
    "url": "http://arxiv.org/abs/2503.20205v1",
    "arxiv_id": "2503.20205v1",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang",
      "Xiang-Ling Chen"
    ],
    "published": "2025-03-26T04:03:12+00:00",
    "summary": "Appropriate traffic state representation is crucial for learning traffic signal control policies. However, most of the current traffic state representations are heuristically designed, with insufficient theoretical support. In this paper, we (1) develop a flexible, efficient, and theoretically grounded method, namely generalized phase pressure (G2P) control, which takes only simple lane features into consideration to decide which phase to be actuated; 2) extend the pressure control theory to a general form for multi-homogeneous-lane road networks based on queueing theory; (3) design a new traffic state representation based on the generalized phase state features from G2P control; and 4) develop a reinforcement learning (RL)-based algorithm template named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight, by combining the generalized phase state representation with MPLight and CoLight, two well-performed RL methods for learning traffic signal control policies. Extensive experiments conducted on multiple real-world datasets demonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic method in the transportation field and other recent human-designed heuristic methods; and that the newly proposed G2P-XLight significantly outperforms SOTA learning-based approaches. Our code is available online."
  },
  {
    "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization",
    "url": "http://arxiv.org/abs/2503.20194v1",
    "arxiv_id": "2503.20194v1",
    "authors": [
      "Zhouhong Gu",
      "Xingzhou Chen",
      "Xiaoran Shi",
      "Tao Wang",
      "Suhang Zheng",
      "Tianyu Li",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "published": "2025-03-26T03:37:52+00:00",
    "summary": "Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in https://github.com/MikeGu721/GAPO."
  },
  {
    "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
    "url": "http://arxiv.org/abs/2503.20176v1",
    "arxiv_id": "2503.20176v1",
    "authors": [
      "RuiXi Qiao",
      "Jie Cheng",
      "Xingyuan Dai",
      "Yonglin Tian",
      "Yisheng Lv"
    ],
    "published": "2025-03-26T03:04:42+00:00",
    "summary": "Skills have been introduced to offline reinforcement learning (RL) as temporal abstractions to tackle complex, long-horizon tasks, promoting consistent behavior and enabling meaningful exploration. While skills in offline RL are predominantly modeled within a continuous latent space, the potential of discrete skill spaces remains largely underexplored. In this paper, we propose a compact discrete skill space for offline RL tasks supported by state-of-the-art transformer-based encoder and diffusion-based decoder. Coupled with a high-level policy trained via offline RL techniques, our method establishes a hierarchical RL framework where the trained diffusion decoder plays a pivotal role. Empirical evaluations show that the proposed algorithm, Discrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs competitively on Locomotion and Kitchen tasks and excels on long-horizon tasks, achieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared to existing offline RL approaches. Furthermore, DDS offers improved interpretability, training stability, and online exploration compared to previous skill-based methods."
  },
  {
    "title": "Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20139v1",
    "arxiv_id": "2503.20139v1",
    "authors": [
      "Yongshuai Liu",
      "Xin Liu"
    ],
    "published": "2025-03-26T01:07:35+00:00",
    "summary": "Model-based reinforcement learning (MBRL) has demonstrated superior sample efficiency compared to model-free reinforcement learning (MFRL). However, the presence of inaccurate models can introduce biases during policy learning, resulting in misleading trajectories. The challenge lies in obtaining accurate models due to limited diverse training data, particularly in regions with limited visits (uncertain regions). Existing approaches passively quantify uncertainty after sample generation, failing to actively collect uncertain samples that could enhance state coverage and improve model accuracy. Moreover, MBRL often faces difficulties in making accurate multi-step predictions, thereby impacting overall performance. To address these limitations, we propose a novel framework for uncertainty-aware policy optimization with model-based exploratory planning. In the model-based planning phase, we introduce an uncertainty-aware k-step lookahead planning approach to guide action selection at each step. This process involves a trade-off analysis between model uncertainty and value function approximation error, effectively enhancing policy performance. In the policy optimization phase, we leverage an uncertainty-driven exploratory policy to actively collect diverse training samples, resulting in improved model accuracy and overall performance of the RL agent. Our approach offers flexibility and applicability to tasks with varying state/action spaces and reward structures. We validate its effectiveness through experiments on challenging robotic manipulation tasks and Atari games, surpassing state-of-the-art methods with fewer interactions, thereby leading to significant performance improvements."
  },
  {
    "title": "Synthesizing world models for bilevel planning",
    "url": "http://arxiv.org/abs/2503.20124v1",
    "arxiv_id": "2503.20124v1",
    "authors": [
      "Zergham Ahmed",
      "Joshua B. Tenenbaum",
      "Christopher J. Bates",
      "Samuel J. Gershman"
    ],
    "published": "2025-03-26T00:10:01+00:00",
    "summary": "Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - \"theories\" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., \"move to\"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions."
  },
  {
    "title": "Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20078v1",
    "arxiv_id": "2503.20078v1",
    "authors": [
      "Volkan Ustun",
      "Soham Hans",
      "Rajay Kumar",
      "Yunzhe Wang"
    ],
    "published": "2025-03-25T21:29:49+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in training dynamic and adaptive synthetic characters for interactive simulations on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make such reinforcement learning experiments more accessible to the simulation community. Military training simulations also benefit from advances in MARL, but they have immense computational requirements due to their complex, continuous, stochastic, partially observable, non-stationary, and doctrine-based nature. Furthermore, these simulations require geo-specific terrains, further exacerbating the computational resources problem. In our research, we leverage Unity's waypoints to automatically generate multi-layered representation abstractions of the geo-specific terrains to scale up reinforcement learning while still allowing the transfer of learned policies between different representations. Our early exploratory results on a novel MARL scenario, where each side has differing objectives, indicate that waypoint-based navigation enables faster and more efficient learning while producing trajectories similar to those taken by expert human players in CSGO gaming environments. This research points out the potential of waypoint-based navigation for reducing the computational costs of developing and training MARL models for military training simulations, where geo-specific terrains and differing objectives are crucial."
  },
  {
    "title": "Unsupervised Learning for Quadratic Assignment",
    "url": "http://arxiv.org/abs/2503.20001v1",
    "arxiv_id": "2503.20001v1",
    "authors": [
      "Yimeng Min",
      "Carla P. Gomes"
    ],
    "published": "2025-03-25T18:37:46+00:00",
    "summary": "We introduce PLUME search, a data-driven framework that enhances search efficiency in combinatorial optimization through unsupervised learning. Unlike supervised or reinforcement learning, PLUME search learns directly from problem instances using a permutation-based loss with a non-autoregressive approach. We evaluate its performance on the quadratic assignment problem, a fundamental NP-hard problem that encompasses various combinatorial optimization problems. Experimental results demonstrate that PLUME search consistently improves solution quality. Furthermore, we study the generalization behavior and show that the learned model generalizes across different densities and sizes."
  },
  {
    "title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback",
    "url": "http://arxiv.org/abs/2503.19988v1",
    "arxiv_id": "2503.19988v1",
    "authors": [
      "Bohan Zhai",
      "Canwen Xu",
      "Yuxiong He",
      "Zhewei Yao"
    ],
    "published": "2025-03-25T18:17:36+00:00",
    "summary": "Text-to-SQL demands precise reasoning to convert natural language questions into structured queries. While large language models (LLMs) excel in many reasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for text-to-SQL remains underexplored. We identify critical limitations: zero-shot CoT offers minimal gains, and Direct Preference Optimization (DPO) applied without CoT yields marginal improvements. We propose ExCoT, a novel framework that iteratively optimizes open-source LLMs by combining CoT reasoning with off-policy and on-policy DPO, relying solely on execution accuracy as feedback. This approach eliminates the need for reward models or human-annotated preferences.   Our experimental results demonstrate significant performance gains: ExCoT improves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider test set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder demonstrating similar improvements. Our best model achieves state-of-the-art performance in the single-model setting on both BIRD and Spider datasets, notably achieving 68.53% on the BIRD test set."
  },
  {
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking",
    "url": "http://arxiv.org/abs/2503.19855v1",
    "arxiv_id": "2503.19855v1",
    "authors": [
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yunjie Ji",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-03-25T17:19:38+00:00",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer."
  },
  {
    "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
    "url": "http://arxiv.org/abs/2503.19948v1",
    "arxiv_id": "2503.19948v1",
    "authors": [
      "Alexander Gambashidze",
      "Konstantin Sobolev",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "published": "2025-03-25T15:30:21+00:00",
    "summary": "Can Visual Language Models (VLMs) effectively capture human visual preferences? This work addresses this question by training VLMs to think about preferences at test time, employing reinforcement learning methods inspired by DeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human Preference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the ImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2 (trained on approximately 25% of its data). These results match traditional encoder-based models while providing transparent reasoning and enhanced generalization. This approach allows to use not only rich VLM world knowledge, but also its potential to think, yielding interpretable outcomes that help decision-making processes. By demonstrating that human visual preferences reasonable by current VLMs, we introduce efficient soft-reward strategies for image ranking, outperforming simplistic selection or scoring methods. This reasoning capability enables VLMs to rank arbitrary images-regardless of aspect ratio or complexity-thereby potentially amplifying the effectiveness of visual Preference Optimization. By reducing the need for extensive markup while improving reward generalization and explainability, our findings can be a strong mile-stone that will enhance text-to-vision models even further."
  },
  {
    "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
    "url": "http://arxiv.org/abs/2503.19699v1",
    "arxiv_id": "2503.19699v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-03-25T14:27:29+00:00",
    "summary": "In this study, we formulate the drone delivery problem as a control problem and solve it using Model Predictive Control. Two experiments are performed: The first is on a less challenging grid world environment with lower dimensionality, and the second is with a higher dimensionality and added complexity. The MPC method was benchmarked against three popular Multi-Agent Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the MPC method solved the problem quicker and required fewer optimal numbers of drones to achieve a minimized cost and navigate the optimal path."
  },
  {
    "title": "Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection",
    "url": "http://arxiv.org/abs/2503.19690v2",
    "arxiv_id": "2503.19690v2",
    "authors": [
      "Bo Leng",
      "Ran Yu",
      "Wei Han",
      "Lu Xiong",
      "Zhuoren Li",
      "Hailong Huang"
    ],
    "published": "2025-03-25T14:17:15+00:00",
    "summary": "Applying reinforcement learning to autonomous driving has garnered widespread attention. However, classical reinforcement learning methods optimize policies by maximizing expected rewards but lack sufficient safety considerations, often putting agents in hazardous situations. This paper proposes a risk-aware reinforcement learning approach for autonomous driving to improve the safety performance when crossing the intersection. Safe critics are constructed to evaluate driving risk and work in conjunction with the reward critic to update the actor. Based on this, a Lagrangian relaxation method and cyclic gradient iteration are combined to project actions into a feasible safe region. Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention Mechanism (MMAM) is incorporated into the actor-critic network, enabling the policy to adapt to dynamic traffic and overcome permutation sensitivity challenges. This allows the policy to focus more effectively on surrounding potential risks while enhancing the identification of passing opportunities. Simulation tests are conducted on different tasks at unsignalized intersections. The results show that the proposed approach effectively reduces collision rates and improves crossing efficiency in comparison to baseline algorithms. Additionally, our ablation experiments demonstrate the benefits of incorporating risk-awareness and MMAM into RL."
  },
  {
    "title": "Learning to chain-of-thought with Jensen's evidence lower bound",
    "url": "http://arxiv.org/abs/2503.19618v1",
    "arxiv_id": "2503.19618v1",
    "authors": [
      "Yunhao Tang",
      "Sid Wang",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T13:03:09+00:00",
    "summary": "We propose a way to optimize chain-of-thought with reinforcement learning, but without external reward function. Our algorithm relies on viewing chain-of-thought as latent variable as part of a probabilistic inference problem. Contrary to the full evidence lower bound, we propose to apply a much simpler Jensen's lower bound, which derives tractable objectives with simple algorithmic components (e.g., without the need for parametric approximate posterior), making it more conducive to modern large-scale training. The lower bound approach naturally interpolates other methods such as supervised fine-tuning and online reinforcement learning, whose practical trade-offs we will illustrate. Finally, we show that on mathematical reasoning problems, optimizing with Jensen's lower bound is as effective as policy gradient with external reward. Taken together, our results showcase as a proof of concept to this new algorithmic paradigm's potential to more generic applications."
  },
  {
    "title": "RL-finetuning LLMs from on- and off-policy data with a single algorithm",
    "url": "http://arxiv.org/abs/2503.19612v1",
    "arxiv_id": "2503.19612v1",
    "authors": [
      "Yunhao Tang",
      "Taco Cohen",
      "David W. Zhang",
      "Michal Valko",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T12:52:38+00:00",
    "summary": "We introduce a novel reinforcement learning algorithm (AGRO, for Any-Generation Reward Optimization) for fine-tuning large-language models. AGRO leverages the concept of generation consistency, which states that the optimal policy satisfies the notion of consistency across any possible generation of the model. We derive algorithms that find optimal solutions via the sample-based policy gradient and provide theoretical guarantees on their convergence. Our experiments demonstrate the effectiveness of AGRO in both on-policy and off-policy settings, showing improved performance on the mathematical reasoning dataset over baseline algorithms."
  },
  {
    "title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.19595v1",
    "arxiv_id": "2503.19595v1",
    "authors": [
      "Yunhao Tang",
      "Kunhao Zheng",
      "Gabriel Synnaeve",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T12:21:26+00:00",
    "summary": "In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic inference time objectives with $k$ samples, with a focus on pass@$k$ and majority voting as two main applications. With language model training on reasoning datasets, we showcase the performance trade-off enabled by training with such objectives. When training on code generation tasks, we show that the approach significantly improves pass@$k$ objectives compared to the baseline method."
  },
  {
    "title": "ZodiAq: An Isotropic Flagella-Inspired Soft Underwater Drone for Safe Marine Exploration",
    "url": "http://arxiv.org/abs/2503.19556v1",
    "arxiv_id": "2503.19556v1",
    "authors": [
      "Anup Teejo Mathew",
      "Daniel Feliu-Talegon",
      "Yusuf Abdullahi Adamu",
      "Ikhlas Ben Hmida",
      "Costanza Armanini",
      "Cesare Stefanini",
      "Lakmal Seneviratne",
      "Federico Renda"
    ],
    "published": "2025-03-25T11:23:31+00:00",
    "summary": "The inherent challenges of robotic underwater exploration, such as hydrodynamic effects, the complexity of dynamic coupling, and the necessity for sensitive interaction with marine life, call for the adoption of soft robotic approaches in marine exploration. To address this, we present a novel prototype, ZodiAq, a soft underwater drone inspired by prokaryotic bacterial flagella. ZodiAq's unique dodecahedral structure, equipped with 12 flagella-like arms, ensures design redundancy and compliance, ideal for navigating complex underwater terrains. The prototype features a central unit based on a Raspberry Pi, connected to a sensory system for inertial, depth, and vision detection, and an acoustic modem for communication. Combined with the implemented control law, it renders ZodiAq an intelligent system. This paper details the design and fabrication process of ZodiAq, highlighting design choices and prototype capabilities. Based on the strain-based modeling of Cosserat rods, we have developed a digital twin of the prototype within a simulation toolbox to ease analysis and control. To optimize its operation in dynamic aquatic conditions, a simplified model-based controller has been developed and implemented, facilitating intelligent and adaptive movement in the hydrodynamic environment. Extensive experimental demonstrations highlight the drone's potential, showcasing its design redundancy, embodied intelligence, crawling gait, and practical applications in diverse underwater settings. This research contributes significantly to the field of underwater soft robotics, offering a promising new avenue for safe, efficient, and environmentally conscious underwater exploration."
  },
  {
    "title": "One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF",
    "url": "http://arxiv.org/abs/2503.19523v2",
    "arxiv_id": "2503.19523v2",
    "authors": [
      "Xin Cai"
    ],
    "published": "2025-03-25T10:23:26+00:00",
    "summary": "In this article, we primarily examine a variety of RL-based and RL-free methods designed to address Reinforcement Learning from Human Feedback (RLHF) and Large Reasoning Models (LRMs). We begin with a concise overview of the typical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based and RL-free algorithms through the perspective of neural structured bandit prediction, providing a clear conceptual framework that uncovers a deeper connection between these seemingly distinct approaches. Following this, we briefly review some core principles of reinforcement learning, drawing attention to an often-overlooked aspect in existing RLHF studies. This leads to a detailed derivation of the standard RLHF objective within a full RL context, demonstrating its equivalence to neural structured bandit prediction. Finally, by reinvestigating the principles behind Proximal Policy Optimization (PPO), we pinpoint areas needing adjustment, which culminates in the introduction of the Generalized Reinforce Optimization (GRO) framework, seamlessly integrating RL-based and RL-free methods in RLHF. We look forward to the community's efforts to empirically validate GRO and invite constructive feedback."
  }
]
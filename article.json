[
  {
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
    "url": "http://arxiv.org/abs/2503.24376v1",
    "arxiv_id": "2503.24376v1",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Rui Wang",
      "Yixiao Ge",
      "Lu Qiu",
      "Ying Shan",
      "Xihui Liu"
    ],
    "published": "2025-03-31T17:55:23+00:00",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals."
  },
  {
    "title": "Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24296v1",
    "arxiv_id": "2503.24296v1",
    "authors": [
      "Yubo Zhang",
      "Pedro Botelho",
      "Trevor Gordon",
      "Gil Zussman",
      "Igor Kadota"
    ],
    "published": "2025-03-31T16:42:11+00:00",
    "summary": "We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average."
  },
  {
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "url": "http://arxiv.org/abs/2503.24290v1",
    "arxiv_id": "2503.24290v1",
    "authors": [
      "Jingcheng Hu",
      "Yinmin Zhang",
      "Qi Han",
      "Daxin Jiang",
      "Xiangyu Zhang",
      "Heung-Yeung Shum"
    ],
    "published": "2025-03-31T16:36:05+00:00",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes."
  },
  {
    "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24289v1",
    "arxiv_id": "2503.24289v1",
    "authors": [
      "Jiacheng Lin",
      "Tian Wang",
      "Kun Qian"
    ],
    "published": "2025-03-31T16:36:00+00:00",
    "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting."
  },
  {
    "title": "Moving Edge for On-Demand Edge Computing: An Uncertainty-aware Approach",
    "url": "http://arxiv.org/abs/2503.24214v1",
    "arxiv_id": "2503.24214v1",
    "authors": [
      "Fangtong Zhou",
      "Ruozhou Yu"
    ],
    "published": "2025-03-31T15:32:05+00:00",
    "summary": "We study an edge demand response problem where, based on historical edge workload demands, an edge provider needs to dispatch moving computing units, e.g. truck-carried modular data centers, in response to emerging hotspots within service area. The goal of edge provider is to maximize the expected revenue brought by serving congested users with satisfactory performance, while minimizing the costs of moving units and the potential service-level agreement violation penalty for interrupted services. The challenge is to make robust predictions for future demands, as well as optimized moving unit dispatching decisions. We propose a learning-based, uncertain-aware moving unit scheduling framework, URANUS, to address this problem. Our framework novelly combines Bayesian deep learning and distributionally robust approximation to make predictions that are robust to data, model and distributional uncertainties in deep learning-based prediction models. Based on the robust prediction outputs, we further propose an efficient planning algorithm to optimize moving unit scheduling in an online manner. Simulation experiments show that URANUS can significantly improve robustness in decision making, and achieve superior performance compared to state-of-the-art reinforcement learning, uncertainty-agnostic learning-based methods, and other baselines."
  },
  {
    "title": "Ride-Sourcing Vehicle Rebalancing with Service Accessibility Guarantees via Constrained Mean-Field Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24183v1",
    "arxiv_id": "2503.24183v1",
    "authors": [
      "Matej Jusup",
      "Kenan Zhang",
      "Zhiyuan Hu",
      "Barna P\u00e1sztor",
      "Andreas Krause",
      "Francesco Corman"
    ],
    "published": "2025-03-31T15:00:11+00:00",
    "summary": "The rapid expansion of ride-sourcing services such as Uber, Lyft, and Didi Chuxing has fundamentally reshaped urban transportation by offering flexible, on-demand mobility via mobile applications. Despite their convenience, these platforms confront significant operational challenges, particularly vehicle rebalancing - the strategic repositioning of thousands of vehicles to address spatiotemporal mismatches in supply and demand. Inadequate rebalancing results in prolonged rider waiting times, inefficient vehicle utilization, and inequitable distribution of services, leading to disparities in driver availability and income.   To tackle these complexities, we introduce scalable continuous-state mean-field control (MFC) and reinforcement learning (MFRL) models that explicitly represent each vehicle's precise location and employ continuous repositioning actions guided by the distribution of other vehicles. To ensure equitable service distribution, an accessibility constraint is integrated within our optimal control formulation, balancing operational efficiency with equitable access to the service across geographic regions. Our approach acknowledges realistic conditions, including inherent stochasticity in transitions, the simultaneous occurrence of vehicle-rider matching, vehicles' rebalancing and cruising, and variability in rider behaviors. Crucially, we relax the traditional mean-field assumption of equal supply-demand volume, better reflecting practical scenarios. Extensive empirical evaluation using real-world data-driven simulation of Shenzhen demonstrates the real-time efficiency and robustness of our approach at the scale of tens of thousands of vehicles.   The code is available at https://github.com/mjusup1501/mf-vehicle-rebalancing."
  },
  {
    "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings",
    "url": "http://arxiv.org/abs/2503.24150v1",
    "arxiv_id": "2503.24150v1",
    "authors": [
      "Kailas Vodrahalli",
      "Wei Wei",
      "James Zou"
    ],
    "published": "2025-03-31T14:35:48+00:00",
    "summary": "Recent advances in generative AI have been driven by alignment techniques such as reinforcement learning from human feedback (RLHF). RLHF and related techniques typically involve constructing a dataset of binary or ranked choice human preferences and subsequently fine-tuning models to align with these preferences. This paper shifts the focus to understanding the preferences encoded in such datasets and identifying common human preferences. We find that a small subset of 21 preference categories (selected from a set of nearly 5,000 distinct preferences) captures >89% of preference variation across individuals. This small set of preferences is analogous to a canonical basis of human preferences, similar to established findings that characterize human variation in psychology or facial recognition studies. Through both synthetic and empirical evaluations, we confirm that our low-rank, canonical set of human preferences generalizes across the entire dataset and within specific topics. We further demonstrate our preference basis' utility in model evaluation, where our preference categories offer deeper insights into model alignment, and in model training, where we show that fine-tuning on preference-defined subsets successfully aligns the model accordingly."
  },
  {
    "title": "Reinforcement Learning for Safe Autonomous Two Device Navigation of Cerebral Vessels in Mechanical Thrombectomy",
    "url": "http://arxiv.org/abs/2503.24140v1",
    "arxiv_id": "2503.24140v1",
    "authors": [
      "Harry Robertshaw",
      "Benjamin Jackson",
      "Jiaheng Wang",
      "Hadi Sadati",
      "Lennart Karstensen",
      "Alejandro Granados",
      "Thomas C Booth"
    ],
    "published": "2025-03-31T14:25:46+00:00",
    "summary": "Purpose: Autonomous systems in mechanical thrombectomy (MT) hold promise for reducing procedure times, minimizing radiation exposure, and enhancing patient safety. However, current reinforcement learning (RL) methods only reach the carotid arteries, are not generalizable to other patient vasculatures, and do not consider safety. We propose a safe dual-device RL algorithm that can navigate beyond the carotid arteries to cerebral vessels.   Methods: We used the Simulation Open Framework Architecture to represent the intricacies of cerebral vessels, and a modified Soft Actor-Critic RL algorithm to learn, for the first time, the navigation of micro-catheters and micro-guidewires. We incorporate patient safety metrics into our reward function by integrating guidewire tip forces. Inverse RL is used with demonstrator data on 12 patient-specific vascular cases.   Results: Our simulation demonstrates successful autonomous navigation within unseen cerebral vessels, achieving a 96% success rate, 7.0s procedure time, and 0.24 N mean forces, well below the proposed 1.5 N vessel rupture threshold.   Conclusion: To the best of our knowledge, our proposed autonomous system for MT two-device navigation reaches cerebral vessels, considers safety, and is generalizable to unseen patient-specific cases for the first time. We envisage future work will extend the validation to vasculatures of different complexity and on in vitro models. While our contributions pave the way towards deploying agents in clinical settings, safety and trustworthiness will be crucial elements to consider when proposing new methodology."
  },
  {
    "title": "Level the Level: Balancing Game Levels for Asymmetric Player Archetypes With Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.24099v1",
    "arxiv_id": "2503.24099v1",
    "authors": [
      "Florian Rupp",
      "Kai Eckert"
    ],
    "published": "2025-03-31T13:55:04+00:00",
    "summary": "Balancing games, especially those with asymmetric multiplayer content, requires significant manual effort and extensive human playtesting during development. For this reason, this work focuses on generating balanced levels tailored to asymmetric player archetypes, where the disparity in abilities is balanced entirely through the level design. For instance, while one archetype may have an advantage over another, both should have an equal chance of winning. We therefore conceptualize game balancing as a procedural content generation problem and build on and extend a recently introduced method that uses reinforcement learning to balance tile-based game levels. We evaluate the method on four different player archetypes and demonstrate its ability to balance a larger proportion of levels compared to two baseline approaches. Furthermore, our results indicate that as the disparity between player archetypes increases, the required number of training steps grows, while the model's accuracy in achieving balance decreases."
  },
  {
    "title": "HACTS: a Human-As-Copilot Teleoperation System for Robot Learning",
    "url": "http://arxiv.org/abs/2503.24070v1",
    "arxiv_id": "2503.24070v1",
    "authors": [
      "Zhiyuan Xu",
      "Yinuo Zhao",
      "Kun Wu",
      "Ning Liu",
      "Junjie Ji",
      "Zhengping Che",
      "Chi Harold Liu",
      "Jian Tang"
    ],
    "published": "2025-03-31T13:28:13+00:00",
    "summary": "Teleoperation is essential for autonomous robot learning, especially in manipulation tasks that require human demonstrations or corrections. However, most existing systems only offer unilateral robot control and lack the ability to synchronize the robot's status with the teleoperation hardware, preventing real-time, flexible intervention. In this work, we introduce HACTS (Human-As-Copilot Teleoperation System), a novel system that establishes bilateral, real-time joint synchronization between a robot arm and teleoperation hardware. This simple yet effective feedback mechanism, akin to a steering wheel in autonomous vehicles, enables the human copilot to intervene seamlessly while collecting action-correction data for future learning. Implemented using 3D-printed components and low-cost, off-the-shelf motors, HACTS is both accessible and scalable. Our experiments show that HACTS significantly enhances performance in imitation learning (IL) and reinforcement learning (RL) tasks, boosting IL recovery capabilities and data efficiency, and facilitating human-in-the-loop RL. HACTS paves the way for more effective and interactive human-robot collaboration and data-collection, advancing the capabilities of robot manipulation."
  },
  {
    "title": "Impact of Amplitude and Phase Damping Noise on Quantum Reinforcement Learning: Challenges and Opportunities",
    "url": "http://arxiv.org/abs/2503.24069v1",
    "arxiv_id": "2503.24069v1",
    "authors": [
      "Mar\u00eda Laura Olivera-Atencio",
      "Lucas Lamata",
      "Jes\u00fas Casado-Pascual"
    ],
    "published": "2025-03-31T13:27:30+00:00",
    "summary": "Quantum machine learning (QML) is an emerging field with significant potential, yet it remains highly susceptible to noise, which poses a major challenge to its practical implementation. While various noise mitigation strategies have been proposed to enhance algorithmic performance, the impact of noise is not fully understood. In this work, we investigate the effects of amplitude and phase damping noise on a quantum reinforcement learning algorithm. Through analytical and numerical analysis, we assess how these noise sources influence the learning process and overall performance. Our findings contribute to a deeper understanding of the role of noise in quantum learning algorithms and suggest that, rather than being purely detrimental, unavoidable noise may present opportunities to enhance QML processes."
  },
  {
    "title": "A Reactive Framework for Whole-Body Motion Planning of Mobile Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic Programmi",
    "url": "http://arxiv.org/abs/2503.23975v1",
    "arxiv_id": "2503.23975v1",
    "authors": [
      "Chenyu Zhang",
      "Shiying Sun",
      "Kuan Liu",
      "Chuanbao Zhou",
      "Xiaoguang Zhao",
      "Min Tan",
      "Yanlong Huang"
    ],
    "published": "2025-03-31T11:37:02+00:00",
    "summary": "As an important branch of embodied artificial intelligence, mobile manipulators are increasingly applied in intelligent services, but their redundant degrees of freedom also limit efficient motion planning in cluttered environments. To address this issue, this paper proposes a hybrid learning and optimization framework for reactive whole-body motion planning of mobile manipulators. We develop the Bayesian distributional soft actor-critic (Bayes-DSAC) algorithm to improve the quality of value estimation and the convergence performance of the learning. Additionally, we introduce a quadratic programming method constrained by the signed distance field to enhance the safety of the obstacle avoidance motion. We conduct experiments and make comparison with standard benchmark. The experimental results verify that our proposed framework significantly improves the efficiency of reactive whole-body motion planning, reduces the planning time, and improves the success rate of motion planning. Additionally, the proposed reinforcement learning method ensures a rapid learning process in the whole-body planning task. The novel framework allows mobile manipulators to adapt to complex environments more safely and efficiently."
  },
  {
    "title": "Noise-based reward-modulated learning",
    "url": "http://arxiv.org/abs/2503.23972v1",
    "arxiv_id": "2503.23972v1",
    "authors": [
      "Jes\u00fas Garc\u00eda Fern\u00e1ndez",
      "Nasir Ahmad",
      "Marcel van Gerven"
    ],
    "published": "2025-03-31T11:35:23+00:00",
    "summary": "Recent advances in reinforcement learning (RL) have led to significant improvements in task performance. However, training neural networks in an RL regime is typically achieved in combination with backpropagation, limiting their applicability in resource-constrained environments or when using non-differentiable neural networks. While noise-based alternatives like reward-modulated Hebbian learning (RMHL) have been proposed, their performance has remained limited, especially in scenarios with delayed rewards, which require retrospective credit assignment over time. Here, we derive a novel noise-based learning rule that addresses these challenges. Our approach combines directional derivative theory with Hebbian-like updates to enable efficient, gradient-free learning in RL. It features stochastic noisy neurons which can approximate gradients, and produces local synaptic updates modulated by a global reward signal. Drawing on concepts from neuroscience, our method uses reward prediction error as its optimization target to generate increasingly advantageous behavior, and incorporates an eligibility trace to facilitate temporal credit assignment in environments with delayed rewards. Its formulation relies on local information alone, making it compatible with implementations in neuromorphic hardware. Experimental validation shows that our approach significantly outperforms RMHL and is competitive with BP-based baselines, highlighting the promise of noise-based, biologically inspired learning for low-power and real-time applications."
  },
  {
    "title": "MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented Experience Replay for Robot Navigation",
    "url": "http://arxiv.org/abs/2503.23908v1",
    "arxiv_id": "2503.23908v1",
    "authors": [
      "Shanze Wang",
      "Mingao Tan",
      "Zhibo Yang",
      "Biao Huang",
      "Xiaoyu Shen",
      "Hailong Huang",
      "Wei Zhang"
    ],
    "published": "2025-03-31T09:58:28+00:00",
    "summary": "Deep Reinforcement Learning (DRL) based navigation methods have demonstrated promising results for mobile robots, but suffer from limited action flexibility in confined spaces. Conventional DRL approaches predominantly learn forward-motion policies, causing robots to become trapped in complex environments where backward maneuvers are necessary for recovery. This paper presents MAER-Nav (Mirror-Augmented Experience Replay for Robot Navigation), a novel framework that enables bidirectional motion learning without requiring explicit failure-driven hindsight experience replay or reward function modifications. Our approach integrates a mirror-augmented experience replay mechanism with curriculum learning to generate synthetic backward navigation experiences from successful trajectories. Experimental results in both simulation and real-world environments demonstrate that MAER-Nav significantly outperforms state-of-the-art methods while maintaining strong forward navigation capabilities. The framework effectively bridges the gap between the comprehensive action space utilization of traditional planning methods and the environmental adaptability of learning-based approaches, enabling robust navigation in scenarios where conventional DRL methods consistently fail."
  },
  {
    "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO",
    "url": "http://arxiv.org/abs/2503.23905v1",
    "arxiv_id": "2503.23905v1",
    "authors": [
      "Qihan Huang",
      "Long Chan",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jingyuan Chen",
      "Chang Yao",
      "Jie Song"
    ],
    "published": "2025-03-31T09:54:55+00:00",
    "summary": "MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at https://github.com/hqhQAQ/Hint-GRPO."
  },
  {
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "url": "http://arxiv.org/abs/2503.23829v1",
    "arxiv_id": "2503.23829v1",
    "authors": [
      "Yi Su",
      "Dian Yu",
      "Linfeng Song",
      "Juntao Li",
      "Haitao Mi",
      "Zhaopeng Tu",
      "Min Zhang",
      "Dong Yu"
    ],
    "published": "2025-03-31T08:22:49+00:00",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels."
  },
  {
    "title": "Accelerating High-Efficiency Organic Photovoltaic Discovery via Pretrained Graph Neural Networks and Generative Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23766v1",
    "arxiv_id": "2503.23766v1",
    "authors": [
      "Jiangjie Qiu",
      "Hou Hei Lam",
      "Xiuyuan Hu",
      "Wentao Li",
      "Siwei Fu",
      "Fankun Zeng",
      "Hao Zhang",
      "Xiaonan Wang"
    ],
    "published": "2025-03-31T06:31:15+00:00",
    "summary": "Organic photovoltaic (OPV) materials offer a promising avenue toward cost-effective solar energy utilization. However, optimizing donor-acceptor (D-A) combinations to achieve high power conversion efficiency (PCE) remains a significant challenge. In this work, we propose a framework that integrates large-scale pretraining of graph neural networks (GNNs) with a GPT-2 (Generative Pretrained Transformer 2)-based reinforcement learning (RL) strategy to design OPV molecules with potentially high PCE. This approach produces candidate molecules with predicted efficiencies approaching 21\\%, although further experimental validation is required. Moreover, we conducted a preliminary fragment-level analysis to identify structural motifs recognized by the RL model that may contribute to enhanced PCE, thus providing design guidelines for the broader research community. To facilitate continued discovery, we are building the largest open-source OPV dataset to date, expected to include nearly 3,000 donor-acceptor pairs. Finally, we discuss plans to collaborate with experimental teams on synthesizing and characterizing AI-designed molecules, which will provide new data to refine and improve our predictive and generative models."
  },
  {
    "title": "The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning",
    "url": "http://arxiv.org/abs/2503.23679v1",
    "arxiv_id": "2503.23679v1",
    "authors": [
      "Mingkai Tian",
      "Guorong Li",
      "Yuankai Qi",
      "Amin Beheshti",
      "Javen Qinfeng Shi",
      "Anton van den Hengel",
      "Qingming Huang"
    ],
    "published": "2025-03-31T03:00:19+00:00",
    "summary": "Zero-shot video captioning requires that a model generate high-quality captions without human-annotated video-text pairs for training. State-of-the-art approaches to the problem leverage CLIP to extract visual-relevant textual prompts to guide language models in generating captions. These methods tend to focus on one key aspect of the scene and build a caption that ignores the rest of the visual input. To address this issue, and generate more accurate and complete captions, we propose a novel progressive multi-granularity textual prompting strategy for zero-shot video captioning. Our approach constructs three distinct memory banks, encompassing noun phrases, scene graphs of noun phrases, and entire sentences. Moreover, we introduce a category-aware retrieval mechanism that models the distribution of natural language surrounding the specific topics in question. Extensive experiments demonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4% improvements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX benchmarks compared to existing state-of-the-art."
  },
  {
    "title": "Multi-Agent Deep Reinforcement Learning for Optimized Multi-UAV Coverage and Power-Efficient UE Connectivity",
    "url": "http://arxiv.org/abs/2503.23669v1",
    "arxiv_id": "2503.23669v1",
    "authors": [
      "Xuli Cai",
      "Poonam Lohan",
      "Burak Kantarci"
    ],
    "published": "2025-03-31T02:26:23+00:00",
    "summary": "In critical situations such as natural disasters, network outages, battlefield communication, or large-scale public events, Unmanned Aerial Vehicles (UAVs) offer a promising approach to maximize wireless coverage for affected users in the shortest possible time. In this paper, we propose a novel framework where multiple UAVs are deployed with the objective to maximize the number of served user equipment (UEs) while ensuring a predefined data rate threshold. UEs are initially clustered using a K-means algorithm, and UAVs are optimally positioned based on the UEs' spatial distribution. To optimize power allocation and mitigate inter-cluster interference, we employ the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm, considering both LOS and NLOS fading. Simulation results demonstrate that our method significantly enhances UEs coverage and outperforms Deep Q-Network (DQN) and equal power distribution methods, improving their UE coverage by up to 2.07 times and 8.84 times, respectively."
  },
  {
    "title": "Dynamic Operating System Scheduling Using Double DQN: A Reinforcement Learning Approach to Task Optimization",
    "url": "http://arxiv.org/abs/2503.23659v1",
    "arxiv_id": "2503.23659v1",
    "authors": [
      "Xiaoxuan Sun",
      "Yifei Duan",
      "Yingnan Deng",
      "Fan Guo",
      "Guohui Cai",
      "Yuting Peng"
    ],
    "published": "2025-03-31T01:48:21+00:00",
    "summary": "In this paper, an operating system scheduling algorithm based on Double DQN (Double Deep Q network) is proposed, and its performance under different task types and system loads is verified by experiments. Compared with the traditional scheduling algorithm, the algorithm based on Double DQN can dynamically adjust the task priority and resource allocation strategy, thus improving the task completion efficiency, system throughput, and response speed. The experimental results show that the Double DQN algorithm has high scheduling performance under light load, medium load and heavy load scenarios, especially when dealing with I/O intensive tasks, and can effectively reduce task completion time and system response time. In addition, the algorithm also shows high optimization ability in resource utilization and can intelligently adjust resource allocation according to the system state, avoiding resource waste and excessive load. Future studies will further explore the application of the algorithm in more complex systems, especially scheduling optimization in cloud computing and large-scale distributed environments, combining factors such as network latency and energy efficiency to improve the overall performance and adaptability of the algorithm."
  },
  {
    "title": "A Survey of Reinforcement Learning-Based Motion Planning for Autonomous Driving: Lessons Learned from a Driving Task Perspective",
    "url": "http://arxiv.org/abs/2503.23650v1",
    "arxiv_id": "2503.23650v1",
    "authors": [
      "Zhuoren Li",
      "Guizhe Jin",
      "Ran Yu",
      "Zhiwen Chen",
      "Nan Li",
      "Wei Han",
      "Lu Xiong",
      "Bo Leng",
      "Jia Hu",
      "Ilya Kolmanovsky",
      "Dimitar Filev"
    ],
    "published": "2025-03-31T01:31:14+00:00",
    "summary": "Reinforcement learning (RL), with its ability to explore and optimize policies in complex, dynamic decision-making tasks, has emerged as a promising approach to addressing motion planning (MoP) challenges in autonomous driving (AD). Despite rapid advancements in RL and AD, a systematic description and interpretation of the RL design process tailored to diverse driving tasks remains underdeveloped. This survey provides a comprehensive review of RL-based MoP for AD, focusing on lessons from task-specific perspectives. We first outline the fundamentals of RL methodologies, and then survey their applications in MoP, analyzing scenario-specific features and task requirements to shed light on their influence on RL design choices. Building on this analysis, we summarize key design experiences, extract insights from various driving task applications, and provide guidance for future implementations. Additionally, we examine the frontier challenges in RL-based MoP, review recent efforts to addresse these challenges, and propose strategies for overcoming unresolved issues."
  },
  {
    "title": "A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control",
    "url": "http://arxiv.org/abs/2503.23626v1",
    "arxiv_id": "2503.23626v1",
    "authors": [
      "Anirudh Satheesh",
      "Keenan Powell"
    ],
    "published": "2025-03-30T23:29:48+00:00",
    "summary": "Traffic congestion in modern cities is exacerbated by the limitations of traditional fixed-time traffic signal systems, which fail to adapt to dynamic traffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have emerged as a solution by dynamically adjusting signal timing based on real-time traffic conditions. However, the main limitation of such methods is that they are not transferable to environments under real-world constraints, such as balancing efficiency, minimizing collisions, and ensuring fairness across intersections. In this paper, we view the ATSC problem as a constrained multi-agent reinforcement learning (MARL) problem and propose a novel algorithm named Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE) to produce effective traffic signal control policies. Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment. We also introduce three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which penalize traffic policies that do not conform to real-world scenarios. Our experimental results on three real-world datasets demonstrate that MAPPO-LCE outperforms three baseline MARL algorithms by across all environments and traffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by 13.10%). Our results show that constrained MARL is a valuable tool for traffic planners to deploy scalable and efficient ATSC methods in real-world traffic networks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE."
  },
  {
    "title": "An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23615v1",
    "arxiv_id": "2503.23615v1",
    "authors": [
      "Julien Soul\u00e9",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Th\u00e9ron"
    ],
    "published": "2025-03-30T22:43:01+00:00",
    "summary": "Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents."
  },
  {
    "title": "Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models",
    "url": "http://arxiv.org/abs/2503.23487v1",
    "arxiv_id": "2503.23487v1",
    "authors": [
      "Irtaza Khalid",
      "Amir Masoud Nourollah",
      "Steven Schockaert"
    ],
    "published": "2025-03-30T15:41:55+00:00",
    "summary": "Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is still known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond problem solving in mathematics and programming, where finding genuine out-of-distribution problems can be difficult. In this paper, we focus on tasks that require systematic reasoning about relational compositions, especially for qualitative spatial and temporal reasoning. These tasks allow us to control the difficulty of problem instances, and measure in a precise way to what extent models can generalise. We find that that the considered LLMs and LRMs overall perform poorly overall, albeit better than random chance."
  },
  {
    "title": "A Systematic Decade Review of Trip Route Planning with Travel Time Estimation based on User Preferences and Behavior",
    "url": "http://arxiv.org/abs/2503.23486v1",
    "arxiv_id": "2503.23486v1",
    "authors": [
      "Nikil Jayasuriya",
      "Deshan Sumanathilaka"
    ],
    "published": "2025-03-30T15:41:44+00:00",
    "summary": "This paper systematically explores the advancements in adaptive trip route planning and travel time estimation (TTE) through Artificial Intelligence (AI). With the increasing complexity of urban transportation systems, traditional navigation methods often struggle to accommodate dynamic user preferences, real-time traffic conditions, and scalability requirements. This study explores the contributions of established AI techniques, including Machine Learning (ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside emerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI, and Federated Learning. In addition to highlighting these innovations, the paper identifies critical challenges such as ethical concerns, computational scalability, and effective data integration, which must be addressed to advance the field. The paper concludes with recommendations for leveraging AI to build efficient, transparent, and sustainable navigation systems."
  },
  {
    "title": "Handling Delay in Real-Time Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.23478v1",
    "arxiv_id": "2503.23478v1",
    "authors": [
      "Ivan Anokhin",
      "Rishav Rishav",
      "Matthew Riemer",
      "Stephen Chung",
      "Irina Rish",
      "Samira Ebrahimi Kahou"
    ],
    "published": "2025-03-30T15:30:27+00:00",
    "summary": "Real-time reinforcement learning (RL) introduces several challenges. First, policies are constrained to a fixed number of actions per second due to hardware limitations. Second, the environment may change while the network is still computing an action, leading to observational delay. The first issue can partly be addressed with pipelining, leading to higher throughput and potentially better policies. However, the second issue remains: if each neuron operates in parallel with an execution time of $\\tau$, an $N$-layer feed-forward network experiences observation delay of $\\tau N$. Reducing the number of layers can decrease this delay, but at the cost of the network's expressivity. In this work, we explore the trade-off between minimizing delay and network's expressivity. We present a theoretically motivated solution that leverages temporal skip connections combined with history-augmented observations. We evaluate several architectures and show that those incorporating temporal skip connections achieve strong performance across various neuron execution times, reinforcement learning algorithms, and environments, including four Mujoco tasks and all MinAtar games. Moreover, we demonstrate parallel neuron computation can accelerate inference by 6-350% on standard hardware. Our investigation into temporal skip connections and parallel computations paves the way for more efficient RL agents in real-time setting."
  },
  {
    "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach",
    "url": "http://arxiv.org/abs/2503.23459v1",
    "arxiv_id": "2503.23459v1",
    "authors": [
      "Chenglong Lu",
      "Shen Liang",
      "Xuewei Wang",
      "Wei Wang"
    ],
    "published": "2025-03-30T14:34:28+00:00",
    "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically with the number of tokens, calling for effective token pruning policies. Most existing policies are handcrafted, lacking adaptivity to varying inputs. Moreover, they fail to consider the sequential nature of token pruning across multiple layers. In this work, for the first time (as far as we know), we exploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy. Formulating token pruning as a sequential decision-making problem, we model it as a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO) where each agent makes an individualized pruning decision for a single token. We also develop reward functions that enable simultaneous collaboration and competition of these agents to balance efficiency and accuracy. On the well-known ImageNet-1k dataset, our method improves the inference speed by up to 44% while incurring only a negligible accuracy drop of 0.4%. The source code is available at https://github.com/daashuai/rl4evit."
  },
  {
    "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents",
    "url": "http://arxiv.org/abs/2503.23427v1",
    "arxiv_id": "2503.23427v1",
    "authors": [
      "Wenhan Liu",
      "Xinyu Ma",
      "Yutao Zhu",
      "Lixin Su",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Zhicheng Dou"
    ],
    "published": "2025-03-30T13:00:52+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \\textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\\%) while achieving even better effectiveness compared to using only the LLM listwise reranker."
  },
  {
    "title": "ToRL: Scaling Tool-Integrated RL",
    "url": "http://arxiv.org/abs/2503.23383v1",
    "arxiv_id": "2503.23383v1",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "published": "2025-03-30T10:16:25+00:00",
    "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for training large language models (LLMs) to autonomously use computational tools via reinforcement learning. Unlike supervised fine-tuning, ToRL allows models to explore and discover optimal strategies for tool use. Experiments with Qwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\% accuracy on AIME~24, surpassing reinforcement learning without tool integration by 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%. Further analysis reveals emergent behaviors such as strategic tool invocation, self-regulation of ineffective code, and dynamic adaptation between computational and analytical reasoning, all arising purely through reward-driven learning."
  },
  {
    "title": "FeRG-LLM : Feature Engineering by Reason Generation Large Language Models",
    "url": "http://arxiv.org/abs/2503.23371v1",
    "arxiv_id": "2503.23371v1",
    "authors": [
      "Jeonghyun Ko",
      "Gyeongyun Park",
      "Donghoon Lee",
      "Kyunam Lee"
    ],
    "published": "2025-03-30T09:07:21+00:00",
    "summary": "One of the key tasks in machine learning for tabular data is feature engineering. Although it is vital for improving the performance of models, it demands considerable human expertise and deep domain knowledge, making it labor-intensive endeavor. To address this issue, we propose a novel framework, \\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason \\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large language model designed to automatically perform feature engineering at an 8-billion-parameter scale. We have constructed two-stage conversational dialogues that enable language models to analyze machine learning tasks and discovering new features, exhibiting their Chain-of-Thought (CoT) capabilities. We use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct Preference Optimization (DPO) to receive feedback improving quality of new features and the model's performance. Our experiments show that FeRG-LLM performs comparably to or better than Llama 3.1 70B on most datasets, while using fewer resources and achieving reduced inference time. It outperforms other studies in classification tasks and performs well in regression tasks. Moreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API costs when generating features, it can be deployed locally, addressing security concerns."
  },
  {
    "title": "Reinforcement Learning for Active Matter",
    "url": "http://arxiv.org/abs/2503.23308v1",
    "arxiv_id": "2503.23308v1",
    "authors": [
      "Wenjie Cai",
      "Gongyi Wang",
      "Yu Zhang",
      "Xiang Qu",
      "Zihan Huang"
    ],
    "published": "2025-03-30T04:27:17+00:00",
    "summary": "Active matter refers to systems composed of self-propelled entities that consume energy to produce motion, exhibiting complex non-equilibrium dynamics that challenge traditional models. With the rapid advancements in machine learning, reinforcement learning (RL) has emerged as a promising framework for addressing the complexities of active matter. This review systematically introduces the integration of RL for guiding and controlling active matter systems, focusing on two key aspects: optimal motion strategies for individual active particles and the regulation of collective dynamics in active swarms. We discuss the use of RL to optimize the navigation, foraging, and locomotion strategies for individual active particles. In addition, the application of RL in regulating collective behaviors is also examined, emphasizing its role in facilitating the self-organization and goal-directed control of active swarms. This investigation offers valuable insights into how RL can advance the understanding, manipulation, and control of active matter, paving the way for future developments in fields such as biological systems, robotics, and medical science."
  },
  {
    "title": "SalesRLAgent: A Reinforcement Learning Approach for Real-Time Sales Conversion Prediction and Optimization",
    "url": "http://arxiv.org/abs/2503.23303v1",
    "arxiv_id": "2503.23303v1",
    "authors": [
      "Nandakishor M"
    ],
    "published": "2025-03-30T03:56:26+00:00",
    "summary": "Current approaches to sales conversation analysis and conversion prediction typically rely on Large Language Models (LLMs) combined with basic retrieval augmented generation (RAG). These systems, while capable of answering questions, fail to accurately predict conversion probability or provide strategic guidance in real time. In this paper, we present SalesRLAgent, a novel framework leveraging specialized reinforcement learning to predict conversion probability throughout sales conversations. Unlike systems from Kapa.ai, Mendable, Inkeep, and others that primarily use off-the-shelf LLMs for content generation, our approach treats conversion prediction as a sequential decision problem, training on synthetic data generated using GPT-4O to develop a specialized probability estimation model. Our system incorporates Azure OpenAI embeddings (3072 dimensions), turn-by-turn state tracking, and meta-learning capabilities to understand its own knowledge boundaries. Evaluations demonstrate that SalesRLAgent achieves 96.7% accuracy in conversion prediction, outperforming LLM-only approaches by 34.7% while offering significantly faster inference (85ms vs 3450ms for GPT-4). Furthermore, integration with existing sales platforms shows a 43.2% increase in conversion rates when representatives utilize our system's real-time guidance. SalesRLAgent represents a fundamental shift from content generation to strategic sales intelligence, providing moment-by-moment conversion probability estimation with actionable insights for sales professionals."
  },
  {
    "title": "Efficient Twin Migration in Vehicular Metaverses: Multi-Agent Split Deep Reinforcement Learning with Spatio-Temporal Trajectory Generation",
    "url": "http://arxiv.org/abs/2503.23290v1",
    "arxiv_id": "2503.23290v1",
    "authors": [
      "Junlong Chen",
      "Jiawen Kang",
      "Minrui Xu",
      "Fan Wu",
      "Hongliang Zhang",
      "Huawei Huang",
      "Dusit Niyato",
      "Shiwen Mao"
    ],
    "published": "2025-03-30T03:00:01+00:00",
    "summary": "Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users' immersive experiences in vehicular metaverses."
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Graph Discovery in D2D-Enabled Federated Learning",
    "url": "http://arxiv.org/abs/2503.23218v1",
    "arxiv_id": "2503.23218v1",
    "authors": [
      "Satyavrat Wagle",
      "Anindya Bijoy Das",
      "David J. Love",
      "Christopher G. Brinton"
    ],
    "published": "2025-03-29T20:42:26+00:00",
    "summary": "Augmenting federated learning (FL) with device-to-device (D2D) communications can help improve convergence speed and reduce model bias through local information exchange. However, data privacy concerns, trust constraints between devices, and unreliable wireless channels each pose challenges in finding an effective yet resource efficient D2D graph structure. In this paper, we develop a decentralized reinforcement learning (RL) method for D2D graph discovery that promotes communication of impactful datapoints over reliable links for multiple learning paradigms, while following both data and device-specific trust constraints. An independent RL agent at each device trains a policy to predict the impact of incoming links in a decentralized manner without exposure of local data or significant communication overhead. For supervised settings, the D2D graph aims to improve device-specific label diversity without compromising system-level performance. For semi-supervised settings, we enable this by incorporating distributed label propagation. For unsupervised settings, we develop a variation-based diversity metric which estimates data diversity in terms of occupied latent space. Numerical experiments on five widely used datasets confirm that the data diversity improvements induced by our method increase convergence speed by up to 3 times while reducing energy consumption by up to 5 times. They also show that our method is resilient to stragglers and changes in the aggregation interval. Finally, we show that our method offers scalability benefits for larger system sizes without increases in relative overhead, and adaptability to various downstream FL architectures and to dynamic wireless environments."
  },
  {
    "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL",
    "url": "http://arxiv.org/abs/2503.23157v1",
    "arxiv_id": "2503.23157v1",
    "authors": [
      "Mohammadreza Pourreza",
      "Shayan Talaei",
      "Ruoxi Sun",
      "Xingchen Wan",
      "Hailong Li",
      "Azalia Mirhoseini",
      "Amin Saberi",
      "Sercan \"O. Arik"
    ],
    "published": "2025-03-29T17:29:30+00:00",
    "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks."
  },
  {
    "title": "Dexterous Non-Prehensile Manipulation for Ungraspable Object via Extrinsic Dexterity",
    "url": "http://arxiv.org/abs/2503.23120v1",
    "arxiv_id": "2503.23120v1",
    "authors": [
      "Yuhan Wang",
      "Yu Li",
      "Yaodong Yang",
      "Yuanpei Chen"
    ],
    "published": "2025-03-29T15:21:41+00:00",
    "summary": "Objects with large base areas become ungraspable when they exceed the end-effector's maximum aperture. Existing approaches address this limitation through extrinsic dexterity, which exploits environmental features for non-prehensile manipulation. While grippers have shown some success in this domain, dexterous hands offer superior flexibility and manipulation capabilities that enable richer environmental interactions, though they present greater control challenges. Here we present ExDex, a dexterous arm-hand system that leverages reinforcement learning to enable non-prehensile manipulation for grasping ungraspable objects. Our system learns two strategic manipulation sequences: relocating objects from table centers to edges for direct grasping, or to walls where extrinsic dexterity enables grasping through environmental interaction. We validate our approach through extensive experiments with dozens of diverse household objects, demonstrating both superior performance and generalization capabilities with novel objects. Furthermore, we successfully transfer the learned policies from simulation to a real-world robot system without additional training, further demonstrating its applicability in real-world scenarios. Project website: https://tangty11.github.io/ExDex/."
  },
  {
    "title": "RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations",
    "url": "http://arxiv.org/abs/2503.23101v1",
    "arxiv_id": "2503.23101v1",
    "authors": [
      "Enrico Marchesini",
      "Benjamin Donnot",
      "Constance Crozier",
      "Ian Dytham",
      "Christian Merz",
      "Lars Schewe",
      "Nico Westerbeck",
      "Cathy Wu",
      "Antoine Marot",
      "Priya L. Donti"
    ],
    "published": "2025-03-29T14:39:17+00:00",
    "summary": "Reinforcement learning (RL) can transform power grid operations by providing adaptive and scalable controllers essential for grid decarbonization. However, existing methods struggle with the complex dynamics, aleatoric uncertainty, long-horizon goals, and hard physical constraints that occur in real-world systems. This paper presents RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on a power simulation framework developed by RTE France, RL2Grid standardizes tasks, state and action spaces, and reward structures within a unified interface for a systematic evaluation and comparison of RL approaches. Moreover, we integrate real control heuristics and safety constraints informed by the operators' expertise to ensure RL2Grid aligns with grid operation requirements. We benchmark popular RL baselines on the grid control tasks represented within RL2Grid, establishing reference performance metrics. Our results and discussion highlight the challenges that power grids pose for RL methods, emphasizing the need for novel algorithms capable of handling real-world physical systems."
  },
  {
    "title": "Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22958v1",
    "arxiv_id": "2503.22958v1",
    "authors": [
      "Supriyo Maji",
      "Linran Zhao",
      "Souradip Poddar",
      "David Z. Pan"
    ],
    "published": "2025-03-29T03:13:56+00:00",
    "summary": "Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing."
  },
  {
    "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning",
    "url": "http://arxiv.org/abs/2503.22948v1",
    "arxiv_id": "2503.22948v1",
    "authors": [
      "Tianyang Xu",
      "Xiaoze Liu",
      "Feijie Wu",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "published": "2025-03-29T02:33:26+00:00",
    "summary": "Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications."
  },
  {
    "title": "Adaptive Interactive Navigation of Quadruped Robots using Large Language Models",
    "url": "http://arxiv.org/abs/2503.22942v1",
    "arxiv_id": "2503.22942v1",
    "authors": [
      "Kangjie Zhou",
      "Yao Mu",
      "Haoyang Song",
      "Yi Zeng",
      "Pengying Wu",
      "Han Gao",
      "Chang Liu"
    ],
    "published": "2025-03-29T02:17:52+00:00",
    "summary": "Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within free space, struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this gap, we propose an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to reach originally unavailable goals. Specifically, we present a primitive tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning method featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan modification in unknown environments. Comprehensive simulations and experiments have demonstrated our method's effectiveness and adaptivity in diverse scenarios. The supplementary video is available at page: https://youtu.be/W5ttPnSap2g."
  },
  {
    "title": "Predictive Traffic Rule Compliance using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22925v1",
    "arxiv_id": "2503.22925v1",
    "authors": [
      "Yanliang Huang",
      "Sebastian Mair",
      "Zhuoqi Zeng",
      "Amr Alanwar",
      "Matthias Althoff"
    ],
    "published": "2025-03-29T01:04:08+00:00",
    "summary": "Autonomous vehicle path planning has reached a stage where safety and regulatory compliance are crucial. This paper presents a new approach that integrates a motion planner with a deep reinforcement learning model to predict potential traffic rule violations. In this setup, the predictions of the critic directly affect the cost function of the motion planner, guiding the choices of the trajectory. We incorporate key interstate rules from the German Road Traffic Regulation into a rule book and use a graph-based state representation to handle complex traffic information. Our main innovation is replacing the standard actor network in an actor-critic setup with a motion planning module, which ensures both predictable trajectory generation and prevention of long-term rule violations. Experiments on an open German highway dataset show that the model can predict and prevent traffic rule violations beyond the planning horizon, significantly increasing safety in challenging traffic conditions."
  },
  {
    "title": "Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models",
    "url": "http://arxiv.org/abs/2503.22886v1",
    "arxiv_id": "2503.22886v1",
    "authors": [
      "Ron Vainshtein",
      "Zohar Rimon",
      "Shie Mannor",
      "Chen Tessler"
    ],
    "published": "2025-03-28T21:28:13+00:00",
    "summary": "Recent advancements in imitation learning have led to transformer-based behavior foundation models (BFMs) that enable multi-modal, human-like control for humanoid agents. While excelling at zero-shot generation of robust behaviors, BFMs often require meticulous prompt engineering for specific tasks, potentially yielding suboptimal results. We introduce \"Task Tokens\", a method to effectively tailor BFMs to specific tasks while preserving their flexibility. Our approach leverages the transformer architecture of BFMs to learn a new task-specific encoder through reinforcement learning, keeping the original BFM frozen. This allows incorporation of user-defined priors, balancing reward design and prompt engineering. By training a task encoder to map observations to tokens, used as additional BFM inputs, we guide performance improvement while maintaining the model's diverse control characteristics. We demonstrate Task Tokens' efficacy across various tasks, including out-of-distribution scenarios, and show their compatibility with other prompting modalities. Our results suggest that Task Tokens offer a promising approach for adapting BFMs to specific control tasks while retaining their generalization capabilities."
  },
  {
    "title": "Markov Potential Game Construction and Multi-Agent Reinforcement Learning with Applications to Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.22867v1",
    "arxiv_id": "2503.22867v1",
    "authors": [
      "Huiwen Yan",
      "Mushuang Liu"
    ],
    "published": "2025-03-28T20:49:47+00:00",
    "summary": "Markov games (MGs) serve as the mathematical foundation for multi-agent reinforcement learning (MARL), enabling self-interested agents to learn their optimal policies while interacting with others in a shared environment. However, due to the complexities of an MG problem, seeking (Markov perfect) Nash equilibrium (NE) is often very challenging for a general-sum MG. Markov potential games (MPGs), which are a special class of MGs, have appealing properties such as guaranteed existence of pure NEs and guaranteed convergence of gradient play algorithms, thereby leading to desirable properties for many MARL algorithms in their NE-seeking processes. However, the question of how to construct MPGs has been open. This paper provides sufficient conditions on the reward design and on the Markov decision process (MDP), under which an MG is an MPG. Numerical results on autonomous driving applications are reported."
  },
  {
    "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.22679v1",
    "arxiv_id": "2503.22679v1",
    "authors": [
      "Weiqi Li",
      "Xuanyu Zhang",
      "Shijie Zhao",
      "Yabin Zhang",
      "Junlin Li",
      "Li Zhang",
      "Jian Zhang"
    ],
    "published": "2025-03-28T17:59:54+00:00",
    "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at https://github.com/lwq20020127/Q-Insight."
  },
  {
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "url": "http://arxiv.org/abs/2503.22677v1",
    "arxiv_id": "2503.22677v1",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "published": "2025-03-28T17:59:53+00:00",
    "summary": "Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs."
  },
  {
    "title": "Reinforcement Learning for Machine Learning Model Deployment: Evaluating Multi-Armed Bandits in ML Ops Environments",
    "url": "http://arxiv.org/abs/2503.22595v1",
    "arxiv_id": "2503.22595v1",
    "authors": [
      "S. Aaron McClendon",
      "Vishaal Venkatesh",
      "Juan Morinelli"
    ],
    "published": "2025-03-28T16:42:21+00:00",
    "summary": "In modern ML Ops environments, model deployment is a critical process that traditionally relies on static heuristics such as validation error comparisons and A/B testing. However, these methods require human intervention to adapt to real-world deployment challenges, such as model drift or unexpected performance degradation. We investigate whether reinforcement learning, specifically multi-armed bandit (MAB) algorithms, can dynamically manage model deployment decisions more effectively. Our approach enables more adaptive production environments by continuously evaluating deployed models and rolling back underperforming ones in real-time. We test six model selection strategies across two real-world datasets and find that RL based approaches match or exceed traditional methods in performance. Our findings suggest that reinforcement learning (RL)-based model management can improve automation, reduce reliance on manual interventions, and mitigate risks associated with post-deployment model failures."
  },
  {
    "title": "On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations",
    "url": "http://arxiv.org/abs/2503.22575v1",
    "arxiv_id": "2503.22575v1",
    "authors": [
      "Rajdeep Singh Hundal",
      "Yan Xiao",
      "Xiaochun Cao",
      "Jin Song Dong",
      "Manuel Rigger"
    ],
    "published": "2025-03-28T16:25:06+00:00",
    "summary": "Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used."
  },
  {
    "title": "Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games",
    "url": "http://arxiv.org/abs/2503.22779v1",
    "arxiv_id": "2503.22779v1",
    "authors": [
      "Junkai Hu",
      "Li Xia"
    ],
    "published": "2025-03-28T16:21:05+00:00",
    "summary": "We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted."
  },
  {
    "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments",
    "url": "http://arxiv.org/abs/2503.22496v1",
    "arxiv_id": "2503.22496v1",
    "authors": [
      "Luke Rowe",
      "Roger Girgis",
      "Anthony Gosselin",
      "Liam Paull",
      "Christopher Pal",
      "Felix Heide"
    ],
    "published": "2025-03-28T15:03:41+00:00",
    "summary": "We introduce Scenario Dreamer, a fully data-driven generative simulator for autonomous vehicle planning that generates both the initial traffic scene - comprising a lane graph and agent bounding boxes - and closed-loop agent behaviours. Existing methods for generating driving simulation environments encode the initial traffic scene as a rasterized image and, as such, require parameter-heavy networks that perform unnecessary computation due to many empty pixels in the rasterized scene. Moreover, we find that existing methods that employ rule-based agent behaviours lack diversity and realism. Scenario Dreamer instead employs a novel vectorized latent diffusion model for initial scene generation that directly operates on the vectorized scene elements and an autoregressive Transformer for data-driven agent behaviour simulation. Scenario Dreamer additionally supports scene extrapolation via diffusion inpainting, enabling the generation of unbounded simulation environments. Extensive experiments show that Scenario Dreamer outperforms existing generative simulators in realism and efficiency: the vectorized scene-generation base model achieves superior generation quality with around 2x fewer parameters, 6x lower generation latency, and 10x fewer GPU training hours compared to the strongest baseline. We confirm its practical utility by showing that reinforcement learning planning agents are more challenged in Scenario Dreamer environments than traditional non-generative simulation environments, especially on long and adversarial driving environments."
  },
  {
    "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model",
    "url": "http://arxiv.org/abs/2503.22480v1",
    "arxiv_id": "2503.22480v1",
    "authors": [
      "Wangtao Sun",
      "Xiang Cheng",
      "Xing Yu",
      "Haotian Xu",
      "Zhao Yang",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ],
    "published": "2025-03-28T14:39:52+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed uncertain reward model to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data. In this paper, we propose the Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model. PURM learns reward distributions directly from preference data and quantifies per-sample uncertainty via the average overlap area between reward distributions. To mitigate reward hacking, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. We propose a lightweight and easy-to-use implementation of PURM. Experiments demonstrate that PURM significantly delays the onset of reward hacking while improving final reward performance, outperforming baseline methods in both stability and effectiveness."
  },
  {
    "title": "Control of Humanoid Robots with Parallel Mechanisms using Kinematic Actuation Models",
    "url": "http://arxiv.org/abs/2503.22459v1",
    "arxiv_id": "2503.22459v1",
    "authors": [
      "Victor Lutz",
      "Ludovic de Matte\u00efs",
      "Virgile Batto",
      "Nicolas Mansard"
    ],
    "published": "2025-03-28T14:13:00+00:00",
    "summary": "Inspired by the mechanical design of Cassie, several recently released humanoid robots are using actuator configuration in which the motor is displaced from the joint location to optimize the leg inertia. This in turn induces a non linearity in the reduction ratio of the transmission which is often neglected when computing the robot motion (e.g. by trajectory optimization or reinforcement learning) and only accounted for at control time. This paper proposes an analytical method to efficiently handle this non-linearity. Using this actuation model, we demonstrate that we can leverage the dynamic abilities of the non-linear transmission while only modeling the inertia of the main serial chain of the leg, without approximating the motor capabilities nor the joint range. Based on analytical inverse kinematics, our method does not need any numerical routines dedicated to the closed-kinematics actuation, hence leading to very efficient computations. Our study focuses on two mechanisms widely used in recent humanoid robots; the four bar knee linkage as well as a parallel 2 DoF ankle mechanism. We integrate these models inside optimization based (DDP) and learning (PPO) control approaches. A comparison of our model against a simplified model that completely neglects closed chains is then shown in simulation."
  },
  {
    "title": "Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning",
    "url": "http://arxiv.org/abs/2503.22456v1",
    "arxiv_id": "2503.22456v1",
    "authors": [
      "Abdullah Vanlioglu"
    ],
    "published": "2025-03-28T14:07:51+00:00",
    "summary": "We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach that enhances the exploration-exploitation tradeoff by dynamically assigning weights to generated outputs based on their advantage and entropy for Reinforcement Learning-based Large Language Model fine-tuning. EGSW integrates entropy regularization with advantage-based weighting to balance policy updates, enabling efficient exploration in high-dimensional state spaces. By employing temperature-scaled softmax weighting over sequences, EGSW prioritizing high-reward, high-uncertainty steps while maintaining training stability. Although originally developed to improve Group Relative Policy Optimization (GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable to other reinforcement learning (RL) algorithms and can be implemented in both step-wise and trajectory-wise settings. Empirical evaluations demonstrate that EGSW enhances GRPO reasoning ability, yielding improvements in sample efficiency. Future work will explore the application of EGSW to advanced RL methodologies."
  },
  {
    "title": "Reinforcement learning for efficient and robust multi-setpoint and multi-trajectory tracking in bioprocesses",
    "url": "http://arxiv.org/abs/2503.22409v1",
    "arxiv_id": "2503.22409v1",
    "authors": [
      "Sebasti\u00e1n Espinel-R\u00edos",
      "Jos\u00e9 L. Avalos",
      "Ehecatl Antonio del Rio Chanona",
      "Dongda Zhang"
    ],
    "published": "2025-03-28T13:19:02+00:00",
    "summary": "Efficient and robust bioprocess control is essential for maximizing performance and adaptability in advanced biotechnological systems. In this work, we present a reinforcement-learning framework for multi-setpoint and multi-trajectory tracking. Tracking multiple setpoints and time-varying trajectories in reinforcement learning is challenging due to the complexity of balancing multiple objectives, a difficulty further exacerbated by system uncertainties such as uncertain initial conditions and stochastic dynamics. This challenge is relevant, e.g., in bioprocesses involving microbial consortia, where precise control over population compositions is required. We introduce a novel return function based on multiplicative reciprocal saturation functions, which explicitly couples reward gains to the simultaneous satisfaction of multiple references. Through a case study involving light-mediated cybergenetic growth control in microbial consortia, we demonstrate via computational experiments that our approach achieves faster convergence, improved stability, and superior control compliance compared to conventional quadratic-cost-based return functions. Moreover, our method enables tuning of the saturation function's parameters, shaping the learning process and policy updates. By incorporating system uncertainties, our framework also demonstrates robustness, a key requirement in industrial bioprocessing. Overall, this work advances reinforcement-learning-based control strategies in bioprocess engineering, with implications in the broader field of process and systems engineering."
  },
  {
    "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
    "url": "http://arxiv.org/abs/2503.22402v1",
    "arxiv_id": "2503.22402v1",
    "authors": [
      "Yizhang Zhu",
      "Runzhi Jiang",
      "Boyan Li",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "published": "2025-03-28T13:11:27+00:00",
    "summary": "Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costs--often overlooked--stand as the \"elephant in the room\" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2x boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL."
  },
  {
    "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
    "url": "http://arxiv.org/abs/2503.22342v1",
    "arxiv_id": "2503.22342v1",
    "authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Yuan Xie",
      "Rongrong Ji"
    ],
    "published": "2025-03-28T11:30:05+00:00",
    "summary": "This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO). GRPO, while effective, incurs high training costs due to the need for sampling multiple completions for each question. Our experiment and theoretical analysis reveals that the number of completions impacts model accuracy yet increases training time multiplicatively, and not all completions contribute equally to policy training -- their contribution depends on their relative advantage. To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates. Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency. Experimental results demonstrate that CPPO achieves up to $8.32\\times$ speedup on GSM8K and $3.51\\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO. We release our code at https://github.com/lzhxmu/CPPO."
  },
  {
    "title": "FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation",
    "url": "http://arxiv.org/abs/2503.22249v1",
    "arxiv_id": "2503.22249v1",
    "authors": [
      "Xianqi Zhang",
      "Hongliang Wei",
      "Wenrui Wang",
      "Xingtao Wang",
      "Xiaopeng Fan",
      "Debin Zhao"
    ],
    "published": "2025-03-28T09:02:32+00:00",
    "summary": "Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short). FLAM integrates a stabilizing reward function with a basic policy. The stabilizing reward function is designed to encourage the robot to learn stable postures, thereby accelerating the learning process and facilitating task completion. Specifically, the robot pose is first mapped to the 3D virtual human model. Then, the human pose is stabilized and reconstructed through a human motion reconstruction model. Finally, the pose before and after reconstruction is used to compute the stabilizing reward. By combining this stabilizing reward with the task reward, FLAM effectively guides policy learning. Experimental results on a humanoid robot benchmark demonstrate that FLAM outperforms state-of-the-art RL methods, highlighting its effectiveness in improving stability and overall performance."
  },
  {
    "title": "CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.22248v1",
    "arxiv_id": "2503.22248v1",
    "authors": [
      "Xinwei Gao",
      "Arambam James Singh",
      "Gangadhar Royyuru",
      "Michael Yuhas",
      "Arvind Easwaran"
    ],
    "published": "2025-03-28T08:59:02+00:00",
    "summary": "Lane keeping in autonomous driving systems requires scenario-specific weight tuning for different objectives. We formulate lane-keeping as a constrained reinforcement learning problem, where weight coefficients are automatically learned along with the policy, eliminating the need for scenario-specific tuning. Empirically, our approach outperforms traditional RL in efficiency and reliability. Additionally, real-world demonstrations validate its practical value for real-world autonomous driving."
  },
  {
    "title": "Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch",
    "url": "http://arxiv.org/abs/2503.22244v1",
    "arxiv_id": "2503.22244v1",
    "authors": [
      "Weizhen Wang",
      "Jianping He",
      "Xiaoming Duan"
    ],
    "published": "2025-03-28T08:52:41+00:00",
    "summary": "Policy gradient methods are one of the most successful methods for solving challenging reinforcement learning problems. However, despite their empirical successes, many SOTA policy gradient algorithms for discounted problems deviate from the theoretical policy gradient theorem due to the existence of a distribution mismatch. In this work, we analyze the impact of this mismatch on the policy gradient methods. Specifically, we first show that in the case of tabular parameterizations, the methods under the mismatch remain globally optimal. Then, we extend this analysis to more general parameterizations by leveraging the theory of biased stochastic gradient descent. Our findings offer new insights into the robustness of policy gradient methods as well as the gap between theoretical foundations and practical implementations."
  },
  {
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
    "url": "http://arxiv.org/abs/2503.22230v1",
    "arxiv_id": "2503.22230v1",
    "authors": [
      "Wei Shen",
      "Guanlin Liu",
      "Zheng Wu",
      "Ruofei Zhu",
      "Qingping Yang",
      "Chao Xin",
      "Yu Yue",
      "Lin Yan"
    ],
    "published": "2025-03-28T08:26:41+00:00",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF."
  },
  {
    "title": "Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps",
    "url": "http://arxiv.org/abs/2503.22162v1",
    "arxiv_id": "2503.22162v1",
    "authors": [
      "Ning Liu",
      "Sen Shen",
      "Xiangrui Kong",
      "Hongtao Zhang",
      "Thomas Br\u00e4unl"
    ],
    "published": "2025-03-28T05:57:23+00:00",
    "summary": "Multi-Agent Pathfinding is used in areas including multi-robot formations, warehouse logistics, and intelligent vehicles. However, many environments are incomplete or frequently change, making it difficult for standard centralized planning or pure reinforcement learning to maintain both global solution quality and local flexibility. This paper introduces a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning, using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. We evaluate the framework in the discrete POGEMA environment and compare it with baseline methods. Experimental outcomes indicate that the proposed framework substantially improves success rate, collision rate, and path efficiency. The model is further tested on the EyeSim platform, where it maintains feasible Pathfinding under frequent changes and large-scale robot deployments."
  },
  {
    "title": "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
    "url": "http://arxiv.org/abs/2503.22137v1",
    "arxiv_id": "2503.22137v1",
    "authors": [
      "Syrine Belakaria",
      "Joshua Kazdan",
      "Charles Marx",
      "Chris Cundy",
      "Willie Neiswanger",
      "Sanmi Koyejo",
      "Barbara E. Engelhardt",
      "Stefano Ermon"
    ],
    "published": "2025-03-28T04:22:53+00:00",
    "summary": "Reinforcement learning from human feedback (RLHF) has become a cornerstone of the training and alignment pipeline for large language models (LLMs). Recent advances, such as direct preference optimization (DPO), have simplified the preference learning step. However, collecting preference data remains a challenging and costly process, often requiring expert annotation. This cost can be mitigated by carefully selecting the data points presented for annotation. In this work, we propose an active learning approach to efficiently select prompt and preference pairs using a risk assessment strategy based on the Sharpe Ratio. To address the challenge of unknown preferences prior to annotation, our method evaluates the gradients of all potential preference annotations to assess their impact on model updates. These gradient-based evaluations enable risk assessment of data points regardless of the annotation outcome. By leveraging the DPO loss derivations, we derive a closed-form expression for computing these Sharpe ratios on a per-tuple basis, ensuring our approach remains both tractable and computationally efficient. We also introduce two variants of our method, each making different assumptions about prior information. Experimental results demonstrate that our method outperforms the baseline by up to 5% in win rates against the chosen completion with limited human preference data across several language models and real-world datasets."
  },
  {
    "title": "Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for Contact-Rich Robotic Tasks",
    "url": "http://arxiv.org/abs/2503.21989v1",
    "arxiv_id": "2503.21989v1",
    "authors": [
      "Heng Zhang",
      "Gokhan Solak",
      "Arash Ajoudani"
    ],
    "published": "2025-03-27T21:11:32+00:00",
    "summary": "Ensuring safety in reinforcement learning (RL)-based robotic systems is a critical challenge, especially in contact-rich tasks within unstructured environments. While the state-of-the-art safe RL approaches mitigate risks through safe exploration or high-level recovery mechanisms, they often overlook low-level execution safety, where reflexive responses to potential hazards are crucial. Similarly, variable impedance control (VIC) enhances safety by adjusting the robot's mechanical response, yet lacks a systematic way to adapt parameters, such as stiffness and damping throughout the task. In this paper, we propose Bresa, a Bio-inspired Reflexive Hierarchical Safe RL method inspired by biological reflexes. Our method decouples task learning from safety learning, incorporating a safety critic network that evaluates action risks and operates at a higher frequency than the task solver. Unlike existing recovery-based methods, our safety critic functions at a low-level control layer, allowing real-time intervention when unsafe conditions arise. The task-solving RL policy, running at a lower frequency, focuses on high-level planning (decision-making), while the safety critic ensures instantaneous safety corrections. We validate Bresa on multiple tasks including a contact-rich robotic task, demonstrating its reflexive ability to enhance safety, and adaptability in unforeseen dynamic environments. Our results show that Bresa outperforms the baseline, providing a robust and reflexive safety mechanism that bridges the gap between high-level planning and low-level execution. Real-world experiments and supplementary material are available at project website https://jack-sherman01.github.io/Bresa."
  },
  {
    "title": "Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams and Teams of LLMs",
    "url": "http://arxiv.org/abs/2503.21983v1",
    "arxiv_id": "2503.21983v1",
    "authors": [
      "Abed Kareem Musaffar",
      "Anand Gokhale",
      "Sirui Zeng",
      "Rasta Tadayon",
      "Xifeng Yan",
      "Ambuj Singh",
      "Francesco Bullo"
    ],
    "published": "2025-03-27T21:01:02+00:00",
    "summary": "As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models -- one inspired by literature and the other data-driven -- and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies."
  },
  {
    "title": "Pretrained Bayesian Non-parametric Knowledge Prior in Robotic Long-Horizon Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21975v1",
    "arxiv_id": "2503.21975v1",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Kejia Chen",
      "Yansong Wu",
      "Liding Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "published": "2025-03-27T20:43:36+00:00",
    "summary": "Reinforcement learning (RL) methods typically learn new tasks from scratch, often disregarding prior knowledge that could accelerate the learning process. While some methods incorporate previously learned skills, they usually rely on a fixed structure, such as a single Gaussian distribution, to define skill priors. This rigid assumption can restrict the diversity and flexibility of skills, particularly in complex, long-horizon tasks. In this work, we introduce a method that models potential primitive skill motions as having non-parametric properties with an unknown number of underlying features. We utilize a Bayesian non-parametric model, specifically Dirichlet Process Mixtures, enhanced with birth and merge heuristics, to pre-train a skill prior that effectively captures the diverse nature of skills. Additionally, the learned skills are explicitly trackable within the prior space, enhancing interpretability and control. By integrating this flexible skill prior into an RL framework, our approach surpasses existing methods in long-horizon manipulation tasks, enabling more efficient skill transfer and task success in complex environments. Our findings show that a richer, non-parametric representation of skill priors significantly improves both the learning and execution of challenging robotic tasks. All data, code, and videos are available at https://ghiara.github.io/HELIOS/."
  },
  {
    "title": "Data-Agnostic Robotic Long-Horizon Manipulation with Vision-Language-Guided Closed-Loop Feedback",
    "url": "http://arxiv.org/abs/2503.21969v1",
    "arxiv_id": "2503.21969v1",
    "authors": [
      "Yuan Meng",
      "Xiangtong Yao",
      "Haihui Ye",
      "Yirui Zhou",
      "Shengqiang Zhang",
      "Zhenshan Bing",
      "Alois Knoll"
    ],
    "published": "2025-03-27T20:32:58+00:00",
    "summary": "Recent advances in language-conditioned robotic manipulation have leveraged imitation and reinforcement learning to enable robots to execute tasks from human commands. However, these methods often suffer from limited generalization, adaptability, and the lack of large-scale specialized datasets, unlike data-rich domains such as computer vision, making long-horizon task execution challenging. To address these gaps, we introduce DAHLIA, a data-agnostic framework for language-conditioned long-horizon robotic manipulation, leveraging large language models (LLMs) for real-time task planning and execution. DAHLIA employs a dual-tunnel architecture, where an LLM-powered planner collaborates with co-planners to decompose tasks and generate executable plans, while a reporter LLM provides closed-loop feedback, enabling adaptive re-planning and ensuring task recovery from potential failures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning and temporal abstraction for efficient action execution, enhancing traceability and robustness. Our framework demonstrates state-of-the-art performance across diverse long-horizon tasks, achieving strong generalization in both simulated and real-world scenarios. Videos and code are available at https://ghiara.github.io/DAHLIA/."
  },
  {
    "title": "Safety Verification and Optimization in Industrial Drive Systems",
    "url": "http://arxiv.org/abs/2503.21965v1",
    "arxiv_id": "2503.21965v1",
    "authors": [
      "Imran Riaz Hasrat",
      "Eun-Young Kang",
      "Christian Uldal Graulund"
    ],
    "published": "2025-03-27T20:27:19+00:00",
    "summary": "Safety and reliability are crucial in industrial drive systems, where hazardous failures can have severe consequences. Detecting and mitigating dangerous faults on time is challenging due to the stochastic and unpredictable nature of fault occurrences, which can lead to limited diagnostic efficiency and compromise safety. This paper optimizes the safety and diagnostic performance of a real-world industrial Basic Drive Module(BDM) using Uppaal Stratego. We model the functional safety architecture of the BDM with timed automata and formally verify its key functional and safety requirements through model checking to eliminate unwanted behaviors. Considering the formally verified correct model as a baseline, we leverage the reinforcement learning facility in Uppaal Stratego to optimize the safe failure fraction to the 90 % threshold, improving fault detection ability. The promising results highlight strong potential for broader safety applications in industrial automation."
  },
  {
    "title": "Reward Design for Reinforcement Learning Agents",
    "url": "http://arxiv.org/abs/2503.21949v1",
    "arxiv_id": "2503.21949v1",
    "authors": [
      "Rati Devidze"
    ],
    "published": "2025-03-27T19:48:02+00:00",
    "summary": "Reward functions are central in reinforcement learning (RL), guiding agents towards optimal decision-making. The complexity of RL tasks requires meticulously designed reward functions that effectively drive learning while avoiding unintended consequences. Effective reward design aims to provide signals that accelerate the agent's convergence to optimal behavior. Crafting rewards that align with task objectives, foster desired behaviors, and prevent undesirable actions is inherently challenging. This thesis delves into the critical role of reward signals in RL, highlighting their impact on the agent's behavior and learning dynamics and addressing challenges such as delayed, ambiguous, or intricate rewards. In this thesis work, we tackle different aspects of reward shaping. First, we address the problem of designing informative and interpretable reward signals from a teacher's/expert's perspective (teacher-driven). Here, the expert, equipped with the optimal policy and the corresponding value function, designs reward signals that expedite the agent's convergence to optimal behavior. Second, we build on this teacher-driven approach by introducing a novel method for adaptive interpretable reward design. In this scenario, the expert tailors the rewards based on the learner's current policy, ensuring alignment and optimal progression. Third, we propose a meta-learning approach, enabling the agent to self-design its reward signals online without expert input (agent-driven). This self-driven method considers the agent's learning and exploration to establish a self-improving feedback loop."
  },
  {
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "url": "http://arxiv.org/abs/2503.21776v1",
    "arxiv_id": "2503.21776v1",
    "authors": [
      "Kaituo Feng",
      "Kaixiong Gong",
      "Bohao Li",
      "Zonghao Guo",
      "Yibing Wang",
      "Tianshuo Peng",
      "Benyou Wang",
      "Xiangyu Yue"
    ],
    "published": "2025-03-27T17:59:51+00:00",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released."
  },
  {
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
    "url": "http://arxiv.org/abs/2503.21860v1",
    "arxiv_id": "2503.21860v1",
    "authors": [
      "Kailin Li",
      "Puhao Li",
      "Tengyu Liu",
      "Yuyang Li",
      "Siyuan Huang"
    ],
    "published": "2025-03-27T17:50:30+00:00",
    "summary": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments."
  },
  {
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation",
    "url": "http://arxiv.org/abs/2503.21729v1",
    "arxiv_id": "2503.21729v1",
    "authors": [
      "Zhicheng Lee",
      "Shulin Cao",
      "Jinxin Liu",
      "Jiajie Zhang",
      "Weichuan Liu",
      "Xiaoyin Che",
      "Lei Hou",
      "Juanzi Li"
    ],
    "published": "2025-03-27T17:44:18+00:00",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG)."
  },
  {
    "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
    "url": "http://arxiv.org/abs/2503.21720v1",
    "arxiv_id": "2503.21720v1",
    "authors": [
      "Souradip Chakraborty",
      "Sujay Bhatt",
      "Udari Madhushani Sehwag",
      "Soumya Suvra Ghosal",
      "Jiahao Qiu",
      "Mengdi Wang",
      "Dinesh Manocha",
      "Furong Huang",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "published": "2025-03-27T17:34:25+00:00",
    "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."
  },
  {
    "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21683v1",
    "arxiv_id": "2503.21683v1",
    "authors": [
      "Hui Wang"
    ],
    "published": "2025-03-27T16:52:25+00:00",
    "summary": "In recent years, large language models (LLMs) have shown significant advancements in natural language processing (NLP), with strong capa-bilities in generation, comprehension, and rea-soning. These models have found applications in education, intelligent decision-making, and gaming. However, effectively utilizing LLMs for strategic planning and decision-making in the game of Gomoku remains a challenge. This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to understand and apply Gomoku strat-egies and logic to make rational decisions. The research methods include enabling the model to \"read the board,\" \"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while en-hancing its abilities through self-play and rein-forcement learning. The results demonstrate that this approach significantly improves the se-lection of move positions, resolves the issue of generating illegal positions, and reduces pro-cess time through parallel position evaluation. After extensive self-play training, the model's Gomoku-playing capabilities have been notably enhanced."
  },
  {
    "title": "A tale of two goals: leveraging sequentiality in multi-goal scenarios",
    "url": "http://arxiv.org/abs/2503.21677v1",
    "arxiv_id": "2503.21677v1",
    "authors": [
      "Olivier Serris",
      "St\u00e9phane Doncieux",
      "Olivier Sigaud"
    ],
    "published": "2025-03-27T16:47:46+00:00",
    "summary": "Several hierarchical reinforcement learning methods leverage planning to create a graph or sequences of intermediate goals, guiding a lower-level goal-conditioned (GC) policy to reach some final goals. The low-level policy is typically conditioned on the current goal, with the aim of reaching it as quickly as possible. However, this approach can fail when an intermediate goal can be reached in multiple ways, some of which may make it impossible to continue toward subsequent goals. To address this issue, we introduce two instances of Markov Decision Process (MDP) where the optimization objective favors policies that not only reach the current goal but also subsequent ones. In the first, the agent is conditioned on both the current and final goals, while in the second, it is conditioned on the next two goals in the sequence. We conduct a series of experiments on navigation and pole-balancing tasks in which sequences of intermediate goals are given. By evaluating policies trained with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that, in most cases, conditioning on the next two goals improves stability and sample efficiency over other approaches."
  },
  {
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21620v1",
    "arxiv_id": "2503.21620v1",
    "authors": [
      "Zhengxi Lu",
      "Yuxiang Chai",
      "Yaxuan Guo",
      "Xi Yin",
      "Liang Liu",
      "Hao Wang",
      "Guanjing Xiong",
      "Hongsheng Li"
    ],
    "published": "2025-03-27T15:39:30+00:00",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain."
  },
  {
    "title": "A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols",
    "url": "http://arxiv.org/abs/2503.21601v1",
    "arxiv_id": "2503.21601v1",
    "authors": [
      "Johannes Voigt",
      "Peter Jiacheng Gu",
      "Peter Rost"
    ],
    "published": "2025-03-27T15:20:59+00:00",
    "summary": "The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility. This can lead to frequent radio link failures and reduced data rates. In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol. Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate. Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method."
  },
  {
    "title": "DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker Availability Windows",
    "url": "http://arxiv.org/abs/2503.21458v1",
    "arxiv_id": "2503.21458v1",
    "authors": [
      "Jinwen Chen",
      "Jiannan Guo",
      "Dazhuo Qiu",
      "Yawen Li",
      "Guanhua Ye",
      "Yan Zhao",
      "Kai Zheng"
    ],
    "published": "2025-03-27T12:46:12+00:00",
    "summary": "With the rapid advancement of mobile networks and the widespread use of mobile devices, spatial crowdsourcing, which involves assigning location-based tasks to mobile workers, has gained significant attention. However, most existing research focuses on task assignment at the current moment, overlooking the fluctuating demand and supply between tasks and workers over time. To address this issue, we introduce an adaptive task assignment problem, which aims to maximize the number of assigned tasks by dynamically adjusting task assignments in response to changing demand and supply. We develop a spatial crowdsourcing framework, namely demand-based adaptive task assignment with dynamic worker availability windows, which consists of two components including task demand prediction and task assignment. In the first component, we construct a graph adjacency matrix representing the demand dependency relationships in different regions and employ a multivariate time series learning approach to predict future task demands. In the task assignment component, we adjust tasks to workers based on these predictions, worker availability windows, and the current task assignments, where each worker has an availability window that indicates the time periods they are available for task assignments. To reduce the search space of task assignments and be efficient, we propose a worker dependency separation approach based on graph partition and a task value function with reinforcement learning. Experiments on real data demonstrate that our proposals are both effective and efficient."
  },
  {
    "title": "On Learning-Based Traffic Monitoring With a Swarm of Drones",
    "url": "http://arxiv.org/abs/2503.21433v1",
    "arxiv_id": "2503.21433v1",
    "authors": [
      "Marko Maljkovic",
      "Nikolas Geroliminis"
    ],
    "published": "2025-03-27T12:18:49+00:00",
    "summary": "Efficient traffic monitoring is crucial for managing urban transportation networks, especially under congested and dynamically changing traffic conditions. Drones offer a scalable and cost-effective alternative to fixed sensor networks. However, deploying fleets of low-cost drones for traffic monitoring poses challenges in adaptability, scalability, and real-time operation. To address these issues, we propose a learning-based framework for decentralized traffic monitoring with drone swarms, targeting the uneven and unpredictable distribution of monitoring needs across urban areas. Our approach introduces a semi-decentralized reinforcement learning model, which trains a single Q-function using the collective experience of the swarm. This model supports full scalability, flexible deployment, and, when hardware allows, the online adaptation of each drone's action-selection mechanism. We first train and evaluate the model in a synthetic traffic environment, followed by a case study using real traffic data from Shenzhen, China, to validate its performance and demonstrate its potential for real-world applications in complex urban monitoring tasks."
  },
  {
    "title": "AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control",
    "url": "http://arxiv.org/abs/2503.21401v1",
    "arxiv_id": "2503.21401v1",
    "authors": [
      "Tianyu Xu",
      "Yaoyu Cheng",
      "Pinxi Shen",
      "Lin Zhao",
      "Electrical",
      "Computer Engineering",
      "National University of Singapore",
      "Singapore",
      "Mechanical Engineering",
      "National University of Singapore",
      "Singapore"
    ],
    "published": "2025-03-27T11:47:20+00:00",
    "summary": "Quadrupedal robots can learn versatile locomotion skills but remain vulnerable when one or more joints lose power. In contrast, dogs and cats can adopt limping gaits when injured, demonstrating their remarkable ability to adapt to physical conditions. Inspired by such adaptability, this paper presents Action Learner (AcL), a novel teacher-student reinforcement learning framework that enables quadrupeds to autonomously adapt their gait for stable walking under multiple joint faults. Unlike conventional teacher-student approaches that enforce strict imitation, AcL leverages teacher policies to generate style rewards, guiding the student policy without requiring precise replication. We train multiple teacher policies, each corresponding to a different fault condition, and subsequently distill them into a single student policy with an encoder-decoder architecture. While prior works primarily address single-joint faults, AcL enables quadrupeds to walk with up to four faulty joints across one or two legs, autonomously switching between different limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot under single- and double-joint faults, demonstrating fault-tolerant, stable walking, smooth gait transitions between normal and lamb gaits, and robustness against external disturbances."
  },
  {
    "title": "AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control",
    "url": "http://arxiv.org/abs/2503.21401v2",
    "arxiv_id": "2503.21401v2",
    "authors": [
      "Tianyu Xu",
      "Yaoyu Cheng",
      "Pinxi Shen",
      "Lin Zhao"
    ],
    "published": "2025-03-27T11:47:20+00:00",
    "summary": "Quadrupedal robots can learn versatile locomotion skills but remain vulnerable when one or more joints lose power. In contrast, dogs and cats can adopt limping gaits when injured, demonstrating their remarkable ability to adapt to physical conditions. Inspired by such adaptability, this paper presents Action Learner (AcL), a novel teacher-student reinforcement learning framework that enables quadrupeds to autonomously adapt their gait for stable walking under multiple joint faults. Unlike conventional teacher-student approaches that enforce strict imitation, AcL leverages teacher policies to generate style rewards, guiding the student policy without requiring precise replication. We train multiple teacher policies, each corresponding to a different fault condition, and subsequently distill them into a single student policy with an encoder-decoder architecture. While prior works primarily address single-joint faults, AcL enables quadrupeds to walk with up to four faulty joints across one or two legs, autonomously switching between different limping gaits when faults occur. We validate AcL on a real Go2 quadruped robot under single- and double-joint faults, demonstrating fault-tolerant, stable walking, smooth gait transitions between normal and lamb gaits, and robustness against external disturbances."
  },
  {
    "title": "Controlling Large Language Model with Latent Actions",
    "url": "http://arxiv.org/abs/2503.21383v1",
    "arxiv_id": "2503.21383v1",
    "authors": [
      "Chengxing Jia",
      "Ziniu Li",
      "Pengyuan Wang",
      "Yi-Chen Li",
      "Zhenyu Hou",
      "Yuxiao Dong",
      "Yang Yu"
    ],
    "published": "2025-03-27T11:25:22+00:00",
    "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA), a framework that integrates a latent action space into pre-trained LLMs. We apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, CoLA's latent action enables greater semantic diversity in text generation. For enhancing downstream tasks, we show that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, CoLA with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, CoLA reduces computation time by half in tasks involving enhanced thinking prompts for LLMs by RL. These results highlight CoLA's potential to advance RL-based adaptation of LLMs for downstream applications."
  },
  {
    "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.21257v1",
    "arxiv_id": "2503.21257v1",
    "authors": [
      "Yongxu Wang",
      "Weiyun Yi",
      "Xinhao Kong",
      "Wanting Li"
    ],
    "published": "2025-03-27T08:28:22+00:00",
    "summary": "With the rapid development of embodied intelligence, leveraging large-scale human data for high-level imitation learning on humanoid robots has become a focal point of interest in both academia and industry. However, applying humanoid robots to precision operation domains remains challenging due to the complexities they face in perception and control processes, the long-standing physical differences in morphology and actuation mechanisms between humanoid robots and humans, and the lack of task-relevant features obtained from egocentric vision. To address the issue of covariate shift in imitation learning, this paper proposes an imitation learning algorithm tailored for humanoid robots. By focusing on the primary task objectives, filtering out background information, and incorporating channel feature fusion with spatial attention mechanisms, the proposed algorithm suppresses environmental disturbances and utilizes a dynamic weight update strategy to significantly improve the success rate of humanoid robots in accomplishing target tasks. Experimental results demonstrate that the proposed method exhibits robustness and scalability across various typical task scenarios, providing new ideas and approaches for autonomous learning and control in humanoid robots. The project will be open-sourced on GitHub."
  },
  {
    "title": "Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection",
    "url": "http://arxiv.org/abs/2503.21099v1",
    "arxiv_id": "2503.21099v1",
    "authors": [
      "Yun Zhu",
      "Le Hui",
      "Hang Yang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ],
    "published": "2025-03-27T02:37:05+00:00",
    "summary": "Both indoor and outdoor scene perceptions are essential for embodied intelligence. However, current sparse supervised 3D object detection methods focus solely on outdoor scenes without considering indoor settings. To this end, we propose a unified sparse supervised 3D object detection method for both indoor and outdoor scenes through learning class prototypes to effectively utilize unlabeled objects. Specifically, we first propose a prototype-based object mining module that converts the unlabeled object mining into a matching problem between class prototypes and unlabeled features. By using optimal transport matching results, we assign prototype labels to high-confidence features, thereby achieving the mining of unlabeled objects. We then present a multi-label cooperative refinement module to effectively recover missed detections through pseudo label quality control and prototype label cooperation. Experiments show that our method achieves state-of-the-art performance under the one object per scene sparse supervised setting across indoor and outdoor datasets. With only one labeled object per scene, our method achieves about 78%, 90%, and 96% performance compared to the fully supervised detector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the scalability of our method. Code is available at https://github.com/zyrant/CPDet3D."
  },
  {
    "title": "Robust Quantum Control using Reinforcement Learning from Demonstration",
    "url": "http://arxiv.org/abs/2503.21085v1",
    "arxiv_id": "2503.21085v1",
    "authors": [
      "Shengyong Li",
      "Yidian Fan",
      "Xiang Li",
      "Xinhui Ruan",
      "Qianchuan Zhao",
      "Zhihui Peng",
      "Re-Bing Wu",
      "Jing Zhang",
      "Pengtao Song"
    ],
    "published": "2025-03-27T02:01:28+00:00",
    "summary": "Quantum control requires high-precision and robust control pulses to ensure optimal system performance. However, control sequences generated with a system model may suffer from model bias, leading to low fidelity. While model-free reinforcement learning (RL) methods have been developed to avoid such biases, training an RL agent from scratch can be time-consuming, often taking hours to gather enough samples for convergence. This challenge has hindered the broad application of RL techniques to larger and more complex quantum control issues, limiting their adaptability. In this work, we use Reinforcement Learning from Demonstration (RLfD) to leverage the control sequences generated with system models and further optimize them with RL to avoid model bias. By avoiding learning from scratch and starting with reasonable control pulse shapes, this approach can increase sample efficiency by reducing the number of samples, which can significantly reduce the training time. Thus, this method can effectively handle pulse shapes that are discretized into more than 1000 pieces without compromising final fidelity. We have simulated the preparation of several high-fidelity non-classical states using the RLfD method. We also find that the training process is more stable when using RLfD. In addition, this method is suitable for fast gate calibration using reinforcement learning."
  },
  {
    "title": "World Model Agents with Change-Based Intrinsic Motivation",
    "url": "http://arxiv.org/abs/2503.21047v1",
    "arxiv_id": "2503.21047v1",
    "authors": [
      "Jeremias Ferrao",
      "Rafael Cunha"
    ],
    "published": "2025-03-26T23:40:03+00:00",
    "summary": "Sparse reward environments pose a significant challenge for reinforcement learning due to the scarcity of feedback. Intrinsic motivation and transfer learning have emerged as promising strategies to address this issue. Change Based Exploration Transfer (CBET), a technique that combines these two approaches for model-free algorithms, has shown potential in addressing sparse feedback but its effectiveness with modern algorithms remains understudied. This paper provides an adaptation of CBET for world model algorithms like DreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both with and without CBET, in the sparse reward environments of Crafter and Minigrid. Our tabula rasa results highlight the possibility of CBET improving DreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in Minigrid with CBET further reducing returns. In the same vein, our transfer learning experiments show that pre-training DreamerV3 with intrinsic rewards does not immediately lead to a policy that maximizes extrinsic rewards in Minigrid. Overall, our results suggest that CBET provides a positive impact on DreamerV3 in more complex environments like Crafter but may be detrimental in environments like Minigrid. In the latter case, the behaviours promoted by CBET in DreamerV3 may not align with the task objectives of the environment, leading to reduced returns and suboptimal policies."
  },
  {
    "title": "AllReduce Scheduling with Hierarchical Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.21013v1",
    "arxiv_id": "2503.21013v1",
    "authors": [
      "Yufan Wei",
      "Mickel Liu",
      "Wenfei Wu"
    ],
    "published": "2025-03-26T22:01:49+00:00",
    "summary": "AllReduce is a technique in distributed computing which saw use in many critical applications of deep learning. Existing methods of AllReduce scheduling oftentimes lack flexibility due to being topology-specific or relying on extensive handcrafted designs that require domain-specific knowledge. In this work, we aim to alleviate this inflexibility by proposing a deep-reinforcement-learning (DRL)-based pipeline that can generate AllReduce scheduling for various network topologies without topology-specific design features. The flow scheduling module of this pipeline consists of two hierarchically-structured DRL policies that work cooperatively to find optimal scheduling. We showcase the performance of our method compared to the baseline methods on three topologies: BCube, DCell, and Jellyfish. Finally, we contributed a Python-based simulation environment simulating AllReduce scheduling on these network topologies."
  },
  {
    "title": "Multi-head Reward Aggregation Guided by Entropy",
    "url": "http://arxiv.org/abs/2503.20995v1",
    "arxiv_id": "2503.20995v1",
    "authors": [
      "Xiaomin Li",
      "Xupeng Chen",
      "Jingxuan Fan",
      "Eric Hanchen Jiang",
      "Mingye Gao"
    ],
    "published": "2025-03-26T21:16:48+00:00",
    "summary": "Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, we introduce ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, we demonstrate that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying our entropy-based penalization. Through extensive experiments on RewardBench safety tasks, our method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. Our proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling."
  },
  {
    "title": "Reinforcement Learning for Efficient Toxicity Detection in Competitive Online Video Games",
    "url": "http://arxiv.org/abs/2503.20968v1",
    "arxiv_id": "2503.20968v1",
    "authors": [
      "Jacob Morrier",
      "Rafal Kocielnik",
      "R. Michael Alvarez"
    ],
    "published": "2025-03-26T20:13:30+00:00",
    "summary": "Online platforms take proactive measures to detect and address undesirable behavior, aiming to focus these resource-intensive efforts where such behavior is most prevalent. This article considers the problem of efficient sampling for toxicity detection in competitive online video games. To make optimal monitoring decisions, video game service operators need estimates of the likelihood of toxic behavior. If no model is available for these predictions, one must be estimated in real time. To close this gap, we propose a contextual bandit algorithm that makes monitoring decisions based on a small set of variables that, according to domain expertise, are associated with toxic behavior. This algorithm balances exploration and exploitation to optimize long-term outcomes and is deliberately designed for easy deployment in production. Using data from the popular first-person action game Call of Duty: Modern Warfare III, we show that our algorithm consistently outperforms baseline algorithms that rely solely on players' past behavior. This finding has substantive implications for the nature of toxicity. It also illustrates how domain expertise can be harnessed to help video game service operators identify and mitigate toxicity, ultimately fostering a safer and more enjoyable gaming experience."
  },
  {
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "url": "http://arxiv.org/abs/2503.20783v1",
    "arxiv_id": "2503.20783v1",
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "published": "2025-03-26T17:59:14+00:00",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero."
  },
  {
    "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning",
    "url": "http://arxiv.org/abs/2503.20752v2",
    "arxiv_id": "2503.20752v2",
    "authors": [
      "Huajie Tan",
      "Yuheng Ji",
      "Xiaoshuai Hao",
      "Minglan Lin",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ],
    "published": "2025-03-26T17:38:06+00:00",
    "summary": "Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the model's ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation. Experimental results demonstrate Reasoning-RFT's three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. Project website: https://tanhuajie.github.io/ReasonRFT"
  },
  {
    "title": "Graph-Enhanced Model-Free Reinforcement Learning Agents for Efficient Power Grid Topological Control",
    "url": "http://arxiv.org/abs/2503.20688v1",
    "arxiv_id": "2503.20688v1",
    "authors": [
      "Eloy Anguiano Batanero",
      "\u00c1ngela Fern\u00e1ndez",
      "\u00c1lvaro Barbero"
    ],
    "published": "2025-03-26T16:20:30+00:00",
    "summary": "The increasing complexity of power grid management, driven by the emergence of prosumers and the demand for cleaner energy solutions, has needed innovative approaches to ensure stability and efficiency. This paper presents a novel approach within the model-free framework of reinforcement learning, aimed at optimizing power network operations without prior expert knowledge. We introduce a masked topological action space, enabling agents to explore diverse strategies for cost reduction while maintaining reliable service using the state logic as a guide for choosing proper actions. Through extensive experimentation across 20 different scenarios in a simulated 5-substation environment, we demonstrate that our approach achieves a consistent reduction in power losses, while ensuring grid stability against potential blackouts. The results underscore the effectiveness of combining dynamic observation formalization with opponent-based training, showing a viable way for autonomous management solutions in modern energy systems or even for building a foundational model for this field."
  },
  {
    "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound",
    "url": "http://arxiv.org/abs/2503.20685v2",
    "arxiv_id": "2503.20685v2",
    "authors": [
      "Yuhao Huang",
      "Ao Chang",
      "Haoran Dou",
      "Xing Tao",
      "Xinrui Zhou",
      "Yan Cao",
      "Ruobing Huang",
      "Alejandro F Frangi",
      "Lingyun Bao",
      "Xin Yang",
      "Dong Ni"
    ],
    "published": "2025-03-26T16:20:02+00:00",
    "summary": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms."
  },
  {
    "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
    "url": "http://arxiv.org/abs/2503.20641v1",
    "arxiv_id": "2503.20641v1",
    "authors": [
      "Han Wu",
      "Yuxuan Yao",
      "Shuqi Liu",
      "Zehua Liu",
      "Xiaojin Fu",
      "Xiongwei Han",
      "Xing Li",
      "Hui-Ling Zhen",
      "Tao Zhong",
      "Mingxuan Yuan"
    ],
    "published": "2025-03-26T15:34:37+00:00",
    "summary": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging."
  },
  {
    "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
    "url": "http://arxiv.org/abs/2503.20844v1",
    "arxiv_id": "2503.20844v1",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui",
      "Yue Gao"
    ],
    "published": "2025-03-26T15:08:58+00:00",
    "summary": "Deep reinforcement learning (DRL) has emerged as a promising approach for robotic control, but its realworld deployment remains challenging due to its vulnerability to environmental perturbations. Existing white-box adversarial attack methods, adapted from supervised learning, fail to effectively target DRL agents as they overlook temporal dynamics and indiscriminately perturb all state dimensions, limiting their impact on long-term rewards. To address these challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR) Attack, a white-box attack method that combines DRL with a gradient-based soft masking mechanism to dynamically identify critical state dimensions and optimize adversarial policies. AGMR selectively allocates perturbations to the most impactful state features and incorporates a dynamic adjustment mechanism to balance exploration and exploitation during training. Extensive experiments demonstrate that AGMR outperforms state-of-the-art adversarial attack methods in degrading the performance of the victim agent and enhances the victim agent's robustness through adversarial defense mechanisms."
  },
  {
    "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20613v1",
    "arxiv_id": "2503.20613v1",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui"
    ],
    "published": "2025-03-26T15:00:07+00:00",
    "summary": "Recently, deep reinforcement learning (DRL) has emerged as a promising approach for robotic control. However, the deployment of DRL in real-world robots is hindered by its sensitivity to environmental perturbations. While existing whitebox adversarial attacks rely on local gradient information and apply uniform perturbations across all states to evaluate DRL robustness, they fail to account for temporal dynamics and state-specific vulnerabilities. To combat the above challenge, we first conduct a theoretical analysis of white-box attacks in DRL by establishing the adversarial victim-dynamics Markov decision process (AVD-MDP), to derive the necessary and sufficient conditions for a successful attack. Based on this, we propose a selective state-aware reinforcement adversarial attack method, named STAR, to optimize perturbation stealthiness and state visitation dispersion. STAR first employs a soft mask-based state-targeting mechanism to minimize redundant perturbations, enhancing stealthiness and attack effectiveness. Then, it incorporates an information-theoretic optimization objective to maximize mutual information between perturbations, environmental states, and victim actions, ensuring a dispersed state-visitation distribution that steers the victim agent into vulnerable states for maximum return reduction. Extensive experiments demonstrate that STAR outperforms state-of-the-art benchmarks."
  },
  {
    "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models",
    "url": "http://arxiv.org/abs/2503.20576v1",
    "arxiv_id": "2503.20576v1",
    "authors": [
      "Siyuan Guo",
      "Huiwu Liu",
      "Xiaolong Chen",
      "Yuming Xie",
      "Liang Zhang",
      "Tao Han",
      "Hechang Chen",
      "Yi Chang",
      "Jun Wang"
    ],
    "published": "2025-03-26T14:23:59+00:00",
    "summary": "In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs."
  },
  {
    "title": "TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion",
    "url": "http://arxiv.org/abs/2503.20839v1",
    "arxiv_id": "2503.20839v1",
    "authors": [
      "Amr Mousa",
      "Neil Karavis",
      "Michele Caprio",
      "Wei Pan",
      "Richard Allmendinger"
    ],
    "published": "2025-03-26T12:49:26+00:00",
    "summary": "Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed using the teacher-student paradigm, where a privileged teacher guides a proprioceptive student policy. However, key challenges such as representation misalignment between the privileged teacher and the proprioceptive-only student, covariate shift due to behavioral cloning, and lack of deployable adaptation lead to poor generalization in real-world scenarios. We propose Teacher-Aligned Representations via Contrastive Learning (TAR), a framework that leverages privileged information with self-supervised contrastive learning to bridge this gap. By aligning representations to a privileged teacher in simulation via contrastive objectives, our student policy learns structured latent spaces and exhibits robust generalization to Out-of-Distribution (OOD) scenarios, surpassing the fully privileged \"Teacher\". Results showed accelerated training by 2x compared to state-of-the-art baselines to achieve peak performance. OOD scenarios showed better generalization by 40 percent on average compared to existing methods. Additionally, TAR transitions seamlessly into learning during deployment without requiring privileged states, setting a new benchmark in sample-efficient, adaptive locomotion and enabling continual fine-tuning in real-world scenarios. Open-source code and videos are available at https://ammousa.github.io/TARLoco/."
  },
  {
    "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems",
    "url": "http://arxiv.org/abs/2503.20507v1",
    "arxiv_id": "2503.20507v1",
    "authors": [
      "Rakesh Nadig",
      "Vamanan Arulchelvan",
      "Rahul Bera",
      "Taha Shahroodi",
      "Gagandeep Singh",
      "Mohammad Sadrosadati",
      "Jisung Park",
      "Onur Mutlu"
    ],
    "published": "2025-03-26T12:47:52+00:00",
    "summary": "Hybrid storage systems (HSS) combine multiple storage devices with diverse characteristics to achieve high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which rearranges stored data across the devices to sustain high HSS performance. Prior works focus on improving only data placement or only data migration in HSS, which leads to sub-optimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique for HSS that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two light-weight autonomous RL agents, a data-placement agent and a data-migration agent, which adapt their policies for the current workload and HSS configuration, and coordinate with each other to improve overall HSS performance. We evaluate Harmonia on a real HSS with up to four heterogeneous storage devices with diverse characteristics. Our evaluation using 17 data-intensive workloads on performance-optimized (cost-optimized) HSS with two storage devices shows that, on average, Harmonia (1) outperforms the best-performing prior approach by 49.5% (31.7%), (2) bridges the performance gap between the best-performing prior work and Oracle by 64.2% (64.3%). On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%). Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB for both RL agents together). We plan to open-source Harmonia's implementation to aid future research on HSS."
  },
  {
    "title": "Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement Learning for Connected Autonomous Vehicles",
    "url": "http://arxiv.org/abs/2503.20462v1",
    "arxiv_id": "2503.20462v1",
    "authors": [
      "Ruoqi Wen",
      "Rongpeng Li",
      "Xing Xu",
      "Zhifeng Zhao"
    ],
    "published": "2025-03-26T11:49:02+00:00",
    "summary": "Deep Reinforcement Learning (DRL) holds significant promise for achieving human-like Autonomous Vehicle (AV) capabilities, but suffers from low sample efficiency and challenges in reward design. Model-Based Reinforcement Learning (MBRL) offers improved sample efficiency and generalizability compared to Model-Free Reinforcement Learning (MFRL) in various multi-agent decision-making scenarios. Nevertheless, MBRL faces critical difficulties in estimating uncertainty during the model learning phase, thereby limiting its scalability and applicability in real-world scenarios. Additionally, most Connected Autonomous Vehicle (CAV) studies focus on single-agent decision-making, while existing multi-agent MBRL solutions lack computationally tractable algorithms with Probably Approximately Correct (PAC) guarantees, an essential factor for ensuring policy reliability with limited training data. To address these challenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-Based Reinforcement Learning framework for CAVs, incorporating a max-min optimization approach to enhance robustness and decision-making. To mitigate the inherent subjectivity of uncertainty estimation in MBRL and avoid incurring catastrophic failures in AV, MA-PMBRL employs a pessimistic optimization framework combined with Projected Gradient Descent (PGD) for both model and policy learning. MA-PMBRL also employs general function approximations under partial dataset coverage to enhance learning efficiency and system-level performance. By bounding the suboptimality of the resulting policy under mild theoretical assumptions, we successfully establish PAC guarantees for MA-PMBRL, demonstrating that the proposed framework represents a significant step toward scalable, efficient, and reliable multi-agent decision-making for CAVs."
  },
  {
    "title": "The Crucial Role of Problem Formulation in Real-World Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20442v1",
    "arxiv_id": "2503.20442v1",
    "authors": [
      "Georg Sch\u00e4fer",
      "Tatjana Krau",
      "Jakob Rehrl",
      "Stefan Huber",
      "Simon Hirlaender"
    ],
    "published": "2025-03-26T11:17:44+00:00",
    "summary": "Reinforcement Learning (RL) offers promising solutions for control tasks in industrial cyber-physical systems (ICPSs), yet its real-world adoption remains limited. This paper demonstrates how seemingly small but well-designed modifications to the RL problem formulation can substantially improve performance, stability, and sample efficiency. We identify and investigate key elements of RL problem formulation and show that these enhance both learning speed and final policy quality. Our experiments use a one-degree-of-freedom (1-DoF) helicopter testbed, the Quanser Aero~2, which features non-linear dynamics representative of many industrial settings. In simulation, the proposed problem design principles yield more reliable and efficient training, and we further validate these results by training the agent directly on physical hardware. The encouraging real-world outcomes highlight the potential of RL for ICPS, especially when careful attention is paid to the design principles of problem formulation. Overall, our study underscores the crucial role of thoughtful problem formulation in bridging the gap between RL research and the demands of real-world industrial systems."
  },
  {
    "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
    "url": "http://arxiv.org/abs/2503.20425v1",
    "arxiv_id": "2503.20425v1",
    "authors": [
      "Kevin Alcedo",
      "Pedro U. Lima",
      "Rachid Alami"
    ],
    "published": "2025-03-26T10:59:08+00:00",
    "summary": "Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings."
  },
  {
    "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
    "url": "http://arxiv.org/abs/2503.20285v1",
    "arxiv_id": "2503.20285v1",
    "authors": [
      "Hongye Cao",
      "Fan Feng",
      "Jing Huo",
      "Shangdong Yang",
      "Meng Fang",
      "Tianpei Yang",
      "Yang Gao"
    ],
    "published": "2025-03-26T07:24:34+00:00",
    "summary": "Model-based offline Reinforcement Learning (RL) constructs environment models from offline datasets to perform conservative policy optimization. Existing approaches focus on learning state transitions through ensemble models, rollouting conservative estimation to mitigate extrapolation errors. However, the static data makes it challenging to develop a robust policy, and offline agents cannot access the environment to gather new data. To address these challenges, we introduce Model-based Offline Reinforcement learning with AdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon rollout by employing adversaria data augmentation to execute alternating sampling with ensemble models to enrich training data. Specifically, this adversarial process dynamically selects ensemble models against policy for biased sampling, mitigating the optimistic estimation of fixed models, thus robustly expanding the training data for policy optimization. Moreover, a differential factor is integrated into the adversarial process for regularization, ensuring error minimization in extrapolations. This data-augmented optimization adapts to diverse offline tasks without rollout horizon tuning, showing remarkable applicability. Extensive experiments on D4RL benchmark demonstrate that MORAL outperforms other model-based offline RL methods in terms of policy learning and sample efficiency."
  },
  {
    "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
    "url": "http://arxiv.org/abs/2503.20208v1",
    "arxiv_id": "2503.20208v1",
    "authors": [
      "Liangzhi Shi",
      "Yulin Liu",
      "Lingqi Zeng",
      "Bo Ai",
      "Zhengdong Hong",
      "Hao Su"
    ],
    "published": "2025-03-26T04:05:50+00:00",
    "summary": "How can robots learn dexterous grasping skills efficiently and apply them adaptively based on user instructions? This work tackles two key challenges: efficient skill acquisition from limited human demonstrations and context-driven skill selection. We introduce AdaDexGrasp, a framework that learns a library of grasping skills from a single human demonstration per skill and selects the most suitable one using a vision-language model (VLM). To improve sample efficiency, we propose a trajectory following reward that guides reinforcement learning (RL) toward states close to a human demonstration while allowing flexibility in exploration. To learn beyond the single demonstration, we employ curriculum learning, progressively increasing object pose variations to enhance robustness. At deployment, a VLM retrieves the appropriate skill based on user instructions, bridging low-level learned skills with high-level intent. We evaluate AdaDexGrasp in both simulation and real-world settings, showing that our approach significantly improves RL efficiency and enables learning human-like grasp strategies across varied object configurations. Finally, we demonstrate zero-shot transfer of our learned policies to a real-world PSYONIC Ability Hand, with a 90% success rate across objects, significantly outperforming the baseline."
  },
  {
    "title": "Generalized Phase Pressure Control Enhanced Reinforcement Learning for Traffic Signal Control",
    "url": "http://arxiv.org/abs/2503.20205v1",
    "arxiv_id": "2503.20205v1",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang",
      "Xiang-Ling Chen"
    ],
    "published": "2025-03-26T04:03:12+00:00",
    "summary": "Appropriate traffic state representation is crucial for learning traffic signal control policies. However, most of the current traffic state representations are heuristically designed, with insufficient theoretical support. In this paper, we (1) develop a flexible, efficient, and theoretically grounded method, namely generalized phase pressure (G2P) control, which takes only simple lane features into consideration to decide which phase to be actuated; 2) extend the pressure control theory to a general form for multi-homogeneous-lane road networks based on queueing theory; (3) design a new traffic state representation based on the generalized phase state features from G2P control; and 4) develop a reinforcement learning (RL)-based algorithm template named G2P-XLight, and two RL algorithms, G2P-MPLight and G2P-CoLight, by combining the generalized phase state representation with MPLight and CoLight, two well-performed RL methods for learning traffic signal control policies. Extensive experiments conducted on multiple real-world datasets demonstrate that G2P control outperforms the state-of-the-art (SOTA) heuristic method in the transportation field and other recent human-designed heuristic methods; and that the newly proposed G2P-XLight significantly outperforms SOTA learning-based approaches. Our code is available online."
  },
  {
    "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization",
    "url": "http://arxiv.org/abs/2503.20194v1",
    "arxiv_id": "2503.20194v1",
    "authors": [
      "Zhouhong Gu",
      "Xingzhou Chen",
      "Xiaoran Shi",
      "Tao Wang",
      "Suhang Zheng",
      "Tianyu Li",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "published": "2025-03-26T03:37:52+00:00",
    "summary": "Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in https://github.com/MikeGu721/GAPO."
  },
  {
    "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
    "url": "http://arxiv.org/abs/2503.20176v1",
    "arxiv_id": "2503.20176v1",
    "authors": [
      "RuiXi Qiao",
      "Jie Cheng",
      "Xingyuan Dai",
      "Yonglin Tian",
      "Yisheng Lv"
    ],
    "published": "2025-03-26T03:04:42+00:00",
    "summary": "Skills have been introduced to offline reinforcement learning (RL) as temporal abstractions to tackle complex, long-horizon tasks, promoting consistent behavior and enabling meaningful exploration. While skills in offline RL are predominantly modeled within a continuous latent space, the potential of discrete skill spaces remains largely underexplored. In this paper, we propose a compact discrete skill space for offline RL tasks supported by state-of-the-art transformer-based encoder and diffusion-based decoder. Coupled with a high-level policy trained via offline RL techniques, our method establishes a hierarchical RL framework where the trained diffusion decoder plays a pivotal role. Empirical evaluations show that the proposed algorithm, Discrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs competitively on Locomotion and Kitchen tasks and excels on long-horizon tasks, achieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared to existing offline RL approaches. Furthermore, DDS offers improved interpretability, training stability, and online exploration compared to previous skill-based methods."
  },
  {
    "title": "Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20139v1",
    "arxiv_id": "2503.20139v1",
    "authors": [
      "Yongshuai Liu",
      "Xin Liu"
    ],
    "published": "2025-03-26T01:07:35+00:00",
    "summary": "Model-based reinforcement learning (MBRL) has demonstrated superior sample efficiency compared to model-free reinforcement learning (MFRL). However, the presence of inaccurate models can introduce biases during policy learning, resulting in misleading trajectories. The challenge lies in obtaining accurate models due to limited diverse training data, particularly in regions with limited visits (uncertain regions). Existing approaches passively quantify uncertainty after sample generation, failing to actively collect uncertain samples that could enhance state coverage and improve model accuracy. Moreover, MBRL often faces difficulties in making accurate multi-step predictions, thereby impacting overall performance. To address these limitations, we propose a novel framework for uncertainty-aware policy optimization with model-based exploratory planning. In the model-based planning phase, we introduce an uncertainty-aware k-step lookahead planning approach to guide action selection at each step. This process involves a trade-off analysis between model uncertainty and value function approximation error, effectively enhancing policy performance. In the policy optimization phase, we leverage an uncertainty-driven exploratory policy to actively collect diverse training samples, resulting in improved model accuracy and overall performance of the RL agent. Our approach offers flexibility and applicability to tasks with varying state/action spaces and reward structures. We validate its effectiveness through experiments on challenging robotic manipulation tasks and Atari games, surpassing state-of-the-art methods with fewer interactions, thereby leading to significant performance improvements."
  },
  {
    "title": "Synthesizing world models for bilevel planning",
    "url": "http://arxiv.org/abs/2503.20124v1",
    "arxiv_id": "2503.20124v1",
    "authors": [
      "Zergham Ahmed",
      "Joshua B. Tenenbaum",
      "Christopher J. Bates",
      "Samuel J. Gershman"
    ],
    "published": "2025-03-26T00:10:01+00:00",
    "summary": "Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - \"theories\" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., \"move to\"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions."
  },
  {
    "title": "Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.20078v1",
    "arxiv_id": "2503.20078v1",
    "authors": [
      "Volkan Ustun",
      "Soham Hans",
      "Rajay Kumar",
      "Yunzhe Wang"
    ],
    "published": "2025-03-25T21:29:49+00:00",
    "summary": "Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in training dynamic and adaptive synthetic characters for interactive simulations on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make such reinforcement learning experiments more accessible to the simulation community. Military training simulations also benefit from advances in MARL, but they have immense computational requirements due to their complex, continuous, stochastic, partially observable, non-stationary, and doctrine-based nature. Furthermore, these simulations require geo-specific terrains, further exacerbating the computational resources problem. In our research, we leverage Unity's waypoints to automatically generate multi-layered representation abstractions of the geo-specific terrains to scale up reinforcement learning while still allowing the transfer of learned policies between different representations. Our early exploratory results on a novel MARL scenario, where each side has differing objectives, indicate that waypoint-based navigation enables faster and more efficient learning while producing trajectories similar to those taken by expert human players in CSGO gaming environments. This research points out the potential of waypoint-based navigation for reducing the computational costs of developing and training MARL models for military training simulations, where geo-specific terrains and differing objectives are crucial."
  },
  {
    "title": "Unsupervised Learning for Quadratic Assignment",
    "url": "http://arxiv.org/abs/2503.20001v1",
    "arxiv_id": "2503.20001v1",
    "authors": [
      "Yimeng Min",
      "Carla P. Gomes"
    ],
    "published": "2025-03-25T18:37:46+00:00",
    "summary": "We introduce PLUME search, a data-driven framework that enhances search efficiency in combinatorial optimization through unsupervised learning. Unlike supervised or reinforcement learning, PLUME search learns directly from problem instances using a permutation-based loss with a non-autoregressive approach. We evaluate its performance on the quadratic assignment problem, a fundamental NP-hard problem that encompasses various combinatorial optimization problems. Experimental results demonstrate that PLUME search consistently improves solution quality. Furthermore, we study the generalization behavior and show that the learned model generalizes across different densities and sizes."
  },
  {
    "title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback",
    "url": "http://arxiv.org/abs/2503.19988v1",
    "arxiv_id": "2503.19988v1",
    "authors": [
      "Bohan Zhai",
      "Canwen Xu",
      "Yuxiong He",
      "Zhewei Yao"
    ],
    "published": "2025-03-25T18:17:36+00:00",
    "summary": "Text-to-SQL demands precise reasoning to convert natural language questions into structured queries. While large language models (LLMs) excel in many reasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for text-to-SQL remains underexplored. We identify critical limitations: zero-shot CoT offers minimal gains, and Direct Preference Optimization (DPO) applied without CoT yields marginal improvements. We propose ExCoT, a novel framework that iteratively optimizes open-source LLMs by combining CoT reasoning with off-policy and on-policy DPO, relying solely on execution accuracy as feedback. This approach eliminates the need for reward models or human-annotated preferences.   Our experimental results demonstrate significant performance gains: ExCoT improves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider test set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder demonstrating similar improvements. Our best model achieves state-of-the-art performance in the single-model setting on both BIRD and Spider datasets, notably achieving 68.53% on the BIRD test set."
  },
  {
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking",
    "url": "http://arxiv.org/abs/2503.19855v1",
    "arxiv_id": "2503.19855v1",
    "authors": [
      "Xiaoyu Tian",
      "Sitong Zhao",
      "Haotian Wang",
      "Shuaiting Chen",
      "Yunjie Ji",
      "Yiping Peng",
      "Han Zhao",
      "Xiangang Li"
    ],
    "published": "2025-03-25T17:19:38+00:00",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer."
  },
  {
    "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
    "url": "http://arxiv.org/abs/2503.19948v1",
    "arxiv_id": "2503.19948v1",
    "authors": [
      "Alexander Gambashidze",
      "Konstantin Sobolev",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "published": "2025-03-25T15:30:21+00:00",
    "summary": "Can Visual Language Models (VLMs) effectively capture human visual preferences? This work addresses this question by training VLMs to think about preferences at test time, employing reinforcement learning methods inspired by DeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human Preference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the ImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2 (trained on approximately 25% of its data). These results match traditional encoder-based models while providing transparent reasoning and enhanced generalization. This approach allows to use not only rich VLM world knowledge, but also its potential to think, yielding interpretable outcomes that help decision-making processes. By demonstrating that human visual preferences reasonable by current VLMs, we introduce efficient soft-reward strategies for image ranking, outperforming simplistic selection or scoring methods. This reasoning capability enables VLMs to rank arbitrary images-regardless of aspect ratio or complexity-thereby potentially amplifying the effectiveness of visual Preference Optimization. By reducing the need for extensive markup while improving reward generalization and explainability, our findings can be a strong mile-stone that will enhance text-to-vision models even further."
  },
  {
    "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
    "url": "http://arxiv.org/abs/2503.19699v1",
    "arxiv_id": "2503.19699v1",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "published": "2025-03-25T14:27:29+00:00",
    "summary": "In this study, we formulate the drone delivery problem as a control problem and solve it using Model Predictive Control. Two experiments are performed: The first is on a less challenging grid world environment with lower dimensionality, and the second is with a higher dimensionality and added complexity. The MPC method was benchmarked against three popular Multi-Agent Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the MPC method solved the problem quicker and required fewer optimal numbers of drones to achieve a minimized cost and navigate the optimal path."
  },
  {
    "title": "Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection",
    "url": "http://arxiv.org/abs/2503.19690v2",
    "arxiv_id": "2503.19690v2",
    "authors": [
      "Bo Leng",
      "Ran Yu",
      "Wei Han",
      "Lu Xiong",
      "Zhuoren Li",
      "Hailong Huang"
    ],
    "published": "2025-03-25T14:17:15+00:00",
    "summary": "Applying reinforcement learning to autonomous driving has garnered widespread attention. However, classical reinforcement learning methods optimize policies by maximizing expected rewards but lack sufficient safety considerations, often putting agents in hazardous situations. This paper proposes a risk-aware reinforcement learning approach for autonomous driving to improve the safety performance when crossing the intersection. Safe critics are constructed to evaluate driving risk and work in conjunction with the reward critic to update the actor. Based on this, a Lagrangian relaxation method and cyclic gradient iteration are combined to project actions into a feasible safe region. Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention Mechanism (MMAM) is incorporated into the actor-critic network, enabling the policy to adapt to dynamic traffic and overcome permutation sensitivity challenges. This allows the policy to focus more effectively on surrounding potential risks while enhancing the identification of passing opportunities. Simulation tests are conducted on different tasks at unsignalized intersections. The results show that the proposed approach effectively reduces collision rates and improves crossing efficiency in comparison to baseline algorithms. Additionally, our ablation experiments demonstrate the benefits of incorporating risk-awareness and MMAM into RL."
  },
  {
    "title": "Learning to chain-of-thought with Jensen's evidence lower bound",
    "url": "http://arxiv.org/abs/2503.19618v1",
    "arxiv_id": "2503.19618v1",
    "authors": [
      "Yunhao Tang",
      "Sid Wang",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T13:03:09+00:00",
    "summary": "We propose a way to optimize chain-of-thought with reinforcement learning, but without external reward function. Our algorithm relies on viewing chain-of-thought as latent variable as part of a probabilistic inference problem. Contrary to the full evidence lower bound, we propose to apply a much simpler Jensen's lower bound, which derives tractable objectives with simple algorithmic components (e.g., without the need for parametric approximate posterior), making it more conducive to modern large-scale training. The lower bound approach naturally interpolates other methods such as supervised fine-tuning and online reinforcement learning, whose practical trade-offs we will illustrate. Finally, we show that on mathematical reasoning problems, optimizing with Jensen's lower bound is as effective as policy gradient with external reward. Taken together, our results showcase as a proof of concept to this new algorithmic paradigm's potential to more generic applications."
  },
  {
    "title": "RL-finetuning LLMs from on- and off-policy data with a single algorithm",
    "url": "http://arxiv.org/abs/2503.19612v1",
    "arxiv_id": "2503.19612v1",
    "authors": [
      "Yunhao Tang",
      "Taco Cohen",
      "David W. Zhang",
      "Michal Valko",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T12:52:38+00:00",
    "summary": "We introduce a novel reinforcement learning algorithm (AGRO, for Any-Generation Reward Optimization) for fine-tuning large-language models. AGRO leverages the concept of generation consistency, which states that the optimal policy satisfies the notion of consistency across any possible generation of the model. We derive algorithms that find optimal solutions via the sample-based policy gradient and provide theoretical guarantees on their convergence. Our experiments demonstrate the effectiveness of AGRO in both on-policy and off-policy settings, showing improved performance on the mathematical reasoning dataset over baseline algorithms."
  },
  {
    "title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning",
    "url": "http://arxiv.org/abs/2503.19595v1",
    "arxiv_id": "2503.19595v1",
    "authors": [
      "Yunhao Tang",
      "Kunhao Zheng",
      "Gabriel Synnaeve",
      "R\u00e9mi Munos"
    ],
    "published": "2025-03-25T12:21:26+00:00",
    "summary": "In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic inference time objectives with $k$ samples, with a focus on pass@$k$ and majority voting as two main applications. With language model training on reasoning datasets, we showcase the performance trade-off enabled by training with such objectives. When training on code generation tasks, we show that the approach significantly improves pass@$k$ objectives compared to the baseline method."
  },
  {
    "title": "ZodiAq: An Isotropic Flagella-Inspired Soft Underwater Drone for Safe Marine Exploration",
    "url": "http://arxiv.org/abs/2503.19556v1",
    "arxiv_id": "2503.19556v1",
    "authors": [
      "Anup Teejo Mathew",
      "Daniel Feliu-Talegon",
      "Yusuf Abdullahi Adamu",
      "Ikhlas Ben Hmida",
      "Costanza Armanini",
      "Cesare Stefanini",
      "Lakmal Seneviratne",
      "Federico Renda"
    ],
    "published": "2025-03-25T11:23:31+00:00",
    "summary": "The inherent challenges of robotic underwater exploration, such as hydrodynamic effects, the complexity of dynamic coupling, and the necessity for sensitive interaction with marine life, call for the adoption of soft robotic approaches in marine exploration. To address this, we present a novel prototype, ZodiAq, a soft underwater drone inspired by prokaryotic bacterial flagella. ZodiAq's unique dodecahedral structure, equipped with 12 flagella-like arms, ensures design redundancy and compliance, ideal for navigating complex underwater terrains. The prototype features a central unit based on a Raspberry Pi, connected to a sensory system for inertial, depth, and vision detection, and an acoustic modem for communication. Combined with the implemented control law, it renders ZodiAq an intelligent system. This paper details the design and fabrication process of ZodiAq, highlighting design choices and prototype capabilities. Based on the strain-based modeling of Cosserat rods, we have developed a digital twin of the prototype within a simulation toolbox to ease analysis and control. To optimize its operation in dynamic aquatic conditions, a simplified model-based controller has been developed and implemented, facilitating intelligent and adaptive movement in the hydrodynamic environment. Extensive experimental demonstrations highlight the drone's potential, showcasing its design redundancy, embodied intelligence, crawling gait, and practical applications in diverse underwater settings. This research contributes significantly to the field of underwater soft robotics, offering a promising new avenue for safe, efficient, and environmentally conscious underwater exploration."
  },
  {
    "title": "One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF",
    "url": "http://arxiv.org/abs/2503.19523v2",
    "arxiv_id": "2503.19523v2",
    "authors": [
      "Xin Cai"
    ],
    "published": "2025-03-25T10:23:26+00:00",
    "summary": "In this article, we primarily examine a variety of RL-based and RL-free methods designed to address Reinforcement Learning from Human Feedback (RLHF) and Large Reasoning Models (LRMs). We begin with a concise overview of the typical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based and RL-free algorithms through the perspective of neural structured bandit prediction, providing a clear conceptual framework that uncovers a deeper connection between these seemingly distinct approaches. Following this, we briefly review some core principles of reinforcement learning, drawing attention to an often-overlooked aspect in existing RLHF studies. This leads to a detailed derivation of the standard RLHF objective within a full RL context, demonstrating its equivalence to neural structured bandit prediction. Finally, by reinvestigating the principles behind Proximal Policy Optimization (PPO), we pinpoint areas needing adjustment, which culminates in the introduction of the Generalized Reinforce Optimization (GRO) framework, seamlessly integrating RL-based and RL-free methods in RLHF. We look forward to the community's efforts to empirically validate GRO and invite constructive feedback."
  }
]
[
  {
    "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
    "url": "http://arxiv.org/abs/2508.05634v1",
    "arxiv_id": "2508.05634v1",
    "authors": [
      "Jianpeng Yao",
      "Xiaopan Zhang",
      "Yu Xia",
      "Zejin Wang",
      "Amit K. Roy-Chowdhury",
      "Jiachen Li"
    ],
    "published": "2025-08-07T17:59:43+00:00",
    "summary": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/."
  },
  {
    "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees",
    "url": "http://arxiv.org/abs/2508.05544v1",
    "arxiv_id": "2508.05544v1",
    "authors": [
      "Guang Yang",
      "Xinyang Liu"
    ],
    "published": "2025-08-07T16:22:49+00:00",
    "summary": "Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications."
  },
  {
    "title": "Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy",
    "url": "http://arxiv.org/abs/2508.05466v1",
    "arxiv_id": "2508.05466v1",
    "authors": [
      "Yun Li",
      "Jicheng Shi",
      "Colin N. Jones",
      "Neil Yorke-Smith",
      "Tamas Keviczky"
    ],
    "published": "2025-08-07T15:04:27+00:00",
    "summary": "This paper studies the finite-horizon robust optimal control of linear systems subject to model mismatch and additive stochastic disturbances. Utilizing the system level synthesis (SLS) parameterization, we propose a novel SLS design using output-feedback affine control policy and extend it to a distributionally robust setting to improve system resilience by minimizing the cost function while ensuring constraint satisfaction against the worst-case uncertainty distribution. The scopes of model mismatch and stochastic disturbances are quantified using the 1-norm and a Wasserstein metric-based ambiguity set, respectively. For the closed-loop dynamics, we analyze the distributional shift between the predicted output-input response -- computed using nominal parameters and empirical disturbance samples -- and the actual closed-loop distribution, highlighting its dependence on model mismatch and SLS parameterization. Assuming convex and Lipschitz continuous cost functions and constraints, we derive a tractable reformulation of the distributionally robust SLS (DR-SLS) problem by leveraging tools from robust control and distributionally robust optimization (DRO). Numerical experiments validate the performance and robustness of the proposed approach."
  },
  {
    "title": "EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting",
    "url": "http://arxiv.org/abs/2508.05454v1",
    "arxiv_id": "2508.05454v1",
    "authors": [
      "Wei Li",
      "Zixin Wang",
      "Qizheng Sun",
      "Qixiang Gao",
      "Fenglei Yang"
    ],
    "published": "2025-08-07T14:48:39+00:00",
    "summary": "Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field."
  },
  {
    "title": "Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees",
    "url": "http://arxiv.org/abs/2508.05441v1",
    "arxiv_id": "2508.05441v1",
    "authors": [
      "Zuyuan Zhang",
      "Arnob Ghosh",
      "Tian Lan"
    ],
    "published": "2025-08-07T14:31:22+00:00",
    "summary": "Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\\alpha$ achieves explicit tail-risk control over the expected loss in the \"worst $(1-\\alpha)\\%$ scenarios.\" Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\\mathcal{P}_{\\varepsilon_{s}}(s,a)$ with radius $\\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability."
  },
  {
    "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
    "url": "http://arxiv.org/abs/2508.05421v1",
    "arxiv_id": "2508.05421v1",
    "authors": [
      "Rong Sha",
      "Binglin Wang",
      "Jun Yang",
      "Xiaoxiao Ma",
      "Chengkun Wu",
      "Liang Yan",
      "Chao Zhou",
      "Jixun Liu",
      "Guochao Wang",
      "Shuhua Yan",
      "Lingxiao Zhu"
    ],
    "published": "2025-08-07T14:14:08+00:00",
    "summary": "Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a few hours, representing $\\sim$100$\\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems."
  },
  {
    "title": "Metallicity of Active Galactic Nuclei from ultraviolet and optical emission lines-II. Revisiting the $C43$ metallicity calibration and its implications",
    "url": "http://arxiv.org/abs/2508.05397v1",
    "arxiv_id": "2508.05397v1",
    "authors": [
      "O. L. Dors",
      "C. B. Oliveira",
      "M. V. Cardaci",
      "G. F. H\u00e4gele",
      "Mark Armah",
      "R. A. Riffel",
      "L. Ramos Vieira",
      "G. C. Almeida",
      "I. N. Morais",
      "P. C. Santos"
    ],
    "published": "2025-08-07T13:50:00+00:00",
    "summary": "In this study, a new semi-empirical calibration is proposed between ultraviolet emission lines (\\ion{C}{iii}]$\\lambda1909$, \\ion{C}{iv}$\\lambda1549$, \\ion{He}{ii}]$\\lambda1640$) of type~2 AGNs and their metallicity ($Z$). This calibration is derived by comparing a large sample of 106 objects (data taken from the literature) located over a wide range of redshifts ($0 \\: \\lesssim \\: z \\: \\lesssim \\: 4.0$) with predictions from photoionization models that adopt a recent C/O-O/H relation derived via estimates using the $T_{\\rm e}$ method, which is considered the most reliable method. We found that the new calibration produces $Z$ values in agreement (within an uncertainty of $\\pm 0.1$ dex) with those from other calibrations and from estimates via the $T_{\\rm e}$-method. We find also that AGN metallicities are already high at early epochs, with no evidence for monotonic evolution across the redshift range $0 \\: \\lesssim \\: z \\: \\lesssim \\: 12$. Notably, the highest metallicities in our sample, reaching up to $\\rm 4\\: Z_{\\odot}$, are found in objects at $2 \\lesssim z \\lesssim 3$. This redshift range coincides with the peak of the cosmic star formation rate history, suggesting a strong connection between the major epoch of star formation, black hole growth, and rapid metal enrichment in the host galaxies of AGNs. Furthermore, our analysis reveals no significant correlation between AGN metallicity and radio properties (radio spectral index or radio luminosity) or host galaxy stellar mass. The lack of a clear mass-metallicity relation, consistent with findings for local AGNs, suggests that the chemical evolution of the nuclear gas is decoupled from the global properties of the host galaxy."
  },
  {
    "title": "Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games",
    "url": "http://arxiv.org/abs/2508.05378v1",
    "arxiv_id": "2508.05378v1",
    "authors": [
      "Zhisen Jiang",
      "Saverio Bolognani",
      "Giuseppe Belgioioso"
    ],
    "published": "2025-08-07T13:21:10+00:00",
    "summary": "The integration of distributed energy resources into transmission grid operations presents a complex challenge, particularly in the context of reactive power procurement for voltage support. This paper addresses this challenge by formulating the voltage regulation problem as a Stackelberg game, where the Transmission System Operator (TSO) designs incentives to guide the reactive power responses of Distribution System Operators (DSOs). We utilize a gradient-based iterative algorithm that updates the incentives to ensure that DSOs adjust their reactive power injections to maintain voltage stability. We incorporate principles from online feedback optimization to enable real-time implementation, utilizing voltage measurements in both TSO's and DSOs' policies. This approach not only enhances the robustness against model uncertainties and changing operating conditions but also facilitates the co-design of incentives and automation. Numerical experiments on a 5-bus transmission grid demonstrate the effectiveness of our approach in achieving voltage regulation while accommodating the strategic interactions of self-interested DSOs."
  },
  {
    "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
    "url": "http://arxiv.org/abs/2508.05342v1",
    "arxiv_id": "2508.05342v1",
    "authors": [
      "Shunlei Li",
      "Longsen Gao",
      "Jin Wang",
      "Chang Che",
      "Xi Xiao",
      "Jiuwen Cao",
      "Yingbai Hu",
      "Hamid Reza Karimi"
    ],
    "published": "2025-08-07T12:48:09+00:00",
    "summary": "Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations."
  },
  {
    "title": "ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning",
    "url": "http://arxiv.org/abs/2508.05310v1",
    "arxiv_id": "2508.05310v1",
    "authors": [
      "Jelle Luijkx",
      "Zlatan Ajanovi\u0107",
      "Laura Ferranti",
      "Jens Kober"
    ],
    "published": "2025-08-07T12:10:46+00:00",
    "summary": "Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: \"I plan to do this, but I am uncertain.\" We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io."
  },
  {
    "title": "Occupancy Learning with Spatiotemporal Memory",
    "url": "http://arxiv.org/abs/2508.04705v1",
    "arxiv_id": "2508.04705v1",
    "authors": [
      "Ziyang Leng",
      "Jiawei Yang",
      "Wenlong Yi",
      "Bolei Zhou"
    ],
    "published": "2025-08-06T17:59:52+00:00",
    "summary": "3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%."
  },
  {
    "title": "Open Scene Graphs for Open-World Object-Goal Navigation",
    "url": "http://arxiv.org/abs/2508.04678v1",
    "arxiv_id": "2508.04678v1",
    "authors": [
      "Joel Loo",
      "Zhanxin Wu",
      "David Hsu"
    ],
    "published": "2025-08-06T17:43:29+00:00",
    "summary": "How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments."
  },
  {
    "title": "Stochastic Calculus for Pathwise Observables of Markov-Jump Processes: Unification of Diffusion and Jump Dynamics",
    "url": "http://arxiv.org/abs/2508.04647v1",
    "arxiv_id": "2508.04647v1",
    "authors": [
      "Lars Torbj\u00f8rn Stutzer",
      "Cai Dieball",
      "Alja\u017e Godec"
    ],
    "published": "2025-08-06T17:15:03+00:00",
    "summary": "Path-wise observables--functionals of stochastic trajectories--are at the heart of time-average statistical mechanics and are central to thermodynamic inequalities such as uncertainty relations, speed limits, and correlation-bounds. They provide a means of thermodynamic inference in the typical situation, when not all dissipative degrees of freedom in a system are experimentally accessible. So far, theories focusing on path-wise observables have been developing in two major directions, diffusion processes and Markov-jump dynamics, in a virtually disjoint manner. Moreover, even the respective results for diffusion and jump dynamics were derived with a patchwork of different approaches that are predominantly indirect. Stochastic calculus was recently shown to provide a direct approach to path-wise observables of diffusion processes, while a corresponding framework for jump dynamics remained elusive. In our work we develop, in an exact parallelism with continuous-space diffusion, a complete stochastic calculus for path-wise observables of Markov-jump processes. We formulate a \"Langevin equation\" for jump processes, define general path-wise observables, and establish their covariation structure, whereby we fully account for transients and time-inhomogeneous dynamics. We prove the known kinds of thermodynamic inequalities in their most general form and discus saturation conditions. We determine the response of path-wise observables to general (incl. thermal) perturbations and carry out the continuum limit to achieve the complete unification of diffusion and jump dynamics. Our results open new avenues in the direction of discrete-state analogs of generative diffusion models and the learning of stochastic thermodynamics from fluctuating trajectories."
  },
  {
    "title": "Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials",
    "url": "http://arxiv.org/abs/2508.04633v1",
    "arxiv_id": "2508.04633v1",
    "authors": [
      "James P. Long",
      "Abhishikta Roy",
      "Ehsan Irajizad",
      "Kim-Anh Do",
      "Yu Shen"
    ],
    "published": "2025-08-06T17:01:15+00:00",
    "summary": "In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model."
  },
  {
    "title": "A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI",
    "url": "http://arxiv.org/abs/2508.04588v1",
    "arxiv_id": "2508.04588v1",
    "authors": [
      "Nicola Casali",
      "Alessandro Brusaferri",
      "Giuseppe Baselli",
      "Stefano Fumagalli",
      "Edoardo Micotti",
      "Gianluigi Forloni",
      "Riaz Hussein",
      "Giovanna Rizzo",
      "Alfonso Mastropietro"
    ],
    "published": "2025-08-06T16:08:55+00:00",
    "summary": "Accurate estimation of intravoxel incoherent motion (IVIM) parameters from diffusion-weighted MRI remains challenging due to the ill-posed nature of the inverse problem and high sensitivity to noise, particularly in the perfusion compartment. In this work, we propose a probabilistic deep learning framework based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling estimation of total predictive uncertainty and decomposition into aleatoric (AU) and epistemic (EU) components. The method was benchmarked against non probabilistic neural networks, a Bayesian fitting approach and a probabilistic network with single Gaussian parametrization. Supervised training was performed on synthetic data, and evaluation was conducted on both simulated and two in vivo datasets. The reliability of the quantified uncertainties was assessed using calibration curves, output distribution sharpness, and the Continuous Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper predictive distributions for the D and f parameters, although slight overconfidence was observed in D*. The Robust Coefficient of Variation (RCV) indicated smoother in vivo estimates for D* with MDNs compared to Gaussian model. Despite the training data covering the expected physiological range, elevated EU in vivo suggests a mismatch with real acquisition conditions, highlighting the importance of incorporating EU, which was allowed by DE. Overall, we present a comprehensive framework for IVIM fitting with uncertainty quantification, which enables the identification and interpretation of unreliable estimates. The proposed approach can also be adopted for fitting other physical models through appropriate architectural and simulation adjustments."
  },
  {
    "title": "Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming",
    "url": "http://arxiv.org/abs/2508.04570v1",
    "arxiv_id": "2508.04570v1",
    "authors": [
      "A. Tarik Leblebici",
      "Sumeyra Hassan",
      "Erdal Panayirci",
      "H. Vincent Poor"
    ],
    "published": "2025-08-06T15:53:40+00:00",
    "summary": "This paper proposes a joint communication and indoor positioning (JCP) system based on visible light communication (VLC) designed for high-precision indoor environments. The framework supports 2D and 3D positioning using received signal strength (RSS) from pilot transmissions, enhanced by the radical axis theorem to improve accuracy under measurement uncertainties. Communication is achieved using spatial modulation (SM) with M-ary pulse amplitude modulation (PAM), where data is conveyed through the modulation symbol and the active light-emitting diode (LED) index, improving spectral efficiency while maintaining low complexity. A pilot-aided least squares (LS) estimator is employed for joint channel and dimming coefficient estimation, enabling robust symbol detection in multipath environments characterized by both line-of-sight (LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician fading. The proposed system incorporates a dimming control mechanism to meet lighting requirements while maintaining reliable communication and positioning performance. Simulation results demonstrate sub-centimeter localization accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below 10^{-6} for low-order PAM schemes. Additionally, comparative analysis across user locations reveals that positioning and communication performance improve significantly near the geometric center of the LED layout. These findings validate the effectiveness of the proposed system for future 6G indoor networks requiring integrated localization and communication under practical channel conditions."
  },
  {
    "title": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments",
    "url": "http://arxiv.org/abs/2508.04537v1",
    "arxiv_id": "2508.04537v1",
    "authors": [
      "Alkesh K. Srivastava",
      "Aamodh Suresh",
      "Carlos Nieto-Granda"
    ],
    "published": "2025-08-06T15:23:22+00:00",
    "summary": "We address the challenge of multi-robot autonomous hazard mapping in high-risk, failure-prone, communication-denied environments such as post-disaster zones, underground mines, caves, and planetary surfaces. In these missions, robots must explore and map hazards while minimizing the risk of failure due to environmental threats or hardware limitations. We introduce a behavior-adaptive, information-theoretic planning framework for multi-robot teams grounded in the concept of Behavioral Entropy (BE), that generalizes Shannon entropy (SE) to capture diverse human-like uncertainty evaluations. Building on this formulation, we propose the Behavior-Adaptive Path Planning (BAPP) framework, which modulates information gathering strategies via a tunable risk-sensitivity parameter, and present two planning algorithms: BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for safe deployment under high risk. We provide theoretical insights on the informativeness of the proposed BAPP framework and validate its effectiveness through both single-robot and multi-robot simulations. Our results show that the BAPP stack consistently outperforms Shannon-based and random strategies: BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot survivability with minimal loss in information gain. In multi-agent deployments, BAPP scales effectively through spatial partitioning, mobile base relocation, and role-aware heterogeneity. These findings underscore the value of behavior-adaptive planning for robust, risk-sensitive exploration in complex, failure-prone environments."
  },
  {
    "title": "Metric Learning in an RKHS",
    "url": "http://arxiv.org/abs/2508.04476v1",
    "arxiv_id": "2508.04476v1",
    "authors": [
      "Gokcan Tatli",
      "Yi Chen",
      "Blake Mason",
      "Robert Nowak",
      "Ramya Korlakai Vinayak"
    ],
    "published": "2025-08-06T14:29:04+00:00",
    "summary": "Metric learning from a set of triplet comparisons in the form of \"Do you think item h is more similar to item i or item j?\", indicating similarity and differences between items, plays a key role in various applications including image retrieval, recommendation systems, and cognitive psychology. The goal is to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric learning using kernel methods and neural networks have shown great empirical promise. While previous works have addressed certain aspects of this problem, there is little or no theoretical understanding of such methods. The exception is the special (linear) case in which the RKHS is the standard Euclidean space $\\mathbb{R}^d$; there is a comprehensive theory for metric learning in $\\mathbb{R}^d$. This paper develops a general RKHS framework for metric learning and provides novel generalization guarantees and sample complexity bounds. We validate our findings through a set of simulations and experiments on real datasets. Our code is publicly available at https://github.com/RamyaLab/metric-learning-RKHS."
  },
  {
    "title": "Case Studies of Generative Machine Learning Models for Dynamical Systems",
    "url": "http://arxiv.org/abs/2508.04459v1",
    "arxiv_id": "2508.04459v1",
    "authors": [
      "Nachiket U. Bapat",
      "Randy C. Paffenroth",
      "Raghvendra V. Cowlagi"
    ],
    "published": "2025-08-06T13:59:14+00:00",
    "summary": "Systems like aircraft and spacecraft are expensive to operate in the real world. The design, validation, and testing for such systems therefore relies on a combination of mathematical modeling, abundant numerical simulations, and a relatively small set of real-world experiments. Due to modeling errors, simplifications, and uncertainties, the data synthesized by simulation models often does not match data from the system's real-world operation. We consider the broad research question of whether this model mismatch can be significantly reduced by generative artificial intelligence models (GAIMs). Unlike text- or image-processing, where generative models have attained recent successes, GAIM development for aerospace engineering applications must not only train with scarce operational data, but their outputs must also satisfy governing equations based on natural laws, e.g., conservation laws. The scope of this paper primarily focuses on two case studies of optimally controlled systems that are commonly understood and employed in aircraft guidance, namely: minimum-time navigation in a wind field and minimum-exposure navigation in a threat field. We report GAIMs that are trained with a relatively small set, of the order of a few hundred, of examples and with underlying governing equations. By focusing on optimally controlled systems, we formulate training loss functions based on invariance of the Hamiltonian function along system trajectories. We investigate three GAIM architectures, namely: the generative adversarial network (GAN) and two variants of the variational autoencoder (VAE). We provide architectural details and thorough performance analyses of these models. The main finding is that our new models, especially the VAE-based models, are able to synthesize data that satisfy the governing equations and are statistically similar to the training data despite small volumes of training data."
  },
  {
    "title": "Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification",
    "url": "http://arxiv.org/abs/2508.04457v1",
    "arxiv_id": "2508.04457v1",
    "authors": [
      "Simon Baur",
      "Wojciech Samek",
      "Jackie Ma"
    ],
    "published": "2025-08-06T13:58:17+00:00",
    "summary": "Reliable uncertainty quantification is crucial for trustworthy decision-making and the deployment of AI models in medical imaging. While prior work has explored the ability of neural networks to quantify predictive, epistemic, and aleatoric uncertainties using an information-theoretical approach in synthetic or well defined data settings like natural image classification, its applicability to real life medical diagnosis tasks remains underexplored. In this study, we provide an extensive uncertainty quantification benchmark for multi-label chest X-ray classification using the MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for convolutional (ResNet) and transformer-based (Vision Transformer) architectures across a wide range of tasks. Additionally, we extend Evidential Deep Learning, HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting. Our analysis provides insights into uncertainty estimation effectiveness and the ability to disentangle epistemic and aleatoric uncertainties, revealing method- and architecture-specific strengths and limitations."
  },
  {
    "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences",
    "url": "http://arxiv.org/abs/2508.03692v1",
    "arxiv_id": "2508.03692v1",
    "authors": [
      "Ao Liang",
      "Youquan Liu",
      "Yu Yang",
      "Dongyue Lu",
      "Linfeng Li",
      "Lingdong Kong",
      "Huaici Zhao",
      "Wei Tsang Ooi"
    ],
    "published": "2025-08-05T17:59:56+00:00",
    "summary": "Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community."
  },
  {
    "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data",
    "url": "http://arxiv.org/abs/2508.03691v1",
    "arxiv_id": "2508.03691v1",
    "authors": [
      "Youquan Liu",
      "Lingdong Kong",
      "Weidong Yang",
      "Xin Li",
      "Ao Liang",
      "Runnan Chen",
      "Ben Fei",
      "Tongliang Liu"
    ],
    "published": "2025-08-05T17:59:55+00:00",
    "summary": "Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation."
  },
  {
    "title": "Streaming Generated Gaussian Process Experts for Online Learning and Control",
    "url": "http://arxiv.org/abs/2508.03679v1",
    "arxiv_id": "2508.03679v1",
    "authors": [
      "Zewen Yang",
      "Dongfa Zhang",
      "Xiaobing Dai",
      "Fengyi Yu",
      "Chi Zhang",
      "Bingkun Huang",
      "Hamid Sadeghian",
      "Sami Haddadin"
    ],
    "published": "2025-08-05T17:50:03+00:00",
    "summary": "Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \\underline{s}treaming \\underline{k}ernel-induced progressivel\\underline{y} generated expert framework of \\underline{G}aussian \\underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches."
  },
  {
    "title": "Theoretical framework for lattice QCD computations of $B\\to K \\ell^+ \\ell^-$ and $\\bar{B}_s\\to \\ell^+\\ell^- \u03b3$ decays rates, including contributions from \"Charming Penguins\"",
    "url": "http://arxiv.org/abs/2508.03655v1",
    "arxiv_id": "2508.03655v1",
    "authors": [
      "R. Frezzotti",
      "G. Gagliardi",
      "V. Lubicz",
      "G. Martinelli",
      "C. T. Sachrajda",
      "F. Sanfilippo",
      "L. Silvestrini",
      "S. Simula",
      "N. Tantalo"
    ],
    "published": "2025-08-05T17:05:49+00:00",
    "summary": "We develop a strategy for computing the $B\\to K\\ell^+\\ell^-$ and $\\bar{B}_s\\to\\gamma\\ell^+\\ell^-$ decay amplitudes using lattice QCD (where $\\ell^\\pm$ are charged leptons). We focus on those terms which contain complex contributions to the amplitude, due to on-shell intermediate states propagating between the weak operator and electromagnetic current(s). Such terms, which are generally estimated using model calculations and represent significant uncertainties in the phenomenological predictions for these decays, cannot be computed using standard lattice QCD techniques. It has recently been shown that such contributions can be computed using spectral-density methods and our proposed strategy, which we discuss in detail, is built on this approach. The complex contributions include the ``charming penguins\" (matrix elements of the current-current operators $O_1^{(c)}$ and $O_2^{(c)}$ defined in Eq. (6) below), in which the charm-quark loop can propagate long distances, particularly close to the region of charmonium resonances. They also include the contributions from the chromomagnetic operator ($O_8$ in standard notation, defined in Eq. (8) below). We discuss the renormalization of the ultra-violet divergences, and in particular those which arise due to ``contact\" terms, and explain how those which appear as inverse powers of the lattice spacing can be subtracted non-perturbatively. We apply the spectral density methods in an instructive exploratory computation of the charming penguin diagram in $B\\to K\\ell^+\\ell^-$ decays in which the virtual photon is emitted from the charm-quark loop (the diagram in Fig. 1(a) below) and discuss the prospects and strategies for the reliable determination of the amplitudes in future dedicated computations."
  },
  {
    "title": "RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data",
    "url": "http://arxiv.org/abs/2508.03578v1",
    "arxiv_id": "2508.03578v1",
    "authors": [
      "Jonas Leo Mueller",
      "Lukas Engel",
      "Eva Dorschky",
      "Daniel Krauss",
      "Ingrid Ullmann",
      "Martin Vossiek",
      "Bjoern M. Eskofier"
    ],
    "published": "2025-08-05T15:46:05+00:00",
    "summary": "Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications."
  },
  {
    "title": "Machine learning the single-$\u039b$ hypernuclei with neural-network quantum states",
    "url": "http://arxiv.org/abs/2508.03575v1",
    "arxiv_id": "2508.03575v1",
    "authors": [
      "Zi-Xiao Zhang",
      "Yi-Long Yang",
      "Wan-Bing He",
      "Peng-Wei Zhao",
      "Bing-Nan Lu",
      "Yu-Gang Ma"
    ],
    "published": "2025-08-05T15:42:01+00:00",
    "summary": "Single-$\\Lambda$ hypernuclei are the most straightforward extension of atomic nuclei. A thorough description of baryonic system beyond first-generation quark sector is indispensable for the maturation of nuclear $ab$ $initio$ methods. This study pioneers the application of neural-network quantum states to hypernuclei, with trainable parameters determined by variational Monte Carlo approach (VMC-NQS). In order to reduce the numerical uncertainty and treat the nucleons and hyperons in a unified manner, spinor grouping (SG) method is proposed to analytically integrate out isospin degrees of freedom. A novel spin purification scheme is developed to address the severe spin contamination occurring in standard energy minimization due to the weakly bound characteristic of light single-$\\Lambda$ hypernuclei. The energy spectrum of $s$-shell hypernuclei is computed with one-thousandth level accuracy and benchmarked against existing stochastic variational results, showing superior performance. By comparing two different sets of Hamiltonian based on pionless effective field theory (pionless EFT), we choose an optimal model and further carry out calculations of selected $p$-shell charge-symmetric hypernuclei with mass number up to 13, exhibiting satisfactory consistency with experimental results. Our findings underscore the potential of VMC-NQS family in approaching exact solution of few-body systems and the accuracy of pionless EFT in modeling hypernuclei. This is crucial for understanding hyperon-nucleon-nucleon and hyperon-hyperon-nucleon interactions, providing a powerful tool for precisely predicting the properties of multi-strangeness hypernuclei."
  },
  {
    "title": "Regulator and gauge dependence of the Abelian gauge coupling in asymptotically safe quantum gravity",
    "url": "http://arxiv.org/abs/2508.03563v1",
    "arxiv_id": "2508.03563v1",
    "authors": [
      "Maksym Riabokon",
      "Marc Schiffer",
      "Fabian Wagner"
    ],
    "published": "2025-08-05T15:31:38+00:00",
    "summary": "Both General Relativity and the Standard Model of particle physics are not UV complete. General Relativity is perturbatively non-renormalizable, while the Standard Model features Landau poles, where couplings are predicted to diverge at finite energies, e.g., in the Abelian gauge sector. Asymptotically safe quantum gravity may resolve both of these issues at the same time. In this paper, we assess the systematic uncertainties associated with this scenario, in particular with the gravitationally induced UV-completion of the Abelian gauge sector. Specifically, we study the dependence of this qualitative feature, namely the existence of a UV-complete gauge sector, on unphysical choices like the gauge, and the regulator function. Intriguingly, in some scenarios, we find simultaneous points of minimal sensitivity relative to both the regulator and gauge parameters, which allow for a UV completion. This provides further indications that the simultaneous UV-completion of quantum gravity and matter via an asymptotically safe fixed point is a robust physical feature, and that physical quantities, like scaling exponents, can become independent of unphysical choices."
  },
  {
    "title": "An Evolutionary Analysis of Narrative Selection",
    "url": "http://arxiv.org/abs/2508.03540v1",
    "arxiv_id": "2508.03540v1",
    "authors": [
      "Federico Innocenti",
      "Roberto Rozzi"
    ],
    "published": "2025-08-05T15:09:57+00:00",
    "summary": "We study the performance of different methods for processing information, incorporating narrative selection within an evolutionary model. All agents update their beliefs according to Bayes' Rule, but some strategically choose the narrative they use in updating according to heterogeneous criteria. We simulate the endogenous composition of the population, considering different laws of motion for the underlying state of the world. We find that conformists -- that is, agents that choose the narrative to conform to the average belief in the population -- have an evolutionary advantage over other agents across all specifications. The survival chances of the remaining types depend on the uncertainty regarding the state of the world. Agents who tend to develop mild beliefs perform better when the uncertainty is high, whereas agents who tend to develop extreme beliefs perform better when the uncertainty is low."
  },
  {
    "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression",
    "url": "http://arxiv.org/abs/2508.03520v1",
    "arxiv_id": "2508.03520v1",
    "authors": [
      "Md Rakibul Hasan",
      "Md Zakir Hossain",
      "Aneesh Krishna",
      "Shafin Rahman",
      "Tom Gedeon"
    ],
    "published": "2025-08-05T14:46:28+00:00",
    "summary": "Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems."
  },
  {
    "title": "MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2508.03511v1",
    "arxiv_id": "2508.03511v1",
    "authors": [
      "Yazhou Zhu",
      "Haofeng Zhang"
    ],
    "published": "2025-08-05T14:37:42+00:00",
    "summary": "Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: https://github.com/YazhouZhu19/MAUP."
  },
  {
    "title": "DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting",
    "url": "http://arxiv.org/abs/2508.02616v1",
    "arxiv_id": "2508.02616v1",
    "authors": [
      "Ali Forootani",
      "Mohammad Khosravi",
      "Masoud Barati"
    ],
    "published": "2025-08-04T17:05:55+00:00",
    "summary": "Time series forecasting plays a vital role across scientific, industrial, and environmental domains, especially when dealing with high-dimensional and nonlinear systems. While Transformer-based models have recently achieved state-of-the-art performance in long-range forecasting, they often suffer from interpretability issues and instability in the presence of noise or dynamical uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting framework that combines the representational power of Transformers with the theoretical rigor of Koopman operator theory. Our model features a modular encoder-propagator-decoder structure, where temporal dynamics are learned via a spectrally constrained, linear Koopman operator in a latent space. We impose structural guarantees-such as bounded spectral radius, Lyapunov based energy regularization, and orthogonal parameterization to ensure stability and interpretability. Comprehensive evaluations are conducted on both synthetic dynamical systems, real-world climate dataset (wind speed and surface pressure), financial time series (cryptocurrency), and electricity generation dataset using the Python package that is prepared for this purpose. Across all experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline Transformer models in terms of accuracy, robustness to noise, and long-term forecasting stability. These results establish DeepKoopFormer as a flexible, interpretable, and robust framework for forecasting in high dimensional and dynamical settings."
  },
  {
    "title": "Quark mass corrections in di-Higgs production amplitude at high-energy",
    "url": "http://arxiv.org/abs/2508.02589v1",
    "arxiv_id": "2508.02589v1",
    "authors": [
      "Sebastian Jaskiewicz"
    ],
    "published": "2025-08-04T16:44:00+00:00",
    "summary": "A large theoretical uncertainty due to the choice of the top-quark mass renormalisation scheme is present in QCD predictions for Higgs boson pair production. In these proceedings, we report on the recent progress in tackling these uncertainties for the $gg\\to HH$ amplitude in the high-energy limit $s,|t|,|u| \\gg m_t^2 \\gg m_H^2$. Using the Method of Regions and Soft-Collinear Effective Theory, the leading power in $m_t$ behaviour of the amplitude is understood to all orders in the strong coupling expansion, and leading logarithmic resummation leads to a significant reduction in the scheme choice uncertainty in the virtual amplitude for di-Higgs production at high energies."
  },
  {
    "title": "Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification",
    "url": "http://arxiv.org/abs/2508.02566v1",
    "arxiv_id": "2508.02566v1",
    "authors": [
      "Javier Fumanal-Idocin",
      "Raquel Fernandez-Peralta",
      "Javier Andreu-Perez"
    ],
    "published": "2025-08-04T16:21:43+00:00",
    "summary": "Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. Unlike classical methods that apply a uniform feature set, DFS customizes feature selection per sample, providing insight into the decision-making process for each case. DFS is especially significant in settings where decision transparency is key, i.e., clinical decisions; however, existing methods use opaque models, which hinder their applicability in real-life scenarios. This paper introduces a novel approach leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. We also show how this method provides a quantitative measure of uncertainty for each feature query and can make the feature selection process computationally lighter by constraining the feature search space. We also discuss when greedy selection of conditional mutual information is equivalent to selecting features that minimize the difference with respect to the global model predictions. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and RL methods, which are mostly considered opaque, compared to our explainable rule-based system."
  },
  {
    "title": "From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC",
    "url": "http://arxiv.org/abs/2508.02528v1",
    "arxiv_id": "2508.02528v1",
    "authors": [
      "Jingsong Liu",
      "Xiaofeng Deng",
      "Han Li",
      "Azar Kazemi",
      "Christian Grashei",
      "Gesa Wilkens",
      "Xin You",
      "Tanja Groll",
      "Nassir Navab",
      "Carolin Mogler",
      "Peter J. Sch\u00fcffler"
    ],
    "published": "2025-08-04T15:36:58+00:00",
    "summary": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis."
  },
  {
    "title": "Quantitative and Predictive Folding Models from Limited Single-Molecule Data Using Simulation-Based Inference",
    "url": "http://arxiv.org/abs/2508.02509v1",
    "arxiv_id": "2508.02509v1",
    "authors": [
      "Lars Dingeldein",
      "Aaron Lyons",
      "Pilar Cossio",
      "Michael Woodside",
      "Roberto Covino"
    ],
    "published": "2025-08-04T15:16:39+00:00",
    "summary": "The study of biomolecular folding has been greatly advanced by single-molecule force spectroscopy (SMFS), which enables the observation of the dynamics of individual molecules. However, extracting quantitative models of fundamental properties such as folding landscapes from SNFS data is very challenging due to instrumental noise, linker artifacts, and the inherent stochasticity of the process, often requiring extensive datasets and complex calibration experiments. Here, we introduce a framework based on simulation-based inference (SBI) that overcomes these limitations by integrating physics-based modeling with deep learning. We apply this framework to analyze constant-force measurements of a DNA hairpin. From a single, short experimental trajectory of only two seconds, we successfully reconstruct the hairpin's free energy landscape and folding dynamics, obtaining results that are in close agreement with established deconvolution methods that require approximately 100 times more data. Furthermore, the Bayesian nature of this approach robustly quantifies uncertainties for inferred parameter values, including the free-energy profile, diffusion coefficients, and linker stiffness, without needing independent measurements of instrumental properties. The inferred model is predictive, generating simulated trajectories that quantitatively reproduce the thermodynamic and kinetic properties of the experimental data. This work establishes SBI as a highly efficient and powerful tool for analyzing single-molecule experiments. The ability to derive statistically robust models from minimal datasets is crucial for investigating complex biomolecular systems where extensive data collection is impractical or impossible. Consequently, our SBI framework enables the rigorous quantitative analysis of previously intractable biomolecular systems, paving the way for novel applications of SMFS."
  },
  {
    "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling",
    "url": "http://arxiv.org/abs/2508.02503v1",
    "arxiv_id": "2508.02503v1",
    "authors": [
      "Maxime Bouscary",
      "Saurabh Amin"
    ],
    "published": "2025-08-04T15:11:51+00:00",
    "summary": "LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\\% to 92\\% on the most complex problems."
  },
  {
    "title": "Pre-discovery TESS Observations of Interstellar Object 3I/ATLAS",
    "url": "http://arxiv.org/abs/2508.02499v1",
    "arxiv_id": "2508.02499v1",
    "authors": [
      "Jorge Martinez-Palomera",
      "Amy Tuson",
      "Christina Hedges",
      "Jessie Dotson",
      "Thomas Barclay",
      "Brian Powell"
    ],
    "published": "2025-08-04T15:07:55+00:00",
    "summary": "3I/ATLAS, also known as C/2025 N1 (ATLAS), is the third known interstellar object to pass through our Solar System. We report serendipitous Transiting Exoplanet Survey Satellite (TESS) observations of 3I/ATLAS taken between 2025-05-07 and 2025-06-02,, 55 days prior to the discovery date (2025-07-01) and 14 days prior to the current earliest observation (2025-05-21). We retrieve the TESS pixel data, perform a robust background correction and use a data-driven approach to refine the object's ephemeris. We find a statistically significant offset between the target's observed and predicted positions and we show that this is dominated by uncertainty in the TESS World Coordinate System (WCS) rather than the ephemeris. 3I/ATLAS is too faint to be detected in the individual 200\\,second TESS integrations, so we perform image stacking to improve detectability. After co-adding the TESS image data, we performed aperture and Pixel Response Function (PRF) photometry to create two light curves for 3I/ATLAS. Each light curve consists of 15 measurements with $\\text{SNR}>3$, collected across two different TESS cameras during the 26\\,days that the object was observed, but the PRF light curve is more robust against image noise. The PRF light curve in the TESS bandpass shows a gradual increase in brightness from $T_{\\text{mag}} = 20.9 \\pm 0.29$ to $T_{\\text{mag}} = 19.57 \\pm 0.15$. This is expected as 3I/ATLAS approaches the inner Solar System. This paper highlights the power of using TESS for Solar System science; by increasing the photometric observing baseline, future studies will be able to investigate the long-term behavior of 3I/ATLAS"
  },
  {
    "title": "Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning",
    "url": "http://arxiv.org/abs/2508.02495v1",
    "arxiv_id": "2508.02495v1",
    "authors": [
      "Kunyu Zhang",
      "Lin Gu",
      "Liangchen Liu",
      "Yingke Chen",
      "Bingyang Wang",
      "Jin Yan",
      "Yingying Zhu"
    ],
    "published": "2025-08-04T15:05:27+00:00",
    "summary": "Many previous studies have proposed extracting image labels from clinical notes to create large-scale medical image datasets at a low cost. However, these approaches inherently suffer from label noise due to uncertainty from the clinical experts. When radiologists and physicians analyze medical images to make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or ``not excluded''. Unfortunately, current text-mining methods overlook these nuances, resulting in the creation of noisy labels. Existing methods for handling noisy labels in medical image analysis, which typically address the problem through post-processing techniques, have largely ignored the important issue of expert-driven uncertainty contributing to label noise. To better incorporate the expert-written uncertainty in clinical notes into medical image analysis and address the label noise issue, we first examine the impact of clinical expert uncertainty on label noise. We then propose a clinical expert uncertainty-aware benchmark, along with a label smoothing method, which significantly improves performance compared to current state-of-the-art approaches."
  },
  {
    "title": "Uncertainty-Aware Perception-Based Control for Autonomous Racing",
    "url": "http://arxiv.org/abs/2508.02494v1",
    "arxiv_id": "2508.02494v1",
    "authors": [
      "Jelena Trisovic",
      "Andrea Carron",
      "Melanie N. Zeilinger"
    ],
    "published": "2025-08-04T15:04:39+00:00",
    "summary": "Autonomous systems operating in unknown environments often rely heavily on visual sensor data, yet making safe and informed control decisions based on these measurements remains a significant challenge. To facilitate the integration of perception and control in autonomous vehicles, we propose a novel perception-based control approach that incorporates road estimation, quantification of its uncertainty, and uncertainty-aware control based on this estimate. At the core of our method is a parametric road curvature model, optimized using visual measurements of the road through a constrained nonlinear optimization problem. This process ensures adherence to constraints on both model parameters and curvature. By leveraging the Frenet frame formulation, we embed the estimated track curvature into the system dynamics, allowing the controller to explicitly account for perception uncertainty and enhancing robustness to estimation errors based on visual input. We validate our approach in a simulated environment, using a high-fidelity 3D rendering engine, and demonstrate its effectiveness in achieving reliable and uncertainty-aware control for autonomous racing."
  },
  {
    "title": "Gauge theory approach to describe ice crystals habit evolution in ice clouds",
    "url": "http://arxiv.org/abs/2508.02472v1",
    "arxiv_id": "2508.02472v1",
    "authors": [
      "Gianluca Di Natale",
      "Francesco Pio De Cosmo",
      "Leandro Cieri"
    ],
    "published": "2025-08-04T14:36:37+00:00",
    "summary": "Ice clouds, particularly cirrus clouds, significantly influence Earth's radiative balance but remain poorly characterized in current climate models. A major uncertainty arises from the variability of their microphysical properties, especially the evolution of ice crystal habits under depositional growth. We propose a heuristic method to describe habit evolution based on four fundamental shapes identified in the literature and from in situ observations: droxtals, plates, columns, and rosettes. These represent the primary forms that are relevant under depositional growth, excluding aggregation. In this study, we employ a non-Abelian gauge theory within a field-theoretical framework, imposing an SU(2) $\\otimes$ U(1) symmetry on the fields associated with each habit probability growth. This symmetry enables the derivation of a modified system of coupled Fokker-Planck equations, capturing the stochastic growth dynamics of ice crystals while incorporating phenomenological mutual influences among habits. This framework outlines a novel theoretical direction for integrating symmetry principles and field-theoretical tools into the modelling of habit dynamics in ice clouds."
  },
  {
    "title": "Numerical Uncertainty in Linear Registration: An Experimental Study",
    "url": "http://arxiv.org/abs/2508.00781v1",
    "arxiv_id": "2508.00781v1",
    "authors": [
      "Niusha Mirhakimi",
      "Yohan Chatelain",
      "Jean-Baptiste Poline",
      "Tristan Glatard"
    ],
    "published": "2025-08-01T17:02:29+00:00",
    "summary": "While linear registration is a critical step in MRI preprocessing pipelines, its numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA) simulations, we assessed the most commonly used linear registration tools within major software packages (SPM, FSL, and ANTs) across multiple image similarity measures, two brain templates, and both healthy control (HC, n=50) and Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the influence of linear registration tools and similarity measures on numerical stability. Among the evaluated tools and with default similarity measures, SPM exhibited the highest stability. FSL and ANTs showed greater and similar ranges of variability, with ANTs demonstrating particular sensitivity to numerical perturbations that occasionally led to registration failure. Furthermore, no significant differences were observed between healthy and PD cohorts, suggesting that numerical stability analyses obtained with healthy subjects may generalise to clinical populations. Finally, we also demonstrated how numerical uncertainty measures may support automated quality control (QC) of linear registration results. Overall, our experimental results characterize the numerical stability of linear registration experimentally and can serve as a basis for future uncertainty analyses."
  },
  {
    "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection",
    "url": "http://arxiv.org/abs/2508.00754v1",
    "arxiv_id": "2508.00754v1",
    "authors": [
      "Yaxin Ma",
      "Benjamin Colburn",
      "Jose C. Principe"
    ],
    "published": "2025-08-01T16:31:23+00:00",
    "summary": "Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models."
  },
  {
    "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation",
    "url": "http://arxiv.org/abs/2508.00750v1",
    "arxiv_id": "2508.00750v1",
    "authors": [
      "Prerana Ramkumar"
    ],
    "published": "2025-08-01T16:25:21+00:00",
    "summary": "Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications."
  },
  {
    "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems",
    "url": "http://arxiv.org/abs/2508.00734v1",
    "arxiv_id": "2508.00734v1",
    "authors": [
      "Liuyun Xu",
      "Seymour M. J. Spence"
    ],
    "published": "2025-08-01T16:04:21+00:00",
    "summary": "Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches."
  },
  {
    "title": "Efficient Solution and Learning of Robust Factored MDPs",
    "url": "http://arxiv.org/abs/2508.00707v1",
    "arxiv_id": "2508.00707v1",
    "authors": [
      "Yannik Schnitzer",
      "Alessandro Abate",
      "David Parker"
    ],
    "published": "2025-08-01T15:23:15+00:00",
    "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods."
  },
  {
    "title": "Chemical abundances of seven stars in the GD-1 stream",
    "url": "http://arxiv.org/abs/2508.00671v1",
    "arxiv_id": "2508.00671v1",
    "authors": [
      "Jing-Kun Zhao",
      "Guang-Wei Li",
      "Wako Aoki",
      "Gang Zhao",
      "Guo-Chao Yang",
      "Jian-Rong Shi",
      "Hai-Ning Li",
      "Tadafumi Matsuno",
      "Miho Ishigaki",
      "Takuma Suda",
      "Satoshi Honda",
      "Yu-Qin Chen",
      "Qian-Fan Xing",
      "Hong-Liang Yan",
      "Yong Yang",
      "Xian-Hao Ye"
    ],
    "published": "2025-08-01T14:46:09+00:00",
    "summary": "We present the first detailed chemical abundances for seven GD-1 stream stars from Subaru/HDS spectroscopy. Atmospheric parameters were derived via color calibrations ($T\\rm_{eff}$) and iterative spectroscopic analysis. LTE abundances for 14 elements ($\\alpha$, odd-Z, iron-peak, n-capture) were measured. Six stars trace the main orbit, one resides in a `blob'. All exhibit tightly clustered metallicities ([Fe/H] = -2.38, {\\bf intrinsic dispersion smaller than 0.05 dex, average uncertainty is about 0.13 dex}). While one star shows binary mass transfer signatures, the other six display consistent abundance patterns (dispersions $<$ uncertainties). Their iron-peak elements (Sc, Cr, Mn, Ni) match Milky Way halo stars. In contrast, Y and Sr are systematically lower than halo stars of similar [Fe/H]. Significantly, six stars show consistently enhanced [Eu/Fe] $\\sim$ 0.60 ($\\sigma$ = 0.08). A tight Ba-Eu correlation (r = 0.83, p=0.04) exists, with [Ba/Fe] = -0.03 $\\pm$ 0.05, indicating a common r-process origin. This extreme chemical homogeneity strongly supports an origin from a single disrupted globular cluster. The lack of light-element anti-correlations may stem from our sample size or the progenitor's low mass."
  },
  {
    "title": "Uncertainty Relation for Pseudo-Hermitian Quantum Systems",
    "url": "http://arxiv.org/abs/2508.00648v1",
    "arxiv_id": "2508.00648v1",
    "authors": [
      "Boubakeur Khantoul",
      "Bilel Hamil",
      "Amar Benchikha"
    ],
    "published": "2025-08-01T14:06:05+00:00",
    "summary": "This study investigates pseudo-Hermitian quantum mechanics, where the Hamiltonian satisfies a modified Hermiticity condition. We extend the uncertainty relation for such systems, demonstrating its equivalence to the standard Hermitian case within a pseudo-Hermitian inner product. Analytical solutions to the time-dependent Schr\\\"odinger equation with a linearly evolving potential are derived. Furthermore, we show that the uncertainty relation for position and momentum remains real and greater than 1/2, highlighting the significance of non-Hermitian systems in quantum mechanics."
  },
  {
    "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators",
    "url": "http://arxiv.org/abs/2508.00643v1",
    "arxiv_id": "2508.00643v1",
    "authors": [
      "Albert Matveev",
      "Sanmitra Ghosh",
      "Aamal Hussain",
      "James-Michael Leahy",
      "Michalis Michaelides"
    ],
    "published": "2025-08-01T13:57:19+00:00",
    "summary": "Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification."
  },
  {
    "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models",
    "url": "http://arxiv.org/abs/2508.00600v1",
    "arxiv_id": "2508.00600v1",
    "authors": [
      "Mingruo Yuan",
      "Shuyi Zhang",
      "Ben Kao"
    ],
    "published": "2025-08-01T12:58:34+00:00",
    "summary": "Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines."
  },
  {
    "title": "Output-recurrent gated state space model for multiphase flows modeling and uncertainty quantification of exhaust vehicles",
    "url": "http://arxiv.org/abs/2508.00588v1",
    "arxiv_id": "2508.00588v1",
    "authors": [
      "Ruilin Chen",
      "Ming Fang",
      "Guihui Ma"
    ],
    "published": "2025-08-01T12:41:40+00:00",
    "summary": "This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for complex multiphase flows modeling and uncertainty quantification of exhaust vehicles during motion. By establishing the state-space formulation of the gas-liquid Navier-Stokes equations applying semigroup theory and Galerkin projection, explicitly characterizing the dynamic coupling evolution between the velocity, pressure, and volume fraction fields. A novel Gated State Space Transition (GSST) unit is designed to learn parameterized transition and input matrices with adaptive timescales, enhancing physical interpretability and computational efficiency. The output recursion mechanism aligns with the numerical solution characteristics of state-space equations, mitigating long-term error accumulation and addressing training-inference pattern mismatch issues inherent in teacher forcing and scheduled sampling. Validations on the underwater cone-head and water-exit hemisphere-head vehicles demonstrate that: OR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and computational efficiency through its physics-informed adaptive state-space unit design and parallel matrix operations; The output recursion mechanism ensures more stable training, better generalization, and higher prediction accuracy than teacher forcing and scheduled sampling; OR-GSSM accurately captures the gas-phase expansion, gas-liquid mixing formation, backflow jet generation, bubble shedding, and entire water-exit process, etc, showcasing outstanding modeling capability; Its uncertainty quantification effectively characterizes flow features and uncertainty distributions, validating prediction reliability. The proposed method resolves the accuracy-real-time trade-off in traditional computational fluid dynamics, advancing machine learning for multiphase flow modeling and uncertainty quantification in exhaust vehicles."
  },
  {
    "title": "Formal Bayesian Transfer Learning via the Total Risk Prior",
    "url": "http://arxiv.org/abs/2507.23768v1",
    "arxiv_id": "2507.23768v1",
    "authors": [
      "Nathan Wycoff",
      "Ali Arab",
      "Lisa O. Singh"
    ],
    "published": "2025-07-31T17:55:16+00:00",
    "summary": "In analyses with severe data-limitations, augmenting the target dataset with information from ancillary datasets in the application domain, called source datasets, can lead to significantly improved statistical procedures. However, existing methods for this transfer learning struggle to deal with situations where the source datasets are also limited and not guaranteed to be well-aligned with the target dataset. A typical strategy is to use the empirical loss minimizer on the source data as a prior mean for the target parameters, which places the estimation of source parameters outside of the Bayesian formalism. Our key conceptual contribution is to use a risk minimizer conditional on source parameters instead. This allows us to construct a single joint prior distribution for all parameters from the source datasets as well as the target dataset. As a consequence, we benefit from full Bayesian uncertainty quantification and can perform model averaging via Gibbs sampling over indicator variables governing the inclusion of each source dataset. We show how a particular instantiation of our prior leads to a Bayesian Lasso in a transformed coordinate system and discuss computational techniques to scale our approach to moderately sized datasets. We also demonstrate that recently proposed minimax-frequentist transfer learning techniques may be viewed as an approximate Maximum a Posteriori approach to our model. Finally, we demonstrate superior predictive performance relative to the frequentist baseline on a genetics application, especially when the source data are limited."
  },
  {
    "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System",
    "url": "http://arxiv.org/abs/2507.23756v1",
    "arxiv_id": "2507.23756v1",
    "authors": [
      "Diana Mortagua"
    ],
    "published": "2025-07-31T17:41:30+00:00",
    "summary": "This study centers on overcoming the challenge of selecting the best annotators for each query in Active Learning (AL), with the objective of minimizing misclassifications. AL recognizes the challenges related to cost and time when acquiring labeled data, and decreases the number of labeled data needed. Nevertheless, there is still the necessity to reduce annotation errors, aiming to be as efficient as possible, to achieve the expected accuracy faster. Most strategies for query-annotator pairs do not consider internal factors that affect productivity, such as mood, attention, motivation, and fatigue levels. This work addresses this gap in the existing literature, by not only considering how the internal factors influence annotators (mood and fatigue levels) but also presenting a new query-annotator pair strategy, using a Knowledge-Based Recommendation System (RS). The RS ranks the available annotators, allowing to choose one or more to label the queried instance using their past accuracy values, and their mood and fatigue levels, as well as information about the instance queried. This work bases itself on existing literature on mood and fatigue influence on human performance, simulating annotators in a realistic manner, and predicting their performance with the RS. The results show that considering past accuracy values, as well as mood and fatigue levels reduces the number of annotation errors made by the annotators, and the uncertainty of the model through its training, when compared to not using internal factors. Accuracy and F1-score values were also better in the proposed approach, despite not being as substantial as the aforementioned. The methodologies and findings presented in this study begin to explore the open challenge of human cognitive factors affecting AL."
  },
  {
    "title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction",
    "url": "http://arxiv.org/abs/2507.23736v1",
    "arxiv_id": "2507.23736v1",
    "authors": [
      "Kyle Naddeo",
      "Nikolas Koutsoubis",
      "Rahul Krish",
      "Ghulam Rasool",
      "Nidhal Bouaynaya",
      "Tony OSullivan",
      "Raj Krish"
    ],
    "published": "2025-07-31T17:19:38+00:00",
    "summary": "Access to medical imaging and associated text data has the potential to drive major advances in healthcare research and patient outcomes. However, the presence of Protected Health Information (PHI) and Personally Identifiable Information (PII) in Digital Imaging and Communications in Medicine (DICOM) files presents a significant barrier to the ethical and secure sharing of imaging datasets. This paper presents a hybrid de-identification framework developed by Impact Business Information Solutions (IBIS) that combines rule-based and AI-driven techniques, and rigorous uncertainty quantification for comprehensive PHI/PII removal from both metadata and pixel data.   Our approach begins with a two-tiered rule-based system targeting explicit and inferred metadata elements, further augmented by a large language model (LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of synthetic datasets simulating realistic clinical PHI/PII. For pixel data, we employ an uncertainty-aware Faster R-CNN model to localize embedded text, extract candidate PHI via Optical Character Recognition (OCR), and apply the NER pipeline for final redaction. Crucially, uncertainty quantification provides confidence measures for AI-based detections to enhance automation reliability and enable informed human-in-the-loop verification to manage residual risks.   This uncertainty-aware deidentification framework achieves robust performance across benchmark datasets and regulatory standards, including DICOM, HIPAA, and TCIA compliance metrics. By combining scalable automation, uncertainty quantification, and rigorous quality assurance, our solution addresses critical challenges in medical data de-identification and supports the secure, ethical, and trustworthy release of imaging data for research."
  },
  {
    "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy",
    "url": "http://arxiv.org/abs/2507.23735v1",
    "arxiv_id": "2507.23735v1",
    "authors": [
      "Markus Buchholz",
      "Ignacio Carlucho",
      "Michele Grimaldi",
      "Yvan R. Petillot"
    ],
    "published": "2025-07-31T17:18:55+00:00",
    "summary": "Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications."
  },
  {
    "title": "High-resolution eikonal imaging and uncertainty quantification of the Kilauea caldera",
    "url": "http://arxiv.org/abs/2507.23692v1",
    "arxiv_id": "2507.23692v1",
    "authors": [
      "Angela F. Gao",
      "John D. Wilding",
      "Ettore Biondi",
      "Katherine L. Bouman",
      "Zachary E. Ross"
    ],
    "published": "2025-07-31T16:09:33+00:00",
    "summary": "Images of the Earth's interior can provide us with insight into the underlying properties of the Earth, such as how seismic activity might emerge and the interplay between seismic and volcanic activity. Understanding these systems requires reliable high-resolution images to understand mechanisms and estimate physical quantities. However, reliable images are often difficult to obtain due to the non-linear nature of seismic wave propagation and the ill-posedness of the related inverse problem. Reconstructions rely on good initial estimates as well as hand-crafted priors, which can ultimately bias solutions. In our work, we present a 3D reconstruction of Kilauea's magmatic system at a previously unattained resolution. Our eikonal tomography procedure improves upon prior imaging results of Kilauea through increased resolution and per-pixel uncertainties estimated through variational inference. In particular, solving eikonal imaging using variational inference with stochastic gradient descent enables stable inversion and uncertainty quantification in the absence of strong prior knowledge of the velocity structure. Our work makes two key contributions: developing a stochastic eikonal tomography scheme with uncertainty quantification and illuminating the structure and melt quantity of the magmatic system that underlies Kilauea."
  },
  {
    "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates",
    "url": "http://arxiv.org/abs/2507.23607v1",
    "arxiv_id": "2507.23607v1",
    "authors": [
      "Tien Huu Do",
      "Antoine Masquelier",
      "Nae Eoun Lee",
      "Jonathan Crowther"
    ],
    "published": "2025-07-31T14:47:16+00:00",
    "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models."
  },
  {
    "title": "Branching ratios and CP asymmetries of $B^0 \\to \u03b7_c f_0$ in the improved perturbative QCD formalism",
    "url": "http://arxiv.org/abs/2507.23578v1",
    "arxiv_id": "2507.23578v1",
    "authors": [
      "Min-Qi Li",
      "Xin Liu",
      "Zhi-Tian Zou",
      "Ying Li",
      "Zhen-Jun Xiao"
    ],
    "published": "2025-07-31T14:08:25+00:00",
    "summary": "Motivated by the idea of fragmented scalar glueball, we investigate the decays $B^0 \\to \\eta_c f_0$ within the improved perturbative QCD (iPQCD) framework by including the known next-to-leading order corrections. Here, $B^0$ and $f_0$ denote the neutral $B_{d,s}^0$ mesons and the light scalar mesons $f_0(500, 980, 1370, 1500)$ under the $q\\bar q$ assignment. The {\\it CP}-averaged branching ratios (BRs) and the {\\it CP} asymmetries of $B^0 \\to \\eta_c f_0$ are evaluated with the $f_0(500)[f_0(1370)]-f_0(980)[f_0(1500)]$ mixing in quark-flavor basis. For effective comparisons with the near-future measurements, we further derive the $B^0 \\to \\eta_c f_0 (\\to \\pi^+ \\pi^-/K^+ K^-)$ BRs under the narrow-width approximation. ${\\rm BR}(B_s^0 \\to \\eta_c f_0(980) (\\to \\pi^+ \\pi^-))= (2.87^{+1.38}_{-1.29}) \\times 10^{-4}$ obtained in the iPQCD formalism agrees with the available measurements and predictions within uncertainties. Large BRs of $B_s^0 \\to \\eta_c f_0(1500) (\\to \\pi^+ \\pi^-/K^+ K^-)$ and large direct {\\it CP} asymmetries of $B^0 \\to \\eta_c f_0(1370, 1500)$ are accessible in the LHCb and Belle-II experiments. The experimental tests of these iPQCD predictions would help us to understand the nature of these light scalars more deeply and provide evidences to decipher $f_0(1500)$ as a primary or fragmented scalar glueball potentially."
  },
  {
    "title": "Subthreshold parameters of $\u03c0\u03c0$ scattering revisited",
    "url": "http://arxiv.org/abs/2507.23566v1",
    "arxiv_id": "2507.23566v1",
    "authors": [
      "Mari\u00e1n Koles\u00e1r",
      "Jaroslav \u0158\u00edha"
    ],
    "published": "2025-07-31T13:54:02+00:00",
    "summary": "Using the most recent experimental data and lattice QCD calculations of $\\pi\\pi$ scattering lengths, while employing dispersive representations of the amplitude based on Roy equations, we compute the subthreshold parameters of this process. We use Monte Carlo sampling to numerically model the probability distribution of the results based on all uncertainties in the inputs. We also investigate the dependence of the results on a theoretical correlation between the $\\pi\\pi$ scattering lengths $a^0_0$ and $a^2_0$, which was previously established in the framework of two-flavour chiral perturbation theory."
  },
  {
    "title": "Latest neutrino results from the FASER experiment and their implications for forward hadron production",
    "url": "http://arxiv.org/abs/2507.23552v1",
    "arxiv_id": "2507.23552v1",
    "authors": [
      "FASER Collaboration",
      "Roshan Mammen Abraham",
      "Xiaocong Ai",
      "Saul Alonso Monsalve",
      "John Anders",
      "Claire Antel",
      "Akitaka Ariga",
      "Tomoko Ariga",
      "Jeremy Atkinson",
      "Florian U. Bernlochner",
      "Tobias Boeckh",
      "Jamie Boyd",
      "Lydia Brenner",
      "Angela Burger",
      "Franck Cadoux",
      "Roberto Cardella",
      "David W. Casper",
      "Charlotte Cavanagh",
      "Xin Chen",
      "Dhruv Chouhan",
      "Andrea Coccaro",
      "Stephane D\u00e9bieux",
      "Ansh Desai",
      "Sergey Dmitrievsky",
      "Radu Dobre",
      "Monica D'Onofrio",
      "Sinead Eley",
      "Yannick Favre",
      "Jonathan L. Feng",
      "Carlo Alberto Fenoglio",
      "Didier Ferrere",
      "Max Fieg",
      "Wissal Filali",
      "Elena Firu",
      "Haruhi Fujimori",
      "Edward Galantay",
      "Ali Garabaglu",
      "Stephen Gibson",
      "Sergio Gonzalez-Sevilla",
      "Yuri Gornushkin",
      "Yotam Granov",
      "Carl Gwilliam",
      "Daiki Hayakawa",
      "Michael Holzbock",
      "Shih-Chieh Hsu",
      "Zhen Hu",
      "Giuseppe Iacobucci",
      "Tomohiro Inada",
      "Luca Iodice",
      "Sune Jakobsen",
      "Hans Joos",
      "Enrique Kajomovitz",
      "Hiroaki Kawahara",
      "Alex Keyken",
      "Felix Kling",
      "Daniela K\u00f6ck",
      "Pantelis Kontaxakis",
      "Umut Kose",
      "Rafaella Kotitsa",
      "Peter Krack",
      "Susanne Kuehn",
      "Thanushan Kugathasan",
      "Sebastian Laudage",
      "Lorne Levinson",
      "Botao Li",
      "Jinfeng Liu",
      "Yi Liu",
      "Margaret S. Lutz",
      "Jack MacDonald",
      "Chiara Magliocca",
      "Toni M\u00e4kel\u00e4",
      "Lawson McCoy",
      "Josh McFayden",
      "Andrea Pizarro Medina",
      "Matteo Milanesio",
      "Th\u00e9o Moretti",
      "Keiko Moriyama",
      "Mitsuhiro Nakamura",
      "Toshiyuki Nakano",
      "Laurie Nevay",
      "Ken Ohashi",
      "Hidetoshi Otono",
      "Lorenzo Paolozzi",
      "Pawan Pawan",
      "Brian Petersen",
      "Titi Preda",
      "Markus Prim",
      "Michaela Queitsch-Maitland",
      "Juan Rojo",
      "Hiroki Rokujo",
      "Andr\u00e9 Rubbia",
      "Jorge Sabater-Iglesias",
      "Osamu Sato",
      "Paola Scampoli",
      "Kristof Schmieden",
      "Matthias Schott",
      "Christiano Sebastiani",
      "Anna Sfyrla",
      "Davide Sgalaberna",
      "Mansoora Shamim",
      "Savannah Shively",
      "Yosuke Takubo",
      "Noshin Tarannum",
      "Ondrej Theiner",
      "Simon Thor",
      "Eric Torrence",
      "Oscar Ivan Valdes Martinez",
      "Svetlana Vasina",
      "Benedikt Vormwald",
      "Yuxiao Wang",
      "Eli Welch",
      "Monika Wielers",
      "Benjamin James Wilson",
      "Jialin Wu",
      "Johannes Martin Wuthrich",
      "Yue Xu",
      "Daichi Yoshikawa",
      "Stefano Zambito",
      "Shunliang Zhang",
      "Xingyu Zhao"
    ],
    "published": "2025-07-31T13:41:26+00:00",
    "summary": "The muon puzzle -- an excess of muons relative to simulation predictions in ultra-high-energy cosmic-ray air showers -- has been reported by many experiments. This suggests that forward particle production in hadronic interactions is not fully understood. Some of the scenarios proposed to resolve this predict reduced production of forward neutral pions and enhanced production of forward kaons (or other particles). The FASER experiment at the LHC is located 480 m downstream of the ATLAS interaction point and is sensitive to neutrinos and muons, which are the decay products of forward charged pions and kaons. In this study, the latest measurements of electron and muon neutrino fluxes are presented using the data corresponding to 9.5 $\\mathrm{fb^{-1}}$ and 65.6 $\\mathrm{fb^{-1}}$ of proton-proton collisions with $\\sqrt{s}=13.6~\\mathrm{TeV}$ by the FASER$\\nu$ and the FASER electronic detector, respectively. These fluxes are compared with predictions from recent hadronic interaction models, including EPOS-LHCr, SIBYLL 2.3e, and QGSJET 3. The predictions are generally consistent with the measured fluxes from FASER, although some discrepancies appear in certain energy bins. More precise flux measurements with additional data will follow soon, enabling validation of pion, kaon, and charm meson production with finer energy binning, reduced uncertainties, and multi-differential analyses."
  },
  {
    "title": "A decomposition of Fisher's information to inform sample size for developing or updating fair and precise clinical prediction models -- Part 3: continuous outcomes",
    "url": "http://arxiv.org/abs/2507.23548v1",
    "arxiv_id": "2507.23548v1",
    "authors": [
      "Rebecca Whittle",
      "Richard D Riley",
      "Lucinda Archer",
      "Gary S Collins",
      "Amardeep Legha",
      "Kym IE Snell",
      "Joie Ensor"
    ],
    "published": "2025-07-31T13:38:45+00:00",
    "summary": "Clinical prediction models enable healthcare professionals to estimate individual outcomes using patient characteristics. Current sample size guidelines for developing or updating models with continuous outcomes aim to minimise overfitting and ensure accurate estimation of population-level parameters, but do not explicitly address the precision of predictions. This is a critical limitation, as wide confidence intervals around predictions can undermine clinical utility and fairness, particularly if precision varies across subgroups. We propose methodology for calculating the sample size required to ensure precise and fair predictions in models with continuous outcomes. Building on linear regression theory and the Fisher's unit information matrix, our approach calculates how sample size impacts the epistemic (model-based) uncertainty of predictions and allows researchers to either (i) evaluate whether an existing dataset is sufficiently large, or (ii) determine the sample size needed to target a particular confidence interval width around predictions. The method requires real or synthetic data representing the target population. To assess fairness,the approach can evaluate prediction precision across subgroups. Extensions to prediction intervals are included to additionally address aleatoric uncertainty. Our methodology provides a practical framework for examining required sample sizes when developing or updating prediction models with continuous outcomes, focusing on achieving precise and equitable predictions. It supports the development of more reliable and fair models, enhancing their clinical applicability and trustworthiness."
  },
  {
    "title": "Solitons, chaos, and quantum phenomena: a deterministic approach to the Schr\u00f6dinger equation",
    "url": "http://arxiv.org/abs/2507.22868v1",
    "arxiv_id": "2507.22868v1",
    "authors": [
      "Dami\u00e0 Gomila"
    ],
    "published": "2025-07-30T17:43:06+00:00",
    "summary": "We show that the Schr\\\"odinger equation describes the ensemble mean dynamics of solitons in a Galilean invariant field theory where we interpret solitons as particles. On a zero background, solitons move classically, following Newton`s second law, however, on a non-zero amplitude chaotic background, their momentum and position fluctuate fulfilling an exact uncertainty relation, which give rise to the emergence of quantum phenomena. The Schrodinger equation for the ensemble of solitons is obtained from this exact uncertainty relation, and the amplitude of the background fluctuations is what corresponds to the value of $\\hbar$. We confirm our analytical results running simulations of solitons moving against a potential barrier and comparing the ensemble probabilities with the predictions of the time dependent Schr\\\"odinger equation, providing a deterministic version of the quantum tunneling effect. We conclude with a discussion of how our theory does not present statistical independence between measurement and experiment outcome."
  },
  {
    "title": "A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model",
    "url": "http://arxiv.org/abs/2507.22854v1",
    "arxiv_id": "2507.22854v1",
    "authors": [
      "Andris Ambainis",
      "Joao F. Doriguello",
      "Debbie Lim"
    ],
    "published": "2025-07-30T17:24:23+00:00",
    "summary": "We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a \"simulator\". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like \"optimism in the face of uncertainty\" and \"posterior sampling\" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\\operatorname{poly}\\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces."
  },
  {
    "title": "Robust Contract with Career Concerns",
    "url": "http://arxiv.org/abs/2507.22852v1",
    "arxiv_id": "2507.22852v1",
    "authors": [
      "Tan Gan",
      "Hongcheng Li"
    ],
    "published": "2025-07-30T17:22:58+00:00",
    "summary": "An employer contracts with a worker to incentivize efforts whose productivity depends on ability; the worker then enters a market that pays him contingent on ability evaluation. With non-additive monitoring technology, the interdependence between market expectations and worker efforts can lead to multiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy (1992)). We identify a sufficient and necessary criterion for the employer to face such strategic uncertainty--one linked to skill-effort complementarity, a pervasive feature of labor markets. To fully implement work, the employer optimally creates private wage discrimination to iteratively eliminate pessimistic market expectations and low worker efforts. Our result suggests that present contractual privacy, employers' coordination motives generate within-group pay inequality. The comparative statics further explain several stylized facts about residual wage dispersion."
  },
  {
    "title": "An Uncertainty Principle for Probabilistic Computation in the Retina",
    "url": "http://arxiv.org/abs/2507.22785v1",
    "arxiv_id": "2507.22785v1",
    "authors": [
      "Jayanth R Taranath",
      "Salim M'Jahad"
    ],
    "published": "2025-07-30T15:51:13+00:00",
    "summary": "We introduce a probabilistic model of early visual processing, beginning with the interaction between a light wavefront and the retina. We argue that perception originates not with deterministic transduction, but with probabilistic threshold crossings shaped by quantum photon arrival statistics and biological variability. We formalize this with an uncertainty relation, \\( \\Delta \\alpha \\cdot \\Delta t \\geq \\eta \\), through the transformation of light into symbolic neural code through the layered retinal architecture. Our model is supported by previous experimental results, which show intrinsic variability in retinal responses even under fixed stimuli. We contrast this with a classical null hypothesis of deterministic encoding and propose experiments to further test our uncertainty relation. By re-framing the retina as a probabilistic measurement device, we lay the foundation for future models of cortical dynamics rooted in quantum-like computation. We are not claiming that the brain could be working as a quantum-system, but rather putting forth the argument that the brain as a classical system could still implement quantum-inspired computations. We define quantum-inspired computation as a scheme that includes both probabilistic and time-sensitive computation, clearly separating it from classically implementable probabilistic systems."
  },
  {
    "title": "A comparison of variable selection methods and predictive models for postoperative bowel surgery complications",
    "url": "http://arxiv.org/abs/2507.22771v1",
    "arxiv_id": "2507.22771v1",
    "authors": [
      "\u00d6zge \u015eahin",
      "Annemiek Kwast",
      "Annemieke Witteveen",
      "Tina Nane"
    ],
    "published": "2025-07-30T15:35:11+00:00",
    "summary": "Accurate prediction of postoperative complications can support personalized perioperative care. However, in surgical settings, data collection is often constrained, and identifying which variables to prioritize remains an open question. We analyzed 767 elective bowel surgeries performed under an Enhanced Recovery After Surgery protocol at Medisch Spectrum Twente (Netherlands) between March 2020 and December 2023. Although hundreds of variables were available, most had substantial missingness or near-constant values and were therefore excluded. After data preprocessing, 34 perioperative predictors were selected for further analysis. Surgeries from 2020 to 2022 ($n=580$) formed the development set, and 2023 cases ($n=187$) provided temporal validation. We modeled two binary endpoints: any and serious postoperative complications (Clavien Dindo $\\ge$ IIIa). We compared weighted logistic regression, stratified random forests, and Naive Bayes under class imbalance (serious complication rate $\\approx$11\\%; any complication rate $\\approx$35\\%). Probabilistic performance was assessed using class-specific Brier scores. We advocate reporting probabilistic risk estimates to guide monitoring based on uncertainty. Random forests yielded better calibration across outcomes. Variable selection modestly improved weighted logistic regression and Naive Bayes but had minimal effect on random forests. Despite single-center data, our findings underscore the value of careful preprocessing and ensemble methods in perioperative risk modeling."
  },
  {
    "title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models",
    "url": "http://arxiv.org/abs/2507.22766v1",
    "arxiv_id": "2507.22766v1",
    "authors": [
      "Felix Kronenwett",
      "Georg Maier",
      "Thomas Laengle"
    ],
    "published": "2025-07-30T15:31:39+00:00",
    "summary": "Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters."
  },
  {
    "title": "Pionic gluons from global QCD analysis of experimental and lattice data",
    "url": "http://arxiv.org/abs/2507.22730v1",
    "arxiv_id": "2507.22730v1",
    "authors": [
      "William Good",
      "Patrick C. Barry",
      "Huey-Wen Lin",
      "W. Melnitchouk",
      "Alex NieMiera",
      "Nobuo Sato"
    ],
    "published": "2025-07-30T14:50:23+00:00",
    "summary": "We perform the first global QCD analysis of parton distribution functions (PDFs) in the pion, with lattice-QCD data on gluonic pseudo--Ioffe-time distributions fitted simultaneously with experimental Drell-Yan and leading neutron electroproduction data. Inclusion of the lattice results with parametrized systematic corrections significantly reduces the uncertainties on the gluon PDF at parton momentum fractions $x \\gtrsim 0.2$, revealing a higher gluon density in the pion at large $x$ than in the proton. The similar gluon momentum fractions in the pion and proton further suggests a relative suppression of the pion gluon density at small $x$."
  },
  {
    "title": "Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection",
    "url": "http://arxiv.org/abs/2507.22693v1",
    "arxiv_id": "2507.22693v1",
    "authors": [
      "Naoki Aizawa",
      "Keita Emura",
      "Kiminao Kogiso"
    ],
    "published": "2025-07-30T13:59:03+00:00",
    "summary": "This study proposes an encrypted PID control system with a disturbance observer (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to achieve control performance while providing resistance to malleability-based attacks. The controller integrates a DOB with a PID structure to compensate for modeling uncertainties by estimating and canceling external disturbances. To enhance security, the system is designed to output error symbols when ciphertexts are falsified during decryption or evaluation, enabling real-time detection of malleability-based signal or parameter falsification. To validate the proposed method, we conduct stage positioning control experiments and attack detection tests using an industrial linear stage. The results show that the encrypted DOB-based PID controller outperforms a conventional encrypted PID controller in terms of tracking accuracy. Furthermore, the system successfully detects two types of malleability-based attacks: one that destabilizes the control system, and another that degrades its performance. The primary contributions of this study are: (i) the implementation of a KHE-based encrypted DOB-PID controller, (ii) the improvement of control performance under uncertainties, and (iii) the experimental demonstration of attack detection capabilities in encrypted control systems."
  },
  {
    "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach",
    "url": "http://arxiv.org/abs/2507.22671v1",
    "arxiv_id": "2507.22671v1",
    "authors": [
      "Sami Saeed Alghamdi",
      "Christopher Bull",
      "Ahmed Kharrufa"
    ],
    "published": "2025-07-30T13:30:04+00:00",
    "summary": "Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools."
  },
  {
    "title": "Robust Voting under Uncertainty",
    "url": "http://arxiv.org/abs/2507.22655v1",
    "arxiv_id": "2507.22655v1",
    "authors": [
      "Satoshi Nakada",
      "Shmuel Nitzan",
      "Takashi Ui"
    ],
    "published": "2025-07-30T13:15:12+00:00",
    "summary": "This paper proposes normative criteria for voting rules under uncertainty about individual preferences. The criteria emphasize the importance of responsiveness, i.e., the probability that the social outcome coincides with the realized individual preferences. Given a convex set of probability distributions of preferences, denoted by $P$, a voting rule is said to be $P$-robust if, for each probability distribution in $P$, at least one individual's responsiveness exceeds one-half. Our main result establishes that a voting rule is $P$-robust if and only if there exists a nonnegative weight vector such that the weighted average of individual responsiveness is strictly greater than one-half under every extreme point of $P$. In particular, if the set $P$ includes all degenerate distributions, a $P$-robust rule is a weighted majority rule without ties."
  },
  {
    "title": "Planning Persuasive Trajectories Based on a Leader-Follower Game Model",
    "url": "http://arxiv.org/abs/2507.22022v1",
    "arxiv_id": "2507.22022v1",
    "authors": [
      "Chaozhe R. He",
      "Yichen Dong",
      "Nan Li"
    ],
    "published": "2025-07-29T17:16:00+00:00",
    "summary": "We propose a framework that enables autonomous vehicles (AVs) to proactively shape the intentions and behaviors of interacting human drivers. The framework employs a leader-follower game model with an adaptive role mechanism to predict human interaction intentions and behaviors. It then utilizes a branch model predictive control (MPC) algorithm to plan the AV trajectory, persuading the human to adopt the desired intention. The proposed framework is demonstrated in an intersection scenario. Simulation results illustrate the effectiveness of the framework for generating persuasive AV trajectories despite uncertainties."
  },
  {
    "title": "Uncertainty Estimation of the Optimal Decision with Application to Cure Process Optimization",
    "url": "http://arxiv.org/abs/2507.21995v1",
    "arxiv_id": "2507.21995v1",
    "authors": [
      "Yezhuo Li",
      "Qiong Zhang",
      "Madhura Limaye",
      "Gang Li"
    ],
    "published": "2025-07-29T16:45:01+00:00",
    "summary": "Decision-making in manufacturing often involves optimizing key process parameters using data collected from simulation experiments. Gaussian processes are widely used to surrogate the underlying system and guide optimization. Uncertainty often inherent in the decisions given by the surrogate model due to limited data and model assumptions. This paper proposes a surrogate model-based framework for estimating the uncertainty of optimal decisions and analyzing its sensitivity with respect to the objective function. The proposed approach is applied to the composite cure process simulation in manufacturing."
  },
  {
    "title": "Post-Training Large Language Models via Reinforcement Learning from Self-Feedback",
    "url": "http://arxiv.org/abs/2507.21931v1",
    "arxiv_id": "2507.21931v1",
    "authors": [
      "Carel van Niekerk",
      "Renato Vukovic",
      "Benjamin Matthias Ruppik",
      "Hsien-chin Lin",
      "Milica Ga\u0161i\u0107"
    ],
    "published": "2025-07-29T15:46:26+00:00",
    "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.   RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.   By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training."
  },
  {
    "title": "Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs",
    "url": "http://arxiv.org/abs/2507.21893v1",
    "arxiv_id": "2507.21893v1",
    "authors": [
      "Saeed Ghorbani"
    ],
    "published": "2025-07-29T15:01:31+00:00",
    "summary": "We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences."
  },
  {
    "title": "Multi-Gap superconductivity in HgS under pressure",
    "url": "http://arxiv.org/abs/2507.21869v1",
    "arxiv_id": "2507.21869v1",
    "authors": [
      "Pietro Maria Forcella",
      "Cesare Tresca",
      "Antonio Sanna",
      "Gianni Profeta"
    ],
    "published": "2025-07-29T14:41:19+00:00",
    "summary": "Mercury chalcogenides are a class of materials that exhibit diverse structural phases under pressure, leading to a range of exotic physical properties, including topological phases and chiral phonons. In particular, the phase diagram of mercury sulfide (HgS) remains difficult to characterize, with significant uncertainty surrounding the transition pressure between phases. Based on recent experimental results, we employ Density Functional Theory and Superconducting Density Functional Theory to investigate the pressure-induced structural phase transition in HgS and its interplay with the emergence of superconductivity as the crystal transitions from the cinnabar phase (space group P3$_1$21) to the rock salt phase (space group Fm$\\bar{3}$m). Remarkably, the rocksalt phase hosts a multigap superconducting state driven by distinct Fermi surface sheets, with two dominant gaps; the unusually high critical temperature of $\\sim$11 K emerges naturally within this multiband scenario, highlighting the role of interband coupling beyond isotropic models. These results place HgS among the few systems where multiband superconducting gap structures emerge under pressure."
  },
  {
    "title": "Two-neutrino $\u03b2\u03b2$ decay to excited states at next-to-leading order",
    "url": "http://arxiv.org/abs/2507.21868v1",
    "arxiv_id": "2507.21868v1",
    "authors": [
      "Daniel Castillo",
      "Dorian Frycz",
      "Beatriz Benavente",
      "Javier Men\u00e9ndez"
    ],
    "published": "2025-07-29T14:40:58+00:00",
    "summary": "We study two-neutrino double-beta decay ($2\\nu\\beta\\beta$) into first-excited $0^+_2$ states of nuclei used in $\\beta\\beta$ decay experiments, including $^{76}$Ge, $^{82}$Se, $^{130}$Te, and $^{136}$Xe. We calculate the corresponding nuclear matrix elements (NMEs) within the nuclear shell model, using various Hamiltonians that describe well the spectroscopy of the initial and final nuclei. We evaluate the next-to-leading order (NLO) long-range NMEs recently introduced within chiral effective field theory, keeping three terms in the expansion of the energy denominator. In most cases, NLO contributions to the half-life are below 5%, but they can increase to 25% due to cancellations in the leading-order Gamow-Teller NME. A detailed analysis in terms of nuclear deformation, including triaxiality, indicates that larger deformation differences between the initial and final states generally lead to smaller NMEs, but the seniority structure of the states also plays a relevant role. The lower range of our predicted half-lives, with uncertainties dominated by the nuclear Hamiltonian used, are slightly longer than the current experimental limit in $^{76}$Ge and the very recent half-life indication in $^{82}$Se."
  },
  {
    "title": "The Lyman-$\u03b1$ Forest from LBGs: First 3D Correlation Measurement with DESI and Prospects for Cosmology",
    "url": "http://arxiv.org/abs/2507.21852v1",
    "arxiv_id": "2507.21852v1",
    "authors": [
      "Hiram K. Herrera-Alcantar",
      "Eric Armengaud",
      "Christophe Y\u00e8che",
      "Calum Gordon",
      "Laura Casas",
      "Andreu Font-Ribera",
      "Christophe Magneville",
      "Corentin Ravoux",
      "J. Aguilar",
      "S. Ahlen",
      "A. Anand",
      "D. Brooks",
      "E. Chaussidon",
      "T. Claybaugh",
      "A. Cuceu",
      "K. S. Dawson",
      "A. de la Macorra",
      "P. Doel",
      "S. Ferraro",
      "J. E. Forero-Romero",
      "E. Gazta\u00f1aga",
      "S. Gontcho A Gontcho",
      "A. X. Gonzalez-Morales",
      "G. Gutierrez",
      "J. Guy",
      "C. Hahn",
      "D. Kirkby",
      "A. Kremin",
      "O. Lahav",
      "A. Lambert",
      "M. Landriau",
      "L. Le Guillou",
      "M. Manera",
      "P. Martini",
      "A. Meisner",
      "R. Miquel",
      "A. Mu\u00f1oz-Guti\u00e9rrez",
      "S. Nadathur",
      "N. Palanque-Delabrouille",
      "W. J. Percival",
      "F. Prada",
      "I. P\u00e9rez-R\u00e0fols",
      "G. Rossi",
      "E. Sanchez",
      "D. Schlegel",
      "M. Schubnell",
      "J. Silber",
      "D. Sprayberry",
      "G. Tarl\u00e9",
      "B. A. Weaver",
      "R. Zhou",
      "H. Zou"
    ],
    "published": "2025-07-29T14:26:36+00:00",
    "summary": "The Lyman-$\\alpha$ (Ly$\\alpha$) forest is a key tracer of large-scale structure at redshifts z > 2, traditionally studied using spectra of quasars. Here, we explore the viability Lyman Break Galaxies (LBGs) as alternative background sources for Ly$\\alpha$ forest studies. We analyze 4,151 Ly$\\alpha$ forest skewers extracted from LBG spectra obtained in the DESI pilot surveys in the COSMOS and XMM-LSS fields. We present the first measurement of the Ly$\\alpha$ forest auto-correlation function derived exclusively from LBG spectra, probing comoving separations up to 48 $h^{-1}$Mpc at an effective redshift of $z_\\mathrm{eff}$ = 2.70. The measured signal is consistent with that from DESI DR2 quasar Ly$\\alpha$ forest spectra at a comparable redshift, validating LBGs as reliable background sources. We also measure the cross-correlation between the LBG Ly$\\alpha$ forest and 13,362 galaxy positions, showing that this observable serves as a sensitive diagnostic for galaxy redshift uncertainties and systematic offsets. Finally, using synthetic LBG spectra and Fisher forecasts, we show that a future wide-area survey over 5000 deg$^2$, targeting 1000 LBGs per deg$^2$ at similar signal-to-noise than our dataset, could enable Ly$\\alpha$ forest baryon acoustic oscillation (BAO) measurements with 0.4% precision on the isotropic BAO scale and 1.3% on the anisotropic (Alcock-Paczynski) scale. Combining BAO with a Ly$\\alpha$ forest full-shape analysis improves the AP constraint to 0.6%. These results open a new path for precision cosmology at high redshift using dense LBG samples."
  },
  {
    "title": "Probabilistic Active Goal Recognition",
    "url": "http://arxiv.org/abs/2507.21846v1",
    "arxiv_id": "2507.21846v1",
    "authors": [
      "Chenyuan Zhang",
      "Cristian Rojas Cardenas",
      "Hamid Rezatofighi",
      "Mor Vered",
      "Buser Say"
    ],
    "published": "2025-07-29T14:22:29+00:00",
    "summary": "In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems."
  },
  {
    "title": "Distribution-Based Masked Medical Vision-Language Model Using Structured Reports",
    "url": "http://arxiv.org/abs/2507.21794v1",
    "arxiv_id": "2507.21794v1",
    "authors": [
      "Shreyank N Gowda",
      "Ruichi Zhang",
      "Xiao Gu",
      "Ying Weng",
      "Lu Yang"
    ],
    "published": "2025-07-29T13:31:24+00:00",
    "summary": "Medical image-language pre-training aims to align medical images with clinically relevant text to improve model performance on various downstream tasks. However, existing models often struggle with the variability and ambiguity inherent in medical data, limiting their ability to capture nuanced clinical information and uncertainty. This work introduces an uncertainty-aware medical image-text pre-training model that enhances generalization capabilities in medical image analysis. Building on previous methods and focusing on Chest X-Rays, our approach utilizes structured text reports generated by a large language model (LLM) to augment image data with clinically relevant context. These reports begin with a definition of the disease, followed by the `appearance' section to highlight critical regions of interest, and finally `observations' and `verdicts' that ground model predictions in clinical semantics. By modeling both inter- and intra-modal uncertainty, our framework captures the inherent ambiguity in medical images and text, yielding improved representations and performance on downstream tasks. Our model demonstrates significant advances in medical image-text pre-training, obtaining state-of-the-art performance on multiple downstream tasks."
  },
  {
    "title": "The impact of large-scale EV charging on the real-time operation of distribution systems: A comprehensive review",
    "url": "http://arxiv.org/abs/2507.21759v1",
    "arxiv_id": "2507.21759v1",
    "authors": [
      "Zhe Yu",
      "Chuang Yang",
      "Qin Wang"
    ],
    "published": "2025-07-29T12:42:54+00:00",
    "summary": "With the large-scale integration of electric vehicles (EVs) in the distribution grid, the unpredictable nature of EV charging introduces considerable uncertainties to the grid's real-time operations. This can exacerbate load fluctuations, compromise power quality, and pose risks to the grid's stability and security. However, due to their dual role as controllable loads and energy storage devices, EVs have the potential to mitigate these fluctuations, balance the variability of renewable energy sources, and provide ancillary services that support grid stability. By leveraging the bidirectional flow of information and energy in smart grids, the adverse effects of EV charging can be minimized and even converted into beneficial outcomes through effective real-time management strategies. This paper explores the negative impacts of EV charging on the distribution system's real-time operations and outlines methods to transform these challenges into positive contributions. Additionally, it provides an in-depth analysis of the real-time management system for EV charging, focusing on state estimation and management strategies."
  },
  {
    "title": "Locally Adaptive Conformal Inference for Operator Models",
    "url": "http://arxiv.org/abs/2507.20975v1",
    "arxiv_id": "2507.20975v1",
    "authors": [
      "Trevor Harris",
      "Yan Liu"
    ],
    "published": "2025-07-28T16:37:56+00:00",
    "summary": "Operator models are regression algorithms for functional data and have become a key tool for emulating large-scale dynamical systems. Recent advances in deep neural operators have dramatically improved the accuracy and scalability of operator modeling, but lack an inherent notion of predictive uncertainty. We introduce Local Spectral Conformal Inference (LSCI), a new framework for locally adaptive, distribution-free uncertainty quantification for neural operator models. LSCI uses projection-based depth scoring and localized conformal inference to generate function-valued prediction sets with statistical guarantees. We prove approximate finite-sample marginal coverage under local exchangeability, and demonstrate significant gains in adaptivity and coverage across synthetic and real-world operator learning tasks."
  },
  {
    "title": "The Concordance of Weak Lensing and Escape Velocity Mass Estimates for Galaxy Clusters",
    "url": "http://arxiv.org/abs/2507.20938v1",
    "arxiv_id": "2507.20938v1",
    "authors": [
      "Alexander Rodriguez",
      "Christopher J. Miller"
    ],
    "published": "2025-07-28T15:51:55+00:00",
    "summary": "In the $\\Lambda$CDM paradigm, the masses of the galaxy clusters inferred using background galaxies via weak-lensing shear should agree with the masses measured using the galaxy projected radius-velocity phase-space data via the escape velocity profile. However, prior work indicates that the correlation between caustic-inferred escape masses and weak lensing masses is statistically consistent with zero. Based on recent advancements in the measurement of the escape edge and its physical interpretation, we conduct a revised comparison between these two independent mass inference techniques for 46 galaxy clusters between $0.05 \\le z \\le 0.3$ and over an order of magnitude in mass, $14.4 \\le {\\rm log}_{10} M/M_{\\odot} \\le 15.4$. We find excellent agreement, with a correlation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between the two mass measurements consistent with zero (0.02 $\\pm$ 0.02 dex). The observed scatter between these direct mass estimates is 0.17 dex and is consistent with the reported individual mass errors, suggesting that there is no need for an additional intrinsic component. We discuss the important practical consequences of these results, focusing on the systematic uncertainties inherent to each technique, and their implications for cosmology."
  },
  {
    "title": "Target-density formation in swarms with stochastic sensing and dynamics",
    "url": "http://arxiv.org/abs/2507.20911v1",
    "arxiv_id": "2507.20911v1",
    "authors": [
      "Jason Hindes",
      "George Stantchev",
      "Klimka Szwaykowska Kasraie",
      "Ira B. Schwartz"
    ],
    "published": "2025-07-28T15:04:50+00:00",
    "summary": "An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target."
  },
  {
    "title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception",
    "url": "http://arxiv.org/abs/2507.20879v1",
    "arxiv_id": "2507.20879v1",
    "authors": [
      "Weicheng Zheng",
      "Xiaofei Mao",
      "Nanfei Ye",
      "Pengxiang Li",
      "Kun Zhan",
      "Xianpeng Lang",
      "Hang Zhao"
    ],
    "published": "2025-07-28T14:33:15+00:00",
    "summary": "Vision-Language Models (VLMs) are advancing autonomous driving, yet their potential is constrained by myopic decision-making and passive perception, limiting reliability in complex environments. We introduce DriveAgent-R1 to tackle these challenges in long-horizon, high-level behavioral decision-making. DriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that adaptively switches between efficient text-based and in-depth tool-based reasoning, and an Active Perception mechanism with a vision toolkit to proactively resolve uncertainties, thereby balancing decision-making efficiency and reliability. The agent is trained using a novel, three-stage progressive reinforcement learning strategy designed to master these hybrid capabilities. Extensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art performance, outperforming even leading proprietary large multimodal models, such as Claude Sonnet 4. Ablation studies validate our approach and confirm that the agent's decisions are robustly grounded in actively perceived visual evidence, paving a path toward safer and more intelligent autonomous systems."
  },
  {
    "title": "Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling",
    "url": "http://arxiv.org/abs/2507.20861v1",
    "arxiv_id": "2507.20861v1",
    "authors": [
      "Marco Faroni",
      "Carlo Odesco",
      "Andrea Zanchettin",
      "Paolo Rocco"
    ],
    "published": "2025-07-28T14:15:10+00:00",
    "summary": "Physics-based simulations and learning-based models are vital for complex robotics tasks like deformable object manipulation and liquid handling. However, these models often struggle with accuracy due to epistemic uncertainty or the sim-to-real gap. For instance, accurately pouring liquid from one container to another poses challenges, particularly when models are trained on limited demonstrations and may perform poorly in novel situations. This paper proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed to mitigate these inaccuracies. By incorporating estimates of model uncertainty, the proposed MCTS strategy biases the search towards actions with lower predicted uncertainty. This approach enhances the reliability of planning under uncertain conditions. Applied to a liquid pouring task, our method demonstrates improved success rates even with models trained on minimal data, outperforming traditional methods and showcasing its potential for robust decision-making in robotics."
  },
  {
    "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments",
    "url": "http://arxiv.org/abs/2507.20850v1",
    "arxiv_id": "2507.20850v1",
    "authors": [
      "Meiting Dang",
      "Yanping Wu",
      "Yafei Wang",
      "Dezong Zhao",
      "David Flynn",
      "Chongfeng Wei"
    ],
    "published": "2025-07-28T14:02:00+00:00",
    "summary": "Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method."
  },
  {
    "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs",
    "url": "http://arxiv.org/abs/2507.20804v1",
    "arxiv_id": "2507.20804v1",
    "authors": [
      "Xueyao Wan",
      "Hang Yu"
    ],
    "published": "2025-07-28T13:16:23+00:00",
    "summary": "Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths."
  },
  {
    "title": "Physical Constraints on the Rhythmicity of the Biological Clock",
    "url": "http://arxiv.org/abs/2507.20750v1",
    "arxiv_id": "2507.20750v1",
    "authors": [
      "YeongKyu Lee",
      "Changbong Hyeon"
    ],
    "published": "2025-07-28T11:57:56+00:00",
    "summary": "Circadian rhythms in living organisms are temporal orders emerging from biochemical circuits driven out of equilibrium. Here, we study how the rhythmicity of a biochemical clock is shaped using the KaiABC system. A phase diagram constructed as a function of KaiC and KaiA concentrations reveals a sharply bounded limit-cycle region, which naturally explains arrhythmia upon protein over-expression. Beyond the Hopf bifurcation, intrinsic noise enables regular oscillation via coherence resonance. Within the limit-cycle region, greater rhythmic precision incurs a higher energetic cost, following the thermodynamic uncertainty relation. The cost-minimizing period of the KaiABC clock ($\\sim$21-hr) is close enough to entrain to 24-hr cycle of environment. Our study substantiates universal physical constraints on the robustness, precision, and efficiency of noisy biological clocks."
  },
  {
    "title": "Generalized Uncertainty Principle as a Mechanism for CP Violation",
    "url": "http://arxiv.org/abs/2507.20727v1",
    "arxiv_id": "2507.20727v1",
    "authors": [
      "Hector Gisbert",
      "Victor Ilisie",
      "Ezequiel Valero"
    ],
    "published": "2025-07-28T11:24:56+00:00",
    "summary": "Within quantum electrodynamics we show that the Generalized Uncertainty Principle induces higher-derivative corrections that promote the topological invariant $F_{\\mu\\nu}\\,\\widetilde F^{\\mu\\nu}$ to the dynamical, non-topological operator $\\partial^\\lambda F_{\\mu\\nu}\\,\\partial_\\lambda \\widetilde F^{\\mu\\nu}$. We explore the resulting phenomenology, focusing on the generation of electric dipole moments. Our findings open a new low-energy window for testing quantum-gravity scenarios through precision measurements of charge-parity violation."
  },
  {
    "title": "Uncertainty-driven Embedding Convolution",
    "url": "http://arxiv.org/abs/2507.20718v1",
    "arxiv_id": "2507.20718v1",
    "authors": [
      "Sungjun Lim",
      "Kangjun Noh",
      "Youngjun Choi",
      "Heeyoung Lee",
      "Kyungwoo Song"
    ],
    "published": "2025-07-28T11:15:25+00:00",
    "summary": "Text embeddings are essential components in modern NLP pipelines. While numerous embedding models have been proposed, their performance varies across domains, and no single model consistently excels across all tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble weights based on embedding uncertainty, grounded in a Bayes-optimal solution under a surrogate loss. Additionally, UEC introduces an uncertainty-aware similarity function that directly incorporates uncertainty into similarity scoring. Extensive experiments on retrieval, classification, and semantic similarity benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling."
  },
  {
    "title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints",
    "url": "http://arxiv.org/abs/2507.19458v1",
    "arxiv_id": "2507.19458v1",
    "authors": [
      "Amir Fard",
      "Arnold X. -X. Yuan"
    ],
    "published": "2025-07-25T17:42:34+00:00",
    "summary": "Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows."
  },
  {
    "title": "DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment",
    "url": "http://arxiv.org/abs/2507.19418v1",
    "arxiv_id": "2507.19418v1",
    "authors": [
      "Yiwei Lou",
      "Yuanpeng He",
      "Rongchao Zhang",
      "Yongzhi Cao",
      "Hanpin Wang",
      "Yu Huang"
    ],
    "published": "2025-07-25T16:36:45+00:00",
    "summary": "Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios."
  },
  {
    "title": "Measurement of the Inelastic Proton-Proton Cross-Section at $\\sqrt{s} \\geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory",
    "url": "http://arxiv.org/abs/2507.19326v1",
    "arxiv_id": "2507.19326v1",
    "authors": [
      "Olena Tkachenko"
    ],
    "published": "2025-07-25T14:38:02+00:00",
    "summary": "Measuring proton-proton interaction cross-sections at center-of-mass energies above 40 TeV remains a significant challenge in particle physics. The Pierre Auger Observatory provides a unique opportunity to study the interactions at the highest energies through the distribution of the depth of maximum shower development ($X_\\mathrm{max}$) observed by its Fluorescence Detector. In previous studies, the determination of the interaction cross-section at ultrahigh energies has relied on the assumption that the tail of the $X_\\mathrm{max}$ distribution is proton-dominated, which restricts the analysis to a limited energy range below the ankle and introduces related systematic uncertainties. In this contribution, we adopt a novel method for the simultaneous estimation of the proton-proton interaction cross-section and the primary cosmic-ray mass composition using data from the Pierre Auger Observatory, avoiding assumptions about one quantity to infer the other and thus improving the accuracy and robustness of our analysis. In addition, a systematic shift in the $X_\\mathrm{max}$ scale is fitted to account for both experimental uncertainties and theoretical constraints on the modeling of particle interactions. The obtained results are consistent with previous analyses and provide additional constraints on hadronic interaction models. The measured proton-proton inelastic cross-section at ultra-high energies agrees well with extrapolations of accelerator data. The inferred cosmic-ray composition and the $X_\\mathrm{max}$-scale shift are also compatible with previous estimates."
  },
  {
    "title": "Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games",
    "url": "http://arxiv.org/abs/2507.19263v1",
    "arxiv_id": "2507.19263v1",
    "authors": [
      "Achille Morenville",
      "\u00c9ric Piette"
    ],
    "published": "2025-07-25T13:38:44+00:00",
    "summary": "In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings."
  },
  {
    "title": "Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation",
    "url": "http://arxiv.org/abs/2507.19246v1",
    "arxiv_id": "2507.19246v1",
    "authors": [
      "Robert Hahn",
      "Sebastian Sch\u00f6ps"
    ],
    "published": "2025-07-25T13:12:58+00:00",
    "summary": "While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort."
  },
  {
    "title": "Constraining the origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory: a three-dimensional approach",
    "url": "http://arxiv.org/abs/2507.19216v1",
    "arxiv_id": "2507.19216v1",
    "authors": [
      "Marta Bianciotto"
    ],
    "published": "2025-07-25T12:37:32+00:00",
    "summary": "Unveiling the sources of ultra-high-energy cosmic rays remains one of the main challenges of high-energy astrophysics. Measurements of anisotropies in their arrival directions are key to identifying their sources, yet magnetic deflections obscure direct associations. In this work, we reconstruct the sky regions of possible origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory by tracing their trajectories through Galactic magnetic fields using up-to-date models, while fully accounting for energy and directional uncertainties. A mixed composition at injection is assumed to model the detected charge distributions of such events. Different classes of astrophysical sources are investigated and tested for a correlation with the inferred regions of origin of the events. By incorporating constraints on the maximum propagation distances, we also allow for a three-dimensional localization of the possible source regions. Our findings provide new constraints on the sources of the highest-energy cosmic particles and offer fresh insights into the role of Galactic magnetic fields in shaping the observed ultra-high-energy cosmic-ray sky."
  },
  {
    "title": "Emulating redshift-mixing due to blending in weak gravitational lensing",
    "url": "http://arxiv.org/abs/2507.19130v1",
    "arxiv_id": "2507.19130v1",
    "authors": [
      "Zekang Zhang",
      "Daniel Gruen",
      "Luca Tortorelli",
      "Shun-Sheng Li",
      "Jamie McCullough"
    ],
    "published": "2025-07-25T10:11:40+00:00",
    "summary": "Galaxies whose images overlap in the focal plane of a telescope, commonly referred to as blends, are often located at different redshifts. Blending introduces a challenge to weak lensing cosmology probes, as such blends are subject to shear signals from multiple redshifts. This effect can be described by joining shear bias and redshift characterisation in the effective redshift distribution, $n_{\\gamma}(z)$, which includes the response of apparent shapes of detected objects to shear of galaxies at redshift $z$. In this work, we propose a novel method to correct $n_{\\gamma}(z)$ for redshift-mixed blending by emulating the shear response to neighbouring galaxies. Specifically, we design a ``half-sky-shearing'' simulation with HSC-Wide-like specifications, in which we extract the response of a detected object's measured ellipticity to shear of neighbouring galaxies among numerous galaxy pairs. We demonstrate the feasibility of accurately emulating these pairwise responses and validate the robustness of our approach under varying observing conditions and galaxy population uncertainties. We find that the effective redshift of sources at the high-redshift tail of the distribution is about 0.05 lower than expected when not modelling the effect. Given appropriately processed image simulations, our correction method can be readily incorporated into future cosmological analyses to mitigate this source of systematic error."
  },
  {
    "title": "Boundary-layer transition in the age of data: from a comprehensive dataset to fine-grained prediction",
    "url": "http://arxiv.org/abs/2507.19120v1",
    "arxiv_id": "2507.19120v1",
    "authors": [
      "Wenhui Chang",
      "Hongyuan Hu",
      "Youcheng Xi",
      "Markus Kloker",
      "Honghui Teng",
      "Jie Ren"
    ],
    "published": "2025-07-25T09:57:06+00:00",
    "summary": "The laminar-to-turbulent transition remains a fundamental and enduring challenge in fluid mechanics. Its complexity arises from the intrinsic nonlinearity and extreme sensitivity to external disturbances. This transition is critical in a wide range of applications, including aerospace, marine engineering, geophysical flows, and energy systems. While the governing physics can be well described by the Navier-Stokes equations, practical prediction efforts often fall short due to the lack of comprehensive models for perturbation initialization and turbulence generation in numerical simulations. To address the uncertainty introduced by unforeseeable environmental perturbations, we propose a fine-grained predictive framework that accurately predicts the transition location. The framework generates an extensive dataset using nonlinear parabolized stability equations (NPSE). NPSE simulations are performed over a wide range of randomly prescribed initial conditions for the generic zero-pressure-gradient flat-plate boundary-layer flow, resulting in a large dataset that captures the nonlinear evolution of instability waves across three canonical transition pathways (Type-K, -H, and -O). From a database of 3000 simulation cases, we extract diagnostic quantities (e.g., wall pressure signals and skin-friction coefficients) from each simulation to construct a feature set that links pre-transition flow characteristics to transition onset locations. Machine learning models are systematically evaluated, with ensemble methods-particularly XGBoost-demonstrating exceptional predictive accuracy (mean relative error of approximately 0.001). Compared to methods currently available (e.g., N-factor, transitional turbulence model), this approach accounts for the physical process and achieves transition prediction without relying on any empirical parameters."
  },
  {
    "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations",
    "url": "http://arxiv.org/abs/2507.19117v1",
    "arxiv_id": "2507.19117v1",
    "authors": [
      "Swapnil Hingmire",
      "Ze Shi Li",
      "Shiyu",
      "Zeng",
      "Ahmed Musa Awon",
      "Luiz Franciscatto Guerra",
      "Neil Ernst"
    ],
    "published": "2025-07-25T09:51:42+00:00",
    "summary": "Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed."
  },
  {
    "title": "Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks",
    "url": "http://arxiv.org/abs/2507.19111v1",
    "arxiv_id": "2507.19111v1",
    "authors": [
      "Bowen Li",
      "Junting Chen"
    ],
    "published": "2025-07-25T09:48:30+00:00",
    "summary": "Dynamic low altitude networks offer significant potential for efficient and reliable data transport via unmanned aerial vehicles (UAVs) relays which usually operate with predetermined trajectories. However, it is challenging to optimize the data routing and resource allocation due to the time-varying topology and the need to control interference with terrestrial systems. Traditional schemes rely on time-expanded graphs with uniform and fine time subdivisions, making them impractical for interference-aware applications. This paper develops a dynamic space-time graph model with a cross-layer optimization framework that converts a joint routing and predictive resource allocation problem into a joint bottleneck path planning and resource allocation problem. We develop explicit deterministic bounds to handle the channel uncertainty and prove a monotonicity property in the problem structure that enables us to efficiently reach the globally optimal solution to the predictive resource allocation subproblem. Then, this approach is extended to multi-commodity transmission tasks through time-frequency allocation, and a bisection search algorithm is developed to find the optimum solution by leveraging the monotonicity of the feasible set family. Simulations verify that the single-commodity algorithm approaches global optimality with more than 30 dB performance gain over the classical graph-based methods for delay-sensitive and large data transportation. At the same time, the multi-commodity method achieves 100X improvements in dense service scenarios and enables an additional 20 dB performance gain by data segmenting."
  },
  {
    "title": "Partial-State DADS Control for Matched Unmodeled Dynamics",
    "url": "http://arxiv.org/abs/2507.18609v1",
    "arxiv_id": "2507.18609v1",
    "authors": [
      "Iasson Karafyllis",
      "Miroslav Krstic"
    ],
    "published": "2025-07-24T17:44:56+00:00",
    "summary": "We extend the Deadzone-Adapted Disturbance Suppression (DADS) control to time-invariant systems with dynamic uncertainties that satisfy the matching condition and for which no bounds for the disturbance and the unknown parameters are known. This problem is equivalent to partial-state adaptive feedback, where the states modeling the dynamic uncertainty are unmeasured. We show that the DADS controller can bypass small-gain conditions and achieve robust regulation for systems in spite of the fact that the strength of the interconnections has no known bound. Moreover, no gain and state drift arise, regardless of the size of the disturbances and unknown parameters. Finally, the paper provides the detailed analysis of a control system where the unmeasured state (or the dynamic uncertainty) is infinite-dimensional and described by a reaction-diffusion Partial Differential Equation, where the diffusion coefficient and the reaction term are unknown. It is shown that even in the infinite-dimensional case, a DADS controller can be designed and guarantees robust regulation of the plant state."
  },
  {
    "title": "Searching for Gravitational Waves with Gaia and its Cross-Correlation with PTA: Absolute vs Relative Astrometry",
    "url": "http://arxiv.org/abs/2507.18593v1",
    "arxiv_id": "2507.18593v1",
    "authors": [
      "Massimo Vaglio",
      "Mikel Falxa",
      "Giorgio Mentasti",
      "Arianna I. Renzini",
      "Adrien Kuntz",
      "Enrico Barausse",
      "Carlo Contaldi",
      "Alberto Sesana"
    ],
    "published": "2025-07-24T17:24:43+00:00",
    "summary": "Astrometric missions like Gaia provide exceptionally precise measurements of stellar positions and proper motions. Gravitational waves traveling between the observer and distant stars can induce small, correlated shifts in these apparent positions, a phenomenon known as astrometric deflection. The precision and scale of astrometric datasets make them well-suited for searching for a stochastic gravitational wave background, whose signature appears in the two-point correlation function of the deflection field across the sky. Although Gaia achieves high accuracy in measuring angular separations in its focal plane, systematic uncertainties in the satellite's absolute orientation limit the precision of absolute position measurements. These orientation errors can be mitigated by focusing on relative angles between star pairs, which effectively cancel out common-mode orientation noise. In this work, we compute the astrometric response and the overlap reduction functions for this relative astrometry approach, correcting previous expressions presented in the literature. We use a Fisher matrix analysis to compare the sensitivity of relative astrometry to that of conventional absolute astrometry. Our analysis shows that while the relative method is theoretically sound, its sensitivity is limited for closely spaced star pairs within a single Gaia field of view. Pairs with large angular separations could provide competitive sensitivity, but are practically inaccessible due to Gaia's scanning law. Finally, we demonstrate that combining astrometric data with observations from pulsar timing arrays leads to slight improvements in sensitivity at frequencies greater than approximately 10^-7 Hz."
  },
  {
    "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation",
    "url": "http://arxiv.org/abs/2507.18562v1",
    "arxiv_id": "2507.18562v1",
    "authors": [
      "Jiafeng Xiong",
      "Yuting Zhao"
    ],
    "published": "2025-07-24T16:36:47+00:00",
    "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference."
  },
  {
    "title": "Large deviations of ionic currents in dilute electrolytes",
    "url": "http://arxiv.org/abs/2507.18556v1",
    "arxiv_id": "2507.18556v1",
    "authors": [
      "Jafar Farhadi",
      "David T. Limmer"
    ],
    "published": "2025-07-24T16:29:45+00:00",
    "summary": "We evaluate the exponentially rare fluctuations of the ionic current for a dilute electrolyte by means of macroscopic fluctuation theory. We consider the fluctuating hydrodynamics of a fluid electrolyte described by a stochastic Poisson-Nernst-Planck equation. We derive the Euler-Lagrange equations that dictate the optimal concentration profiles of ions conditioned on exhibiting a given current, whose form determines the likelihood of that current in the long-time limit. For a symmetric electrolyte under small applied voltages, number density fluctuations are small, and ionic current fluctuations are Gaussian with a variance determined by the Nernst-Einstein conductivity. Under large applied potentials, where number densities vary, the ionic current distribution is generically non-Gaussian. Its structure is constrained thermodynamically by Gallavotti-Cohen symmetry and the thermodynamic uncertainty principle."
  },
  {
    "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction",
    "url": "http://arxiv.org/abs/2507.18498v1",
    "arxiv_id": "2507.18498v1",
    "authors": [
      "Zongzheng Zhang",
      "Xuchong Qiu",
      "Boran Zhang",
      "Guantian Zheng",
      "Xunjiang Gu",
      "Guoxuan Chi",
      "Huan-ang Gao",
      "Leichen Wang",
      "Ziming Liu",
      "Xinrun Li",
      "Igor Gilitschenski",
      "Hongyang Li",
      "Hang Zhao",
      "Hao Zhao"
    ],
    "published": "2025-07-24T15:13:11+00:00",
    "summary": "Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction."
  },
  {
    "title": "A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks",
    "url": "http://arxiv.org/abs/2507.18492v1",
    "arxiv_id": "2507.18492v1",
    "authors": [
      "Mirhan \u00dcrkmez",
      "Carsten Kalles\u00f8e",
      "Jan Dimon Bendtsen",
      "Eric C. Kerrigan",
      "John Leth"
    ],
    "published": "2025-07-24T15:04:53+00:00",
    "summary": "Water utilities aim to reduce the high electrical costs of Water Distribution Networks (WDNs), primarily driven by pumping. However, pump scheduling is challenging due to model uncertainties and water demand forecast errors. This paper presents a Robust Model Predictive Control (RMPC) method for optimal and reliable pump scheduling, extending a previous efficient robust control method tailored to our model. A linear model with bounded additive disturbances is used to represent tank water level evolution, with uncertainty bounds derived from WDN simulation and demand data. At each time step, a pump scheduling policy, affine in past disturbances, is optimized to satisfy system constraints over a prediction horizon. The resulting policies are then applied in a receding horizon fashion. The optimization problem is formulated to require $\\mathcal{O}(N^6)$ computations per iteration with an interior-point method, which is reduced to $\\mathcal{O}(N^3)$ by reformulating it into a sparse form. When evaluated on a model representing the water distribution network of Randers, a medium-sized town in Denmark, the method surpasses nominal and constraint-tightening model predictive control (MPC) approaches in terms of meeting constraints and provides comparable economic outcomes."
  },
  {
    "title": "Relativistic Calculations of Energy Levels, Field Shift Factors, and Polarizabilities of Mercury and Copernicium",
    "url": "http://arxiv.org/abs/2507.18490v1",
    "arxiv_id": "2507.18490v1",
    "authors": [
      "Hongxu Liu",
      "Jize Han",
      "Yanmei Yu",
      "Yanfeng Ge",
      "Yong Liu",
      "Zhiguo Huang"
    ],
    "published": "2025-07-24T15:03:31+00:00",
    "summary": "Mercury (Hg) and superheavy element copernicium (Cn) are investigated using equation-of-motion relativistic coupled-cluster (EOM-RCC) and configuration interaction plus many-body perturbation theory (CI+MBPT) methods. Key atomic properties including ionization potentials (IP), excitation energies (EEs), isotope field shift factors (F), and static electric dipole polarizabilities ({\\alpha}) are calculated for ground and low-lying excited states. To evaluate the theoretical accuracy, calculations for both Hg and Cn are performed, with experimental data of Hg serving as benchmarks. Furthermore, basis set dependence has been systematically evaluated in the EOM-RCC calculations, with corresponding uncertainty estimates having been provided. The calculated atomic properties could provide valuable insights into the electronic structure and chemical behavior of superheavy elements."
  },
  {
    "title": "INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data",
    "url": "http://arxiv.org/abs/2507.18488v1",
    "arxiv_id": "2507.18488v1",
    "authors": [
      "Mario Figueira",
      "Michela Cameletti",
      "Luca Patelli"
    ],
    "published": "2025-07-24T15:01:28+00:00",
    "summary": "Environmental processes often exhibit complex, non-linear patterns and discontinuities across space and time, posing significant challenges for traditional geostatistical modeling approaches. In this paper, we propose a hybrid spatio-temporal modeling framework that combines the interpretability and uncertainty quantification of Bayesian models -- estimated using the INLA-SPDE approach -- with the predictive power and flexibility of Random Forest (RF). Specifically, we introduce two novel algorithms, collectively named INLA-RF, which integrate a statistical spatio-temporal model with RF in an iterative two-stage framework. The first algorithm (INLA-RF1) incorporates RF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2) uses RF to directly correct selected latent field nodes. Both hybrid strategies enable uncertainty propagation between modeling stages, an aspect often overlooked in existing hybrid approaches. In addition, we propose a Kullback-Leibler divergence-based stopping criterion. We evaluate the predictive performance and uncertainty quantification capabilities of the proposed algorithms through two simulation studies. Results suggest that our hybrid approach enhances spatio-temporal prediction while maintaining interpretability and coherence in uncertainty estimates."
  },
  {
    "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
    "url": "http://arxiv.org/abs/2507.18484v1",
    "arxiv_id": "2507.18484v1",
    "authors": [
      "Xiao Yang",
      "Lingxuan Wu",
      "Lizhong Wang",
      "Chengyang Ying",
      "Hang Su",
      "Jun Zhu"
    ],
    "published": "2025-07-24T14:56:21+00:00",
    "summary": "Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving."
  },
  {
    "title": "Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins",
    "url": "http://arxiv.org/abs/2507.18423v1",
    "arxiv_id": "2507.18423v1",
    "authors": [
      "Mizuki Funato",
      "Yohei Sawada"
    ],
    "published": "2025-07-24T14:00:18+00:00",
    "summary": "Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions."
  },
  {
    "title": "A narrowband burst from FRB 20190520B simultaneously observed by FAST and Parkes",
    "url": "http://arxiv.org/abs/2507.17696v1",
    "arxiv_id": "2507.17696v1",
    "authors": [
      "Yuhao Zhu",
      "Chenhui Niu",
      "Shi Dai",
      "Di Li",
      "Pei Wang",
      "Yi Feng",
      "Jingwen Wu",
      "Yongkun Zhang",
      "Xianghan Cui",
      "Junshuo Zhang",
      "Jinhuang Cao"
    ],
    "published": "2025-07-23T17:03:01+00:00",
    "summary": "Fast Radio Bursts (FRBs) are short-duration radio transients with mysterious origins. Since its uncertainty, there are very few FRBs that are observed by different instruments, simultaneously. This study presents a detailed analysis of a burst from FRB 20190520B observed by FAST and Parkes at the same time. The spectrum of this individual burst ended at the upper limit of the FAST frequency band and was simultaneously detected by the Parkes telescope in the 1.5-1.8 GHz range. By employing spectral energy distribution (SED) and spectral sharpness methods, we confirmed the presence of narrowband radiation in FRB 20190520B, which is crucial for understanding its radiation mechanisms. Our findings support the narrowband characteristics that most repeaters exhibit. This work also highlights the necessity of continued multiband observations to explore its periodicity and frequency-dependent properties, contributing to an in-depth understanding of FRB phenomena."
  },
  {
    "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks",
    "url": "http://arxiv.org/abs/2507.17695v1",
    "arxiv_id": "2507.17695v1",
    "authors": [
      "Ilias Chatzistefanidis",
      "Navid Nikaein"
    ],
    "published": "2025-07-23T17:01:23+00:00",
    "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance."
  },
  {
    "title": "The Joint Asymptotic Distribution of Entropy and Complexity",
    "url": "http://arxiv.org/abs/2507.17625v1",
    "arxiv_id": "2507.17625v1",
    "authors": [
      "Angelika Silbernagel",
      "Christian Wei\u00df"
    ],
    "published": "2025-07-23T15:53:30+00:00",
    "summary": "We derive the asymptotic distribution of ordinal-pattern frequencies under weak dependence conditions and investigate the long-run covariance matrix not only analytically for moving-average, Gaussian, and the novel generalized coin-tossing processes, but also approximately by a simulation-based approach. Then, we deduce the asymptotic distribution of the entropy-complexity pair, which emerged as a popular tool for summarizing the time-series dynamics. Here, we make the necessary distinction between a uniform and a non-uniform ordinal pattern distribution and, thus, obtain two different limit theorems. On this basis, we consider a test for serial dependence and check its finite-sample performance. Moreover, we use our asymptotic results to approximate the estimation uncertainty of entropy-complexity pairs."
  },
  {
    "title": "Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling",
    "url": "http://arxiv.org/abs/2507.17526v1",
    "arxiv_id": "2507.17526v1",
    "authors": [
      "Leandro Von Krannichfeldt",
      "Kristina Orehounig",
      "Olga Fink"
    ],
    "published": "2025-07-23T14:07:33+00:00",
    "summary": "Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling."
  },
  {
    "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models",
    "url": "http://arxiv.org/abs/2507.17477v1",
    "arxiv_id": "2507.17477v1",
    "authors": [
      "Haoran Sun",
      "Zekun Zhang",
      "Shaoning Zeng"
    ],
    "published": "2025-07-23T13:00:00+00:00",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance."
  },
  {
    "title": "Analysing Models for Volatility Clustering with Subordinated Processes: VGSA and Beyond",
    "url": "http://arxiv.org/abs/2507.17431v1",
    "arxiv_id": "2507.17431v1",
    "authors": [
      "Sourojyoti Barick",
      "Sudip Ratan Chandra"
    ],
    "published": "2025-07-23T11:41:46+00:00",
    "summary": "This paper explores a comprehensive class of time-changed stochastic processes constructed by subordinating Brownian motion with Levy processes, where the subordination is further governed by stochastic arrival mechanisms such as the Cox Ingersoll Ross (CIR) and Chan Karolyi Longstaff Sanders (CKLS) processes. These models extend classical jump frameworks like the Variance Gamma (VG) and CGMY processes, allowing for more flexible modeling of market features such as jump clustering, heavy tails, and volatility persistence. We first revisit the theory of Levy subordinators and establish strong consistency results for the VG process under Gamma subordination. Building on this, we prove asymptotic normality for both the VG and VGSA (VG with stochastic arrival) processes when the arrival process follows CIR or CKLS dynamics. The analysis is then extended to the more general CGMY process under stochastic arrival, for which we derive analogous consistency and limit theorems under positivity and regularity conditions on the arrival process. A simulation study accompanies the theoretical work, confirming our results through Monte Carlo experiments, with visualizations and normality testing (via Shapiro-Wilk statistics) that show approximate Gaussian behavior even for processes driven by heavy-tailed jumps. This work provides a rigorous and unified probabilistic framework for analyzing subordinated models with stochastic time changes, with applications to financial modeling and inference under uncertainty."
  },
  {
    "title": "Confidence Calibration in Vision-Language-Action Models",
    "url": "http://arxiv.org/abs/2507.17383v1",
    "arxiv_id": "2507.17383v1",
    "authors": [
      "Thomas P Zollo",
      "Richard Zemel"
    ],
    "published": "2025-07-23T10:26:10+00:00",
    "summary": "Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification."
  },
  {
    "title": "Exploring Spatial Diversity for Region-based Active Learning",
    "url": "http://arxiv.org/abs/2507.17367v1",
    "arxiv_id": "2507.17367v1",
    "authors": [
      "Lile Cai",
      "Xun Xu",
      "Lining Zhang",
      "Chuan-Sheng Foo"
    ],
    "published": "2025-07-23T10:04:25+00:00",
    "summary": "State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves $95\\%$ performance of fully supervised methods with only $5-9\\%$ of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation."
  },
  {
    "title": "On Distributionally Robust Lossy Source Coding",
    "url": "http://arxiv.org/abs/2507.17366v1",
    "arxiv_id": "2507.17366v1",
    "authors": [
      "Giuseppe Serra",
      "Photios A. Stavrou",
      "Marios Kountouris"
    ],
    "published": "2025-07-23T10:01:59+00:00",
    "summary": "In this paper, we investigate the problem of distributionally robust source coding, i.e., source coding under uncertainty in the source distribution, discussing both the coding and computational aspects of the problem. We propose two extensions of the so-called Strong Functional Representation Lemma (SFRL), considering the cases where, for a fixed conditional distribution, the marginal inducing the joint coupling belongs to either a finite set of distributions or a Kullback-Leibler divergence sphere (KL-Sphere) centered at a fixed nominal distribution. Using these extensions, we derive distributionally robust coding schemes for both the one-shot and asymptotic regimes, generalizing previous results in the literature. Focusing on the case where the source distribution belongs to a given KL-Sphere, we derive an implicit characterization of the points attaining the robust rate-distortion function (R-RDF), which we later exploit to implement a novel algorithm for computing the R-RDF. Finally, we characterize the analytical expression of the R-RDF for Bernoulli sources, providing a theoretical benchmark to evaluate the estimation performance of the proposed algorithm."
  },
  {
    "title": "Integrating Belief Domains into Probabilistic Logic Programs",
    "url": "http://arxiv.org/abs/2507.17291v1",
    "arxiv_id": "2507.17291v1",
    "authors": [
      "Damiano Azzolini",
      "Fabrizio Riguzzi",
      "Theresa Swift"
    ],
    "published": "2025-07-23T07:52:09+00:00",
    "summary": "Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications."
  },
  {
    "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
    "url": "http://arxiv.org/abs/2507.16806v1",
    "arxiv_id": "2507.16806v1",
    "authors": [
      "Mehul Damani",
      "Isha Puri",
      "Stewart Slocum",
      "Idan Shenfeld",
      "Leshem Choshen",
      "Yoon Kim",
      "Jacob Andreas"
    ],
    "published": "2025-07-22T17:56:01+00:00",
    "summary": "When language models (LMs) are trained via reinforcement learning (RL) to generate natural language \"reasoning chains\", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or \"hallucinate\") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models."
  },
  {
    "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning",
    "url": "http://arxiv.org/abs/2507.16796v1",
    "arxiv_id": "2507.16796v1",
    "authors": [
      "Mian Ibad Ali Shah",
      "Enda Barrett",
      "Karl Mason"
    ],
    "published": "2025-07-22T17:46:28+00:00",
    "summary": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities."
  },
  {
    "title": "Quantum teleportation of an elemental silicon nanophotonic CNOT gate",
    "url": "http://arxiv.org/abs/2507.16783v1",
    "arxiv_id": "2507.16783v1",
    "authors": [
      "Kai-Chi Chang",
      "Xiang Cheng",
      "Felix Ribuot-Hirsch",
      "Murat Can Sarihan",
      "Yujie Chen",
      "Jaime Gonzalo Flor Flores",
      "Mingbin Yu",
      "Patrick Guo-Qiang Lo",
      "Dim-Lee Kwong",
      "Chee Wei Wong"
    ],
    "published": "2025-07-22T17:29:49+00:00",
    "summary": "Large-scale quantum computers possess the capacity to effectively tackle practical problems that can be insurmountable for classical computers. The main challenge in building these quantum computers is to realize scalable modules for remote qubits and entanglement. By assembling small, specialized parts into a larger architecture, the modular approach mitigates complexity and uncertainty. Such a distributed architecture requires non-local quantum gate operations between remote qubits. An essential method for implementing such operations, known as quantum gate teleportation, requires only local operations, classical communication, and shared entanglement. Till today, the quantum gate teleportation using a photonic chip has remained elusive. Here we experimentally demonstrate the quantum teleportation of an on-chip controlled-NOT (CNOT) gate, assisted with the scalable silicon chip platform, high-fidelity local quantum logic gates, linear optical components, post-selected entanglement, and coincidence measurements from photonic qubits. First, we measure and characterize our teleported chip-scale CNOT gate with an average truth table fidelity of 93.1 +- 0.3%. Second, for different input polarization states, we obtain an average quantum state fidelity of 87.0 +- 2.2% with our teleported on-chip CNOT gate. Third, we use our non-local CNOT gate for remote entanglement creation of four Bell states, with an average quantum state fidelity of 86.2 +- 0.8%. Fourthly, we fully characterize our teleported on-chip CNOT gate with a quantum process fidelity 83.1 +- 2.0%, and an average non-local CNOT gate fidelity of 86.5 +- 2.2%. Our teleported photonic on-chip quantum logic gate could be extended both to multiple qubits and chip-scale modules towards fault-tolerant and large-scale distributed quantum computation."
  },
  {
    "title": "A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling",
    "url": "http://arxiv.org/abs/2507.16771v1",
    "arxiv_id": "2507.16771v1",
    "authors": [
      "Michael Grosskopf",
      "Kellin Rumsey",
      "Ayan Biswas",
      "Earl Lawrence"
    ],
    "published": "2025-07-22T17:20:07+00:00",
    "summary": "The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case."
  },
  {
    "title": "Planck constraints on the scale dependence of isotropic cosmic birefringence",
    "url": "http://arxiv.org/abs/2507.16714v1",
    "arxiv_id": "2507.16714v1",
    "authors": [
      "M. Ballardini",
      "A. Gruppuso",
      "S. Paradiso",
      "S. S. Sirletti",
      "P. Natoli"
    ],
    "published": "2025-07-22T15:50:28+00:00",
    "summary": "The rotation of the linear polarisation plane of photons during propagation, also known as cosmic birefringence, is a powerful probe of parity-violating extensions of standard electromagnetism. Using Planck legacy data, we confirm previous estimates of the isotropic birefringence angle, finding $\\beta \\simeq 0.30 \\pm 0.05$ [deg] at 68% CL, not including the systematic error from the instrumental polarisation angle. If this is a genuine signal, it could be explained by theories of Chern--Simons-type coupled to electromagnetism, which could lead to a harmonic scale-dependent birefringence signal, if the hypothesis of an ultra-light (pseudo) scalar field does not hold. To investigate these models, we pursue two complementary approaches: first, we fit the birefringence angle estimated at different multipoles, $\\beta_{\\ell}$, with a power-law model and second, we perform a non-parametric Bayesian reconstruction of it. Both methods yield results consistent with a non-vanishing constant birefringence angle. The first method shows no significant dependence on the harmonic scale (up to $1.8\\sigma$ CL), while the second method demonstrates that a constant model is favored by Bayesian evidence. This conclusion is robust across all four published Planck CMB solutions. Finally, we forecast that upcoming CMB observations by Simons Observatory, LiteBIRD and a wishful CMB-Stage 4 experiment could reduce current uncertainties by a factor of approximately 7."
  },
  {
    "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains",
    "url": "http://arxiv.org/abs/2507.16670v1",
    "arxiv_id": "2507.16670v1",
    "authors": [
      "Amandeep Kaur",
      "Gyan Prakash"
    ],
    "published": "2025-07-22T15:02:54+00:00",
    "summary": "Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty."
  },
  {
    "title": "FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture",
    "url": "http://arxiv.org/abs/2507.16668v1",
    "arxiv_id": "2507.16668v1",
    "authors": [
      "Somayeh Sobati-M"
    ],
    "published": "2025-07-22T15:01:16+00:00",
    "summary": "Modern smart grids demand fast, intelligent, and energy-aware computing at the edge to manage real time fluctuations and ensure reliable operation. This paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration and Twin based Execution a next-generation fog cloud framework designed to enhance autonomy, resilience, and efficiency in distributed energy systems. FOGNITE combines three core components: federated learning, reinforcement learning, and digital twin validation. Each fog node trains a local CNN LSTM model on private energy consumption data, enabling predictive intelligence while preserving data privacy through federated aggregation. A reinforcement learning agent dynamically schedules tasks based on current system load and energy conditions, optimizing for performance under uncertainty.   To prevent unsafe or inefficient decisions, a hierarchical digital twin layer simulates potential actions before deployment, significantly reducing execution errors and energy waste. We evaluate FOGNITE on a real world testbed of Raspberry Pi devices, showing up to a 93.7% improvement in load balancing accuracy and a 63.2% reduction in energy waste compared to conventional architectures. By shifting smart grid control from reactive correction to proactive optimization, FOGNITE represents a step toward more intelligent, adaptive, and sustainable energy infrastructures"
  },
  {
    "title": "A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market",
    "url": "http://arxiv.org/abs/2507.16655v1",
    "arxiv_id": "2507.16655v1",
    "authors": [
      "Mahdi Goldani"
    ],
    "published": "2025-07-22T14:51:45+00:00",
    "summary": "In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis."
  },
  {
    "title": "Bayesian Variational Inference for Mixed Data Mixture Models",
    "url": "http://arxiv.org/abs/2507.16545v1",
    "arxiv_id": "2507.16545v1",
    "authors": [
      "Junyang Wang",
      "James Bennett",
      "Victor Lhoste",
      "Sarah Filippi"
    ],
    "published": "2025-07-22T12:54:03+00:00",
    "summary": "Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. In addition, we show that the posterior means from CAVI for this model converge to the true parameter value as the sample size n tends to infinity, providing theoretical justification for our method."
  },
  {
    "title": "A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in Nonlinear Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2507.16520v1",
    "arxiv_id": "2507.16520v1",
    "authors": [
      "Aria Delshad",
      "Maryam Babazadeh"
    ],
    "published": "2025-07-22T12:30:06+00:00",
    "summary": "This paper proposes a reinforcement learning (RL)-based backstepping control strategy to achieve fixed time consensus in nonlinear multi-agent systems with strict feedback dynamics. Agents exchange only output information with their neighbors over a directed communication graph, without requiring full state measurements or symmetric communication. Achieving fixed time consensus, where convergence occurs within a pre-specified time bound that is independent of initial conditions is faced with significant challenges due to the presence of unknown nonlinearities, inter-agent couplings, and external disturbances. This work addresses these challenges by integrating actor critic reinforcement learning with a novel fixed time adaptation mechanism. Each agent employs an actor critic architecture supported by two estimator networks designed to handle system uncertainties and unknown perturbations. The adaptation laws are developed to ensure that all agents track the leader within a fixed time regardless of their initial conditions. The consensus and tracking errors are guaranteed to converge to a small neighborhood of the origin, with the convergence radius adjustable through control parameters. Simulation results demonstrate the effectiveness of the proposed approach and highlight its advantages over state-of-the-art methods in terms of convergence speed and robustness."
  },
  {
    "title": "Euclid preparation: Expected constraints on initial conditions",
    "url": "http://arxiv.org/abs/2507.15819v1",
    "arxiv_id": "2507.15819v1",
    "authors": [
      "Euclid Collaboration",
      "F. Finelli",
      "Y. Akrami",
      "A. Andrews",
      "M. Ballardini",
      "S. Casas",
      "D. Karagiannis",
      "Z. Sakr",
      "J. Valiviita",
      "G. Alestas",
      "N. Bartolo",
      "J. R. Bermejo-Climent",
      "S. Nesseris",
      "D. Paoletti",
      "D. Sapone",
      "I. Tutusaus",
      "A. Ach\u00facarro",
      "G. Ca\u00f1as-Herrera",
      "J. Jasche",
      "G. Lavaux",
      "N. Aghanim",
      "B. Altieri",
      "A. Amara",
      "L. Amendola",
      "S. Andreon",
      "N. Auricchio",
      "C. Baccigalupi",
      "D. Bagot",
      "M. Baldi",
      "S. Bardelli",
      "P. Battaglia",
      "A. Biviano",
      "E. Branchini",
      "M. Brescia",
      "S. Camera",
      "V. Capobianco",
      "C. Carbone",
      "J. Carretero",
      "M. Castellano",
      "G. Castignani",
      "S. Cavuoti",
      "K. C. Chambers",
      "A. Cimatti",
      "C. Colodro-Conde",
      "G. Congedo",
      "C. J. Conselice",
      "L. Conversi",
      "Y. Copin",
      "F. Courbin",
      "H. M. Courtois",
      "M. Cropper",
      "A. Da Silva",
      "H. Degaudenzi",
      "S. de la Torre",
      "G. De Lucia",
      "A. M. Di Giorgio",
      "H. Dole",
      "M. Douspis",
      "F. Dubath",
      "C. A. J. Duncan",
      "X. Dupac",
      "S. Dusini",
      "S. Escoffier",
      "M. Farina",
      "R. Farinelli",
      "F. Faustini",
      "S. Ferriol",
      "P. Fosalba",
      "M. Frailis",
      "E. Franceschi",
      "M. Fumana",
      "S. Galeotta",
      "K. George",
      "B. Gillis",
      "C. Giocoli",
      "J. Gracia-Carpio",
      "A. Grazian",
      "F. Grupp",
      "S. V. H. Haugan",
      "W. Holmes",
      "I. M. Hook",
      "F. Hormuth",
      "A. Hornstrup",
      "K. Jahnke",
      "M. Jhabvala",
      "B. Joachimi",
      "E. Keih\u00e4nen",
      "S. Kermiche",
      "A. Kiessling",
      "B. Kubik",
      "M. K\u00fcmmel",
      "M. Kunz",
      "H. Kurki-Suonio",
      "A. M. C. Le Brun",
      "S. Ligori",
      "P. B. Lilje",
      "V. Lindholm",
      "I. Lloro",
      "G. Mainetti",
      "D. Maino",
      "E. Maiorano",
      "O. Mansutti",
      "S. Marcin",
      "O. Marggraf",
      "M. Martinelli",
      "N. Martinet",
      "F. Marulli",
      "R. J. Massey",
      "E. Medinaceli",
      "S. Mei",
      "Y. Mellier",
      "M. Meneghetti",
      "E. Merlin",
      "G. Meylan",
      "A. Mora",
      "M. Moresco",
      "L. Moscardini",
      "C. Neissner",
      "S. -M. Niemi",
      "C. Padilla",
      "S. Paltani",
      "F. Pasian",
      "K. Pedersen",
      "W. J. Percival",
      "V. Pettorino",
      "S. Pires",
      "G. Polenta",
      "M. Poncet",
      "L. A. Popa",
      "L. Pozzetti",
      "F. Raison",
      "R. Rebolo",
      "A. Renzi",
      "J. Rhodes",
      "G. Riccio",
      "E. Romelli",
      "M. Roncarelli",
      "C. Rosset",
      "R. Saglia",
      "B. Sartoris",
      "M. Schirmer",
      "T. Schrabback",
      "A. Secroun",
      "E. Sefusatti",
      "G. Seidel",
      "M. Seiffert",
      "S. Serrano",
      "P. Simon",
      "C. Sirignano",
      "G. Sirri",
      "A. Spurio Mancini",
      "L. Stanco",
      "J. Steinwagner",
      "P. Tallada-Cresp\u00ed",
      "D. Tavagnacco",
      "A. N. Taylor",
      "I. Tereno",
      "N. Tessore",
      "S. Toft",
      "R. Toledo-Moreo",
      "F. Torradeflot",
      "L. Valenziano",
      "T. Vassallo",
      "G. Verdoes Kleijn",
      "A. Veropalumbo",
      "Y. Wang",
      "J. Weller",
      "A. Zacchei",
      "G. Zamorani",
      "F. M. Zerbi",
      "E. Zucca",
      "V. Allevato",
      "E. Bozzo",
      "C. Burigana",
      "R. Cabanac",
      "M. Calabrese",
      "A. Cappi",
      "D. Di Ferdinando",
      "J. A. Escartin Vigo",
      "L. Gabarra",
      "J. Mart\u00edn-Fleitas",
      "S. Matthew",
      "N. Mauri",
      "R. B. Metcalf",
      "A. A. Nucita",
      "A. Pezzotta",
      "M. P\u00f6ntinen",
      "C. Porciani",
      "I. Risso",
      "V. Scottez",
      "M. Sereno",
      "M. Tenti",
      "M. Viel",
      "M. Wiesmann",
      "I. T. Andika",
      "M. Archidiacono",
      "F. Atrio-Barandela",
      "S. Avila",
      "A. Balaguera-Antolinez",
      "D. Bertacca",
      "M. Bethermin",
      "A. Blanchard",
      "L. Blot",
      "H. B\u00f6hringer",
      "S. Borgani",
      "M. L. Brown",
      "S. Bruton",
      "A. Calabro",
      "B. Camacho Quevedo",
      "F. Caro",
      "C. S. Carvalho",
      "T. Castro",
      "F. Cogato",
      "S. Conseil",
      "A. R. Cooray",
      "S. Davini",
      "F. De Paolis",
      "G. Desprez",
      "A. D\u00edaz-S\u00e1nchez",
      "J. J. Diaz",
      "S. Di Domizio",
      "J. M. Diego",
      "P. Dimauro",
      "A. Enia",
      "Y. Fang",
      "A. G. Ferrari",
      "A. Finoguenov",
      "A. Fontana",
      "A. Franco",
      "K. Ganga",
      "J. Garc\u00eda-Bellido",
      "T. Gasparetto",
      "V. Gautard",
      "E. Gaztanaga",
      "F. Giacomini",
      "F. Gianotti",
      "G. Gozaliasl",
      "A. Gruppuso",
      "M. Guidi",
      "C. M. Gutierrez",
      "S. Hemmati",
      "C. Hern\u00e1ndez-Monteagudo",
      "H. Hildebrandt",
      "J. Hjorth",
      "S. Joudaki",
      "J. J. E. Kajava",
      "Y. Kang",
      "V. Kansal",
      "K. Kiiveri",
      "C. C. Kirkpatrick",
      "S. Kruk",
      "M. Lattanzi",
      "V. Le Brun",
      "J. Le Graet",
      "L. Legrand",
      "M. Lembo",
      "F. Lepori",
      "G. Leroy",
      "G. F. Lesci",
      "J. Lesgourgues",
      "L. Leuzzi",
      "T. I. Liaudat",
      "J. Macias-Perez",
      "G. Maggio",
      "M. Magliocchetti",
      "F. Mannucci",
      "R. Maoli",
      "C. J. A. P. Martins",
      "L. Maurin",
      "M. Migliaccio",
      "M. Miluzio",
      "P. Monaco",
      "C. Moretti",
      "G. Morgante",
      "S. Nadathur",
      "K. Naidoo",
      "A. Navarro-Alsina",
      "L. Pagano",
      "F. Passalacqua",
      "K. Paterson",
      "L. Patrizii",
      "A. Pisani",
      "D. Potter",
      "S. Quai",
      "M. Radovich",
      "P. Reimberg",
      "P. -F. Rocci",
      "G. Rodighiero",
      "S. Sacquegna",
      "M. Sahl\u00e9n",
      "D. B. Sanders",
      "E. Sarpa",
      "A. Schneider",
      "D. Sciotti",
      "E. Sellentin",
      "L. C. Smith",
      "K. Tanidis",
      "C. Tao",
      "G. Testera",
      "R. Teyssier",
      "S. Tosi",
      "A. Troja",
      "M. Tucci",
      "C. Valieri",
      "A. Venhola",
      "D. Vergani",
      "F. Vernizzi",
      "G. Verza",
      "P. Vielzeuf",
      "N. A. Walton"
    ],
    "published": "2025-07-21T17:25:32+00:00",
    "summary": "The Euclid mission of the European Space Agency will deliver galaxy and cosmic shear surveys, which will be used to constrain initial conditions and statistics of primordial fluctuations. We present highlights for the Euclid scientific capability to test initial conditions beyond LCDM with the main probes, i.e. 3D galaxy clustering from the spectroscopic survey, the tomographic approach to 3x2pt statistics from photometric galaxy survey, and their combination. We provide Fisher forecasts from the combination of Euclid spectroscopic and photometric surveys for spatial curvature, running of the spectral index of the power spectrum of curvature perturbations, isocurvature perturbations, and primordial features. For the parameters of these models we also provide the combination of Euclid forecasts (pessimistic and optimistic) with current and future measurements of the cosmic microwave background (CMB) anisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide Fisher forecasts for how the power spectrum and bispectrum from the Euclid spectroscopic survey will constrain the local, equilateral, and orthogonal shapes of primordial non-Gaussianity. We also review how Bayesian field-level inference of primordial non-Gaussianity can constrain local primordial non-Gaussianity. We show how Euclid, with its unique combination of the main probes, will provide the tightest constraints on low redshift to date. By targeting a markedly different range in redshift and scale, Euclid's expected uncertainties are complementary to those obtained by CMB primary anisotropy, returning the tightest combined constraints on the physics of the early Universe."
  },
  {
    "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction",
    "url": "http://arxiv.org/abs/2507.15803v1",
    "arxiv_id": "2507.15803v1",
    "authors": [
      "Danhui Chen",
      "Ziquan Liu",
      "Chuxi Yang",
      "Dan Wang",
      "Yan Yan",
      "Yi Xu",
      "Xiangyang Ji"
    ],
    "published": "2025-07-21T17:02:57+00:00",
    "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in."
  },
  {
    "title": "Deterministic Quantum Search via Recursive Oracle Expansion",
    "url": "http://arxiv.org/abs/2507.15797v1",
    "arxiv_id": "2507.15797v1",
    "authors": [
      "John Burke",
      "Ciaran McGoldrick"
    ],
    "published": "2025-07-21T16:57:15+00:00",
    "summary": "We introduce a novel deterministic quantum search algorithm that provides a practical alternative to conventional probabilistic search approaches. Our scheme eliminates the inherent uncertainty of quantum search without relying on arbitrary phase rotations, a key limitation of other deterministic methods. The algorithm achieves certainty by recursively expanding the base oracle so that it marks all states prefixed by the same two bits as the target, encompassing exactly one-quarter of the search space. This enables a step-by-step reduction of the superposition until the target state can be measured with certainty. The algorithm achieves deterministic success with a query complexity of $O(N^{\\log_2(3)/2}) \\approx O(N^{0.7925})$, falling between Grover's $O(\\sqrt{N})$ scaling and the classical $O(N)$. Our approach relies exclusively on two-qubit nearest-neighbour diffusion operators, avoiding global diffusion entirely. We show that, despite the increased query complexity, this design reduces the total number of two-qubit gates required for diffusion by more than an order of magnitude for search spaces up to at least 18 qubits, with even greater advantages on hardware with limited qubit connectivity. The scheme's inherent determinism, reliance on simple nearest-neighbour, low-depth operations, and scalable recursive structure make it well-suited for hardware implementation. Additionally, we show that the algorithm naturally supports partial database search, enabling deterministic identification of selected target bits without requiring a full search, further broadening its applicability."
  },
  {
    "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs",
    "url": "http://arxiv.org/abs/2507.15782v1",
    "arxiv_id": "2507.15782v1",
    "authors": [
      "Ruochu Yang",
      "Yu Zhou",
      "Fumin Zhang",
      "Mengxue Hou"
    ],
    "published": "2025-07-21T16:37:50+00:00",
    "summary": "Household robots have been a longstanding research topic, but they still lack human-like intelligence, particularly in manipulating open-set objects and navigating large environments efficiently and accurately. To push this boundary, we consider a generalized multi-object collection problem in large scene graphs, where the robot needs to pick up and place multiple objects across multiple locations in a long mission of multiple human commands. This problem is extremely challenging since it requires long-horizon planning in a vast action-state space under high uncertainties. To this end, we propose a novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a multimodal action cost similarity function, our algorithm can both reflect the history and look into the future to optimize plans, striking a good balance of quality and efficiency. Simulation experiments demonstrate that compared with latest works, our algorithm improves the overall mission performance by 30% in terms of fulfilling human commands, maximizing mission success rates, and minimizing mission costs."
  },
  {
    "title": "Learning Climate Sensitivity from Future Observations, Fast and Slow",
    "url": "http://arxiv.org/abs/2507.15767v1",
    "arxiv_id": "2507.15767v1",
    "authors": [
      "Adam Michael Bauer",
      "Cristian Proistosescu",
      "Kelvin K Droegemeier"
    ],
    "published": "2025-07-21T16:25:02+00:00",
    "summary": "Climate sensitivity has remained stubbornly uncertain since the Charney Report was published some 45 years ago. Two factors in future climate projections could alter this dilemma: (i) an increased ratio of CO$_2$ forcing relative to aerosol cooling, owing to both continued accumulation of CO$_2$ and declining aerosol emissions, and (ii) a warming world, whereby CO$_2$-induced warming becomes more pronounced relative to climate variability. Here, we develop a novel modeling approach to explore the rates of learning about equilibrium climate sensitivity and the transient climate response (TCR) and identify the physical drivers underpinning these learning rates. Our approach has the advantage over past work by accounting for the full spectrum of parameter uncertainties and covariances, while also taking into account serially correlated internal climate variability. Moreover, we provide a physical explanation of how quickly we may hope to learn about climate sensitivity. We find that, although we are able to constrain future TCR regardless of the true underlying value, constraining ECS is more difficult, with low values of ECS being more easily ascertained than high values. This asymmetry can be explained by most of the warming this century being attributable to the fast climate mode, which is more useful for constraining TCR than it is for ECS. We further show that our inability to constrain the deep ocean response is what limits our ability to learn high values of ECS."
  },
  {
    "title": "Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces",
    "url": "http://arxiv.org/abs/2507.15741v1",
    "arxiv_id": "2507.15741v1",
    "authors": [
      "G\u00e1bor Lugosi",
      "Marcos Matabuena"
    ],
    "published": "2025-07-21T15:54:13+00:00",
    "summary": "This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces. Leveraging a newly defined notion of homoscedasticity, we develop a conformal prediction algorithm that offers finite-sample coverage guarantees and fast convergence rates of the oracle estimator. In heteroscedastic settings, we forgo these non-asymptotic guarantees to gain statistical efficiency, proposing a local $k$--nearest--neighbor method without conformal calibration that is adaptive to the geometry of each particular nonlinear space. Both procedures work with any regression algorithm and are scalable to large data sets, allowing practitioners to plug in their preferred models and incorporate domain expertise. We prove consistency for the proposed estimators under minimal conditions. Finally, we demonstrate the practical utility of our approach in personalized--medicine applications involving random response objects such as probability distributions and graph Laplacians."
  },
  {
    "title": "Symplectic coherence: a measure of position-momentum correlations in quantum states",
    "url": "http://arxiv.org/abs/2507.15738v1",
    "arxiv_id": "2507.15738v1",
    "authors": [
      "Varun Upreti",
      "Ulysse Chabaud"
    ],
    "published": "2025-07-21T15:48:53+00:00",
    "summary": "The interdependence of position and momentum, as highlighted by the Heisenberg uncertainty principle, is a cornerstone of quantum physics. Yet, position-momentum correlations have received little systematic attention. Motivated by recent developments in bosonic quantum physics that underscore their relevance in quantum thermodynamics, metrology, and computing, we establish a general framework to study and quantify position-momentum correlations in quantum states. We introduce symplectic coherence, a faithful and easily computable measure defined as the Frobenius norm of the block of the covariance matrix encoding position-momentum correlations, and demonstrate that symplectic coherence is monotone under relevant operations and robust under small perturbations. Furthermore, using a recent mapping by Barthe et al. (Phys. Rev. Lett. 134, 070604) which relates the covariance matrix of a bosonic state to the density matrix of a finite-dimensional system, we show that position-momentum correlations correspond to beyond-classical correlations in a virtual finite-dimensional quantum state, with symplectic coherence mapping naturally to geometric quantum discord. Taking energy constraints into account, we determine the maximal position-momentum correlations achievable at fixed energy, revealing structural insights about the corresponding optimal states. Finally, we illustrate the operational relevance of symplectic coherence through several examples in quantum information tasks and quantum thermodynamics. In the process, we establish new technical results on matrix norms and quantum covariance matrices, and demonstrate the conceptual significance of viewing covariance matrices as density matrices of virtual quantum states."
  },
  {
    "title": "Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems",
    "url": "http://arxiv.org/abs/2507.15727v1",
    "arxiv_id": "2507.15727v1",
    "authors": [
      "Xuchuang Wang",
      "Bo Sun",
      "Hedyeh Beyhaghi",
      "John C. S. Lui",
      "Mohammad Hajiesmaili",
      "Adam Wierman"
    ],
    "published": "2025-07-21T15:36:34+00:00",
    "summary": "This paper introduces a novel multi-agent ski-rental problem that generalizes the classical ski-rental dilemma to a group setting where agents incur individual and shared costs. In our model, each agent can either rent at a fixed daily cost, or purchase a pass at an individual cost, with an additional third option of a discounted group pass available to all. We consider scenarios in which agents' active days differ, leading to dynamic states as agents drop out of the decision process. To address this problem from different perspectives, we define three distinct competitive ratios: overall, state-dependent, and individual rational. For each objective, we design and analyze optimal deterministic and randomized policies. Our deterministic policies employ state-aware threshold functions that adapt to the dynamic states, while our randomized policies sample and resample thresholds from tailored state-aware distributions. The analysis reveals that symmetric policies, in which all agents use the same threshold, outperform asymmetric ones. Our results provide competitive ratio upper and lower bounds and extend classical ski-rental insights to multi-agent settings, highlighting both theoretical and practical implications for group decision-making under uncertainty."
  },
  {
    "title": "Evaluation of hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles using electrochemical permeation and thermal desorption spectroscopy",
    "url": "http://arxiv.org/abs/2507.15711v1",
    "arxiv_id": "2507.15711v1",
    "authors": [
      "Nicholas Winzer"
    ],
    "published": "2025-07-21T15:20:15+00:00",
    "summary": "Hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles was investigated using electrochemical permeation (EP) and thermal desorption spectroscopy (TDS). The trapping parameters for the test materials were evaluated by fitting the measurements with a finite element model based on the McNabb-Foster equations using least-squares optimisation. The measurements showed that hydrogen diffusion in ferrite is slowed significantly by the presence of fine (<5 nm) (Ti,Cr)C particles; coarser particles had little or no effect. The TDS measurements were consistent with hydrogen traps with a high energy barrier. The uniqueness of the hydrogen trapping parameters obtained using the fitting procedure was evaluated. It was found that the system was overdetermined; the measurements could be fitted with multiple combinations of trapping parameters. Consequently, it was not possible to determine the individual trapping parameters using this procedure. Trapping parameters were also evaluated from TDS measurements by applying Kissinger's equation. Using this procedure a trap binding energy of 0.24 eV was calculated for all materials, albeit with a high degree of uncertainty."
  },
  {
    "title": "Ubiquity of Uncertainty in Neuron Systems",
    "url": "http://arxiv.org/abs/2507.15702v1",
    "arxiv_id": "2507.15702v1",
    "authors": [
      "Brandon B. Le",
      "Bennett Lamb",
      "Luke Benfer",
      "Sriharsha Sambangi",
      "Nisal Geemal Vismith",
      "Akshaj Jagarapu"
    ],
    "published": "2025-07-21T15:11:23+00:00",
    "summary": "We demonstrate that final-state uncertainty is ubiquitous in multistable systems of coupled neuronal maps, meaning that predicting whether one such system will eventually be chaotic or nonchaotic is often nearly impossible. We propose a \"chance synchronization\" mechanism that governs the emergence of unpredictability in neuron systems and support it by using basin classification, uncertainty exponent, and basin entropy techniques to analyze five simple discrete-time systems, each consisting of a different neuron model. Our results illustrate that uncertainty in neuron systems is not just a product of noise or high-dimensional complexity; it is also a fundamental property of low-dimensional, deterministic models, which has profound implications for understanding brain function, modeling cognition, and interpreting unpredictability in general multistable systems."
  },
  {
    "title": "Missing baryons recovered: a measurement of the gas fraction in galaxies and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing",
    "url": "http://arxiv.org/abs/2507.14136v1",
    "arxiv_id": "2507.14136v1",
    "authors": [
      "Boryana Hadzhiyska",
      "Simone Ferraro",
      "Gerrit S. Farren",
      "Noah Sailer",
      "Rongpu Zhou"
    ],
    "published": "2025-07-18T17:59:24+00:00",
    "summary": "We present new constraints on the halo masses and matter density profiles of DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs) and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing convergence map from ACT DR6. This provides an independent, lensing-based calibration of halo masses, complementary to methods relying on clustering or dynamics. We derive constraints on the mean halo mass for three DESI-selected samples, finding $\\log(M_{\\rm halo}/(M_\\odot/h)) \\approx 13.18$, 13.03 and 13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a halo model approach, we also compare the projected galaxy-matter density profiles with previously reported gas profiles inferred from measurements of the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the key uncertainties in interpreting kSZ signals -- the unknown host halo mass distribution -- by providing an independent and consistent mass calibration. The agreement between the gas and total mass profiles at large aperture suggests that sufficiently far from the group center (2--3 virial radii), we recover all the baryons, offering a resolution to the 'missing baryon' problem. We further study the cumulative gas fractions for all galaxies as well as for the most massive galaxy groups in the sample ($\\log(M_{\\rm halo}/(M_\\odot/h)) \\approx 13.5$), finding values that are physically sensible and in agreement with previous findings using kSZ and X-ray data: compared to the TNG300 simulation, the observed gas fractions are systematically lower at fixed radius by $\\gtrsim$4$\\sigma$, providing compelling, independent evidence for stronger baryonic feedback in the real Universe. These findings highlight the power of combining CMB lensing with galaxy surveys to probe the interplay between baryons and dark matter in group-sized halos."
  },
  {
    "title": "On the relation between perspective-neutral, algebraic, and effective quantum reference frames",
    "url": "http://arxiv.org/abs/2507.14131v1",
    "arxiv_id": "2507.14131v1",
    "authors": [
      "Philipp A. Hoehn",
      "Julian De Vuyst",
      "Artur Tsobanjan"
    ],
    "published": "2025-07-18T17:58:04+00:00",
    "summary": "The framework of internal quantum reference frames (QRFs) constitutes a universal toolset for dealing with symmetries in quantum theory and has led to new revelations in quantum gravity, gauge theories and foundational physics. Multiple approaches have emerged, sometimes differing in scope and the way symmetries are implemented, raising the question as to their relation. Here, we investigate the relation between three approaches to QRFs for gauge symmetries, namely the effective semiclassical, algebraic, and perspective-neutral (PN) approaches. Rather than constructing Hilbert spaces, as the PN approach, the effective approach is based on a quantum phase space parametrized by expectation values and fluctuations, while the emphasis of the algebraic approach is on the state space of complex linear functionals on a kinematical algebra. Nevertheless, external frame information is treated as gauge in all three formalisms, manifested in constraints on states and algebra. We show that these three approaches are, in fact, equivalent for ideal QRFs, distinguished by sharp orientations, which is the previous setting of the first two approaches. Our demonstration pertains to single constraints, including relativistic ones, and encompasses QRF changes. In particular, the QRF transformations of the PN framework agree semiclassically with those of the older effective approach, by which it was inspired. As a physical application, we explore the QRF covariance of uncertainties and fluctuations, which turn out to be frame-dependent. This is particularly well-suited for the effective and algebraic approaches, for which these quantities form a natural basis. Finally, we pave the way towards extending these two approaches to non-ideal QRFs by studying the projection and gauge-fixing operations of the Page-Wootters formalism, built into the PN framework, on algebraic states."
  },
  {
    "title": "Integrating Forecasting Models Within Steady-State Analysis and Optimization",
    "url": "http://arxiv.org/abs/2507.14117v1",
    "arxiv_id": "2507.14117v1",
    "authors": [
      "Aayushya Agarwal",
      "Larry Pileggi"
    ],
    "published": "2025-07-18T17:48:04+00:00",
    "summary": "Extreme weather variations and the increasing unpredictability of load behavior make it difficult to determine power grid dispatches that are robust to uncertainties. While machine learning (ML) methods have improved the ability to model uncertainty caused by loads and renewables, accurately integrating these forecasts and their sensitivities into steady-state analyses and decision-making strategies remains an open challenge. Toward this goal, we present a generalized methodology that seamlessly embeds ML-based forecasting engines within physics-based power flow and grid optimization tools. By coupling physics-based grid modeling with black-box ML methods, we accurately capture the behavior and sensitivity of loads and weather events by directly integrating the inputs and outputs of trained ML forecasting models into the numerical methods of power flow and grid optimization. Without fitting surrogate load models, our approach obtains the sensitivities directly from data to accurately predict the response of forecasted devices to changes in the grid. Our approach combines the sensitivities of forecasted devices attained via backpropagation and the sensitivities of physics-defined grid devices. We demonstrate the efficacy of our method by showcasing improvements in sensitivity calculations and leveraging them to design a robust power dispatch that improves grid reliability under stochastic weather events. Our approach enables the computation of system sensitivities to exogenous factors which supports broader analyses that improve grid reliability in the presence of load variability and extreme weather conditions."
  },
  {
    "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
    "url": "http://arxiv.org/abs/2507.14102v1",
    "arxiv_id": "2507.14102v1",
    "authors": [
      "Shravan Venkatraman",
      "Pavan Kumar S",
      "Rakesh Raj Madavan",
      "Chandrakala S"
    ],
    "published": "2025-07-18T17:30:56+00:00",
    "summary": "Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL"
  },
  {
    "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation",
    "url": "http://arxiv.org/abs/2507.14099v1",
    "arxiv_id": "2507.14099v1",
    "authors": [
      "Markus Buchholz",
      "Ignacio Carlucho",
      "Michele Grimaldi",
      "Maria Koskinopoulou",
      "Yvan R. Petillot"
    ],
    "published": "2025-07-18T17:25:54+00:00",
    "summary": "Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges."
  },
  {
    "title": "Global Bayesian Analysis of $\\mathrm{J}/\u03c8$ Photoproduction on Proton and Lead Targets",
    "url": "http://arxiv.org/abs/2507.14087v1",
    "arxiv_id": "2507.14087v1",
    "authors": [
      "Heikki M\u00e4ntysaari",
      "Hendrik Roch",
      "Farid Salazar",
      "Bj\u00f6rn Schenke",
      "Chun Shen",
      "Wenbin Zhao"
    ],
    "published": "2025-07-18T17:12:19+00:00",
    "summary": "We perform a global Bayesian analysis of diffractive $\\mathrm{J}/\\psi$ production in $\\gamma+p$ and $\\gamma+\\mathrm{Pb}$ collisions using a color glass condensate (CGC) based calculation framework. As past calculations have shown that CGC-based models typically overpredict the $\\mathrm{J}/\\psi$ production in $\\gamma+\\mathrm{Pb}$ collisions at high center of mass energy, we address the question of whether it is possible to describe coherent and incoherent diffractive $\\mathrm{J}/\\psi$ data from $\\gamma+p$ collisions at HERA and the LHC, and from $\\gamma+\\mathrm{Pb}$ collisions at the LHC simultaneously. Our results indicate that a simultaneous description of $\\gamma+p$ and $\\gamma+\\mathrm{Pb}$ data is challenging, with results improving when an overall $K$-factor -- scaling $\\gamma+p$ and $\\gamma+\\mathrm{Pb}$ cross sections to absorb model uncertainties -- is introduced."
  },
  {
    "title": "Direct Measurement of the Accretion Disk Formed in Prompt Collapse Mergers with Future Gravitational-Wave Observatories",
    "url": "http://arxiv.org/abs/2507.14071v1",
    "arxiv_id": "2507.14071v1",
    "authors": [
      "Arnab Dhani",
      "Alessandro Camilletti",
      "Alessio Ludovico De Santis",
      "Andrea Cozzumbo",
      "David Radice",
      "Domenico Logoteta",
      "Albino Perego",
      "Jan Harms",
      "Marica Branchesi"
    ],
    "published": "2025-07-18T16:49:25+00:00",
    "summary": "The production site of heavy r-process elements, such as Gold and Uranium, is uncertain. Neutron star mergers are the only astrophysical phenomenon in which we have witnessed their formation. However, the amount of heavy elements resulting from the merger remains poorly constrained, mainly due to uncertainties on the mass and angular momentum of the disk formed in the merger remnant. Matter accretion from the disk is also thought to power gamma ray-bursts. We discover from numerical relativity simulations that the accretion disk influences the ringdown gravitational-wave signal produced by binaries that promptly collapse to black-hole at merger. We propose a method to \\emph{directly} measure the mass of the accretion disk left during black hole formation in binary mergers using observatories such as the Einstein Telescope or Cosmic Explorer with a relative error of 10\\% for binaries at a distance of up to 30~Mpc, corresponding to an event rate of 0.001 to 0.25 events per year."
  },
  {
    "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
    "url": "http://arxiv.org/abs/2507.14067v1",
    "arxiv_id": "2507.14067v1",
    "authors": [
      "Shuliang Liu",
      "Qi Zheng",
      "Jesse Jiaxi Xu",
      "Yibo Yan",
      "He Geng",
      "Aiwei Liu",
      "Peijie Jiang",
      "Jia Liu",
      "Yik-Cheung Tam",
      "Xuming Hu"
    ],
    "published": "2025-07-18T16:44:41+00:00",
    "summary": "Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking"
  },
  {
    "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
    "url": "http://arxiv.org/abs/2507.14056v1",
    "arxiv_id": "2507.14056v1",
    "authors": [
      "Alejandro Rodriguez-Garcia",
      "Anindya Ghosh",
      "Srikanth Ramaswamy"
    ],
    "published": "2025-07-18T16:34:06+00:00",
    "summary": "Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks."
  },
  {
    "title": "Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors",
    "url": "http://arxiv.org/abs/2507.14034v1",
    "arxiv_id": "2507.14034v1",
    "authors": [
      "Jochen Wulf",
      "Jurg Meierhofer",
      "Frank Hannich"
    ],
    "published": "2025-07-18T16:06:03+00:00",
    "summary": "Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems."
  },
  {
    "title": "$\\texttt{raccoon}$: A Python package for removing wiggle artifacts in the JWST NIRSpec integral field spectroscopy",
    "url": "http://arxiv.org/abs/2507.13341v1",
    "arxiv_id": "2507.13341v1",
    "authors": [
      "Anowar J. Shajib"
    ],
    "published": "2025-07-17T17:58:38+00:00",
    "summary": "$\\texttt{raccoon}$ is a Python package for removing resampling noise - commonly referred to as \"wiggles'' - from spaxel-level spectra in datacubes obtained from the JWST Near Infrared Spectrograph's (NIRSpec) integral field spectroscopy (IFS) mode. These wiggles arise as artifacts during resampling of the 2D raw data into 3D datacubes, due to the point spread function (PSF) being undersampled. The standard JWST data reduction pipeline does not correct for this noise. The wiggle artifacts can significantly degrade the scientific usability of the data, particularly at the spaxel level, undermining the exquisite spatial resolution of NIRSpec. $\\texttt{raccoon}$ provides an empirical correction by modeling and removing these artifacts, thereby restoring the fidelity of the extracted spectra. $\\texttt{raccoon}$ forward-models the wiggles as a chirp function impacting one or more template spectra that are directly fit to the original data across the entire wavelength range. The best-fit wiggle model is then used to clean the data while propagating the associated uncertainties."
  },
  {
    "title": "A Framework for Waterfall Pricing Using Simulation-Based Uncertainty Modeling",
    "url": "http://arxiv.org/abs/2507.13324v1",
    "arxiv_id": "2507.13324v1",
    "authors": [
      "Nicola Jean",
      "Giacomo Le Pera",
      "Lorenzo Giada",
      "Claudio Nordio"
    ],
    "published": "2025-07-17T17:43:30+00:00",
    "summary": "We present a novel framework for pricing waterfall structures by simulating the uncertainty of the cashflow generated by the underlying assets in terms of value, time, and confidence levels. Our approach incorporates various probability distributions calibrated on the market price of the tranches at inception. The framework is fully implemented in PyTorch, leveraging its computational efficiency and automatic differentiation capabilities through Adjoint Algorithmic Differentiation (AAD). This enables efficient gradient computation for risk sensitivity analysis and optimization. The proposed methodology provides a flexible and scalable solution for pricing complex structured finance instruments under uncertainty"
  },
  {
    "title": "Testing halo models for constraining astrophysical feedback with multi-probe modeling: I. 3D Power spectra and mass fractions",
    "url": "http://arxiv.org/abs/2507.13317v1",
    "arxiv_id": "2507.13317v1",
    "authors": [
      "Pranjal R. S.",
      "Shivam Pandey",
      "Dhayaa Anbajagane",
      "Elisabeth Krause",
      "Klaus Dolag"
    ],
    "published": "2025-07-17T17:36:13+00:00",
    "summary": "Upcoming Stage-IV surveys will deliver measurements of distribution of matter with unprecedented precision, demanding highly accurate theoretical models for cosmological parameter inference. A major source of modeling uncertainty lies in astrophysical processes associated with galaxy formation and evolution, which remain poorly understood. Probes such as the thermal and kinematic Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio bursts offer a promising avenue for mapping the distribution and thermal properties of cosmic baryons. A unified analytical framework capable of jointly modeling these observables is essential for fully harnessing the complementary information while mitigating probe-specific systematics. In this work, we present a detailed assessment of existing analytical models, which differ in their assumptions and prescriptions for simultaneously describing the distribution of matter and baryons in the universe. Using the Magneticum hydrodynamical simulation, we test these models by jointly analyzing the 3D auto- and cross-power spectra of the matter and baryonic fields that underpin the above probes. We find that all models can reproduce the power spectra at sub-percent to few-percent accuracy, depending on the tracer combination and number of free parameters. Their ability to recover underlying halo properties, such as the evolution of gas abundance and thermodynamic profiles with halo mass, varies considerably. Our results suggest that these models require further refinement and testing for reliable interpretation of multi-wavelength datasets."
  },
  {
    "title": "Systematic study of the validity of the eikonal model including uncertainties",
    "url": "http://arxiv.org/abs/2507.13291v1",
    "arxiv_id": "2507.13291v1",
    "authors": [
      "Daniel Shiu",
      "Chlo\u00eb Hebborn",
      "Filomena M. Nunes"
    ],
    "published": "2025-07-17T16:57:12+00:00",
    "summary": "Nuclear reactions at intermediate beam energies are often interpreted using the eikonal model. In the analysis of complex reaction probes, where few-body reaction methods are needed, the eikonal method may be used as an efficient way for describing the fragment-target reaction process. In this work, we perform a systematic study to test the validity of the eikonal approximation for nucleon-nucleus reactions. We also quantify uncertainties due to the nucleon optical potential on reaction observables. We inspect the validity of the eikonal model and its semiclassical correction by comparing it to exact solutions (obtained from solving the optical model equation with a finite differences method) for a wide range of reactions. We also study the effect of relativistic corrections, both kinematic and dynamic, by effectively incorporating the relativistic effects at intermediate energies. The uncertainties from a Bayesian global optical potential (KDUQ) are propagated to the observables of interest. Our study includes neutron and proton reactions on $^{27}$Al, $^{40}$Ca, $^{90}$Zr and $^{208}$Pb, for a wide range of energies $E_{lab}=0-400$ MeV. Our results show that for the proton absorption cross section, the eikonal model can be used down to around $60$ MeV and the semiclassical correction extends its use to $30$ MeV. However, the validity of the eikonal model for the neutron total cross section only goes down to $\\approx120$ MeV, a range extended to $\\approx 50$ MeV when using the semiclassical correction. We find the semi-classical correction to the eikonal model to be less effective in describing the angular distributions. The $1\\sigma$ uncertainty intervals on the observables we studied is less than $5$% for most of the energies considered, but increases rapidly for higher energies, namely energies outside the range of KDUQ ($E_{lab}>200$ MeV)."
  },
  {
    "title": "Multi-wavelength study of the high Galactic latitude supernova remnant candidate \\snr\\ associated with the Calvera pulsar",
    "url": "http://arxiv.org/abs/2507.13210v1",
    "arxiv_id": "2507.13210v1",
    "authors": [
      "Emanuele Greco",
      "Michela Rigoselli",
      "Sandro Mereghetti",
      "Fabrizio Bocchino",
      "Marco Miceli",
      "Vincenzo Sapienza",
      "Salvatore Orlando"
    ],
    "published": "2025-07-17T15:18:48+00:00",
    "summary": "The candidate supernova remnant (SNR) G118.4+37.0 (Calvera's SNR), discovered as a faint radio ring at high Galactic latitude and coincident with extended Fermi/LAT gamma-ray emission, is likely associated to the X-ray pulsar 1RXS J141256.0+792204 (Calvera). Previous XMM-Newton data hinted at soft diffuse X-ray emission inside the ring but lacked sufficient exposure for detailed characterisation. We obtained new XMM-Newton observations, and produced count-rate images, equivalent width and median photon energy maps to identify optimal regions for spectral analysis. We complemented these observations with a reanalysis of Fermi/LAT gamma-ray data and new Telescopio Nazionale Galileo observations aimed to search for Halpha emission. The X-ray diffuse emission is well described by a model of shock-heated plasma with temperature kT \\sim 0.15 keV, mildly under-solar N and o abundances and densities ne=0.1-0.7 cm-3. According to our estimates, Calvera's SNR is 10-20 kya old and lies at a distance of 4-5 kpc. A distinti \"Clump\" region shows hared emission equally well described by a thermal (kT\\sim 1.7 keV) or a non thermal model (Gamma \\sim 2.7). The brightest X-ray area is close to the gamma-ray peak and to an isolated Alpha filament. G118.4+37.0 is a middle-aged remnant which expands in a tenuous medium and encountered a denser phase, likely the relic of the wind activity of the massive progenitor star. The estimated SNR distance is consistent within the uncertainties with that estimated for Calvera, confirming that this peculiar pulsar was born in the explosion of a massive star high above the Galactic disk. Our measured ambient density, together with the patchy morphology of the gamma-ray emission and the detection of Halpha filaments indicates that a hadronic origin is compatible with the gamma-ray flux, though a mixed leptonic-hadronic cannot be excluded"
  },
  {
    "title": "Performance Portable Gradient Computations Using Source Transformation",
    "url": "http://arxiv.org/abs/2507.13204v1",
    "arxiv_id": "2507.13204v1",
    "authors": [
      "Kim Liegeois",
      "Brian Kelley",
      "Eric Phipps",
      "Sivasankaran Rajamanickam",
      "Vassil Vassilev"
    ],
    "published": "2025-07-17T15:15:25+00:00",
    "summary": "Derivative computation is a key component of optimization, sensitivity analysis, uncertainty quantification, and nonlinear solvers. Automatic differentiation (AD) is a powerful technique for evaluating such derivatives, and in recent years, has been integrated into programming environments such as Jax, PyTorch, and TensorFlow to support derivative computations needed for training of machine learning models, resulting in widespread use of these technologies. The C++ language has become the de facto standard for scientific computing due to numerous factors, yet language complexity has made the adoption of AD technologies for C++ difficult, hampering the incorporation of powerful differentiable programming approaches into C++ scientific simulations. This is exacerbated by the increasing emergence of architectures such as GPUs, which have limited memory capabilities and require massive thread-level concurrency. Portable scientific codes rely on domain specific programming models such as Kokkos making AD for such codes even more complex. In this paper, we will investigate source transformation-based automatic differentiation using Clad to automatically generate portable and efficient gradient computations of Kokkos-based code. We discuss the modifications of Clad required to differentiate Kokkos abstractions. We will illustrate the feasibility of our proposed strategy by comparing the wall-clock time of the generated gradient code with the wall-clock time of the input function on different cutting edge GPU architectures such as NVIDIA H100, AMD MI250x, and Intel Ponte Vecchio GPU. For these three architectures and for the considered example, evaluating up to 10 000 entries of the gradient only took up to 2.17x the wall-clock time of evaluating the input function."
  },
  {
    "title": "Medium-induced modification of azimuthal correlations of electrons from heavy-flavor hadron decays with charged particles in Pb-Pb collisions at $\\sqrt{s_{\\rm{NN}} = 5.02}$ TeV",
    "url": "http://arxiv.org/abs/2507.13197v1",
    "arxiv_id": "2507.13197v1",
    "authors": [
      "ALICE Collaboration"
    ],
    "published": "2025-07-17T15:08:05+00:00",
    "summary": "The azimuthal-correlation distributions between electrons from the decays of heavy-flavor hadrons and associated charged particles in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV are reported for the 0-10% and 30-50% centrality classes. This is the first measurement to provide access to the azimuthal-correlation observables in the heavy-flavor sector in Pb-Pb collisions. The analysis is performed for trigger electrons from heavy-flavor hadron decays with transverse momentum $4 < p_{\\rm T}^{\\rm e} < 16$ GeV/$c$, considering associated particles within the transverse-momentum range $1 < p_{\\rm T}^{\\rm assoc} < 7$ GeV/$c$, and a pseudorapidity difference of $|\\Delta\\eta| < 1$ between the trigger electron and associated particles. The per-trigger nuclear modification factor ($I_{\\rm AA}$) is calculated to compare the near- and away-side peak yields to those in pp collisions at $\\sqrt{s} = 5.02$ TeV. In 0-10% central collisions, the $I_{\\rm AA}$ indicates a hint of enhancement of associated-particle yields with $p_{\\rm T} < 3$ GeV/$c$ on the near side, and a suppression of yields with $p_{\\rm T} > 4$ GeV/$c$ on the away side. The $I_{\\rm AA}$ for electron triggers from heavy-flavor hadron decays is compared with that for light-flavor and strange-particle triggers to investigate the dependence on different fragmentation processes and parton-medium dynamics, and is found to be the same within uncertainties."
  },
  {
    "title": "$\\overline\u03a3^{\\pm}$ production in pp and p-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV with ALICE",
    "url": "http://arxiv.org/abs/2507.13183v1",
    "arxiv_id": "2507.13183v1",
    "authors": [
      "ALICE Collaboration"
    ],
    "published": "2025-07-17T14:53:23+00:00",
    "summary": "The transverse momentum spectra and integrated yields of $\\overline{\\Sigma}^{\\pm}$ have been measured in pp and p-Pb collisions at $\\sqrt{s_{\\mathrm{NN}}} = 5.02$ TeV with the ALICE experiment. Measurements are performed via the newly accessed decay channel $\\overline{\\Sigma}^{\\pm} \\rightarrow {\\rm\\overline{n}}\\pi^{\\pm}$. A new method of antineutron reconstruction with the PHOS electromagnetic spectrometer is developed and applied to this analysis. The $p_{\\rm T}$ spectra of $\\overline{\\Sigma}^{\\pm}$ are measured in the range $0.5 < p_{\\rm T} < 3$ GeV/$c$ and compared to predictions of the PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4 models. The EPOS LHC and EPOS4 models provide the best descriptions of the measured spectra both in pp and p-Pb collisions, while models which do not account for multiparton interactions provide a considerably worse description at high $p_{\\rm T}$. The total yields of $\\overline{\\Sigma}^{\\pm}$ in both pp and p-Pb collisions are compared to predictions of the Thermal-FIST model and dynamical models PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4. All models reproduce the total yields in both colliding systems within uncertainties. The nuclear modification factors $R_{\\rm pPb}$ for both $\\overline{\\Sigma}^{+}$ and $\\overline{\\Sigma}^{-}$ are evaluated and compared to those of protons, $\\Lambda$ and $\\Xi$ hyperons, and predictions of EPOS LHC and EPOS4 models. No deviations of $R_{\\rm pPb}$ for $\\overline{\\Sigma}^{\\pm}$ from the model predictions or measurements for other hadrons are found within uncertainties."
  },
  {
    "title": "The Time-Energy Principle in Algebraic Geometry",
    "url": "http://arxiv.org/abs/2507.13134v1",
    "arxiv_id": "2507.13134v1",
    "authors": [
      "Renaud Gauthier"
    ],
    "published": "2025-07-17T13:55:00+00:00",
    "summary": "We consider the time-energy uncertainty principle from Quantum Mechanics and provide its Algebro-Geometric interpretation within the context of stacks."
  },
  {
    "title": "Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces",
    "url": "http://arxiv.org/abs/2507.13092v1",
    "arxiv_id": "2507.13092v1",
    "authors": [
      "Hyo-Jeong Jang",
      "Hye-Bin Shin",
      "Seong-Whan Lee"
    ],
    "published": "2025-07-17T13:03:20+00:00",
    "summary": "Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications."
  },
  {
    "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis",
    "url": "http://arxiv.org/abs/2507.12461v1",
    "arxiv_id": "2507.12461v1",
    "authors": [
      "Trong-Thang Pham",
      "Anh Nguyen",
      "Zhigang Deng",
      "Carol C. Wu",
      "Hien Van Nguyen",
      "Ngan Le"
    ],
    "published": "2025-07-16T17:58:35+00:00",
    "summary": "Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets."
  },
  {
    "title": "Precision measurement of the ${\\it\u039e}_b^0$ baryon lifetime",
    "url": "http://arxiv.org/abs/2507.12402v1",
    "arxiv_id": "2507.12402v1",
    "authors": [
      "LHCb collaboration",
      "R. Aaij",
      "A. S. W. Abdelmotteleb",
      "C. Abellan Beteta",
      "F. Abudin\u00e9n",
      "T. Ackernley",
      "A. A. Adefisoye",
      "B. Adeva",
      "M. Adinolfi",
      "P. Adlarson",
      "C. Agapopoulou",
      "C. A. Aidala",
      "Z. Ajaltouni",
      "S. Akar",
      "K. Akiba",
      "P. Albicocco",
      "J. Albrecht",
      "R. Aleksiejunas",
      "F. Alessio",
      "P. Alvarez Cartelle",
      "R. Amalric",
      "S. Amato",
      "J. L. Amey",
      "Y. Amhis",
      "L. An",
      "L. Anderlini",
      "M. Andersson",
      "P. Andreola",
      "M. Andreotti",
      "S. Andres Estrada",
      "A. Anelli",
      "D. Ao",
      "F. Archilli",
      "Z Areg",
      "M. Argenton",
      "S. Arguedas Cuendis",
      "A. Artamonov",
      "M. Artuso",
      "E. Aslanides",
      "R. Ata\u00edde Da Silva",
      "M. Atzeni",
      "B. Audurier",
      "J. A. Authier",
      "D. Bacher",
      "I. Bachiller Perea",
      "S. Bachmann",
      "M. Bachmayer",
      "J. J. Back",
      "P. Baladron Rodriguez",
      "V. Balagura",
      "A. Balboni",
      "W. Baldini",
      "Z. Baldwin",
      "L. Balzani",
      "H. Bao",
      "J. Baptista de Souza Leite",
      "C. Barbero Pretel",
      "M. Barbetti",
      "I. R. Barbosa",
      "R. J. Barlow",
      "M. Barnyakov",
      "S. Barsuk",
      "W. Barter",
      "J. Bartz",
      "S. Bashir",
      "B. Batsukh",
      "P. B. Battista",
      "A. Bay",
      "A. Beck",
      "M. Becker",
      "F. Bedeschi",
      "I. B. Bediaga",
      "N. A. Behling",
      "S. Belin",
      "A. Bellavista",
      "K. Belous",
      "I. Belov",
      "I. Belyaev",
      "G. Benane",
      "G. Bencivenni",
      "E. Ben-Haim",
      "A. Berezhnoy",
      "R. Bernet",
      "S. Bernet Andres",
      "A. Bertolin",
      "C. Betancourt",
      "F. Betti",
      "J. Bex",
      "Ia. Bezshyiko",
      "O. Bezshyyko",
      "J. Bhom",
      "M. S. Bieker",
      "N. V. Biesuz",
      "P. Billoir",
      "A. Biolchini",
      "M. Birch",
      "F. C. R. Bishop",
      "A. Bitadze",
      "A. Bizzeti",
      "T. Blake",
      "F. Blanc",
      "J. E. Blank",
      "S. Blusk",
      "V. Bocharnikov",
      "J. A. Boelhauve",
      "O. Boente Garcia",
      "T. Boettcher",
      "A. Bohare",
      "A. Boldyrev",
      "C. S. Bolognani",
      "R. Bolzonella",
      "R. B. Bonacci",
      "N. Bondar",
      "A. Bordelius",
      "F. Borgato",
      "S. Borghi",
      "M. Borsato",
      "J. T. Borsuk",
      "E. Bottalico",
      "S. A. Bouchiba",
      "M. Bovill",
      "T. J. V. Bowcock",
      "A. Boyer",
      "C. Bozzi",
      "J. D. Brandenburg",
      "A. Brea Rodriguez",
      "N. Breer",
      "J. Brodzicka",
      "A. Brossa Gonzalo",
      "J. Brown",
      "D. Brundu",
      "E. Buchanan",
      "M. Burgos Marcos",
      "A. T. Burke",
      "C. Burr",
      "J. S. Butter",
      "J. Buytaert",
      "W. Byczynski",
      "S. Cadeddu",
      "H. Cai",
      "Y. Cai",
      "A. Caillet",
      "R. Calabrese",
      "S. Calderon Ramirez",
      "L. Calefice",
      "S. Cali",
      "M. Calvi",
      "M. Calvo Gomez",
      "P. Camargo Magalhaes",
      "J. I. Cambon Bouzas",
      "P. Campana",
      "D. H. Campora Perez",
      "A. F. Campoverde Quezada",
      "S. Capelli",
      "M. Caporale",
      "L. Capriotti",
      "R. Caravaca-Mora",
      "A. Carbone",
      "L. Carcedo Salgado",
      "R. Cardinale",
      "A. Cardini",
      "P. Carniti",
      "L. Carus",
      "A. Casais Vidal",
      "R. Caspary",
      "G. Casse",
      "M. Cattaneo",
      "G. Cavallero",
      "V. Cavallini",
      "S. Celani",
      "I. Celestino",
      "S. Cesare",
      "F. Cesario Laterza Lopes",
      "A. J. Chadwick",
      "I. Chahrour",
      "H. Chang",
      "M. Charles",
      "Ph. Charpentier",
      "E. Chatzianagnostou",
      "R. Cheaib",
      "M. Chefdeville",
      "C. Chen",
      "J. Chen",
      "S. Chen",
      "Z. Chen",
      "M. Cherif",
      "A. Chernov",
      "S. Chernyshenko",
      "X. Chiotopoulos",
      "V. Chobanova",
      "M. Chrzaszcz",
      "A. Chubykin",
      "V. Chulikov",
      "P. Ciambrone",
      "X. Cid Vidal",
      "G. Ciezarek",
      "P. Cifra",
      "P. E. L. Clarke",
      "M. Clemencic",
      "H. V. Cliff",
      "J. Closier",
      "C. Cocha Toapaxi",
      "V. Coco",
      "J. Cogan",
      "E. Cogneras",
      "L. Cojocariu",
      "S. Collaviti",
      "P. Collins",
      "T. Colombo",
      "M. Colonna",
      "A. Comerma-Montells",
      "L. Congedo",
      "J. Connaughton",
      "A. Contu",
      "N. Cooke",
      "G. Cordova",
      "C. Coronel",
      "I. Corredoira",
      "A. Correia",
      "G. Corti",
      "J. Cottee Meldrum",
      "B. Couturier",
      "D. C. Craik",
      "M. Cruz Torres",
      "E. Curras Rivera",
      "R. Currie",
      "C. L. Da Silva",
      "S. Dadabaev",
      "L. Dai",
      "X. Dai",
      "E. Dall'Occo",
      "J. Dalseno",
      "C. D'Ambrosio",
      "J. Daniel",
      "P. d'Argent",
      "G. Darze",
      "A. Davidson",
      "J. E. Davies",
      "O. De Aguiar Francisco",
      "C. De Angelis",
      "F. De Benedetti",
      "J. de Boer",
      "K. De Bruyn",
      "S. De Capua",
      "M. De Cian",
      "U. De Freitas Carneiro Da Graca",
      "E. De Lucia",
      "J. M. De Miranda",
      "L. De Paula",
      "M. De Serio",
      "P. De Simone",
      "F. De Vellis",
      "J. A. de Vries",
      "F. Debernardis",
      "D. Decamp",
      "S. Dekkers",
      "L. Del Buono",
      "B. Delaney",
      "H. -P. Dembinski",
      "J. Deng",
      "V. Denysenko",
      "O. Deschamps",
      "F. Dettori",
      "B. Dey",
      "P. Di Nezza",
      "I. Diachkov",
      "S. Didenko",
      "S. Ding",
      "Y. Ding",
      "L. Dittmann",
      "V. Dobishuk",
      "A. D. Docheva",
      "A. Doheny",
      "C. Dong",
      "A. M. Donohoe",
      "F. Dordei",
      "A. C. dos Reis",
      "A. D. Dowling",
      "L. Dreyfus",
      "W. Duan",
      "P. Duda",
      "L. Dufour",
      "V. Duk",
      "P. Durante",
      "M. M. Duras",
      "J. M. Durham",
      "O. D. Durmus",
      "A. Dziurda",
      "A. Dzyuba",
      "S. Easo",
      "E. Eckstein",
      "U. Egede",
      "A. Egorychev",
      "V. Egorychev",
      "S. Eisenhardt",
      "E. Ejopu",
      "L. Eklund",
      "M. Elashri",
      "J. Ellbracht",
      "S. Ely",
      "A. Ene",
      "J. Eschle",
      "S. Esen",
      "T. Evans",
      "F. Fabiano",
      "S. Faghih",
      "L. N. Falcao",
      "B. Fang",
      "R. Fantechi",
      "L. Fantini",
      "M. Faria",
      "K. Farmer",
      "D. Fazzini",
      "L. Felkowski",
      "M. Feng",
      "M. Feo",
      "A. Fernandez Casani",
      "M. Fernandez Gomez",
      "A. D. Fernez",
      "F. Ferrari",
      "F. Ferreira Rodrigues",
      "M. Ferrillo",
      "M. Ferro-Luzzi",
      "S. Filippov",
      "R. A. Fini",
      "M. Fiorini",
      "M. Firlej",
      "K. L. Fischer",
      "D. S. Fitzgerald",
      "C. Fitzpatrick",
      "T. Fiutowski",
      "F. Fleuret",
      "A. Fomin",
      "M. Fontana",
      "L. F. Foreman",
      "R. Forty",
      "D. Foulds-Holt",
      "V. Franco Lima",
      "M. Franco Sevilla",
      "M. Frank",
      "E. Franzoso",
      "G. Frau",
      "C. Frei",
      "D. A. Friday",
      "J. Fu",
      "Q. F\u00fchring",
      "T. Fulghesu",
      "G. Galati",
      "M. D. Galati",
      "A. Gallas Torreira",
      "D. Galli",
      "S. Gambetta",
      "M. Gandelman",
      "P. Gandini",
      "B. Ganie",
      "H. Gao",
      "R. Gao",
      "T. Q. Gao",
      "Y. Gao",
      "Y. Gao",
      "Y. Gao",
      "L. M. Garcia Martin",
      "P. Garcia Moreno",
      "J. Garc\u00eda Pardi\u00f1as",
      "P. Gardner",
      "K. G. Garg",
      "L. Garrido",
      "C. Gaspar",
      "A. Gavrikov",
      "L. L. Gerken",
      "E. Gersabeck",
      "M. Gersabeck",
      "T. Gershon",
      "S. Ghizzo",
      "Z. Ghorbanimoghaddam",
      "L. Giambastiani",
      "F. I. Giasemis",
      "V. Gibson",
      "H. K. Giemza",
      "A. L. Gilman",
      "M. Giovannetti",
      "A. Giovent\u00f9",
      "L. Girardey",
      "M. A. Giza",
      "F. C. Glaser",
      "V. V. Gligorov",
      "C. G\u00f6bel",
      "L. Golinka-Bezshyyko",
      "E. Golobardes",
      "D. Golubkov",
      "A. Golutvin",
      "S. Gomez Fernandez",
      "W. Gomulka",
      "I. Gon\u00e7ales Vaz",
      "F. Goncalves Abrantes",
      "M. Goncerz",
      "G. Gong",
      "J. A. Gooding",
      "I. V. Gorelov",
      "C. Gotti",
      "E. Govorkova",
      "J. P. Grabowski",
      "L. A. Granado Cardoso",
      "E. Graug\u00e9s",
      "E. Graverini",
      "L. Grazette",
      "G. Graziani",
      "A. T. Grecu",
      "L. M. Greeven",
      "N. A. Grieser",
      "L. Grillo",
      "S. Gromov",
      "C. Gu",
      "M. Guarise",
      "L. Guerry",
      "V. Guliaeva",
      "P. A. G\u00fcnther",
      "A. -K. Guseinov",
      "E. Gushchin",
      "Y. Guz",
      "T. Gys",
      "K. Habermann",
      "T. Hadavizadeh",
      "C. Hadjivasiliou",
      "G. Haefeli",
      "C. Haen",
      "S. Haken",
      "G. Hallett",
      "P. M. Hamilton",
      "J. Hammerich",
      "Q. Han",
      "X. Han",
      "S. Hansmann-Menzemer",
      "L. Hao",
      "N. Harnew",
      "T. H. Harris",
      "M. Hartmann",
      "S. Hashmi",
      "J. He",
      "A. Hedes",
      "F. Hemmer",
      "C. Henderson",
      "R. Henderson",
      "R. D. L. Henderson",
      "A. M. Hennequin",
      "K. Hennessy",
      "L. Henry",
      "J. Herd",
      "P. Herrero Gascon",
      "J. Heuel",
      "A. Hicheur",
      "G. Hijano Mendizabal",
      "J. Horswill",
      "R. Hou",
      "Y. Hou",
      "D. C. Houston",
      "N. Howarth",
      "J. Hu",
      "W. Hu",
      "X. Hu",
      "W. Hulsbergen",
      "R. J. Hunter",
      "M. Hushchyn",
      "D. Hutchcroft",
      "M. Idzik",
      "D. Ilin",
      "P. Ilten",
      "A. Iniukhin",
      "A. Iohner",
      "A. Ishteev",
      "K. Ivshin",
      "H. Jage",
      "S. J. Jaimes Elles",
      "S. Jakobsen",
      "E. Jans",
      "B. K. Jashal",
      "A. Jawahery",
      "C. Jayaweera",
      "V. Jevtic",
      "Z. Jia",
      "E. Jiang",
      "X. Jiang",
      "Y. Jiang",
      "Y. J. Jiang",
      "E. Jimenez Moya",
      "N. Jindal",
      "M. John",
      "A. John Rubesh Rajan",
      "D. Johnson",
      "C. R. Jones",
      "S. Joshi",
      "B. Jost",
      "J. Juan Castella",
      "N. Jurik",
      "I. Juszczak",
      "D. Kaminaris",
      "S. Kandybei",
      "M. Kane",
      "Y. Kang",
      "C. Kar",
      "M. Karacson",
      "A. Kauniskangas",
      "J. W. Kautz",
      "M. K. Kazanecki",
      "F. Keizer",
      "M. Kenzie",
      "T. Ketel",
      "B. Khanji",
      "A. Kharisova",
      "S. Kholodenko",
      "G. Khreich",
      "T. Kirn",
      "V. S. Kirsebom",
      "O. Kitouni",
      "S. Klaver",
      "N. Kleijne",
      "D. K. Klekots",
      "K. Klimaszewski",
      "M. R. Kmiec",
      "S. Koliiev",
      "L. Kolk",
      "A. Konoplyannikov",
      "P. Kopciewicz",
      "P. Koppenburg",
      "A. Korchin",
      "M. Korolev",
      "I. Kostiuk",
      "O. Kot",
      "S. Kotriakhova",
      "E. Kowalczyk",
      "A. Kozachuk",
      "P. Kravchenko",
      "L. Kravchuk",
      "O. Kravcov",
      "M. Kreps",
      "P. Krokovny",
      "W. Krupa",
      "W. Krzemien",
      "O. Kshyvanskyi",
      "S. Kubis",
      "M. Kucharczyk",
      "V. Kudryavtsev",
      "E. Kulikova",
      "A. Kupsc",
      "V. Kushnir",
      "B. Kutsenko",
      "J. Kvapil",
      "I. Kyryllin",
      "D. Lacarrere",
      "P. Laguarta Gonzalez",
      "A. Lai",
      "A. Lampis",
      "D. Lancierini",
      "C. Landesa Gomez",
      "J. J. Lane",
      "G. Lanfranchi",
      "C. Langenbruch",
      "J. Langer",
      "O. Lantwin",
      "T. Latham",
      "F. Lazzari",
      "C. Lazzeroni",
      "R. Le Gac",
      "H. Lee",
      "R. Lef\u00e8vre",
      "A. Leflat",
      "S. Legotin",
      "M. Lehuraux",
      "E. Lemos Cid",
      "O. Leroy",
      "T. Lesiak",
      "E. D. Lesser",
      "B. Leverington",
      "A. Li",
      "C. Li",
      "C. Li",
      "H. Li",
      "J. Li",
      "K. Li",
      "L. Li",
      "M. Li",
      "P. Li",
      "P. -R. Li",
      "Q. Li",
      "T. Li",
      "T. Li",
      "Y. Li",
      "Y. Li",
      "Y. Li",
      "Z. Lian",
      "Q. Liang",
      "X. Liang",
      "Z. Liang",
      "S. Libralon",
      "A. L. Lightbody",
      "C. Lin",
      "T. Lin",
      "R. Lindner",
      "H. Linton",
      "R. Litvinov",
      "D. Liu",
      "F. L. Liu",
      "G. Liu",
      "K. Liu",
      "S. Liu",
      "W. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. L. Liu",
      "G. Loachamin Ordonez",
      "A. Lobo Salvia",
      "A. Loi",
      "T. Long",
      "J. H. Lopes",
      "A. Lopez Huertas",
      "C. Lopez Iribarnegaray",
      "S. L\u00f3pez Soli\u00f1o",
      "Q. Lu",
      "C. Lucarelli",
      "D. Lucchesi",
      "M. Lucio Martinez",
      "Y. Luo",
      "A. Lupato",
      "E. Luppi",
      "K. Lynch",
      "X. -R. Lyu",
      "G. M. Ma",
      "S. Maccolini",
      "F. Machefert",
      "F. Maciuc",
      "B. Mack",
      "I. Mackay",
      "L. M. Mackey",
      "L. R. Madhan Mohan",
      "M. J. Madurai",
      "D. Magdalinski",
      "D. Maisuzenko",
      "J. J. Malczewski",
      "S. Malde",
      "L. Malentacca",
      "A. Malinin",
      "T. Maltsev",
      "G. Manca",
      "G. Mancinelli",
      "C. Mancuso",
      "R. Manera Escalero",
      "F. M. Manganella",
      "D. Manuzzi",
      "D. Marangotto",
      "J. F. Marchand",
      "R. Marchevski",
      "U. Marconi",
      "E. Mariani",
      "S. Mariani",
      "C. Marin Benito",
      "J. Marks",
      "A. M. Marshall",
      "L. Martel",
      "G. Martelli",
      "G. Martellotti",
      "L. Martinazzoli",
      "M. Martinelli",
      "D. Martinez Gomez",
      "D. Martinez Santos",
      "F. Martinez Vidal",
      "A. Martorell i Granollers",
      "A. Massafferri",
      "R. Matev",
      "A. Mathad",
      "V. Matiunin",
      "C. Matteuzzi",
      "K. R. Mattioli",
      "A. Mauri",
      "E. Maurice",
      "J. Mauricio",
      "P. Mayencourt",
      "J. Mazorra de Cos",
      "M. Mazurek",
      "M. McCann",
      "T. H. McGrath",
      "N. T. McHugh",
      "A. McNab",
      "R. McNulty",
      "B. Meadows",
      "G. Meier",
      "D. Melnychuk",
      "D. Mendoza Granada",
      "P. Menendez Valdes Perez",
      "F. M. Meng",
      "M. Merk",
      "A. Merli",
      "L. Meyer Garcia",
      "D. Miao",
      "H. Miao",
      "M. Mikhasenko",
      "D. A. Milanes",
      "A. Minotti",
      "E. Minucci",
      "T. Miralles",
      "B. Mitreska",
      "D. S. Mitzel",
      "A. Modak",
      "L. Moeser",
      "R. D. Moise",
      "E. F. Molina Cardenas",
      "T. Momb\u00e4cher",
      "M. Monk",
      "S. Monteil",
      "A. Morcillo Gomez",
      "G. Morello",
      "M. J. Morello",
      "M. P. Morgenthaler",
      "J. Moron",
      "W. Morren",
      "A. B. Morris",
      "A. G. Morris",
      "R. Mountain",
      "H. Mu",
      "Z. M. Mu",
      "E. Muhammad",
      "F. Muheim",
      "M. Mulder",
      "K. M\u00fcller",
      "F. Mu\u00f1oz-Rojas",
      "R. Murta",
      "V. Mytrochenko",
      "P. Naik",
      "T. Nakada",
      "R. Nandakumar",
      "T. Nanut",
      "I. Nasteva",
      "M. Needham",
      "E. Nekrasova",
      "N. Neri",
      "S. Neubert",
      "N. Neufeld",
      "P. Neustroev",
      "J. Nicolini",
      "D. Nicotra",
      "E. M. Niel",
      "N. Nikitin",
      "L. Nisi",
      "Q. Niu",
      "P. Nogarolli",
      "P. Nogga",
      "C. Normand",
      "J. Novoa Fernandez",
      "G. Nowak",
      "C. Nunez",
      "H. N. Nur",
      "A. Oblakowska-Mucha",
      "V. Obraztsov",
      "T. Oeser",
      "A. Okhotnikov",
      "O. Okhrimenko",
      "R. Oldeman",
      "F. Oliva",
      "E. Olivart Pino",
      "M. Olocco",
      "C. J. G. Onderwater",
      "R. H. O'Neil",
      "J. S. Ordonez Soto",
      "D. Osthues",
      "J. M. Otalora Goicochea",
      "P. Owen",
      "A. Oyanguren",
      "O. Ozcelik",
      "F. Paciolla",
      "A. Padee",
      "K. O. Padeken",
      "B. Pagare",
      "T. Pajero",
      "A. Palano",
      "M. Palutan",
      "C. Pan",
      "X. Pan",
      "S. Panebianco",
      "G. Panshin",
      "L. Paolucci",
      "A. Papanestis",
      "M. Pappagallo",
      "L. L. Pappalardo",
      "C. Pappenheimer",
      "C. Parkes",
      "D. Parmar",
      "B. Passalacqua",
      "G. Passaleva",
      "D. Passaro",
      "A. Pastore",
      "M. Patel",
      "J. Patoc",
      "C. Patrignani",
      "A. Paul",
      "C. J. Pawley",
      "A. Pellegrino",
      "J. Peng",
      "X. Peng",
      "M. Pepe Altarelli",
      "S. Perazzini",
      "D. Pereima",
      "H. Pereira Da Costa",
      "M. Pereira Martinez",
      "A. Pereiro Castro",
      "C. Perez",
      "P. Perret",
      "A. Perrevoort",
      "A. Perro",
      "M. J. Peters",
      "K. Petridis",
      "A. Petrolini",
      "S. Pezzulo",
      "J. P. Pfaller",
      "H. Pham",
      "L. Pica",
      "M. Piccini",
      "L. Piccolo",
      "B. Pietrzyk",
      "G. Pietrzyk",
      "R. N. Pilato",
      "D. Pinci",
      "F. Pisani",
      "M. Pizzichemi",
      "V. M. Placinta",
      "M. Plo Casasus",
      "T. Poeschl",
      "F. Polci",
      "M. Poli Lener",
      "A. Poluektov",
      "N. Polukhina",
      "I. Polyakov",
      "E. Polycarpo",
      "S. Ponce",
      "D. Popov",
      "S. Poslavskii",
      "K. Prasanth",
      "C. Prouve",
      "D. Provenzano",
      "V. Pugatch",
      "G. Punzi",
      "J. R. Pybus",
      "S. Qasim",
      "Q. Q. Qian",
      "W. Qian",
      "N. Qin",
      "S. Qu",
      "R. Quagliani",
      "R. I. Rabadan Trejo",
      "R. Racz",
      "J. H. Rademacker",
      "M. Rama",
      "M. Ram\u00edrez Garc\u00eda",
      "V. Ramos De Oliveira",
      "M. Ramos Pernas",
      "M. S. Rangel",
      "F. Ratnikov",
      "G. Raven",
      "M. Rebollo De Miguel",
      "F. Redi",
      "J. Reich",
      "F. Reiss",
      "Z. Ren",
      "P. K. Resmi",
      "M. Ribalda Galvez",
      "R. Ribatti",
      "G. Ricart",
      "D. Riccardi",
      "S. Ricciardi",
      "K. Richardson",
      "M. Richardson-Slipper",
      "K. Rinnert",
      "P. Robbe",
      "G. Robertson",
      "E. Rodrigues",
      "A. Rodriguez Alvarez",
      "E. Rodriguez Fernandez",
      "J. A. Rodriguez Lopez",
      "E. Rodriguez Rodriguez",
      "J. Roensch",
      "A. Rogachev",
      "A. Rogovskiy",
      "D. L. Rolf",
      "P. Roloff",
      "V. Romanovskiy",
      "A. Romero Vidal",
      "G. Romolini",
      "F. Ronchetti",
      "T. Rong",
      "M. Rotondo",
      "S. R. Roy",
      "M. S. Rudolph",
      "M. Ruiz Diaz",
      "R. A. Ruiz Fernandez",
      "J. Ruiz Vidal",
      "J. J. Saavedra-Arias",
      "J. J. Saborido Silva",
      "S. E. R. Sacha Emile R.",
      "R. Sadek",
      "N. Sagidova",
      "D. Sahoo",
      "N. Sahoo",
      "B. Saitta",
      "M. Salomoni",
      "I. Sanderswood",
      "R. Santacesaria",
      "C. Santamarina Rios",
      "M. Santimaria",
      "L. Santoro",
      "E. Santovetti",
      "A. Saputi",
      "D. Saranin",
      "A. Sarnatskiy",
      "G. Sarpis",
      "M. Sarpis",
      "C. Satriano",
      "M. Saur",
      "D. Savrina",
      "H. Sazak",
      "F. Sborzacchi",
      "A. Scarabotto",
      "S. Schael",
      "S. Scherl",
      "M. Schiller",
      "H. Schindler",
      "M. Schmelling",
      "B. Schmidt",
      "N. Schmidt",
      "S. Schmitt",
      "H. Schmitz",
      "O. Schneider",
      "A. Schopper",
      "N. Schulte",
      "M. H. Schune",
      "G. Schwering",
      "B. Sciascia",
      "A. Sciuccati",
      "I. Segal",
      "S. Sellam",
      "A. Semennikov",
      "T. Senger",
      "M. Senghi Soares",
      "A. Sergi",
      "N. Serra",
      "L. Sestini",
      "A. Seuthe",
      "B. Sevilla Sanjuan",
      "Y. Shang",
      "D. M. Shangase",
      "M. Shapkin",
      "R. S. Sharma",
      "I. Shchemerov",
      "L. Shchutska",
      "T. Shears",
      "L. Shekhtman",
      "Z. Shen",
      "S. Sheng",
      "V. Shevchenko",
      "B. Shi",
      "Q. Shi",
      "W. S. Shi",
      "Y. Shimizu",
      "E. Shmanin",
      "R. Shorkin",
      "J. D. Shupperd",
      "R. Silva Coutinho",
      "G. Simi",
      "S. Simone",
      "M. Singha",
      "N. Skidmore",
      "T. Skwarnicki",
      "M. W. Slater",
      "E. Smith",
      "K. Smith",
      "M. Smith",
      "L. Soares Lavra",
      "M. D. Sokoloff",
      "F. J. P. Soler",
      "A. Solomin",
      "A. Solovev",
      "N. S. Sommerfeld",
      "R. Song",
      "Y. Song",
      "Y. Song",
      "Y. S. Song",
      "F. L. Souza De Almeida",
      "B. Souza De Paula",
      "E. Spadaro Norella",
      "E. Spedicato",
      "J. G. Speer",
      "P. Spradlin",
      "V. Sriskaran",
      "F. Stagni",
      "M. Stahl",
      "S. Stahl",
      "S. Stanislaus",
      "M. Stefaniak",
      "E. N. Stein",
      "O. Steinkamp",
      "H. Stevens",
      "D. Strekalina",
      "Y. Su",
      "F. Suljik",
      "J. Sun",
      "J. Sun",
      "L. Sun",
      "D. Sundfeld",
      "W. Sutcliffe",
      "V. Svintozelskyi",
      "K. Swientek",
      "F. Swystun",
      "A. Szabelski",
      "T. Szumlak",
      "Y. Tan",
      "Y. Tang",
      "Y. T. Tang",
      "M. D. Tat",
      "J. A. Teijeiro Jimenez",
      "A. Terentev",
      "F. Terzuoli",
      "F. Teubert",
      "E. Thomas",
      "D. J. D. Thompson",
      "A. R. Thomson-Strong",
      "H. Tilquin",
      "V. Tisserand",
      "S. T'Jampens",
      "M. Tobin",
      "T. T. Todorov",
      "L. Tomassetti",
      "G. Tonani",
      "X. Tong",
      "T. Tork",
      "D. Torres Machado",
      "L. Toscano",
      "D. Y. Tou",
      "C. Trippl",
      "G. Tuci",
      "N. Tuning",
      "L. H. Uecker",
      "A. Ukleja",
      "D. J. Unverzagt",
      "A. Upadhyay",
      "B. Urbach",
      "A. Usachov",
      "A. Ustyuzhanin",
      "U. Uwer",
      "V. Vagnoni",
      "V. Valcarce Cadenas",
      "G. Valenti",
      "N. Valls Canudas",
      "J. van Eldik",
      "H. Van Hecke",
      "E. van Herwijnen",
      "C. B. Van Hulse",
      "R. Van Laak",
      "M. van Veghel",
      "G. Vasquez",
      "R. Vazquez Gomez",
      "P. Vazquez Regueiro",
      "C. V\u00e1zquez Sierra",
      "S. Vecchi",
      "J. Velilla Serna",
      "J. J. Velthuis",
      "M. Veltri",
      "A. Venkateswaran",
      "M. Verdoglia",
      "M. Vesterinen",
      "W. Vetens",
      "D. Vico Benet",
      "P. Vidrier Villalba",
      "M. Vieites Diaz",
      "X. Vilasis-Cardona",
      "E. Vilella Figueras",
      "A. Villa",
      "P. Vincent",
      "B. Vivacqua",
      "F. C. Volle",
      "D. vom Bruch",
      "N. Voropaev",
      "K. Vos",
      "C. Vrahas",
      "J. Wagner",
      "J. Walsh",
      "E. J. Walton",
      "G. Wan",
      "A. Wang",
      "B. Wang",
      "C. Wang",
      "G. Wang",
      "H. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "M. Wang",
      "N. W. Wang",
      "R. Wang",
      "X. Wang",
      "X. Wang",
      "X. W. Wang",
      "Y. Wang",
      "Y. Wang",
      "Y. W. Wang",
      "Z. Wang",
      "Z. Wang",
      "Z. Wang",
      "J. A. Ward",
      "M. Waterlaat",
      "N. K. Watson",
      "D. Websdale",
      "Y. Wei",
      "J. Wendel",
      "B. D. C. Westhenry",
      "C. White",
      "M. Whitehead",
      "E. Whiter",
      "A. R. Wiederhold",
      "D. Wiedner",
      "M. A. Wiegertjes",
      "C. Wild",
      "G. Wilkinson",
      "M. K. Wilkinson",
      "M. Williams",
      "M. J. Williams",
      "M. R. J. Williams",
      "R. Williams",
      "S. Williams",
      "Z. Williams",
      "F. F. Wilson",
      "M. Winn",
      "W. Wislicki",
      "M. Witek",
      "L. Witola",
      "T. Wolf",
      "E. Wood",
      "G. Wormser",
      "S. A. Wotton",
      "H. Wu",
      "J. Wu",
      "X. Wu",
      "Y. Wu",
      "Z. Wu",
      "K. Wyllie",
      "S. Xian",
      "Z. Xiang",
      "Y. Xie",
      "T. X. Xing",
      "A. Xu",
      "L. Xu",
      "L. Xu",
      "M. Xu",
      "Z. Xu",
      "Z. Xu",
      "Z. Xu",
      "K. Yang",
      "X. Yang",
      "Y. Yang",
      "Z. Yang",
      "V. Yeroshenko",
      "H. Yeung",
      "H. Yin",
      "X. Yin",
      "C. Y. Yu",
      "J. Yu",
      "X. Yuan",
      "Y Yuan",
      "E. Zaffaroni",
      "J. A. Zamora Saa",
      "M. Zavertyaev",
      "M. Zdybal",
      "F. Zenesini",
      "C. Zeng",
      "M. Zeng",
      "C. Zhang",
      "D. Zhang",
      "J. Zhang",
      "L. Zhang",
      "R. Zhang",
      "S. Zhang",
      "S. Zhang",
      "Y. Zhang",
      "Y. Z. Zhang",
      "Z. Zhang",
      "Y. Zhao",
      "A. Zhelezov",
      "S. Z. Zheng",
      "X. Z. Zheng",
      "Y. Zheng",
      "T. Zhou",
      "X. Zhou",
      "Y. Zhou",
      "V. Zhovkovska",
      "L. Z. Zhu",
      "X. Zhu",
      "X. Zhu",
      "Y. Zhu",
      "V. Zhukov",
      "J. Zhuo",
      "Q. Zou",
      "D. Zuliani",
      "G. Zunica"
    ],
    "published": "2025-07-16T16:46:33+00:00",
    "summary": "A sample of $pp$ collision data, corresponding to an integrated luminosity of 5.4 fb$^{-1}$ and collected by the LHCb experiment during LHC Run 2, is used to measure the ratio of the lifetime of the ${\\it{\\Xi}}_b^0$ baryon to that of the ${\\it{\\Lambda}}_b^0$ baryon, $r_{\\tau}\\equiv\\tau_{{\\it{\\Xi}}_b^0}/\\tau_{{\\it{\\Lambda}}_b^0}$. The value ${r_{\\tau}^{\\rm Run\\,2}=1.004\\pm0.009\\pm0.006}$ is obtained, where the first uncertainty is statistical and the second systematic. This value is averaged with the corresponding value from Run 1 to obtain ${r_{\\tau} = 1.004\\pm0.008\\pm0.005}$. Multiplying by the known value of the ${\\it{\\Lambda}}_b^0$ lifetime yields ${{\\tau_{{\\it{\\Xi}}_b^0}} = 1.475\\pm0.012\\pm0.008\\pm0.009~{\\rm ps}}$, where the last uncertainty is due to the limited knowledge of the ${\\it{\\Lambda}}_b^0$ lifetime. This measurement improves the precision of the current world average of the ${\\it{\\Xi}}_b^0$ lifetime by about a factor of two, and is in good agreement with the most recent theoretical predictions."
  },
  {
    "title": "Surrogate modeling for uncertainty quantification in nonlinear dynamics",
    "url": "http://arxiv.org/abs/2507.12358v1",
    "arxiv_id": "2507.12358v1",
    "authors": [
      "S. Marelli",
      "S. Sch\u00e4r",
      "B. Sudret"
    ],
    "published": "2025-07-16T15:57:09+00:00",
    "summary": "Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use."
  },
  {
    "title": "The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models",
    "url": "http://arxiv.org/abs/2507.12335v1",
    "arxiv_id": "2507.12335v1",
    "authors": [
      "Morgan Deal",
      "Ga\u00ebl Buldgen",
      "Louis Manchon",
      "Yveline Lebreton",
      "Arlette Noels",
      "Richard Scuflaire"
    ],
    "published": "2025-07-16T15:25:47+00:00",
    "summary": "The transport of chemical elements in stellar interiors is one of the greatest sources of uncertainties of solar and stellar modelling. The Sun, with its exquisite spectroscopic, helioseismic and neutrino observations, offers a prime environment to test the prescriptions used for both microscopic and macroscopic transport processes. We study in detail the impact of various formalisms for atomic diffusion on helioseismic constraints in both CLES (Scuflaire et al., 2008a) and Cesam2k2 (Morel and Lebreton 2008; Marques et al. 2013; Deal et al. 2018) models and compare both codes in detail. Moreover, due to the inability of standard models using microscopic diffusion to reproduce light element depletion in the Sun (Li, Be), another efficient process must be included to reproduce these constraints (rotation-induced: Eggenberger et al. 2022, overshooting -- or penetrative convection -- below the convective envelope: Th\\'evenin et al. 2017, or ad hoc turbulence: Lebreton and Maeder 1987; Richer, Michaud, and Turcotte 2000). However, introducing such an extra mixing leads to issues with the CNO neutrino fluxes (see Buldgen et al. 2023), which seem to be systematically lower than the Borexino observations (Appel et al., 2022. Another key aspect to consider when reconciling models with neutrino fluxes is the impact of electronic screening (Mussack and D\\\"appen, 2011)."
  },
  {
    "title": "Uncertainty and entropies of classical channels",
    "url": "http://arxiv.org/abs/2507.12310v1",
    "arxiv_id": "2507.12310v1",
    "authors": [
      "Takla Nateeboon"
    ],
    "published": "2025-07-16T15:06:17+00:00",
    "summary": "In this thesis, I studied a mathematical development to define and quantify the uncertainty inherent in classical channels. This thesis starts with the introduction and background on how to formally think about uncertainty in the domain of classical states. The concept of probability vector majorization and its variants, relative majorization and conditional majorization, are reviewed. This thesis introduces three conceptually distinct approaches to formalize the notion of uncertainty inherent in classical channels. These three approaches define the same preordering on the domain of classical channels, leading to characterizations from many perspectives. With the solid foundation of uncertainty comparison, classical channel entropy is then defined to be an additive monotone with respect to the majorization relation. The well-known entropies in the domain of classical states are uniquely extended to the domain of channels via the optimal extensions, providing not only a solid foundation but also the quantifiers of uncertainty inherent in classical channels."
  },
  {
    "title": "Forecasting Climate Policy Uncertainty: Evidence from the United States",
    "url": "http://arxiv.org/abs/2507.12276v1",
    "arxiv_id": "2507.12276v1",
    "authors": [
      "Donia Besher",
      "Anirban Sengupta",
      "Tanujit Chakraborty"
    ],
    "published": "2025-07-16T14:24:31+00:00",
    "summary": "Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers strive to balance economic growth with environmental goals. High levels of CPU can slow down investments in green technologies, make regulatory planning more difficult, and increase public resistance to climate reforms, especially during times of economic stress. This study addresses the challenge of forecasting the US CPU index by building the Bayesian Structural Time Series (BSTS) model with a large set of covariates, including economic indicators, financial cycle data, and public sentiments captured through Google Trends. The key strength of the BSTS model lies in its ability to efficiently manage a large number of covariates through its dynamic feature selection mechanism based on the spike-and-slab prior. To validate the effectiveness of the selected features of the BSTS model, an impulse response analysis is performed. The results show that macro-financial shocks impact CPU in different ways over time. Numerical experiments are performed to evaluate the performance of the BSTS model with exogenous variables on the US CPU dataset over different forecasting horizons. The empirical results confirm that BSTS consistently outperforms classical and deep learning frameworks, particularly for semi-long-term and long-term forecasts."
  },
  {
    "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters",
    "url": "http://arxiv.org/abs/2507.12262v1",
    "arxiv_id": "2507.12262v1",
    "authors": [
      "Zachary James",
      "Joseph Guinness"
    ],
    "published": "2025-07-16T14:09:49+00:00",
    "summary": "Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset."
  },
  {
    "title": "What are we talking about when we discuss the Born-Oppenheimer approximation?",
    "url": "http://arxiv.org/abs/2507.12223v1",
    "arxiv_id": "2507.12223v1",
    "authors": [
      "Olimpia Lombardi",
      "Sebastian Fortin",
      "Juan Camilo Martinez Gonzalez",
      "Hernan Lucas Accorinti"
    ],
    "published": "2025-07-16T13:30:47+00:00",
    "summary": "Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a comprehensive article examining the Born-Oppenheimer Approximation (BOA). Their central objective is to challenge our position on the matter-namely, that the BOA incorporates a classical assumption incompatible with the Heisenberg Uncertainty Principle. In contrast, HLT contend that the BOA involves no such classical assumption and, as a result, supports the view that chemistry can be reduced to physics. The purpose of this paper is to offer a critical analysis of the HLT article and to clarify why we consider their arguments unpersuasive."
  },
  {
    "title": "Explainable Evidential Clustering",
    "url": "http://arxiv.org/abs/2507.12192v1",
    "arxiv_id": "2507.12192v1",
    "authors": [
      "Victor F. Lopes de Souza",
      "Karima Bakhti",
      "Sofiane Ramdani",
      "Denis Mottet",
      "Abdelhak Imoussaten"
    ],
    "published": "2025-07-16T12:44:25+00:00",
    "summary": "Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of \"tolerable\" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time."
  },
  {
    "title": "Learning, fast and slow: a two-fold algorithm for data-based model adaptation",
    "url": "http://arxiv.org/abs/2507.12187v1",
    "arxiv_id": "2507.12187v1",
    "authors": [
      "Laura Boca de Giuli",
      "Alessio La Bella",
      "Riccardo Scattolini"
    ],
    "published": "2025-07-16T12:34:17+00:00",
    "summary": "This article addresses the challenge of adapting data-based models over time. We propose a novel two-fold modelling architecture designed to correct plant-model mismatch caused by two types of uncertainty. Out-of-domain uncertainty arises when the system operates under conditions not represented in the initial training dataset, while in-domain uncertainty results from real-world variability and flaws in the model structure or training process. To handle out-of-domain uncertainty, a slow learning component, inspired by the human brain's slow thinking process, learns system dynamics under unexplored operating conditions, and it is activated only when a monitoring strategy deems it necessary. This component consists of an ensemble of models, featuring (i) a combination rule that weights individual models based on the statistical proximity between their training data and the current operating condition, and (ii) a monitoring algorithm based on statistical control charts that supervises the ensemble's reliability and triggers the offline training and integration of a new model when a new operating condition is detected. To address in-domain uncertainty, a fast learning component, inspired by the human brain's fast thinking process, continuously compensates in real time for the mismatch of the slow learning model. This component is implemented as a Gaussian process (GP) model, trained online at each iteration using recent data while discarding older samples. The proposed methodology is tested on a benchmark energy system referenced in the literature, demonstrating that the combined use of slow and fast learning components improves model accuracy compared to standard adaptation approaches."
  },
  {
    "title": "Canonical Bayesian Linear System Identification",
    "url": "http://arxiv.org/abs/2507.11535v1",
    "arxiv_id": "2507.11535v1",
    "authors": [
      "Andrey Bryutkin",
      "Matthew E. Levine",
      "I\u00f1igo Urteaga",
      "Youssef Marzouk"
    ],
    "published": "2025-07-15T17:58:55+00:00",
    "summary": "Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data."
  },
  {
    "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation",
    "url": "http://arxiv.org/abs/2507.11488v1",
    "arxiv_id": "2507.11488v1",
    "authors": [
      "Pakizar Shamoi",
      "Nuray Toganas",
      "Muragul Muratbekova",
      "Elnara Kadyrgali",
      "Adilet Yerkin",
      "Ayan Igali",
      "Malika Ziyada",
      "Ayana Adilova",
      "Aron Karatayev",
      "Yerdauit Torekhan"
    ],
    "published": "2025-07-15T17:01:45+00:00",
    "summary": "Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical."
  },
  {
    "title": "A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification",
    "url": "http://arxiv.org/abs/2507.11420v1",
    "arxiv_id": "2507.11420v1",
    "authors": [
      "Mingcong Li"
    ],
    "published": "2025-07-15T15:45:55+00:00",
    "summary": "Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies."
  },
  {
    "title": "Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty",
    "url": "http://arxiv.org/abs/2507.11413v1",
    "arxiv_id": "2507.11413v1",
    "authors": [
      "Christos N. Efrem",
      "Ioannis Krikidis"
    ],
    "published": "2025-07-15T15:35:50+00:00",
    "summary": "We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average."
  },
  {
    "title": "Towards NNLO QCD predictions for off-shell top-quark pair production and decays",
    "url": "http://arxiv.org/abs/2507.11410v1",
    "arxiv_id": "2507.11410v1",
    "authors": [
      "Luca Buonocore",
      "Massimiliano Grazzini",
      "Stefan Kallweit",
      "Jonas M. Lindert",
      "Chiara Savoini"
    ],
    "published": "2025-07-15T15:30:13+00:00",
    "summary": "We consider QCD radiative corrections to $W^+W^-b {\\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\\Gamma_t\\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\\%$ level $\\unicode{x2013}$ significantly smaller than the $5\\%$ residual perturbative uncertainties."
  },
  {
    "title": "Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions",
    "url": "http://arxiv.org/abs/2507.11394v1",
    "arxiv_id": "2507.11394v1",
    "authors": [
      "Syed Afrid Jahan",
      "Hendrik Roch",
      "Chun Shen"
    ],
    "published": "2025-07-15T15:05:53+00:00",
    "summary": "We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\\sqrt{s_\\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions."
  },
  {
    "title": "Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning",
    "url": "http://arxiv.org/abs/2507.11385v1",
    "arxiv_id": "2507.11385v1",
    "authors": [
      "George D. Pasparakis",
      "Ioannis A. Kougioumtzoglou",
      "Michael D. Shields"
    ],
    "published": "2025-07-15T14:54:57+00:00",
    "summary": "A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics."
  },
  {
    "title": "The miniJPAS survey quasar selection V: combined algorithm",
    "url": "http://arxiv.org/abs/2507.11380v1",
    "arxiv_id": "2507.11380v1",
    "authors": [
      "Ignasi P\u00e9rez-R\u00e0fols",
      "L. Raul Abramo",
      "Gin\u00e9s Mart\u00ednez-Solaeche",
      "Nat\u00e1lia V. N. Rodrigues",
      "Matthew M. Pieri",
      "Marina Burjal\u00e8s-del-Amo",
      "Maria Escol\u00e0-Gallinat",
      "Montserrat Ferr\u00e9-Abad",
      "Mireia Isern-Vizoso",
      "Jailson Alcaniz",
      "Narciso Benitez",
      "Silvia Bonoli",
      "Saulo Carneiro",
      "Javier Cenarro",
      "David Crist\u00f3bal-Hornillos",
      "Renato Dupke",
      "Alessandro Ederoclite",
      "Rosa Mar\u00eda Gonz\u00e1lez Delgado",
      "Siddhartha Gurung-Lopez",
      "Antonio Hern\u00e1n-Caballero",
      "Carlos Hern\u00e1ndez-Monteagudo",
      "Carlos L\u00f3pez-Sanjuan",
      "Antonio Mar\u00edn-Franch",
      "Valerio Marra",
      "Claudia Mendes de Oliveira",
      "Mariano Moles",
      "Laerte Sodr\u00e9 Jr.",
      "Keith Taylor",
      "Jes\u00fas Varela",
      "H\u00e9ctor V\u00e1zquez Rami\u00f3"
    ],
    "published": "2025-07-15T14:50:04+00:00",
    "summary": "Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\\sigma_{\\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\\sigma_{\\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration."
  },
  {
    "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption",
    "url": "http://arxiv.org/abs/2507.11357v1",
    "arxiv_id": "2507.11357v1",
    "authors": [
      "Emile van Krieken",
      "Pasquale Minervini",
      "Edoardo Ponti",
      "Antonio Vergari"
    ],
    "published": "2025-07-15T14:27:05+00:00",
    "summary": "The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons."
  },
  {
    "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces",
    "url": "http://arxiv.org/abs/2507.11352v1",
    "arxiv_id": "2507.11352v1",
    "authors": [
      "Yunhao Yang",
      "Neel P. Bhatt",
      "Christian Ellis",
      "Alvaro Velasquez",
      "Zhangyang Wang",
      "Ufuk Topcu"
    ],
    "published": "2025-07-15T14:24:01+00:00",
    "summary": "Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics."
  },
  {
    "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI",
    "url": "http://arxiv.org/abs/2507.10510v1",
    "arxiv_id": "2507.10510v1",
    "authors": [
      "Jiangkai Wu",
      "Zhiyuan Ren",
      "Liming Liu",
      "Xinggong Zhang"
    ],
    "published": "2025-07-14T17:34:49+00:00",
    "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat."
  },
  {
    "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour",
    "url": "http://arxiv.org/abs/2507.10445v1",
    "arxiv_id": "2507.10445v1",
    "authors": [
      "Chris Madge",
      "Matthew Purver",
      "Massimo Poesio"
    ],
    "published": "2025-07-14T16:28:00+00:00",
    "summary": "In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy."
  },
  {
    "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data",
    "url": "http://arxiv.org/abs/2507.10425v1",
    "arxiv_id": "2507.10425v1",
    "authors": [
      "Alvaro H. C. Correia",
      "Christos Louizos"
    ],
    "published": "2025-07-14T16:10:55+00:00",
    "summary": "Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift."
  },
  {
    "title": "Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions",
    "url": "http://arxiv.org/abs/2507.10377v1",
    "arxiv_id": "2507.10377v1",
    "authors": [
      "T. Dumont",
      "A. Bonhomme",
      "A. Griffiths",
      "A. Choplin",
      "M. A. Aloy",
      "G. Meynet",
      "K. Godbey",
      "C. Simenel",
      "G. Scamps",
      "F. Castillo",
      "A. Cosoli-Ortega",
      "S. Courtin"
    ],
    "published": "2025-07-14T15:16:46+00:00",
    "summary": "The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\\alpha},{\\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\\alpha},{\\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\\alpha},{\\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition."
  },
  {
    "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter",
    "url": "http://arxiv.org/abs/2507.10355v1",
    "arxiv_id": "2507.10355v1",
    "authors": [
      "Bo Jiang",
      "Xueyang Ze",
      "Beibei Wang",
      "Xixi Wang",
      "Xixi Wan",
      "Bin Luo"
    ],
    "published": "2025-07-14T14:56:49+00:00",
    "summary": "Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach."
  },
  {
    "title": "Gaussian Process Methods for Very Large Astrometric Data Sets",
    "url": "http://arxiv.org/abs/2507.10317v1",
    "arxiv_id": "2507.10317v1",
    "authors": [
      "Timothy Hapitas",
      "Lawrence M. Widrow",
      "Thavisha E. Dharmawardena",
      "Daniel Foreman-Mackey"
    ],
    "published": "2025-07-14T14:25:17+00:00",
    "summary": "We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics."
  },
  {
    "title": "How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models",
    "url": "http://arxiv.org/abs/2507.10314v1",
    "arxiv_id": "2507.10314v1",
    "authors": [
      "Nina Mackensen",
      "Nicole Reindl",
      "Klaus Werner",
      "Matti Dorsch",
      "Shuyu Tan"
    ],
    "published": "2025-07-14T14:18:50+00:00",
    "summary": "PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\\text{RX J}0122.9\\text{ -}7521$. We find $T_\\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \\times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\\text{RX J}0122.9\\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\\text{RX J}0122.9\\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\\text{RX J}0122.9\\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\\text{RX J}0122.9\\text{ -}7521$."
  },
  {
    "title": "High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy",
    "url": "http://arxiv.org/abs/2507.10291v1",
    "arxiv_id": "2507.10291v1",
    "authors": [
      "Kristine V. Ung",
      "Connor A. Roncaioli",
      "Ronald L. Walsworth",
      "Sean M. Blakley"
    ],
    "published": "2025-07-14T13:57:37+00:00",
    "summary": "The negatively charged diamond nitrogen-vacancy ($\\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\\mathrm{^{1}E}$ singlet state in the $\\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\\mathrm{^{1}E}$ into the neutral $\\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited."
  },
  {
    "title": "History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions",
    "url": "http://arxiv.org/abs/2507.10201v1",
    "arxiv_id": "2507.10201v1",
    "authors": [
      "Gleb Shishaev",
      "Vasily Demyanov",
      "Daniel Arnold"
    ],
    "published": "2025-07-14T12:14:17+00:00",
    "summary": "The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure."
  },
  {
    "title": "Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture",
    "url": "http://arxiv.org/abs/2507.10166v1",
    "arxiv_id": "2507.10166v1",
    "authors": [
      "Filip Surmaa",
      "Anahita Jamshidnejad"
    ],
    "published": "2025-07-14T11:26:59+00:00",
    "summary": "This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems."
  }
]
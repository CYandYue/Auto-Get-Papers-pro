## üì∞ Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2026-02-18 | [Understanding the kinetics of static recrystallization in Mg-Zn-Ca alloys using an integrated PRISMS simulation framework](http://arxiv.org/abs/2602.16701v1) | David Montiel, Philip Staublin et al. | Recrystallization is a phenomenon in which a plastically deformed polycrystalline microstructure with a high dislocation density transforms into another that has low dislocation density. This evolution is driven by the stored energy in dislocations, rather than grain growth driven by grain boundary energy alone. One difficulty in quantitative modeling of recrystallization is the uncertainty in material parameters, which can be addressed by integration of experimental data into simulations. In this work, we compare simulated static recrystallization dynamics of a Mg-3Zn-0.1Ca wt.% alloy to experiments involving thermomechanical processing followed by measurements of the recrystallization fraction over time. The simulations are performed by combining PRISMS software for crystal plasticity and phase-field models (PRISMS-Plasticity and PRISMS-PF, respectively) in an integrated computational materials engineering framework. At 20% strain and annealing at 350 ¬∞C, the model accurately describes recrystallization dynamics up to a mobility-dependent time scale factor. While the average grain boundary mobility and the fraction of plastic work converted into stored energy are not precisely known, by fitting simulations to experimental data, we show that the average grain boundary mobility can be determined if the fraction of plastic work converted to stored energy is known, or vice versa. For low annealing temperatures, we observe a discrepancy between the model and experiments in the late stages of recrystallization, where a slowdown in recrystallization kinetics occurs in the experiments. We discuss possible sources of this slowdown and propose additional physical mechanisms that need to be accounted for in the model to improve its predictions. |
| 2026-02-18 | [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](http://arxiv.org/abs/2602.16699v1) | Wenxuan Ding, Nicholas Tomlin et al. | LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies. |
| 2026-02-18 | [Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems](http://arxiv.org/abs/2602.16678v1) | Harrison Perone, Christopher W. Hays | In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches. |
| 2026-02-18 | [HOLISMOKES XX. Lens models of binary lens galaxies with five images of Supernova Winny](http://arxiv.org/abs/2602.16620v1) | L. R. Ecker, A. G. Schweinfurth et al. | Strongly lensed supernovae (SNe) provide a powerful way to study cosmology, SNe and galaxies. Modelling the lens system is key to extracting astrophysical and cosmological information. We present adaptive-optics-assisted high-resolution images of SN Winny (SN 2025wny) in the J and K filters obtained with the Large Binocular Telescope (LBT). The LBT imaging confirms the presence of a fifth point source, whose colour is consistent with that of the other SN images at similar phases, while lens modelling robustly supports its interpretation as an additional image of SN~Winny. We measure the positions of the five SN images with uncertainties varying between 1 and 14 milliarcseconds. We build the first mass models using lenstronomy and GLEE, and explore three classes of mass models for the two lens galaxies G1 and G2. The optimal model class of the three is a singular isothermal ellipsoid for G1, a singular isothermal sphere for G2, and an external shear. We infer the enclosed masses within the Einstein radius as 4.61^{+0.06}_{-0.04} \times 10^{11}\,M_\odot for G1 and 1.01\pm0.02 \times 10^{11}\,M_\odot for G2. The lensing configuration by the two lens galaxies can produce two additional magnified SN images beyond the five observed ones; the exclusion of such model configurations further constrains the lens model parameters. Our model fits to the observed image positions with an RMS of ~0.0012" - 0.0025", within the observed positional uncertainties. The predicted magnifications of the multiple images vary between ~1.6 (for the faintest fifth image E) to ~10 (for the brightest image A). The predicted relative lensing magnifications of the multiple images do not match that of the observed within 2œÉuncertainties. The differences in the relative magnifications could be due to millilensing/microlensing. Our mass models form the basis for future analyses of this unique system. (abridged) |
| 2026-02-18 | [Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles](http://arxiv.org/abs/2602.16594v1) | Abhishek Goudar, Angela P. Schoellig | Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments. |

<!-- LATEST_PAPERS_END -->


<!-- HISTORICAL_PAPERS_START -->

<details>
<summary>üìö View Historical Papers (1495 entries)</summary>



| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2026-02-18 | [GOTO identification and broadband modelling of the counterpart to the SVOM GRB 250818B](http://arxiv.org/abs/2602.16559v1) | Sergey Belkin, Gavin P. Lamb et al. | Rapid localisation and follow-up of gamma-ray bursts (GRBs) increasingly rely on low-latency triggers from new missions coupled to wide-field robotic optical facilities. We present the discovery and multi-wavelength follow-up of GRB 250818B, detected by the Space Variable Objects Monitor (SVOM) and localised optically by the Gravitational-wave Optical Transient Observer (GOTO). We compile and homogenise X-ray, optical/NIR, and radio data to build broadband light curves and spectral energy distributions. The afterglow is unusually luminous for a nominal short GRB, lying on the bright end of the short-GRB population in X-rays and optical and among the most luminous high-redshift short-GRB afterglows in the radio. MeerKAT detects the source at 3.1 GHz, while ALMA provides deep higher-frequency limits. Keck/LRIS spectroscopy shows continuum and metal absorption (Fe II, Mg II, Mg I), giving $z=1.216$. Synchrotron forward-shock modelling favours a constant-density medium and strongly prefers refreshed (energy-injection) emission, well described by a two-component jet with $E_{K,iso} \sim 4\times10^{52}$ erg, $n_0 \sim 3.6$ cm$^{-3}$, $Œ∏_j \simeq 0.10$ rad ($\sim 5.7$ deg), and $p \simeq 1.64$. The host association is ambiguous: the nearest LS DR10 galaxy candidate ($r_{AB} \sim 24.7$) is offset by $\sim 4$ arcsec ($\sim 34$ kpc) with chance-alignment probability $P_{cc} \sim 0.2$, and current imaging does not exclude a fainter, near-coincident host. SED fitting of the candidate host suggests a low-mass galaxy. GRB 250818B highlights the power of rapid wide-field counterpart identification in the SVOM era, while host-association uncertainty can still limit offset-based interpretation. |
| 2026-02-18 | [Quantum Estimation Theory Limits in Neutrino Oscillation Experiments](http://arxiv.org/abs/2602.16534v1) | Claudia Frugiuele, Marco G. Genoni et al. | Measurements of the Pontecorvo-Maki-Nakagawa-Sakata (PMNS) neutrino mixing parameters have entered a precision era, enabling increasingly stringent tests of neutrino oscillations. Within the framework of quantum estimation theory, we investigate whether flavor measurements, the only observables currently accessible experimentally, are optimal for extracting the oscillation parameters. We compute the Quantum Fisher Information (QFI) and the classical Fisher Information (FI) associated with ideal flavor projections for all oscillation parameters, considering accelerator muon (anti)neutrino and reactor electron antineutrino beams propagating in vacuum. Two main results emerge. First, flavor measurements saturate the QFI at the first oscillation maximum for $Œ∏_{13}$, $Œ∏_{23}$, and $Œ∏_{12}$, demonstrating their information-theoretic optimality for these parameters. In contrast, they are far from optimal for $Œ¥_{CP}$. In particular, only a small fraction of the available information on $Œ¥_{CP}$ is extracted at the first maximum; the sensitivity improves at the second maximum, in line with the strategy of ESS$ŒΩ$SB, a planned facility. Second, the QFI associated with $Œ¥_{CP}$ is approximately one order of magnitude smaller than that of the mixing angles, indicating that the neutrino state intrinsically encodes less information about CP violation. Nevertheless, this quantum bound lies well below current experimental uncertainties, implying that the present precision on $Œ¥_{CP}$ is not fundamentally limited. Our results provide a quantitative framework to disentangle fundamental from practical limitations and establish a benchmark for optimizing future neutrino facilities. |
| 2026-02-18 | [Model selection confidence sets for time series models with applications to electricity load data](http://arxiv.org/abs/2602.16527v1) | Piersilvio De Bortoli, Davide Ferrari et al. | This paper studies the Model Selection Confidence Set (MSCS) methodology for univariate time series models involving autoregressive and moving average components, and applies it to study model selection uncertainty in the Italian electricity load data. Rather than relying on a single model selected by an arbitrary criterion, the MSCS identifies a set of models that are statistically indistinguishable from the true data-generating process at a given confidence level. The size and composition of this set reveal crucial information about model selection uncertainty: noisy data scenarios produce larger sets with many candidate models, while more informative cases narrow the set considerably. To study the importance of each model term, we consider numerical statistics measuring the frequency with which each term is included in both the entire MSCS and in Lower Boundary Models (LBM), its most parsimonious specifications. Applied to Italian hourly electricity load data, the MSCS methodology reveals marked intraday variation in model selection uncertainty and isolates a collection of model specifications that deliver competitive short-term forecasts while highlighting key drivers of electricity load like intraday hourly lags, temperature, calendar effects and solar energy generation. |
| 2026-02-18 | [Generative deep learning improves reconstruction of global historical climate records](http://arxiv.org/abs/2602.16515v1) | Zhen Qian, Teng Liu et al. | Accurate assessment of anthropogenic climate change relies on historical instrumental data, yet observations from the early 20th century are sparse, fragmented, and uncertain. Conventional reconstructions rely on disparate statistical interpolation, which excessively smooths local features and creates unphysical artifacts, leading to systematic underestimation of intrinsic variability and extremes. Here, we present a unified, probabilistic generative deep learning framework that overcomes these limitations and reveals previously unresolved historical climate variability back to 1850. Leveraging a learned generative prior of Earth system dynamics, our model performs probabilistic inference to recover spatiotemporally consistent historical temperature and precipitation fields from sparse observations. Our approach preserves the higher-order statistics of climate dynamics, transforming reconstruction into a robust uncertainty-aware assessment. We demonstrate that our reconstruction overcomes pronounced biases in widely used historical reference products, including those underlying IPCC assessments, especially regarding extreme weather events. Notably, we uncover higher early 20th-century global warming levels compared to existing reconstructions, primarily driven by more pronounced polar warming, with mean Arctic warming trends exceeding established benchmarks by 0.15--0.29¬∞C per decade for 1900--1980. Conversely, for the modern era, our reconstruction indicates that the broad Arctic warming trend is likely overestimated in recent assessments, yet explicitly resolves previously unrecognized intense, localized hotspots in the Barents Sea and Northeastern Greenland. Furthermore, based on our seamless global reconstruction that recovers precipitation variability across the oceans and under-monitored regions, we uncover an intensification of the global hydrological cycle. |
| 2026-02-18 | [The Crusts of Neutron Stars Revisited: Approximations within a Polytropic Equation of State Approach](http://arxiv.org/abs/2602.16477v1) | F. K√∂pp, J. E. Horvath et al. | In this work, we revisit several thin-crust approximations presented in the literature and compare them with the exact solutions of the Tolman--Oppenheimer--Volkoff (TOV) equations. In addition, we employ three different equations of state (EoSs), including one with a pasta phase, each based on a distinct theoretical framework: the variational method, relativistic Brueckner--Hartree--Fock theory, and relativistic mean-field theory.   We emphasize that these approximations require only the TOV solutions for the core and the EoS properties at the core--crust interface; in our approach, only the energy density is needed. Finally, the relativistic approximation, as well as the Newtonian approximation with corrections, shows good agreement with the exact solutions. This indicates that a simple treatment of the crust is sufficient for structural purposes, independently of the uncertainties in the sub-nuclear equation of state, which are not very large.   The unified EoS SINPA (relativistic mean-field theory), including the pasta phase, was used to study the thin-crust approximation, while degeneracy in the $M$--$R$ relation is demonstrated through: (i) anisotropic pressure in the modified TOV equations, (ii) the $f(R, L_m, T)$ gravity model, and (iii) dark matter admixture. As demonstrated, modifications to the description of gravitation introduce degeneracies in the mass--radius relation that are challenging to disentangle or quantify precisely. |
| 2026-02-17 | [Deformed Heisenberg algebra and its Hilbert space representations](http://arxiv.org/abs/2602.15801v1) | Lat√©vi M. Lawson, Ibrahim Nonkan√© et al. | A deformation of Heisenberg algebra induces among other consequences a loss of Hermiticity of some operators that generate this algebra. Therefore, these operators are not Hermitian, nor is the Hamiltonian operator built from them. In the present paper, we propose a position deformation of Heisenberg algebra with both maximal length and minimal momentum uncertainties. By using a pseudo-similarity transformation to the non-Hermitian operators, we prove their Hermiticity with a suitable positive-definite pseudo-metric operator. We then construct Hilbert space representations associated with these pseudo-Hermitian operators. Finally, we study the eigenvalue problem of a free particle in this deformed space and we show that this deformation curved the quantum levels allowing particles to jump from one state to another with low energy transitions. |
| 2026-02-17 | [Development of an accurate formalism to predict properties of two-neutron halo nuclei: case study of $^{22}$C](http://arxiv.org/abs/2602.15765v1) | Patrick McGlynn, Chlo√´ Hebborn | When moving away from stability or in loosely-bound systems, few-body clusterized structures like two-neutron halo nuclei appear. These emerge from the interplay between the many- and few-body degrees of freedom, and/or strong coupling between bound and continuum states. This motivates the development of models that can accurately describe few-body dynamics while enforcing shell effects. This work has two goals: understanding how to accurately enforce the Pauli principle in few-body models, as well as presenting new technical developments that allow for more robust and cheaper three-body calculations. We focus on properties of the two-neutron halo 22C, but expect the conclusions to apply to other few-body systems. We use a three-body, hyperspherical harmonics formalism combined with the R-matrix method. We compare predictions for properties of 22C starting from phenomenological interactions and using two methods to remove Pauli-forbidden states, the projection and supersymmetric methods. We also present the algorithms and derivations used. Additionally, we explore model space truncations that allow for reduced computational time. We show convergence of the calculation of both bound and scattering states for $K_{max}\sim 40$. The two methods to enforce the Pauli-exclusion principle lead to different predictions of 22C properties; the projection method is more accurate. We find one efficient channel truncation that reduces the computational cost of our calculations by 20%. Our study clarifies that the projection method is more accurate than the supersymmetric one to enforce the Pauli-exclusion principle. Technical and algorithmic developments enable accurate and efficient computation of two-neutron halo properties. This development paves the way to robust uncertainty quantification in three-body predictions, and is a useful starting point to tackle more complex systems and observables. |
| 2026-02-17 | [RaCo: Ranking and Covariance for Practical Learned Keypoints](http://arxiv.org/abs/2602.15755v1) | Abhiram Shenoi, Philipp Lindenberger et al. | This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo. |
| 2026-02-17 | [How to Train a Shallow Ensemble](http://arxiv.org/abs/2602.15747v1) | Moritz Sch√§fer, Matthias Kellner et al. | Shallow ensembles provide a convenient strategy for uncertainty quantification in machine learning interatomic potentials, that is computationally efficient because the different ensemble members share a large part of the model weights. In this work, we systematically investigate training strategies for shallow ensembles to balance calibration performance with computational cost. We first demonstrate that explicit optimization of a negative log-likelihood (NLL) loss improves calibration with respect to approaches based on ensembles of randomly initialized models, or on a last-layer Laplace approximation. However, models trained solely on energy objectives yield miscalibrated force estimates. We show that explicitly modeling force uncertainties via an NLL objective is essential for reliable calibration, though it typically incurs a significant computational overhead. To address this, we validate an efficient protocol: full-model fine-tuning of a shallow ensemble originally trained with a probabilistic energy loss, or one sampled from the Laplace posterior. This approach results in negligible reduction in calibration quality compared to training from scratch, while reducing training time by up to 96%. We evaluate this protocol across a diverse range of materials, including amorphous carbon, ionic liquids (BMIM), liquid water (H$_2$O), barium titanate (BaTiO$_3$), and a model tetrapeptide (Ac-Ala3-NHMe), establishing practical guidelines for reliable uncertainty quantification in atomistic machine learning. |
| 2026-02-17 | [Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems](http://arxiv.org/abs/2602.15721v1) | Jingtian Yan, Yulun Zhang et al. | We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart. |
| 2026-02-17 | [Hot subdwarf stars from the Hamburg Quasar Survey](http://arxiv.org/abs/2602.15692v1) | Ulrich Heber, Lennard Kufleitner et al. | Hot subluminous stars (sdO/B) are evolved low mass stars originating from red giants that lost their envelope almost entirely. The multitude of observed phenomena imply that several pathways may form hot subdwarfs, most involving close binary channels. The Hamburg Quasar Survey (HQS) led to the discovery of many faint blue stars including hot subdwarf. Many of the HQS-sdB stars have been studied in detail, but analyses of the helium-rich sdOB and sdO stars are lacking. The recent development of hybrid LTE/non-LTE model spectra 2nd generation Bamberg model grids enables us to improve the spectroscopic analyses of the sdB stars as well as of the previously unstudied sdO stars allowing precise atmospheric parameters to be derived, while consistently accounting for parameter correlations and systematic uncertainties. ... We use spectral energy distributions to identify composite-colour sdB binaries and present the result of detailed spectroscopic analyses of 122 non-composite subdwarfs from the HQS to identify potential evolutionary pathways. ...Their derived mass distribution and median mass of 0.45 Msun is consistent with the canonical EHB mass. ... The helium-rich sdOB and sdO stars, are found near the helium main-sequence (He-MS). The derived mass distribution of the extremely He-rich subdwarfs is broader (0.48 to 1.05 Msun) and peaks at a median of 0.70 Msun, significantly larger than those of the hydrogen-rich stars. Intermediate He-rich subdwarfs are also He-MS stars, but of lower mass (0.55 Msun) than the extremely He-rich. This strongly supports the merger scenario for the origin of He-rich sdO stars, in which two helium white dwarfs merge following orbital decay driven by gravitational-wave emission, producing a He-rich sdO or sdOB star. From comparison to the results of similar studies we speculate that older populations produce more massive He-WD mergers.(abbreviated) |
| 2026-02-17 | [Dosimetric Study of Lung Modulation and Motion Effects in Carbon ion Therapy for Lung Cancer](http://arxiv.org/abs/2602.15672v1) | Maria Chiara Martire, Lennart Volz et al. | Carbon-ion radiotherapy provides high dose conformity for lung cancer, but its benefit is limited by two sources of uncertainties: interplay between scanned beam delivery and tumor motion, and dose modulation from heterogeneous lung tissue. This study quantifies the separate and combined dosimetric impact of these effects using the GSI TRiP4D treatment planning system. Eighteen lung cancer 4DCT datasets from TCIA were analyzed. A modulation power ($P_{\mathrm{mod}}$) was assigned to lung voxels. Three values were sampled from a Gaussian distribution ($200Œº\mathrm{m} \pm 67Œº\mathrm{m}$), and an extreme value of $750Œº\mathrm{m}$ was tested. Interplay doses were computed by combining scanned-beam delivery with patient-specific respiratory motion. Four scenarios were studied: static, static with modulation, interplay, and interplay with modulation. Metrics included $D95\%$, $V95\%$, homogeneity index (HI), lung $V16\mathrm{Gy}$, and heart $V20\mathrm{Gy}$. Interplay reduced target coverage by $5.2 \pm 1.5$ pp ($D95\%$), $12.1 \pm 5.9$ pp ($V95\%$), and $8.3 \pm 2.4$ pp (HI). Extreme $P_{\mathrm{mod}}$ alone caused small degradations. When combined with interplay, it partially compensated the loss. This effect decreased with 4D optimization. Fractionation mitigated interplay, leaving lung modulation as the main residual effect. |
| 2026-02-17 | [Reinforcement Learning in Real Option Models](http://arxiv.org/abs/2602.15643v1) | Jodi Dianetti, Giorgio Ferrari et al. | We investigate an entropy-regularized reinforcement learning (RL) approach to optimal stopping problems motivated by real option models. Classical stopping rules are strict and non-randomized, limiting natural exploration in RL settings. To address this, we introduce entropy regularization, allowing randomized stopping policies that balance exploitation and exploration. We derive an explicit analytical solution to the regularized problem and prove convergence of the associated free boundary to the classical stopping threshold as the entropy vanishes. The regularized problem admits a natural formulation as a singular stochastic control problem. Building on this structure, we propose both model-based and model-free policy iteration algorithms to learn the optimal boundary. The model-free method operates without knowledge of system dynamics, using only trajectories from the stochastic environment. We establish convergence guarantees and illustrate strong numerical performance. This framework provides a principled and tractable approach for data-driven stopping problems under uncertainty. |
| 2026-02-17 | [Searching for Axion-like particle Dark Matter with Time-domain Polarization: Constraints from a protoplanetary disk](http://arxiv.org/abs/2602.15611v1) | Kanako Narita, Tomohiro Fujita et al. | Axion-like particles (ALPs) can induce a birefringence effect that rotates the polarization angle of light, offering a probe of ultralight dark matter. We analyze archival near-infrared polarimetric data of the protoplanetary disk (PPD) around HD 163296. Whereas previous studies considered only single-epoch snapshots, we perform a consistent multi-epoch time-series analysis, extracting the polarization angle and its uncertainty from the polarized images. The resulting six-epoch time series is consistent with a constant polarization angle within the measurement uncertainties, while being sensitive to timescales of $\sim 170-400$ days. The typical polarization angle uncertainties are $1.6$--$6.4$ degrees, partly driven by multiple scattering in the optically thick disk, which broadens the intrinsic polarization angle distribution and introduces additional dispersion in the representative polarization angle. Based on these data, we derive the first upper limits on the ALP-photon coupling from PPD polarization variability, $g_{aŒ≥} \lesssim 7.5 \times 10^{-12} (m_a / 10^{-22}\,{\rm eV})\,{\rm GeV}^{-1}$. Furthermore, we forecast that achieving a polarization angle uncertainty of $œÉ\sim 0.1$ degrees would enable world-leading sensitivity to ALP-induced birefringence. |
| 2026-02-17 | [Efficient Road Renovation Scheduling under Uncertainty using Lower Bound Pruning](http://arxiv.org/abs/2602.15554v1) | Robbert Bosch, Patricia Rogetzer et al. | Urban infrastructure degrades over time, necessitating periodic renovation to maintain functionality and safety. When renovation is delayed beyond the infrastructure's remaining lifespan, costly emergency interventions become necessary to prevent failure. Decision makers must therefore balance expected emergency intervention costs against traffic congestion impacts. We formalize this trade-off as a road network maintenance scheduling problem with uncertain deadlines, which presents optimization challenges including computationally expensive evaluation and an exponentially growing solution space. To address these challenges, this paper contributes a hybrid optimization approach combining machine learning with genetic algorithms for large-scale infrastructure renovation scheduling under uncertainty. We formulate the problem as a bi-level multi-objective optimization problem that explicitly accounts for uncertain infrastructure lifespans through probabilistic failure models. We develop a progressive lower bound evaluation method that integrates machine learning surrogate models with a multi-objective genetic algorithm to improve solution quality by enabling more iterations within fixed computational budgets. We demonstrate the method's effectiveness on substantially larger problem instances (76 projects) than previously addressed in the literature, achieving statistically significant improvements across multiple performance metrics by increasing computational efficiency up to 40 times compared to standard approaches. |
| 2026-02-16 | [Sub-part-per-trillion test of the Standard Model with atomic hydrogen](http://arxiv.org/abs/2602.14980v1) | Lothar Maisenbacher, Vitaly Wirthl et al. | Quantum electrodynamics (QED), the first relativistic quantum field theory, describes light-matter interactions at a fundamental level and is one of the pillars of the Standard Model (SM). Through the extraordinary precision of QED, the SM predicts the energy levels of simple systems such as the hydrogen atom with up to 13 significant digits, making hydrogen spectroscopy an ideal test bed. The consistency of physical constants extracted from different transitions in hydrogen using QED, such as the proton charge radius $r_\mathrm{p}$, constitutes a test of the theory. However, values of $r_\mathrm{p}$ from recent measurements of atomic hydrogen are partly discrepant with each other and with a more precise value from spectroscopy of muonic hydrogen. This prevents a test of QED at the level of experimental uncertainties. Here we present a measurement of the 2S-6P transition in atomic hydrogen with sufficient precision to distinguish between the discrepant values of $r_\mathrm{p}$ and enable rigorous testing of QED and the SM overall. Our result $ŒΩ^{}_{\text{2S-6P}}$ = 730,690,248,610.79(48) kHz gives a value of $r_\mathrm{p}$ = 0.8406(15) fm at least 2.5-fold more precise than from other atomic hydrogen determinations and in excellent agreement with the muonic value. The SM prediction of the transition frequency (730,690,248,610.79(23) kHz) is in excellent agreement with our result, testing the SM to 0.7 parts per trillion (ppt) and, specifically, bound-state QED corrections to 0.5 parts per million (ppm), their most precise test so far. |
| 2026-02-16 | [Kalman Filtering Based Flight Management System Modeling for AAM Aircraft](http://arxiv.org/abs/2602.14948v1) | Balram Kandoria, Aryaman Singh Samyal | Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations. |
| 2026-02-16 | [Activation-Space Uncertainty Quantification for Pretrained Networks](http://arxiv.org/abs/2602.14934v1) | Richard Bergna, Stefan Depeweg et al. | Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time. |
| 2026-02-16 | [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](http://arxiv.org/abs/2602.14913v1) | Farbod Siahkali, Ashwin Verma et al. | Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes. |
| 2026-02-16 | [Tarnished by Tools: Cost of Systematics in Golden Dark Siren Cosmology](http://arxiv.org/abs/2602.14898v1) | Giovanni Benetti, Koustav Chandra et al. | Golden dark sirens - exceptionally well-localized gravitational-wave (GW) sources without electromagnetic counterparts - offer a powerful route to precision measurements of the Hubble constant, $H_0$, with next-generation (XG) detectors. The statistical promise of this method, however, places stringent demands on waveform accuracy and detector calibration, as even small systematic errors can dominate over statistical uncertainties at high signal-to-noise ratios. We investigate the impact of waveform-modeling systematics on golden dark siren cosmology using a synthetic population of binary black holes consistent with current GW observations and analyzed in the XG-detector era. By comparing state-of-the-art waveform models against numerical-relativity-based reference signals, we quantify modeling inaccuracies from both modeling and data-analysis perspectives and assess how they propagate into biases in luminosity distance, host-galaxy association, and single-event $H_0$ inference. We find that while current waveform models often allow recovery of statistically consistent $H_0$ posteriors, small waveform-induced biases can significantly affect three-dimensional localization and host galaxy ranking, occasionally leading to incorrect redshift assignments. We further derive order-of-magnitude requirements on detector calibration accuracy needed to ensure that calibration systematics remain subdominant for golden dark sirens observed with XG networks. To realize sub-percent $H_0$ measurements with golden dark sirens will require waveform and calibration accuracies that scale as $\mathcal{O}(œÅ^{-2})$ with signal-to-noise ratio, motivating sustained advances in waveform modeling, numerical relativity, and detector calibration for the XG era. |
| 2026-02-16 | [Recalibrating the Sensitivities of the STIS First-Order, Medium-Resolution Modes](http://arxiv.org/abs/2602.14891v1) | Alex Fullerton | The sensitivities of STIS first-order, medium resolution modes were redetermined from on-orbit observations and CALSPEC models (version 11) of the primary white-dwarf spectrophotometric standard stars G191-B2B, GD 71, and GD 153. The sensitivity of an additional configuration was updated by comparing observations of the secondary standard BD +75 325 with the STIS low-resolution spectrum that has been calibrated consistently with the version 11 models. The procedures used to derive the sensitivities and verify the PHOTTAB reference files prior to their activation in CRDS (on May 1, 2025) are described. Results are presented in graphical form in an extensive appendix. Issues and uncertainties are discussed briefly, along with recommendations for future work. |
| 2026-02-16 | [The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling](http://arxiv.org/abs/2602.14862v1) | Pierre-Alexandre Mattei, Bruno Loureiro | Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model. |
| 2026-02-16 | [High-precision measurement of $^{215}$Po half-life via delayed-coincidence analysis](http://arxiv.org/abs/2602.14829v1) | Lorenzo Ascenzo, Melissa Hoda Baiocchi et al. | We performed a high-precision study of the $^{215}$ Po $Œ±$-decay using a LaBr$_3$ scintillating detector in a low-background environment. The $^{227}$Ac intrinsic contamination in the LaBr$_3$ crystal undergoes a decay chain, producing the intermediate pair of $^{219}$Rn$\rightarrow^{215}$Po$\rightarrow^{211}$Pb decays. The fast time response and good energy resolution of the detector allow for extracting the short half-life of $^{215}$Po from the time correlation of the two subsequent $Œ±$-decays by using the delayed coincidence method. Thanks to high statistics and a comprehensive uncertainty assessment, we obtain the most precise half-life value to date of $^{215}$Po, corresponding to $1.77804\pm0.00091$(stat.)$\pm0.00067$(syst.) ms. |
| 2026-02-16 | [The impact of the formation channel on gravitational-wave-galaxy cross-correlations](http://arxiv.org/abs/2602.14825v1) | Kabir Chakravarti, Federico R. Urban | The angular, harmonic cross-correlation between gravitational wave (GW) events and galaxy catalogues contains rich information on the large-scale structure and the origin of compact binary mergers. In this work, we study how uncertainties in the binary formation channel affect the predicted cross-correlation signal for both current-generation and next-generation networks of detectors. We generate five mock GW catalogues for which we vary the progenitor-to-remnant mass-transfer function and the time-delay probability distribution between progenitor and remnant. We then cross-correlate these catalogues with galaxy samples modelled on the 2MASS Photometric Redshift catalogue (2MPZ) and the Gaia-unWISE quasar catalogue (Quaia). We find that the mass-transfer function has negligible effect on the cross-correlation signal, with differences remaining within redshift uncertainties. In contrast, the time-delay distribution dramatically affects the redshift distribution of the GW events and, with it, the cross-correlation signal, particularly for shallow galaxy catalogues. In particular, current-generation facilities can achieve significant detections only for the longest time delays when cross-correlated with 2MPZ, whilst all cross-correlations with the deeper Quaia catalogue are marginally detectable or consistent with zero. Our exploratory results thus demonstrate that forecasts on cosmological or astrophysical parameters derived from GW-galaxy cross-correlations are, as expected, strongly sensitive to the assumed binary formation history. |
| 2026-02-16 | [The empirical distribution of sequential LS factors in Multi-level Dynamic Factor Models](http://arxiv.org/abs/2602.14813v1) | Gian Pietro Bellocca, Ignacio Garr√≥n et al. | The research question we answer in this paper is whether the asymptotic distribution derived by Bai (2003) for Principal Components (PC) factors in dynamic factor models (DFMs) can approximate the empirical distribution of the sequential Least Squares (SLS) estimator of global and group-specific factors in multi-level dynamic factor models (ML-DFMs). Monte Carlo experiments confirm that under general forms of the idiosyncratic covariance matrix, the finite-sample distribution of SLS global and group-specific factors can be well approximated using the asymptotic distribution of PC factors. We also analyse the performance of alternative estimators of the asymptotic mean squared error (MSE) of the SLS factors and show that the MSE estimator that allows for idiosyncratic cross-sectional correlation and accounts for estimation uncertainty of factor loadings is best. |
| 2026-02-13 | [Profiling systematic uncertainties in Simulation-Based Inference with Factorizable Normalizing Flows](http://arxiv.org/abs/2602.13184v1) | Davide Valsecchi, Mauro Doneg√† et al. | Unbinned likelihood fits aim at maximizing the information one can extract from experimental data, yet their application in realistic statistical analyses is often hindered by the computational cost of profiling systematic uncertainties. Additionally, current machine learning-based inference methods are typically limited to estimating scalar parameters in a multidimensional space rather than full differential distributions. We propose a general framework for Simulation-Based Inference (SBI) that efficiently profiles nuisance parameters while measuring multivariate Distributions of Interest (DoI), defined as learnable invertible transformations of the feature space. We introduce Factorizable Normalizing Flows to model systematic variations as parametric deformations of a nominal density, preserving tractability without combinatorial explosion. Crucially, we develop an amortized training strategy that learns the conditional dependence of the DoI on nuisance parameters in a single optimization process, bypassing the need for repetitive training during the likelihood scan. This allows for the simultaneous extraction of the underlying distribution and the robust profiling of nuisances. The method is validated on a synthetic dataset emulating a high-energy physics measurement with multiple systematic sources, demonstrating its potential for unbinned, functional measurements in complex analyses. |
| 2026-02-13 | [Optimal Take-off under Fuzzy Clearances](http://arxiv.org/abs/2602.13166v1) | Hugo Henry, Arthur Tsai et al. | This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments. |
| 2026-02-13 | [SCOPE: Selective Conformal Optimized Pairwise LLM Judging](http://arxiv.org/abs/2602.13110v1) | Sher Badshah, Ali Emami et al. | Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $Œ±$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $Œ±= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na√Øve baselines, \textsc{Scope} accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation. |
| 2026-02-13 | [Barron-Wiener-Laguerre models](http://arxiv.org/abs/2602.13098v1) | Rahul Manavalan, Filip Tronarp | We propose a probabilistic extension of Wiener-Laguerre models for causal operator learning. Classical Wiener-Laguerre models parameterize stable linear dynamics using orthonormal Laguerre bases and apply a static nonlinear map to the resulting features. While structurally efficient and interpretable, they provide only deterministic point estimates. We reinterpret the nonlinear component through the lens of Barron function approximation, viewing two-layer networks, random Fourier features, and extreme learning machines as discretizations of integral representations over parameter measures. This perspective naturally admits Bayesian inference on the nonlinear map and yields posterior predictive uncertainty. By combining Laguerre-parameterized causal dynamics with probabilistic Barron-type nonlinear approximators, we obtain a structured yet expressive class of causal operators equipped with uncertainty quantification. The resulting framework bridges classical system identification and modern measure-based function approximation, providing a principled approach to time-series modeling and nonlinear systems identification. |
| 2026-02-13 | [UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph](http://arxiv.org/abs/2602.13086v1) | Haichao Liu, Yuanjiang Xue et al. | Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip. |
| 2026-02-13 | [Agentic AI for Robot Control: Flexible but still Fragile](http://arxiv.org/abs/2602.13081v1) | Oscar Lima, Marc Vinci et al. | Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API. |
| 2026-02-13 | [Photometric classification of supernovae detected by the Zwicky Transient Facility using noise augmentation](http://arxiv.org/abs/2602.13036v1) | A. Townsend, J. Nordin et al. | Modern time-domain surveys, such as the Zwicky Transient Facility (ZTF), detect far more extragalactic transients than can be spectroscopically classified. Photometric classification offers a scalable alternative, enabling the identification of larger, fainter, and higher-redshift supernova samples suitable for applications such as Type Ia supernova (SN Ia) cosmology. We present a feature-based photometric classifier for SNe detected by ZTF, with the primary goal of constructing a photometric SN Ia sample for cosmological analyses. Our approach utilises the autoencoder architecture of ParSNIP (Boone 2021) to capture the intrinsic diversity of SN light curves. We trained the model on a spectroscopically classified ZTF SN sample, incorporating a realistic noise augmentation procedure that simulates the flux uncertainties of fainter sources. Light curve features were used to train a gradient-boosted decision tree classifier, implemented in both binary (SN Ia vs. non-Ia) and multi-class configurations. We validated our classifier on independent, fainter ZTF data with and without noise augmentation. To evaluate real-time performance, we also applied our classifier to live ZTF alerts and conducted a spectroscopic classification survey within the ePESSTO+ collaboration. We found that noise augmentation significantly improves classification performance, particularly for fainter sources. Our binary classifier achieves an SN Ia recall of (98.1 $\pm$ 0.4)%, averaged across five train-test splits. SN Ia recall exceeds 98% for events with a peak apparent magnitude up to 20 and more than 10 detections, and remains above 96% up to magnitude 20.5. Overall, 95% of sources were correctly classified in both binary and multi-class modes. Our classifier performs efficiently on real ZTF data and enables construction of a large photometric SN Ia sample for cosmology. |
| 2026-02-13 | [Look Inward to Explore Outward: Learning Temperature Policy from LLM Internal States via Hierarchical RL](http://arxiv.org/abs/2602.13035v1) | Yixiao Zhou, Yang Li et al. | Reinforcement Learning from Verifiable Rewards (RLVR) trains large language models (LLMs) from sampled trajectories, making decoding strategy a core component of learning rather than a purely inference-time choice. Sampling temperature directly controls the exploration--exploitation trade-off by modulating policy entropy, yet existing methods rely on static values or heuristic adaptations that are decoupled from task-level rewards. We propose Introspective LLM, a hierarchical reinforcement learning framework that learns to control sampling temperature during generation. At each decoding step, the model selects a temperature based on its hidden state and samples the next token from the resulting distribution. Temperature and token policies are jointly optimized from downstream rewards using a coordinate ascent scheme. Experiments on mathematical reasoning benchmarks show that learned temperature policies outperform fixed and heuristic baselines, while exhibiting interpretable exploration behaviors aligned with reasoning uncertainty. |
| 2026-02-13 | [Bayesian Optimization Based Grid Point Allocation for LPV and Robust Control](http://arxiv.org/abs/2602.13009v1) | E. Javier Olucha, Arash Sadeghzadeh et al. | This paper investigates systematic selection of optimal grid points for grid-based Linear Parameter-Varying (LPV) and robust controller synthesis. In both settings, the objective is to identify a set of local models such that the controller synthesized for these local models will satisfy global stability and performance requirements for the entire system. Here, local models correspond to evaluations of the LPV or uncertain plant at fixed values of the scheduling signal or realizations of the uncertainty set, respectively. Then, Bayesian optimization is employed to discover the most informative points that govern the closed-loop performance of the designed LPV or robust controller for the complete system until no significant further performance increase or a user specified limit is reached. Furthermore, when local model evaluations are computationally demanding or difficult to obtain, the proposed method is capable to minimize the number of evaluations and adjust the overall computational cost to the available budget. Lastly, the capabilities of the proposed method in automatically obtaining a sufficiently informative grid set are demonstrated on three case-studies: a robust controller design for an unbalanced disk, a multi-objective robust attitude controller design for a satellite with uncertain parameters and two flexible rotating solar arrays, and an LPV controller design for a robotic arm. |
| 2026-02-13 | [Uncertainty in Federated Granger Causality: From Origins to Systemic Consequences](http://arxiv.org/abs/2602.13004v1) | Ayush Mohanty, Nazal Mohamed et al. | Granger Causality (GC) provides a rigorous framework for learning causal structures from time-series data. Recent federated variants of GC have targeted distributed infrastructure applications (e.g., smart grids) with distributed clients that generate high-dimensional data bound by data-sovereignty constraints. However, Federated GC algorithms only yield deterministic point estimates of causality and neglect uncertainty. This paper establishes the first methodology for rigorously quantifying uncertainty and its propagation within federated GC frameworks. We systematically classify sources of uncertainty, explicitly differentiating aleatoric (data noise) from epistemic (model variability) effects. We derive closed-form recursions that model the evolution of uncertainty through client-server interactions and identify four novel cross-covariance components that couple data uncertainties with model parameter uncertainties across the federated architecture. We also define rigorous convergence conditions for these uncertainty recursions and obtain explicit steady-state variances for both server and client model parameters. Our convergence analysis demonstrates that steady-state variances depend exclusively on client data statistics, thus eliminating dependence on initial epistemic priors and enhancing robustness. Empirical evaluations on synthetic benchmarks and real-world industrial datasets demonstrate that explicitly characterizing uncertainty significantly improves the reliability and interpretability of federated causal inference. |
| 2026-02-12 | [Agentic Test-Time Scaling for WebAgents](http://arxiv.org/abs/2602.12276v1) | Nicholas Lee, Lutfi Eren Erdogan et al. | Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule. |
| 2026-02-12 | [Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks](http://arxiv.org/abs/2602.12244v1) | Zhihong Liu, Yang Li et al. | Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-horizon planning in large environments given ambiguous human instructions. At its core, AHAT utilizes an LLM trained to map task instructions and textual scene graphs into grounded subgoals defined in the Planning Domain Definition Language (PDDL). These subgoals are subsequently solved to generate feasible and optimal long-horizon plans through explicit symbolic reasoning. To enhance the model's ability to decompose complex and ambiguous intentions, we introduce TGPO, a novel reinforcement learning algorithm that integrates external correction of intermediate reasoning traces into Group Relative Policy Optimization (GRPO). Experiments demonstrate that AHAT achieves significant performance gains over state-of-the-art prompting, planning, and learning methods, particularly in human-style household tasks characterized by brief instructions but requiring complex execution plans. |
| 2026-02-12 | [Status of the $S_8$ Tension: A 2026 Review of Probe Discrepancies](http://arxiv.org/abs/2602.12238v1) | Ioannis Pantos, Leandros Perivolaropoulos | The parameter $S_8 \equiv œÉ_8 (Œ©_m/0.3)^{0.5}$ quantifies the amplitude of matter density fluctuations. A persistent discrepancy exists between early-universe CMB observations and late-universe probes. This review assesses the ``$S_8$ tension'' against a new 2026 baseline: a unified ``Combined CMB'' framework incorporating Planck, ACT DR6, and SPT-3G. This combined analysis yields $S_8 = 0.836^{+0.012}_{-0.013}$, providing a higher central value and reduced uncertainties compared to Planck alone. Compiling measurements from 2019--2026, we reveal a striking bifurcation: DES Year 6 results exhibit a statistically significant tension of $2.4œÉ$--$2.7œÉ$ \citep{DESY6}, whereas KiDS Legacy results demonstrate statistical consistency at $<1œÉ$ \citep{Wright2025}. We examine systematic origins of this dichotomy, including photometric redshift calibration, intrinsic alignment modeling, and shear measurement pipelines. We further contextualize these findings with cluster counts (where eROSITA favors high values while SPT favors low), galaxy-galaxy lensing, and redshift-space distortions. The heterogeneous landscape suggests survey-specific systematic effects contribute substantially to observed discrepancies, though new physics beyond $Œõ$CDM cannot be excluded. |
| 2026-02-12 | [Bandit Learning in Matching Markets with Interviews](http://arxiv.org/abs/2602.12224v1) | Amirmahdi Mirfakhar, Xuchuang Wang et al. | Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \textit{low-cost hints} that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm's action space to allow \emph{strategic deferral} (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination. We design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the $O(\log T)$ regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms. |
| 2026-02-12 | [Probing baryonic feedback with fast radio bursts: joint analyses with cosmic shear and galaxy clustering](http://arxiv.org/abs/2602.12174v1) | Amy Wayland, David Alonso et al. | Cosmological inference from weak lensing (WL) surveys is increasingly limited by uncertainties in baryonic physics, which suppress the non-linear matter power spectrum on small scales. Multi-probe analyses that incorporate complementary tracers of the gas distribution around haloes offer a pathway to calibrate these effects and recover unbiased cosmological information. In this work, we forecast the constraining power of a joint analysis combining fiducial data from a Stage-IV WL survey with measurements of the dispersion measure from fast radio bursts (FRBs). We evaluate the ability of this approach to simultaneously constrain cosmological parameters and the astrophysical processes governing baryonic feedback, and we quantify the impact of key FRB systematics, including redshift uncertainties and source clustering. We find that, even after accounting for these effects, a 3$\times$2-point analysis of WL and FRBs significantly improves cosmological constraints, reducing the degradation factor on $S_8$ by $\sim 80\%$ compared to WL alone. We further show that FRBs alone are sensitive only to a degenerate combination of the key baryonic parameters, $\log_{10} M_{\rm c}$ and $Œ∑_{\rm b}$, and that the inclusion of WL measurements breaks this degeneracy. Finally, we extend our framework to incorporate galaxy clustering measurements using Luminous Red Galaxy and Emission Line Galaxy samples, performing a unified 6$\times$2-point analysis of WL, dispersion measures of FRBs, and galaxy clustering. While this combined approach tightens constraints on $Œ©_{\rm m}$ and $\log_{10} M_{\rm c}$, it does not lead to a significant improvement in $S_8$ constraints beyond those obtained from WL and FRBs alone. |
| 2026-02-12 | [STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction](http://arxiv.org/abs/2602.12143v1) | Xiaoxiao Wang, Chunxiao Li et al. | As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model. |
| 2026-02-12 | [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](http://arxiv.org/abs/2602.12087v1) | Alfredo Reichlin, Adriano Pacciarelli et al. | Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making. |
| 2026-02-12 | [Cosmographic Connection Between Cosmological And Planck Scales: The Barrow-Tsallis Entropy](http://arxiv.org/abs/2602.12077v1) | Yu. L. Bolotin, V. V. Yanovsky et al. | One of the fundamental challenges of quantum gravity is to understand how the microscopic degrees of freedom of the cosmological horizon shape the evolution of the Universe. One possible approach to this problem is based on the Barrow--Tsallis entropy. This entropy accounts for both quantum gravitational effects and the nonextensive effects inherent in any long-range interaction. Using a general method we developed for finding the parameters of cosmological models, we discovered a relationship between the parameter describing the microscopic structure of quantum foam and the parameter associated with macroscopic nonextensive effects. We also used our method for finding the parameters of cosmological models to evaluate the feasibility of using fractional derivatives to describe the late evolution of the Universe. The resulting relationships are exact. Therefore, the uncertainty in the relationship between the model parameters depends only on the current uncertainty in the values of the cosmographic parameters. |
| 2026-02-12 | [Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning](http://arxiv.org/abs/2602.12065v1) | Xiang Liu, Sen Cui et al. | Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning. |
| 2026-02-12 | [Time-Inhomogeneous Volatility Aversion for Financial Applications of Reinforcement Learning](http://arxiv.org/abs/2602.12030v1) | Federico Cacciamani, Roberto Daluiso et al. | In finance, sequential decision problems are often faced, for which reinforcement learning (RL) emerges as a promising tool for optimisation without the need of analytical tractability. However, the objective of classical RL is the expected cumulated reward, while financial applications typically require a trade-off between return and risk. In this work, we focus on settings where one cares about the time split of the total return, ruling out most risk-aware generalisations of RL which optimise a risk measure defined on the latter. We notice that a preference for homogeneous splits, which we found satisfactory for hedging, can be unfit for other problems, and therefore propose a new risk metric which still penalises uncertainty of the single rewards, but allows for an arbitrary planning of their target levels. We study the properties of the resulting objective and the generalisation of learning algorithms to optimise it. Finally, we show numerical results on toy examples. |
| 2026-02-11 | [Unmasking LHAASO J2108+5157: Near Infrared Insights into a Mysterious TeV Source](http://arxiv.org/abs/2602.11148v1) | Josep Mart√≠, Pedro L. Luque-Escamilla et al. | LHAASO J2108+5157 is one of the few ultra-high energy gamma-ray sources in the LHAASO catalogue without secure counterpart at longer wavelengths. Several Galactic scenarios have been proposed, including an evolved supernova remnant and a pulsar wind nebula. Yet, no shocked gas, shell-like structure, or compact pulsar candidate has been identified. Follow-up observations with VERITAS and the LST-1 prototype have not firmly clarified its nature. A recent microquasar candidate from GMRT radio data remains uncertain. Here we present the first dedicated near-infrared study of the field, combining deep JHKs imaging with narrow band observations targeting the H2 v=1-0 S(1) line. Our observations were initially planned to encompass the full source region, but now only partially cover the latest updated position and size of LHAASO J2108+5157. We find no evidence of shocked emission, extended nebular structures, or an accreting compact object signature in the covered field. The GMRT radio source, despite its jet-like morphology, exhibits near-infrared properties incompatible with both a Galactic microquasar and a nearby radio galaxy, discouraging an association with the gamma-ray emission. Our analysis reveals no convincing counterpart consistent within the positional uncertainty, leaving LHAASO J2108+5157 as an enigmatic ultra-high energy emitter that requires deeper observations. |
| 2026-02-11 | [Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](http://arxiv.org/abs/2602.11146v1) | Gongye Liu, Bo Yang et al. | Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment. |
| 2026-02-11 | [Statistical Learning Analysis of Physics-Informed Neural Networks](http://arxiv.org/abs/2602.11097v1) | David A. Barajas-Solano | We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $Œ¥(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs. |
| 2026-02-11 | [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](http://arxiv.org/abs/2602.11090v1) | Carlos Stein Brito | Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics. |
| 2026-02-11 | [A Robust Optimization Approach for Regenerator Placement in Fault-Tolerant Networks Under Discrete Cost Uncertainty](http://arxiv.org/abs/2602.11058v1) | Mohammad Khosravi, Setareh Maghsudi | We focus on robust, survivable communication networks, where network links and nodes are affected by an uncertainty set. In this sense, any network links might fail. Besides, a signal can only travel a maximum distance before its quality falls below a certain threshold, necessitating its regeneration by regenerators installed at network nodes. In addition, the price of installing and maintaining regenerators belongs to a discrete uncertainty set. Robust optimization seeks a solution with guaranteed performance against all scenarios modeled in an uncertainty set. Thus, the problem is to find a subset of nodes with minimum cost for the placement of the regenerator, ensuring that all nodes can communicate even if a subset of network links fails. To solve the problem optimally, we propose two solution approaches, including one flow-based and one cut-based integer programming formulation, as well as their iterative exact method. Our theoretical and experimental results show the effectiveness of our methods. |
| 2026-02-11 | [Bayesian inference for tidal heating with extreme mass ratio inspirals](http://arxiv.org/abs/2602.11039v1) | Zhong-Wu Xia, Sheng Long et al. | Extreme mass ratio inspirals (EMRIs) provide unique probes of near-horizon dissipation through the tidal heating. We present a full Bayesian analysis of tidal heating in equatorial eccentric EMRIs by performing injection-recovery studies and inferring posterior constraints on the reflectivity parameter $|\mathcal{R}|^2$ while sampling in the full EMRI parameter space. We find that in the strong-field regime the posterior uncertainties are smaller, indicating a stronger constraining capability on the tidal heating. Using two-year signals with an optimal signal-to-noise ratio (SNR) of $œÅ=50$, EMRIs can put bounds on $|\mathcal{R}|^2$ at the level of $10^{-3}$--$ 10^{-4}$ for a rapidly spinning central object. Moreover, we show that neglecting the tidal heating can induce clear systematic biases in the intrinsic parameters of the EMRI system. These results establish EMRIs as promising precision probes for detecting and constraining black hole event horizons. |
| 2026-02-11 | [Strong potential in a box for applications to femtoscopy](http://arxiv.org/abs/2602.11027v1) | Gleb Romanenko, Francesca Bellini | Understanding the short-range nucleon-nucleon interaction is essential for the interpretation of correlation femtoscopy measurements in high-energy hadronic and nuclear collisions. We present an analytical treatment of the strong interaction in two-nucleon systems by modelling it with a square-well potential and solving the Schroedinger equation in the presence of the Coulomb interaction. The resulting pair wave function is regular at small relative distances and allows for the inclusion of multiple partial waves. We apply this framework to proton-proton femtoscopy and compute theoretical correlation functions for realistic source sizes. We demonstrate that the commonly used Lednicky-Lyuboshits asymptotic approximation overestimates the correlation signal for small sources. Comparisons with numerical calculations using the CATS framework and the Argonne v18 potential show good agreement within current experimental uncertainties. The proposed analytical approach provides a practical and flexible tool for femtoscopic analyses of nucleon and baryon pairs. |
| 2026-02-11 | [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](http://arxiv.org/abs/2602.10953v1) | Mingyu Cao, Alvaro Correia et al. | Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding. |
| 2026-02-11 | [What do people want to fact-check?](http://arxiv.org/abs/2602.10935v1) | Bijean Ghafouri, Dorsaf Sallami et al. | Research on misinformation has focused almost exclusively on supply, asking what falsehoods circulate, who produces them, and whether corrections work. A basic demand-side question remains unanswered. When ordinary people can fact-check anything they want, what do they actually ask about? We provide the first large-scale evidence on this question by analyzing close to 2{,}500 statements submitted by 457 participants to an open-ended AI fact-checking system. Each claim is classified along five semantic dimensions (domain, epistemic form, verifiability, target entity, and temporal reference), producing a behavioral map of public verification demand. Three findings stand out. First, users range widely across topics but default to a narrow epistemic repertoire, overwhelmingly submitting simple descriptive claims about present-day observables. Second, roughly one in four requests concerns statements that cannot be empirically resolved, including moral judgments, speculative predictions, and subjective evaluations, revealing a systematic mismatch between what users seek from fact-checking tools and what such tools can deliver. Third, comparison with the FEVER benchmark dataset exposes sharp structural divergences across all five dimensions, indicating that standard evaluation corpora encode a synthetic claim environment that does not resemble real-world verification needs. These results reframe fact-checking as a demand-driven problem and identify where current AI systems and benchmarks are misaligned with the uncertainty people actually experience. |
| 2026-02-11 | [The metal-poor tail of the APOGEE survey II. Spectral analysis of Mg and Si in very metal-poor APOGEE spectra](http://arxiv.org/abs/2602.10883v1) | M. Montelius, A. Angrilli Muglia et al. | H-band spectra contain very limited spectral information for stars at the most metal-poor tail ( Fe/H < -2.5) because the available Fe lines in FGK stars in this wavelength range are weak. The first paper in this series successfully identified a sample of 327 very metal-poor stars (with [Fe/H] < -2) from the APOGEE database, 289 of which are on the red giant branch. The spectra of these stars were not properly analysed by the APOGEE main pipeline because they are very metal poor. In this work, we measure metallicities for these stars using the abundances of the elements Mg and Si. We demonstrate that the absorption lines of the elements Mg and Si are of good quality despite the challenging combination of (low) metallicity, wavelength regime, spectral resolution, and signal-to-noise ratios available for these spectra. A specialised pipeline was designed to measure the abundance of Mg and Si in APOGEE spectra and yielded a robust estimate of the overall metallicity. In order to provide reliable measurements, we tested three different sets of assumptions for Mg and Si enhancement. We present Mg and Si abundances as well as overall metallicities for 327 stars, all of which had previously gotten null values from the main APOGEE pipeline for either the calibrated M/H or [Fe/H] . The typical uncertainties for our measurements are 0.2 dex. We found five stars in our sample with unusual [Si/Mg] abundances higher than 0.5, and we connect this signature to globular cluster stars, and this might be related to specific supernova events. Our data suggest a concentration of high [Si/Mg] stars in the Sextans dwarf galaxy. Other dwarf galaxies are found to agree well with results in the literature. Our derived metallicities range between -3.1 $\leq$ [M/H] $\leq$ -2.25, thereby pushing the metal-poor tail of APOGEE results down by 0.6 dex. |
| 2026-02-10 | [Learning Agile Quadrotor Flight in the Real World](http://arxiv.org/abs/2602.10111v1) | Yunfan Ren, Zhiyuan Zhu et al. | Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes. |
| 2026-02-10 | [Dark Energy Survey Year 6 Results: Cosmological Constraints from Cosmic Shear](http://arxiv.org/abs/2602.10065v1) | DES Collaboration, T. M. C. Abbott et al. | We present legacy cosmic shear measurements and cosmological constraints using six years of Dark Energy Survey imaging data. From these data, we study ~140 million galaxies (8.29 galaxies/arcmin$^2$) that are 50% complete at i=24.0 and extend beyond z=1.2. We divide the galaxies into four redshift bins, and obtain cosmic shear measurement with a signal-to-noise of 83, a factor of 2 higher than the Year 3 analysis. We model the uncertainties due to shear and redshift calibrations, and discard measurements on small angular scales to mitigate baryon feedback and other small-scale uncertainties. We consider two fiducial models to account for the intrinsic alignment (IA) of the galaxies. We conduct a blind analysis in the context of the $Œõ$CDM model and find $S_8 \equiv œÉ_8(Œ©_m/0.3)^{0.5}=0.798^{+0.014}_{-0.015}$ (marginalized mean with 68% CL) when using the non-linear alignment model (NLA) and $S_{8} = 0.783^{+0.019}_{-0.015}$ with the tidal alignment and tidal torque model (TATT), providing 1.8% and 2.5% uncertainty on $S_8$. Compared to constraints from the cosmic microwave background from Planck 2018, ACT DR6 and SPT-3G DR1, we find consistency in the full parameter space at 1.1$œÉ$ (1.7$œÉ$) and in $S_8$ at 2.0$œÉ$ (2.3$œÉ$) for NLA (TATT). The result using the NLA model is preferred according to the Bayesian evidence. We find that the model choice for IA and baryon feedback can impact the value of our $S_8$ constraint up to $1œÉ$. For our fiducial model choices, the resultant uncertainties in $S_8$ are primarily degraded by the removal of scales, as well as the marginalization over the IA parameters. We demonstrate that our result is internally consistent and robust to different choices in calibrating the data, owing to methodological improvements in shear and redshift measurement, laying the foundation for next-generation cosmic shear programs. |
| 2026-02-10 | [Conformal Prediction Sets for Instance Segmentation](http://arxiv.org/abs/2602.10045v1) | Kerri Lu, Dan M. Kluger et al. | Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees. |
| 2026-02-10 | [Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning](http://arxiv.org/abs/2602.10044v1) | Akshay Mete, Shahid Aamir Sheikh et al. | Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts. |
| 2026-02-10 | [Drug Release Modeling using Physics-Informed Neural Networks](http://arxiv.org/abs/2602.09963v1) | Daanish Aleem Qureshi, Khemraj Shukla et al. | Accurate modeling of drug release is essential for designing and developing controlled-release systems. Classical models (Fick, Higuchi, Peppas) rely on simplifying assumptions that limit their accuracy in complex geometries and release mechanisms. Here, we propose a novel approach using Physics-Informed Neural Networks (PINNs) and Bayesian PINNs (BPINNs) for predicting release from planar, 1D-wrinkled, and 2D-crumpled films. This approach uniquely integrates Fick's diffusion law with limited experimental data to enable accurate long-term predictions from short-term measurements, and is systematically benchmarked against classical drug release models. We embedded Fick's second law into PINN as loss with 10,000 Latin-hypercube collocation points and utilized previously published experimental datasets to assess drug release performance through mean absolute error (MAE) and root mean square error (RMSE), considering noisy conditions and limited-data scenarios. Our approach reduced mean error by up to 40% relative to classical baselines across all film types. The PINN formulation achieved RMSE <0.05 utilizing only the first 6% of the release time data (reducing 94% of release time required for the experiments) for the planar film. For wrinkled and crumpled films, the PINN reached RMSE <0.05 in 33% of the release time data. BPINNs provide tighter and more reliable uncertainty quantification under noise. By combining physical laws with experimental data, the proposed framework yields highly accurate long-term release predictions from short-term measurements, offering a practical route for accelerated characterization and more efficient early-stage drug release system formulation. |
| 2026-02-10 | [Robust Processing and Learning: Principles, Methods, and Wireless Applications](http://arxiv.org/abs/2602.09848v1) | Shixiong Wang, Wei Dai et al. | This tutorial-style overview article examines the fundamental principles and methods of robustness, using wireless sensing and communication (WSC) as the narrative and exemplifying framework. First, we formalize the conceptual and mathematical foundations of robustness, highlighting the interpretations and relations across robust statistics, optimization, and machine learning. Key techniques, such as robust estimation and testing, distributionally robust optimization, and regularized and adversary training, are investigated. Together, the costs of robustness in system design, for example, the compromised nominal performances and the extra computational burdens, are discussed. Second, we review recent robust signal processing solutions for WSC that address model mismatch, data scarcity, adversarial perturbation, and distributional shift. Specific applications include robust ranging-based localization, modality sensing, channel estimation, receive combining, waveform design, and federated learning. Through this effort, we aim to introduce the classical developments and recent advances in robustness theory to the general signal processing community, exemplifying how robust statistical, optimization, and machine learning approaches can address the uncertainties inherent in WSC systems. |
| 2026-02-10 | [Stabilized Maximum-Likelihood Iterative Quantum Amplitude Estimation for Structural CVaR under Correlated Random Fields](http://arxiv.org/abs/2602.09847v1) | Alireza Tabarraei | Conditional Value-at-Risk (CVaR) is a central tail-risk measure in stochastic structural mechanics, yet its accurate evaluation under high-dimensional, spatially correlated material uncertainty remains computationally prohibitive for classical Monte Carlo methods. Leveraging bounded-expectation reformulations of CVaR compatible with quantum amplitude estimation, we develop a quantum-enhanced inference framework that casts CVaR evaluation as a statistically consistent, confidence-constrained maximum-likelihood amplitude estimation problem. The proposed method extends iterative quantum amplitude estimation (IQAE) by embedding explicit maximum-likelihood inference within a rigorously controlled interval-tracking architecture. To ensure global correctness under finite-shot noise and the non-injective oscillatory response induced by Grover amplification, we introduce a stabilized inference scheme incorporating multi-hypothesis feasibility tracking, periodic low-depth disambiguation, and a bounded restart mechanism governed by an explicit failure-probability budget. This formulation preserves the quadratic oracle-complexity advantage of amplitude estimation while providing finite-sample confidence guarantees and reduced estimator variance. The framework is demonstrated on benchmark problems with spatially correlated lognormal Young's modulus fields generated using a Nystrom low-rank Gaussian kernel model. Numerical results show that the proposed estimator achieves substantially lower oracle complexity than classical Monte Carlo CVaR estimation at comparable confidence levels, while maintaining rigorous statistical reliability. This work establishes a practically robust and theoretically grounded quantum-enhanced methodology for tail-risk quantification in stochastic continuum mechanics. |
| 2026-02-10 | [Study of $B^+ \to Œº^+ ŒΩ_Œº$ decays at Belle and Belle II](http://arxiv.org/abs/2602.09800v1) | Belle, Belle II Collaborations et al. | We report a measurement of the branching fraction for the leptonic decay $B^+\toŒº^+ŒΩ_Œº$. This work presents the first $B^+\toŒº^+ŒΩ_Œº$ result using Belle~II data, an updated Belle measurement that supersedes the previous result, and their combination, which yields the most precise search to date. The analysis is based on $1076\,\mathrm{fb}^{-1}$ of $e^+e^-$ collision data collected at a center-of-mass energy of $10.58\,\mathrm{GeV}$ with the Belle and Belle~II detectors at the KEKB and SuperKEKB colliders, respectively. We measure $\mathcal{B}(B^+\toŒº^+ŒΩ_Œº)=(4.4\pm1.9\pm 1.0)\times10^{-7}$, where the first uncertainty is statistical and the second systematic. The observed significance relative to the background-only hypothesis is 2.4 standard deviations. We set a 90\% confidence level upper limit of $\mathcal{B}(B^+\toŒº^+ŒΩ_Œº)<6.7\times10^{-7}$ using a frequentist approach and a 90\% credibility level upper limit of $\mathcal{B}(B^+\toŒº^+ŒΩ_Œº)<7.2\times 10^{-7}$ using a Bayesian approach. These are the most stringent limits to date. The result is interpreted as an exclusion region in the parameter space of type~II and type~III two-Higgs-doublet models. We search for stable sterile neutrinos with masses $m_N\in[0,1.5]\,\mathrm{GeV}$. No signal is observed, and the resulting exclusion on the squared mixing parameter $|U_{ŒºN}|^2$ provides improvement over previous limits. We report a measurement of the partial branching fraction of semileptonic $B\to X_u\ellŒΩ_\ell$ decays with $p_Œº^B>2.2\,\mathrm{GeV}$, obtaining $Œî\mathcal{B}(B\to X_u\ellŒΩ_\ell)=(2.72\pm0.05\pm0.29)\times10^{-4}$. We present a model-dependent study of weak annihilation decays using the muon momentum spectrum. We observe a signal of 2.4 standard deviations above the background-only hypothesis in regions where the distribution resembles that of $B\to X_u\ellŒΩ_\ell$ decays. |
| 2026-02-10 | [When Less is More: The LLM Scaling Paradox in Context Compression](http://arxiv.org/abs/2602.09789v1) | Ruishan Guo, Yibing Liu et al. | Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation. |
| 2026-02-10 | [DiffuReason: Bridging Latent Reasoning and Generative Refinement for Sequential Recommendation](http://arxiv.org/abs/2602.09744v1) | Jie Jiang, Yang Wu et al. | Latent reasoning has emerged as a promising paradigm for sequential recommendation, enabling models to capture complex user intent through multi-step deliberation. Yet existing approaches often rely on deterministic latent chains that accumulate noise and overlook the uncertainty inherent in user intent, and they are typically trained in staged pipelines that hinder joint optimization and exploration. To address these challenges, we propose DiffuReason, a unified "Think-then-Diffuse" framework for sequential recommendation. It integrates multi-step Thinking Tokens for latent reasoning, diffusion-based refinement for denoising intermediate representations, and end-to-end Group Relative Policy Optimization (GRPO) alignment to optimize for ranking performance. In the Think stage, the model generates Thinking Tokens that reason over user history to form an initial intent hypothesis. In the Diffuse stage, rather than treating this hypothesis as the final output, we refine it through a diffusion process that models user intent as a probabilistic distribution, providing iterative denoising against reasoning noise. Finally, GRPO-based reinforcement learning enables the reasoning and refinement modules to co-evolve throughout training, without the constraints of staged optimization. Extensive experiments on four benchmarks demonstrate that DiffuReason consistently improves diverse backbone architectures. Online A/B tests on a large-scale industrial platform further validate its practical effectiveness. |
| 2026-02-09 | [Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning](http://arxiv.org/abs/2602.08986v1) | Isaac Xu, Martin Gillis et al. | In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To address this, we propose a weighted loss objective for neural networks that combines node-wise imbalance weighting with focal weighting components, the latter leveraging modern quantification of ensemble uncertainties. By emphasizing rare nodes rather than rare observations (data points), and focusing on uncertain nodes for each model output distribution during training, we observe improvements in recall by up to a factor of five on benchmark datasets, along with statistically significant gains in $F_{1}$ score. We also show our approach aids convolutional networks on challenging tasks, as in situations with suboptimal encoders or limited data. |
| 2026-02-09 | [Granulation signatures as seen by Kepler short-cadence data. I. A decoupling between granulation and oscillation timescales for dwarfs](http://arxiv.org/abs/2602.08978v1) | Jens R. Larsen, Mia S. Lundkvist et al. | Granulation is the observable signature of convection in envelopes of low-mass stars, forming the background in stellar power spectra. While well-studied in evolved giants, granulation on the MS has received less attention. We here study and characterise granulation signatures of MS and SGB stars, extending previous studies of giants to provide a continuous physical picture across evolutionary stages. We analyse 753 Kepler short-cadence stars using a Bayesian nested-sampling framework to evaluate three background descriptions and compare model preferences. This yields full posterior distributions for all parameters, enabling robust comparisons across a diverse stellar sample. No universal preference between background models is found. Assuming a Gaussian oscillation envelope, $ŒΩ_\mathrm{max}$ estimates are sensitive to model misspecification, with the resulting systematics exceeding the formal uncertainties. The envelope width scales with $ŒΩ_\mathrm{max}$ across models and shows a dependence on effective temperature. Total granulation amplitudes in dwarfs broadly follow giant-based scalings, however a decoupling appears between the timescale of the primary granulation and the oscillations for MS stars cooler than the Sun. The prolonged granulation timescale is reproduced by 3D simulations of a K-dwarf, driven by reduced convective velocities due to more efficient convective energy transport in denser envelopes. The prolonged granulation timescale increases the frequency separation to the oscillation excess, potentially aiding seismic detectability, while the reduced convective velocities may influence the excitation of stellar oscillations and relate to the low amplitudes observed in cool dwarfs. Finally, we contribute a dataset linking granulation, oscillations, and stellar parameters, providing a foundation for future investigations into their interdependence across the HR diagram. |
| 2026-02-09 | [Contraction Metric Based Safe Reinforcement Learning Force Control for a Hydraulic Actuator with Real-World Training](http://arxiv.org/abs/2602.08977v1) | Lucca Maitan, Lucas Toschi et al. | Force control in hydraulic actuators is notoriously difficult due to strong nonlinearities, uncertainties, and the high risks associated with unsafe exploration during learning. This paper investigates safe reinforcement learning (RL) for hy draulic force control with real-world training using contraction metric certificates. A data-driven model of a hydraulic actuator, identified from experimental data, is employed for simulation based pretraining of a Soft Actor-Critic (SAC) policy that adapts the PI gains of a feedback-linearization (FL) controller. To reduce instability during online training, we propose a quadratic-programming (QP) contraction filter that leverages a learned contraction metric to enforce approximate exponential convergence of trajectories, applying minimal corrections to the policy output. The approach is validated on a hydraulic test bench, where the RL controller is trained directly on hardware and benchmarked against a simulation-trained agent and a fixed-gain baseline. Experimental results show that real-hardware training improves force-tracking performance compared to both alternatives, while the contraction filter mitigates chattering and instabilities. These findings suggest that contraction-based certificates can enable safe RL in high force hydraulic systems, though robustness at extreme operating conditions remains a challenge. |
| 2026-02-09 | [Two Robust Interstellar Meteor Candidates in the Post-2018 CNEOS Fireball Database](http://arxiv.org/abs/2602.08956v1) | Richard Cloete, Abraham Loeb | We report the identification of two previously unrecognized interstellar meteor candidates in the NASA CNEOS fireball database. Using the empirically calibrated low-discrepancy uncertainty model of Pe√±a-Asensio et al.\ (2025) for post-2018 CNEOS velocity accuracy (1$œÉ$: speed 0.55~km~s$^{-1}$, right ascension 1.35$^\circ$, declination 0.84$^\circ$), we transform CNEOS velocity vectors to heliocentric orbits and assess interstellar candidacy via $10^{6}$-draw Monte Carlo simulations. Two post-2018 events have heliocentric speeds robustly exceeding the Solar System escape speed. CNEOS-22 (2022-07-28; 6.0$^\circ$S, 86.9$^\circ$W; eastern tropical Pacific) has $v_{\rm hel}=46.98$~km~s$^{-1}$, exceeding escape by $Œî= 5.18 \pm 0.60$~km~s$^{-1}$ ($z_Œî=8.7œÉ$), with interstellar speed $v_{\infty,\odot}=21.5$~km~s$^{-1}$. CNEOS-25 (2025-02-12; 73.4$^\circ$N, 49.3$^\circ$E; Barents Sea) has $v_{\rm hel}=45.63$~km~s$^{-1}$, exceeding escape by $Œî= 3.22 \pm 0.58$~km~s$^{-1}$ ($z_Œî=5.5œÉ$), with $v_{\infty,\odot}=16.9$~km~s$^{-1}$. For both events, none of $10^{6}$ realizations yield a gravitationally bound orbit ($p_{\rm bound} < 3\times 10^{-6}$). The adopted error model would need to underestimate the true uncertainties by factors of 5--9 for either candidate to be marginally bound. |
| 2026-02-09 | [Platform Design, Earnings Transparency and Minimum Wage Policies: Evidence from A Natural Experiment on Lyft](http://arxiv.org/abs/2602.08955v1) | Rubing Li, Xiao Liu et al. | We study the impact of a major policy and design change at Lyft that altered both driver earnings and platform transparency, offering insights into how such changes affect stakeholders and platform outcomes. In February 2024, Lyft began a staggered rollout of a new policy that guaranteed drivers a minimum share of rider payments and increased transparency by displaying estimated earnings per ride upfront. This policy was first introduced in major urban markets, creating a natural experiment to evaluate its effects. Using data from over 47 million rides across urban and neighboring suburban markets, we apply dynamic staggered difference-in-differences and geographic border strategies to measure causal effects on driver behavior, rider experience, and platform performance. We find the policy significantly increased driver engagement-particularly among those with lower pre-policy earnings or higher income uncertainty-leading to more hours worked, higher utilization, and greater trip volume. These supply-side changes also generated positive spillovers on rider demand. We disentangle the separate effects of earnings guarantees and transparency and show that while both were beneficial, transparency may have also triggered strategic driver behaviors. In ongoing work, we develop a counterfactual simulation framework linking driver supply and rider intents to ride production, showing how small behavioral shifts could further amplify platform outcomes. We also train a self-supervised model on driver trajectories to detect multihoming, examining whether the observed supply increase reflects net expansion or substitution from other platforms. Together, our findings highlight the potential for platform-led policies to serve as alternatives to regulation and offer design insights for managing platform change. |
| 2026-02-09 | [Seasonal Variation of Polar Ice: Implications for Ultrahigh Energy Neutrino Detectors](http://arxiv.org/abs/2602.08921v1) | Alexander Kyriacou, Steven Prohira et al. | The upper $100 \, \mathrm{m}$ to $150 \, \mathrm{m}$ of the polar ice sheet, called the firn, has a time-dependent density due to seasonal variations in the surface temperature and snow accumulation. We present RF simulations of an in-ice neutrino-induced radio source that show that these density anomalies create variations in the amplitude and propagation times of radio signals propagating through polar firn at an altitude of ${\sim}3000 \, \mathrm{m}$ above sea level. The received power from signals generated in the ice that refract within the upper ${\sim} 15 \, \mathrm{m}$ firn are subject to a seasonal variation on the order of 10\%. These variations result in an irreducible background uncertainty on the reconstructed neutrino energy and arrival direction for detectors using ice as a detection medium. |
| 2026-02-09 | [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](http://arxiv.org/abs/2602.08920v1) | Manh Cuong Dao, Quang Hung Pham et al. | Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers. |
| 2026-02-09 | [Denoise Stepwise Signals by Diffusion Model Based Approach](http://arxiv.org/abs/2602.08904v1) | Xingdi Tong, Chenyu Wen | Stepwise signals are ubiquitous in single-molecule detections, where abrupt changes in signal levels typically correspond to molecular conformational changes or state transitions. However, these features are inevitably obscured by noise, leading to uncertainty in estimating both signal levels and transition points. Traditional frequency-domain filtering is ineffective for denoising stepwise signals, as edge-related high-frequency components strongly overlap with noise. Although Hidden Markov Model-based approaches are widely used, they rely on stationarity assumptions and are not specifically designed for signal denoising. Here, we propose a diffusion model-based algorithm for stepwise signal denoising, named the Stepwise Signal Diffusion Model (SSDM). During training, SSDM learns the statistical structure of stepwise signals via a forward diffusion process that progressively adds noise. In the following reverse process, the model reconstructs clean signals from noisy observations, integrating a multi-scale convolutional network with an attention mechanism. Training data are generated by simulating stepwise signals through a Markov process with additive Gaussian noise. Across a broad range of signal-to-noise ratios, SSDM consistently outperforms traditional methods in both signal level reconstruction and transition point detection. Its effectiveness is further demonstrated on experimental data from single-molecule Forster Resonance Energy Transfer and nanopore DNA translocation measurements. Overall, SSDM provides a general and robust framework for recovering stepwise signals in various single-molecule detections and other physical systems exhibiting discrete state transitions. |
| 2026-02-09 | [Accelerated Stabilization of Switched Linear MIMO Systems using Generalized Homogeneity](http://arxiv.org/abs/2602.08903v1) | Moussa Labbadi, Andrey Polyakov et al. | This paper addresses the problem of exponential and accelerated finite-time, as well as nearly fixed-time, stabilization of switched linear MIMO systems. The proposed approach relies on a generalized homogenization framework for switched linear systems and employs implicit Lyapunov functions for control design, covering both common and multiple Lyapunov function settings. Linear matrix equations and inequalities are derived to characterize the dilation generator and to synthesize the controller gains. Robustness of the resulting control laws with respect to system uncertainties and external disturbances is analyzed. The effectiveness of the proposed approach is illustrated through numerical examples. |
| 2026-02-09 | [A mapping method of age estimation for binary stars: Application to the $Œ±$ Centauri system A and B](http://arxiv.org/abs/2602.08879v1) | F. Th√©venin, V. A. Baturin et al. | Given the wealth of data provided by Gaia and the upcoming PLATO mission, it is essential to improve stellar models to obtain accurate stellar ages. Our objective is to apply a mapping technique to estimate the age of a system and the initial chemical composition. We also evaluate the influence of observational uncertainties in mass and heavy-element mixtures on results. We applied an inverse calibration method to the evolution of a multiple stellar system, assuming that the stars share the same age and initial chemical composition. This approach determines age, the initial mass fractions of helium ($Y_{ini}$) and heavy elements ($Z_{ini}$), as well as the convective mixing-length parameters ($Œ±_A $ and $Œ±_B$). It uses the observed luminosities ($L_A$ and $L_B$), radii ($R_A$ and $R_B$), and surface chemical compositions ($Z/X_A$ and $Z/X_B$). We used the most recent observational data for $M$, $R$, $L$, and $[Fe/H]$ of $Œ±$ Centauri A and B as input data for our method. We compared two assumptions for the $Z/X$ ratio, following the results for the solar composition. For an assumed high solar $Z/X_\odot =0.0245$, we obtain an age of $7.8 \pm 0.6$ Ga, $Y_{ini} = 0.284 \pm 0.004$, and $Z_{ini} = 0.0335 \pm 0.0015$. For a low solar $Z/X_\odot = 0.0181$, the derived age is $8.7 \pm 0.6$ Ga, $Y_{ini} = 0.267 \pm 0.008$, and $Z_{ini} = 0.025 \pm 0.002$. Observational errors in the stellar masses of $\pm$0.002 lead to an age error of 0.6 Ga. Overshooting of $0.05-0.20H_p$ at the boundary of the convective core increases the age by $0.6-2.1$ Ga. Models with higher $Z/X$ and radiative cores, with ages of $7.2-7.8$ Ga, appear preferable and show better agreement with the observed asteroseismic frequencies. |
| 2026-02-06 | [Agentic Uncertainty Reveals Agentic Overconfidence](http://arxiv.org/abs/2602.06948v1) | Jean Kaddour, Srijan Patel et al. | Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration. |
| 2026-02-06 | [QED Effects in PDFs -- A Les Houches Comparison Study](http://arxiv.org/abs/2602.06908v1) | Thomas Cridge, Juan Cruz Martinez et al. | In the last decade, and even more so in the last few years, our knowledge of the internal structure of the proton has become more accurate and precise thanks to the large amount of data available and developments in theory and methodology. The reduction of the uncertainties associated to these developments has brought previously neglected effects into focus as their typical magnitude are competitive with the size of the uncertainties. One such effect is the inclusion of QED into PDF fits. Typically this is a percent effect, and thus while theoretically important, it has had a relatively limited impact on phenomenological studies up to this point. In this proceeding we study some of the effects which, while peripheral to the inclusion of QED in the proton, can considerably change the relative size and shape of the QCD+QED fit with respect to the QCD only determination. These may become important in the future as precision continues to increase. After a comparison of the QCD+QED PDFs with the QCD only PDFs of various global PDF fitting groups, we focus largely upon NNPDF4.0, which shows the biggest effect when including QED. Focusing largely on a single set of PDFs also enables more subtle effects to be analysed, making it an ideal candidate for this study. |
| 2026-02-06 | [Non-spherical BUFFALOs: a weak lensing view of the Frontier Field clusters and associated systematics](http://arxiv.org/abs/2602.06904v1) | A. Niemiec, A. Acebron et al. | Galaxy clusters are tracers of the large scale structures of the Universe, making the time evolution of their mass function dependent on key cosmological parameters, such as the cosmic matter density or the amplitude of density fluctuations $œÉ_8$. Accurate measurements of cluster's total masses are therefore essential, yet they can be challenging, particularly for clusters with complex morphologies, as simple mass profiles are often adopted to fit the measurements. In this work, we focus on the Frontier Fields galaxy clusters: a sample of six extremely massive systems, that, in most cases, exhibit highly complex mass distributions. The BUFFALO survey extended the Hubble Space Telescope observations for the Frontier Fields galaxy clusters, providing high-resolution multi-band imaging within a few Mpc. Combining this high-quality imaging dataset with ancillary spectroscopy, we produce weak-lensing catalogues with very high source densities, about 50 sources/arcmin$^2$. This allows us to robustly estimate the individual weak-lensing cluster masses and quantify the sensitivity of these measurements on different factors, such as the cluster centring, the uncertainty on the redshift distribution or the foreground contamination and boost factor correction. This provides a data-driven analysis of the different sources of systematics that can impact such measurements. We find that the largest sources of systematic bias arise for the most disturbed clusters, such as the multi-modal, merging galaxy cluster Abell 2744. This analysis sets a comprehensive framework for assessing the impact of systematics on the weak-lensing estimates of cluster masses, and in particular, in the case of unrelaxed clusters. This can play a key role in forthcoming cosmological analyses based on wide-field surveys such as Euclid and the Legacy Survey of Space and Time of the Rubin Observatory. |
| 2026-02-06 | [T-STAR: A Context-Aware Transformer Framework for Short-Term Probabilistic Demand Forecasting in Dock-Based Shared Micro-Mobility](http://arxiv.org/abs/2602.06866v1) | Jingyi Cheng, Gon√ßalo Homem de Almeida Correia et al. | Reliable short-term demand forecasting is essential for managing shared micro-mobility services and ensuring responsive, user-centered operations. This study introduces T-STAR (Two-stage Spatial and Temporal Adaptive contextual Representation), a novel transformer-based probabilistic framework designed to forecast station-level bike-sharing demand at a 15-minute resolution. T-STAR addresses key challenges in high-resolution forecasting by disentangling consistent demand patterns from short-term fluctuations through a hierarchical two-stage structure. The first stage captures coarse-grained hourly demand patterns, while the second stage improves prediction accuracy by incorporating high-frequency, localized inputs, including recent fluctuations and real-time demand variations in connected metro services, to account for temporal shifts in short-term demand. Time series transformer models are employed in both stages to generate probabilistic predictions. Extensive experiments using Washington D.C.'s Capital Bikeshare data demonstrate that T-STAR outperforms existing methods in both deterministic and probabilistic accuracy. The model exhibits strong spatial and temporal robustness across stations and time periods. A zero-shot forecasting experiment further highlights T-STAR's ability to transfer to previously unseen service areas without retraining. These results underscore the framework's potential to deliver granular, reliable, and uncertainty-aware short-term demand forecasts, which enable seamless integration to support multimodal trip planning for travelers and enhance real-time operations in shared micro-mobility services. |
| 2026-02-06 | [SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization](http://arxiv.org/abs/2602.06864v1) | Zhuocheng Zhang, Haizhou Zhao et al. | Robotic tasks involving contact interactions pose significant challenges for trajectory optimization due to discontinuous dynamics. Conventional formulations typically assume deterministic contact events, which limit robustness and adaptability in real-world settings. In this work, we propose SURE, a robust trajectory optimization framework that explicitly accounts for contact timing uncertainty. By allowing multiple trajectories to branch from possible pre-impact states and later rejoin a shared trajectory, SURE achieves both robustness and computational efficiency within a unified optimization framework. We evaluate SURE on two representative tasks with unknown impact times. In a cart-pole balancing task involving uncertain wall location, SURE achieves an average improvement of 21.6% in success rate when branch switching is enabled during control. In an egg-catching experiment using a robotic manipulator, SURE improves the success rate by 40%. These results demonstrate that SURE substantially enhances robustness compared to conventional nominal formulations. |
| 2026-02-06 | [Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF](http://arxiv.org/abs/2602.06834v1) | Allen Tao, Jun Yang et al. | Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, leading to reduced accuracy and instability in visual servoing. In this work, we build upon learning-based keypoint detection for textureless objects and propose a method that enhances robustness by tightly integrating perception and control in a closed loop. Specifically, we employ an Extended Kalman Filter (EKF) that integrates per-frame keypoint measurements to estimate 6D object pose, which drives pose-based visual servoing (PBVS) for control. The resulting camera motion, in turn, enhances the tracking of subsequent keypoints, effectively closing the perception-control loop. Additionally, unlike standard PBVS, we propose a probabilistic control law that computes both camera velocity and its associated uncertainty, enabling uncertainty-aware control for safe and reliable operation. We validate our approach on real-world robotic platforms using quantitative metrics and grasping experiments, demonstrating that our method outperforms traditional visual servoing techniques in both accuracy and practical application. |
| 2026-02-06 | [Wild Guesses and Mild Guesses in Active Concept Learning](http://arxiv.org/abs/2602.06818v1) | Anirudh Chari, Neil Pattanaik | Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought. |
| 2026-02-06 | [Dynamical low-rank approximation for the semiclassical Schrodinger equation with uncertainties](http://arxiv.org/abs/2602.06808v1) | Liu Liu, Limin Xu et al. | In this paper, we propose a dynamical low-rank (DLR) approximation framework for solving the semiclassical Schrodinger equation with uncertainties. The primary numerical challenges arise from the dual nature of the oscillations: the spatial oscillations inherent in the semiclassical limit and the high-frequency oscillations in the random space induced by uncertainties. We extend two robust integrators -- the projector-splitting integrator and the unconventional integrator -- to the semiclassical regime to evolve the solution on a low-rank manifold. Through extensive numerical experiments, we demonstrate that the DLR method is significantly more computationally efficient than the standard stochastic Galerkin method, as it captures the essential quantum dynamics using a much smaller number of basis functions. Our findings reveal that despite the complex oscillatory patterns of the wave function, its evolution remains concentrated in a low-rank subspace for the cases investigated. Specifically, we observe that the DLR method achieves high fidelity with a remarkably small numerical rank, which remains robust even as the semiclassical parameter $\varepsilon$ decreases. Within our problem settings, the results further suggest that the rank growth is primarily driven by the randomness and regularity of the potential. These results provide practical insights into the low-rank structure of uncertain quantum systems and offer an efficient approach for high-dimensional uncertainty quantification in the semiclassical regime. |
| 2026-02-06 | [Primordial Black Hole Abundance from Reionization](http://arxiv.org/abs/2602.06794v1) | Ziwen Yin, Hanyu Cheng et al. | We derive robust constraints on the initial abundance of evaporating primordial black holes (PBHs) using the reionization history of the Universe as a cosmological probe. We focus on PBHs that inject electromagnetic (EM) energy into the intergalactic medium (IGM) after recombination, in the mass range $3.2\times 10^{13}\,\mathrm{g} \lesssim M_{\rm PBH} \lesssim 5\times 10^{14}\,\mathrm{g}$. For each PBH mass, we compute the redshift-dependent energy injection from Hawking evaporation using \texttt{BlackHawk}, fully accounting for the time evolution of the PBH mass and the complete spectrum of emitted Standard Model particles and gravitons. The resulting photons and electrons are propagated through the primordial plasma using \texttt{DarkHistory}, which self-consistently models EM cascades and determines the fraction of injected energy deposited into ionization, excitation, and heating of the IGM. These modifications to the ionization and thermal histories are incorporated into a Gaussian Process reconstruction of the free-electron fraction based on low-$\ell$ CMB polarization data from the \textit{Planck} mission. This non-parametric approach allows for a statistically well-defined separation between exotic high-redshift energy injection and late-time astrophysical reionization, allowing PBH evaporation to be constrained through its contribution to the high-redshift optical depth. Requiring consistency with current CMB measurements, we obtain upper limits on the initial PBH abundance that are robust against reionization modeling uncertainties and systematically more conservative than existing bounds, reflecting the fully numerical and time-dependent treatment of Hawking evaporation and energy deposition. Our results demonstrate the power of reionization observables as a precision probe of PBH evaporation and other scenarios involving late-time energy injection. |
| 2026-02-06 | [Current precision in interacting hybrid Normal-Superconducting systems](http://arxiv.org/abs/2602.06781v1) | Nahual Sobrino, Fabio Taddei et al. | We study Andreev-mediated transport and current fluctuations in interacting normal-superconducting quantum-dot systems. Using a generalized master equation based on real-time diagrammatics and full counting statistics, we compute the steady-state current, zero-frequency noise, and rate of entropy production in the large superconducting-gap limit. We show how Coulomb interactions modify Andreev-mediated transport by renormalizing resonant conditions and suppressing superconducting coherence, leading to a pronounced reduction of current precision even when average currents are only weakly affected. These effects are particularly evident at high temperatures, where conventional Coulomb-blockade features are thermally smeared while fluctuation properties remain highly sensitive. By analyzing thermodynamic uncertainty relations, we demonstrate that violations of the quantum bound present in the noninteracting regime are progressively reduced and eventually suppressed as interactions increase, whereas the recently proposed hybrid bound remains satisfied. Our results clarify how Coulomb interactions, and nonequilibrium fluctuations jointly determine transport properties in hybrid superconducting devices, and establish current precision as a robust benchmark for interacting Andreev transport beyond the noninteracting limit. |
| 2026-02-05 | [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](http://arxiv.org/abs/2602.06030v1) | Kavana Venkatesh, Yinhan He et al. | Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs. |
| 2026-02-05 | [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](http://arxiv.org/abs/2602.06029v1) | Yingke Li, Anjali Parashar et al. | Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments. |
| 2026-02-05 | [GUARDIAN: Safety Filtering for Systems with Perception Models Subject to Adversarial Attacks](http://arxiv.org/abs/2602.06026v1) | Nicholas Rober, Alex Rose et al. | Safety filtering is an effective method for enforcing constraints in safety-critical systems, but existing methods typically assume perfect state information. This limitation is especially problematic for systems that rely on neural network (NN)-based state estimators, which can be highly sensitive to noise and adversarial input perturbations. We address these problems by introducing GUARDIAN: Guaranteed Uncertainty-Aware Reachability Defense against Adversarial INterference, a safety filtering framework that provides formal safety guarantees for systems with NN-based state estimators. At runtime, GUARDIAN uses neural network verification tools to provide guaranteed bounds on the system's state estimate given possible perturbations to its observation. It then uses a modified Hamilton-Jacobi reachability formulation to construct a safety filter that adjusts the nominal control input based on the verified state bounds and safety constraints. The result is an uncertainty-aware filter that ensures safety despite the system's reliance on an NN estimator with noisy, possibly adversarial, input observations. Theoretical analysis and numerical experiments demonstrate that GUARDIAN effectively defends systems against adversarial attacks that would otherwise lead to a violation of safety constraints. |
| 2026-02-05 | [Towards uncertainty quantification of a model for cancer-on-chip experiments](http://arxiv.org/abs/2602.06018v1) | Silvia Bertoluzza, Vittoria Bianchi et al. | This study is a first step towards using data-informed differential models to predict and control the dynamics of cancer-on-chip experiments. We consider a conceptualized one-dimensional device, containing a cancer and a population of white blood cells. The interaction between the cancer and the population of cells is modeled by a chemotaxis model inspired by Keller-Segel-type equations, which is solved by a Hybridized Discontinuous Galerkin method. Our goal is using (synthetic) data to tune the parameters of the governing equations and to assess the uncertainty on the predictions of the dynamics due to the residual uncertainty on the parameters remaining after the tuning procedure. To this end, we apply techniques from uncertainty quantification for parametric differential models. We first perform a global sensitivity analysis using both Sobol and Morris indices to assess how parameter uncertainty impacts model predictions, and fix the value of parameters with negligible impact. Subsequently, we conduct an inverse uncertainty quantification analysis by Bayesian techniques to compute a data-informed probability distribution of the remaining model parameters. Finally, we carry out a forward uncertainty quantification analysis to compute the impact of the updated (residual) parametric uncertainties on the quantities of interest of the model. The whole procedure is sped up by using surrogate models, based on sparse-grids, to approximate the mapping of the uncertain parameters to the quantities of interest. |
| 2026-02-05 | [A Bayesian approach to differential prevalence analysis with applications in microbiome studies](http://arxiv.org/abs/2602.05938v1) | Juho Pelto, Kari Auranen et al. | Recent evidence suggests that analyzing the presence/absence of taxonomic features can offer a compelling alternative to differential abundance analysis in microbiome studies. However, standard approaches face challenges with boundary cases and multiple testing. To address these challenges, we developed DiPPER (Differential Prevalence via Probabilistic Estimation in R), a method based on Bayesian hierarchical modeling. We benchmarked our method against existing differential prevalence and abundance methods using data from 67 publicly available human gut microbiome studies. We observed considerable variation in performance across methods, with DiPPER outperforming alternatives by combining high sensitivity with effective error control. DiPPER also demonstrated superior replication of findings across independent studies. Furthermore, DiPPER provides differential prevalence estimates and uncertainty intervals that are inherently adjusted for multiple testing. |
| 2026-02-05 | [Codified Finite-state Machines for Role-playing](http://arxiv.org/abs/2602.05905v1) | Letian Peng, Yupeng Hou et al. | Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration. |
| 2026-02-05 | [Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks](http://arxiv.org/abs/2602.05873v1) | Minyoung Kim | Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach. |
| 2026-02-05 | [ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent](http://arxiv.org/abs/2602.05825v1) | Lena Hegemann, Xinyi Wen et al. | Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas. |
| 2026-02-05 | [Principled Confidence Estimation for Deep Computed Tomography](http://arxiv.org/abs/2602.05812v1) | Matteo G√§tzner, Johannes Kirschner | We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging. |
| 2026-02-05 | [Sound Field Estimation Using Optimal Transport Barycenters in the Presence of Phase Errors](http://arxiv.org/abs/2602.05715v1) | Yuyang Liu, Johan Karlsson et al. | This study introduces a novel approach for estimating plane-wave coefficients in sound field reconstruction, specifically addressing challenges posed by error-in-variable phase perturbations. Such systematic errors typically arise from sensor mis-calibration, including uncertainties in sensor positions and response characteristics, leading to measurement-induced phase shifts in plane wave coefficients. Traditional methods often result in biased estimates or non-convex solutions. To overcome these issues, we propose an optimal transport (OT) framework. This framework operates on a set of lifted non-negative measures that correspond to observation-dependent shifted coefficients relative to the unperturbed ones. By applying OT, the supports of the measures are transported toward an optimal average in the phase space, effectively morphing them into an indistinguishable state. This optimal average, known as barycenter, is linked to the estimated plane-wave coefficients using the same lifting rule. The framework addresses the ill-posed nature of the problem, due to the large number of plane waves, by adding a constant to the ground cost, ensuring the sparsity of the transport matrix. Convex consistency of the solution is maintained. Simulation results confirm that our proposed method provides more accurate coefficient estimations compared to baseline approaches in scenarios with both additive noise and phase perturbations. |
| 2026-02-04 | [Capacity Bounds on Doppler OFDM Channels](http://arxiv.org/abs/2602.04862v1) | Pablo Orellana, Zheng Li et al. | Low Earth orbit (LEO) satellite systems experience significant Doppler effects due to high mobility. While Doppler shifts can be largely compensated, residual frequency uncertainty induces a structured form of channel uncertainty that can limit achievable rates. We model this effect using a block-fading channel of the form $ \mathbf{H} = \mathbf{F} + s \mathbf{G} $, where $s$ is an unknown scalar random parameter. We first study this model in a general $N\times N$ MIMO setting. For this channel, we derive achievable rate lower bounds based on explicit transmission schemes and capacity upper bounds using a duality approach. We study Gaussian signaling and propose a practical superposition scheme with subspace alignment (SN) and successive interference cancellation, where a coarse-layer stream serves as an implicit pilot for decoding refined-layer data. We characterize asymptotic capacity in the near-coherent and high-SNR regimes, and show via Doppler-OFDM simulations that the proposed SN scheme achieves near-optimal rates with low complexity. |
| 2026-02-04 | [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](http://arxiv.org/abs/2602.04853v1) | Dhruv Madhwal, Lyuxin David Zhang et al. | Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks. We find that although accuracy gains from decomposition diminish in frontier models, disagreements between prompting regimes remain highly indicative of potential errors. Because factual knowledge is stable while hallucinations are stochastic, cross-regime agreement provides a precise signal of internal uncertainty. We leverage this signal to implement a training-free abstention policy that requires no retrieval or fine-tuning. Our results show that disagreement-based abstention outperforms standard uncertainty baselines as an error detector, improving both F1 and AUROC across settings. This demonstrates that decomposition-based prompting can serve as a practical diagnostic probe for model reliability in closed-book QA. |
| 2026-02-04 | [Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](http://arxiv.org/abs/2602.04821v1) | Joydeep Chandra, Satyam Kumar Navneet et al. | Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency. |
| 2026-02-04 | [Properties of the core and other solution concepts of Bel coalitional games in the ex-ante scenario](http://arxiv.org/abs/2602.04817v1) | Michel Grabisch, Silvia Lorenzini | We study the properties of the core and other solution concepts of Bel coalitional games, that generalize classical coalitional games by introducing uncertainty in the framework. In this uncertain environment, we work with contracts, that specify how agents divide the values of the coalitions in the different states of the world. Every agent can have different a priori knowledge on the true state of the world, which is modeled through the Dempster-Shafer theory, while agents' preferences between contracts are modeled by the Choquet integral. We focus on the "ex-ante" scenario, when the contract is evaluated before uncertainty is resolved. We investigate the geometrical structure of the ex-ante core when agents have the same a priori knowledge which is a probability distribution. Finally, we define the (pre)nucleolus, the kernel and the bargaining set (a la Mas-Colell) in the ex-ante situation and we study their properties. It is found that the inclusion relations among these solution concepts are the same as in the classical case. Coincidence of the ex-ante core and the ex-ante bargaining set holds for convex Bel coalitional games, at the price of strengthening the definition of bargaining sets. |
| 2026-02-04 | [Exploring supernova neutrino mass ordering at DUNE via quantum entanglement](http://arxiv.org/abs/2602.04800v1) | Adikiran Johny, Athulkrishna R et al. | The Deep Underground Neutrino Experiment (DUNE) offers strong sensitivity to neutrinos from a Galactic core collapse supernova, providing a powerful probe of neutrino flavor conversion and the neutrino mass ordering. In this work, we study supernova neutrino oscillations at DUNE using quantum entanglement as an organizing framework. Treating the three flavor neutrino system as an effective multipartite quantum state, we quantify flavor correlations through the entanglement of formation, concurrence, and negativity, expressed directly in terms of flavor survival and transition probabilities. Benchmark scenarios defined by representative variations of the electron neutrino survival probability are constructed for each entanglement measure. Event rates and fluences are computed for a supernova at 10 kpc, and the mass ordering sensitivity is evaluated using detector-level simulations performed with the \texttt{SNOwGLoBES} framework, employing the Garching supernova flux model and including the dominant detection channels in liquid argon: $ŒΩ_e$ and $\barŒΩ_e$ charged-current interactions on argon and elastic scattering on electrons. We analyze both individual and combined detection channels and incorporate $5\%$ normalization and energy calibration systematic uncertainties. Our results show that DUNE achieves a $5œÉ$ determination of the neutrino mass ordering for a supernova at distances of $\sim 20$~kpc for the $ŒΩ_e$ charged current channel and $\sim 2$~kpc for the $\barŒΩ_e$ channel, with the reach depending on the entanglement scenario considered. These results demonstrate that entanglement based observables provide a complementary and robust framework for probing supernova neutrino oscillations and the neutrino mass ordering. |
| 2026-02-04 | [A Hierarchical Bayesian Analysis of Neutron-Skin Thicknesses and Implications for the Symmetry-Energy Slope](http://arxiv.org/abs/2602.04794v1) | A. Azizi, C. A. Bertulani et al. | Neutron-skin thicknesses provide a sensitive probe of the isovector sector of the nuclear equation of state and its density dependence, commonly characterized by the symmetry-energy slope parameter L. A wide variety of experimental and observational methods have been used to extract neutron skins, ranging from hadronic and electromagnetic probes of finite nuclei to inferences from neutron-star observations. Each approach carries distinct theoretical and systematic uncertainties, complicating global interpretations and obscuring genuine physical trends. In this work we present a hierarchical Bayesian framework for the statistically consistent synthesis of heterogeneous neutron-skin constraints. The neutron-skin thickness is modeled as a smooth latent function of isospin asymmetry and nuclear size, while method-dependent bias parameters and intrinsic nuisance widths are introduced to account for unmodeled experimental and theoretical systematics. Focusing on the tin isotopes, we infer probabilistic neutron-skin trends from 100Sn to 140Sn, finding minimal uncertainties near stability and increasing uncertainties toward the proton-rich and neutron-rich extremes. We assess the consistency of nuclear energy-density functionals and obtain conditional constraints on the symmetry-energy parameters. The resulting posterior exhibits a pronounced compression of the symmetry-energy slope parameter L, reflecting the dominant sensitivity of neutron skins to sub-saturation symmetry pressure. We demonstrate that our hierarchical Bayesian framework provides robust and transparent constraints on the sub-saturation isovector sector of the nuclear equation of state. |
| 2026-02-04 | [Species Sensitivity Distribution revisited: a Bayesian nonparametric approach](http://arxiv.org/abs/2602.04788v1) | Louise Alamichel, Julyan Arbel et al. | We present a novel approach to ecological risk assessment by recasting the Species Sensitivity Distribution (SSD) method within a Bayesian nonparametric (BNP) framework. Widely mandated by environmental regulatory bodies globally, SSD has faced criticism due to its historical reliance on parametric assumptions when modeling species variability. By adopting nonparametric mixture models, we address this limitation, establishing a statistically robust foundation for SSD. Our BNP approach offers several advantages, including its efficacy in handling small datasets or censored data, which are common in ecological risk assessment, and its ability to provide principled uncertainty quantification alongside simultaneous density estimation and clustering. We utilize a specific nonparametric prior as the mixing measure, chosen for its robust clustering properties, a crucial consideration given the lack of strong prior beliefs about the number of components. Through simulation studies and analysis of real datasets, we demonstrate the superiority of our BNP-SSD over classical SSD methods. We also provide a BNP-SSD Shiny application, making our methodology available to the Ecotoxicology community. Moreover, we exploit the inherent clustering structure of the mixture model to explore patterns in species sensitivity. Our findings underscore the effectiveness of the proposed approach in improving ecological risk assessment methodologies. |
| 2026-02-04 | [Interval-Based AUC (iAUC): Extending ROC Analysis to Uncertainty-Aware Classification](http://arxiv.org/abs/2602.04775v1) | Yuqi Li, Matthew M. Engelhard | In high-stakes risk prediction, quantifying uncertainty through interval-valued predictions is essential for reliable decision-making. However, standard evaluation tools like the receiver operating characteristic (ROC) curve and the area under the curve (AUC) are designed for point scores and fail to capture the impact of predictive uncertainty on ranking performance. We propose an uncertainty-aware ROC framework specifically for interval-valued predictions, introducing two new measures: $AUC_L$ and $AUC_U$. This framework enables an informative three-region decomposition of the ROC plane, partitioning pairwise rankings into correct, incorrect, and uncertain orderings. This approach naturally supports selective prediction by allowing models to abstain from ranking cases with overlapping intervals, thereby optimizing the trade-off between abstention rate and discriminative reliability. We prove that under valid class-conditional coverage, $AUC_L$ and $AUC_U$ provide formal lower and upper bounds on the theoretical optimal AUC ($AUC^*$), characterizing the physical limit of achievable discrimination. The proposed framework applies broadly to interval-valued prediction models, regardless of the interval construction method. Experiments on real-world benchmark datasets, using bootstrap-based intervals as one instantiation, validate the framework's correctness and demonstrate its practical utility for uncertainty-aware evaluation and decision-making. |
| 2026-02-04 | [Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty](http://arxiv.org/abs/2602.04763v1) | Rui Liu, Pratap Tokekar et al. | Multi-agent systems are increasingly equipped with heterogeneous multimodal sensors, enabling richer perception but introducing modality-specific and agent-dependent uncertainty. Existing multi-agent collaboration frameworks typically reason at the agent level, assume homogeneous sensing, and handle uncertainty implicitly, limiting robustness under sensor corruption. We propose Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty (A2MAML), a principled approach for uncertainty-aware, modality-level collaboration. A2MAML models each modality-specific feature as a stochastic estimate with uncertainty prediction, actively selects reliable agent-modality pairs, and aggregates information via Bayesian inverse-variance weighting. This formulation enables fine-grained, modality-level fusion, supports asymmetric modality availability, and provides a principled mechanism to suppress corrupted or noisy modalities. Extensive experiments on connected autonomous driving scenarios for collaborative accident detection demonstrate that A2MAML consistently outperforms both single-agent and collaborative baselines, achieving up to 18.7% higher accident detection rate. |
| 2026-02-04 | [Uncertainty in Island-based Ecosystem Services and Climate Change](http://arxiv.org/abs/2602.04762v1) | Nazli Demirel, Ioannis N. Vogiatzakis et al. | Small and medium-sized islands are acutely exposed to climate change and ecosystem degradation, yet the extent to which uncertainty is systematically addressed in scientific assessments of their ecosystem services remains poorly understood. This study revisits 226 peer-reviewed articles drawn from two global systematic reviews on island ecosystem services and climate change, applying a structured post hoc analysis to evaluate how uncertainty is treated across methods, service categories, ecosystem realms, and decision contexts. Studies were classified according to whether uncertainty was explicitly analysed, just mentioned, or ignored. Only 30 percent of studies incorporated uncertainty explicitly, while more than half did not address it at all. Scenario-based approaches dominated uncertainty assessment, whereas probabilistic and ensemble-based frameworks remained limited. Cultural ecosystem services and extreme climate impacts exhibited the lowest levels of uncertainty integration, and few studies connected uncertainty treatment to policy relevant decision frameworks. Weak or absent treatment of uncertainty emerges as a structural challenge in island systems, where narrow ecological thresholds, strong land-sea coupling, limited spatial buffers, and reduced institutional redundancy amplify the consequences of decision-making under incomplete knowledge. Systematic mapping of how uncertainty is framed, operationalised, or neglected reveals persistent methodological and conceptual gaps and informs concrete directions for strengthening uncertainty integration in future island-focused ecosystem service and climate assessments. Embedding uncertainty more robustly into modelling practices, participatory processes, and policy tools is essential for enhancing scientific credibility, governance relevance, and adaptive capacity in insular socio-ecological systems. |
| 2026-02-03 | [Deep-Learning Denoising of Radio Observations for Ultra-High-Energy Cosmic-Ray Detection](http://arxiv.org/abs/2602.03818v1) | Zhisen Lai, Oscar Macias et al. | Ultra-high-energy cosmic rays (UHECRs) can be detected via the broadband radio pulses produced by their extensive air showers. The Giant Radio Array for Neutrino Detection (GRAND) is a planned radio observatory that aims to deploy autonomous antenna arrays over areas of order $\sim 10^5\,\mathrm{km}^2$ to detect this emission. However, Galactic and instrumental radio backgrounds make the identification of low signal-to-noise ratio (SNR) pulses a central challenge. Here, we present a deep convolutional denoiser model that jointly processes each GRAND antenna trace in the time and frequency domains, allowing the network to learn transient pulse morphology and broadband spectral features while suppressing background noise. By training the model on $4.1\times 10^5$ simulated traces that include detailed UHECR radio emission and realistic detector response and noise, we find a median output-SNR improvement of $\sim 15-23\,\mathrm{dB}$ in the $50-200~\mathrm{MHz}$ band and a reduction of the normalized mean squared error of the waveform by about an order of magnitude relative to a Hilbert-envelope denoiser baseline. We also verify that applying the denoiser to noise-only windows does not produce spurious pulse candidates. Near the detection threshold, the denoiser increases the number of antennas contributing reliable pulse timing by a factor of $\sim 2-3$, which correspondingly tightens direction reconstruction uncertainties. When we additionally require accurate recovery of the waveform shape, the denoiser yields a median gain of $\sim 3-4$ antennas usable for energy reconstruction at SNR$\simeq 5-6$, strengthening event-level direction and energy estimates in sparse radio arrays. |
| 2026-02-03 | [Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion](http://arxiv.org/abs/2602.03817v1) | Oscar Ovanger, Levi Harris et al. | Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \textbf{F}usion under \textbf{IN}dependent \textbf{C}onditional \textbf{H}ypotheses (\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \texttt{\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}} |
| 2026-02-03 | [Conformal Reachability for Safe Control in Unknown Environments](http://arxiv.org/abs/2602.03799v1) | Xinhang Ma, Junlin Wu et al. | Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward. |
| 2026-02-03 | [Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity](http://arxiv.org/abs/2602.03794v1) | Yingxuan Yang, Chengrui Qu et al. | LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling. |
| 2026-02-03 | [A Scene Graph Backed Approach to Open Set Semantic Mapping](http://arxiv.org/abs/2602.03781v1) | Martin G√ºnther, Felix Igelbrink et al. | While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process.   Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts. |
| 2026-02-03 | [Modern Machine Learning and Particle Physics Phenomenology at the LHC](http://arxiv.org/abs/2602.03728v1) | Maria Ubiali | Modern machine learning is driving a paradigm shift in particle physics phenomenology at the Large Hadron Collider. This short review examines the transformative role of machine learning across the entire theoretical prediction pipeline, from parton-level calculations to full simulations. We discuss applications to scattering amplitude computations, phase space integration, Parton Distribution Function determination, and parameter extraction. Some critical frontiers for the field including uncertainty quantification, the role of symmetries, and interpretability are highlighted. |
| 2026-02-03 | [Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties](http://arxiv.org/abs/2602.03691v1) | Max H. Cohen, Pio Ong et al. | Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor. |
| 2026-02-03 | [RIPPLE: Lifecycle-aware Embedding of Service Function Chains in Multi-access Edge Computing](http://arxiv.org/abs/2602.03662v1) | Federico Giarr√®, Holger Karl | In Multi-access Edge Computing networks, services can be deployed on nearby edge clouds (EC) as service function chains (SFCs) to meet strict quality of service (QoS) requirements. As users move, frequent SFC reconfigurations are required, but these are non-trivial: SFCs can serve users only when all required virtual network functions (VNFs) are available, and VNFs undergo time-consuming lifecycle operations before becoming operational. We show that ignoring lifecycle dynamics oversimplifies deployment, jeopardizes QoS, and must be avoided in practical SFC management. To address this, forecasts of user connectivity can be leveraged to proactively deploy VNFs and reconfigure SFCs. But forecasts are inherently imperfect, requiring lifecycle and connectivity uncertainty to be jointly considered. We present RIPPLE, a lifecycle-aware SFC embedding approach to deploy VNFs at the right time and location, reducing service interruptions. We show that RIPPLE closes the gap with solutions that unrealistically assume instantaneous lifecycle, even under realistic lifecycle constraints. |
| 2026-02-03 | [Noisy nonlocal aggregation model with gradient flow structures](http://arxiv.org/abs/2602.03654v1) | Su Yang, Weiqi Chu et al. | Interacting particle systems provide a fundamental framework for modeling collective behavior in biological, social, and physical systems. In many applications, stochastic perturbations are essential for capturing environmental variability and individual uncertainty, yet their impact on long-term dynamics and equilibrium structure remains incompletely understood, particularly in the presence of nonlocal interactions. We investigate a stochastic interacting particle system governed by potential-driven interactions and its continuum density formulation in the large-population limit. We introduce an energy functional and show that the macroscopic density evolution has a gradient-flow structure in the Wasserstein-2 space. The associated variational framework yields equilibrium states through constrained energy minimization and illustrates how noise regulates the density and mitigates singular concentration. We demonstrate the connection between microscopic and macroscopic descriptions through numerical examples in one and two dimensions. Within the variational framework, we compute energy minimizers and perform a linear stability analysis. The numerical results show that the stable minimizers agree with the long-time dynamics of the macroscopic density model. |
| 2026-02-03 | [CALM: A Self-Adaptive Orchestration Approach for QoS-Aware Routing in Small Language Model based Systems](http://arxiv.org/abs/2602.03632v1) | Hemang Jain, Divyansh Pandey et al. | AI-enabled systems are subjected to various types of runtime uncertainties, ranging from dynamic workloads, resource requirements, model drift, etc. These uncertainties have a big impact on the overall Quality of Service (QoS). This is particularly true in the case of Language Model (LM) enabled systems where the autoregressive nature of token generation introduces variability in latency, energy usage and response quality. These systems, powered by LLMs, are either resource-intensive (if run on-prem) or raise privacy/cost concerns (if leveraged using APIs). While deploying a Small Language Model (SLM) can be resource-efficient, it often falls short in addressing the diversity and scale of real-world requirements. To this, we argue that, rather than relying on any one SLM, leveraging a coordinated fleet of SLMs, each with specialized strengths can enable systems to dynamically adapt to shifting contexts and workload patterns. However, realizing the full potential of such an approach demands intelligent orchestration and continuous adaptation. To this end, we introduce CALM , a self-adaptive orchestration mechanism based on MAPE-K. Our approach continuously monitors user queries, analyzes the QoS metrics of the SLMs, identifies the optimal SLM to be used, routes the query to the identified SLM and further to enhance the effectiveness and efficiency, leverages caching and scheduling to decide the SLMs to be kept in memory. Our evaluation shows that CALM reduces latency by approximately 40% and energy consumption by 50%, while preserving domain-specific task performance when compared to single-LLM baselines. |
| 2026-02-02 | [New explanations and inference for least angle regression](http://arxiv.org/abs/2602.02491v1) | Karl B. Gregory, Daniel J. Nordman | Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart "path" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population "correlation" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples. |
| 2026-02-02 | [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](http://arxiv.org/abs/2602.02486v1) | Jialiang Zhu, Gongrui Zhang et al. | LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search. |
| 2026-02-02 | [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](http://arxiv.org/abs/2602.02456v1) | Albert Gassol Puigjaner, Angelos Zacharia et al. | Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them. |
| 2026-02-02 | [Robust Safety-Critical Control of Networked SIR Dynamics](http://arxiv.org/abs/2602.02452v1) | Saba Samadi, Brooks A. Butler et al. | We present a robust safety-critical control framework tailored for networked susceptible-infected-recovered (SIR) epidemic dynamics, leveraging control barrier functions (CBFs) and robust control barrier functions to address the challenges of epidemic spread and mitigation. In our networked SIR model, each node must keep its infection level below a critical threshold, despite dynamic interactions with neighboring nodes and inherent uncertainties in the epidemic parameters and measurement errors, to ensure public health safety. We first derive a CBF-based controller that guarantees infection thresholds are not exceeded in the nominal case. We enhance the framework to handle realistic epidemic scenarios under uncertainties by incorporating compensation terms that reinforce safety against uncertainties: an independent method with constant bounds for uniform uncertainty, and a novel approach that scales with the state to capture increased relative noise in early or suppressed outbreak stages. Simulation results on a networked SIR system illustrate that the nominal CBF controller maintains safety under low uncertainty, while the robust approaches provide formal safety guarantees under higher uncertainties; in particular, the novel method employs more conservative control efforts to provide larger safety margins, whereas the independent approach optimizes resource allocation by allowing infection levels to approach the boundaries in steady epidemic regimes. |
| 2026-02-02 | [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](http://arxiv.org/abs/2602.02427v1) | Qihao Wen, Jiahao Wang et al. | Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency. |
| 2026-02-02 | [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](http://arxiv.org/abs/2602.02419v1) | Qingni Wang, Yue Fan et al. | Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference. |
| 2026-02-02 | [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](http://arxiv.org/abs/2602.02393v1) | Ruiqi Wu, Xuanhua He et al. | We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency. |
| 2026-02-02 | [Self-Supervised Learning from Structural Invariance](http://arxiv.org/abs/2602.02381v1) | Yipeng Zhang, Hafez Ghaemi et al. | Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos. |
| 2026-02-02 | [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](http://arxiv.org/abs/2602.02378v1) | Raunak Jain, Mudita Khurana et al. | As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria. |
| 2026-02-02 | [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](http://arxiv.org/abs/2602.02370v1) | Uma Meleti, Jeffrey J. Nirschl | Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists. |
| 2026-01-30 | [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](http://arxiv.org/abs/2601.23285v1) | MH Farhadi, Ali Rabiee et al. | Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy. |
| 2026-01-30 | [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](http://arxiv.org/abs/2601.23273v1) | Siran Peng, Weisong Zhao et al. | Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings. |
| 2026-01-30 | [Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference](http://arxiv.org/abs/2601.23252v1) | David Yallup, Namu Kroupa et al. | Model comparison and calibrated uncertainty quantification often require integrating over parameters, but scalable inference can be challenging for complex, multimodal targets. Nested Sampling is a robust alternative to standard MCMC, yet its typically sequential structure and hard constraints make efficient accelerator implementations difficult. This paper introduces Nested Slice Sampling (NSS), a GPU-friendly, vectorized formulation of Nested Sampling that uses Hit-and-Run Slice Sampling for constrained updates. A tuning analysis yields a simple near-optimal rule for setting the slice width, improving high-dimensional behavior and making per-step compute more predictable for parallel execution. Experiments on challenging synthetic targets, high dimensional Bayesian inference, and Gaussian process hyperparameter marginalization show that NSS maintains accurate evidence estimates and high-quality posterior samples, and is particularly robust on difficult multimodal problems where current state-of-the-art methods such as tempered SMC baselines can struggle. An open-source implementation is released to facilitate adoption and reproducibility. |
| 2026-01-30 | [Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph](http://arxiv.org/abs/2601.23233v1) | Nguyen Minh Duc, Viet Cuong Ta | Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task. |
| 2026-01-30 | [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](http://arxiv.org/abs/2601.23229v1) | Ali Asadi, Krishnendu Chatterjee et al. | Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question. |
| 2026-01-30 | [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](http://arxiv.org/abs/2601.23222v1) | Hamza Kalisch, Constantin Seibold et al. | While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations. |
| 2026-01-30 | [A complete characterisation of conditional entropies](http://arxiv.org/abs/2601.23213v1) | Roberto Rubboli, Erkka Haapasalo et al. | Entropies are fundamental measures of uncertainty with central importance in information theory and statistics and applications across all the quantitative sciences. Under a natural set of operational axioms, the most general form of entropy is captured by the family of R√©nyi entropies, parameterized by a real number $Œ±$. Conditional entropy extends the notion of entropy by quantifying uncertainty from the viewpoint of an observer with access to potentially correlated side information. However, despite their significance and the emergence of various useful definitions, a complete characterization of measures of conditional entropy that satisfy a natural set of operational axioms has remained elusive. In this work, we provide a complete characterization of conditional entropy, defined through a set of axioms that are essential for any operationally meaningful definition: additivity for independent random variables, invariance under relabeling, and monotonicity under conditional mixing channels. We prove that the most general form of conditional entropy is captured by a family of measures that are exponential averages of R√©nyi entropies of the conditioned distribution and parameterized by a real parameter and a probability measure on the positive reals. Finally, we show that these quantities determine the rate of transformation under conditional mixing and provide a set of second laws of quantum thermodynamics with side information for states diagonal in the energy eigenbasis. |
| 2026-01-30 | [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](http://arxiv.org/abs/2601.23188v1) | Zhongxiang Sun, Qipeng Wang et al. | Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness. |
| 2026-01-30 | [Distribution-informed Efficient Conformal Prediction for Full Ranking](http://arxiv.org/abs/2601.23128v1) | Wenbo Liao, Huipeng Huang et al. | Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage. |
| 2026-01-30 | [A centimeter-sized gas pressure sensor for high-vacuum measurements at cryogenic temperatures](http://arxiv.org/abs/2601.23117v1) | Christoph Reinhardt, Lea Lara Stankewitz et al. | Gas pressure sensors based on nanomechanical membranes have recently demonstrated an ultra-wide ten-decade measurement range, a gas-type-independent response, and a self-calibrating operation with uncertainties of approximately $1\,\%$. The readout relied on tabletop free-space laser interferometers. Here we present a centimeter-sized, portable implementation in which a square Si$_3$N$_4$ membrane is read out via a fiber-based laser interferometer. We perform pressure measurements between $5\times10^{-5}$ and $10^{-1}$~mbar in a confined $0.7$~L volume cooled to $78$~K. Because no suitable commercial pressure sensor exists for direct cryogenic comparison, we benchmark our device against room-temperature commercial gauges connected to the cold volume through a pipe of limited conductance. The measured relationship between the two sensors is compared with models accounting for temperature- and pumping-induced pressure gradients within the measurement chamber. These models agree with the measurements to within $<10\,\%$ for helium and $<13\,\%$ for nitrogen. The achieved readout sensitivity of $S_x = 8\times10^{-14}\,\mathrm{m}/\sqrt{\mathrm{Hz}}$ theoretically enables resolving the thermal displacement noise spectrum of a trampoline membrane at atmospheric pressure, with a peak response of $48\,S_x$ $\left(25\,S_x\right)$ at $295\,\mathrm{K}$ $\left(78\,\mathrm{K}\right)$. Our results suggest that the previously achieved pressure measurement range of ten decades with trampoline membranes is compatible with fiber-based optical readout. This paves the way for widely applicable pressure sensors in the centimeter size range in cryogenic environments. |
| 2026-01-29 | [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](http://arxiv.org/abs/2601.22139v1) | Xin Chen, Feng Jiang et al. | Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1} |
| 2026-01-29 | [SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization](http://arxiv.org/abs/2601.22131v1) | Leonard Papenmeier, Petru Tighineanu | Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions. |
| 2026-01-29 | [Making Foundation Models Probabilistic via Singular Value Ensembles](http://arxiv.org/abs/2601.22068v1) | Mehmet Ozgur Turkoglu, Dominik J. M√ºhlematter et al. | Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy. |
| 2026-01-29 | [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](http://arxiv.org/abs/2601.22041v1) | Naomi Pitzer, Daniela Mihai | Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation. |
| 2026-01-29 | [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](http://arxiv.org/abs/2601.22027v1) | Johannes Kirmayr, Lukas Stappen et al. | Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings. |
| 2026-01-29 | [Optimal Placement of Movable Antennas for Angle-of-Departure Estimation Under User Location Uncertainty](http://arxiv.org/abs/2601.21997v1) | Luc√≠a Pallar√©s-Rodr√≠guez, Angelo Coluccia et al. | Movable antennas (MA) have gained significant attention in recent years to overcome the limitations of extremely large antenna arrays in terms of cost and power consumption. In this paper, we investigate the use of MA arrays at the base station (BS) for angle-of-departure (AoD) estimation under uncertainty in the user equipment (UE) location. Specifically, we (i) derive the theoretical performance limits through the Cram√©r-Rao bound (CRB) and (ii) optimize the antenna positions to ensure robust performance within the UE's uncertainty region. Numerical results show that dynamically optimizing antenna placement by explicitly considering the uncertainty region yields superior performance compared to fixed arrays, demonstrating the ability of MA systems to adapt and outperform conventional arrays. |
| 2026-01-29 | [Liquid Interfaces: A Dynamic Ontology for the Interoperability of Autonomous Systems](http://arxiv.org/abs/2601.21993v1) | Dhiogo de S√°, Carlos Schmiedel et al. | Contemporary software architectures struggle to support autonomous agents whose reasoning is adaptive, probabilistic, and context-dependent, while system integration remains dominated by static interfaces and deterministic contracts. This paper introduces Liquid Interfaces, a coordination paradigm in which interfaces are not persistent technical artifacts, but ephemeral relational events that emerge through intention articulation and semantic negotiation at runtime.We formalize this model and present the Liquid Interface Protocol (LIP),which governs intention-driven interaction, negotiated execution, and enforce ephemerality under semantic uncertainty. We further discuss the governance implications of this approach and describe a reference architecture that demonstrates practical feasibility. Liquid Interfaces provide a principled foundation for adaptive coordination in agent-based systems |
| 2026-01-29 | [Generalized Information Gathering Under Dynamics Uncertainty](http://arxiv.org/abs/2601.21988v1) | Fernando Palafox, Jingqi Li et al. | An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems. |
| 2026-01-29 | [Stellar Populations in the Extreme Outer Halo of the Spiral Galaxy M96](http://arxiv.org/abs/2601.21987v1) | J. Christopher Mihos, Patrick R. Durrell et al. | We use deep Hubble Space Telescope imaging to study stellar populations in the outer halo of the spiral galaxy M96, located in the dynamically active Leo I galaxy group. Our imaging targets two fields at a projected distance of 50 kpc from the galaxy's center, with a 50% photometric completeness limit of F814W = 28.0, nearly two magnitudes below the tip of the red giant branch. In both fields we find a clear detection of red giant stars in M96's halo, with a space density that corresponds to an equivalent broadband surface brightness of $Œº_V \approx $ 31.7 mag arcsec$^{-2}$. We find little evidence for any difference in the spatial density or color of the RGB stars in the two fields. Using isochrone matching we derive a median metallicity for the red giants of [M/H] = -1.36 with an interquartile spread of $\pm$0.75 dex. Adopting a power-law radial density profile, we also derive a total halo mass of $M_h = 7.8^{+17.4}_{-4.9}\times10^9$ M$_\odot$, implying a stellar halo mass fraction of $M_{*,halo}/M_{*,tot} = 15^{+33}_{-9}$%, on the high end for spiral galaxies, but with significant uncertainty. Finally, we find that M96 appears offset from the stellar halo mass-metallicity relationship for spirals, with a halo that is distinctly metal-poor for its halo mass. While a variety of systematic effects could have conspired to drive M96 off this relationship, if confirmed our results may argue for a markedly different accretion history for M96 compared to other spirals in the nearby universe. |
| 2026-01-29 | [Spectroscopic Variability of the Broad H$Œ≤$ Emission Line in Sloan Digital Sky Survey Quasars](http://arxiv.org/abs/2601.21974v1) | Collin M. Dabbieri, Jessie C. Runnoe et al. | We present a catalog of broad H$Œ≤$ variability properties for all spectra of quasars with $z<0.8$ and at least two observations included in the Sloan Digital Sky Survey (SDSS) Data Release 16 quasar catalog. For each spectrum, we perform a spectral decomposition to isolate the broad H$Œ≤$ emission. We measure the luminosity, FWHM, equivalent width, centroid, and Pearson skewness coefficient of broad H$Œ≤$ and provide derived physical properties such as the single-epoch black hole mass and the bolometric luminosity. For each pair of spectra in the sample, we calculate the change in radial velocity of the centroid of broad H$Œ≤$ emission ($Œîv_{rad}$) as well as other derived properties related to broad H$Œ≤$ shape variability. We use forward-modeling methods to estimate the uncertainty in our measurements and discuss an improved method for estimating the uncertainty in $Œîv_{rad}$ in the case where a spectral decomposition is used to isolate the broad H$Œ≤$ emission. We find that $Œîv_{rad}$ is not normally distributed and that the shape of the distribution depends on the interval between observations. We discuss the effect of the predominance of the Reverberation Mapping subsample in the sample of pairs of spectra in SDSS. |
| 2026-01-28 | [Probabilistic Interpolation of Sagittarius A*'s Multi-Wavelength Light Curves Using Diffusion Models](http://arxiv.org/abs/2601.20863v1) | Gabriel Sasseville, Julie Hlavacek-Larrondo et al. | Understanding the variability of Sagittarius A* (Sgr A*) requires coordinated, multi-wavelength observations that span the electromagnetic spectrum. In this work, we focus on data from four key observatories: Chandra in the X-ray (2-8 keV), GRAVITY on the Very Large Telescope in the near-infrared (2.2 microns), Spitzer in the infrared (4.5 microns), and ALMA in the submillimeter (340 GHz). These multi-band observations are essential for probing the physics of accretion and emission near the black hole's event horizon, yet they suffer from irregular sampling, band-dependent noise, and substantial data gaps. These limitations complicate efforts to robustly identify flares and measure cross-band time lags, key diagnostics of the physical processes driving variability. To address this challenge, we introduce a diffusion-based generative model, for interpolating sparse, multivariate astrophysical time series. This represents the first application of score-based diffusion models to astronomical time series. We also present the first transformer-based model for light curve reconstruction that includes calibrated uncertainty estimates. The models are trained on simulated light curves constructed to match the statistical and observational characteristics of real Sgr A* data. These simulations capture correlated multi-band variability, realistic observation cadences, and wavelength-specific noise. We compare our models against a multi-output Gaussian Process. The diffusion model achieves superior accuracy and competitive calibration across both simulated and real datasets, demonstrating the promise of diffusion models for high-fidelity, uncertainty-aware reconstruction of multi-wavelength variability in Sgr A*. |
| 2026-01-28 | [Symplectic Optimization on Gaussian States](http://arxiv.org/abs/2601.20832v1) | Christopher Willby, Tomohiro Hashizume et al. | Computing Gaussian ground states via variational optimization is challenging because the covariance matrices must satisfy the uncertainty principle, rendering constrained or Riemannian optimization costly, delicate, and thus difficult to scale, particularly in large and inhomogeneous systems. We introduce a symplectic optimization framework that addresses this challenge by parameterizing covariance matrices directly as positive-definite symplectic matrices using unit-triangular factorizations. This approach enforces all physical constraints exactly, yielding a globally unconstrained variational formulation of the bosonic ground-state problem. The unconstrained structure also naturally supports solution reuse across nearby Hamiltonians: warm-starting from previously optimized covariance matrices substantially reduces the number of optimization steps required for convergence in families of related configurations, as encountered in crystal lattices, molecular systems, and fluids. We demonstrate the method on weakly dipole-coupled lattices, recovering ground-state energies, covariance matrices, and spectral gaps accurately. The framework further provides a foundation for large-scale approximate treatments of weakly non-quadratic interactions and offers potential scaling advantages through tensor-network enhancements. |
| 2026-01-28 | [Plotting correlated data](http://arxiv.org/abs/2601.20805v1) | Lukas Koch | A very common task in data visualization is to plot many data points with some measured y-value as a function of fixed x-values. Uncertainties on the y-values are typically presented as vertical error bars that represent either a Frequentist confidence interval or Bayesian credible interval for each data point. Most of the time, these error bars represent a 68\% confidence/credibility level, which leads to the intuition that a model fits the data reasonably well if its prediction lies within the error bars of roughly two thirds of the data points. Unfortunately, this and other intuitions no longer work when the uncertainties of the data points are correlated. If the error bars only show the square root of diagonal elements of some covariance matrix with non-negligible off-diagonal elements, we simply do not have enough information in the plot to judge whether a drawn model line agrees well with the data or not. In this paper we will demonstrate this problem and discuss ways to add more information to the plots to make it easier to judge the agreement between the data and some model prediction in the plot, as well as glean some insight where the model might be deficient. This is done by explicitly showing the contribution of the first principal component of the uncertainties, and by displaying the conditional uncertainties of all data points. |
| 2026-01-28 | [Observation of the decay $œá_{c1}(3872)\rightarrow J\mskip -3mu/\mskip -2muœàŒº^+Œº^-$](http://arxiv.org/abs/2601.20790v1) | LHCb collaboration, R. Aaij et al. | The first observation of the $œá_{c1}(3872)\rightarrow J\mskip -3mu/\mskip -2muœàŒº^+Œº^-$ decay is reported using proton-proton collision data recorded with the LHCb detector corresponding to an integrated luminosity of $9fb^{-1}$. The decay mode is observed for the first time, with a significance of $6.5œÉ$. Its branching fraction is measured relative to the $œá_{c1}(3872)\rightarrow J\mskip -3mu/\mskip -2muœàœÄ^+œÄ^-$ decay mode \begin{align*} \frac{\cal{BF}(œá_{c1}(3872)\rightarrow J\mskip -3mu/\mskip -2muœàŒº^+Œº^-)}{\cal{BF}(œá_{c1}(3872)\rightarrow J\mskip -3mu/\mskip -2muœàœÄ^+œÄ^-)} = \left(1.64\pm 0.32\pm 0.05\right)\times10^{-3}, \end{align*} where the first uncertainty includes both statistical contributions and systematic contributions which are uncorrelated between data-taking periods, and the second represents the systematic contributions that are correlated between data-taking periods. |
| 2026-01-28 | [Semiclassical effective description of a quantum particle on a sphere with non-central potential](http://arxiv.org/abs/2601.20787v1) | Guillermo Chacon-Acosta, H. Hernandez-Hernandez et al. | We develop a semiclassical framework for studying quantum particles constrained to curved surfaces using the momentous quantum mechanics formalism, which extends classical phase-space to include quantum fluctuation variables (moments). In a spherical geometry, we derive quantum-corrected Hamiltonians and trajectories that incorporate quantum back-reaction effects absent in classical descriptions. For the free particle, quantum fluctuations induce measurable phase shifts in azimuthal precession of approximately 8-12%, with uncertainty growth rates proportional to initial moment correlations. When a non-central Makarov potential is introduced, quantum corrections dramatically amplify its asymmetry. For strong coupling ($Œ≥$ = -1.9), the quantum-corrected force drives trajectories preferentially toward the southern hemisphere on timescales 40% shorter than classical predictions, with trajectory densities exhibiting up to 3-fold enhancement in the preferred region. Throughout evolution, the solutions rigorously satisfy Heisenberg uncertainty relations, validating the truncation scheme. These results demonstrate that quantum effects fundamentally alter semiclassical dynamics in curved constrained systems, with direct implications for charge transport in carbon nanostructures, exciton dynamics in curved quantum wells, and reaction pathways in cyclic molecules. |
| 2026-01-28 | [Machine-learning wall model of large-eddy simulation for low- and high-speed flows over rough surfaces](http://arxiv.org/abs/2601.20786v1) | Rong Ma, Adrian Lozano-Duran | We present a wall model for large-eddy simulation that incorporates surface-roughness effects and is applicable across low- and high-speed flows, for both transitional and fully rough conditions. The model, implemented using an artificial neural network, is trained on a direct numerical simulation database of compressible turbulent channel flows over rough walls. The dataset contains 372 cases spanning a wide range of irregular roughness topographies, including Gaussian and Weibull distributions, Mach numbers 0~3.3, and friction Reynolds numbers 180~2000. We employ an information-theoretic, dimensionless learning method to identify the inputs with the highest predictive power for the dimensionless wall friction and wall heat flux. Predictions are accompanied by a confidence score derived from a spectrally normalized neural Gaussian process, which quantifies uncertainty in regions that deviate from the training dataset. The model performance is first evaluated a-priori on 110 turbulent channel flow cases, yielding prediction errors below 4%. The model is assessed a-posteriori in wall-modeled large-eddy simulations across diverse test cases. These include over 160 subsonic and supersonic turbulent channel flows with rough walls, a transonic high-pressure turbine (HPT) blade with Gaussian roughness, a high-speed compression ramp with sandpaper roughness, and three hypersonic blunt bodies with sand-grain roughness. Results show that the proposed wall model typically achieves a-posteriori predictive accuracy within 10% for wall shear stress and within 15% for wall heat flux, with high confidence in the channel flows and HPT blade cases. In the rough-wall compression ramp and hypersonic blunt bodies, the model captures the heating augmentation with errors ranging 0%~20%. In the cases with the highest errors, the reduced performance is correctly detected by a drop in the confidence score. |
| 2026-01-28 | [Optimal Sensor Placement in Gaussian Processes via Column Subset Selection](http://arxiv.org/abs/2601.20781v1) | Jessie Chen, Hangjie Ji et al. | Gaussian process regression uses data measured at sensor locations to reconstruct a spatially dependent function with quantified uncertainty. However, if only a limited number of sensors can be deployed, it is important to determine how to optimally place the sensors to minimize a measure of the uncertainty in the reconstruction. We consider the Bayesian D-optimal criterion to determine the optimal sensor locations by choosing sensors from a candidate set of sensors. Since this is an NP-hard problem, our approach models sensor placement as a column subset selection problem (CSSP) on the covariance matrix, computed using the kernel function on the candidate sensor points. We propose an algorithm that uses the Golub-Klema-Stewart framework (GKS) to select sensors and provide an analysis of lower bounds on the D-optimality of these sensor placements. To reduce the computational cost in the GKS step, we propose and analyze algorithms for the D-optimal sensor placements using Nystr√∂m approximations on the covariance matrix. Moreover, we propose several algorithms that select sensors via Nystr√∂m approximation of the covariance matrix, utilizing the randomized Nystr√∂m approximation, random pivoted Cholesky and greedy pivoted Cholesky. We demonstrate the performance of our method on two applications: thin liquid film dynamics and sea surface temperature. |
| 2026-01-28 | [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](http://arxiv.org/abs/2601.20776v1) | Huanyu Tian, Martin Huber et al. | This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention. |
| 2026-01-28 | [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](http://arxiv.org/abs/2601.20773v1) | Rub√©n Jim√©nez, Oriol Pujol | Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $Œ±$-governed smoothing and regularization scheme with H√∂lder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas. |
| 2026-01-28 | [Anytime-Valid Quantum Tomography via Confidence Sequences](http://arxiv.org/abs/2601.20761v1) | Aldo Cumitini, Luca Barletta et al. | In this letter, we address the problem of developing quantum state tomography (QST) methods that remain valid at any time during a sequence of measurements. Specifically, the aim is to provide a rigorous quantification of the uncertainty associated with the current state estimate as data are acquired incrementally. To this end, the proposed framework augments existing QST techniques by associating current point estimates of the state with confidence sets that are guaranteed to contain the true quantum state with a user-defined probability. The methodology is grounded in recent statistical advances in anytime-valid confidence sequences. Numerical results confirm the theoretical coverage properties of the proposed anytime-valid QST. |
| 2026-01-27 | [Banana Split: Improved Cosmological Constraints with Two Light-Curve-Shape and Color Populations Using Union3.1+UNITY1.8](http://arxiv.org/abs/2601.19854v1) | David Rubin, Taylor Hoyt et al. | SNe Ia have been used to provide key constraints on the equation-of-state parameter of dark energy. They are generally standardized under the assumption that they belong to a single population, with luminosities standardized in a continuous (roughly linear) fashion using the observed light-curve timescale. We update the Union3+UNITY1.5 SN cosmology analysis in light of increasing evidence for at least two core populations of SNe Ia and apply this "UNITY1.8" model to the updated "Union3.1" compilation (Hoyt et al. 2026). In addition to finding evidence for two different light-curve-shape (x1) distributions, we also find that the color distributions are different, that the light-curve-shape/magnitude standardization relations are different, and that these populations have different distributions across host-galaxy stellar mass and redshift. Importantly, we find that the residual host-mass luminosity step found in prior SN Ia cosmology analyses is now consistent with zero for unreddened SNe. We report a significantly tightened constraint on the split in the red-color standardization between SNe in low- and high-mass galaxies. We find that the estimated uncertainties shrink on cosmological parameters when fitting the same SNe assuming two modes versus one mode. We confirm similar trends in simulated data when running both versions of UNITY on the same (two-mode) simulations. For a flat LambdaCDM cosmology, we find Om = 0.334+0.025-0.024 from SNe alone; for a flat w0-wa cosmology, we find w0 = -0.760+0.084-0.082 and wa = -0.79+0.28-0.30 when including SNe, BAO, and CMB. In the 2D w0-wa plane, adding SNe to BAO and compressed CMB increases the tension with flat LambdaCDM from 2.1 sigma to 2.6 sigma. |
| 2026-01-27 | [Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty](http://arxiv.org/abs/2601.19843v1) | Doga Yilmaz, Jialin Zhu et al. | We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting. Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed. However, one orthogonal, much needed, but under-explored area is data uncertainty. In standard 4D Gaussian Splatting, data uncertainty can manifest as view sparsity, missing frames, camera asynchronization, etc. So far, there has been little research to holistically incorporating various types of data uncertainty under a single framework. To this end, we propose Graphical X Splatting, or GraphiXS, a new probabilistic framework that considers multiple types of data uncertainty, aiming for a fundamental augmentation of the current 4D Gaussian Splatting paradigm into a probabilistic setting. GraphiXS is general and can be instantiated with a range of primitives, e.g. Gaussians, Student's-t. Furthermore, GraphiXS can be used to `upgrade' existing methods to accommodate data uncertainty. Through exhaustive evaluation and comparison, we demonstrate that GraphiXS can systematically model various uncertainties in data, outperform existing methods in many settings where data are missing or polluted in space and time, and therefore is a major generalization of the current 4D Gaussian Splatting research. |
| 2026-01-27 | [Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation](http://arxiv.org/abs/2601.19832v1) | Elena Merlo, Marta Lagomarsino et al. | Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems. |
| 2026-01-27 | [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](http://arxiv.org/abs/2601.19824v1) | Andre Paulino de Lima, Paula Castro et al. | There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced. |
| 2026-01-27 | [Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation](http://arxiv.org/abs/2601.19794v1) | Ganesh Sundaram, Jonas Ulmen et al. | The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions. |
| 2026-01-27 | [Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis](http://arxiv.org/abs/2601.19773v1) | Zhuohan Long, Zhijie Bao et al. | Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark . |
| 2026-01-27 | [A Multiobjective Water Allocation Model for Economic Efficiency and Environmental Sustainability: Case Study](http://arxiv.org/abs/2601.19746v1) | Nahid Sultana, M M Rizvi et al. | The management of irrigation water systems has become increasingly complex due to competing demands for agricultural production, groundwater sustainability, and environmental flow requirements, particularly under hydrologic variability and climate uncertainty. Addressing these challenges requires optimization frameworks that can jointly determine optimal crop allocation, groundwater pumping, and environmental flow releases while maintaining economic and hydrological feasibility. However, existing hydro-economic models, including the widely used Lewis and Randall formulation, may overestimate net benefits by allowing infeasible negative pumping and surface water allocations. We extend the Lewis and Randall framework by reformulating groundwater pumping and surface water use as non-negative, demand-driven decision variables and by explicitly incorporating environmental flow and canal capacity constraints. Three models are developed to maximize economic benefit, minimize environmental deficits, and a multiobjective model that evaluates the trade-offs between these two objectives. An illustrative test case examining optimal crop area allocation and environmental flow management across dry, average, and wet years, using data from the Rajshahi Barind Tract in northwestern Bangladesh, is presented. The results show that the proposed formulation produces economically and hydrologically consistent solutions, identifying optimal strategies when either net benefits or environmental protection is prioritized, as well as Pareto-optimal trade-offs when both objectives are considered together. These findings provide practical insights for balancing farm income, groundwater sustainability, and ecological protection, offering a robust decision-support tool for irrigation management in water-limited river basins. |
| 2026-01-27 | [Normalized Fractional Order Entropy-Based Decision-Making Models under Risk](http://arxiv.org/abs/2601.19715v1) | Poulami Paul, Chanchal Kundu | Constructing efficient portfolios requires balancing expected returns with risk through optimal stock selection, while accounting for investor preferences. In a recent work by Paul and Kundu (2026), the fractional-order entropy due to Ubriaco was introduced as an uncertainty measure to capture varying investor attitudes toward risk. Building on this foundation, we introduce a novel normalized fractional order entropy aligned with investors' risk preferences that combines normalized fractional entropy with expected utility and variance. Risk sensitivity is modeled through the fractional parameter, interpolating between conservative or risk aversion and adventurous or high risk tolerance attitudes. Furthermore, the robustness and statistical significance of the fractional order entropy-based risk measure, termed normalized expected utility-fractional entropy (NEU-FE) and normalized expected utility-fractional entropy-variance (NEU-FEV) risk measures are explained with the help of machine learning tools, including Random forest, Ridge regression, Lasso Regression and artificial neural networks by using Indian stock market (NIFTY50). The results confirm that the proposed decision models support investors in making high-quality portfolio investments. |
| 2026-01-27 | [Differentiable Semantic ID for Generative Recommendation](http://arxiv.org/abs/2601.19711v1) | Junchen Fu, Xuri Ge et al. | Generative recommendation provides a novel paradigm in which each item is represented by a discrete semantic ID (SID) learned from rich content. Most existing methods treat SIDs as predefined and train recommenders under static indexing. In practice, SIDs are typically optimized only for content reconstruction rather than recommendation accuracy. This leads to an objective mismatch: the system optimizes an indexing loss to learn the SID and a recommendation loss for interaction prediction, but because the tokenizer is trained independently, the recommendation loss cannot update it. A natural approach is to make semantic indexing differentiable so that recommendation gradients can directly influence SID learning, but this often causes codebook collapse, where only a few codes are used. We attribute this issue to early deterministic assignments that limit codebook exploration, resulting in imbalance and unstable optimization.   In this paper, we propose DIGER (Differentiable Semantic ID for Generative Recommendation), a first step toward effective differentiable semantic IDs for generative recommendation. DIGER introduces Gumbel noise to explicitly encourage early-stage exploration over codes, mitigating codebook collapse and improving code utilization. To balance exploration and convergence, we further design two uncertainty decay strategies that gradually reduce the Gumbel noise, enabling a smooth transition from early exploration to exploitation of learned SIDs. Extensive experiments on multiple public datasets demonstrate consistent improvements from differentiable semantic IDs. These results confirm the effectiveness of aligning indexing and recommendation objectives through differentiable SIDs and highlight differentiable semantic indexing as a promising research direction. |
| 2026-01-27 | [Joint Power Allocation and Antenna Placement for Pinching-Antenna Systems under User Location Uncertainty](http://arxiv.org/abs/2601.19704v1) | Hao Feng, Ming Zeng et al. | Pinching antenna systems have attracted much attention recently owing to its capability to maintain reliable line-of-sight (LoS) communication in high-frequency bands. By guiding signals through a waveguide and emitting them via a movable pinching antenna, these systems enable dynamic control of signal propagation and spatial adaptability. However, their performance heavily depends on effective resource allocation-encompassing power, bandwidth, and antenna positioning-which becomes challenging under imperfect channel state information (CSI) and user localization uncertainty. Existing studies largely assume perfect CSI or ideal user positioning, while our prior work considered uniform localization errors, an oversimplified assumption. In this paper, we develop a robust resource allocation framework for multiuser downlink pinching antenna systems under Gaussian-distributed localization uncertainty, which more accurately models real-world positioning errors. An energy efficiency (EE) maximization problem is formulated subject to probabilistic outage constraints, and an analytical power allocation strategy is derived under given antenna positions. On this basis, the heuristic particle swarm optimization (PSO) algorithm is employed to identify the antenna position that achieves the global EE configuration. Simulation results illustrate that the proposed scheme greatly enhances both EE and system reliability compared with fixed-antenna benchmark, validating its effectiveness for practical high-frequency wireless deployments. |
| 2026-01-26 | [A Novel Lensed Point Source Modeling Pipeline using GIGA-Lens with Application to SN Zwicky and SN iPTF16geu](http://arxiv.org/abs/2601.18787v1) | Saul Baltasar, Nicolas Ratier-Werbin et al. | We introduce a novel modeling pipeline for strongly lensed point sources, using the GIGA-Lens framework, running on four A100 GPUs via the JAX platform. Using simulations, we demonstrate accurate and precise recovery of image positions, fluxes, and time delays, together with inference of complex lens mass distributions -- including the mass density slope, $Œ≥$ -- from images of lensed point sources alone. We further show that we can achieve statistical uncertainty of $\sim 3.6\%$ ($\sim 2.5\, \mathrm{km\, s^{-1}/Mpc}$) on $H_0$ from a single system, with full forward modeling, i.e., simultaneous inference of all lens model parameters together with $H_0$. We apply our pipeline to two well-studied lensed SNe Ia, Zwicky and iPTF16geu. For SN iPTF16geu, unlike previous modeling efforts, we model only the images of the lensed point source (the SN) and do not use the lensed images of the extended host-galaxy. Nevertheless, we are able to infer all of the mass parameters modeled in earlier studies, and our best-fit values, including $Œ≥$, are fully consistent with published results. In the case of SN Zwicky, taking the same approach, however, we obtain an alternative best-fit model compared to published results, underscoring the importance of fully exploring the model parameter space. |
| 2026-01-26 | [Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery](http://arxiv.org/abs/2601.18765v1) | Shutong Chen, Adnan Aijaz et al. | Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery. |
| 2026-01-26 | [Anticipation in Action: Evaluating Stimulus-Preceding Negativity as an Implicit Trigger for Adaptive Mixed Reality](http://arxiv.org/abs/2601.18750v1) | Francesco Chiossi, Elnur Imamaliyev et al. | Mixed Reality (MR) interfaces increasingly rely on gaze for interaction , yet distinguishing visual attention from intentional action remains difficult, leading to the Midas Touch problem. Existing solutions require explicit confirmations, while brain-computer interfaces may provide an implicit marker of intention using Stimulus-Preceding Negativity (SPN). We investigated how Intention (Select vs. Observe) and Feedback (With vs. Without) modulate SPN during gaze-based MR interactions. During realistic selection tasks, we acquired EEG and eye-tracking data from 28 participants. SPN was robustly elicited and sensitive to both factors: observation without feedback produced the strongest amplitudes, while intention to select and expectation of feedback reduced activity, suggesting SPN reflects anticipatory uncertainty rather than motor preparation. Complementary decoding with deep learning models achieved reliable person-dependent classification of user intention, with accuracies ranging from 75% to 97% across participants. These findings identify SPN as an implicit marker for building intention-aware MR interfaces that mitigate the Midas Touch. |
| 2026-01-26 | [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](http://arxiv.org/abs/2601.18735v1) | Jusheng Zhang, Yijia Fan et al. | Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems. |
| 2026-01-26 | [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](http://arxiv.org/abs/2601.18702v1) | Hansheng Ren | Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI. |
| 2026-01-26 | [A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts](http://arxiv.org/abs/2601.18656v1) | Sarika Aggarwal, Phillip B. Nicol et al. | Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations. |
| 2026-01-26 | [Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](http://arxiv.org/abs/2601.18639v1) | Ojasva Mishra, Xiaolong Wu et al. | The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($œÑ=1.0$~s, $Œît=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments. |
| 2026-01-26 | [Physics-Informed Uncertainty Enables Reliable AI-driven Design](http://arxiv.org/abs/2601.18638v1) | Tingkai Xue, Chin Chun Ooi et al. | Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs. |
| 2026-01-26 | [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](http://arxiv.org/abs/2601.18586v1) | Miguel Costa, Arthur Vandervoort et al. | Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities. |
| 2026-01-26 | [Self-Refining Video Sampling](http://arxiv.org/abs/2601.18577v1) | Sangwon Jang, Taekyung Ki et al. | Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler. |
| 2026-01-23 | [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](http://arxiv.org/abs/2601.16936v1) | Anton Zamyatin, Patrick Indri et al. | In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble. |
| 2026-01-23 | [The shape function of the observed growth index](http://arxiv.org/abs/2601.16921v1) | Ziad Sakr, Jinglan Zheng | The growth index $Œ≥$ is a powerful trigger for detecting deviations from $Œõ$CDM. However, its value is often determined by considering an asymptotic constant value that works for all redshift, or else following a chosen parameterisation. Here we formulate the growth index as function of three quantities that could be directly related to observables in redshift bins, $fœÉ_8(z_i)$, $f(z_i)$ and $H(z_i)$. We determine its value and its derivative at observed nodal center of redshift bins and use the shape function method, after showing insightful connection with its underlying governing virtual-work conservation principle, to construct a redshift dependence of the $Œ≥$ without assuming a specific parameterization. We then use the resulting shape function to test if we can disentangle between different scenarios where there are discrepancies between its three constituent measured components. We also tested whether it can be used to rule out models of modified gravity, or extended parametric models of the growth index that capture more general behaviors with an additional parameter as function of the scale factor or dark energy. Adopting forecasted measurements from next generation surveys on the three quantities used to construct $Œ≥$, we find that reported discrepancies between them could be detected with our method, but at the bins where the errors and lost of precision from our addition of degrees of freedom is small with respect to the deviation of $Œ≥$. The same could be concluded for first order extensions to $Œ≥$ or common modified gravity models, and to a lesser degree for dynamical dark energy models after supposing the latest DESI values. We conclude that this method is a strong tool to investigate cosmology in a model-independent way especially with forthcoming data delivered by further stage-IV surveys with more stringent uncertainties.(Abridged) |
| 2026-01-23 | [Dynamics of AGN feedback in the X-ray bright East and Southwest arms of M87, mapped by XRISM](http://arxiv.org/abs/2601.16901v1) | A. Simionescu, C. Kilbourne et al. | As the central galaxy in the nearest cluster, M87 provides the best spatial resolution for disentangling the complex interactions between AGN jets and the surrounding environment. We investigate the velocity structure of the multitemperature X-ray gas in M87, particularly in the eastern and southwestern arms associated with past AGN outbursts, using high-resolution spectroscopy from XRISM/Resolve. We analyze a mosaic of XRISM/Resolve observations covering the core of M87, fitting single- and multi-temperature models to spectra extracted from different regions and energy bands. We assess the line-of-sight velocities and velocity dispersions of the hotter ambient and cooler uplifted gas phases, and evaluate systematic uncertainties related to instrumental gain calibration. The hotter ICM phase, traced by Fe He-$Œ±$ emission, shows velocity dispersions below $\sim100$ km/s, and no significant velocity shifts between the arms and a relaxed offset region, suggesting limited dynamical impact from older AGN lobes. In contrast, the cooler gas phase appears to exhibit larger line of sight velocity gradients up to several hundred km/s as well as a higher velocity dispersion than the ambient hot phase, although these conclusions remain tentative pending improvements in the robustness of the gain calibration at lower energies. The first microcalorimeter-resolved map of gas dynamics in M87 supports the uplift scenario for the X-ray arms, with the cooler gas in the east and southwest seemingly moving in opposite directions along the line of sight. The kinetic energy is a small fraction of the gravitational potential energy associated with the gas uplift, and XRISM further suggests that AGN-driven motions may be short-lived in the hot ambient ICM. These constraints provide important input towards shaping future models of AGN feedback. |
| 2026-01-23 | [Universal relation between dipole polarizability of finite nuclei and neutron-star compactness](http://arxiv.org/abs/2601.16894v1) | P. S. Koliogiannis, T. Ghosh et al. | The nuclear equation of state, which determines the structure and properties of neutron stars, remains subject to substantial theoretical uncertainties, leading to model dependence in predicted observables. Universal relations have emerged as a powerful tool to mitigate this dependence by linking neutron star observables in a framework-independent manner. In this work, we introduce a new universal relation that \emph{bridges} finite nuclei and neutron stars through the dimensionless quantity $Œ∂= Œ≤_{1.4}\tilde{L}^{-1}$, which couples the compactness of a $1.4~M_{\odot}$ neutron star to the slope of the nuclear symmetry energy at saturation. The relation is examined under a broad set of relativistic energy density functionals with point-coupling and meson-exchange interactions, as well as non-relativistic Skyrme functionals. We demonstrate that $Œ∂$ exhibits a strong exponential correlation with the electric dipole polarizability $Œ±_D$ in finite nuclei across all considered equations of state. By exploiting experimental $Œ±_D$ data for selected neutron-rich nuclei, we constrain $Œ∂$ and translate these constraints into equation-of-state-independent bounds on the neutron star radius $R_{1.4}$ and the symmetry-energy slope $L$, providing insights into the properties of neutron star matter. |
| 2026-01-23 | [Stochastic Modeling and Resource Dimensioning of Multi-Cellular Edge Intelligent Systems](http://arxiv.org/abs/2601.16848v1) | Jaume Anguera Peris, Joakim Jald√©n | Edge intelligence enables AI inference at the network edge, co-located with or near the radio access network, rather than in centralized clouds or on mobile devices. It targets low-latency, resource-constrained applications with large data volumes, requiring tight integration of wireless access and on-site computing. Yet system performance and cost-efficiency hinge on joint pre-deployment dimensioning of radio and computational resources, especially under spatial and temporal uncertainty. Prior work largely emphasizes run-time allocation or relies on simplified models that decouple radio and computing, missing end-to-end correlations in large-scale deployments. This paper introduces a unified stochastic framework to dimension multi-cell edge-intelligent systems. We model network topology with Poisson point processes, capturing random user and base-station locations, inter-cell interference, distance-based fractional power control, and peak-power constraints. By combining this with queueing theory and empirical AI inference workload profiling, we derive tractable expressions for end-to-end offloading delay. These enable a non-convex joint optimization that minimizes deployment cost under statistical QoS guarantees, expressed through strict tail-latency and inference-accuracy constraints. We prove the problem decomposes into convex subproblems, yielding global optimality. Numerical results in noise- and interference-limited regimes identify cost-efficient design regions and configurations that cause under-utilization or user unfairness. Smaller cells reduce transmission delay but raise per-request computing cost due to weaker server multiplexing, whereas larger cells show the opposite trend. Densification reduces computational costs only when frequency reuse scales with base-station density; otherwise, sparser deployments improve fairness and efficiency in interference-limited settings. |
| 2026-01-23 | [Calibrated Probabilistic Interpolation for GEDI Biomass](http://arxiv.org/abs/2601.16834v1) | Robin Young, Srinivasan Keshav | Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.   To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation. |
| 2026-01-23 | [Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results](http://arxiv.org/abs/2601.16830v1) | Andrew Thompson, Miles McCrory | We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion. |
| 2026-01-23 | [Improved measurements of the age of JWST galaxies at z=6-10](http://arxiv.org/abs/2601.16772v1) | M. Lopez-Corredoira, C. M. Gutierrez | From James Webb Space Telescope (JWST) surveys, 31 galaxies with average redshift 7.3 are selected containing large Balmer break, Lyman-$Œ±$ break (V-shaped SED versus $Œª$). Apart from Hubble Space Telescope (HST) and JWST-NIRCam (Near-infrared camera) photometry for these galaxies, there are JWST-NIRSpec (Near-infrared spectrograph) spectra for 13 galaxies and mid-infrared photometry (mostly JWST-MIRI) for 15 of them. Spectroscopical analyses included Balmer emission lines, Balmer + 4000 angstroms breaks or CaII lines. Spectral energy distribution (SED) fitting with photometry include old and young stellar populations, emission lines associated to HII regions, AGN, interstellar dust extinction and intergalactic extinction from neutral hydrogen. By adopting realistic extinction curves and taking into account the V-shaped SED and low emission at near infrared at rest, the analyses show that AGN contribution in these galaxies ('little red dots' most of them) should be small on average in the reddest wavelengths, though important for few of the 31 galaxies. Average age of the 31 galaxies: $0.61\pm 0.31$(95% CL) Gyr, while the average age of the $Œõ$CDM universe is 0.70 Gyr. This corresponds to a formation epoch $z_{ form.}>11.2$(97.5% CL). Reddest galaxies present largest ages. One of these very red galaxies gets an age incompatible to be younger than the age of the Universe within $>4.7œÉ$. TP-AGB effect cannot explain this tension. None the less, there may be other uncertainties in the models, so this tension is a provisional result and further research is needed to confirm it. |
| 2026-01-23 | [From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model](http://arxiv.org/abs/2601.16769v1) | Ian Carb√≥ Casals | Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availability when measuring uncertainty. We present a Bayesian state-space framework that turns aggregated news sentiment into a smoothed time series with uncertainty. The model treats each weekly sentiment value as a noisy measurement of an underlying sentiment process, with observation uncertainty scaled by the effective information weight $n_{tj}$: when coverage is high, latent sentiment is anchored more strongly to the observed aggregate; when coverage is low, inference relies more on the latent dynamics and uncertainty increases. Using news data grouped into multiple categories, we find broadly similar latent dynamics across categories, while larger differences appear in observation noise. The framework is designed for descriptive monitoring and can be extended to other text sources where information availability varies over time. |
| 2026-01-23 | [Do LLM hallucination detectors suffer from low-resource effect?](http://arxiv.org/abs/2601.16766v1) | Debtanu Datta, Mohan Kishore Chilukuri et al. | LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource languages like Bengali. We study the intersection of these issues and ask: do hallucination detectors suffer from the low-resource effect? We conduct experiments on five tasks across three domains (factual recall, STEM, and Humanities). Experiments with four LLMs and three hallucination detectors reveal a curious finding: As expected, the task accuracies in low-resource languages experience large drops (compared to English). However, the drop in detectors' accuracy is often several times smaller than the drop in task accuracy. Our findings suggest that even in low-resource languages, the internal mechanisms of LLMs might encode signals about their uncertainty. Further, the detectors are robust within language (even for non-English) and in multilingual setups, but not in cross-lingual settings without in-language supervision. |
| 2026-01-22 | [Cyclic sunspot activity during the first millennium CE as reconstructed from radiocarbon](http://arxiv.org/abs/2601.16203v1) | Ilya Usoskin, Sami K. Solanki et al. | Context. Solar activity, dominated by the 11-year cyclic evolution, has been observed directly since 1610. Before that, indirect cosmogenic proxy data are used to reconstruct it over millennia. Recently, the precision of radiocarbon measurements has improved sufficiently to allow reconstructing solar activity over millennia. Aims. The first detailed reconstruction of solar activity, represented by annual sunspot numbers, is presented for 1-969 CE. Methods. The reconstruction of sunspot numbers from D14C was performed using a physics-based method involving several steps: using a carbon-cycle box model, the 14C production rate, corrected for the geomagnetic shielding, was computed from the measured data; The open solar magnetic flux was computed using a model of the heliospheric cosmic-ray modulation; Sunspot numbers were calculated using a model of the evolution of the Sun's magnetic field. The Markov Chain Monte Carlo approach was used to account for different sources of uncertainty. Results. Annual sunspot numbers were reconstructed for the first millennium CE. This period includes one extreme solar event of 774 CE and one Grand solar minimum of 650-730 CE. We could identify 91 solar cycles, of which 26 were well-defined, while 24 and 41 were reasonably and poorly defined, respectively. The mean cycle length was 10.6 years, but the lengths of individual cycles vary between 8 and 15 years. The existence of empirical Waldmeier's relations remains inconclusive. No significant periodicities were found beyond the 11-year cycle. Conclusions. This work fills the gap in the solar cycle statistics between the previously reconstructed first millennium BCE and the second millennium CE, providing vital constraints for the solar dynamo and irradiance models. A consistent 3-millennium-long reconstruction of sunspot numbers, based on a composite multi-proxy cosmogenic record, is pending. |
| 2026-01-22 | [Stochastic Control Barrier Functions under State Estimation: From Euclidean Space to Lie Groups](http://arxiv.org/abs/2601.16198v1) | Ruoyu Lin, Magnus Egerstedt | Ensuring safety for autonomous systems under uncertainty remains challenging, particularly when safety of the true state is required despite the true state not being fully known. Control barrier functions (CBFs) have become widely adopted as safety filters. However, standard CBF formulations do not explicitly account for state estimation uncertainty and its propagation, especially for stochastic systems evolving on manifolds. In this paper, we propose a safety-critical control framework with a provable bound on the finite-time safety probability for stochastic systems under noisy state information. The proposed framework explicitly incorporates the uncertainty arising from both process and measurement noise, and synthesizes controllers that adapt to the level of uncertainty. The framework admits closed-form solutions in linear settings, and experimental results demonstrate its effectiveness on systems whose state spaces range from Euclidean space to Lie groups. |
| 2026-01-22 | [Studying energy-resolved transport with wavepacket dynamics on quantum computers](http://arxiv.org/abs/2601.16180v1) | Melody Lee, Roland C. Farrell | Probing energy-dependent transport in quantum simulators requires preparing states with tunable energy and small energy variance. Existing approaches often study quench dynamics of simple initial states, such as computational basis states, which are far from energy eigenstates and therefore limit the achievable energy resolution. In this work, we propose using wavepackets to probe transport properties with improved energy resolution. To demonstrate the utility of this approach, we prepare and evolve wavepackets on Quantinuum's H2-2 quantum computer and identify an energy-dependent localization transition in the Anderson model on an 8x7 lattice--a finite-size mobility edge. We observe that a wavepacket initialized at low energy remains spatially localized under time evolution, while a high-energy wavepacket delocalizes, consistent with the presence of a mobility edge. Crucial to our experiments is an error mitigation strategy that infers the noiseless output bit string distribution using maximum-likelihood estimation. Compared to post-selection, this method removes systematic errors and reduces statistical uncertainty by up to a factor of 5. We extend our methods to the many-particle regime by developing a quantum algorithm for preparing quasiparticle wavepackets in a one-dimensional model of interacting fermions. This technique has modest quantum resource requirements, making wavepacket-based studies of transport in many-body systems a promising application for near-term quantum computers. |
| 2026-01-22 | [Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints](http://arxiv.org/abs/2601.16174v1) | Yiyao Yang | Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods. |
| 2026-01-22 | [\textit{Ab initio} Gamow density matrix renormalization group for broad nuclear many-body resonances](http://arxiv.org/abs/2601.16168v1) | A. Sehovic, K. Fossez et al. | \textbf{Background} The reach of \textit{ab initio} theory has greatly increased in recent decades. However, predicting the location of the drip lines remains challenging due to uncertainties in nuclear forces and difficulties in describing nuclei that behave as open quantum systems. \textbf{Purpose} In this work, we extend the \textit{ab initio} Gamow Density Matrix Renormalization Group (G-DMRG) approach to the regime of broad many-body resonances to pave the way for systematic tests of nuclear forces in light exotic nuclei. \textbf{Methods} To stabilize calculations, we introduce a new truncation scheme in the reference space, and propose an orbital ordering based on entanglement considerations. We then show how continuum couplings increase entanglement in the many-body problem, and propose a new truncation scheme to stabilize the renormalization and accelerate calculations in extreme conditions. Finally, we demonstrate that natural orbitals can be used to efficiently describe broad resonances by introducing a new ordering scheme and by redefining the reference space based on occupations. \textbf{Results} Leveraging our findings, we propose a recipe to converge \textit{ab initio} G-DMRG calculations and apply it in low-lying states of \isotope[5,6]{He} and \isotope[4]{H}, demonstrating control of the renormalization and the emergence of convergence patterns. We also obtain the first direct \textit{ab initio} calculation of the $J^œÄ= {1/2}^+$ ground state of \isotope[5]{H}. \textbf{Conclusions} We demonstrate that entanglement due to continuum couplings can be controlled in extreme conditions and successfully extend the G-DMRG approach in the regime of broad many-body resonances. |
| 2026-01-22 | [On computational schemes for the Magnus expansion of the in-medium similarity renormalization group](http://arxiv.org/abs/2601.16133v1) | Matthias Heinz | The in-medium similarity renormalization group (IMSRG) is a popular many-body method used for computations of nuclei. It solves the many-body Schr√∂dinger equation through a continuous unitary transformation of the many-body Hamiltonian. The IMSRG transformation is typically truncated at the normal-ordered two-body level, the IMSRG(2), but recently several approaches have been developed to capture the effects of normal-ordered three-body operators, the IMSRG(3). In particular, a factorized approximation to the IMSRG(3) proposes to capture the leading effects of three-body operators at the same computational cost as the IMSRG(2) approximation. This approach often employs an approximate scheme for solving the IMSRG equations, the so-called hunter-gatherer scheme. In this work, I study the uncertainty associated with this scheme. I find that the hunter-gatherer scheme differs by up to $7\,\mathrm{MeV}$ for ground-state energies and $0.5\,\mathrm{MeV}$ for excitation energies from standard IMSRG(2) approaches. These differences are in some cases comparable to the expected size of IMSRG(3) corrections. |
| 2026-01-22 | [Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision](http://arxiv.org/abs/2601.16109v1) | Yashuai Yan, Tobias Egle et al. | We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion. |
| 2026-01-22 | [SAMTok: Representing Any Mask with Two Words](http://arxiv.org/abs/2601.16093v1) | Yikang Zhou, Tao Zhang et al. | Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available. |
| 2026-01-22 | [Enhanced Representation-Based Sampling for the Efficient Generation of Datasets for Machine-Learned Interatomic Potentials](http://arxiv.org/abs/2601.16047v1) | Moritz Ren√© Sch√§fer, Johannes K√§stner | In this work, we present Enhanced Representation-Based Sampling (ERBS), a novel enhanced sampling method designed to generate structurally diverse training datasets for machine-learned interatomic potentials. ERBS automatically identifies collective variables by dimensionality reduction of atomic descriptors and applies a bias potential inspired by the On-the-Fly Probability Enhanced Sampling framework. We highlight the ability of Gaussian moment descriptors to capture collective molecular motions and explore the impact of biasing parameters using alanine dipeptide as a benchmark system. We show that free energy surfaces can be reconstructed with high fidelity using only short biased trajectories as training data. Further, we apply the method to the iterative construction of a liquid water dataset and compare the quality of simulated self-diffusion coefficients for models trained with molecular dynamics and ERBS data. Further, we active-learn models for liquid water with and without enhanced sampling and compare the quality of simulated self-diffusion coefficients. The self-diffusion coefficients closely match those simulated with a reference model at a significantly reduced dataset size. Finally, we compare the sampling behaviour of enhanced sampling methods by benchmarking the mean squared displacements of \ce{BMIM+BF4-} trajectories simulated with uncertainty-driven dynamics and ERBS and find that the latter significantly increases the exploration of configurational space. |
| 2026-01-22 | [Data-Driven Conditional Flexibility Index](http://arxiv.org/abs/2601.16028v1) | Moritz Wedemeyer, Eike Cramer et al. | With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information. |
| 2026-01-21 | [RayRoPE: Projective Ray Positional Encoding for Multi-view Attention](http://arxiv.org/abs/2601.15275v1) | Yu Wu, Minsik Jeon et al. | We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information. |
| 2026-01-21 | [Complexity analysis and practical resolution of the data classification problem with private characteristics](http://arxiv.org/abs/2601.15178v1) | David Pantoja, Ismael Rodriguez et al. | In this work we analyze the problem of, given the probability distribution of a population, questioning an unknown individual that is representative of the distribution so that our uncertainty about certain characteristics is significantly reduced -but the uncertainty about others, deemed private or sensitive, is not. Thus, the goal of the problem is extracting information being relevant to a legitimate purpose while preserving the privacy of individuals, which is crucial to enable non-intrusive selection processes in several areas. For instance, it is essential in the design of non-discriminatory personnel selection, promotion, and layoff processes in companies and institutions; in the retrieval of customer information being relevant to the service provided by a company (and no more); in certifications not revealing sensitive industrial information being irrelevant for the certification itself; etc. Interactive questioning processes are constructed for this purpose, which requires generalizing the notion of decision trees to account the amount of desired and undesired information retrieved for each branch of the plan. Our findings about this problem are both theoretical and practical: on the one hand, we prove its NP-completeness by a reduction from the Set Cover problem; and on the other hand, given this intractability, we provide heuristic solutions to find reasonable solutions in affordable time. In particular, a greedy algorithm and two genetic algorithms are presented. Our experiments indicate that the best results are obtained using a genetic algorithm reinforced with a greedy strategy. |
| 2026-01-21 | [Path-OED for infinite-dimensional Bayesian linear inverse problems governed by PDEs](http://arxiv.org/abs/2601.15168v1) | J. Nicholas Neuberger, Alen Alexanderian et al. | We consider infinite-dimensional Bayesian linear inverse problems governed by time-dependent partial differential equations (PDEs) and develop a mathematical and computational framework for optimal design of mobile sensor paths in this setting. The proposed path optimal experimental design (path-OED) framework is established rigorously in a function space setting and elaborated for the case of Bayesian c-optimality, which quantifies the posterior variance in a linear functional of the inverse parameter. The latter is motivated by goal-oriented formulations, where we seek to minimize the uncertainty in a scalar prediction of interest. To facilitate computations, we complement the proposed infinite-dimensional framework with discretized formulations, in suitably weighted finite-dimensional inner product spaces, and derive efficient methods for finding optimal sensor paths. The resulting computational framework is flexible, scalable, and can be adapted to a broad range of linear inverse problems and design criteria. We also present extensive computational experiments, for a model inverse problem constrained by an advection-diffusion equation, to demonstrate the effectiveness of the proposed approach. |
| 2026-01-21 | [The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models](http://arxiv.org/abs/2601.15165v1) | Zanlin Ni, Shenzhi Wang et al. | Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap |
| 2026-01-21 | [Stochastic EMS for Optimal 24/7 Carbon-Free Energy Operations](http://arxiv.org/abs/2601.15135v1) | Natanon Tongamrak, Kannapha Amaruchkul et al. | This paper proposes a two-stage stochastic optimization formulation to determine optimal operation and procurement plans for achieving a 24/7 carbon-free energy (CFE) compliance at minimized cost. The system in consideration follows primary energy technologies in Thailand including solar power, battery storage, and a diverse portfolio of renewable and carbon-based energy procurement sources. Unlike existing literature focused on long-term planning, this study addresses near real-time operations using a 15-minute resolution. A novel feature of the formulation is the explicit treatment of CFE compliance as a model parameter, enabling flexible targets such as a minimum percentage of hourly matching or a required number of carbon-free days within a multi-day horizon. The mixed-integer linear programming formulation accounts for uncertainties in load and solar generation by integrating deep learning-based forecasting within a receding horizon framework. By optimizing battery profiles and multi-source procurement simultaneously, the proposed system provides a feasible pathway for transitioning to carbon-free operations in emerging energy markets. |
| 2026-01-21 | [From carbon management strategies to implementation: Modeling and physical simulation of CO2 pipeline infrastructure - a case study for Germany](http://arxiv.org/abs/2601.15090v1) | Mehrnaz Anvari, Marius Neuwirth et al. | Carbon capture and storage or utilization (CCUS) will play an important role to achieve climate neutrality in many economies. Pipelines are widely regarded as the most efficient means of CO2 transport; however, they are currently non-existent. Policy-makers and companies need to develop large-scale infrastructure under substantial uncertainty. Methods and analyses are needed to support pipeline planning and strategy development. This paper presents an integrated method for designing CO2 pipeline networks by combining energy system scenarios with physical network simulation. Using Germany as a case study, we derive spatially highly resolved CO2 balances to develop a dense-phase CO2 pipeline topology that follows existing gas pipeline corridors. The analyzed system includes existing sites for cement and lime production, waste incineration, carbon users, four coastal CO2 hubs, and border crossing points. We then apply the multiphysical network simulator MYNTS to assess the technical feasibility of this network. We determine pipeline diameters, pump locations, and operating conditions that ensure stable dense-phase transport. The method explicitly accounts for elevation and possible impurities. The results indicate that a system of about 7000 km pipeline length and a mixed normed diameter of DN700 on main corridors and of DN500/DN400 on branches presents a feasible solution to connect most sites. Investment costs for the optimized pipeline system are calculated to be about 17 billion Euros. The method provides a reproducible framework and is transferable to other countries and to European scope. |
| 2026-01-21 | [Space-time evolution of particle emission in p$-$Pb collisions at $\mathbf{\sqrt{s_{\rm NN}}=~5.02}$ TeV with 3D kaon femtoscopy](http://arxiv.org/abs/2601.15081v1) | ALICE Collaboration | The measurement of three-dimensional femtoscopic correlations between identical charged kaons (K$^\pm$K$^\pm$) produced in p$-$Pb collisions at center-of-mass energy per nucleon pair $\sqrt{s{_{\rm NN}}} = 5.02$ TeV with ALICE at the LHC is presented for the first time. This measurement, supplementary to those in pp and Pb$-$Pb collisions, allows understanding the particle-production mechanisms at different charged-particle multiplicities and provides information on the dynamics of the source of particles created in p$-$Pb collisions, for which a general consensus does not yet exist. It is shown that the measured source sizes increase with charged-particle multiplicity and decrease with increasing pair transverse momentum. These trends for K$^\pm$K$^\pm$ are similar to the ones observed earlier in identical charged-pion and K$_{\rm s}^{0}$K$_{\rm s}^{0}$ correlations in Pb$-$Pb collisions at various energies and in $œÄ^\pm œÄ^\pm$ correlations in p$-$Pb collisions at $\sqrt{s{_{\rm NN}}} = 5.02$ TeV. At comparable multiplicity, the source sizes measured in p$-$Pb collisions agree within uncertainties with those observed in pp collisions, and there is an indication that they are smaller than those observed in Pb$-$Pb collisions. The obtained results are also compared with predictions from the hadronic interaction model EPOS~3, which tends to underestimate the source size for the most central collisions and agrees with the data for semicentral and peripheral events. Furthermore, the time of maximal emission for kaons is extracted. It turns out to be comparable with the value obtained in highly peripheral Pb$-$Pb collisions at the same energy, indicating that the kaon emission evolution is similar to that in p$-$Pb collisions. |
| 2026-01-21 | [One- and three-dimensional identical charged-kaon femtoscopic correlations in Pb--Pb collisions at $\mathbf{ \sqrt{s_\mathrm{NN}}=5.02}$ TeV](http://arxiv.org/abs/2601.15054v1) | ALICE Collaboration | The identical charged-kaon correlations induced by quantum-statistics effects and final-state interactions are measured in Pb$-$Pb collisions at $\sqrt{s_{\rm NN}} = 5.02$ TeV. The results of one- (1D) and three-dimensional (3D) analyses show that the obtained system-size parameters (radii) are smaller for more peripheral collisions and decrease with increasing pair transverse momentum $k_{\rm T}$. The 1D parameters agree within uncertainties with those obtained in Pb$-$Pb collisions at $\sqrt{s_{\rm NN}}=2.76$ TeV. The observed power-law dependence of the extracted 3D radii as a function of the pair transverse momentum is a signature of the collective flow in the particle-emitting system created in Pb$-$Pb collisions. This dependence is well reproduced by the integrated hydrokinetic model calculations except for the outward projection of the radius (measured in the longitudinally co-moving system) for the most central collisions. The time of maximal emission for kaons is extracted from the 3D analysis in a wide collision centrality range from 0 to 90%. Its reduction with decreasing charged-particle multiplicity is well reproduced by the hydrokinetic model predictions, and means that kaons are emitted earlier in more peripheral events. |
| 2026-01-21 | [Information mechanics: conservation and exchange](http://arxiv.org/abs/2601.15028v1) | Takuya Isomura | Inference and learning are commonly cast in terms of optimisation, yet the fundamental constraints governing uncertainty reduction remain unclear. This work presents a first-principles framework inherent to Bayesian updating, termed information mechanics (infomechanics). Any pointwise reduction in posterior surprisal is exactly balanced by information gained from data, independently of algorithms, dynamics, or implementation. Imposing additivity, symmetry, and robustness collapses the freedom of this identity to only two independent conservation relations. One governs the global redistribution of uncertainty and recovers Shannon entropy. The other captures a complementary local geometric component, formalised as Fisher information. Together, these conserved quantities motivate a non-additive state function, the information potential $Œ¶$, which isolates structural degrees of freedom beyond entropy while remaining invariant under reparametrisation. $Œ¶$ quantifies local sharpness and ruggedness in posterior beliefs and vanishes uniquely for isotropic Gaussian distributions. In a low-temperature regime, $Œ¶$ scales logarithmically with the effective number of local optima, linking information geometry to computational complexity. This formalises an information-computation exchange, whereby information acquisition reshapes the inference landscape and reduces computational demands. By separating invariant informational constraints from inference mechanisms, this framework provides a unified, algorithm-independent foundation for inference, learning, and computation across biological and artificial systems. |
| 2026-01-21 | [ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data](http://arxiv.org/abs/2601.15025v1) | Marian Renz, Martin G√ºnther et al. | While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform. |
| 2026-01-20 | [Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration](http://arxiv.org/abs/2601.14235v1) | LSST Dark Energy Science Collaboration, Eric Aubourg et al. | The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors. |
| 2026-01-20 | [HALT: Hallucination Assessment via Latent Testing](http://arxiv.org/abs/2601.14210v1) | Rohan Bhatnagar, Youran Sun et al. | Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI. |
| 2026-01-20 | [Dynamical mass of a solar-like oscillator at the main-sequence turnoff from Gaia astrometry & ground-based spectroscopy](http://arxiv.org/abs/2601.14197v1) | P. G. Beck, T. Masseron et al. | Asteroseismology is widely used for precise determining of masses of solar-like oscillating stars by performing individual-frequency modeling or applying homological scaling relations. However, these methods lack dynamical validation on the main sequence due to the absence of eclipsing double-lined binary system (SB2) as benchmark objects. By providing the orbital inclination, astrometric binary systems from ESA Gaia DR3 offer an abundant alternative for eclipsing systems. We present KIC693187 as the first SB2, hosting a solar-like oscillating post-main-sequence star with dynamical masses. By combining Gaia astrometry with spectroscopic obtained with the Las Cumbres Observatory network (LCO), we find $M_1^\mathrm{dyn}$=0.99$\pm$0.05$M_\odot$ and $M_2^\mathrm{dyn}$=0.89$\pm$0.04$M_\odot$ for the primary and secondary, respectively. Asteroseismic parameters were extracted from photometry of the NASA \Kepler satellite. The mass from individual frequency modeling is $M_1^\mathrm{IF}$=0.92$\pm$0.01$M_\odot$. Taking into account the systematic uncertainty of 0.04$M_\odot$ for best fit models from individual frequency fitting, we find an agreement within 1.2$œÉ$. From scaling relations we obtain a mass range of 0.93 to 0.98$M_\odot$ by using the observed large frequency separations (\dnu) in the scaling relations for the primary. By using standard corrections for departures from the asymptotic regime of \dnu, we obtained a mass range of 0.83 to 1.03$M_\odot$. The upper ends of both ranges agree well with the dynamical mass of the primary. This approach provides the first empirical validation for main-sequence solar-like oscillators and opens a new window for validating asteroseismology. Through a dedicatded program targeting astrometric SB2 binary systems, ESA's PLATO space mission will provide will enlarge the benchmark sample substantially. |
| 2026-01-20 | [Measurement of the Z$Œ≥$ production cross section and search for anomalous neutral triple gauge couplings in pp collisions at $\sqrt{s}$ = 13 TeV](http://arxiv.org/abs/2601.14102v1) | CMS Collaboration | A measurement of the fiducial cross section of the associated production of a Z boson and a high-$p_\mathrm{T}$ photon, where the Z decays to two neutrinos, and a search for anomalous triple gauge couplings are reported. The results are based on data collected by the CMS experiment at the LHC in proton-proton collisions at $\sqrt{s}$ = 13 TeV during 2016$-$2018, corresponding to an integrated luminosity of 138 fb$^{-1}$. The fiducial Z$Œ≥$ cross section, where a photon with a $p_\mathrm{T}$ greater than 225 GeV is produced in association with a Z, and the Z decays to a $ŒΩ\barŒΩ$ pair (Z($ŒΩ\barŒΩ$)$Œ≥$), is measured to be 23.3$^{+1.4}_{-1.3}$ fb, in agreement, within uncertainties, with the standard model prediction. The differential cross section as a function of the photon $p_\mathrm{T}$ has been measured and compared with standard model predictions computed at next-to-leading and at next-to-next-to-leading order in perturbative quantum chromodynamics. Constraints have been placed on the presence of anomalous couplings that affect the ZZ$Œ≥$ and Z$Œ≥Œ≥$ vertex using the $p_\mathrm{T}$ spectrum of the photons. The observed 95% confidence level intervals for $CP$-conserving $h_3^Œ≥$ and $h_4^Œ≥$ are determined to be ($-$3.4, 3.5) $\times$ 10$^{-4}$ and ($-$6.8, 6.8) $\times$ 10$^{-7}$, and for $h_3^\mathrm{Z}$ and $h_4^\mathrm{Z}$ they are ($-$2.2, 2.2) $\times$ 10$^{-4}$ and ($-$4.1, 4.2) $\times$ 10$^{-7}$, respectively. These are the strictest limits to date on $h_3^Œ≥$, $h_3^\mathrm{Z}$ and $h_4^\mathrm{Z}$. |
| 2026-01-20 | [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](http://arxiv.org/abs/2601.13885v1) | Esma Balkƒ±r, Alice Pernthaller et al. | Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 œÑ over random sampling, with 95% accuracy on confident predictions. |
| 2026-01-20 | [HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction](http://arxiv.org/abs/2601.13801v1) | Yuhua Jin, Nikita Kuzmin et al. | Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction. |
| 2026-01-20 | [Variational Dual-path Attention Network for CSI-Based Gesture Recognition](http://arxiv.org/abs/2601.13745v1) | N. Zhang | Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems. |
| 2026-01-20 | [SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation](http://arxiv.org/abs/2601.13732v1) | Andreas Wiedholz, Rafael Paintner et al. | The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison. |
| 2026-01-20 | [Thermodynamics and Gravitational Signatures of Rotating Black Holes in the Generalized Extended Uncertainty Principle](http://arxiv.org/abs/2601.13725v1) | Nikko John Leo S. Lobos | We investigate the phenomenological implications of quantum gravity on rotating black holes within the framework of the Generalized Extended Uncertainty Principle (GEUP), which incorporates both minimal length (ultraviolet) and large-scale (infrared) corrections. Lacking a full non-perturbative formulation of quantum gravity, we adopt a metric-based approach. We construct a stationary, axisymmetric ansatz via the Newman-Janis algorithm to model the kinematic features of a rotating black hole subject to Generalized Extended Uncertainty Principle (GEUP) corrections. The thermodynamic analysis reveals that in the infrared-dominated regime, the Hawking temperature scales as $T_H \sim M^{-3}$, leading to a rapid cooling phase that significantly prolongs the lifetime of supermassive black holes. We derive the modified Teukolsky Master Equation for gravitational perturbations and demonstrate that the background geometry preserves the isospectrality between axial and polar modes. In the eikonal limit, the quasinormal mode (QNM) spectrum exhibits orthogonal shifts: the minimal length parameter $Œ≤$ induces a spectral blueshift and enhanced damping, while the large-scale parameter $Œ±$ induces a spectral redshift and suppressed damping. Finally, we constrain the theory using observational data from LIGO/Virgo and the Event Horizon Telescope. We establish that the shadow of M87* is approximately $10^6$ times more sensitive to large-scale corrections than Sgr A*, placing stringent bounds on the EUP parameter, while gravitational wave spectroscopy provides complementary constraints on the GUP sector. |
| 2026-01-20 | [Cost-Effectiveness of Adult Hepatitis A Vaccination Strategies in Korea Under an Aging Susceptibility Profile](http://arxiv.org/abs/2601.13714v1) | Yuna Lim, Gerardo Chowell et al. | Hepatitis A severity increases sharply with age, while Korea is experiencing a cohort shift in which low seroprevalence adult cohorts are aging into older, higher fatality age groups. This demographic and immunological transition creates an urgent policy question regarding how adult vaccination should be prioritized under resource constraints. We evaluated three adult vaccination scenarios targeting low seroprevalence age groups (S1) 20 to 39 years, (S2) 40 to 59 years, and (S3) 20 to 59 years. Using an age structured dynamic transmission model calibrated to Korean data, we derived dynamically feasible vaccination allocation trajectories under realistic capacity constraints using an optimal control framework and linked these trajectories to long term transmission model simulations. We conducted DALY based cost effectiveness analyses over a lifetime horizon from both healthcare system and societal perspectives, and characterized uncertainty using probabilistic sensitivity analysis (PSA) and cost effectiveness acceptability curves (CEACs). Robustness was examined using one way sensitivity analyses. In the base case, S2 consistently yields the most favorable and robust cost effectiveness profile under both perspectives, with the lowest ICER. S3 achieved the largest reduction in DALYs but requires substantially higher incremental costs, resulting in a higher ICER than S2. S1 produces the smallest DALY reduction and is the least efficient strategy. PSA and CEACs confirm that S2 remains the preferred option across most willingness to pay ranges. S2 offers the most balanced and robustly cost effective strategy in Korea, capturing substantial mortality reduction while limiting additional program costs. S3 may be justified when higher budgets or willingness to pay thresholds are acceptable, but S2 provides the clearest value for money under epidemiological and economic conditions. |
| 2026-01-20 | [Stochastic Dynamic Pricing of Electric Vehicle Charging with Heterogeneous User Behavior: A Stackelberg Game Framework](http://arxiv.org/abs/2601.13571v1) | Yongqi Zhang, Dong Ngoduy et al. | The rapid adoption of electric vehicles (EVs) introduces complex spatiotemporal demand management challenges for charging station operators (CSOs), exacerbated by demand imbalances, behavioral heterogeneity, and system uncertainty. Traditional dynamic pricing models, often relying on deterministic EV-CS pairings and network equilibrium assumptions, frequently oversimplify user behavior and lack scalability. This study proposes a stochastic, behaviorally heterogeneous dynamic pricing framework formulated as a bi-level Stackelberg game. The upper level optimizes time-varying pricing to maximize system-wide utility, while the lower level models decentralized EV users via a multinomial logit (MNL) choice model incorporating price sensitivity, battery aging, risk attitudes, and network travel costs. Crucially, the model avoids network equilibrium constraints to enhance scalability, with congestion effects represented via queuing-theoretic approximations. To efficiently solve the resulting large-scale optimization problem, a rolling-horizon approach combining the Dynamic Probabilistic Sensitivity Analysis-guided Cross-Entropy Method (PSA-CEM) with the Method of Successive Averages (MSA) is implemented. A real-world case study in Clayton, Melbourne, validates the framework using 22 charging stations. Simulation results demonstrate that the proposed mechanism substantially reduces queuing penalties and improves user utility compared to fixed and time-of-use pricing. The framework provides a robust, scalable tool for strategic EV charging management, balancing realism with computational efficiency. |
| 2026-01-20 | [Near-field Physical Layer Security: Robust Beamforming under Location Uncertainty](http://arxiv.org/abs/2601.13549v1) | Chao Zhou, Changsheng You et al. | In this paper, we study robust beamforming design for near-field physical-layer-security (PLS) systems, where a base station (BS) equipped with an extremely large-scale array (XL-array) serves multiple near-field legitimate users (Bobs) in the presence of multiple near-field eavesdroppers (Eves). Unlike existing works that mostly assume perfect channel state information (CSI) or location information of Eves, we consider a more practical and challenging scenario, where the locations of Bobs are perfectly known, while only imperfect location information of Eves is available at the BS. We first formulate a robust optimization problem to maximize the sum-rate of Bobs while guaranteeing a worst-case limit on the eavesdropping rate under location uncertainty. By transforming Cartesian position errors into the polar domain, we reveal an important near-field angular-error amplification effect: for the same location error, the closer the Eve, the larger the angle error, severely degrading the performance of conventional robust beamforming methods based on imperfect channel state information. To address this issue, we first establish the conditions for which the first-order Taylor approximation of the near-field channel steering vector under location uncertainty is largely accurate. Then, we propose a two-stage robust beamforming method, which first partitions the uncertainty region into multiple fan-shaped sub-regions, followed by the second stage to formulate and solve a refined linear-matrix-inequality (LMI)-based robust beamforming optimization problem. In addition, the proposed method is further extended to scenarios with multiple Bobs and multiple Eves. Finally, numerical results validate that the proposed method achieves a superior trade-off between rate performance and secrecy robustness, hence significantly outperforming existing benchmarks under Eve location uncertainty. |
| 2026-01-20 | [TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning](http://arxiv.org/abs/2601.13545v1) | Shirin Shahabi, Spencer Graham et al. | Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com |
| 2026-01-20 | [MIU2Net: weak-lensing mass inversion using deep learning with nested U-structures](http://arxiv.org/abs/2601.13538v1) | Han W. G., An Zhao et al. | One of the primary goals of next-generation gravitational lensing surveys is to measure the large-scale distribution of dark matter, which requires accurate mass inversion to convert weak-lensing shear maps into convergence (kappa) fields. This work develops a mass inversion method tailored for upcoming space missions such as CSST and Euclid, aiming to recover both the mass distribution and the convergence power spectrum with high fidelity. We introduce MIU2Net, a versatile deep-learning framework for kappa-map reconstruction based on the U2-Net architecture. A new loss function is constructed to jointly estimate the convergence field and its frequency-domain energy distribution, effectively balancing optimal mean squared error and optimal power-spectrum recovery. The method incorporates realistic observational effects into shear fields, including shape noise, reduced shear, and complex masks. Under noise levels anticipated for future space-based lensing surveys, MIU2Net recovers the convergence power spectrum with 4% uncertainties up to l approximately 500, significantly outperforming Wiener filtering and MCALens. Beyond two-point statistics, the method accurately reconstructs the convergence distribution, peak centroid, and peak amplitude. Compared to other learning-based approaches such as DeepMass, MIU2Net reduces the root-mean-square error by 5% without smoothing and by 38% with a 1-arcmin smoothing scale. MIU2Net represents a substantial advancement in mass inversion methodology, offering improved accuracy in both RMSE and power-spectrum reconstruction. It provides a promising tool for mapping dark matter environments and large-scale structures in the era of next-generation space lensing surveys. |
| 2026-01-20 | [c-C3H2 deuteration towards prestellar and starless cores in the Perseus Molecular Cloud](http://arxiv.org/abs/2601.13495v1) | J. Ferrer Asensio, S. Scibelli et al. | Deuterium fractionation becomes highly efficient in cold, dense cores where CO is frozen out. Cyclopropenylidene (c-C3H2), an early-formed carbon ring, and its deuterated isotopologues trace gas-phase deuteration in these environments. We present a statistical study of c-C3H2 deuteration in starless and prestellar cores of the Perseus Molecular Cloud using observations of c-C3H2, c-C3HD and c-C3D2 obtained with the Yebes 40 m, ARO 12 m and IRAM 30 m telescopes towards 16 cores. Gaussian fits and RADEX modeling yield column densities for the detected species. c-C3H2 is detected in 14/15 covered cores, c-C3HD in 15/16, and c-C3D2 in 9/16. Derived column densities range from 0.5-8.1 x 10^{13} cm^{-2} for c-C3H2, 0.2-2.1 x 10^{12} cm^{-2} for c-C3HD, and 0.6-1.6 x 10^{11} cm^{-2} for c-C3D2. The ortho-to-para ratio of c-C3H2 is obtained for all but one core, with a median value of 3.5\pm0.4. Statistically corrected D/H ratios span 0.5-9.2% (median 1.5\pm0.2%), and D2/D ratios 9-55% (median 25.9\pm4.3%). No trend is found between the c-C3H2 ortho-to-para ratio and core evolutionary stage traced by n(H2). The median D/H ratio in Perseus appears lower than values reported for Taurus and Chamaeleon, while the D2/D ratio agrees with Taurus within uncertainties. A positive correlation between D/H and n(H2) supports the use of D/H as an evolutionary tracer. D2/D does not correlate with n(H2), but shows a positive correlation with T_{kin}, suggesting that its formation is influenced by a mildly endothermic pathway. |
| 2026-01-19 | [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](http://arxiv.org/abs/2601.13462v1) | Amine Rostane | Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections. |
| 2026-01-19 | [Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds](http://arxiv.org/abs/2601.13436v1) | Szabolcs Szentp√©teri, Bal√°zs Csan√°d Cs√°ji | Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments. |
| 2026-01-19 | [TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction](http://arxiv.org/abs/2601.13422v1) | Dahai Yu, Rongchao Xu et al. | Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines. |
| 2026-01-19 | [Pathway-based Bayesian factor models for gene expression data](http://arxiv.org/abs/2601.13419v1) | Lorenzo Mauri, Federica Stolf et al. | Interpreting gene expression data requires methods that can uncover coordinated patterns corresponding to biological pathways. Traditional approaches such as principal component analysis and factor models reduce dimensionality, but latent components may have unclear biological meaning. Current approaches to incorporate pathway annotations impose restrictive assumptions, require extensive hyperparameter tuning, and do not provide principled uncertainty quantification, hindering the robustness and reproducibility of results. Here, we develop Bayesian Analysis with gene-Sets Informed Latent space (BASIL), a scalable Bayesian factor modeling framework that incorporates gene pathway annotations into latent variable analysis for RNA-sequencing data. BASIL places structured priors on factor loadings, shrinking them toward combinations of annotated gene sets, enhancing biological interpretability and stability, while simultaneously learning new unstructured components. BASIL provides accurate covariance estimates and uncertainty quantification, without resorting to computationally expensive Markov chain Monte Carlo sampling. An automatic empirical Bayes procedure eliminates the need for manual hyperparameter tuning, promoting reproducibility and usability in practice. In simulations and large-scale human transcriptomic datasets, BASIL consistently outperforms state-of-the-art approaches, accurately reconstructing gene-gene covariance, selecting the correct latent dimension, and identifying biologically coherent modules. |
| 2026-01-19 | [A Two-Stage Bayesian Framework for Multi-Fidelity Online Updating of Spatial Fragility Fields](http://arxiv.org/abs/2601.13396v1) | Abdullah M. Braik, Maria Koliou | This paper addresses a long-standing gap in natural hazard modeling by unifying physics-based fragility functions with real-time post-disaster observations. It introduces a Bayesian framework that continuously refines regional vulnerability estimates as new data emerges. The framework reformulates physics-informed fragility estimates into a Probit-Normal (PN) representation that captures aleatory variability and epistemic uncertainty in an analytically tractable form. Stage 1 performs local Bayesian updating by moment-matching PN marginals to Beta surrogates that preserve their probability shapes, enabling conjugate Beta-Bernoulli updates with soft, multi-fidelity observations. Fidelity weights encode source reliability, and the resulting Beta posteriors are re-projected into PN form, producing heteroscedastic fragility estimates whose variances reflect data quality and coverage. Stage 2 assimilates these heteroscedastic observations within a probit-warped Gaussian Process (GP), which propagates information from high-fidelity sites to low-fidelity and unobserved regions through a composite kernel that links space, archetypes, and correlated damage states. The framework is applied to the 2011 Joplin tornado, where wind-field priors and computer-vision damage assessments are fused under varying assumptions about tornado width, sampling strategy, and observation completeness. Results show that the method corrects biased priors, propagates information spatially, and produces uncertainty-aware exceedance probabilities that support real-time situational awareness. |
| 2026-01-16 | [Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations](http://arxiv.org/abs/2601.11460v1) | Franziska Herbert, Vignesh Prasad et al. | Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems. |
| 2026-01-16 | [Learning-Based Shrinking Disturbance-Invariant Tubes for State- and Input-Dependent Uncertainty](http://arxiv.org/abs/2601.11426v1) | Abdelrahman Ramadan, Sidney Givigi | We develop a learning-based framework for constructing shrinking disturbance-invariant tubes under state- and input-dependent uncertainty, intended as a building block for tube Model Predictive Control (MPC), and certify safety via a lifted, isotone (order-preserving) fixed-point map. Gaussian Process (GP) posteriors become $(1-Œ±)$ credible ellipsoids, then polytopic outer sets for deterministic set operations. A two-time-scale scheme separates learning epochs, where these polytopes are frozen, from an inner, outside-in iteration that converges to a compact fixed point $Z^\star\!\subseteq\!\mathcal G$; its state projection is RPI for the plant. As data accumulate, disturbance polytopes tighten, and the associated tubes nest monotonically, resolving the circular dependence between the set to be verified and the disturbance model while preserving hard constraints. A double-integrator study illustrates shrinking tube cross-sections in data-rich regions while maintaining invariance. |
| 2026-01-16 | [Quantum-enhanced optimization for patient stratification in clinical trials](http://arxiv.org/abs/2601.11413v1) | Laia Domingo, Christine Johnson | Clinical trials are notorious for their high failure rates and steep costs, leading to wasted time and resources spend, prolonged development timelines, and delayed patient access to new therapies. A key contributor to these failures is biological uncertainty, which complicates trial design and weakens the ability to detect true treatment effects. In particular, inadequate patient stratification often results in covariate imbalances across treatment arms, masking treatment effects and reducing statistical power, even when therapies are effective for specific patient subpopulations. This work presents an optimization-based, quantum-enhanced approach to patient stratification that explicitly minimizes covariate imbalance across numerical and categorical variables, without altering protocol design or trial endpoints. Using real clinical trial data, we demonstrate that hybrid quantum-classical optimization methods achieve high-quality stratification while scaling efficiently to larger cohorts. In our benchmark study, the quantum-enhanced pipeline delivered over a 100x improvement in computational efficiency compared to classical approaches, enabling faster iteration and practical deployment at scale. This report shows how improved stratification can lead to decision-relevant gains, including up to a fivefold increase in statistical significance in treatment effect estimation, reducing treatment-effect dilution and increasing trial sensitivity. Together, these results show that optimization-driven stratification can strengthen clinical trial design, improve confidence in downstream decisions, and reduce the risk of costly late-stage failure. |
| 2026-01-16 | [SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction](http://arxiv.org/abs/2601.11396v1) | Hanlin Wu, Pengfei Lin et al. | As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency. |
| 2026-01-16 | [Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning](http://arxiv.org/abs/2601.11393v1) | Haomiao Tang, Jinpeng Wang et al. | Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions. |
| 2026-01-16 | [Cutting Corners on Uncertainty: Zonotope Abstractions for Stream-based Runtime Monitoring](http://arxiv.org/abs/2601.11358v1) | Bernd Finkbeiner, Martin Fr√§nzle et al. | Stream-based monitoring assesses the health of safety-critical systems by transforming input streams of sensor measurements into output streams that determine a verdict. These inputs are often treated as accurate representations of the physical state, although real sensors introduce calibration and measurement errors. Such errors propagate through the monitor's computations and can distort the final verdict. Affine arithmetic with symbolic slack variables can track these errors precisely, but independent measurement noise introduces a fresh slack variable upon each measurement event, causing the monitor's state representation to grow without bound over time. Therefore, any bounded-memory monitoring algorithm must unify slack variables at runtime in a way that generates a sound approximation.   This paper introduces zonotopes as an abstract domain for online monitoring of RLola specifications. We demonstrate that zonotopes precisely capture the affine state of the monitor and that their over-approximation produces a sound bounded-memory monitor. We present a comparison of different zonotope over-approximation strategies in the context of runtime monitoring, evaluating their performance and false-positive rates. |
| 2026-01-16 | [How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting](http://arxiv.org/abs/2601.11344v1) | Parker Seegmiller, Joseph Gatto et al. | Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows. |
| 2026-01-16 | [Hadronic tau decays at higher orders in QCD](http://arxiv.org/abs/2601.11277v1) | Gauhar Abbas, Vartika Singh | We investigate higher-order perturbative corrections to hadronic $œÑ$ decays by applying nonlinear sequence-transformation techniques to the QCD correction $Œ¥^{(0)}$. In particular, we employ the Shanks transformation and several of its generalisations constructed through Wynn's $\varepsilon$-algorithm, which are known to accelerate the convergence of slowly convergent or divergent series. These methods are used to extract higher-order information from the fixed-order perturbative expansion of $Œ¥^{(0)}$. Within this framework, we estimate the perturbative coefficients $c_{5,1}$-$c_{12,1}$. In particular, we obtain $c_{5,1}=294^{+41}_{-21}$, $c_{6,1}=3415^{+450}_{-467}$, and $c_{7,1}=2.23^{+0.75}_{-0.49}\times 10^4$, where the quoted uncertainties reflect the spread among the different sequence transformations employed. Our analysis demonstrates that Shanks-type sequence transformations based on Wynn's $\varepsilon$-algorithm provide an efficient and systematic tool for probing higher-order perturbative effects in hadronic $œÑ$ decays in the absence of explicit multi-loop calculations. |
| 2026-01-16 | [Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production](http://arxiv.org/abs/2601.11196v1) | Luisa Carpinelli, Filippo Natoli et al. | Artificial intelligence (AI) has moved to the center of policy, market, and academic debates, but its macroeconomic footprint is still only partly understood. This paper provides an overview on how the current AI wave is captured in US national accounts, combining a simple macro-accounting framework with a stylized description of the AI production process. We highlight the crucial role played by data centers, which constitute the backbone of the AI ecosystem and have attracted formidable investment in 2025, as they are indispensable for meeting the rapidly increasing worldwide demand for AI services. We document that the boom in IT and AI-related capital expenditure in the first three quarters of the year has given an outsized boost to aggregate demand, while its contribution to GDP growth is smaller once the high import content of AI hardware is netted out. Furthermore, simple calculations suggest that, at current utilization rates and pricing, the production of services originating in new AI data centers could contribute to GDP over the turn of the next quarters on a scale comparable to that of investment spending to date. Short reinvestment cycles and uncertainty about future AI demand, while not currently acting as a macroeconomic drag, can nevertheless fuel macroeconomic risks over the medium term. |
| 2026-01-16 | [Performance Analysis of Cell-Free Massive MIMO under Imperfect LoS Phase Tracking](http://arxiv.org/abs/2601.11179v1) | Noor Ul Ain, Lorenzo Miretti et al. | We study the impact of imperfect line-of-sight (LoS) phase tracking on the performance of cell-free massive MIMO networks. Unlike prior works that assume perfectly known or completely unknown phases, we consider a realistic regime where LoS phases are estimated with residual uncertainty due to hardware impairments, mobility, and synchronization errors. To this end, we propose a Rician fading model where LoS components are rotated by imperfect phase estimates and attenuated by a deterministic phase-error penalty factor. We derive a linear MMSE channel estimator that captures statistical phase errors and unifies prior results, reducing to the Bayesian MMSE estimator with perfect phase knowledge and to a zero-mean model in the absence of phase knowledge. To address the non-Gaussian setting, we introduce a virtual uplink model that preserves second-order statistics of channel estimation, enabling the derivation of tractable centralized and distributed MMSE beamformers. To ensure fair assessment of the network performance, we apply these beamformers to the true uplink model and compute the spectral efficiency bounds available in the literature. Numerical results show that our framework bridges idealized assumptions and practical tracking limitations, providing rigorous performance benchmarks and design insights for 6G cell-free networks. |
| 2026-01-15 | [Euclid preparation. 3D reconstruction of the cosmic web with simulated Euclid Deep spectroscopic samples](http://arxiv.org/abs/2601.10709v1) | Euclid Collaboration, K. Kraljic et al. | The ongoing Euclid mission aims to measure spectroscopic redshifts for approximately two million galaxies using the H $Œ±$ line emission detected in near-infrared slitless spectroscopic data from the Euclid Deep Fields (EDFs). These measurements will reach a flux limit of $5\times 10^{-17}\,{\rm erg}\,{\rm cm}^{-2}\,{\rm s}^{-1}$ in the redshift range $0.4<z<1.8$, opening the door to numerous investigations involving galaxy evolution, extending well beyond the mission's core objectives. The achieved H $Œ±$ luminosity depth will lead to a sufficiently high sampling, enabling the reconstruction of the large-scale galaxy environment. We assess the quality of the reconstruction of the galaxy cosmic web environment with the expected spectroscopic dataset in EDFs. The analysis is carried out on the Flagship and GAEA galaxy mock catalogues. The quality of the reconstruction is first evaluated using geometrical and topological statistics measured on the cosmic web, namely the length of filaments, the area of walls, the volume of voids, and its connectivity and multiplicity. We then quantify how accurately gradients in galaxy properties with distance from filaments can be recovered. As expected, the small-scale redshift-space distortions, have a strong impact on filament lengths and connectivity, but can be mitigated by compressing galaxy groups before skeleton extraction. The cosmic web reconstruction is biased when relying solely on H $Œ±$ emitters. This limitation can be mitigated by applying stellar mass weighting during the reconstruction. However, this approach introduces non-trivial biases that need to be accounted for when comparing to theoretical predictions. Redshift uncertainties pose the greatest challenge in recovering the expected dependence of galaxy properties, though the well-established stellar mass transverse gradients towards filaments can still be observed. |
| 2026-01-15 | [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](http://arxiv.org/abs/2601.10690v1) | Andrew F. Ilersich, Kevin Course et al. | Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches. |
| 2026-01-15 | [High-fidelity stellar extinction with Gaia and APOGEE -- I. The method and a new extinction curve](http://arxiv.org/abs/2601.10595v1) | Jie Yu, Luca Casagrande et al. | The scarcity of high-fidelity extinction measurements remains a bottleneck in deriving accurate stellar properties from Gaia parallaxes. In this work, we aim to derive precision extinction estimates for APOGEE DR19 stars, establishing a new benchmark for Galactic stellar population studies. We first determine reddening by comparing observed colorsr, etrieved from photometric surveys or standardized synthetic magnitudes from Gaia BP/RP spectra, to intrinsic colors predicted via an XGBoost model. The model is trained on minimally reddened stars to infer intrinsic colors and their associated uncertainties, using APOGEE stellar parameters (Teff, logg, [Fe/H], and [alpha/Fe]). The derived reddening values are then converted into extinctions using an anchor ratio of A_BP / A_RP = 1.694 +/- 0.004, derived from red-clump-like stars. Here, we provide extinction measurements in 39 filters across 10 photometric systems and introduce a new empirical extinction curve optimized for broadband passbands. Our extinction estimates (Av) outperform existing results (Bayestar19, StarHorse, SEDEX), achieving a typical precision of 0.03 mag in Av. Notably, we identify systematic deviations of up to 30% between monochromatic and passband-integrated extinction ratios at wavelengths greater than 700 nm. This result highlights the necessity of adopting passband-specific coefficients when correcting extinction to derive stellar parameters. As the foundation for a forthcoming series of papers, these benchmark measurements will be used to (1) revise asteroseismic scaling relations, (2) calibrate differential reddening in open clusters, and (3) reconcile heterogeneous dust maps into a unified, all-sky extinction scheme. |
| 2026-01-15 | [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](http://arxiv.org/abs/2601.10591v1) | Arundeep Chinta, Lucas Vinh Tran et al. | Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications. |
| 2026-01-15 | [From aggressive to conservative early stopping in Bayesian group sequential designs](http://arxiv.org/abs/2601.10590v1) | Zhangyi He, Feng Yu et al. | Group sequential designs (GSDs) are widely used in confirmatory trials to allow interim monitoring while preserving control of the type I error rate. In the frequentist framework, O'Brien-Fleming-type stopping boundaries dominate practice because they impose highly conservative early stopping while allowing more liberal decisions as information accumulates. Bayesian GSDs, in contrast, are most often implemented using fixed posterior probability thresholds applied uniformly at all analyses. While such designs can be calibrated to control the overall type I error rate, they do not penalise early analyses and can therefore lead to substantially more aggressive early stopping. Such behaviour can risk premature conclusions and inflation of treatment effect estimates, raising concerns for confirmatory trials. We introduce two practically implementable refinements that restore conservative early stopping in Bayesian GSDs. The first introduces a two-phase structure for posterior probability thresholds, applying more stringent criteria in the early phase of the trial and relaxing them later to preserve power. The second replaces posterior probability monitoring at interim looks with predictive probability criteria, which naturally account for uncertainty in future data and therefore suppress premature stopping. Both strategies require only one additional tuning parameter and can be efficiently calibrated. In the HYPRESS setting, both approaches achieve higher power than the conventional Bayesian design while producing alpha-spending profiles closely aligned with O'Brien-Fleming-type behaviour at early looks. These refinements provide a principled and tractable way to align Bayesian GSDs with accepted frequentist practice and regulatory expectations, supporting their robust application in confirmatory trials. |
| 2026-01-15 | [Combinatorial Optimization Augmented Machine Learning](http://arxiv.org/abs/2601.10583v1) | Maximilian Schiffer, Heiko Hoppe et al. | Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning. |
| 2026-01-15 | [Electro-optic frequency comb Doppler thermometry](http://arxiv.org/abs/2601.10575v1) | Sean M. Bresler, Erin M. Adkins et al. | We demonstrate a Doppler thermometer based on direct optical frequency comb spectroscopy of an $^{85}$Rb vapor with a chirped electro-optic frequency comb (EOFC). The direct EOFC Doppler thermometer is accurate to within its approximately 1 K statistical uncertainty. We experimentally compare direct EOFC spectroscopy with conventional Doppler spectroscopy using a single-frequency, step-scanned laser probe. Our results show that direct EOFC spectroscopy mitigates transit-induced optical pumping distortion of the atomic lineshape, which is the dominant systematic temperature shift in alkali atom Doppler thermometry. Optical Bloch equation simulations of conventional and direct EOFC Doppler spectroscopy confirm that EOFC spectroscopy can use higher optical power to reduce statistical noise without optical pumping distortion. Our results indicate that EOFC Doppler thermometry is a promising approach to realizing a primary thermometer with size and measurement rate sufficient for applications including pharmaceutical manufacturing and nuclear waste monitoring. |
| 2026-01-15 | [CROCS: A Two-Stage Clustering Framework for Behaviour-Centric Consumer Segmentation with Smart Meter Data](http://arxiv.org/abs/2601.10494v1) | Luke W. Yerbury, Ricardo J. G. B. Campello et al. | With grid operators confronting rising uncertainty from renewable integration and a broader push toward electrification, Demand-Side Management (DSM) -- particularly Demand Response (DR) -- has attracted significant attention as a cost-effective mechanism for balancing modern electricity systems. Unprecedented volumes of consumption data from a continuing global deployment of smart meters enable consumer segmentation based on real usage behaviours, promising to inform the design of more effective DSM and DR programs. However, existing clustering-based segmentation methods insufficiently reflect the behavioural diversity of consumers, often relying on rigid temporal alignment, and faltering in the presence of anomalies, missing data, or large-scale deployments.   To address these challenges, we propose a novel two-stage clustering framework -- Clustered Representations Optimising Consumer Segmentation (CROCS). In the first stage, each consumer's daily load profiles are clustered independently to form a Representative Load Set (RLS), providing a compact summary of their typical diurnal consumption behaviours. In the second stage, consumers are clustered using the Weighted Sum of Minimum Distances (WSMD), a novel set-to-set measure that compares RLSs by accounting for both the prevalence and similarity of those behaviours. Finally, community detection on the WSMD-induced graph reveals higher-order prototypes that embody the shared diurnal behaviours defining consumer groups, enhancing the interpretability of the resulting clusters.   Extensive experiments on both synthetic and real Australian smart meter datasets demonstrate that CROCS captures intra-consumer variability, uncovers both synchronous and asynchronous behavioural similarities, and remains robust to anomalies and missing data, while scaling efficiently through natural parallelisation. These results... |
| 2026-01-15 | [Job Anxiety in Post-Secondary Computer Science Students Caused by Artificial Intelligence](http://arxiv.org/abs/2601.10468v1) | Daniyaal Farooqi, Gavin Pu et al. | The emerging widespread usage of AI has led to industry adoption to improve efficiency and increase earnings. However, a major consequence of this is AI displacing employees from their jobs, leading to feelings of job insecurity and uncertainty. This is especially true for computer science students preparing to enter the workforce. To investigate this, we performed semi-structured interviews with (n = 25) students across computer science undergraduate and graduate programs at the University of Toronto to determine the extent of job replacement anxiety. Through thematic analysis, it was determined that computer science students indeed face stress and anxiety from AI displacement of jobs, leading to different strategies of managing pressure. Subfields such as software engineering and web development are strongly believed to be vulnerable to displacement, while specialized subfields like quantum computing and AI research are deemed more secure. Many students feel compelled to upskill by using more AI technologies, taking AI courses, and specializing in AI through graduate school. Some students also reskill by pursuing other fields of study seen as less vulnerable to AI displacement. Finally, international students experience additional job replacement anxiety because of pressure to secure permanent residence. Implications of these findings include feelings of low security in computer science careers, oversaturation of computer science students pursuing AI, and potential dissuasion of future university students from pursuing computer science. |
| 2026-01-15 | [ASTRA: A Python Package for Cross-Instrument Stellar and Telluric Template Construction](http://arxiv.org/abs/2601.10439v1) | Andr√© M. Silva, J. P. Faria et al. | ASTRA is a Python package that provides a modular, instrument-independent interface for working with high-resolution stellar spectra. Designed to support data from multiple spectrographs, including ESPRESSO (Pepe et al., 2021), HARPS (Mayor et al., 2003; Pepe et al., 2002), MAROON-X (Seifahrt et al., 2022), and CARMENES (Quirrenbach et al., 2014). ASTRA offers a unified abstraction over their data formats, enabling consistent access to fluxes, wavelengths, uncertainties, and metadata across instruments. Furthermore, it applies the necessary wavelength and flux calibrations that are needed, as described by the official pipelines of each instrument. |
| 2026-01-14 | [Parallaxes, Proper Motions, and Near-Infrared Photometry for 173 L and T Dwarfs From The US Naval Observatory Infrared Astrometry Program](http://arxiv.org/abs/2601.09671v1) | Frederick J. Vrba, Adam C. Schneider et al. | We present near-infrared parallax and proper motion astrometry for 74 L-dwarfs and 99 T-dwarfs, as single objects or in binary systems, obtained with the ASTROCAM astrometric imager on the USNO, Flagstaff Station 1.55-m telescope over two observing periods. For all 173 objects the median number of observational epochs was 62 with a median time frame of 5.25 years, resulting in median uncertainties of $œÉ$($œÄ_{abs}$) = 1.51 mas, $œÉ$($Œº_{abs}$) = 1.02 mas yr$^{-1}$, and $œÉ$($V_{\rm tan}$) = 1.01 km s$^{-1}$. Our observations provide the first parallax/proper motion results for 16 objects and the highest precision parallaxes/proper motions for an additional 116 objects. A serendipitous overlap of 40 objects with Gaia DR3 astrometry allows direct comparison and confirmation of our results, along with an investigation on the effects of resolved binarity on astrometric results. We also provide a uniform set of $J$-, $H$-, $K_{S}$-band photometry in the UKIRT/MKO system, most of it being from new observations. We use these results to examine objects included in this study of special-interest populations, consisting of binaries, wide companions, young objects, subdwarfs, and brown dwarf spectral standards. |
| 2026-01-14 | [TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion](http://arxiv.org/abs/2601.09633v1) | Sahil Mishra, Srinitish Srinivasan et al. | Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric "is-a" relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies. |
| 2026-01-14 | [Confronting eikonal and post-Kerr methods with numerical evolution of scalar field perturbations in spacetimes beyond Kerr](http://arxiv.org/abs/2601.09607v1) | Ciro De Simone, Sebastian H. V√∂lkel et al. | The accurate computation of quasinormal modes from rotating black holes beyond general relativity is crucial for testing fundamental physics with gravitational waves. In this study, we assess the accuracy of the eikonal and post-Kerr approximations in predicting the quasinormal mode spectrum of a scalar field on a deformed Kerr spacetime. To obtain benchmark results and to analyze the ringdown dynamics from generic perturbations, we further employ a 2+1-dimensional numerical time-evolution framework. This approach enables a systematic quantification of theoretical uncertainties across multiple angular harmonics, a broad range of spin parameters, and progressively stronger deviations from the Kerr geometry. We then confront these modeling errors with simple projections of statistical uncertainties in quasinormal mode frequencies as a function of the signal-to-noise ratio, thereby exploring the domain of validity of approximate methods for prospective high-precision black-hole spectroscopy. We also report that near-horizon deformations can affect prograde and retrograde modes differently and provide a geometrical explanation. |
| 2026-01-14 | [High-Resolution Spectroscopy of $^{173}$Yb$^{+}$ Ions](http://arxiv.org/abs/2601.09585v1) | J. Jiang, A. V. Viatkina et al. | Compared to other stable isotopes of $\rm{Yb}^+$, $^{173}\rm{Yb}^+$ has a richer hyperfine structure, which leads to more favorable clock transitions, spectroscopic techniques for probing new physics, and more sophisticated quantum computing architectures. However, to date, its electronic spectrum remains poorly characterized. Here, we report on efficient laser cooling, state preparation, and detection of a single trapped $^{173}\rm{Yb}^+$ ion. The previously unobserved $^2\!S_{1/2} \rightarrow {}^2\!D_{3/2}$ electric quadrupole transition at 436 nm is coherently excited, and the isotope shift between $^{171}\rm{Yb}^+$ and $^{173}\rm{Yb}^+$ on this transition is determined with an uncertainty of 1.4 Hz. Using microwave spectroscopy, we resolve the hyperfine structure (HFS) of the ${}^2\!D_{3/2}$ state with a relative uncertainty below $10^{-8}$. From the HFS measurement data, we infer for ${}^{173}$Yb a nuclear magnetic octupole moment $Œ©= -0.062(8)\,({\rm b} \times Œº_N)$ with uncertainty reduced by more than 2 orders of magnitude compared to previous studies. The data also allow us to determine hyperfine anomalies for the ${}^2\!S_{1/2}$ and ${}^2\!D_{3/2}$ states. |
| 2026-01-14 | [Is it possible to determine unambiguously the Berry phase solely from quantum oscillations?](http://arxiv.org/abs/2601.09560v1) | Bogdan M. Fominykh, Valentin Yu. Irkhin et al. | The Berry phase, a fundamental geometric phase in quantum systems, has become a crucial tool for probing the topological properties of materials. Quantum oscillations, such as Shubnikov-de Haas (SdH) oscillations, are widely used to extract this phase, but its unambiguous determination remains challenging. This work highlights the inherent ambiguities in interpreting the oscillation phase solely from SdH data, primarily due to the influence of the spin factor $R_S$, which depends on the Land√© $g$-factor and effective mass. While the Lifshitz-Kosevich (LK) theory provides a framework for analyzing oscillations, the unknown g-factor introduces significant uncertainty. For instance, a zero oscillation phase could arise either from a nontrivial Berry phase or a negative $R_S$. We demonstrate that neglecting $R_S$ in modern studies, especially for topological materials with strong spin-orbit coupling, can lead to doubtful conclusions. Through theoretical analysis and numerical examples, we show how the interplay between the Berry phase and Zeeman effect complicates phase determination. Additionally, we also discuss another underappreciated mechanism - the magnetic field dependence of the Fermi level. Our discussion underscores the need for complementary experimental techniques to resolve these ambiguities and calls for further research to refine the interpretation of quantum oscillations in topological systems. |
| 2026-01-14 | [Residual Power Flow for Neural Solvers](http://arxiv.org/abs/2601.09533v1) | Jochen Stiasny, Jochen Cremer | The energy transition challenges operational tasks based on simulations and optimisation. These computations need to be fast and flexible as the grid is ever-expanding, and renewables' uncertainty requires a flexible operational environment. Learned approximations, proxies or surrogates -- we refer to them as Neural Solvers -- excel in terms of evaluation speed, but are inflexible with respect to adjusting to changing tasks. Hence, neural solvers are usually applicable to highly specific tasks, which limits their usefulness in practice; a widely reusable, foundational neural solver is required. Therefore, this work proposes the Residual Power Flow (RPF) formulation. RPF formulates residual functions based on Kirchhoff's laws to quantify the infeasibility of an operating condition. The minimisation of the residuals determines the voltage solution; an additional slack variable is needed to achieve AC-feasibility. RPF forms a natural, foundational subtask of tasks subject to power flow constraints. We propose to learn RPF with neural solvers to exploit their speed. Furthermore, RPF improves learning performance compared to common power flow formulations. To solve operational tasks, we integrate the neural solver in a Predict-then-Optimise (PO) approach to combine speed and flexibility. The case study investigates the IEEE 9-bus system and three tasks (AC Optimal Power Flow (OPF), power-flow and quasi-steady state power flow) solved by PO. The results demonstrate the accuracy and flexibility of learning with RPF. |
| 2026-01-14 | [Probing the two-quasiparticle $K^œÄ=8^+$ isomeric structure and enhanced stability in the proton drip-line nuclei](http://arxiv.org/abs/2601.09526v1) | Zhen-Zhen Zhang, Hua-Lei Wang et al. | Stimulated by recent experimental discoveries [{Phys. Lett. B \textbf{847}, 138310 (2023)} and {Phys. Rev. Lett. \textbf{132}, 072502 (2024)}], two-quasiparticle $K^œÄ=8^+$ isomeric structure (related to the neutron $h_{9/2}$ and $f_{7/2}$ orbitals) in $^{160}_{76}$Os$_{84}$ that lies at the two-proton drip line has been studied by means of the configuration-constrained potential-energy-surface calculations. Calculated results indicate that, for such an isomer, the excitation energy can be well reproduced and its oblate shape can be enhanced by the polarization effects of the two high-$K$ orbits. Comparing with experimental data, two sets of the widely used Woods-Saxon parameters, especially, the spin-orbit coupling one, are evaluated and argued. It is found that, considering the uncertainty of the spin-orbit coupling strength, the energy crossing or inversion of the $h_{9/2}$ and $f_{7/2}$ neutrons can occur, which may lead to three kinds of different evolution-trends of two-quasiparticle excitation energies with the changing quadrupole deformation $Œ≤_2$. With decreasing spin-orbit coupling interaction, the structure of the $K^œÄ=8^+$ isomeric state will evolute from $ŒΩh_{9/2}f_{7/2}$ ($ŒΩ9/2^-[505] \otimes 7/2^-[503]$) to the mixing of $ŒΩh_{9/2}f_{7/2}$ and $ŒΩh_{9/2}^2$ ($ŒΩ9/2^-[505] \otimes 7/2^-[514]$) to $ŒΩh_{9/2}^2$, indicating that its structural probes is still of interest and an arbitrary assignment may be risky. The related theoretical calculations and experimental evidences e.g., the transition properties, are desirable. In addition, similar to that in superheavy nuclei, it is suggested that the stability inversion between high-$K$ isomeric states and ground states might occur in this proton drip-line mass region, e.g., in the hitherto unknown nucleus $^{162}_{78}$Pt$_{84}$. |
| 2026-01-14 | [Class Adaptive Conformal Training](http://arxiv.org/abs/2601.09522v1) | Badr-Eddine Marani, Julio Silva-Rodriguez et al. | Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees. |
| 2026-01-14 | [Demographics of Close-In TESS Exoplanets Orbiting FGK Main-sequence Stars](http://arxiv.org/abs/2601.09492v1) | Kaiming Cui, David J. Armstrong et al. | Understanding the demographics of close-in planets is crucial for insights into exoplanet formation and evolution. We present a detailed analysis of occurrence rates for close-in (0.5-16 day) planets with radii between 2 and 20$\,R_{\oplus}$ around FGK main-sequence stars. Our study uses a comprehensive sample from four years of TESS Science Processing Operations Center full-frame image data cross-matched with Gaia, analysed through our rigorous detection, vetting, and validation pipeline. Using high-confidence planet candidates, we apply a hierarchical Bayesian model to determine occurrence rates in the two-dimensional orbital period-radius plane. Our results are presented using 10-by-10 bins across the period-radius parameter space, offering unprecedented resolution and statistical precision. We find an overall occurrence rate of $9.4^{+0.7}_{-0.6}\%$. When using identical binning, our occurrence rate posteriors distributions align with Kepler's but have a magnitude smaller uncertainties on average. For hot Jupiters, we estimate the overall occurrence rate of $0.39^{+0.03}_{-0.02}\%$. This value is consistent with the previous Kepler FGK-type result within $1œÉ$. We find an overall occurrence rate of Neptunian desert planets of $0.08\pm0.01\%$, to our knowledge the first such determination. Additionally, in a volume-limited Gaia subsample within 100 pc in the same parameter region, we measure an overall planet occurrence rate of $15.4^{+1.6}_{-1.5}\%$ and a hot Jupiter occurrence rate of $0.42^{+0.16}_{-0.12}\%$. Our results establishes an improved foundation for constraining theoretical models of exoplanet populations. |
| 2026-01-14 | [Inclusive and exclusive semileptonic decays of heavy mesons on the lattice](http://arxiv.org/abs/2601.09480v1) | Zhi Hu, Alessandro Barone et al. | We report the recent progress from our group in extracting observables of both inclusive and exclusive semileptonic heavy-meson decays directly from lattice QCD four-point correlators. On the inclusive side, we illustrate how to estimate the systematic uncertainties from omitted higher-order terms and non-zero smearing of the kernel approximation, building on two important features of the Chebyshev expansion. On the exclusive side, we perform BCL parameterizations of the pseudoscalar to pseudoscalar form factors and compare the fitted coefficients with those from earlier results by HPQCD. We also perform a HQET-based parameterization of the P-wave form factors to shed new light on the 1/2-vs-3/2 puzzle. This work constitutes a step toward a unified lattice treatment of inclusive and exclusive semileptonic decays, relevant for the Vcb puzzle. In this study, we use lattice ensembles from the RBC/UKQCD collaboration for numerical investigations. Future developments from our group will focus on the control of other systematic effects for inclusive decays and investigations of other techniques with reduced statistical errors to extract exclusive contributions from lattice four-point correlators. |
| 2026-01-13 | [An Optimal Observable Machine for reinterpretable measurements in high-energy physics](http://arxiv.org/abs/2601.08813v1) | Torben Mohr, Alejandro Quiroga Trivi√±o et al. | A machine-learning-based framework for constructing generator-level observables optimized for parameter extraction in particle physics analyses is introduced, referred to as the Optimal Observable Machine (OOM). Unfoldable differential distributions are learned that maximize sensitivity to a parameter of interest while remaining robust against detector effects, systematic uncertainties, and biases introduced by the unfolding procedure. Detector response and systematic uncertainties are explicitly incorporated into the training through a likelihood-based loss function, enabling a direct optimization of the expected measurement precision while minimizing the bias from any assumption on the parameter of interest itself. The approach is demonstrated in an application to top quark physics, focusing on the measurement of a recently observed pseudoscalar excess at the top quark pair production threshold in dilepton final states. It is shown that a generator-level observable with enhanced sensitivity and long-term reinterpretability can be constructed using this method. |
| 2026-01-13 | [APEX-SWE](http://arxiv.org/abs/2601.08806v1) | Abhi Kottamasu, Akul Datta et al. | We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50). |
| 2026-01-13 | [Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation](http://arxiv.org/abs/2601.08728v1) | Runfeng Qu, Ole Hall et al. | Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision |
| 2026-01-13 | [Stellar masses of optically dark galaxies: uncertainty introduced by the attenuation law and star-formation histories](http://arxiv.org/abs/2601.08693v1) | Yash Lapasia, Sandro Tacchella et al. | JWST observations have suggested that some high-redshift galaxies may be ultra-massive, thereby challenging standard models of early galaxy formation and cosmology. We analyse the stellar masses using different modelling assumptions and with new data of three galaxies (S1, S2 and S3), whose NIRCam/grism redshifts were consistent with $z>5$. These three optically dark galaxies have previously been reported to host exceptionally high stellar masses and star-formation rates, implying extremely high star-formation efficiencies. Recent NIRSpec/IFU observations for S1 indicate a spectroscopic redshift of $z_{\rm spec}=3.2461^{+0.0001}_{-0.0002}$, which is lower than previously reported. Using the Bayesian spectral energy distribution (SED) modelling tool \texttt{Prospector}, we investigate the impact of key model assumptions on stellar mass estimates, such as the choice of star-formation history (SFH) priors (constant versus rising SFH base for the non-parametric prior), the dust attenuation law, and the treatment of emission line fluxes. Our analysis yields revised stellar masses of $\log(M_{\star}/M_{\odot}) \approx 10.36^{+0.47}_{-0.32}, 10.95^{+0.11}_{-0.10}$ and $10.31^{+0.24}_{-0.19}$ for S1, S2, and S3, respectively. We find that adopting a rising SFH base prior results in lower inferred stellar masses compared to a constant SFH base prior. We identify a significant degeneracy between the dust attenuation curve slope, the amount of dust attenuation, and stellar mass. Our results highlight various systematics in SED modelling due to SFH priors and dust attenuation that can influence stellar mass estimates of heavily dust obscured sources. Nevertheless, even with these revised stellar mass estimates, two of the three galaxies remain among the most massive and actively star-forming systems at their respective redshifts, implying high star-formation efficiencies. |
| 2026-01-13 | [Ultrafast quantum optics with attosecond control](http://arxiv.org/abs/2601.08671v1) | Mohamed Sennary, Javier Rivera-Dean et al. | Modern Quantum optics largely remains quasi-stationary, far from intrinsic optical field timescales. Ultrafast quantum optics seeks to generate, shape, and measure quantum states of light on femtosecond and attosecond timescales. Here we introduce a quantum light field squeezer (QLFS) that enables the generation and attosecond control of ultrafast broadband squeezed light. Using degenerate four-wave mixing in a quasi-collinear focusing geometry, our approach overcomes conventional broadband phase-matching limits, producing intensity- and phase-squeezed states directly from few-cycle laser pulses. Our ultrafast quantum optical metrology reveals a time-dependent squeezing distribution across individual half-cycles of the electric field. Incorporating this time-dependent squeezing into strong-field simulations shows that the temporal redistribution of quantum uncertainty reshapes the high-harmonic emission. Moreover, by tuning the relative pulse delay and phase-matching angle, we achieve attosecond precision in controlling the squeezing characteristics by visualizing inferred effective Wigner representations of the quantum light field. Beyond characterization, we demonstrate that the quantum light-induced tunneling-current noise is sensitive to the nonclassical intensity-noise statistics of the driving squeezed light, with sub-femtosecond control. Together, these results extend the generation, control, and effective phase-space representation of squeezed light into the ultrafast and attosecond regime, opening new avenues for quantum optics in strong-field and solid-state systems. |
| 2026-01-13 | [Provably Safe Reinforcement Learning using Entropy Regularizer](http://arxiv.org/abs/2601.08646v1) | Abhijit Mazumdar, Rafal Wisniewski et al. | We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms. |
| 2026-01-13 | [Percentile-based probabilistic optimization for systematic and random uncertainties in radiation therapy](http://arxiv.org/abs/2601.08644v1) | Albin Fredriksson, Erik Engwall et al. | Geometric uncertainty can degrade treatment quality in radiation therapy. While margins and robust optimization mitigate these effects, they provide only implicit control over clinical goal fulfillment probability. We therefore develop a probabilistic planning framework using a percentile-based optimization function that targets a specified probability of clinical goal fulfillment.   Systematic and random uncertainties were explicitly modeled over full treatment courses. A scenario dose approximation method based on interpolation between a fixed set of doses was used, enabling efficient simulation of treatment courses during optimization. The framework was evaluated on a prostate case treated with volumetric-modulated arc therapy (VMAT) and a brain case treated with pencil beam scanning (PBS) proton therapy. Plans were compared to conventional margin-based and worst-case robust optimization using probabilistic evaluation.   For the prostate case, probabilistic optimization improved organ at risk (OAR) sparing while maintaining target coverage compared to margin-based planning, increasing average OAR goal fulfillment probability by 13.3 percentage points and reducing 90th percentile OAR doses by an average of 3.5~Gy. For the brain case, probabilistic optimization improved target minimum dose passing probabilities (e.g., 88\% vs.~22\% for $D_{95}$) and brainstem maximum dose passing probability (70\% vs.~30\%), while maintaining comparable or improved OAR sparing compared to worst-case optimization.   Probabilistic optimization enables explicit and interpretable control over goal fulfillment probabilities. Combining full treatment course modeling with efficient approximate dose calculation, the proposed framework improved the trade-off between target coverage and OAR sparing compared to conventional planning approaches in both photon and proton therapy. |
| 2026-01-13 | [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](http://arxiv.org/abs/2601.08641v1) | Yichen Luo, Yebo Feng et al. | The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.   To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects. |
| 2026-01-13 | [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](http://arxiv.org/abs/2601.08617v1) | Leo Fillioux, Omprakash Chakraborty et al. | With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities. |
| 2026-01-13 | [Sparsifying transform priors in Gaussian graphical models](http://arxiv.org/abs/2601.08596v1) | Marcus Gehrmann, H√•kon Tjelmeland | Bayesian methods constitute a popular approach for estimating the conditional independence structure in Gaussian graphical models, since they can quantify the uncertainty through the posterior distribution. Inference in this framework is typically carried out with Markov chain Monte Carlo (MCMC). However, the most widely used choice of prior distribution for the precision matrix, the so called G-Wishart distribution, suffers from an intractable normalizing constant, which gives rise to the problem of double intractability in the updating steps of the MCMC algorithm. In this article, we propose a new class of prior distributions for the precision matrix, termed ST priors, that allow for the construction of MCMC algorithms that do not suffer from double intractability issues. A realization from an ST prior distribution is obtained by applying a sparsifying transform on a matrix from a distribution with support in the set of all positive definite matrices. We carefully present the theory behind the construction of our proposed class of priors and also perform some numerical experiments, where we apply our methods on a human gene expression dataset. The results suggest that our proposed MCMC algorithm is able to converge and achieve acceptable mixing when applied on the real data. |
| 2026-01-12 | [Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests](http://arxiv.org/abs/2601.07820v1) | Manar Ali, Judith Sieker et al. | In human conversation, both interlocutors play an active role in maintaining mutual understanding. When addressees are uncertain about what speakers mean, for example, they can request clarification. It is an open question for language models whether they can assume a similar addressee role, recognizing and expressing their own uncertainty through clarification. We argue that reference games are a good testbed to approach this question as they are controlled, self-contained, and make clarification needs explicit and measurable. To test this, we evaluate three vision-language models comparing a baseline reference resolution task to an experiment where the models are instructed to request clarification when uncertain. The results suggest that even in such simple tasks, models often struggle to recognize internal uncertainty and translate it into adequate clarification behavior. This demonstrates the value of reference games as testbeds for interaction qualities of (vision and) language models. |
| 2026-01-12 | [Learning Through Dialogue: Unpacking the Dynamics of Human-LLM Conversations on Political Issues](http://arxiv.org/abs/2601.07796v1) | Shaz Furniturewala, Gerard Christopher Yeo et al. | Large language models (LLMs) are increasingly used as conversational partners for learning, yet the interactional dynamics supporting users' learning and engagement are understudied. We analyze the linguistic and interactional features from both LLM and participant chats across 397 human-LLM conversations about socio-political issues to identify the mechanisms and conditions under which LLM explanations shape changes in political knowledge and confidence. Mediation analyses reveal that LLM explanatory richness partially supports confidence by fostering users' reflective insight, whereas its effect on knowledge gain operates entirely through users' cognitive engagement. Moderation analyses show that these effects are highly conditional and vary by political efficacy. Confidence gains depend on how high-efficacy users experience and resolve uncertainty. Knowledge gains depend on high-efficacy users' ability to leverage extended interaction, with longer conversations benefiting primarily reflective users. In summary, we find that learning from LLMs is an interactional achievement, not a uniform outcome of better explanations. The findings underscore the importance of aligning LLM explanatory behavior with users' engagement states to support effective learning in designing Human-AI interactive systems. |
| 2026-01-12 | ["TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt](http://arxiv.org/abs/2601.07786v1) | Abdullah Al Mujahid, Mia Mohammad Imran | As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness. |
| 2026-01-12 | [Euclid preparation. Calibrated intrinsic galaxy alignments in the Euclid Flagship simulation](http://arxiv.org/abs/2601.07785v1) | Euclid Collaboration, K. Hoffmann et al. | Intrinsic alignments of galaxies are potentially a major contaminant of cosmological analyses of weak gravitational lensing. We construct a semi-analytic model of galaxy ellipticities and alignments in the \Euclid Flagship simulation to predict this contamination in Euclid's weak lensing observations. Galaxy shapes and orientations are determined by the corresponding properties of the host haloes in the underlying $N$-body simulation, as well as the relative positions of galaxies within their halo. Alignment strengths are moderated via stochastic misalignments, separately for central and satellite galaxies and conditional on the galaxy's redshift, luminosity, and rest-frame colour. The resulting model is calibrated against galaxy ellipticity statistics from the COSMOS Survey, selected alignment measurements based on Sloan Digital Sky Survey samples, and galaxy orientations extracted from the Horizon-AGN hydrodynamic simulation at redshift $z=1$. The best-fit model has a total of 12 alignment parameters and generally reproduces the calibration data sets well within the $1œÉ$ statistical uncertainties of the observations and the \flagship simulation, with notable exceptions for the most luminous sub-samples on small physical scales. The statistical power of the calibration data and the volume of the single \flagship realisation are still too small to provide informative prior ranges for intrinsic alignment amplitudes in relevant galaxy samples. As a first application, we predict that \Euclid end-of-mission tomographic weak gravitational lensing two-point statistics are modified by up to order $10\,\%$ due to intrinsic alignments. |
| 2026-01-12 | [Are LLM Decisions Faithful to Verbal Confidence?](http://arxiv.org/abs/2601.07767v1) | Jiawei Wang, Yanfei Zhou et al. | Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce $\textbf{RiskEval}$: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions. |
| 2026-01-12 | [Single Production of a Vector-Like Top as a Probe of Charged Higgs Bosons at a Muon-Proton Collider](http://arxiv.org/abs/2601.07758v1) | R. Benbrik, M. Berrouj et al. | We investigate the discovery prospects for a vector-like top partner ($T$) within the Type-II Two-Higgs-Doublet Model (2HDM-II) at future high-energy $Œºp$ colliders. The analysis focuses on the charged Higgs decay mode $Œº^+ p \to ŒΩ_Œº\, \bar{b}T \to ŒΩ_Œº\, \bar{b}H^+b$, with the subsequent decay $H^+\to t\bar{b}$ yielding a final state with multiple $b$-jets and a charged lepton. A detailed detector-level simulation is carried out for benchmark configurations with charged Higgs masses around 600-650~GeV and vector-like top masses in the range $m_T \simeq 1.0$-$1.8$~TeV. For an integrated luminosity of 100~fb$^{-1}$, discovery significances above $5œÉ$ are obtained across several benchmark points, remaining robust against systematic uncertainties up to 20\%. At higher luminosities of 234~fb$^{-1}$, the sensitivity exceeds $10œÉ$ for the lightest benchmarks and stays above $5œÉ$ even in the presence of 30\% systematics. |
| 2026-01-12 | [Systematic Biases in Gravitational-Wave Parameter Estimation from Neglecting Orbital Eccentricity in Space-Based Detectors](http://arxiv.org/abs/2601.07739v1) | Jin-Zhao Yang, Jia-Hao Zhong et al. | Accurate modeling of gravitational-wave signals is essential for reliable inference of compact-binary source parameters, particularly for future space-based detectors operating in the milli- and deci-Hertz bands. In this work, we systematically investigate the parameter-estimation biases induced by neglecting orbital eccentricity when analyzing eccentric compact-binary coalescences with quasi-circular waveform templates. Focusing on the deci-Hertz detector B-DECIGO and the milli-Hertz detector LISA, we model eccentric inspiral signals using a frequency-domain waveform that incorporates eccentricity-induced higher harmonics and the time-dependent response of spaceborne detectors. We quantify systematic biases in the chirp mass, symmetric mass ratio, and luminosity distance using both Bayesian inference and the Fisher-Cutler-Vallisneri (FCV) formalism, and assess their significance relative to statistical uncertainties. By constructing mock gravitational-wave catalogs spanning stellar-mass and massive black-hole binaries, we identify critical initial eccentricities at which systematic errors become comparable to statistical errors. We find that for B-DECIGO, even very small eccentricities, $e_0\sim 10^{-4}-10^{-3}$ at 0.1 Hz, can lead to significant biases, whereas for LISA such effects typically arise at larger eccentricities, $e_0\sim 10^{-2}-10^{-1}$ at $10^{-4}$ Hz, due to the smaller number of in-band cycles. Comparisons between FCV predictions and full Bayesian analyses demonstrate good agreement within the regime where waveform mismatches remain small, especially when extrinsic parameters are pre-aligned to minimize mismatches. Our results highlight the necessity of incorporating eccentricity in waveform models for future space-based gravitational-wave observations. |
| 2026-01-12 | [Variational Contrastive Learning for Skeleton-based Action Recognition](http://arxiv.org/abs/2601.07666v1) | Dang Dinh Nguyen, Decky Aspandi Latif et al. | In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods. |
| 2026-01-12 | [Impact of Nuclear Reaction Rates on Calcium Production in Population III Stars: A Global Analysis](http://arxiv.org/abs/2601.07623v1) | Qing Wang, Ertao Li et al. | We investigate the sensitivity of calcium production to nuclear reaction rates of a 40 solar-mass Population III star using 1D multi-zone stellar models. A comprehensive nuclear reaction network was constructed, and all $(p,Œ≥)$ and $(p,Œ±)$ reaction rates were individually varied by a factor of 10 up and down, identifying 13 preliminary key reactions for calcium production. To propagate the reaction rate uncertainties on calcium production, two sets of Monte Carlo simulations were performed for these key reactions: one adopting STARLIB reaction rates and the other incorporating updated rates from recent experimental data and evaluations. Our results show that Monte Carlo simulations using the updated rates show good agreement with the observed calcium abundance of the extremely iron-poor star SMSS J031300.36-670839.3 within the 68% confidence interval predicted by the models. In contrast, the observed calcium abundance lies marginally outside the 68% C.I. when using the STARLIB rates. Spearman rank-order correlation analysis and SHAP values show that the $(p,Œ≥)$ and $(p,Œ±)$ reactions of F18 and F19 exhibit strong coupled effects on calcium production. These reaction-rate uncertainties need to be reduced to constrain the stellar model predictions. Our study provides insights for future nuclear physics experiments aimed at reducing reaction rate uncertainties in the nucleosynthesis of Population III Stars.   Additionally, comparisons between 20 solar-mass and 40 solar-mass Population III stellar models confirm that the latter, with updated reaction rates, is more capable of reproducing the observed Ca abundance and [Ca/Mg] ratio. |
| 2026-01-12 | [Amplitude analysis and branching fraction measurement of $J/œà\to Œõ\barŒ£^0Œ∑+\mathrm{c.c}$](http://arxiv.org/abs/2601.07617v1) | BESIII Collaboration, M. Ablikim et al. | Based on a sample of $(10087\pm44)\times10^{6}$ $J/œà$ events collected with the BESIII detector, a partial-wave analysis of $ J/œà\to Œõ\bar{ Œ£}^0Œ∑+\mathrm{c.c} $ is performed for the first time. The dominant contributions are found to be excited $Œõ$ states with $J^P=1/2^-$ and $J^P=1/2^+$ in the $Œ∑Œõ$ mass spectra. The measured masses and widths are $M=1668.8\pm3.1\pm21.2$ MeV/$c^2$ and $Œì=52.7\pm4.2\pm17.8$ MeV for the $Œõ(1670)$, and $M=1881.5\pm16.5\pm20.3$ MeV/$c^2$ and $Œì=82.4\pm18.2\pm8.9$ MeV for the $Œõ(1810)$, respectively. The branching fraction is determined to be $ \mathcal{B}(J/œà\to Œõ\bar{ Œ£}^0Œ∑+\mathrm{c.c}) $ = $(3.44 \pm 0.11 \pm 0.13) \times 10^{-5}$. The first uncertainties are statistical and the second systematic. |
| 2026-01-09 | [AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs](http://arxiv.org/abs/2601.06022v1) | Chengming Cui, Tianxin Wei et al. | Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse. |
| 2026-01-09 | [Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation](http://arxiv.org/abs/2601.05981v1) | Yinsong Wang, Xinzhe Luo et al. | Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR. |
| 2026-01-09 | [AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty](http://arxiv.org/abs/2601.05978v1) | Dror Jacoby, Yanzhi Li et al. | As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Service (QoS) requirements. This creates a critical challenge: making admission decisions under uncertainty regarding future network capacity. To address this, we develop a proactive slice admission control framework for mmWave x-haul networks subject to rain-induced fluctuations. Our objective is to improve network performance, ensure QoS, and optimize revenue, thereby surpassing the limitations of standard reactive approaches. The proposed framework integrates a deep learning predictor of future network conditions with a proactive Q-learning-based slice admission control mechanism. We validate our solution using real-world data from a mmWave x-haul deployment in a dense urban area, incorporating realistic models of link capacity attenuation and dynamic slice demands. Extensive evaluations demonstrate that our proactive solution achieves 2-3x higher long-term average revenue under dynamic link conditions, providing a scalable and resilient framework for adaptive admission control. |
| 2026-01-09 | [Feasibility of event-by-event primary mass discrimination using radio observables and supervised machine learning](http://arxiv.org/abs/2601.05969v1) | Washington R. de Carvalho, Lech Wiktor Piotrowski | In this work, we investigate the feasibility of event-by-event primary mass discrimination using radio observables only. Although the analysis does not require an explicit reconstruction of the shower maximum ($X_{max}$), the discrimination power still arises from the sensitivity of the radio observables to the longitudinal development of the extensive air shower (EAS). Such radio-based approaches could be particularly relevant for radio-only experiments, such as GRAND. To assess this feasibility, we obtained conservative upper limits for the discrimination accuracy using a supervised machine-learning (ML) algorithm, namely a random forest (RF). The input features used were the peak electric fields and the spectral slopes, which have complementary discrimination power, along with the antenna distances to the shower axis. The RF was trained and tested using large event sets generated by the fast radio emission simulation and simplified detector response implemented in the RDSim framework. We obtained discrimination accuracies between 81\% and 96\% over the studied zenith range, even after normalizing each shower by its own electromagnetic energy. Since the analysis includes deliberately conservative choices, such as a large 10\% uncertainty on the reconstructed EM energy, these quoted values should be interpreted as conservative upper limits suitable for a feasibility assessment. Our results demonstrate that event-by-event primary mass discrimination using radio observables is, in principle, feasible. |
| 2026-01-09 | [The Linear Point Standard Ruler with DESI DR1 and DR2 Data](http://arxiv.org/abs/2601.05967v1) | N. Uberoi, F. Nikakhtar et al. | The linear point, a purely geometric feature in the monopole of the two-point correlation function, has been proposed as an alternative standard ruler. Compared to the peak in the correlation function, it is more robust to late-time nonlinear effects at the percent level. In light of improved simulations and high quality data, we revisit the robustness of the linear point and use it as an alternative to template-based fitting approaches typically used in BAO analyses. We present the linear point measurements on galaxy samples from the first and second data releases (DR1 and DR2) of the DESI survey. We convert the linear point into a dimensionless parameter $Œ±_{iso,LP}$, defined as the ratio of the linear point in the fiducial cosmology and the observed value, analogous to the isotropic BAO scaling parameter $Œ±_{iso}$ used in previous BAO measurements. Using the 2nd generation of AbacusSummit mock catalogs, we find that linear point measurements are more precise when calculated in the post-reconstruction regime with 15-60% smaller uncertainties than those pre-reconstruction. We find a systematic shift in the linear point measurements compared against the isotropic BAO measurements in mocks; we attribute this to the isotropic damping parameter responsible for smearing the linear point in the nonlinear regime. We propose a sample-dependent correction that mitigates the impact of late-time nonlinear effects. While this introduces a cosmology dependence in an otherwise model-independent measurement, this is necessary given the sub-percent precision dictated by current cosmological surveys. Comparing $Œ±_{iso,LP}$ with isotropic BAO measurements made on the DESI DR1 and DR2 galaxy samples, we find excellent agreement after applying this correction, particularly post-reconstruction. We discuss future scope regarding cosmological inference with linear point measurements. |
| 2026-01-09 | [A Critical Examination of Active Learning Workflows in Materials Science](http://arxiv.org/abs/2601.05946v1) | Akhil S. Nair, Lucas Foppa | Active learning (AL) plays a critical role in materials science, enabling applications such as the construction of machine-learning interatomic potentials for atomistic simulations and the operation of self-driving laboratories. Despite its widespread use, the reliability and effectiveness of AL workflows depend on implicit design assumptions that are rarely examined systematically. Here, we critically assess AL workflows deployed in materials science and investigate how key design choices, such as surrogate models, sampling strategies, uncertainty quantification and evaluation metrics, relate to their performance. By identifying common pitfalls and discussing practical mitigation strategies, we provide guidance to practitioners for the efficient design, assessment, and interpretation of AL workflows in materials science. |
| 2026-01-09 | [Evaluating star formation rates at z = 5](http://arxiv.org/abs/2601.05916v1) | D. Ismail, K. Kraljic et al. | Inferring the star formation rates (SFR) in high redshift galaxies remains challenging, owing to observational limitations or uncertainties in calibration methods that link luminosities to SFRs. We utilize two state-of-the-art hydrodynamical simulations NewHorizon and NewCluster, post-processed with the radiative transfer code Skirt, to investigate the systematic uncertainties and biases in the inferred SFRs for z=5 galaxies; an epoch where galaxies build-up their stellar mass. We create synthetic observables for widely-used tracers: Halpha nebular line, [CII] 158 micron fine-structure line, total infrared (IR) continuum luminosity, and hybrid (IR + UV). We find that Halpha-inferred SFRs, time-averaged over 10 Myr, are sensitive to the choice of calibration and exhibit substantial scatter driven by dust attenuation, viewing angle, and dust-to-metal ratio. Adopting a steeper attenuation curve reduces this scatter significantly but does not fully eliminate systematic uncertainties. IR continuum-based SFRs trace intrinsic SFRs time-averaged over 100 Myr timescales when a well-sampled continuum emission between restframe 8 and 1000 micron is available and underestimate them with typical approaches when IR data are limited. Nevertheless, IR SFRs display a considerable scatter, largely due to UV photon leakage and strong variations in the star formation history. When UV data are available, hybrid (IR + UV) SFRs provide a more robust estimate, reducing scatter compared to IR-based SFRs while avoiding explicit attenuation corrections. Finally, we derive a [CII]-SFR relation finding a steeper relation than previous studies, however with significant scatter linked to gas density and metallicity. Overall, IR-, hybrid-, and [CII]-based tracers remain more robust than Halpha against variations in optical depth. |
| 2026-01-09 | [Sub-Planck structure quantification in non-Gaussian probability densities](http://arxiv.org/abs/2601.05898v1) | Darren W. Moore, Vojtƒõch ≈†varc et al. | Sub-Planck structures in non-Gaussian probability densities of phase space variables are pervasive in bosonic quantum systems. They are almost universally present if the bosonic system evolves via nonlinear dynamics or nonlinear measurements. So far, identification and comparison of such structures remains qualitative. Here we provide a universally applicable and experimentally friendly method to identify, quantify and compare sub-Planck structures from directly measurable or estimated probability densities of single phase space variables. We demonstrate the efficacy of this method on experimental high order Fock states of a single-atom mechanical oscillator, showing provably finer sub-Planck structures as the Fock occupation increases despite the accompanying uncertainty increase in the phonon, position, and momentum bases. |
| 2026-01-09 | [Toward Quantitative Electric-Field Measurements of Inception Clouds in Nanosecond Discharges Using E-FISH Assisted by Machine Learning](http://arxiv.org/abs/2601.05872v1) | Mhedine Alicherif, Edwin Sugeng et al. | This study investigates the spatio-temporal evolution of the electric field during the early stages of a nanosecond positive corona discharge in atmospheric-pressure air by combining time-resolved E-FISH measurements, machine-learning-assisted field inversion (based on a recently developed operator-learning model), and iCCD optical emission imaging. The objective is to quantitatively characterize the electric field in the vicinity of the high-voltage electrode during inception and the transition toward streamer formation. By averaging over a large number of discharge events and operating in a regime where the discharge remains statistically axisymmetric, the proposed approach enables reconstruction of the electric-field profiles with nanosecond resolution. The results show a rapid increase of the field during the first nanoseconds, followed by the formation of a shell-like structure exhibiting the highest reduced fields prior to destabilization. The reconstructed reduced electric-field magnitude reaches peak values in the range of approximately 230-270 Td, with an estimated uncertainty of about 20-30% associated with calibration and profile-shape effects. These values correspond to the regime where electron-impact excitation and photoionization processes become highly efficient, consistent with the observed transition from a stable inception cloud to streamer destabilization. After the onset of streamer branching, increasing asymmetry limits the applicability of the inversion, and the reconstructed fields represent averaged contributions rather than the local field at individual streamer heads. The methodology thus identifies the conditions under which quantitative E-field mapping is reliable and establishes a framework for extending electric-field diagnostics to the inception phase of nanosecond atmospheric discharges. |
| 2026-01-09 | [Probing the morphology of the Gum Nebula through pulsar observables and a novel distance estimation method](http://arxiv.org/abs/2601.05791v1) | Ashish Kumar, Avinash A. Deshpande et al. | Various existing models of the Gum Nebula differ significantly in their parameters and suggested origins, which can be independently tested for consistency with data on some key observables of pulsars in the direction of the nebula. Our analysis of such data on the Vela pulsar, assuming a dominant scattering region in its foreground, suggests that the fractional distance of the scatterer is $0.89 \pm 0.01$, and for the given distance of the Vela pulsar, it translates to $254 \pm 16$ pc. Using independent distances of ten pulsars, we suggest a refined description of the Gum Nebula electron density model with its basic morphology similar to that used in the YMW16 model, which now provides better estimates of pulsar distances in these directions. In our new Gum Nebula model, as expected, the Vela pulsar would be behind the frontal edge of the Gum shell, which was intriguingly located in front of the nebula in the YMW16 model. We also present a new technique to better constrain the pulsar distances using their dispersion measure and temporal broadening simultaneously, and find that it is less affected by the uncertainties in the Galactic electron density distribution models. Notably, the new approach shows that the expected temporal broadening as a function of trial distance does not follow a monotonic increasing trend, but instead exhibits oscillations at regions of enhanced electron density. This behaviour is expected, as the method employs the integral form of temporal broadening with the appropriate weighting kernel, leading to more reliable estimates. |
| 2026-01-08 | [Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data](http://arxiv.org/abs/2601.05227v1) | James Rice | I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It√¥ SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.   A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning. |
| 2026-01-08 | [CAOS: Conformal Aggregation of One-Shot Predictors](http://arxiv.org/abs/2601.05219v1) | Maja Waldron | One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage. |
| 2026-01-08 | [Measurement of the Higgs boson total decay width using the H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ decay channel in proton-proton collisions at $\sqrt{s}$ = 13 TeV](http://arxiv.org/abs/2601.05168v1) | CMS Collaboration | The Higgs boson (H) decay width is determined from the ratio of off- and on-shell production of H $\to$ WW $\to$ e$ŒΩŒºŒΩ$ using proton-proton collision data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected at $\sqrt{s}$ = 13 TeV by the CMS experiment at the LHC. The off-shell signal strength is measured as $Œº_\text{off-shell}$ = 1.2$^{+0.8}_{-0.7}$. The Higgs boson total decay width is $Œì_\text{H}$ = 3.9$^{+2.7}_{-2.2}$ MeV, in agreement with the standard model prediction. The uncertainty in this result represents a factor of three improvement over the previous CMS result in this decay channel. |
| 2026-01-08 | [Detection of time delay between UV and X-ray variability in Mrk 1044 using AstroSat observations](http://arxiv.org/abs/2601.05135v1) | M. Reshma, C. S. Stalin et al. | Active galactic nuclei are known to exhibit flux variations across the entire electromagnetic spectrum. Among these, correlations between UV/optical and X-ray flux variations serve as a key diagnostics for understanding the physical connection between the accretion disk and the corona. In this work, we present the results of analysis of ultraviolet (UV) and X-ray flux variations in the narrow line Seyfert 1 galaxy Mrk 1044. Simultaneous observations in the far-UV band (FUV: 1300$-$1800 √Ö) and the X-ray band (0.5$-$7 keV) obtained during 31 August $-$ 8 September 2018 with the Ultraviolet Imaging Telescope and the Soft X-ray Telescope onboard \textit{AstroSat} were used for this study. Significant flux variability was detected in both FUV and X-ray bands. The fractional root mean square variability amplitude ($F_{\rm var}$) was found to be 0.036 $\pm$ 0.001 in the FUV band and 0.384 $\pm$ 0.004 in the X-ray band. To explore potential time lag between the two bands, cross-correlation analysis was performed using both the interpolated cross-correlation function (ICCF) and just another vehicle for estimating lags in nuclei (JAVELIN) methods. Results from both approaches are consistent within 2$œÉ$ uncertainty, indicating that X-ray variations lead the FUV variations, with measured lags of 2.25$\pm$0.05 days (ICCF) and $2.35_{-0.01}^{+0.02}$ days (JAVELIN). This is the first detection of a time delay between UV and X-ray variations in Mrk 1044. The observed UV lag supports the disk reprocessing scenario, wherein X-ray emission from the corona irradiates the accretion disk, driving the observed UV variability. |
| 2026-01-08 | [Online Bayesian Learning of Agent Behavior in Differential Games](http://arxiv.org/abs/2601.05087v1) | Francesco Bianchin, Robert Lefringhausen et al. | This work introduces an online Bayesian game-theoretic method for behavior identification in multi-agent dynamical systems. By casting Hamilton-Jacobi-Bellman optimality conditions as linear-in-parameter residuals, the method enables fast sequential Bayesian updates, uncertainty-aware inference, and robust prediction from limited, noisy data-without history stacks. The approach accommodates nonlinear dynamics and nonquadratic value functions through basis expansions, providing flexible models. Experiments, including linear-quadratic and nonlinear shared-control scenarios, demonstrate accurate prediction with quantified uncertainty, highlighting the method's relevance for adaptive interaction and real-time decision making. |
| 2026-01-08 | [Reinforced Efficient Reasoning via Semantically Diverse Exploration](http://arxiv.org/abs/2601.05053v1) | Ziqi Zhao, Zhaochun Ren et al. | Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl. |
| 2026-01-08 | [ALP and $Z^\prime$ boson at the Electron-Ion collider](http://arxiv.org/abs/2601.04962v1) | Amit Adhikary, Dilip Kumar Ghosh et al. | We study the sensitivity of the upcoming electron-ion (EIC) collider to purely electrophilic new physics in the GeV mass range. Within an effective field theory framework, we consider two different scenarios: an axion-like particle (ALP) and a new heavy neutral vector gauge boson $Z^\prime $, each couples to electrons only. We analyze electron-proton collisions at $\sqrt{s}= 141$ GeV with an integrated luminosity of $100~{\rm fb}^{-1}$, focusing primarily on the tri-electron final state. Additionally, loop-induced ALP-photon couplings driven photon final states are also explored. Incorporating realistic detector effects and systematic uncertainties, we obtain projected exclusion limits on the relevant cross-sections and couplings. We find that the results from EIC can significantly extend the sensitivity to electrophilic axion-like particles and $Z^\prime $ bosons in regions of parameter space that remain weakly constrained by existing experiments. |
| 2026-01-08 | [When Sellers Are Uncertain about Quality](http://arxiv.org/abs/2601.04942v1) | Keita Kuwahara | Second-hand markets have expanded rapidly with the growth of online consumer-to-consumer (C2C) platforms. A key feature of C2C markets is that sellers are typically non-professionals and often face uncertainty about the quality of the goods they sell. This creates scope for platforms to introduce systems that reduce sellers' uncertainty about quality. However, an important question remains: is it socially desirable for sellers to have more precise quality information? We present results showing that while improved information always benefits sellers, it can either benefit or harm buyers. We derive a necessary and sufficient condition under which buyers benefit, and show that this condition holds in many cases, especially when buyers' valuations are not too large relative to sellers' costs. These findings suggest that platforms should consider reducing sellers' uncertainty about quality as a means of improving market efficiency. |
| 2026-01-08 | [Multi-Messenger Studies with High-Energy Neutrinos and Gamma Rays: The WST Opportunity](http://arxiv.org/abs/2601.04939v1) | Fabian Sch√ºssler, Sofia Bisero et al. | The search for the sources of ultra-high-energy cosmic rays (UHECRs) using high-energy neutrinos represents a frontier in high-energy astrophysics. However, a critical bottleneck remains: the ability to rapidly survey the sizable sky areas defined by the localization uncertainties of neutrino detectors and to provide rapid spectroscopic classification of the multitude of optical transients found within them.   By deploying a large field-of-view with high-multiplex Multi-Object Spectroscopy (MOS) on a large aperture telescope, one can instantaneously cover neutrino error circles, thus providing crucial spectroscopic classifications of potential counterparts discovered, for example, by the Vera C. Rubin Observatory (LSST) with unprecedented efficiency. Furthermore, simultaneous operation of a giant panoramic central Integral Field Spectrograph (IFS) would allow for detailed kinematic and environmental characterization of primary candidates. This facility would unlock deep synergies between next-generation neutrino telescopes (IceCube-Gen2, KM3NeT) and gamma-ray observatories (CTAO), transforming unique multi-messenger alerts into a comprehensive physical understanding. |
| 2026-01-08 | [Measurement of differential $t$-channel single top (anti)quark production cross-sections at 13 TeV with the ATLAS detector](http://arxiv.org/abs/2601.04938v1) | ATLAS Collaboration | The production of single top quarks and top antiquarks via the $t$-channel exchange of a virtual $W$ boson is measured in proton-proton collisions at a centre-of-mass energy of 13 TeV at the Large Hadron Collider. The full Run 2 data sample recorded with the ATLAS detector in the years 2015-2018 is used, corresponding to an integrated luminosity of 140 fb$^{-1}$. The absolute and normalised production cross-sections are measured differentially as a function of the transverse momentum and absolute rapidity of the top quark and top antiquark. In addition, the ratio of top quark to top antiquark production cross-sections is measured. The measured distributions are compared with next-to-leading-order quantum chromodynamics predictions obtained with different combinations of matrix-element generators, parton-shower programs and proton parton distribution functions, as well as to next-to-next-to-leading-order calculations. Overall, good agreement is observed between the measurements and the theoretical predictions. For most measured distributions, the sensitivity to differences between the predictions is limited by the systematic uncertainties in the measurement. The measured differential distributions are also interpreted in an effective field theory approach to constrain the Wilson-Coefficient $C_{Qq}^{3,1}$ associated with a four-quark operator. The interpretation accounts for the effect of the selection efficiency, which is altered significantly by non-zero contributions from $C_{Qq}^{3,1}$. |
| 2026-01-07 | [A Comprehensive Computational Framework for Materials Design, Ab Initio Modeling, and Molecular Docking](http://arxiv.org/abs/2601.04186v1) | Md Rakibul Karim Akanda, Michael P. Richard | To facilitate rational molecular and materials design, this research proposes an integrated computational framework that combines stochastic simulation, ab initio quantum chemistry, and molecular docking. The suggested workflow allows systematic investigation of structural stability, binding affinity, and electronic properties across biological and materials science domains by utilizing complementary tools like Avogadro for molecular construction and visualization, AutoDock for docking and interaction analysis, and ORCA for high-level electronic structure computations. Uncertainty, configurational sampling, and optimization in high-dimensional chemical spaces are addressed by combining Monte Carlo-based and annealing-inspired techniques. The work shows how materials science ideas such as polymer design, thin films, crystalline lattices, and bioelectronic systems can be applied to drug development. On-device, open-source computational methods are viable, scalable, and economical, as demonstrated by comparative platform analysis. All things considered, the findings highlight the need of an integrated, repeatable computational pipeline for speeding up de novo molecule assembly and materials architecture while lowering experimental risk and expense. |
| 2026-01-07 | [Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schr√∂dinger Equation](http://arxiv.org/abs/2601.04176v1) | Pietro de Oliveira Esteves | We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility. |
| 2026-01-07 | [Active subspace methods and derivative-based Shapley effects for functions with non-independent variables](http://arxiv.org/abs/2601.04132v1) | Matieyendou Lamboni, Sergei Kucherenko | Lower-dimensional subspaces that impact estimates of uncertainty are often described by Linear combinations of input variables, leading to active variables. This paper extends the derivative-based active subspace methods and derivative-based Shapley effects to cope with functions with non-independent variables, and it introduces sensitivity-based active subspaces. While derivative-based subspace methods focus on directions along which the function exhibits significant variation, sensitivity-based subspace methods seek a reduced set of active variables that enables a reduction in the function's variance. We propose both theoretical results using the recent development of gradients of functions with non-independent variables and practical settings by making use of optimal computations of gradients, which admit dimension-free upper-bounds of the biases and the parametric rate of convergence. Simulations show that the relative performance of derivative-based and sensitivity-based active subspaces methods varies across different functions. |
| 2026-01-07 | [Mean Square Errors of factors extracted using principal components, linear projections, and Kalman filter](http://arxiv.org/abs/2601.04087v1) | Matteo Barigozzi, Diego Fresoli et al. | Factor extraction from systems of variables with a large cross-sectional dimension, $N$, is often based on either Principal Components (PC)-based procedures, or Kalman filter (KF)-based procedures. Measuring the uncertainty of the extracted factors is important when, for example, they have a direct interpretation and/or they are used to summarized the information in a large number of potential predictors. In this paper, we compare the finite $N$ mean square errors (MSEs) of PC and KF factors extracted under different structures of the idiosyncratic cross-correlations. We show that the MSEs of PC-based factors, implicitly based on treating the true underlying factors as deterministic, are larger than the corresponding MSEs of KF factors, obtained by treating the true factors as either serially independent or autocorrelated random variables. We also study and compare the MSEs of PC and KF factors estimated when the idiosyncratic components are wrongly considered as if they were cross-sectionally homoscedastic and/or uncorrelated. The relevance of the results for the construction of confidence intervals for the factors are illustrated with simulated data. |
| 2026-01-07 | [Counterexamples to the conjectured ordering between the waiting-time bound and the thermodynamic uncertainty bound on entropy production](http://arxiv.org/abs/2601.04039v1) | Jie Gu | Two widely used model-free lower bounds on the steady-state entropy production rate of a continuous-time Markov jump process are the thermodynamic uncertainty relation (TUR) bound $œÉ_\text{TUR}$, derived from the mean and variance of a current, and the waiting-time distribution (WTD) bound $œÉ_\mathcal{L}$, derived from the irreversibility of partially observed transition sequences together with their waiting times. It has been conjectured that $œÉ_{\mathcal L}$ is always at least as tight as $œÉ_{\mathrm{TUR}}$ when both are constructed from the same partially observed link. Here we provide four-state counterexamples in a nonequilibrium steady state where $œÉ_{\mathcal L}<œÉ_{\mathrm{TUR}}$. This result shows that no universal ordering exists between these two inference bounds under partial observation. |
| 2026-01-07 | [Can Dynamic Spectrum Sharing Protect Passive Radio Sciences?](http://arxiv.org/abs/2601.03966v1) | Gregory Hellbourg | Dynamic Spectrum Sharing (DSS) is increasingly promoted as a key element of modern spectrum policy, driven by the rising demand from commercial wireless systems and advances in spectrum access technologies. Passive radio sciences, including radio astronomy, Earth remote sensing, and meteorology, operate under fundamentally different constraints. They rely on exceptionally low interference spectrum and are highly vulnerable to even brief radio frequency interference. We examine whether DSS can benefit passive services or whether it introduces new failure modes and enforcement challenges. We propose just-in-time quiet zones (JITQZ) as a mechanism for protecting high value observations and assess hybrid frameworks that preserve static protection for core passive bands while allowing constrained dynamic access in adjacent frequencies. We analyze the roles of propagation uncertainty, electromagnetic compatibility constraints, and limited spectrum awareness. Using a game theoretic framework, we show why non-cooperative sharing fails, identify conditions for sustained cooperation, and examine incentive mechanisms including pseudonymetry-enabled attribution that promote compliance. We conclude that DSS can support passive radio sciences only as a high-reliability, safety-critical system. Static allocations remain essential, and dynamic access is viable only with conservative safeguards and enforceable accountability. |
| 2026-01-07 | [Broadband spectroscopy of astrophysical ice analogues: IV. Optical constants of N$_2$ ice in the terahertz and mid-infrared ranges](http://arxiv.org/abs/2601.03951v1) | F. Kruczkiewicz, A. A. Gavdush et al. | Context. Understanding the optical properties of astrophysical ices is crucial for modeling dust continuum emission and radiative transfer in cold, dense interstellar environments. Molecular nitrogen (N$_2$), a major nitrogen reservoir in protoplanetary disks, plays a key role in nitrogen chemistry, yet the lack of direct terahertz (THz)--infrared (IR) optical constants for N$_2$ ice introduces uncertainties in radiative transfer models, snowline locations, and disk mass estimates.   Aims. We present direct measurements of the optical properties of N$_2$ ice over a broad THz--IR spectral range using terahertz pulsed spectroscopy (TPS) and Fourier-transform infrared spectroscopy (FTIR), supported by density functional theory (DFT) calculations and comparison with literature data.   Methods. N$_2$ ice was grown at cryogenic temperatures by gas-phase deposition onto a cold silicon window. The THz complex refractive index was directly reconstructed from TPS data, while the IR response was derived from FTIR measurements using Kramers--Kronig relations. The optical response was parameterized with a Lorentz dielectric model and validated by DFT calculations.   Results. The complex refractive index of N$_2$ ice is quantified from $ŒΩ= 0.3$--$16$~THz ($Œª= 1$~mm--$18.75~Œº$m). Resonant absorption peaks at $ŒΩ_\mathrm{L} = 1.47$ and $2.13$~THz with damping constants $Œ≥_\mathrm{L} = 0.03$ and $0.22$~THz are attributed to optically active phonons of the $Œ±$-N$_2$ crystal.   Conclusions. We provide a complete set of the THz--IR optical constants for \ce{N2} ice by combining TPS and FTIR spectroscopy. Our results have implications for future observational and modeling studies of protoplanetary disk evolution and planet formation. |
| 2026-01-07 | [Combining simulation-based inference and universal relations for precise and accurate neutron star science](http://arxiv.org/abs/2601.03945v1) | Christian J. Kr√ºger, Sebastian H. V√∂lkel | In this work, we propose a novel approach for identifying, constructing, and validating precise and accurate universal relations for neutron star bulk quantities. A central element is simulation-based inference (SBI), which we adopt to treat uncertainties due to the unknown nuclear equation of state (EOS) as intrinsic non-trivial noise. By assembling a large set of bulk properties of non-rotating neutron stars across multiple state-of-the-art EOS models, we are able to systematically explore universal relations in high-dimensional parameter spaces. Our framework further identifies the most promising parameter combinations, enabling a more focused and traditional construction of explicit universal relations. At the same time, SBI does not rely on explicit relations; instead, it directly provides predictive distributions together with a quantitative measure of systematic uncertainties, which are not captured by conventional approaches. As an example, we report a new universal relation that allows us to obtain the radius as a function of mass, fundamental mode, and one pressure mode. Our analysis shows that SBI can surpass the predictive power of this universal relation while also mitigating systematic errors. Finally, we demonstrate how universal relations can be further calibrated to mitigate systematic errors accurately. |
| 2026-01-07 | [Material exploration through active learning -- METAL](http://arxiv.org/abs/2601.03933v1) | Joakim Brorsson, Henrik Klein Moberg et al. | The discovery and design of new materials are paramount in the development of green technologies. High entropy oxides represent one such group that has only been tentatively explored, mainly due to the inherent problem of navigating vast compositional spaces. Thanks to the emergence of machine learning, however, suitable tools are now readily available. Here, the task of finding oxygen carriers for chemical looping processes has been tackled by leveraging active learning-based strategies combined with first-principles calculations. High efficiency and efficacy have, moreover, been achieved by exploiting the power of recently developed machine learning interatomic potentials. Firstly, the proposed approaches were validated based on an established computational framework for identifying high entropy perovskites that can be used in chemical looping air separation and dry reforming. Chief among the insights thus gained was the identification of the best performing strategies, in the form of greedy or Thompson-based sampling based on uncertainty estimates obtained from Gaussian processes. Building on this newfound knowledge, the concept was applied to a more complex problem, namely the discovery of high entropy oxygen carriers for chemical looping oxygen uncoupling. This resulted in both qualitative as well as quantitative outcomes, including lists of specific materials with high oxygen transfer capacities and configurational entropies. Specifically, the best candidates were based on the known oxygen carrier CaMnO3 but also contained a variety of additional species, of which some, e.g., Ti; Co; Cu; and Ti, were expected while others were not, e.g., Y and Sm. The results suggest that adopting active learning approaches is critical in materials discovery, given that these methods are already shifting research practice and soon will be the norm. |
| 2026-01-07 | [The uncertainty in water mass fraction of wet planets](http://arxiv.org/abs/2601.03932v1) | Michael Lozovsky | Planets with masses between Earth and Neptune often have radii that imply the presence of volatiles, suggesting that water may be abundant in their interiors. However, directly observing the precise water mass fraction and water distribution remains unfeasible. In our study, we employ an internal structure code MAGRATHEA to model planets with high water content and explore potential interior distributions. Departing from traditional assumptions of a layered structure, we determine water and rock distribution based on water-rock miscibility criteria. We model {wet planets} with an iron core and a homogeneous mixture of rock and water above it. At the outer regions of the planet, the pressure and temperature are below the rock-water miscibility point (the second critical point), causing the segregation of water and rock. Consequently, a shell of water is formed in the outermost layers. By considering the water-rock miscibility and the vapor state of water, our approach highlights the uncertainty in estimating the water mass fraction of detected exoplanets. |
| 2026-01-06 | [Heavy Black-Holes Also Matter in Standard Siren Cosmology](http://arxiv.org/abs/2601.03257v1) | Gr√©goire Pierra, Alexander Papadopoulos | With the release of the Gravitational-Wave Transient Catalog GWTC-4.0 by the LIGO-Virgo-KAGRA (LVK) collaboration, 218 candidate detections of gravitational waves (GWs) from compact binary coalescences (CBCs) have been reported. This milestone represents a major advancement for GW cosmology, as many methods, particularly those employing the spectral siren approach, critically depend on the number of available sources. We investigate the impact of a novel parametric model describing the full population mass spectrum of CBCs on the estimation of the Hubble constant. This model is designed to test the impact of heavy black holes in GW cosmology. We perform a joint inference of cosmological and population parameters using 142 CBCs from GWTC-4.0 with a false alarm rate smaller than 0.25 per year, using both spectral and dark siren approaches. With spectral sirens, we estimate the Hubble constant to be $H_0 = 78.8^{+19.0}_{-15.3}\,{\rm km s^{-1} Mpc^{-1}}$ (68% CL), while the dark siren method yields $H_0 = 82.5^{+16.8}_{-14.3}\,{\rm km s^{-1} Mpc^{-1}}$ (68% CL). These results improve the uncertainty by approximately 30.4% and 36.2%, respectively. We show that this improvement is linked to the presence of a new mass scale in the binary black hole mass spectrum at $63.3^{+4.8}_{-4.8}\,M_{\odot}$, which provides additional constraints on the Hubble constant. Besides providing the tightest standard-siren constraints on $H_0$, this highlights the importance of a heavy-mass feature in the black hole spectrum. |
| 2026-01-06 | [Counterfactual Fairness with Graph Uncertainty](http://arxiv.org/abs/2601.03203v1) | Davi Val√©rio, Chrysoula Zerva et al. | Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints. |
| 2026-01-06 | [Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning](http://arxiv.org/abs/2601.03190v1) | Naixin Zhai, Pengyang Shao et al. | Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines. |
| 2026-01-06 | [Breaking the Dimensional Barrier: Dynamic Portfolio Choice with Parameter Uncertainty via Pontryagin Projection](http://arxiv.org/abs/2601.03175v1) | Jeonggyu Huh, Hyeng Keun Koo | We study continuous-time portfolio choice in diffusion markets with parameter $Œ∏\in Œò$ and uncertainty law $q(dŒ∏)$. Nature draws latent $Œ∏\sim q$ at time 0; the investor cannot observe it and must deploy a single $Œ∏$-blind feedback policy maximizing an ex-ante CRRA objective averaged over diffusion noise and $Œ∏$. Our methods access $q$ only by sampling and assume no parametric form. We extend Pontryagin-Guided Direct Policy Optimization (PG-DPO) by sampling $Œ∏$ inside the simulator and computing discrete-time gradients via backpropagation through time (BPTT), and we propose projected PG-DPO (P-PGDPO) that projects costate estimates to satisfy the $q$-aggregated Pontryagin first-order condition, yielding a deployable rule. We prove a BPTT-PMP correspondence uniform on compacts and a residual-based $Œ∏$-blind policy-gap bound under local stability with explicit discretization/Monte Carlo errors; experiments show projection-driven stability and accurate decision-time benchmark recovery in high dimensions. |
| 2026-01-06 | [Conditioning Aircraft Trajectory Prediction on Meteorological Data with a Physics-Informed Machine Learning Approach](http://arxiv.org/abs/2601.03152v1) | Amy Hodgkin, Nick Pepper et al. | Accurate aircraft trajectory prediction (TP) in air traffic management systems is confounded by a number of epistemic uncertainties, dominated by uncertain meteorological conditions and operator specific procedures. Handling this uncertainty necessitates the use of probabilistic, machine learned models for generating trajectories. However, the trustworthiness of such models is limited if generated trajectories are not physically plausible. For this reason we propose a physics-informed approach in which aircraft thrust and airspeed are learned from data and are used to condition the existing Base of Aircraft Data (BADA) model, which is physics-based and enforces energy-based constraints on generated trajectories. A set of informative features are identified and used to condition a probabilistic model of aircraft thrust and airspeed, with the proposed scheme demonstrating a 20% improvement in skilfulness across a set of six metrics, compared against a baseline probabilistic model that ignores contextual information such as meteorological conditions. |
| 2026-01-06 | [A Probabilistic Digital Twin of UK En Route Airspace for Training and Evaluating AI Agents for Air Traffic Control](http://arxiv.org/abs/2601.03113v1) | Nick Pepper, Adam Keane et al. | This paper presents the first probabilistic Digital Twin of operational en route airspace, developed for the London Area Control Centre. The Digital Twin is intended to support the development and rigorous human-in-the-loop evaluation of AI agents for Air Traffic Control (ATC), providing a virtual representation of real-world airspace that enables safe exploration of higher levels of ATC automation.   This paper makes three significant contributions: firstly, we demonstrate how historical and live operational data may be combined with a probabilistic, physics-informed machine learning model of aircraft performance to reproduce real-world traffic scenarios, while accurately reflecting the level of uncertainty inherent in ATC. Secondly, we develop a structured assurance case, following the Trustworthy and Ethical Assurance framework, to provide quantitative evidence for the Digital Twin's accuracy and fidelity. This is crucial to building trust in this novel technology within this safety-critical domain. Thirdly, we describe how the Digital Twin forms a unified environment for agent testing and evaluation. This includes fast-time execution (up to x200 real-time), a standardised Python-based ``gym'' interface that supports a range of AI agent designs, and a suite of quantitative metrics for assessing performance. Crucially, the framework facilitates competency-based assessment of AI agents by qualified Air Traffic Control Officers through a Human Machine Interface. We also outline further applications and future extensions of the Digital Twin architecture. |
| 2026-01-06 | [Computationally Efficient Estimation of Localized Treatment Effects in High-Dimensional Design Spaces using Gaussian Process Regression](http://arxiv.org/abs/2601.03105v1) | Abdulrahman A. Ahmed, M. Amin Rahimian et al. | Population-scale agent-based simulations of the opioid epidemic help evaluate intervention strategies and overdose outcomes in heterogeneous communities and provide estimates of localized treatment effects, which support the design of locally-tailored policies for precision public health. However, it is prohibitively costly to run simulations of all treatment conditions in all communities because the number of possible treatments grows exponentially with the number of interventions and levels at which they are applied. To address this need efficiently, we develop a metamodel framework, whereby treatment outcomes are modeled using a response function whose coefficients are learned through Gaussian process regression (GPR) on locally-contextualized covariates. We apply this framework to efficiently estimate treatment effects on overdose deaths in Pennsylvania counties. In contrast to classical designs such as fractional factorial design or Latin hypercube sampling, our approach leverages spatial correlations and posterior uncertainty to sequentially sample the most informative counties and treatment conditions. Using a calibrated agent-based opioid epidemic model, informed by county-level overdose mortality and baseline dispensing rate data for different treatments, we obtained county-level estimates of treatment effects on overdose deaths per 100,000 population for all treatment conditions in Pennsylvania, achieving approximately 5% average relative error using one-tenth the number of simulation runs required for exhaustive evaluation. Our bi-level framework provides a computationally efficient approach to decision support for policy makers, enabling rapid evaluation of alternative resource-allocation strategies to mitigate the opioid epidemic in local communities. The same analytical framework can be applied to guide precision public health interventions in other epidemic settings. |
| 2026-01-06 | [Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees](http://arxiv.org/abs/2601.03097v1) | Omayra Yago Nieto, Alexandre Anahory Simoes et al. | We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.   The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.   Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems. |
| 2026-01-06 | [Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs](http://arxiv.org/abs/2601.03087v1) | David Hartmann, Lena Pohlmann et al. | Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Œî$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \textsc{CivilComments} and \textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\varepsilon=0.02$ for \textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations. |
| 2026-01-06 | [A Conditional Variational Framework for Channel Prediction in High-Mobility 6G OTFS Networks](http://arxiv.org/abs/2601.03084v1) | Mohsen Kazemian, J√ºrgen Jasperneite | This paper proposes a machine learning (ML) based method for channel prediction in high mobility orthogonal time frequency space (OTFS) channels. In these scenarios, rapid variations caused by Doppler spread and time varying multipath propagation lead to fast channel decorrelation, making conventional pilot based channel estimation methods prone to outdated channel state information (CSI) and excessive overhead. Therefore, reliable channel prediction methods become essential to support robust detection and decoding in OTFS systems. In this paper, we propose conditional variational autoencoder for channel prediction (CVAE4CP) method, which learns the conditional distribution of OTFS delay Doppler channel coefficients given physical system and mobility parameters. By incorporating these parameters as conditioning information, the proposed method enables the prediction of future channel coefficients before their actual realization, while accounting for inherent channel uncertainty through a low dimensional latent representation. The proposed framework is evaluated through extensive simulations under high mobility conditions. Numerical results demonstrate that CVAE4CP consistently outperforms a competing learning based baseline in terms of normalized mean squared error (NMSE), particularly at high Doppler frequencies and extended prediction horizons. These results confirm the effectiveness and robustness of the proposed approach for channel prediction in rapidly time varying OTFS systems. |
| 2026-01-05 | [BEDS: Bayesian Emergent Dissipative Structures](http://arxiv.org/abs/2601.02329v1) | Laurent Caraffa | We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.   We derive fundamental mathematical constants (e, œÄ, œÜ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking G√∂del's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.   As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence. |
| 2026-01-05 | [A comprehensive search for high-velocity X-ray sources: New compact object binary candidates in the Gaia era](http://arxiv.org/abs/2601.02287v1) | Yue Zhao, Poshak Gandhi et al. | We perform a comprehensive search for high-velocity X-ray sources with large X-ray/optical flux ratios ($F_\mathrm{X}/F_\mathrm{G}$), identifying candidates for interacting black hole or neutron star binaries potentially accelerated by supernova natal kicks. We cross-match X-ray points sources from a variety of catalogues (Chandra, XMM-Newton, Swift and eROSITA) with Gaia DR3. Using Gaia coordinates, parallaxes, and proper motions, we compute peculiar velocities ($\upsilon_\mathrm{pec}$) relative to Galactic disc rotation. Remaining agnostic about radial velocities (RVs), we vary RVs to find the minimum possible $\upsilon_\mathrm{pec}$ values ($\upsilon_\mathrm{pec, min}$). Uncertainties on $\upsilon_\mathrm{pec, min}$ are estimated via Monte Carlo resampling, and we select X-ray sources that have $1\,œÉ$ lower limits on $\upsilon_\mathrm{pec, min}\geq 200\,\mathrm{km~s^{-1}}$ and high $F_\mathrm{X}/F_\mathrm{G}$ values. We show that this velocity threshold excludes most contaminants (e.g., cataclysmic variables and active binaries) while retaining a sensible fraction of compact object binaries, demonstrating that $\upsilon_\mathrm{pec}$ could serve as an effective indicator for the presence of a neutron star or black hole companion. Our selection yields a sample of 2372 sources, from which we construct a gold sample of 7 sources that have relatively well-constrained astrometry and confident optical counterparts. Follow-up is necessary to confirm and characterise their high-energy emission, as well as a Galactic disc vs. halo origin. |
| 2026-01-05 | [DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies](http://arxiv.org/abs/2601.02267v1) | Renke Wang, Zhenyu Zhang et al. | Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html |
| 2026-01-05 | [The Stellar Winds Atlas I: Current uncertainties in mass-loss rates](http://arxiv.org/abs/2601.02263v1) | Amedeo Romagnolo, Floor S. Broekgaarden et al. | Stellar winds are a major source of uncertainty in understanding the life and deaths of massive stars. Across studies in the field, prescriptions for stellar winds differ substantially in both their physical assumptions and implementation, making them a dominant contributor to model-to-model variation. In this work, we present a systematic analysis of the physical assumptions underlying commonly adopted wind prescriptions for optically thin and optically thick winds of hot stars, as well as the winds of cool supergiants. Our analysis reveals substantial discrepancies across all regimes: predicted mass-loss rates for optically thin winds differ by more than an order of magnitude, while rates for cool supergiants vary by several orders of magnitude, with even wider uncertainties arising in extrapolation regimes beyond the Humphreys-Davidson limit. These disparities introduce significant ambiguity into the predicted formation of Wolf-Rayet (WR) stars, a problem further compounded by the inconsistent application of transition criteria. A central issue is the "cool Wolf-Rayet problem", a temperature regime where the classical electron-scattering Eddington factor ($Œì_{\rm e}$) loses physical consistency. Because this factor is widely used to determine WR mass-loss rates, its failure forces models to rely on uncertain extrapolations and ad-hoc corrections. We conclude that the dominant stellar wind uncertainties arise from a mismatch between the physical assumptions in stellar wind models and the structure of the stars to which they are applied. Our framework clarifies the origins of current theoretical discrepancies and identifies the key physical bottlenecks that must be addressed to improve mass-loss modeling for massive stars. |
| 2026-01-05 | [Risk-Averse Markov Decision Processes: Applications to Electricity Grid and Reservoir Management](http://arxiv.org/abs/2601.02207v1) | Arash Khojaste, Jonathan Pearce et al. | This paper develops risk-averse models to support system operators in planning and operating the electricity grid under uncertainty from renewable power generation. We incorporate financial risk hedging using conditional value at risk (CVaR) within a Markov Decision Process (MDP) framework and propose efficient, exact solution methods for these models. In addition, we introduce a power reliability-oriented risk measure and present new, computationally efficient models for risk-averse grid planning and operations. |
| 2026-01-05 | [Single- and Multi-Objective Stochastic Optimization for Next-Generation Networks in the Generative AI and Quantum Computing Era](http://arxiv.org/abs/2601.02175v1) | Trinh Van Chien, Bui Trong Duc et al. | Next Generation (NG) networks move beyond simply connecting devices to creating an ecosystem of connected intelligence, especially with the support of generative Artificial Intelligence (AI) and quantum computation. These systems are expected to handle large-scale deployments and high-density networks with diverse functionalities. As a result, there is an increasing demand for efficient and intelligent algorithms that can operate under uncertainty from both propagation environments and networking systems. Traditional optimization methods often depend on accurate theoretical models of data transmission, but in real-world NG scenarios, they suffer from high computational complexity in large-scale settings. Stochastic Optimization (SO) algorithms, designed to accommodate extremely high density and extensive network scalability, have emerged as a powerful solution for optimizing wireless networks. This includes various categories that range from model-based approaches to learning-based approaches. These techniques are capable of converging within a feasible time frame while addressing complex, large-scale optimization problems. However, there is currently limited research on SO applied for NG networks, especially the upcoming Sixth-Generation (6G). In this survey, we emphasize the relationship between NG systems and SO by eight open questions involving the background, key features, and lesson learned. Overall, our study starts by providing a detailed overview of both areas, covering fundamental and widely used SO techniques, spanning from single to multi-objective signal processing. Next, we explore how different algorithms can solve NG challenges, such as load balancing, optimizing energy efficiency, improving spectral efficiency, or handling multiple performance trade-offs. Lastly, we highlight the challenges in the current research and propose new directions for future studies. |
| 2026-01-05 | [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](http://arxiv.org/abs/2601.02151v1) | Muxi Diao, Lele Yang et al. | Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities. |
| 2026-01-05 | [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](http://arxiv.org/abs/2601.02076v1) | Yingte Shu, Yuchuan Tian et al. | Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding. |
| 2026-01-05 | [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](http://arxiv.org/abs/2601.02008v1) | Midhat Urooj, Ayan Banerjee et al. | Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI. |
| 2026-01-05 | [Euclid: Improving redshift distribution reconstruction using a deep-to-wide transfer function](http://arxiv.org/abs/2601.02005v1) | Y. Kang, S. Paltani et al. | The Euclid mission seeks to understand the Universe expansion history and the nature of dark energy, which requires a very accurate estimate of redshift distribution. Achieving this accuracy relies on reference samples with spectroscopic redshifts, together with a procedure to match them to survey sources for which only photometric redshifts are available. One important source of systematic uncertainty is the mismatch in photometric properties between galaxies in the Euclid survey and the reference objects. We develop a method to degrade the photometry of objects with deep photometry to match the properties of any shallower survey in the multi-band photometric space, preserving all the correlations between the fluxes and their uncertainties. We compare our transfer method with more demanding image-based methods, such as Balrog from the Dark Energy Survey Collaboration. According to metrics, our method outperforms Balrog. We implement it in the redshift distribution reconstruction, based on the self-organising map approach of arXiv:1509.03318, and test it using a realistic sample from the Euclid Flagship Simulation. We find that the key ingredient is to ensure that the reference objects are distributed in the colour space the same way as the wide-survey objects, which can be efficiently achieved with our transfer method. In our best implementation, the mean redshift biases are consistently reduced across the tomographic bins, bringing a significant fraction of them within the Euclid accuracy requirements in all tomographic bins. Equally importantly, the tests allow us to pinpoint which step in the calibration pipeline has the strongest impact on achieving the required accuracy. Our approach also reproduces the overall redshift distributions, which are crucial for applications such as angular clustering. |
| 2026-01-02 | [Callisto's Nonresonant Orbit as an Outcome of Circum-Jovian Disk Substructure](http://arxiv.org/abs/2601.00786v1) | Teng Ee Yap, Konstantin Batygin | The Galilean moons of Io, Europa, and Ganymede exhibit a 4:2:1 commensurability in their mean motions, a configuration known as the Laplace resonance. The prevailing view for the origin of this three-body resonance involves the convergent migration of the moons, resulting from gas-driven torques in the circum-Jovian disk wherein they accreted. To account for Callisto's exclusion from the resonant chain, a late and/or slow accretion of the fourth and outermost Galilean moon is typically invoked, stalling its migration. Here, we consider an alternative scenario in which Callisto's nonresonant orbit is a consequence of disk substructure. Using a suite of N-body simulations that self-consistently account for satellite-disk interactions, we show that a pressure bump can function as a migration trap, isolating Callisto and alleviating constraints on its timing of accretion. Our simulations position the bump interior to the birthplaces of all four moons. In exploring the impact of bump structure on simulation outcomes, we find that it cannot be too sharp nor flat to yield the observed orbital architecture. In particular, a "Goldilocks" zone is mapped in parameter space, corresponding to a well-defined range in bump aspect ratio. Within this range, Io, Europa, and Ganymede are sequentially trapped at the bump, and ushered across it through resonant lockstep migration with their neighboring, exterior moon. The implications of our work are discussed in the context of uncertainties regarding Callisto's interior structure, arising from the possibility of non-hydrostatic contributions to its shape and gravity field, unresolved by the Galileo spacecraft. |
| 2026-01-02 | [Spatiotemporal Detection and Uncertainty Visualization of Atmospheric Blocking Events](http://arxiv.org/abs/2601.00775v1) | Mingzhe Li, Peer Nowack et al. | Atmospheric blocking events are quasi-stationary high-pressure systems that disrupt the typical paths of polar and subtropical air currents, often producing prolonged extreme weather events such as summer heat waves or winter cold spells. Despite their critical role in shaping mid-latitude weather, accurately modeling and analyzing blocking events in long meteorological records remains a significant challenge. To address this challenge, we present an uncertainty visualization framework for detecting and characterizing atmospheric blocking events. First, we introduce a geometry-based detection and tracking method, evaluated on both pre-industrial climate model simulations (UKESM) and reanalysis data (ERA5), which represent historical Earth observations assimilated from satellite and station measurements onto regular numerical grids using weather models. Second, we propose a suite of uncertainty-aware summaries: contour boxplots that capture representative boundaries and their variability, frequency heatmaps that encode occurrences, and 3D temporal stacks that situate these patterns in time. Third, we demonstrate our framework in a case study of the 2003 European heatwave, mapping the spatiotemporal occurrences of blocking events using these summaries. Collectively, these uncertainty visualizations reveal where blocking events are most likely to occur and how their spatial footprints evolve over time. We envision our framework as a valuable tool for climate scientists and meteorologists: by analyzing how blocking frequency, duration, and intensity vary across regions and climate scenarios, it supports both the study of historical blocking events and the assessment of scenario-dependent climate risks associated with changes in extreme weather linked to blocking. |
| 2026-01-02 | [Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI](http://arxiv.org/abs/2601.00742v1) | Turab Lookman, YuJie Liu et al. | This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by "human-out-of-the-loop" discovery processes. |
| 2026-01-02 | [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](http://arxiv.org/abs/2601.00737v1) | Uƒüurcan √ñzalp | Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network. |
| 2026-01-02 | [Stability of vehicular admission control schemes in urban traffic networks under modelling uncertainty](http://arxiv.org/abs/2601.00732v1) | Michalis Ramp, Andreas Kasis et al. | Urban transportation networks face significant challenges due to traffic congestion, leading to adverse environmental and socioeconomic impacts. Vehicular admission control (VAC) strategies have emerged as a promising solution to alleviate congestion. By leveraging information and communication technologies, VAC strategies regulate vehicle entry into the network to optimize different traffic metrics of interest over space and time. Despite the significant development of VAC strategies, their stability at the presence of modelling uncertainty remains under-explored. This paper investigates the stability properties of a class of decentralized VAC schemes under modelling uncertainty. Specifically, we consider large-scale, heterogeneous urban traffic networks characterised by nonlinear dynamics and concave macroscopic fundamental diagrams with bounded uncertainty between flow, density, and speed. In this context, we examine a broad class of decentralized VAC dynamics, described by general nonlinear forms. Using passivity theory, we derive scalable, locally verifiable conditions on the design of VAC schemes, that enable stability guarantees in the presence of modelling uncertainty. Several examples are presented to illustrate the applicability of the proposed design framework. Our analytical results are validated through numerical simulations on a 6-region system, demonstrating their effectiveness and practical relevance. |
| 2026-01-02 | [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](http://arxiv.org/abs/2601.00696v1) | Yash Jain, Xinjie Liu et al. | Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities. |
| 2026-01-02 | [Turbulence is ineffective in causing raindrop growth in polluted clouds](http://arxiv.org/abs/2601.00637v1) | K. Shri Vignesh, Ambedkar Sanket Sukdeo et al. | Aerosol-cloud interactions represent the largest uncertainty in climate-change assessment, and while cloud turbulence is considered crucial for droplet growth, its precise role remains unclear. Our laboratory-controlled studies show that turbulence does not always enhance collision and coalescence; instead, its influence emerges only when droplets have a sufficiently broad size distribution. The dissipative-scale droplet behaviour underscores the importance of improved parameterisations to accurately model cloud microphysics. |
| 2026-01-02 | [Bayesian optimization for re-analysis and calibration of extreme sea state events simulated with a spectral third-generation wave model](http://arxiv.org/abs/2601.00628v1) | C√©dric Goeury, Thierry Fouquet et al. | Accurate hindcasting of extreme sea state events is essential for coastal engineering, risk assessment, and climate studies. However, the reliability of numerical wave models remains limited by uncertainties in physical parameterizations and model inputs. This study presents a novel calibration framework based on Bayesian Optimization (BO), leveraging the Tree structured Parzen Estimator (TPE) to efficiently estimate uncertain sink term parameters, specifically bottom friction dissipation, depth induced breaking, and wave dissipation from strong opposing currents, in the ANEMOC-3 hindcast wave model. The proposed method enables joint optimization of continuous parameters and discrete model structures, significantly reducing discrepancies between model outputs and observations. Applied to a one month period encompassing multiple intense storm events along the French Atlantic coast, the calibrated model demonstrates improved agreement with buoy measurements, achieving lower bias, RMSE, and scatter index relative to the default sea$-$state solver configuration. The results highlight the potential of BO to automate and enhance wave model calibration, offering a scalable and flexible approach applicable to a wide range of geophysical modeling problems. Future extensions include multi-objective optimization, uncertainty quantification, and integration of additional observational datasets. |
| 2026-01-02 | [Quantifying the uncertainty of molecular dynamics simulations : Good-Turing statistics revisited](http://arxiv.org/abs/2601.00618v1) | Vasiliki Tsampazi, Nicholas M. Glykos | We have previously shown that Good-Turing statistics can be applied to molecular dynamics trajectories to estimate the probability of observing completely new (thus far unobserved) biomolecular structures, and showed that the method is stable, dependable and its predictions verifiable. The major problem with that initial algorithm was the requirement for calculating and storing in memory the two-dimensional RMSD matrix of the currently available trajectory. This requirement precluded the application of the method to very long simulations. Here we describe a new variant of the Good-Turing algorithm whose memory requirements scale linearly with the number of structures in the trajectory, making it suitable even for extremely long simulations. We show that the new method gives essentially identical results with the older implementation, and present results obtained from trajectories containing up to 22 million structures. A computer program implementing the new algorithm is available from standard repositories. |
| 2026-01-02 | [Noise-Robust Tiny Object Localization with Flows](http://arxiv.org/abs/2601.00617v1) | Huixin Sun, Linlin Yang et al. | Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset. |
| 2025-12-31 | [Scaling Open-Ended Reasoning to Predict the Future](http://arxiv.org/abs/2512.25070v1) | Nikhil Chandak, Shashwat Goel et al. | High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible. |
| 2025-12-31 | [Feeling Blue: Constructing a Robust SALT3 UV Template and Constraining its Redshift Dependency](http://arxiv.org/abs/2512.25064v1) | Qinan Wang, David O. Jones et al. | Upcoming cosmological surveys will obtain numerous rest-frame ultraviolet (UV) observations of Type Ia supernovae (SNe Ia), yet there is concern about how standardizable SNe Ia are in the UV. In this work, we train a robust optical--UV SED model for SNe Ia (SALT3-UV) with the open-source model-training software $\texttt{SALTshaker}$. We incorporate a spectroscopic UV data sample from HST, including 67 UV spectra from 18 nearby SNe Ia. Unlike previous training spectra, the HST spectra have sufficiently precise calibration that they do not require additional warping to match coincident photometric data. Additionally, while including this new SN Ia sample necessitates incorporating auxiliary photometric data from ZTF and ATLAS that have insufficient calibration for cosmological analyses, the improvements in the calibration of these data is anticipated in the near future. Compared to the previous SALT3-K21 model, the SALT3-UV model shows a significant improvement in the UV down to $2000\mathring{\text{A}}$, with over a threefold improvement in model uncertainty and a more physically accurate continuum and line features. We further evaluate potential redshift evolution in the UV template by separating the UV training sample into low- and high-$z$ subsamples. Our results reveal a non-negligible $\gtrsim 0.05$ mag difference between low- and high-$z$ SALT3-UV models in the $g-$band at $z\gtrsim0.5$ and the $u-$band at $z\gtrsim0.2$. We demonstrate that, if confirmed, such evolution could lead to a few-percent bias in the measurement of $w$ if high-$z$ rest-frame UV data are included in future cosmological surveys such as LSST and $\textit{Roman}$. |
| 2025-12-31 | [Sequential Bayesian parameter-state estimation in dynamical systems with noisy and incomplete observations via a variational framework](http://arxiv.org/abs/2512.25056v1) | Liliang Wang, Alex Gorodetsky | Online joint estimation of unknown parameters and states in a dynamical system with uncertainty quantification is crucial in many applications. For example, digital twins dynamically update their knowledge of model parameters and states to support prediction and decision-making. Reliability and computational speed are vital for DTs. Online parameter-state estimation ensures computational efficiency, while uncertainty quantification is essential for making reliable predictions and decisions. In parameter-state estimation, the joint distribution of the state and model parameters conditioned on the data, termed the joint posterior, provides accurate uncertainty quantification. Because the joint posterior is generally intractable to compute, this paper presents an online variational inference framework to compute its approximation at each time step. The approximation is factorized into a marginal distribution over the model parameters and a state distribution conditioned on the parameters. This factorization enables recursive updates through a two-stage procedure: first, the parameter posterior is approximated via variational inference; second, the state distribution conditioned on the parameters is computed using Gaussian filtering based on the estimated parameter posterior. The algorithmic design is supported by a theorem establishing upper bounds on the joint posterior approximation error. Numerical experiments demonstrate that the proposed method (i) matches the performance of the joint particle filter in low-dimensional problems, accurately inferring both unobserved states and unknown parameters of dynamical and observation models; (ii) remains robust under noisy, partial observations and model discrepancies in a chaotic Lorenz 96 system; and (iii) scales effectively to a high-dimensional convection-diffusion system, where it outperforms the joint ensemble Kalman filter. |
| 2025-12-31 | [ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning](http://arxiv.org/abs/2512.25023v1) | Timo Kaufmann, Yannick Metz et al. | Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy. |
| 2025-12-31 | [Strategies for Overcoming Gradient Troughs in the ADAPT-VQE Algorithm](http://arxiv.org/abs/2512.25004v1) | Jonas Stadelmann, Julian √úbelher et al. | The adaptive derivative-assembled problem-tailored variational quantum eigensolver (ADAPT-VQE) provides a promising approach for simulating highly correlated quantum systems on quantum devices, as it strikes a balance between hardware efficiency, trainability, and accuracy. Although ADAPT-VQE avoids many of the shortcomings of other VQEs, it is sometimes hindered by a phenomenon known as gradient troughs. This refers to a non-monotonic convergence of the gradients, which may become very small even though the minimum energy has not been reached. This results in difficulties finding the right operators to add to the ansatz, due to the limited number of shots and statistical uncertainties, leading to stagnation in the circuit structure optimization. In this paper, we propose ways to detect and mitigate this phenomenon. Leveraging the non-commutative algebra of the ansatz, we develop heuristics for determining where to insert new operators into the circuit. We find that gradient troughs are more likely to arise when the same locations are used repeatedly for new operator insertions. Our novel protocols, which add new operators in different ansatz positions, allow us to escape gradient troughs and thereby lower the measurement cost of the algorithm. This approach achieves an effective balance between cost and efficiency, leading to faster convergence without compromising the low circuit depth and gate count of ADAPT-VQE. |
| 2025-12-31 | [Matrix Thermodynamic Uncertainty Relation for Non-Abelian Charge Transport](http://arxiv.org/abs/2512.24956v1) | Domingos S. P. Salazar | Thermodynamic uncertainty relations (TURs) bound the precision of currents by entropy production, but quantum transport of noncommuting (non-Abelian) charges challenges standard formulations because different charge components cannot be monitored within a single classical frame. We derive a process-level matrix TUR starting from the operational entropy production $Œ£= D(œÅ'_{SE}\|œÅ'_S\!\otimes\!œÅ_E)$. Isolating the experimentally accessible bath divergence $D_{\mathrm{bath}}=D(œÅ'_E\|œÅ_E)$, we prove a fully nonlinear, saturable lower bound valid for arbitrary current vectors $Œîq$: $D_{\mathrm{bath}} \ge B(Œîq,V,V')$, where the bound depends only on the transported-charge signal $Œîq$ and the pre/post collision covariance matrices $V$ and $V'$. In the small-fluctuation regime $D_{\mathrm{bath}}\geq\frac12\,Œîq^{\mathsf T}V^{-1}Œîq+O(\|Œîq\|^4)$, while beyond linear response it remains accurate. Numerical strong-coupling qubit collisions illustrate the bound and demonstrate near-saturation across broad parameter ranges using only local measurements on the bath probe. |
| 2025-12-31 | [MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control](http://arxiv.org/abs/2512.24955v1) | Yongwei Zhang, Yuanzhe Xing et al. | Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $Œª$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available. |
| 2025-12-31 | [The uncertainty constants: A unified framework of two, three and four observables](http://arxiv.org/abs/2512.24950v1) | Minyi Huang | Uncertainty is a fundamental and important concept in quantum mechanics. Recent works have revealed both the product and sum forms of uncertainty constants for three observables. Such a result is intimately to the properties of Pauli operators. In this work, using the technique in matrix theory, we give an alternative proof for the case of three observables, and generalize the result to the case of four measurements. Comparing with the original proof, such a derivation is simplified. Moreover, the discussions can deal with the summation form of uncertainty relation for two, three and four observables in a unified way. |
| 2025-12-31 | [Stochastic factors can matter: improving robust growth under ergodicity](http://arxiv.org/abs/2512.24906v1) | Balint Binkert, David Itkin et al. | Drifts of asset returns are notoriously difficult to model accurately and, yet, trading strategies obtained from portfolio optimization are very sensitive to them. To mitigate this well-known phenomenon we study robust growth-optimization in a high-dimensional incomplete market under drift uncertainty of the asset price process $X$, under an additional ergodicity assumption, which constrains but does not fully specify the drift in general. The class of admissible models allows $X$ to depend on a multivariate stochastic factor $Y$ and fixes (a) their joint volatility structure, (b) their long-term joint ergodic density and (c) the dynamics of the stochastic factor process $Y$. A principal motivation of this framework comes from pairs trading, where $X$ is the spread process and models with the above characteristics are commonplace. Our main results determine the robust optimal growth rate, construct a worst-case admissible model and characterize the robust growth-optimal strategy via a solution to a certain partial differential equation (PDE). We demonstrate that utilizing the stochastic factor leads to improvement in robust growth complementing the conclusions of the previous study by Itkin et. al. (arXiv:2211.15628 [q-fin.MF], forthcoming in $\textit{Finance and Stochastics}$), which additionally robustified the dynamics of the stochastic factor leading to $Y$-independent optimal strategies. Our analysis leads to new financial insights, quantifying the improvement in growth the investor can achieve by optimally incorporating stochastic factors into their trading decisions. We illustrate our theoretical results on several numerical examples including an application to pairs trading. |
| 2025-12-31 | [AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference](http://arxiv.org/abs/2512.24847v1) | Linhao Fan, Hongqiang Fang et al. | High-quality reconstruction of Aerosol Optical Depth (AOD) fields is critical for Atmosphere monitoring, yet current models remain constrained by the scarcity of complete training data and a lack of uncertainty quantification.To address these limitations, we propose AODDiff, a probabilistic reconstruction framework based on diffusion-based Bayesian inference. By leveraging the learned spatiotemporal probability distribution of the AOD field as a generative prior, this framework can be flexibly adapted to various reconstruction tasks without requiring task-specific retraining. We first introduce a corruption-aware training strategy to learns a spatiotemporal AOD prior solely from naturally incomplete data. Subsequently, we employ a decoupled annealing posterior sampling strategy that enables the more effective and integration of heterogeneous observations as constraints to guide the generation process. We validate the proposed framework through extensive experiments on Reanalysis data. Results across downscaling and inpainting tasks confirm the efficacy and robustness of AODDiff, specifically demonstrating its advantage in maintaining high spatial spectral fidelity. Furthermore, as a generative model, AODDiff inherently enables uncertainty quantification via multiple sampling, offering critical confidence metrics for downstream applications. |
| 2025-12-29 | [Galaxy Zoo Evo: 1 million human-annotated images of galaxies](http://arxiv.org/abs/2512.23691v1) | Mike Walmsley, Steven Bamford et al. | We introduce Galaxy Zoo Evo, a labeled dataset for building and evaluating foundation models on images of galaxies. GZ Evo includes 104M crowdsourced labels for 823k images from four telescopes. Each image is labeled with a series of fine-grained questions and answers (e.g. "featured galaxy, two spiral arms, tightly wound, merging with another galaxy"). These detailed labels are useful for pretraining or finetuning. We also include four smaller sets of labels (167k galaxies in total) for downstream tasks of specific interest to astronomers, including finding strong lenses and describing galaxies from the new space telescope Euclid. We hope GZ Evo will serve as a real-world benchmark for computer vision topics such as domain adaption (from terrestrial to astronomical, or between telescopes) or learning under uncertainty from crowdsourced labels. We also hope it will support a new generation of foundation models for astronomy; such models will be critical to future astronomers seeking to better understand our universe. |
| 2025-12-29 | [Joint Modeling of Longitudinal and Survival Data: A Bayesian Approach for Predicting Disease Progression](http://arxiv.org/abs/2512.23627v1) | Nithisha Suryadevara, Vivek Reddy Srigiri | Joint modeling of longitudinal and survival data has become increasingly important in medical research, particularly for understanding disease progression in chronic conditions where both repeated biomarker measurements and time-to-event outcomes are available. Traditional two-stage methods, which analyze longitudinal and survival components separately, often result in biased estimates and suboptimal predictions due to failure to account for their interdependence.   In this study, we propose a Bayesian hierarchical joint modeling framework with an emphasis on predictive evaluation and clinical interpretability. The model simultaneously characterizes the longitudinal trajectory of a biomarker and the associated survival outcome through shared random effects, capturing the intrinsic association between disease dynamics and event risk. The Bayesian formulation allows flexible incorporation of prior information, accommodates irregular measurement times and missing data, and provides full posterior distributions for uncertainty quantification via credible intervals.   We evaluate the proposed framework using both simulated data designed to mimic realistic patient trajectories and a real-world clinical dataset involving patients with chronic liver disease. Results demonstrate that the Bayesian joint model consistently outperforms conventional two-stage approaches in terms of parameter estimation accuracy and predictive performance, as measured by time-dependent area under the curve and Brier scores. The proposed approach provides a robust and interpretable tool for dynamic, patient-specific prognosis, supporting clinical decision-making in personalized medicine. |
| 2025-12-29 | [Distribution-Free Process Monitoring with Conformal Prediction](http://arxiv.org/abs/2512.23602v1) | Christopher Burger | Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods. |
| 2025-12-29 | [Profile Bayesian Optimization for Expensive Computer Experiments](http://arxiv.org/abs/2512.23581v1) | Courtney Kyger, James Fernandez et al. | We propose a novel Bayesian optimization (BO) procedure aimed at identifying the ``profile optima'' of a deterministic black-box computer simulation that has a single control parameter and multiple nuisance parameters. The profile optima capture the optimal response values as a function of the control parameter. Our objective is to identify them across the entire plausible range of the control parameter. Classic BO, which targets a single optimum over all parameters, does not explore the entire control parameter range. Instead, we develop a novel two-stage acquisition scheme to balance exploration across the control parameter and exploitation of the profile optima, leveraging deep and shallow Gaussian process surrogates to facilitate uncertainty quantification. We are motivated by a computer simulation of a diffuser in a rotating detonation combustion engine, which returns the energy lost through diffusion as a function of various design parameters. We aim to identify the lowest possible energy loss as a function of the diffuser's length; understanding this relationship will enable well-informed design choices. Our ``profile Bayesian optimization'' procedure outperforms traditional BO and profile optimization methods on a variety of benchmarks and proves effective in our motivating application. |
| 2025-12-29 | [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](http://arxiv.org/abs/2512.23508v1) | Alessio Benavoli, Alessandro Facchini et al. | How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences. |
| 2025-12-29 | [Encoding higher-order argumentation frameworks with supports to propositional logic systems](http://arxiv.org/abs/2512.23507v1) | Shuai Tang | Argumentation frameworks ($AF$s) have been extensively developed, but existing higher-order bipolar $AF$s suffer from critical limitations: attackers and supporters are restricted to arguments, multi-valued and fuzzy semantics lack unified generalization, and encodings often rely on complex logics with poor interoperability. To address these gaps, this paper proposes a higher-order argumentation framework with supports ($HAFS$), which explicitly allows attacks and supports to act as both targets and sources of interactions. We define a suite of semantics for $HAFS$s, including extension-based semantics, adjacent complete labelling semantics (a 3-valued semantics), and numerical equational semantics ([0,1]-valued semantics). Furthermore, we develop a normal encoding methodology to translate $HAFS$s into propositional logic systems ($\mathcal{PLS}$s): $HAFS$s under complete labelling semantics are encoded into ≈Åukasiewicz's three-valued propositional logic ($\mathcal{PL}_3^L$), and those under equational semantics are encoded into fuzzy $\mathcal{PLS}$s ($\mathcal{PL}_{[0,1]}$) such as G√∂del and Product fuzzy logics. We prove model equivalence between $HAFS$s and their encoded logical formulas, establishing the logical foundation of $HAFS$ semantics. Additionally, we investigate the relationships between 3-valued complete semantics and fuzzy equational semantics, showing that models of fuzzy encoded semantics can be transformed into complete semantics models via ternarization, and vice versa for specific t-norms. This work advances the formalization and logical encoding of higher-order bipolar argumentation, enabling seamless integration with lightweight computational solvers and uniform handling of uncertainty. |
| 2025-12-29 | [Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery](http://arxiv.org/abs/2512.23505v1) | Mehdi Heydari Shahna | Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.   This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.   Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.   The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions. |
| 2025-12-29 | [Deep classifier kriging for probabilistic spatial prediction of air quality index](http://arxiv.org/abs/2512.23474v1) | Junyu Chen, Pratik Nag et al. | Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation. |
| 2025-12-29 | [Uncertainty calibration for latent-variable regression models](http://arxiv.org/abs/2512.23444v1) | Zina-Sabrina Duma, Otto Lamminp√§√§ et al. | Uncertainty quantification is essential for scientific analysis, as it allows for the evaluation and interpretation of variability and reliability in complex systems and datasets. In their original form, multivariate statistical regression models (partial least-squares regression, PLS, principal component regression, PCR) along with their kernelized versions (kernel partial least-squares regression, K-PLS, kernel principal component regression, K-PCR), do not incorporate uncertainty quantification as part of their output. In this study, we propose a method inspired by conformal inference to estimate and calibrate the uncertainty of multivariate statistical models. The result of this method is a point prediction accompanied by prediction intervals that depend on the input data. We tested the proposed method on both traditional and kernelized versions of PLS and PCR. The method is demonstrated using synthetic data, as well as laboratory near-infrared (NIR) and airborne hyperspectral regression models for estimating functional plant traits. The model was able to successfully identify the uncertain regions in the simulated data and match the magnitude of the uncertainty. In real-case scenarios, the optimised model was not overconfident nor underconfident when estimating from test data: for example, for a 95% prediction interval, 95% of the true observations were inside the prediction interval. |
| 2025-12-29 | [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](http://arxiv.org/abs/2512.23441v1) | Taha Emre, Arunava Chakravarty et al. | Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases. |
| 2025-12-26 | [Index-Tracking Portfolio Construction and Rebalancing under Bayesian Sparse Modelling and Uncertainty Quantification](http://arxiv.org/abs/2512.22109v1) | Dimitrios Roxanas | We study the construction and rebalancing of sparse index-tracking portfolios from an operational research perspective, with explicit emphasis on uncertainty quantification and implementability. The decision variables are portfolio weights constrained to sum to one; the aims are to track a reference index closely while controlling the number of names and the turnover induced by rebalancing. We cast index tracking as a high-dimensional linear regression of index returns on constituent returns, and employ a sparsity-inducing Laplace prior on the weights. A single global shrinkage parameter controls the trade-off between tracking error and sparsity, and is calibrated by an empirical-Bayes stochastic approximation scheme. Conditional on this calibration, we approximate the posterior distribution of the portfolio weights using proximal Langevin-type Markov chain Monte Carlo algorithms tailored to the budget constraint. This yields posterior uncertainty on tracking error, portfolio composition and prospective rebalancing moves. Building on these posterior samples, we propose rules for rebalancing that gate trades through magnitude-based thresholds and posterior activation probabilities, thereby trading off expected tracking error against turnover and portfolio size. A case study on tracking the S&P~500 index is carried out to showcase how our tools shape the decision process from portfolio construction to rebalancing. |
| 2025-12-26 | [Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models](http://arxiv.org/abs/2512.22098v1) | Marco Dalla Pria, Matteo Ruggiero et al. | We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity. |
| 2025-12-26 | [Proton therapy range uncertainty reduction using vendor-agnostic tissue characterization on a virtual photon-counting CT head scan](http://arxiv.org/abs/2512.22026v1) | S. Vrba≈°ki, G. Staniƒá et al. | In this work, we proposed virtual imaging simulators as an alternative approach to experimental validation of beam range uncertainty in complex patient geometry using a computational model of a human head and a photon-counting CT scanner. We validate the accuracy of stopping power ratio (SPR) calculations using a conventional stoichiometric calibration approach and a prototype software, TissueXplorer. A validated CT simulator (DukeSim) was used to generate photon-counting CT projections of a computational head model, which were reconstructed with an open-source toolbox (ASTRA). The dose of 2 Gy was delivered through protons in a single fraction to target two different cases of nasal and brain tumors with a single lateral beam angle. Ground truth treatment plan was made directly on the computational head model using clinical treatment planning software (RayStation). This plan was then recalculated on the corresponding CT images for which SPR values were estimated using both the conventional method and the prototype software TissueXplorer. The mean percentage difference in estimating the stopping power ratio with TissueXplorer in all head tissues inside the scanned volume was 0.28%. Stopping power ratios obtained with this method showed smaller dose distribution differences from the ground truth plan than the conventional stoichiometric calibration method on the computational head model. Virtual imaging offers an alternative approach to validation of the SPR prediction from CT imaging, as well as its effect on the dose distribution and thus downstream clinical outcomes. According to this simulation study, software solutions that utilize spectral information, such as TissueXplorer, hold promise for more accurate prediction of the stopping power ratio than the conventional stoichiometric approach. |
| 2025-12-26 | [A Sieve-based Estimator for Entropic Optimal Transport](http://arxiv.org/abs/2512.21981v1) | Rami V. Tabri | The entropically regularized optimal transport problem between probability measures on compact Euclidean subsets can be represented as an information projection with moment inequality constraints. This allows its Fenchel dual to be approximated by a sequence of convex, finite-dimensional problems using sieve methods, enabling tractable estimation of the primal value and dual optimizers from samples. Assuming only continuity of the cost function, I establish almost sure consistency of these estimators. I derive a finite-sample convergence rate for the primal value estimator, showing logarithmic dependence on sieve complexity, and quantify uncertainty for the dual optimal value estimator via matching stochastic bounds involving suprema of centered Gaussian processes. These results provide the first statistical guarantees for sieve-based estimators of entropic optimal transport, extending beyond the empirical Sinkhorn approach. |
| 2025-12-26 | [Data relativistic uncertainty framework for low-illumination anime scenery image enhancement](http://arxiv.org/abs/2512.21944v1) | Yiquan Gao, John See | By contrast with the prevailing works of low-light enhancement in natural images and videos, this study copes with the low-illumination quality degradation in anime scenery images to bridge the domain gap. For such an underexplored enhancement task, we first curate images from various sources and construct an unpaired anime scenery dataset with diverse environments and illumination conditions to address the data scarcity. To exploit the power of uncertainty information inherent with the diverse illumination conditions, we propose a Data Relativistic Uncertainty (DRU) framework, motivated by the idea from Relativistic GAN. By analogy with the wave-particle duality of light, our framework interpretably defines and quantifies the illumination uncertainty of dark/bright samples, which is leveraged to dynamically adjust the objective functions to recalibrate the model learning under data uncertainty. Extensive experiments demonstrate the effectiveness of DRU framework by training several versions of EnlightenGANs, yielding superior perceptual and aesthetic qualities beyond the state-of-the-art methods that are incapable of learning from data uncertainty perspective. We hope our framework can expose a novel paradigm of data-centric learning for potential visual and language domains. Code is available. |
| 2025-12-26 | [TITAN DR1: An Improved, Validated, and Systematically-Controlled Recalibration of ATLAS Photometry toward Type Ia Supernova Cosmology](http://arxiv.org/abs/2512.21903v1) | Elijah G. Marlin, Yukei S. Murakami et al. | ATLAS (Asteroid Terrestrial Last Alert System) is a time-domain survey using four telescopes, covering the entire sky. It has observed over 10,000 spectroscopically confirmed Type Ia supernovae (SNe~Ia), with thousands of cosmology-grade light curves (to be released as TITAN DR1). To prepare this massive, low-redshift dataset for cosmology, we evaluate and cross-calibrate ATLAS forced photometry using tertiary stars from the DES (Dark Energy Survey) Y6 release. The 5000 deg$^2$ DES footprint overlaps regions both in and out of the PS1 (Pan-STARRS DR1) footprint, allowing tests of the primary calibrator for the ATLAS Refcat2 catalog. Initial offsets are at the $\sim$40 mmag scale. To improve this we determine $Œî$ zeropoint offsets for two cases: (1) pixel-to-pixel offsets within individual CCDs (reduced from $\sim$8 to $\sim$4 mmag RMS) and (2) chip-to-chip offsets across the 9 CCDs and filters (reduced from $\sim$17 to $\sim$3 mmag RMS). We also identify the largest systematic uncertainty as a transmission-function color dependence, requiring shifts in the assumed ATLAS filters at the $\sim$30 mmag level if uncorrected. We validate our calibration using (a) CALSPEC standards, (b) an independent tertiary catalog, and (c) distance moduli of cross-matched SNe~Ia, all showing improved consistency. Overall, we estimate combined calibration-related systematics at the $\sim$5--10 mmag level, supporting competitive cosmological constraints with the TITAN SN~Ia dataset. |
| 2025-12-26 | [The First X-ray Polarimetry of GRS 1739--278 Reveals Its Rapidly Spinning Black Hole](http://arxiv.org/abs/2512.21899v1) | Qing-Chang Zhao, Michal Dovciak et al. | We present a joint spectro-polarimetric analysis of the black hole X-ray binary GRS~1739--278 during its 2025 mini-outburst, using simultaneous observations from \ixpe\ and \nustar. The \ixpe\ data show a polarization degree of ${\rm PD} = (2.3 \pm 0.4)\%$ and a polarization angle of ${\rm PA} = 62^\circ \pm 5^\circ$ in the 2--8~keV range. The model-independent analysis reveals that the PD increases from $\sim 2\%$ at 2~keV to $\sim 10\%$ in the 6--8~keV band, while the PA remains stable across the \ixpe\ band within statistical uncertainties. Broadband spectral modeling of the combined \ixpe\ and \nustar\ datasets shows that hard Comptonization contributes negligibly in this soft-state observation, while a substantial reflected component is required in addition to the thermal disk emission. We then model the \ixpe\ Stokes spectra using the \texttt{kynbbrr} model. The best-fitting results indicate that high-spin configurations enhance the contribution of reflected returning radiation, which dominates the observed polarization properties. From the \texttt{kynbbrr} modeling, we infer an extreme black hole spin of $a = 0.994^{+0.004}_{-0.003}$ and a system inclination of $i = 54^\circ{}^{+8^\circ}_{-4^\circ}$. Owing to the large contribution from returning radiation, the observed polarization direction is nearly parallel to the projected system axis, the position angle of which is predicted to be $58^\circ \pm 4^\circ$. Our results demonstrate that X-ray polarimetry, combined with broadband spectroscopy, directly probes the geometry and relativistic effects in accretion disks around stellar-mass black holes. |
| 2025-12-26 | [Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space](http://arxiv.org/abs/2512.21887v1) | Weichen Zhang, Peizhi Tang et al. | Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments. |
| 2025-12-26 | [Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation](http://arxiv.org/abs/2512.21866v1) | Yiming Qian, Thorsten Neumann et al. | We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.   On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).   Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit. |
| 2025-12-26 | [Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models](http://arxiv.org/abs/2512.21815v1) | Mengqi He, Xinyu Tian et al. | Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms. |
| 2025-12-24 | [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](http://arxiv.org/abs/2512.21336v1) | Ziyu Chen, Xinbei Jiang et al. | Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions. |
| 2025-12-24 | [Autonomous Uncertainty Quantification for Computational Point-of-care Sensors](http://arxiv.org/abs/2512.21335v1) | Artem Goncharov, Rajesh Ghosh et al. | Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and inaccurate clinical decisions. To address this challenge, here we present an autonomous uncertainty quantification technique developed for POC diagnostics. As our testbed, we used a paper-based, computational vertical flow assay (xVFA) platform developed for rapid POC diagnosis of Lyme disease, the most prevalent tick-borne disease globally. The xVFA platform integrates a disposable paper-based assay, a handheld optical reader and a neural network-based inference algorithm, providing rapid and cost-effective Lyme disease diagnostics in under 20 min using only 20 uL of patient serum. By incorporating a Monte Carlo dropout (MCDO)-based uncertainty quantification approach into the diagnostics pipeline, we identified and excluded erroneous predictions with high uncertainty, significantly improving the sensitivity and reliability of the xVFA in an autonomous manner, without access to the ground truth diagnostic information of patients. Blinded testing using new patient samples demonstrated an increase in diagnostic sensitivity from 88.2% to 95.7%, indicating the effectiveness of MCDO-based uncertainty quantification in enhancing the robustness of neural network-driven computational POC sensing systems. |
| 2025-12-24 | [Fast SAM2 with Text-Driven Token Pruning](http://arxiv.org/abs/2512.21333v1) | Avilasha Mandal, Chaoning Zhang et al. | Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications. |
| 2025-12-24 | [Model-independent ZH production cross section at FCC-ee](http://arxiv.org/abs/2512.21290v1) | Ang Li, Jan Eysermans et al. | This paper presents prospects for a model-independent measurement of the $ZH$ production cross section at the FCC-ee using the recoil-mass technique at center-of-mass energies of $\sqrt{s}=240$ and $365$ GeV. The analysis considers the muon, electron, and hadronic decay modes of the associated $Z$ boson. Event selections rely primarily on the kinematic properties of the reconstructed $Z$ decay products, ensuring maximal independence from specific Higgs boson decay modes, while multivariate techniques are employed to further enhance sensitivity. Statistical interpretations of the individual final states yield relative precisions of $0.52\%$ for the combined leptonic channels and $0.38\%$ for the hadronic channel at $\sqrt{s}=240$ GeV with an integrated luminosity of $10.8$ ab$^{-1}$. Their full statistical combination leads to total uncertainties of $0.31\%$ at $\sqrt{s}=240$ GeV and $0.52\%$ at $\sqrt{s}=365$ GeV with $3.12$ ab$^{-1}$. Dedicated statistical tests demonstrate model independence at the level of the obtained precision. This study presents, for the first time, a consistent and combined analysis of the leptonic and hadronic final states, achieving the most precise projected determination of the $ZH$ production cross section at future lepton colliders, with model independence demonstrated within the statistical precision. |
| 2025-12-24 | [Impurity peaking of SPARC H-modes: a sensitivity study on physics and engineering assumptions](http://arxiv.org/abs/2512.21286v1) | Marco Muraca, Pablo Rodriguez-Fernandez et al. | In this paper, an overview of the impurity transport for three H-mode plasmas in the upcoming SPARC tokamak has been provided. The simulations have been performed within the ASTRA+STRAHL framework, using FACIT and TGLF-SAT2 to predict, respectively, neoclassical and turbulent core transport, while a neural network trained on EPED simulations has been employed to calculate the pedestal height and width self-consistently. A benchmark with previous simulations at constant impurity fraction has been provided for three H-modes, spanning different plasma current and magnetic field values. For a scenario, additional simulations have been performed to account for uncertainties in the modeling assumptions. The predictions are nearly insensitive to changes in the top of pedestal W concentrations. Varying the Ar pedestal concentration has shown a small effect on the impurity peaking and nearly constant fusion gain values, due to multiple effects on pedestal pressure, main ion dilution and density peaking. The inclusion of rotation in ASTRA simulations has shown minimal impact on confinement and impurity transport predictions. An exploratory study has been provided with a first set of simulations treating D and T separately, experiencing a maximum fusion power at 55-45% DT fuel composition, and an asymmetric distribution with respect to the D concentration. All the results, including sensitivity scans of toroidal velocity and ion temperature and density gradients, highlighted that turbulent impurity transport prevails on the neoclassical component, aligning with previous ITER predictions, and suggesting that next generation devices like SPARC, operating at low collisionality, will experience low W accumulation. |
| 2025-12-24 | [Uncertainty in security: managing cyber senescence](http://arxiv.org/abs/2512.21251v1) | Martijn Dekker | My main worry, and the core of my research, is that our cybersecurity ecosystem is slowly but surely aging and getting old and that aging is becoming an operational risk. This is happening not only because of growing complexity, but more importantly because of accumulation of controls and measures whose effectiveness are uncertain. I introduce a new term for this aging phenomenon: cyber senescence. I will begin my lecture with a short historical overview in which I sketch a development over time that led to this worry for the future of cybersecurity. It is this worry that determined my research agenda and its central theme of the role of uncertainty in cybersecurity. My worry is that waste is accumulating in cyberspace. This waste consists of a multitude of overlapping controls whose risk reductions are uncertain. Unless we start pruning these control frameworks, this waste accumulation causes aging of cyberspace and could ultimately lead to a system collapse. |
| 2025-12-24 | [LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation](http://arxiv.org/abs/2512.21243v1) | Anatoly O. Onishchenko, Alexey K. Kovalev et al. | Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io . |
| 2025-12-24 | [Assessing systematic uncertainties from spectral re-analysis of Cyg X-1 with different coronal geometries](http://arxiv.org/abs/2512.21230v1) | Abdurakhmon Nosirov, Jiachen Jiang et al. | In this work, we carry out a new spectral reanalysis of NuSTAR and Suzaku observations of the disk reflection spectra in the stellar-mass black hole X-ray binary Cyg~X-1. We compare three types of models: a broken power-law disk emissivity profile with no assumption about the coronal shape used in the previous work of the same observations, a compact lamppost corona, and an extended disk-like corona motivated by recent X-ray polarization results. Our goal is to measure the systematic uncertainties caused by the assumed geometry, with a focus on key parameters such as the black hole spin and the inclination of the inner accretion disk. We find that the disk-like corona gives a fit that is statistically similar to the broken power-law and lamppost models, but it leads to more physically reasonable results, such as a lower inclination angle of about $30^{\circ}$. By using a variable disk density model, we measure the disk density to be $n_{\rm e}\approx10^{20}$\,cm$^{-3}$, which is similar to earlier results. While the extended corona model infers a wider allowed parameter space for black hole spin and the inner radius of the disk-shaped coronal region, this reflects the additional physical freedom of the model. Even so, the disk-like corona remains a strong and physically well-motivated candidate for explaining the X-ray emission from Cyg~X-1. |
| 2025-12-24 | [Causal-driven attribution (CDA): Estimating channel influence without user-level data](http://arxiv.org/abs/2512.21211v1) | Georgios Filippou, Boi Mai Quach et al. | Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models. |
| 2025-12-24 | [Schr√∂dinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation](http://arxiv.org/abs/2512.21201v1) | Yu He, Da Huang et al. | Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \textbf{Schr√∂dinger's Navigator}, a navigation framework inspired by Schr√∂dinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schr√∂dinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation. |
| 2025-12-23 | [Active Intelligence in Video Avatars via Closed-loop World Modeling](http://arxiv.org/abs/2512.20615v1) | Xuanhua He, Tianyu Yang et al. | Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior. |
| 2025-12-23 | [LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving](http://arxiv.org/abs/2512.20563v1) | Long Nguyen, Micha Fauth et al. | Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead. |
| 2025-12-23 | [The role of charge in thermodynamic uncertainty relations](http://arxiv.org/abs/2512.20558v1) | David Christian Ohnmacht, Wolfgang Belzig et al. | We demonstrate that the charge value of transport mechanisms heavily impacts the validity of thermodynamic uncertainty relations (TURs). Specifically, we show within the framework of full counting statistics, that the recently established quantum TUR can be violated by the presence of transport processes that carry more than one charge, like Andreev reflection processes in normal metal-superconductor junctions. We propose a modified quantum TUR, which incorporates the charge value and demonstrate that this charge-dependent quantum TUR can only be violated if the highest charge transport process exceeds this charge value. In particular, we establish that the breaking of the quantum TUR solely originates from the charge value of the highest charge transport process. Namely, our analytical considerations do not invoke the existence of superconductivity, and these considerations generally hold for non-interacting electronic transport which can be described by the scattering formalism. |
| 2025-12-23 | [Precision spectroscopy of the 2S-$n$P transitions in atomic hydrogen](http://arxiv.org/abs/2512.20543v1) | Lothar Maisenbacher | Precision spectroscopy of atomic hydrogen is an important way to test bound-state quantum electrodynamics (QED), one of the building blocks of the Standard Model. In its simplest form, such a test consists of the comparison of a measured transition frequency with its QED prediction, which can be calculated with very high precision for the hydrogen atom. However, these calculations require some input in the form of physical constants, such as the Rydberg constant $R_\infty$ and the proton charge radius $r_\mathrm{p}$, both of which are currently determined to a large degree by hydrogen spectroscopy itself. Therefore, the frequency of at least three different transitions needs to be measured in order to test QED. Equivalently, a comparison of the values of $R_\infty$ and $r_\mathrm{p}$ determined from measurements of different transitions constitutes a test of QED.  To this end, laser spectroscopy of optical 2S-$n$P transitions has been performed in this work. As these transitions are one-photon transitions, they are affected by a different set of systematic effects than the two-photon transitions on which most other spectroscopic measurements of hydrogen are based. In order to contribute to the test of QED, their transition frequencies must be determined with a relative uncertainty on the order of one part in $10^{12}$, corresponding to approximately 1 kHz in absolute terms. This is in turn approximately a factor of 10000 smaller than the relatively broad natural linewidth of the 2S-$n$P transitions, and a successful measurement requires both a very large experimental signal-to-noise ratio and a detailed theoretical understanding of the line shape of the observed resonance.  The 2S-$n$P transitions were probed on a cryogenic beam of hydrogen atoms, which were optically excited to the metastable 2S level. The atomic beam was crossed at right angles with counter-propagating spectroscopy laser beams, which further excited the atoms to the $n$P level. The fluorescence from the subsequent rapid spontaneous decay served as experimental signal. The excitation with two counter-propagating beams led to two Doppler shifts of equal magnitude, but opposite sign, which thus canceled each other out. A velocity-resolved detection was used to determine any residual Doppler shifts, which could be excluded within the measurement uncertainty for both of the measurements discussed below.  In a first experiment, the 2S-4P transition was probed. Quantum interference of neighboring atomic resonances produced subtle distortions of the line shape, which were found to be significant because of the very large resolution relative to the linewidth. The line shifts caused by the distortions were directly observed and could be removed by use of a line shape model based on perturbative calculations. With this, the transition frequency was determined with a relative uncertainty of 4 parts in $10^{12}$. In combination with the very precisely measured 1S-2S transition frequency, this allowed the, at the time, most precise determination of $R_\infty$ and $r_\mathrm{p}$ from atomic hydrogen. Moreover, good agreement was found with the much more precise value of $r_\mathrm{p}$ extracted from spectroscopy of muonic hydrogen, which had been in significant disagreement with previous data from (electronic) hydrogen, causing concern about the validity of QED. This result has since been confirmed by other experiments. The 2S-4P measurement is treated in the appendix of this thesis.  The 2S-4P measurement, despite its large signal-to-noise ratio, was limited by counting statistics. To improve precision, a transition with a narrower linewidth and an improved experimental signal was necessary. Hence, the study of the 2S-6P transition, which offers a three times smaller natural linewidth, was begun. The atomic beam apparatus was upgraded, resulting in a corresponding decrease of the experimentally observed linewidth, and a close to an order of magnitude larger flux of atoms in the low-velocity tail of the atomic beam. Together with a detector redesign, this led to an up to 16 times larger signal than for the 2S-4P measurement, opening the path to increased precision. The Doppler-shift suppression was also rebuilt to support such precision, including a fiber collimator developed for this purpose, which provides high-quality spectroscopy beams at the new transition wavelength of 410 nm.  This enabled a measurement of the 2S-6P transition frequency with a statistical uncertainty of 430 Hz, five times lower than for the 2S-4P measurement and corresponding to a suppression of the Doppler shift by six orders of magnitude. At this level of precision, the light force shift from the diffraction of atoms at the light grating formed by the counter-propagating spectroscopy beams becomes significant. This light force shift was directly observed for the first time for the 2S-$n$P transitions and found to be well-described by a model derived for this purpose. The size of all other systematic effects, except the very precisely known recoil shift, is estimated to be below 500 Hz each. The blind data analysis is ongoing at the time of writing and thus no transition frequencies can yet be given. However, a preliminary analysis suggests a five-fold improvement in the determination of $R_\infty$ and $r_\mathrm{p}$ as compared to the 2S-4P measurement, and a two-fold improvement over the currently most precise determination from atomic hydrogen. This places the uncertainty of the determined value of $r_\mathrm{p}$ within a factor of five of that of the muonic value. The 2S-6P measurement is treated in the main text of this thesis. |
| 2025-12-23 | [Expected Revenue, Risk, and Grid Impact of Bitcoin Mining: A Decision-Theoretic Perspective](http://arxiv.org/abs/2512.20518v1) | Yuting Cai, Ruthav Sadali et al. | Most current assessments use ex post proxies that miss uncertainty and fail to consistently capture the rapid change in bitcoin mining. We introduce a unified, ex ante statistical model that derives expected return, downside risk, and upside potential profit from the first principles of mining: Each hash is a Bernoulli trial with a Bitcoin block difficulty-based success probability. The model yields closed-form expected revenue per hash-rate unit, risk metrics in different scenarios, and upside-profit probabilities for different fleet sizes. Empirical calibration closely matches previously reported observations, yielding a unified, faithful quantification across hardware, pools, and operating conditions. This foundation enables more reliable analysis of mining impacts and behavior. |
| 2025-12-23 | [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](http://arxiv.org/abs/2512.20457v1) | Marco Aruta, Francesco Improta et al. | In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario. |
| 2025-12-23 | [The Exact Uncertainty Relation and Geometric Speed Limits in Krylov Space](http://arxiv.org/abs/2512.20359v1) | Mohsen Alishahiha, Souvik Banerjee | We show that Hall's exact uncertainty relation acquires a simple geometric form in the Krylov basis generated by the Liouvillian. In this canonical operator frame, the uncertainty equality implies that the operator amplitude vector evolves on the unit Krylov sphere with constant speed fixed solely by the first Lanczos coefficient. This yields an exact linear bound on geometric operator evolution, independent of higher Lanczos coefficients and valid for arbitrary Hamiltonians, integrable or chaotic. Our results provide the first unified geometric interpretation of exact quantum speed limits and operator growth, identifying the first Lanczos coefficient as the intrinsic speed scale of quantum dynamics. |
| 2025-12-23 | [Status of the Muon g-2/EDM Experiment at J-PARC](http://arxiv.org/abs/2512.20335v1) | Graziano Venanzoni | The Muon g-2/EDM Experiment at J-PARC will employ a novel way to measure the muon magnetic anomaly, a_mu = (g-2)_mu/2, by using a low-emittance beam of positive muons stored in a compact muon storage magnet. The experimental method includes new technologies such as a three-dimensional spiral injection, an MRI-type storage magnet with superb field uniformity, and a positron tracking detector. The expected systematic uncertainty will be at the same level as that of the Fermilab Muon g-2 experiment, providing an important cross-check of the "storage-ring method" employed at BNL and Fermilab. I will present the current status of the experiment, ongoing tests and design optimizations, and the plans for improvements of the experimental precision. |
| 2025-12-23 | [UbiQVision: Quantifying Uncertainty in XAI for Image Recognition](http://arxiv.org/abs/2512.20288v1) | Akshat Dubey, Aleksandar An≈æel et al. | Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty. |
| 2025-12-23 | [Adaptive Multi-task Learning for Probabilistic Load Forecasting](http://arxiv.org/abs/2512.20232v1) | Onintze Zaballa, Ver√≥nica √Ålvarez et al. | Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment. |
| 2025-12-22 | [Picosecond laser test unit for photosensor characterization at ambient and low temperatures](http://arxiv.org/abs/2512.19667v1) | Matthias Raphael Stock, Hans Th. J. Steiger et al. | Accurate single photoelectron (SPE) characterization of photosensors is essential for controlling systematic uncertainties in low-light neutrino and dark matter detectors. We present a compact laboratory setup for the characterization of photosensors under controlled, low-light conditions. Specifically, we demonstrate its use with photomultiplier tubes (PMTs) operated at the SPE-level, using picosecond laser pulses and waveform digitization to determine key PMT properties. Measurements as a function of supply voltage and temperature ($-50^\circ$C to $+20^\circ$C) are performed on ET Enterprises 9821(Q)B tubes and a Hamamatsu R9980 assembly, which show exponential gain-voltage behavior and device-to-device variation. Cooling increases the gain by $\sim 0.1\,\%/^\circ$C, while the transit time spread (TTS) and peak-to-valley ratio (P/V) exhibit no clear temperature dependence. TTS decreases with voltage. Late pulses remain at the percent level and prepulses at the sub-percent level. Cable length affects both apparent gain and TTS. A model-independent, data-driven self-convolution method is introduced to quantify double photoelectron contributions from pulse charge spectra. The procedures provide a reproducible, practice-oriented reference for SPE-level PMT characterization and can be extended to other photosensor types. |
| 2025-12-22 | [Scenario Reduction for the Two-Stage Stochastic Unit Commitment Problem](http://arxiv.org/abs/2512.19614v1) | Yannick Werner, Juan Miguel Morales et al. | The two-stage stochastic unit commitment problem has become an important tool to support decision-making under uncertainty in power systems. Representing the uncertainty by a large number of scenarios guarantees accurate results but challenges the solution process. One way to overcome this is by using scenario reduction methods, which aim at finding a distribution supported on fewer scenarios, but leading to similar optimal first-stage decisions. In this paper, we recap the classical scenario reduction theory based on the distance of probability distributions and the optimal mass transportation problem. We then review and compare various formulations of the underlying cost function of the latter used in the literature. Using the Forward Selection Algorithm, we show that a specific formulation of the cost function can be proven to select the best possible scenario from a given sample on the first draw with respect to the Relative Approximation Error. We demonstrate this result and compare the quality of the approximation as well as the computational performance of the different cost functions using a modified version of the IEEE RTS 24-Bus System. In many cases, we find that the optimal solution of the two-stage stochastic unit commitment problem with 200 scenarios can be approximated with around 2% scenarios when using this cost function. |
| 2025-12-22 | [Neutron star crust and outer core equation of state from chiral effective field theory with quantified uncertainties](http://arxiv.org/abs/2512.19593v1) | H. G√∂ttling, L. Hoff et al. | We study the order-by-order expansion of the energy per particle of asymmetric nuclear matter up to twice saturation density in chiral effective field theory (EFT) within a Bayesian framework. For this, we develop a two-dimensional Gaussian process (2D GP) that is trained using many-body perturbation theory results based on chiral two- and three-nucleon interactions from leading to next-to-next-to-next-to-leading order (N$^3$LO). This allows for an efficient evaluation of the equation of state (EOS) and thermodynamic derivatives with EFT truncation uncertainties. After benchmarking our 2D GP against Bayesian uncertainties for pure neutron matter and symmetric matter, we study the energy per particle, pressure, and chemical potentials of neutron star matter in $Œ≤$-equilibrium including EFT uncertainties. We investigate the phase diagram of neutron-rich matter from neutron- to proton-drip and to the uniform phase, including surface and Coulomb corrections. Based on this, we construct EOSs for the inner crust of neutron stars that are consistent with the chiral EFT results for uniform matter at N$^3$LO. |
| 2025-12-22 | [Possibilistic Inferential Models for Post-Selection Inference in High-Dimensional Linear Regression](http://arxiv.org/abs/2512.19588v1) | Yaohui Lin | Valid uncertainty quantification after model selection remains challenging in high-dimensional linear regression, especially within the possibilistic inferential model (PIM) framework. We develop possibilistic inferential models for post-selection inference based on a regularized split possibilistic construction (RSPIM) that combines generic high-dimensional selectors with PIM validification through sample splitting. A first subsample is used to select a sparse model; ordinary least-squares refits on an independent inference subsample yield classical t/F pivots, which are then turned into consonant plausibility contours. In Gaussian linear models this leads to coor-dinatewise intervals with exact finite-sample strong validity conditional on the split and selected model, uniformly over all selectors that use only the selection data. We further analyze RSPIM in a sparse p >> n regime under high-level screening conditions, develop orthogonalized and bootstrap-based extensions for low-dimensional targets with high-dimensional nuisance, and study a maxitive multi-split aggregation that stabilizes inference across random splits while preserving strong validity. Simulations and a riboflavin gene-expression example show that calibrated RSPIM intervals are well behaved under both Gaussian and heteroskedastic errors and are competitive with state-of-the-art post-selection methods, while plausibility contours provide transparent diagnostics of post-selection uncertainty. |
| 2025-12-22 | [LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller](http://arxiv.org/abs/2512.19576v1) | Kirill Djebko, Tom Baumann et al. | Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit√§t W√ºrzburg in cooperation with the Technische Universit√§t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers. |
| 2025-12-22 | [Optimal Uncertainty Quantification under General Moment Constraints on Input Subdomains](http://arxiv.org/abs/2512.19572v1) | Rong Jin, Xingsheng Sun | We present an optimal uncertainty quantification (OUQ) framework for systems whose uncertain inputs are characterized by truncated moment constraints defined over subdomains. Based on this partial information, rigorous optimal upper and lower bounds on the probability of failure (PoF) are derived over the admissible set of probability measures, providing a principled basis for system safety certification. We formulate the OUQ problem under general subdomain moment constraints and develop a high-performance computational framework to compute the optimal bounds. This approach transforms the original infinite-dimensional optimization problems into finite-dimensional unconstrained ones parameterized solely by free canonical moments. To address the prohibitive cost of PoF evaluation in high-dimensional settings, we incorporate inverse transform sampling (ITS), enabling efficient and accurate PoF estimation within the OUQ optimization. We also demonstrate that constraining inputs only by zeroth-order moments over subdomains yields a formulation equivalent to evidence theory. Three groups of numerical examples demonstrate the framework's effectiveness and scalability. Results show that increasing the number of subdomains or the moment order systematically tightens the bound interval. For high-dimensional problems, the ITS strategy reduces computational costs by up to two orders of magnitude while maintaining relative error below 1%. Furthermore, we identify regimes where optimal bounds are sensitive to subdomain partitioning or higher-order moments, guiding uncertainty reduction efforts for safety certification. |
| 2025-12-22 | [Ultra-high precision high voltage system for PTOLEMY](http://arxiv.org/abs/2512.19437v1) | R. Ammendola, A. Apponi et al. | The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $Œ≤$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \SI{500}{meV} at the tritium $Œ≤$-spectrum endpoint of \SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \SI{0.2}{ppm} per \SI{1}{kV} single board and $\lesssim$ \SI{50}{mV} over the \SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \SI{0.05}{ppm} over \SI{20}{kV}. |
| 2025-12-22 | [On the new physics in Bhabha luminometry at future $e^+e^-$ colliders](http://arxiv.org/abs/2512.19423v1) | Clara L. Del Pio, Francesco P. Ucci | The absolute machine luminosity is a key quantity to achieve the high-precision physics program of future $e^+e^-$ collider. It is determined by measuring a theoretically well-known process, which, ideally, can be computed with arbitrary precision in the perturbation theory. However, yet undiscovered new physics could give a non-negligible contribution to the cross section of the luminosity monitoring process, thus invalidating the uncertainty determination of measured quantities. We assess the theoretical error of non-Standard Model origin to the small-angle Bhabha scattering in various future colliders scenarios. In addition, a possible running strategy to constrain unknown heavy interactions is proposed, relying on asymmetries that do not depend on the absolute luminosity. |
| 2025-12-22 | [Power feedback strategy based on efficiency trajectory analysis for HCPV sun tracking](http://arxiv.org/abs/2512.19394v1) | Manuel G. Satu√©, Fernando Casta√±o et al. | This paper presents a control strategy for sun trackers which adapts continuously to different sources of error, avoiding the necessity of any kind of calibration by analyzing the produced electric power to sense the position of the Sun. The proposed strategy is able to meet the strict specifications for HCPV sun trackers despite of mechanical uncertainties (misalignments in the structure itself, misalignment of the solar modules with respect to the wing, etc.) and installation uncertainties (misalignments of the platform with respect to geographical north). Experimental results with an industrial-grade solar tracker showing the validity of the proposed control strategy under sunny and moderate cloudy conditions, as well as with different installation precisions by un-calibrating the system on purpose are exposed. |
| 2025-12-22 | [DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition](http://arxiv.org/abs/2512.19387v1) | Yueyao Chen, Kai-Ni Wang et al. | Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.   Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.   Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.   Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability. |
| 2025-12-19 | [Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy](http://arxiv.org/abs/2512.17899v1) | Aditya Gahlawat, Ahmed Aboudonia et al. | Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach. |
| 2025-12-19 | [The role of charm and unflavored mesons in prompt atmospheric lepton fluxes](http://arxiv.org/abs/2512.17886v1) | Laksha Pradip Das, Diksha Garg et al. | The all-sky very-high-energy ($10^4-10^6$ GeV) atmospheric muon flux measured by IceCube shows a spectral hardening at the highest energies, indicating the presence of a prompt component. IceCube has also measured the atmospheric muon neutrino flux at high energy. However, since this flux is dominated by astrophysical neutrinos, only an upper bound can be placed on the prompt atmospheric $ŒΩ_Œº+\barŒΩ_Œº$ contribution. In this work, we provide a new evaluation of the prompt atmospheric muon flux including an intrinsic charm component in the cosmic ray-air interactions. The latter enhances the forward production of $\bar{D}^0$, $D^-$, and $Œõ_c$, which subsequently decay into final states containing muons and muon neutrinos. We show how the increase in the prompt muon flux due to intrinsic charm is accompanied by a corresponding enhancement in the prompt muon neutrino flux. We implement different intrinsic charm production models in MCEq to calculate the resulting lepton fluxes. We discuss the challenges of achieving predictions that are simultaneously consistent with both IceCube's high-energy atmospheric muon flux measurements and IceCube upper bound on the prompt muon neutrino flux, and we quantify the resulting discrepancies. As possible solutions, we explore scaling of the unflavored meson contributions to the prompt atmospheric muon flux to assess how such adjustments can reconcile these differences. The tensions emphasized in our work call for a refinement of the hadronic interaction models, especially the production of unflavored mesons, and for new experimental data sensitive to unflavored meson and heavy flavor production with reliable estimates of the associated uncertainties. We suggest that the energy and zenith angle dependence of muon and neutrino flux ratios from future neutrino telescope measurements may help to disentangle different scenarios. |
| 2025-12-19 | [Bayesian Methods for the Investigation of Temperature-Dependence in Conductivity](http://arxiv.org/abs/2512.17792v1) | Andrew R. McCluskey, Samuel W. Coles et al. | Temperature-dependent transport data, including diffusion coefficients and ionic conductivities, are routinely analysed by fitting empirical models such as the Arrhenius equation. These fitted models yield parameters such as the activation energy, and can be used to extrapolate to temperatures outside the measured range. Researchers frequently face challenges in this analysis: quantifying the uncertainty of fitted parameters, assessing whether the data quality is sufficient to support a particular empirical model, and using these models to predict behaviour at extrapolated temperatures. Bayesian methods offer a coherent framework that addresses all of these challenges. This tutorial introduces the use of Bayesian methods for analysing temperature-dependent transport data, covering parameter estimation, model selection, and extrapolation with uncertainty propagation, with illustrative examples from molecular dynamics simulations of superionic materials. |
| 2025-12-19 | [Constraining the Prompt Atmospheric Neutrino Flux Combining IceCube's Cascade and Track Samples](http://arxiv.org/abs/2512.17760v1) | R. Abbasi, M. Ackermann et al. | The IceCube Neutrino Observatory has observed a diffuse flux of high-energy astrophysical neutrinos for more than a decade. A relevant background to the astrophysical flux is prompt atmospheric neutrinos, originating from the decay of charmed mesons produced in cosmic-ray-induced air showers. The production rate of charmed mesons in the very forward phase space of hadronic interactions, and consequently, the prompt neutrino flux, remains uncertain and has not yet been observed by neutrino detectors. An accurate measurement of this flux would enhance our understanding of fundamental particle physics such as hadronic interactions in high-energy cosmic-ray-induced air showers and the nucleon structure. Furthermore, an experimental characterization of this background flux will improve the precision of astrophysical neutrino flux spectral measurements. In this work, we perform a combined fit of cascade-like and track-like neutrino events in IceCube to constrain the prompt atmospheric neutrino flux. Given that the prompt flux is a sub-dominant contribution, treating systematic uncertainties arising from the potential mis-modeling of the conventional and astrophysical neutrino fluxes is critical for its measurement. Our analysis yields a non-zero best-fit result, which is, however, consistent with the null hypothesis of no prompt flux within one standard deviation. Consequently, we establish an upper bound on the flux at $4\times 10^{-16}$ (GeV m$^2$ s sr)$^{-1}$ at 10 TeV. |
| 2025-12-19 | [Perceptions of the Metaverse at the Peak of the Hype Cycle: A Cross-Sectional Study Among Turkish University Students](http://arxiv.org/abs/2512.17750v1) | Mehmet Ali Erkan, Halil Eren Ko√ßak | During the height of the hype in late 2021, the Metaverse drew more attention from around the world than ever before. It promised new ways to interact with people in three-dimensional digital spaces. This cross-sectional study investigates the attitudes, perceptions, and predictors of the willingness to engage with the Metaverse among 381 Turkish university students surveyed in December 2021. The study employs Fisher's Exact Tests and binary logistic regression to assess the influence of demographic characteristics, prior digital experience, and perception-based factors. The results demonstrate that demographic factors, such as gender, educational attainment, faculty association, social media engagement, and previous virtual reality exposure, do not significantly forecast the propensity to participate in the Metaverse. Instead, the main things that affect people's intentions to adopt are how they see things. Belief in the Metaverse's capacity to revolutionize societal frameworks, especially human rights, surfaced as the most significant positive predictor of willingness. Conversely, apprehensions regarding psychological harm, framed as a possible 'cyber syndrome' represented a significant obstacle to participation. Perceptions of technical compatibility and ethical considerations showed complex effects, showing that optimism, uncertainty, and indifference affect willingness in different ways. In general, the results show that early adoption of the Metaverse is based on how people see it, not on their demographics. The research establishes a historically informed benchmark of user skepticism and prudent assessment during the advent of Web 3.0, underscoring the necessity of addressing collective psychological, ethical, and normative issues to promote future engagement. |
| 2025-12-19 | [Gravity Prior and Temporal Horizon Shape Interceptive Behavior under Active Inference](http://arxiv.org/abs/2512.17735v1) | Marta Russo, Antonella Maselli et al. | Accurate interception of moving objects, such as catching a ball, requires the nervous system to overcome sensory delays, noise, and environmental dynamics. One key challenge is predicting future object motion in the presence of sensory uncertainty and inherent neural processing latencies. Theoretical frameworks such as internal models and optimal control have emphasized the role of predictive mechanisms in motor behavior. Active Inference extends these ideas by positing that perception and action arise from minimizing variational free energy under a generative model of the world. In this study, we investigate how different predictive strategies and the inclusion of environmental dynamics, specifically an internal model of gravity, influence interceptive control within an Active Inference agent. We simulate a simplified ball-catching task in which the agent moves a cursor horizontally to intercept a parabolically falling object. Four strategies are compared: short temporal horizon prediction of the next position or long horizon estimation of the interception point, each with or without a gravity prior. Performance is evaluated across diverse initial conditions using spatial and temporal error, action magnitude, and movement corrections. All strategies produce successful interception behavior, but those that incorporate gravity and longer temporal horizons outperform others. Including a gravity prior significantly improves spatial and temporal accuracy. Predicting the future interception point yields lower action values and smoother trajectories compared to short-horizon prediction. These findings suggest that internal models of physical dynamics and extended predictive horizons can enhance interceptive control, providing a unified computational account of how the brain may integrate sensory uncertainty, physical expectations, and motor planning. |
| 2025-12-19 | [Reconstructing Pre-Satellite Tropical Cyclogenesis Climatology Using Deep Learning](http://arxiv.org/abs/2512.17711v1) | Chanh Kieu, Thanh T. N. Nguyen et al. | A reliable tropical cyclone (TC) climatology is the key to assessing historical and future changes in TC activities. While global TC records have been systematically maintained since the early 1940s, substantial uncertainties remain for the pre-satellite era during which TC observations relied mostly on scattered aircraft reconnaissance and sporadic ship reports. This study presents a deep learning (DL) approach to reconstruct historical TC activity in the western North Pacific (WNP) basin, with a main focus on the pre-satellite era. Using data feature enrichment tailored for tropical cyclogenesis (TCG), we demonstrate that DL can effectively capture the main characteristics and changes in TCG climatology during the post-satellite era. With additional cross-validations, the reconstruction of TCG climatology is then extended to a pre-satellite period (1940-1960) during which TC base-track datasets are most uncertain. Our DL reconstruction reveals a significant missing of TCG in the current best-track data between September and November during the pre-satellite era. Such a TCG undercount in the best track data occurs mainly around 10-15$^\circ$N in the central WNP, while coastal regions show better consistency with DL reconstruction. These findings not only highlight the potential of DL for improving historical assessments of TC activity, but also advance our understanding of TCG processes by identifying key environmental conditions conducive to TC formation. The DL approach presented herein can be applied to other ocean basins, climate proxies, or reanalysis datasets for future TC climate studies. |
| 2025-12-19 | [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](http://arxiv.org/abs/2512.17696v1) | Yuri Calleo | The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning. |
| 2025-12-19 | [Imputation Uncertainty in Interpretable Machine Learning Methods](http://arxiv.org/abs/2512.17689v1) | Pegah Golchian, Marvin N. Wright | In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage. |
| 2025-12-19 | [Reduced basis emulator for elastic scattering in continuum-discretized coupled-channel calculations](http://arxiv.org/abs/2512.17687v1) | Jin Lei | I develop a reduced basis emulator for continuum-discretized coupled-channel (CDCC) calculations that achieves speedups of $\sim 10^2$ while maintaining sub-percent accuracy. The emulator is constructed using the proper orthogonal decomposition (POD) method applied to snapshots of CDCC solutions computed at sampled points in the optical potential parameter space. The prediction is performed via Galerkin projection onto the reduced basis. I demonstrate the method using deuteron scattering on $^{58}$Ni at 21.6 MeV as a test case, emulating 18 optical potential parameters simultaneously. The emulator reproduces elastic scattering cross sections with errors below 0.1\% across a wide parameter range. This development enables efficient uncertainty quantification and Bayesian parameter estimation for nuclear reaction calculations that were previously computationally prohibitive. |
| 2025-12-18 | [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](http://arxiv.org/abs/2512.16909v1) | Yuanchen Ju, Yongyuan Liang et al. | Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments. |
| 2025-12-18 | [The Fourier Ratio: Uncertainty, Restriction, and Approximation for Compactly Supported Measures](http://arxiv.org/abs/2512.16751v1) | A. Iosevich, Z. Li et al. | We introduce a continuous analog of the Fourier ratio for compactly supported Borel measures. For a measure \(Œº\) on \(\mathbb{R}^d\) and \(f\in L^2(Œº)\), the Fourier ratio compares \(L^1\) and \(L^2\) norms of a regularized Fourier transform at scale \(R\). We develop a fractal uncertainty principle giving sharp two-sided bounds in terms of covering numbers of spatial and frequency supports, with applications to exact signal recovery. We show that small Fourier ratio implies efficient approximation by low-degree trigonometric polynomials in \(L^1\), \(L^2\), and \(L^\infty\). In contrast, restriction estimates reveal a sharp gap between curved measures and random fractal measures, yielding strong lower bounds on approximation degree. Applications to convex surface measures are also obtained. |
| 2025-12-18 | [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](http://arxiv.org/abs/2512.16740v1) | Yunkai Yang, Yudong Zhang et al. | With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation. |
| 2025-12-18 | [A faint M$_{\rm UV} = -14.5$ Lyman continuum leaker in the reionization epoch: unprecedented Ly$Œ±$ properties at z=5.725](http://arxiv.org/abs/2512.16728v1) | M. Messa, E. Vanzella et al. | We report the unprecedented Ly$Œ±$ properties of AMORE6, an extremely metal-poor ($12+\log({\rm O/H}) < 6$), low-mass ($M_\star = 4.4\times10^{5}\,M_\odot$), and ultra-compact (effective radius $\lesssim30$ pc) dwarf galaxy at $z=5.7253$, gravitationally lensed by the cluster A2744. A prominent, narrow, and nearly-symmetric Ly$Œ±$ emission line is detected at the systemic redshift (the latter traced by H$Œ≤$, from JWST/NIRCam slitless spectroscopy), with rest-frame $EW=150\pm10$ √Ö, $\rm FWHM=58\pm1$ km s$^{-1}$, and a slight asymmetry, resulting in a $\rm \sim10\%$ flux excess in the red wing of the line. The negligible velocity offset from systemic ($dv = 4\pm67$ km s$^{-1}$, $3œÉ$ uncertainty), together with the sharpness and symmetry of the profile, indicates minimal radiative transfer effects implying a neutral hydrogen column density consistent with an optically thin medium, compatible with a non-zero ionizing photon escape fraction. If indirect spectral diagnostics calibrated at $z<4.5$ remain the only viable tools to identify LyC leakers during reionization, then based on its strongest indicator (Ly$Œ±$), AMORE6 stands out as one of the most compelling LyC-leaking candidates yet discovered in the reionization epoch. |
| 2025-12-18 | [KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling](http://arxiv.org/abs/2512.16723v1) | Lei Wang, Xin Tan et al. | Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\% accuracy while baselines drop below 20\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design. |
| 2025-12-18 | [Computing Lower and Upper Hitting Probabilities for Imprecise Markov Chains](http://arxiv.org/abs/2512.16696v1) | Marco Sangalli, Erik Quaeghebeur et al. | We study the computation of lower and upper probabilities of hitting a target set of states for imprecise Markov chains. For these, transition uncertainty is modelled by a convex set of transition matrices. In the precise case, hitting probabilities are the minimal nonnegative solution of a linear system and admit a closed-form expression. We study the notion of reachability in the imprecise setting. The literature review highlights few different definitions of lower reachability; thus we explore the relations among them, presenting examples to clarify their logical implications. Using this revised definition of reachability for imprecise Markov chain, we partition the state space into classes of states whose hitting probabilities are trivially zero or one and those which require further computation. For these nontrivial states, we show that lower and upper hitting probabilities are the unique solutions of two nonlinear fixed-point equations. For the practical computation of lower and upper hitting probabilities, we propose iterative algorithms that alternate between solving a linear system and choosing an extreme point from the set of transition matrices. Numerical experiments demonstrate that, in practice, these algorithms converge in substantially fewer iterations than the theoretically established worst-case bound. |
| 2025-12-18 | [Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents](http://arxiv.org/abs/2512.16614v1) | Giulia Boato, Andrea Montibeller et al. | AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process. |
| 2025-12-18 | [Resilience of coupled systems under deep uncertainty and dynamic complexity: An integrative literature review](http://arxiv.org/abs/2512.16608v1) | Jannie Coenen, V√≠tor Vasconcelos et al. | Resilience in coupled systems is increasingly critical in addressing global challenges such as climate change and pandemics. These systems show unpredictable behaviour due to dynamic complexity and deep uncertainty across spatiotemporal scales. Despite growing interest, few studies systematically integrate both concepts when assessing resilience. This paper conducts an integrative review of 102 English-language publications to identify gaps in current approaches. Findings reveal that most papers address lower levels of uncertainty and rarely consider dynamic complexity and deep uncertainty simultaneously, which limits the effectiveness of resilience strategies. To advance systems research, we propose a conceptual framework and practical tools to support researchers and decision-makers in evaluating and improving resilience. The paper also outlines future research directions for more robust, adaptive, and integrative resilience assessments. |
| 2025-12-18 | [Current-Induced Modulation of Spin-Wave Propagation in a Y-Junction via Transverse Spin-Transfer Torque](http://arxiv.org/abs/2512.16563v1) | Lorenzo Gnoatto, Rai M. Menezes et al. | We report the transverse control of spin-wave propagation in the configuration where the spin-wave wavevector k is perpendicular to the charge-current density J. Building on theoretical predictions of spin-wave refraction by nonuniform spin-polarized currents, and guided by micromagnetic simulations used to optimize the device geometry and current distribution, we experimentally explore a Y-shaped Permalloy structure in which a locally injected current perturbs the spin-wave dispersion. Measurements reveal current-dependent amplitude differences between the two output branches, providing initial experimental indications consistent with transverse, spin-transfer-torque-driven deflection. Although the magnitude of the effect is modest and accompanied by significant uncertainties, the observed trends qualitatively follow expectations from the simulations. These results demonstrate the feasibility of influencing spin-wave routing through local current injection and establish a proof-of-concept basis for current-controlled manipulation of spin-wave propagation in reconfigurable magnonic circuits. |
| 2025-12-18 | [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](http://arxiv.org/abs/2512.16494v1) | Mengyuan Liu, Jiajie Liu et al. | The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW. |
| 2025-12-17 | [When sufficiency is insufficient: the functional information bottleneck for identifying probabilistic neural representations](http://arxiv.org/abs/2512.15671v1) | Ishan Kalburge, M√°t√© Lengyel | The neural basis of probabilistic computations remains elusive, even amidst growing evidence that humans and other animals track their uncertainty. Recent work has proposed that probabilistic representations arise naturally in task-optimized neural networks trained without explicitly probabilistic inductive biases. However, prior work has lacked clear criteria for distinguishing probabilistic representations, those that perform transformations characteristic of probabilistic computation, from heuristic neural codes that merely reformat inputs. We propose a novel information bottleneck framework, the functional information bottleneck (fIB), that crucially evaluates a neural representation based not only on its statistical sufficiency but also on its minimality, allowing us to disambiguate heuristic from probabilistic coding. To demonstrate the power of this framework, we study a variety of task-optimized neural networks that had been suggested to develop probabilistic representations in earlier work: networks trained to perform static inference tasks (such as cue combination and coordinate transformation) or dynamic state estimation tasks (Kalman filtering). In contrast to earlier claims, our minimality requirement reveals that probabilistic representations fail to emerge in these networks: they do not develop minimal codes of Bayesian posteriors in their hidden layer activities, and instead rely on heuristic input recoding. Therefore, it remains an open question under which conditions truly probabilistic representations emerge in neural networks. More generally, our work provides a stringent framework for identifying probabilistic neural codes. Thus, it lays the foundation for systematically examining whether, how, and which posteriors are represented in neural circuits during complex decision-making tasks. |
| 2025-12-17 | [A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition](http://arxiv.org/abs/2512.15650v1) | Stephen Tivenan, Indranil Sahoo et al. | Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period. |
| 2025-12-17 | [Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation](http://arxiv.org/abs/2512.15643v1) | Jairo F√∫quene-Pati√±o | In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study. |
| 2025-12-17 | [Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error](http://arxiv.org/abs/2512.15624v1) | Akash Yadav, Ruda Zhang | Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics. |
| 2025-12-17 | [A Decision-Theoretic Approach for Managing Misalignment](http://arxiv.org/abs/2512.15584v1) | Daniel A. Herrmann, Abinav Chari et al. | When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty. |
| 2025-12-17 | [Extensive Observational Evidence for Massive Star Stellar Wind Variability at Low Metallicities: implications for mass-loss rate determination](http://arxiv.org/abs/2512.15539v1) | Timothy N. Parsons, Raman K. Prinja et al. | Mass-loss from massive stars is fundamental to stellar and galactic evolution and enrichment of the interstellar medium. Reliable determination of mass-loss rate is dependent upon unravelling details of massive star outflows, including optical depth structure of the stellar wind. That parameter introduces significant uncertainty due to the nearly ubiquitous presence of large-scale optically thick wind structure. We utilize suitable available ultraviolet spectra of 20 Large and Small Magellanic Cloud (LMC, SMC) OB stars to extend existing Galactic results quantifying uncertainty inherent in individual observations to lower metallicity environments. This is achieved by measuring standard deviations of mean optical depths of multiple observations of suitable wind-formed absorption profiles as a proportion of their mean optical depths. We confirm earlier findings that wind structure is prevalent at low metallicities and demonstrate that quantifying the consequent uncertainty is to some extent possible, despite the near-complete absence of time series UV spectroscopic observations in those environments. We find that the uncertainty inherent in any single observation of stellar wind optical depth at low metallicity is of similar magnitude to that already identified at Galactic metallicity (up to 45% for cooler OB stars). We further demonstrate how the effect of varying narrow absorption components in wind-formed UV spectral profiles is unlikely to be properly accounted for in existing mass-loss models. We present further evidence of a binary companion to the SMC O-type giant star AzV 75. The importance of obtaining high cadence multi-epoch, or genuine time series, UV spectroscopic observations at low metallicities is highlighted. |
| 2025-12-17 | [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](http://arxiv.org/abs/2512.15492v1) | Adri√°n Detavernier, Jasper De Bock | We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability. |
| 2025-12-17 | [Energy Inference of Black-Box Quantum Computers Using Quantum Speed Limit](http://arxiv.org/abs/2512.15472v1) | Nobumasa Ishida, Yoshihiko Hasegawa | Cloud-based quantum computers do not provide users with access to hardware-level information such as the underlying Hamiltonians, which obstructs the characterization of their physical properties. We propose a method to infer the energy scales of gate Hamiltonians in such black-box quantum processors using only user-accessible data, by exploiting quantum speed limits. Specifically, we reinterpret the Margolus-Levitin and Mandelstam-Tamm bounds as estimators of the energy expectation value and variance, respectively, and relate them to the shortest time for the processor to orthogonalize a quantum state. This shortest gate time, expected to lie on the nanosecond scale, is inferred from job execution times measured in seconds by employing gate-time amplification. We apply the method to IBM's superconducting quantum processor and estimate the energy scales associated with single-, two-, and three-qubit gates. The order of estimated energy is consistent with typical drive energies in superconducting qubit systems, suggesting that current gate operations approach the quantum speed limit. Our results demonstrate that fundamental energetic properties of black-box quantum computers can be quantitatively accessed through operational time measurements, reflecting the conjugate relationship between time and energy imposed by the uncertainty principle. |
| 2025-12-17 | [Robustness Measures for Stochastic Parallel Machine Scheduling and Train Unit Shunting](http://arxiv.org/abs/2512.15471v1) | Casper Loman, Loriana Pascual et al. | In many real world scheduling problems, the processing times of tasks are subject to uncertainty. This makes it essential to design schedules that are robust and able to handle potential disruptions. Therefore, we investigate measures that give us information about the robustness of a schedule. Although many measures can be found in literature, there is no consensus on which measures are the best. We identify 14 robustness measures from the literature, as well as introduce 4 new ones. To find out which of these measures are best used for generating robust schedules, we perform an elaborate simulation study to investigate how well these robustness measures correlate with the stability of the objective function under disturbances (quality robustness) and with the stability of the schedule itself (solution robustness). We first consider the Stochastic Parallel Machine Scheduling Problem (SPMSP) with precedence constraints, which is a very general setting that is relevant for many practical situations. We then perform a second simulation study by taking the best performing measures from the first experiment, and using them for the Train Unit Shunting Problem with Service Scheduling (TUSPwSS). After establishing the correlation with quality and solution robustness, we included the measures as objective in a local search algorithm. We make a comparison between the theoretical setting of the SPMSP and the TUSPwSS, and identify a set of robustness measures that can be applied in many different settings. We show that we can achieve up to 90% decreases in delays compared to using no robustness measures. Lastly, we also identify properties that can be used to predict the effectiveness of such a robustness measure. |
| 2025-12-17 | [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](http://arxiv.org/abs/2512.15405v1) | Jianfei Ma, Wee Sun Lee | At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency. |
| 2025-12-16 | [Drell-Yan at the Electron-Ion Collider](http://arxiv.org/abs/2512.14690v1) | Henry T. Klest | The photon is arguably the most universally important particle across all fields of physics. Despite its status as a fundamental particle, at high energies the photon can be seen as a hadronic source of partons. The partonic content of the photon is very poorly constrained compared to that of the proton, with photon PDF uncertainties typically one or two orders of magnitude larger than their proton counterparts, despite the fact that its source, the $Œ≥\to q\bar{q}$ splitting, is perturbatively calculable. The high luminosity, excellent particle identification, and far-backward electron tagging capabilities of the Electron-Ion Collider make it an ideal environment for studying photon parton distribution functions. Similar to the $p+p$ or $œÄ+p$ systems, photoproduction at the EIC can be thought of as two parton distributions colliding. One of the most powerful processes in such collisions is production of lepton pairs, i.e. $h+p\rightarrow l^+l^-+X$, known as the Drell--Yan process. This process has the ability to access for the first time the transverse-momentum-dependent parton distributions of the photon. The transversely polarized proton beam of the EIC additionally provides a possible means of accessing the transversity distribution of the proton without relying on fragmentation functions. |
| 2025-12-16 | [Analysis and Uncertainty Quantification of Thermal Transport Measurements through Bayesian Parameter Estimation](http://arxiv.org/abs/2512.14659v1) | Jeremy Drew, Shravan Godse et al. | The thermal transport community is increasingly interested in rigorous uncertainty quantification (UQ) of their measurements. In this work, we argue that Bayesian parameter estimation (BPE) represents a powerful framework for both analysis/fitting and UQ. We provide a detailed walkthrough of the technique (including code to duplicate our results) and example analysis based on measuring the thermal conductance of a gold/sapphire interface with FDTR. Comparisons are made against traditional analysis/UQ techniques adopted by the thermal transport community. Notable advantages of BPE include the interpretability of its results, including the capacity to indicate incorrect input assumptions, as well as a way to balance overall goodness of fit against prior knowledge of feasible parameter values. In some cases, incorporating this additional information can affect not only the magnitude of error bars but the inferred values themselves. |
| 2025-12-16 | [Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty](http://arxiv.org/abs/2512.14653v1) | Yiwen Zhao, Jiatong Shi et al. | Singing voice synthesis (SVS) has seen remarkable advancements in recent years. However, compared to speech and general audio data, publicly available singing datasets remain limited. In practice, this data scarcity often leads to performance degradation in long-tail scenarios, such as imbalanced pitch distributions or rare singing styles. To mitigate these challenges, we propose uncertainty-based optimization to improve the training process of end-to-end SVS models. First, we introduce differentiable data augmentation in the adversarial training, which operates in a sample-wise manner to increase the prior uncertainty. Second, we incorporate a frame-level uncertainty prediction module that estimates the posterior uncertainty, enabling the model to allocate more learning capacity to low-confidence segments. Empirical results on the Opencpop and Ofuton-P, across Chinese and Japanese, demonstrate that our approach improves performance in various perspectives. |
| 2025-12-16 | [kSZ for everyone: the pseudo-Cl approach to stacking](http://arxiv.org/abs/2512.14625v1) | Lea Harscouet, Kevin Wolz et al. | We present a harmonic-space estimator for the cross-correlation between the kinematic Sunyaev-Zel'dovich effect and the reconstructed galaxy momentum field that offers several practical advantages over the traditional stacking approach. The estimator is easy to deploy using relatively modest computational resources and recovers all information available in the galaxy-kSZ cross-correlation. In particular, by using well-understood power spectrum estimation techniques, its statistical uncertainties, including potential correlated uncertainties with other large-scale structure observables, can be easily and accurately estimated. Moreover, standard kSZ stacking measurements can be reconstructed exactly from the estimator at a lower computational cost, employing harmonic-space, catalog-level techniques to recover all small-scale information. |
| 2025-12-16 | [Charged Higgs Pairs at the LHC: A NLO Analysis](http://arxiv.org/abs/2512.14587v1) | Mohamed Ahmed, Lisa Biermann et al. | Charged Higgs-boson pair production at hadron colliders yields information about the trilinear couplings involving charged Higgs fields in extensions of the Standard Model (SM). We consider the two-Higgs doublet model (2HDM) extension and obtain next-to-leading order QCD predictions for the charged-Higgs pair production ($H^+H^-$ production). All production modes, i.e. Drell--Yan-like production, gluon fusion and vector-boson fusion are included in the analysis. We determine uncertainties originating from the scale dependence, the parton-density functions and strong coupling $Œ±_s$ at the LHC. We observe that the QCD corrections lead to a significant reduction of the relative scale dependences. These improved predictions will allow for a quantitative interpretation of experimental measurements, in case that charged Higgs states will be observed. |
| 2025-12-16 | [Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error](http://arxiv.org/abs/2512.14492v1) | Martin Slawski | The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach. |
| 2025-12-16 | [Precise Predictions for $Œº^{\pm}e^-\rightarrowŒº^{\pm}e^-$ at the MUonE Experiment](http://arxiv.org/abs/2512.14431v1) | Alan Price | The proposed fixed-target experiment, MUonE, at CERN will aim to measure the hadronic contribution to the running of the QED coupling by analysing the scattering of muons on electrons. Here we present state-of-the-art predictions for the process $Œº^{\pm}e^-\rightarrowŒº^{\pm}e^-$, where for the first time an all-order resummation of soft and soft-collinear logarithms has been performed. Further, we match this resummation with the complete next-to-leading and the dominant next-to-next-to-leading higher-order corrections. We find that the resummation has a dominant effect in the signal region, while the systematic matching significantly reduces the perturbative uncertainty. |
| 2025-12-16 | [Towards Real Time Control of Water Engineering with Nonlinear Hyperbolic Partial Differential Equations](http://arxiv.org/abs/2512.14387v1) | Fabio DiFonzo, Michael Holst et al. | This paper examines aspirational requirements for software addressing mixed-integer optimization problems constrained by the nonlinear Shallow Water partial differential equations (PDEs), motivated by applications such as river-flow management in hydropower cascades. Realistic deployment of such software would require the simultaneous treatment of nonlinear and potentially non-smooth PDE dynamics, limited theoretical guarantees on the existence and regularity of control-to-state mappings under varying boundary conditions, and computational performance compatible with operational decision-making. In addition, practical settings motivate consideration of uncertainty arising from forecasts of demand, inflows, and environmental conditions. At present, the theoretical foundations, numerical optimization methods, and large-scale scientific computing tools required to address these challenges in a unified and tractable manner remain the subject of ongoing research across the associated research communities. Rather than proposing a complete solution, this work uses the problem as a case study to identify and organize the mathematical, algorithmic, and computational components that would be necessary for its realization. The resulting framework highlights open challenges and intermediate research directions, and may inform both more circumscribed related problems and the design of future large-scale collaborative efforts aimed at addressing such objectives. |
| 2025-12-16 | [Measurements of the branching fractions of $œá_{cJ}\to œÜœÜŒ∑, œÜœÜŒ∑^{\prime}$ and $œÜK^+K^-Œ∑$](http://arxiv.org/abs/2512.14369v1) | BESIII Collaboration, M. Ablikim et al. | Using a sample of $(2712.4 \pm 14.3)\times 10^6 ~œà$(3686) events collected by the BESIII detector at the BEPCII collider, we measure the branching fractions of the decays $œá_{cJ}\to œÜœÜŒ∑,~œÜœÜŒ∑^{\prime}$, and~$œÜK^+K^-Œ∑$ ($J = 0, 1, 2$). The obtained branching fractions are $\mathcal{B}(œá_{c0} \to œÜœÜŒ∑) = (7.40 \pm 0.23 \pm 0.55)\times10^{-4}$, $\mathcal{B}(œá_{c1} \to œÜœÜŒ∑) = (3.33 \pm 0.14 \pm 0.25)\times10^{-4}$, $\mathcal{B}(œá_{c2} \to œÜœÜŒ∑) = (5.46 \pm 0.17 \pm 0.40)\times10^{-4}$, $\mathcal{B}(œá_{c0} \to œÜœÜŒ∑^\prime) = (2.96 \pm 0.23 \pm 0.29)\times10^{-4}$, $\mathcal{B}(œá_{c1} \to œÜœÜŒ∑^\prime) = (0.69 \pm 0.10 \pm 0.08)\times10^{-4}$, $\mathcal{B}(œá_{c2} \to œÜœÜŒ∑^\prime) = (0.65 \pm 0.09 \pm 0.07)\times10^{-4}$, $\mathcal{B}(œá_{c0} \to œÜK^+K^-Œ∑) = (1.23 \pm 0.08 \pm 0.10)\times10^{-4}$, $\mathcal{B}(œá_{c1} \to œÜK^+K^-Œ∑) = (1.00 \pm 0.07 \pm 0.07)\times10^{-4}$, and $\mathcal{B}(œá_{c2} \to œÜK^+K^-Œ∑) = (1.82 \pm 0.09 \pm 0.14)\times10^{-4}$, where $K^+K^-$ is not from the decay of a $œÜ$ meson, the first uncertainties are statistical and the second systematic. The branching fractions of $œá_{cJ}\to œÜœÜŒ∑$ are measured with precision improved by factors of $1.5-1.9$, and those of $œá_{cJ}\to œÜœÜŒ∑^\prime$ and $œÜK^+K^-Œ∑$ are measured for the first time. |
| 2025-12-16 | [ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning](http://arxiv.org/abs/2512.14331v1) | Rishabh Dev Yadav, Avirup Das et al. | Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines. |
| 2025-12-15 | [XID+PRIMA, II: Stepping Through Hyperspectral Imaging to Deblend PRIMAger Beyond the Extragalactic Confusion Limit](http://arxiv.org/abs/2512.13682v1) | J. M. S. Donnellan, B. Pautasso et al. | The PRobe far-Infrared Mission for Astrophysics concept aims to map large areas with spectral coverage and sensitivities inaccessible to previous FIR space telescopes, covering 25-235um. We synthesise images representing a deep imaging survey, with realistic instrumental and confusion noise, reflecting the latest PRIMAger instrument specifications. We present a new Bayesian modelling approach XID+stepwise that exploits PRIMAger's hyperspectral imaging to derive self-consistent, informative flux priors by sequentially propagating constraints from short to long wavelengths. With Euclid-like prior source positions, this method recovers fluxes to within 20% to 0.2-0.7 mJy across 45-84 um, which correspond to factors of 1.3-3.4 fainter than the confusion limit. For the most confusion-dominated channels, accurate fluxes are measured to 0.9, 2.5, 7.6 and 14.8 mJy at 92, 126, 183 and 235 um, respectively, which are factors of 3-5 better than the confusion limit. Using a deeper Euclid-based prior catalogue and weak ancillary flux priors at 25 um yields further improvements, reaching up to a factor ~7 fainter than the confusion limit at 96 um. Additionally, we demonstrate that positional priors from blind source detection followed by deblending via XID+ enables PRIMAger to achieve sensitivity beyond the confusion limits using PRIMAger data alone. We show that IR-luminous galaxies at z~2 are robustly detected in a large fraction of the PRIMAger channels (>98% in 12 out of the 16 considered channels), providing dense sampling of the FIR SED even for sources several factors below the confusion limit. We explore the impact on our results for a range of systematic effects, including cirrus contamination, optical degradation, and calibration uncertainties. These findings indicate that confusion noise will not limit the key science from PRIMA extragalactic imaging surveys when employing XID+. |
| 2025-12-15 | [Towards Quantum Advantage in Chemistry](http://arxiv.org/abs/2512.13657v1) | Scott N. Genin, Ohyun Kwon et al. | Molecular simulations are widely regarded as leading candidates to demonstrate quantum advantage--defined as the point at which quantum methods surpass classical approaches in either accuracy or scale. Yet the qubit counts and error rates required to realize such an advantage remain uncertain; resource estimates for ground-state electronic structure span orders of magnitude, and no quantum-native method has been validated at a commercially relevant scale. Here we address this uncertainty by executing the iterative qubit coupled-cluster (iQCC) algorithm, designed for fault-tolerant quantum hardware, at unprecedented scale using a quantum solver on classical processors, enabling simulations of transition organo-metallic complexes requiring hundreds of logical qubits and millions of entangling gates. Using this approach, we compute the lowest triplet excited state (T$_1$) energies of Ir(III) and Pt(II) phosphorescent organometallic compounds and show that iQCC achieves the lowest mean absolute error (0.05 eV) and highest R$^2$ (0.94) relative to experiment, outperforming leading classical methods. We find these systems remain classically tractable up to $\sim$200 logical qubits, establishing the threshold at which quantum advantage in computational chemistry may emerge and clarifying resource requirements for future quantum computers. |
| 2025-12-15 | [The impact of non-Gaussianity when searching for Primordial Black Holes with LISA](http://arxiv.org/abs/2512.13648v1) | Antonio Junior Iovino, Gabriele Perna et al. | LISA can observe cosmological millihertz (mHz) gravitational wave (GW) backgrounds that may offer a decisive test for asteroid-mass primordial black hole (PBH) dark matter (DM). In standard scenarios, failing to detect a scalar-induced gravitational wave (SIGW) background would exclude the last viable window for PBH DM formed through critical collapse. We show that this conclusion becomes much weaker in the presence of astrophysical foregrounds and strongly non-Gaussian primordial density perturbations, by studying how these phenomena affect the link between SIGWs and PBHs, and reevaluate LISA's sensitivity to asteroid-mass PBHs. In addition, we analyse the interplay between PBHs and SIGWs to gain further insights into the nature of primordial non-Gaussianity. We find that uncertainties in $f_{\rm NL}$ can induce substantial uncertainties in the PBH abundance, which ultimately limits LISA's capacity to fully probe the asteroid-mass PBH DM window. |
| 2025-12-15 | [Adaptive Sampling for Hydrodynamic Stability](http://arxiv.org/abs/2512.13532v1) | Anshima Singh, David J. Silvester | An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis. |
| 2025-12-15 | [Deployable Prototype Testing and Control Allocation of the CABLESSail Concept for Solar Sail Shape Control and Momentum Management](http://arxiv.org/abs/2512.13493v1) | Soojeong Lee, Michael States et al. | This paper presents prototype testing and a control allocation algorithm for the Cable-Actuated Bio-inspired Lightweight Elastic Solar Sail (CABLESSail) concept aimed at performing momentum management of a solar sail. CABLESSail uses actuated cables routed along the structural booms of the solar sail to control the shape of the solar sail and changes the solar radiation pressure disturbance torques acting on it. Small-scale prototype tests of CABLESSail are presented in this paper, which demonstrate the effectiveness of cable actuation on deployable booms. A novel control allocation method is also presented in this paper that provides a computationally-efficient manner to determine the deformations required in each of the structural booms to impart the desired momentum management torque on the solar sail. Numerical simulation results with the proposed algorithm demonstrate robustness to uncertainty in the shape of the sail membrane, resulting in reliable generation of momentum management torques that exceed or meet the capabilities of state-of-the-art solar sail actuators. Both the prototype tests and control allocation methods presented in this paper represent key steps in raising the technology readiness level of the CABLESSail concept. |
| 2025-12-15 | [Climate change impacts on net load under technological uncertainty in European power systems](http://arxiv.org/abs/2512.13461v1) | Luna Bloin-Wibe, Erich Fischer et al. | Renewable energy sources play a major role in future net-zero energy systems. However, achieving energy system resilience remains challenging, since renewables depend on weather fluctuations, and future energy systems are subject to major design uncertainty. Existing literature mostly treats these types of uncertainty separately. Therefore, the assessment of uncertainties surrounding climate change and energy system design, and particularly their interactions, is insufficiently understood. To close this gap, we evaluate net load to assess energy system stress without relying on perfect foresight, while maintaining temporal and spatial correlations of the climate system. Net load is calculated from hourly historical and future climate model data translated to energy variables. To scope the extent of plausible energy systems, we consider eight different design scenarios inspired by the European Ten-Year Network Development Plan (TYNDP) and different levels of transmission expansion. We find that climate change impacts on net load are highly sensitive to the energy system design, implying that energy systems can be designed so that they are either hindered or helped by climate change. Furthermore, within a system scenario, climate change can change the frequency and seasonality of high net load events and their technological and meteorological composition. Wind-dominated systems with currently electrified heating levels, for instance, feature a 30% increase of high net load events under climate change, mostly in summer and fall, while fully electrified net zero systems are impacted by high net load events in winter and spring, which decrease by 50% with climate change. Our work thus calls for a wider perspective on energy-climate stress that captures the non-linear interactions of climate change and system design uncertainty, thereby overcoming the current focus on cold Dunkelflauten. |
| 2025-12-15 | [Computational discovery of ferromagnetic AT6X6 kagome compounds](http://arxiv.org/abs/2512.13431v1) | Shiya Chen, Zhen Zhang et al. | We present a systematic high-throughput density-functional-theory investigation of the structural and magnetic stability of 312 substitutional compounds in the magnetic kagome AT6X6 family. Our screening confirms the stability of many previously reported structures and predicts several additional stable candidates. Within collinear spin configurations, we find that Fe-based systems predominantly adopt antiferromagnetic ground states, whereas Mn-based analogues exhibit a more balanced distribution between ferromagnetic and antiferromagnetic order. For compounds exhibiting several nearly degenerate collinear configurations, we analyze the nature of their magnetic ground states, assess the possible emergence of non-collinear order, and discuss the limitations and uncertainties inherent to standard density-functional approaches. Our electronic-structure analysis further reveals that newly predicted ferromagnetic kagome systems display characteristic features of topological metals, with rich magnetic configurations that can be tuned by chemical substitution. Overall, these ferromagnetic kagome compounds constitute a broad and still largely unexplored materials platform for the emergence of exciting magneto-transport phenomena. |
| 2025-12-15 | [Prospects for Measuring Black Hole Masses using TDEs with the Vera C. Rubin Observatory](http://arxiv.org/abs/2512.13409v1) | K. Decker French, Brenna Mockler et al. | Tidal Disruption Events (TDEs) provide an opportunity to study supermassive black holes that are otherwise quiescent. The Vera C. Rubin Legacy Survey of Space and Time will be capable of discovering thousands of TDEs each year, allowing for a dramatic increase in the number of discovered TDEs. The optical light curves from TDEs can be used to model the physical parameters of the black hole and disrupted star, but the sampling and photometric uncertainty of the real data will couple with model degeneracies to limit our ability to recover these parameters. In this work, we aim to model the impact of the Rubin survey strategy on simulated TDE light curves to quantify the typical errors in the recovered parameters. Black hole masses $5.5< \log M_{\rm BH}/M_\odot < 8.2$ can be recovered with typical errors of 0.26 dex, with early coverage removing large outliers. Recovery of the mass of the disrupted star is difficult, limited by the degeneracy with the accretion efficiency. Only 57\% of the cases have accurate recovery of whether the events are full or partial, so we caution the use this method to assess whether TDEs are partially or fully disrupted systems. Black hole mass measurements obtained from Rubin observations of TDEs will provide powerful constraints on the black hole mass function, black hole -- galaxy co-evolution, and the population of black hole spins, though continued work to understand the origin of TDE observables and how the TDE rate varies among galaxies will be necessarily to fully utilize the upcoming rich data set from Rubin. |
| 2025-12-15 | [Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling](http://arxiv.org/abs/2512.13354v1) | Geremy Loacham√≠n, Eleni D. Koronaki et al. | This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty. |
| 2025-12-15 | [Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases](http://arxiv.org/abs/2512.13346v1) | Yukun Lu, Bingjie Li et al. | Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent "Don't know/can't remember" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, >7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level. |
| 2025-12-12 | [Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs](http://arxiv.org/abs/2512.11791v1) | Wentao Jiang, Vamsi Varra et al. | Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment. |
| 2025-12-12 | [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](http://arxiv.org/abs/2512.11773v1) | Britton Jordan, Jordan Thompson et al. | Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements. |
| 2025-12-12 | [LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems](http://arxiv.org/abs/2512.11750v1) | Ernesto Casablanca, Oliver Sch√∂n et al. | Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks. |
| 2025-12-12 | [Determination of $B$-meson distribution amplitudes from $B\to œÄ,K,D$ transition form factors](http://arxiv.org/abs/2512.11741v1) | Dong-Hao Li, Cai-Dian L√º et al. | Recent work on $B \to œÄ$, $K$ and $B\to D$ form factors from lattice QCD and light-cone sum rules has made it possible to constrain the inverse moment $Œª_B$ of the $B$-meson light-cone distribution amplitudes by performing a global fit of $B\to œÄ,K,D$ form factors. We have compiled the $B\to œÄ,K,D$ form factors calculated by the HPQCD, MILC, and RBC/UKQCD collaborations in the large $q^2$ region. By employing an three-parameter ansatz of the $B$-meson light-cone distribution amplitudes, we express the $B\to œÄ,K,D$ form factors at $q^2=0$ that are calculated from light-cone sum rules, in terms of the inverse moment $Œª_B$ of the leading-twist $B$-meson light-cone distribution amplitude. In the $B \to œÄ\ell ŒΩ$ channel, we also include the available $q^2$-binned experimental data from the BaBar, Belle, and Belle~II collaborations. Using the Bourrely-Caprini-Lellouch parametrization, we perform a global fit and obtain $Œª_B=217(19)_{-17}^{+82}$~MeV and $|V_{\text{ub}}|=3.68(13)_{-1}^{+0}\times10^{-3}$. The second uncertainty is obtained by constraining $Œª_B>200$ MeV and varying the inverse logarithmic moments $\hatœÉ_1\in[-0.7,0.7]$ and $\hatœÉ_2\in[-6,6]$, which represents the model-dependent uncertainty from the $B$-meson light-cone distribution amplitudes. When taking into account $Œª_B$ and $\hatœÉ_1$ as fitting parameters simultaneously, the intervals of our preditions are $Œª_B=[208, 324]$~MeV and $\hatœÉ_1=[-0.7, 0.27]$. |
| 2025-12-12 | [The Impact of Initial Mass Dependent Convective Boundary Mixing on the Structure and Fates of Massive Stars](http://arxiv.org/abs/2512.11728v1) | Emily E. Whitehead, Raphael Hirschi et al. | While convection has been known to play a key role in stars for many decades, its implementation in one-dimensional stellar evolution codes still represents a major uncertainty today. The purpose of this work is to investigate the impact of initial mass dependent convective boundary mixing (CBM), often referred to as overshooting, on the frequency and type of nuclear burning shell interactions that occur in low metallicity massive stars and the subsequent effect on their fates. Two grids of models were calculated using the Modules for Experiments in Stellar Astrophysics (MESA) code and a 22-isotope nuclear network, each with a different strength of CBM applied. One grid uses the typical CBM value for diffusive overshooting used in literature whereas the other grid uses CBM values guided by the results of 3D convection simulations. Interactions between the carbon, neon and oxygen shells (C-Ne-O) are common throughout both grids. The higher CBM grid also exhibits more frequent H-He and He-C interactions at lower initial masses than in the lower CBM grid. Several models also undergo multiple interaction events during evolution. While future work will be needed to fully assess the impact of the new CBM and the interactions it leads to, one expects interesting effects like unusual nucleosynthesis including more common or enhanced i- and gamma-process nucleosynthesis. Furthermore, SN precursors and a significant change to the pre-SN structure are also expected, with many models not having the commonly expected onion-ring like structure and having a different explosion probability. |
| 2025-12-12 | [Machine-learned global glacier ice volumes](http://arxiv.org/abs/2512.11685v1) | N. Maffezzoli, E. Rignot et al. | We present a global dataset of glacier ice thickness modeled with IceBoost v2.0, a machine learning model trained on 7 million ice thickness measurements and informed by physical and geometrical predictors. We model the distributed ice thickness for every glacier in the two latest Randolph Glacier Inventory releases (v6.0 and v7.0), totaling 215,547 and 274,531 glacier outlines, respectively, plus 955 ice masses contiguous with the Greenland Ice Sheet. We find a global glacier volume of $(149 \pm 38)\times 10^3$ km$^3$, consistent with the previous ensemble estimate of $(147 \pm 28)\times 10^3$ km$^3$. The corresponding sea-level equivalent, $323 \pm 91$ mm, is likewise consistent with the earlier value of $315 \pm 63$ mm. Compared to measurements, IceBoost error is 20-45% lower than the other solutions in the high Arctic, highlighting the value of machine-learning approaches. Confidence in our solution is highest at higher latitudes. Over mountainous terrain, small glaciers, and under-represented lower-latitude regions, confidence is lower. IceBoost v2.0 demonstrates strong generalization at ice sheet margins. On the Geikie Plateau (East Greenland), we find nearly twice as much ice as previously reported, highlighting the method's potential to infer bed topography in parts of the ice sheets. The quality of the solutions depends on the accuracy of the training data, the Digital Elevation Model, ice velocity fields, and glacier geometries, including nunataks. Using the Jensen Gap, we probe the model's curvature with respect to input errors and find it is strongly concave over low-slope, thick-ice regions, implying a potential downward bias in predicted thickness under input uncertainty. The released dataset can be used to model future glacier evolution and sea-level rise, inform the design of glaciological surveys and field campaigns, as well as guide policies on freshwater management. |
| 2025-12-12 | [Influence of Exchange-Correlation Functionals and Neural Network Architectures on Li$^+$-Ion Conductivity in Solid-State Electrolyte from Molecular Dynamics Simulations with Machine-Learning Force Fields](http://arxiv.org/abs/2512.11650v1) | Zicun Li, Huanjing Gong et al. | With the rapid advancement of machine learning techniques for materials simulations, machine-learned force fields (MLFFs) have become a powerful tool that complements first-principles calculations by enabling high-accuracy molecular dynamics simulations over extended timescales. Typically, MLFFs are trained on data generated from density functional theory (DFT) using a specific exchange-correlation (XC) functional, with the goal of reproducing DFT-level properties. However, the uncertainties in MLFF-based simulations--arising from variations in both MLFF model architectures and the choice of XC functionals--remain insufficiently understood. In this work, we construct MLFF models of different architectures trained on DFT data from both semilocal and hybrid functionals to describe Li$^+$ diffusion in the solid-state electrolyte Li$_6$PS$_5$Cl. We systematically investigate how different XC functionals influence the Li$^+$ diffusion coefficient. To reduce statistical uncertainty, the mean squared displacements are averaged over 300 independent molecular dynamics (MD) trajectories of 70 ps each, yielding statistical variations below $1\%$. This enables a clear assessment of the respective influences of the functional and the MLFF model. Due to its tendency to underestimate band gaps and migration barriers, the semilocal functional predicts consistently higher Li$^+$ diffusion coefficients, compared to the hybrid functional. Furthermore, comparisons among various neural network methods reveal that the differences in predicted diffusion coefficients arising from different network architectures are of the same order of magnitude as those caused by different functionals, indicating that the choice of the network model itself substantially influences the MLFF predictions. This observation calls from an urgent need for standardized protocols to minimize model-dependent biases in MLFF-based MD. |
| 2025-12-12 | [Polarization Entanglement in Atomic Biphotons via OAM-to-Spin Mapping](http://arxiv.org/abs/2512.11625v1) | Chang-Wei Lin, Yi-Ting Ma et al. | We demonstrate polarization-entangled biphotons in a cold-atom double-$Œõ$ system, overcoming atomic selection rules that suppress polarization correlations and favor orbital angular momentum (OAM) entanglement. Using spatial light modulators, we coherently map a selected two-dimensional OAM subspace onto the polarization basis and thereby open an otherwise inaccessible polarization channel. Quantum-state tomography confirms that the mapping preserves the biphoton coherence. The four polarization Bell states are generated with fidelities of $92\text{-}94\%$ with few-percent statistical uncertainties, and an average Clauser-Horne-Shimony-Holt parameter of $S=2.44$ verifies the survival of nonlocal correlations. To the best of our knowledge, this work presents the first demonstration of OAM-to-polarization entanglement transfer in a cold-atom spontaneous four-wave mixing platform and establishes a practical interface for integrating atomic OAM resources with polarization-based quantum communication networks. |
| 2025-12-12 | [AI Benchmark Democratization and Carpentry](http://arxiv.org/abs/2512.11588v1) | Gregor von Laszewski, Wesley Brewer et al. | Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.   Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.   Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry. |
| 2025-12-12 | [The magnitude of the dark ages 21-cm signal in the context of existing early and late time constraints on $Œõ$CDM](http://arxiv.org/abs/2512.11568v1) | H. T. J. Bevins | The dark ages 21-cm signal is a promising probe of the currently unobserved infant universe between the formation of the Cosmic Microwave Background around $z \approx 1100$ and the first galaxies around $z\approx 30$. A detection of the signal will help researchers understanding the nature of dark matter and dark energy, the expansion of the universe and any extensions to the concordance $Œõ$CDM model that could explain the reported Cosmic Dawn 21-cm signal from EDGES and the Hubble tension. In this letter we take existing constraints on the $Œõ$CDM cosmological model from two early time probes, Planck and WMAP, and two late time probes, DES galaxy lensing and clustering and Baryon Acoustic Oscillations, and propagate these through to constraints on the magnitude of the dark ages 21-cm signal. We constrain the magnitude and central frequency of the signal while methodically accounting for uncertainties in the cosmological parameters. We find that within the context of our modelling assumptions and the $Œõ$CDM paradigm, the depth of the dark ages 21-cm signal is known to better than 1 mK and the central frequency to within 0.05 MHz. |
| 2025-12-11 | [Detection prospects for heavy WIMP dark matter near supermassive black holes, particularly in M31](http://arxiv.org/abs/2512.10923v1) | Andrei E. Egorov | This work analyzes the detection prospects for weakly interacting massive particles (WIMPs) in dark matter (DM) density spikes around nearby supermassive black holes (SMBHs) by observations in very high energy gamma-ray band. Such spikes are unique targets, which provide a possibility to discover the basic thermal s-wave annihilating WIMP with any mass up to the theoretical unitarity limit ~ 100 TeV. All relevant SMBHs were checked, and only MW* and M31* were identified as worthwhile objects. Cherenkov Telescope Array (CTA) sensitivity to heavy WIMPs in M31* was estimated. It was obtained that CTA will be able to probe a major part of TeV-scale WIMP parameter space in case of optimistic spike density configuration in M31*. In certain scenarios, M31* may yield even stronger constraints than MW*. Relevant systematic uncertainties were explored. |
| 2025-12-11 | [Shedding Light on Large Space-Based Telescopes: Modeling Stray Light due to Primary Mirror Damage from Micrometeoroid Impacts](http://arxiv.org/abs/2512.10915v1) | Megan T. Gialluca, Jonathan W. Arenberg et al. | A large space-based telescope aimed at detecting and characterizing the atmospheres of Earth-like planets orbiting Sun-like stars will require unprecedented contrast and stability. However, damage to the primary mirror due to micrometeoroid impacts will provide a stochastic, time-dependent source of stray light in the coronagraph's field of view that could significantly lengthen exposure times and reduce the expected science yield. To better quantify the impact of stray light and inform the Habitable Worlds Observatory mission design process, we present estimates of stray light in different micrometeoroid damage scenarios for a broad range of targets, and use that to find the expected decrease in science yield (i.e., the expected number of detected exoEarth candidates). We find that stray light due to micrometeoroid damage may significantly reduce yield, by 30% -- 60% in some cases, but significant uncertainties remain due to the unknown maximum expected impactor energy, and the relationship between impact energy and expected crater size. Micrometeoroid damage therefore needs further exploration, as it has the potential to reduce scientific yield, and in turn drive the development of mitigation strategies, selection of telescope designs, and selection of observing priorities in the future. |
| 2025-12-11 | [Spectral Theory of the Weighted Fourier Transform with respect to a Function in $\mathbb{R}^n$: Uncertainty Principle and Diffusion-Wave Applications](http://arxiv.org/abs/2512.10880v1) | Gustavo Dorrego, Luciano Luque | In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case in \cite{Dorrego}, to the $n$-dimensional Euclidean space $\mathbb{R}^{n}$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Œî_{œÜ,œâ})^{s}$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation, demonstrating that the fundamental solution can be expressed in terms of the Fox H-function, intrinsically related to the generalized $œâ$-Mellin transform introduced in \cite{Dorrego}. In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case, to the n-dimensional Euclidean space $\mathbb{R}^n$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Œî_{œÜ,œâ})^s$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation involving the weighted Hilfer derivative. We demonstrate that the fundamental solution can be explicitly expressed in terms of the Fox H-function, revealing an intrinsic connection with the generalized Mellin transform. |
| 2025-12-11 | [Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling](http://arxiv.org/abs/2512.10873v1) | Qitian Lu, Himanshu Sharma et al. | Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks. |
| 2025-12-11 | [Bayesian Symbolic Regression via Posterior Sampling](http://arxiv.org/abs/2512.10849v1) | Geoffrey F. Bomarito, Patrick E. Leser | Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications. |
| 2025-12-11 | [Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS](http://arxiv.org/abs/2512.10819v1) | Mohammed Aboelela | Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger. |
| 2025-12-11 | [AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators](http://arxiv.org/abs/2512.10773v1) | Samaksh Ujjawal, Shivansh Pratap Singh et al. | Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests. |
| 2025-12-11 | [Symbol-Level Precoding for Integrated Sensing and Covert Communication](http://arxiv.org/abs/2512.10752v1) | Yufei Wang, Qiang Li et al. | Integrated sensing and communication (ISAC) systems have emerged as a promising solution to improve spectrum efficiency and enable functional convergence. However, ensuring secure information transmission while maintaining high-quality sensing performance remains a significant challenge. In this paper, we investigate an integrated sensing and covert communication (ISCC) system, in which a base station (BS) simultaneously serves multiple downlink users and senses malicious targets that may act as both potential eavesdroppers (Eves) and wardens. We propose a novel symbol-level precoding (SLP)-based waveform design for ISCC that achieves covert communication intrinsically, without requiring additional transmission resources such as artificial noise. The proposed design integrates symbol shaping to enhance reliability for legitimate users and noise shaping to obscure transmission activities from the targets. For imperfect channel state information (CSI), the framework incorporates bounded uncertainty models for user channels and target angles, yielding a more robust design. The resulting ISCC waveform optimization problem is non-convex; to address this, we develop a low-complexity proximal distance algorithm (PDA) with closed-form updates under both PSK and QAM modulations. Simulation results demonstrate that the proposed method achieves superior covertness and sensing-communication performance with negligible degradation compared to traditional beamforming and conventional SLP approaches without noise-shaping mechanisms. |
| 2025-12-11 | [Efficient pulsar distance measurement with multiple nanohertz gravitational-wave sources](http://arxiv.org/abs/2512.10729v1) | Si-Ren Xiao, Ji-Yu Song et al. | In recent years, pulsar timing arrays (PTAs) have reported evidence for a nanohertz gravitational-wave (GW) background. As radio telescope sensitivity improves, PTAs are also expected to detect continuous gravitational waves from individual supermassive black hole binaries. Nanohertz GWs generate both Earth and pulsar terms in the timing data, and the time delay between the two terms encodes the pulsar distance. Precise pulsar distance measurements are critical to fully exploiting pulsar-term information, which can improve the measurement precision of GW sources' sky position parameters and thus enhance the GW sky-localization capability. In this work, we propose a new pulsar distance estimation method by using pulsar-term phase information from GWs. We construct two-dimensional distance posteriors for pulsar pairs based on the simulated GW signals and combine them to constrain individual pulsar distances. Compared with the existing one-dimensional method, our approach reduces the impact of source-parameter uncertainties on pulsar distance measurements. Considering four GW sources and a PTA of 20 pulsars with a white-noise level of 20 ns, we find that a significant fraction of pulsars at distances $\lesssim 1.4$ kpc can achieve sub-parsec distance precision over a 15-year observation. |
| 2025-12-11 | [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](http://arxiv.org/abs/2512.10720v1) | Lingjing Kong, Shaoan Xie et al. | Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems. |
| 2025-12-10 | [A Precise $Œ±_s$ Determination from the R-improved QCD Static Energy](http://arxiv.org/abs/2512.09888v1) | Jose M. Mena-Valle | The strong coupling $Œ±_s$ is extracted with high precision through fits to lattice-QCD data for the static energy. Our theoretical framework is based on R-improving the three-loop fixed-order prediction for the static energy: we remove the $u=1/2$ renormalon and resum the associated large infrared logarithms. Combined with radius-dependent renormalization scales (the so-called profile functions), this procedure extends the range of validity of perturbation theory to distances as large as $\sim 0.5\,$fm. In addition, we resum large ultrasoft logarithms to N$^3$LL accuracy using renormalization-group evolution. Since the standard four-loop R-evolution treats N$^4$LL and higher-order contributions asymmetrically, we also incorporate this potential source of bias in our analysis. Our estimate of the perturbative uncertainty is obtained through a random scan over the parameters controlling the profile functions and the implementation of R-evolution. We analyze how the extracted value of $Œ±_s$ depends on the shortest and longest distances included in the fit, on the details of the R-evolution procedure, on the fitting strategy itself, and on the accuracy of ultrasoft resummation. From our final analysis, and after evolution to the $Z$ pole, we obtain $Œ±^{(n_f=5)}_s(m_Z)=0.1170\pm 0.0009$, a result fully compatible with the world average and with a comparable uncertainty. |
| 2025-12-10 | [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](http://arxiv.org/abs/2512.09850v1) | Simone Cuonzo, Nina Deliu | We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.   We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees. |
| 2025-12-10 | [Kinematics of Distant Milky Way Halo RR Lyrae Stars out to 160 kpc](http://arxiv.org/abs/2512.09795v1) | Yuting Feng, Puragra Guhathakurta et al. | We present a kinematical study of the outer halo (r_GC approximately 60 to 160 kpc) of the Milky Way based on spectroscopy of 55 RR Lyrae stars obtained with the ESI instrument on the Keck II telescope. Our spectroscopic targets were selected from three photometric surveys: NGVS, DES, and Pan-STARRS1. We derive center-of-mass radial velocities with uncertainties of 6 to 35 km s^-1. The halo velocity dispersion measured from our sample is 70 plus/minus 7 km s^-1. The velocity field shows a possible dipole-like structure, with redshifted northern and blueshifted southern hemispheres. Fitting a Milky Way - Large Magellanic Cloud dipole perturbation model yields a weak or marginal dipole signal with amplitude -30 (+16, -20) km s^-1 and apex direction (l, b) = (-38.2 (+42.4, -31.5), -41.3 (+27.9, -23.8)) deg, along with a bulk compression velocity of -16 plus/minus 11 km s^-1. Although limited by sky coverage and sample size, our results are consistent with the presence of LMC-induced disequilibrium in the distant halo beyond 100 kpc. In addition to the 55 RR Lyrae stars, our spectroscopy reveals that 10 additional photometrically selected RR Lyrae candidates are actually quasar or blazar contaminants, highlighting the need for caution regarding such contaminants in sparsely sampled photometric surveys. Our study demonstrates that single-epoch spectroscopy of RR Lyrae stars is a viable method for probing the kinematics of the outer halo, and future surveys such as Rubin LSST and DESI-II have the potential to significantly advance this effort. |
| 2025-12-10 | [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](http://arxiv.org/abs/2512.09775v1) | Vladimir Balditsyn, Philippe Lalanda et al. | The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts. |
| 2025-12-10 | [First measurement of the absolute branching fractions of $Œ£^+$ nonleptonic decays and test of the $ŒîI = 1/2$ rule % $Œ£^+ \to p œÄ^0$ and $Œ£^+ \to n œÄ^+$](http://arxiv.org/abs/2512.09628v1) | BESIII Collaboration, M. Ablikim et al. | Based on $(10087 \pm 44) \times 10^6$ $J/œà$ events collected by the BESIII detector at the center-of-mass energy $\sqrt{s} = 3.097$ GeV, the first absolute measurement of the branching fractions for the decays $Œ£^+ \to p œÄ^0$ and $Œ£^+ \to n œÄ^+$ is performed. The branching fractions are determined to be $B_{Œ£^+ \to p œÄ^0} = (49.79 \pm 0.06 \pm 0.22)\%$ and $B_{Œ£^+ \to n œÄ^+} = (49.87 \pm 0.05 \pm 0.29)\%$, where the first uncertainties are statistical and the second systematic. These results show significant deviations from the PDG values for both decays, with differences of 4.4$œÉ$ for $Œ£^+ \to p œÄ^0$ and 3.4$œÉ$ for $Œ£^+ \to n œÄ^+$. Furthermore, the $ŒîI = 1/2$ rule is tested in nonleptonic $Œ£^\pm$ decays. The observed results deviate from zero by more than $5œÉ$, indicating the presence of the $ŒîI = 3/2$ transition amplitude in the $Œ£$ hyperon decays. |
| 2025-12-10 | [Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates](http://arxiv.org/abs/2512.09586v1) | Prashant Kumar Choudhary, Nouhaila Innan et al. | Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery. |
| 2025-12-10 | [Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis](http://arxiv.org/abs/2512.09576v1) | David Seu, Nicolas Longepe et al. | Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets. |
| 2025-12-10 | [Can Intense Quantum Light Beat Classical Uncertainty Relations?](http://arxiv.org/abs/2512.09558v1) | Felipe Reibnitz Willemann, Mauro Antezza et al. | Uncertainty relations are fundamental to quantum mechanics, encoding limits on the simultaneous measurement of conjugate observables. Violations of joint uncertainty bounds can certify entanglement -- a resource critical for quantum information protocols and increasingly relevant in strong-field physics. Here, we investigate the pairwise time-delay and frequency-bandwidth uncertainties for arbitrary multimode quantum states of light, deriving a general lower bound for their joint product. We find that the nonclassical correction scales inversely with the average photon number, a behavior rooted in the so-called ``monogamy of entanglement''. These results clarify the intensity scaling of quantum advantages in nonclassical light states and highlight the interplay between entanglement and photon statistics. |
| 2025-12-10 | [Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search](http://arxiv.org/abs/2512.09538v1) | Ekaterina Fadeeva, Maiya Goloburda et al. | Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance. |
| 2025-12-10 | [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](http://arxiv.org/abs/2512.09517v1) | Nabil Anan Orka, Ehtashamul Haque et al. | This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (Œµ = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis. |
| 2025-12-09 | [Update of the nonlocal sub-leading ${O}_1$ - ${O}_7$ contribution to $\bar B \to X_s Œ≥$ at LO](http://arxiv.org/abs/2512.08902v1) | Michael Benzke, Maria Vittoria Garzelli et al. | In all previous calculations of the non-local sub-leading contribution to the inclusive penguin decay $\bar B \to X_s Œ≥$ due to the interference of the electroweak operators ${O}_1^c$ - ${O}_{7Œ≥}$ the local Voloshin term was subtracted. In view of the ongoing analysis at order $Œ±_s$, we present a calculation of the complete non-local contribution which takes into account the high correlation between the uncertainties of the local Voloshin and the non-local term of the previous analyses. |
| 2025-12-09 | [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](http://arxiv.org/abs/2512.08879v1) | Mohammad Abu-Shaira, Ajita Rattani et al. | Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression. |
| 2025-12-09 | [Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse](http://arxiv.org/abs/2512.08864v1) | Steve Barrett, Malcolm Murray et al. | Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone. |
| 2025-12-09 | [Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference](http://arxiv.org/abs/2512.08828v1) | Swaraj Bose, Walter Dempsey | Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS). |
| 2025-12-09 | [Stationary Point Constrained Inference via Diffeomorphisms](http://arxiv.org/abs/2512.08735v1) | Michael Price, Debdeep Pati et al. | Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability. |
| 2025-12-09 | [$J/œà$-Meson Nucleon Scattering Length from Threshold Photoproduction on Light Nuclei](http://arxiv.org/abs/2512.08701v1) | Igor I. Strakovsky, William J. Briscoe et al. | The quality of recent SRC/CT Collaboration $J/œà$ photoproduction data off a $^4$He target from Hall~D at Jefferson Laboratory, combined with the feasibility of measuring the reaction close to the free-nucleon energy threshold, opens the door to using incoherent $J/œà$ photoproduction to access a variety of interesting physics aspects. An example is an estimate of the $J/œà~p$ scattering length $|Œ±_{J/œà~p}|$ on the bound proton obtained using the Vector Meson Dominance model. This value can be compared with that of the free proton from the GlueX Collaboration. One may then project what would be expected from the SRC/CT Collaboration Experiment E12--25--002, which was recently approved by the JLab PAC. Using a plane-wave theoretical model to generate quasi-data, we find the experiment could achieve a result of $|Œ±_{J/œà~p}| = 3.08\pm 0.45~\mathrm{mfm}$, an uncertainty competitive with that of the free-proton measurement. A comparison between the two would allow an evaluation of the effects of medium modification in the case of light nuclei. |
| 2025-12-09 | [Atomic and molecular systems for radiation thermometry](http://arxiv.org/abs/2512.08668v1) | Stephen P. Eckel, Eric B. Norrgard et al. | Atoms and simple molecules are excellent candidates for new standards and sensors because they are both all identical and their properties are determined by the immutable laws of quantum physics. Here, we introduce the concept of building a standard and sensor of radiative temperature using atoms and molecules. Such standards are based on precise measurement of the rate at which blackbody radiation (BBR) either excites or stimulates emission for a given atomic transition. We summarize the recent results of two experiments while detailing the rate equation models required for their interpretation. The cold atom thermometer (CAT) uses a gas of laser cooled $^{85}$Rb Rydberg atoms to probe the BBR spectrum near 130~GHz. This primary, {\it i.e.}, not traceable to a measurement of like kind, temperature measurement currently has a total uncertainty of approximately 1~\%, with clear paths toward improvement. The compact blackbody radiation atomic sensor (CoBRAS) uses a vapour of $^{85}$Rb and monitors fluorescence from states that are either populated by BBR or populated by spontaneous emission to measure the blackbody spectrum near 24.5~THz. The CoBRAS has an excellent relative precision of $u(T)\approx 0.13$~K, with a clear path toward implementing a primary |
| 2025-12-09 | [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](http://arxiv.org/abs/2512.08656v1) | Lauritz Rismark Fosso, Herman Bi√∏rn Amundsen et al. | Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions. |
| 2025-12-09 | [Measurement of the differential $t$-channel production cross-section of single top quarks and top antiquarks at $\sqrt{s} = $13 TeV with the ATLAS detector](http://arxiv.org/abs/2512.08573v1) | Maren Stratmann | Differential production cross-sections of single top quarks and top antiquarks are measured in proton-proton collisions at a centre-of-mass energy $\sqrt{s} = $13 TeV. The full Run-2 dataset collected by the ATLAS detector at the LHC in the years 2015-2018 is used. The differential cross-sections are measured as a function of the transverse momentum and absolute rapidity of the top (anti)quark. The measurement results are compared to predictions obtained from fixed order calculations, different matrix-element event generators and different parton distribution function sets. The results agree with the theoretical predictions within the measurement uncertainties. An effective field theory interpretation of the measurement sets constraints on the contribution of the four-fermion operator $O_{Qq}^{3,1}$. |
| 2025-12-09 | [Astrometric Reconnaissance of Exoplanetary Systems (ARES). I. Methodology validation with HST point-source images of Proxima Centauri](http://arxiv.org/abs/2512.08533v1) | M. Libralato, L. Bedin et al. | We present the first results of the Astrometric Reconnaissance of Exoplanetary Systems (ARES) project, aimed at validating and characterizing candidate exoplanets around the nearest systems using multi-epoch Hubble Space Telescope (HST) data. In this first paper, we focus on Proxima Centauri, leveraging archival and recent HST observations in point-source imaging mode. We refine the geometric-distortion calibration of the HST detector used, and develop a robust methodology to derive high-precision astrometric parameters by combining HST measurements with the Gaia DR3 catalog. We determine Proxima's position, proper motion, and parallax with uncertainties at the $\sim$0.4-mas, 50-$Œº$as yr$^{-1}$, and 0.2-mas level, respectively, achieving consistent results with what measured by Gaia within $\sim$1$œÉ$. We further investigate the presence of the candidate exoplanet Proxima c by analyzing the proper-motion anomaly derived from combining long-term HST-based and short-term Gaia astrometry. Under the assumption of a circular, face-on orbit, we obtain an estimated mass of $m_c = 3.4^{+5.2}_{-3.4}$ $M_\odot$, broadly consistent with radial-velocity constraints but limited by our current uncertainties. These results establish the foundation for the next phase of ARES, which will exploit HST spatial-scanning observations to achieve astrometric precisions of a few tens of $Œº$as and enable a direct search for astrometric signatures of low-mass companions. |
| 2025-12-08 | [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](http://arxiv.org/abs/2512.07795v1) | Nearchos Potamitis, Lars Klein et al. | Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench . |
| 2025-12-08 | [Distribution-informed Online Conformal Prediction](http://arxiv.org/abs/2512.07770v1) | Dongjian Hu, Junxi Wu et al. | Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines. |
| 2025-12-08 | [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](http://arxiv.org/abs/2512.07756v1) | Mayank Anand, Ujair Alam et al. | Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM. |
| 2025-12-08 | [Symmetric Vaccine Efficacy](http://arxiv.org/abs/2512.07739v1) | Lucy D'Agostino McGowan, Sarah C. Lotspeich et al. | Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package. |
| 2025-12-08 | [Time-asymptotic behavior of the Boltzmann equation with random inputs in whole space and its stochastic Galerkin approximation](http://arxiv.org/abs/2512.07735v1) | Shi Jin, Qi Shao et al. | We consider the Boltzmann equation with random uncertainties arising from the initial data and collision kernel in the {\it whole space}, along with their stochastic Galerkin (SG) approximations. By employing Green's function method, we show that, the higher-order derivatives of the solution with respect to the random variable exhibit polynomial decay over time. These results are then applied to analyze the SG method for the SG system and to demonstrate the polynomial decay of the numerical error over time. |
| 2025-12-08 | [Decomposition Sampling for Efficient Region Annotations in Active Learning](http://arxiv.org/abs/2512.07606v1) | Jingna Qiu, Frauke Wilm et al. | Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git. |
| 2025-12-08 | [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](http://arxiv.org/abs/2512.07564v1) | Kassoum Sanogo, Renzo Ardiccioni | Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems. |
| 2025-12-08 | [Data-Driven Robust Safety Verification for Markov Decision Processes](http://arxiv.org/abs/2512.07550v1) | Abhijit Mazumdar, Manuela L. Bujorianu et al. | In this paper, we propose a data-driven robust safety verification framework for stochastic dynamical systems modeled as Markov decision processes with time-varying and uncertain transition probabilities. Rather than assuming access to the exact nominal transition kernel, we consider the realistic setting where only samples from multiple system executions are available. These samples may correspond to different transition models inside an ambiguity set around the nominal transition kernel. Using these observations, we construct a unified ambiguity set that captures both inherent run-to-run variability in the transition dynamics and finite-sample statistical uncertainty. This ambiguity set is formalized through a Wasserstein-distance ball around a nominal empirical distribution and naturally induces an interval Markov decision process representation of the underlying system. Within this representation, we introduce a robust safety function that characterizes reach-avoid type probabilistic safety under all transition kernels consistent with the interval Markov decision process. We further derive high-confidence safety guarantees for the true, unknown time-varying system. A numerical example illustrates the applicability and effectiveness of the proposed approach. |
| 2025-12-08 | [XMCQDPT2-Fidelity Transfer-Learning Potentials and a Wavepacket Oscillation Model with Power-Law Decay for Ultrafast Photodynamics](http://arxiv.org/abs/2512.07537v1) | Ivan V. Dudakov, Pavel M. Radzikovitsky et al. | A central pursuit in theoretical chemistry is the accurate simulation of photochemical reactions, which are governed by nonadiabatic transitions through conical intersections. Machine learning has emerged as a transformative tool for constructing the necessary potential energy surfaces, but applying it to excited states faces a fundamental barrier: the cost of generating high-level quantum chemistry data. We overcome this challenge by developing machine-learning interatomic potentials (MLIPs) that achieve multi-state multi-reference perturbation theory accuracy through various techniques, such as transfer, multi-state, and $Œî$-learning. Applied to the methaniminium cation, our highest-fidelity transfer-learning model uncovers its complete photodissociation landscape following S$_2$ photoexcitation. The comprehensive XMCQDPT2/SA(3)-CASSCF(12,12) electronic structure description captures all competing decay channels, including S$_1$ branching into photoisomerization and direct H$_2$-loss pathways. Our results show that the population dynamics generally depends on the MLIP model, correlating with its performance. At the same time, the introduction of MLIP-uncertainty corrections based on the predictions of an ensemble of models brings different approaches into agreement, validating this metric as essential for reliable dynamics. To interpret the population dynamics, we introduce a wavepacket oscillation model - a mechanistically transparent, power-law kinetics framework that extracts state-specific lifetimes directly from first-principles simulations. The model quantitatively reproduces the ultrafast decay, creating a direct link between quantum transition probabilities and classical rate constants. The kinetic fits yield channel-specific lifetimes, supporting the recently discovered photochemical pathway mediated by a novel $œÉœÄ^*/S_0$ conical intersection. |
| 2025-12-08 | [Precision Higgs Probe of Type-II Seesaw](http://arxiv.org/abs/2512.07532v1) | Saiyad Ashanujjaman, P. S. Bhupal Dev et al. | Despite direct searches at the LHC excluding triplet-like Higgs bosons up to several hundred GeV over much of the type-II seesaw model parameter space, parts of it -- most notably those featuring ``cascade decays'' of the charged Higgs bosons into their neutral partners and off-shell $W$ bosons -- still remain unconstrained. Meanwhile, measurements of the diphoton signal strength of the Standard Model (SM) Higgs boson -- potentially modified by loop contributions from triplet-like Higgs states -- are in good agreement with the SM expectation, with combined experimental uncertainties currently at approximately 8\%. Given the trend in previous measurements, it is expected that future precision Higgs measurements at the HL-LHC and a future lepton collider such as CEPC, FCC-ee, or Muon Collider will be consistent the standard diphoton signal strength, albeit with significantly reduced uncertainties, down to about 0.7\%. Presuming this and considering all relevant constraints, we explore whether such increasingly precise diphoton measurements can indirectly probe the parameter space that currently evades direct searches. We find that sub-percent-level determinations of the diphoton rate will decisively probe a substantial fraction of this otherwise elusive region. |
| 2025-12-05 | [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](http://arxiv.org/abs/2512.05954v1) | Shima Imani, Seungwhan Moon et al. | We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems |
| 2025-12-05 | [Constraining r-process nucleosynthesis via enhanced accuracy neutron-capture experiments](http://arxiv.org/abs/2512.05944v1) | C. Domingo-Pardo, C. Lederer-Woods et al. | The isotopic abundances of r-process elements in the solar system are traditionally derived as residuals from the subtraction of s-process contributions from total solar abundances. However, the uncertainties in s-process nucleosynthesis -particularly those arising from Maxwellian Averaged Cross Sections (MACS)- propagate directly into the r-process residuals, affecting their reliability. Building upon the seminal work of Goriely (1999), who introduced a multi-event s-process model to quantify these uncertainties, we revisit the problem using a simplified yet effective approach. By assuming that the relative uncertainty in s-process isotopic abundances scales linearly with the MACS uncertainties from data libraries (KADoNiS), we identify a subset of isotopes for which the r-process residuals remain significantly uncertain. Using updated solar abundances (Lodders 2025) and s-process contributions from Bisterzo et al. (2014), we present a short list of isotopes that are prime candidates for improved (n,g) measurements at CERN n_TOF in the near future. Our analysis provides a practical framework for prioritizing future experimental efforts that will profit from upgrades and enhancements of the n_TOF facility. It also highlights the need to revisit key neutron-capture cross sections to refine our understanding of the r-process isotopic abundance pattern, commonly used as a benchmark in stellar models of explosive nucleosynthesis. |
| 2025-12-05 | [Adsorption energies are necessary but not sufficient to identify good catalysts](http://arxiv.org/abs/2512.05938v1) | Shahana Chatterjee, Alexander Davis et al. | As a core technology for green chemical synthesis and electrochemical energy storage, electrocatalysis is central to decarbonization strategies aimed at combating climate change. In this context, computational and machine learning driven catalyst discovery has emerged as a major research focus. These approaches frequently use the thermodynamic overpotential, calculated from adsorption free energies of reaction intermediates, as a key parameter in their analysis. In this paper, we explore the large-scale applicability of such overpotential estimates for identifying good catalyst candidates by using datasets from the Open Catalyst Project (OC20 and OC22). We start by quantifying the uncertainty in predicting adsorption energies using \textit{ab initio} methods and find that $\sim$0.3-0.5 eV is a conservative estimate for a single adsorption energy prediction. We then compute the overpotential of all materials in the OC20 and OC22 datasets for the hydrogen and oxygen evolution reactions. We find that while the overpotential allows the identification of known good catalysts such as platinum and iridium oxides, the uncertainty is large enough to misclassify a broad fraction of the datasets as ``good'', which limits its value as a screening criterion. These results question the reliance on overpotential estimation as a primary evaluation metric to sort through catalyst candidates and calls for a shift in focus in the computational catalysis and machine learning communities towards other metrics such as synthesizability, stability, lifetime or affordability. |
| 2025-12-05 | [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](http://arxiv.org/abs/2512.05930v1) | Shima Imani, Seungwhan Moon et al. | Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities. |
| 2025-12-05 | [World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty](http://arxiv.org/abs/2512.05927v1) | Zhiting Mei, Tenny Yin et al. | Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection. |
| 2025-12-05 | [The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning](http://arxiv.org/abs/2512.05883v1) | Juan Sosa, Carlos A. Mart√≠nez et al. | This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice. |
| 2025-12-05 | [Safe Output Regulation of Coupled Hyperbolic PDE-ODE Systems](http://arxiv.org/abs/2512.05822v1) | Ji Wang, Miroslav Krstic | This paper presents a safe output regulation control strategy for a class of systems modeled by a coupled $2\times 2$ hyperbolic PDE-ODE structure, subject to fully distributed disturbances throughout the system. A state-feedback controller is developed by the {nonovershooting backstepping} method to simultaneously achieve exponential output regulation and enforce safety constraints on the system output, which is the state furthest from the control input. To handle unmeasured PDE states and external disturbances, a state observer and a disturbance estimator are designed. Explicit bounds on the estimation errors are derived and used to construct a robust safe regulator that accounts for the uncertainties. The proposed control scheme guarantees that: 1) If the system output is initially within the safe region, it remains there; otherwise, it will be rescued to the safety within a prescribed time; 2) The output tracking error converges to zero exponentially; 3) The observer accurately estimates both the distributed states and external disturbances, with estimation errors converging to zero exponentially; 4) All signals in the closed-loop system remain bounded. The effectiveness of the proposed method is demonstrated through a UAV delivery scenario with a cable-suspended payload, where the payload is regulated to track a desired reference while avoiding collisions with barriers. |
| 2025-12-05 | [UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection](http://arxiv.org/abs/2512.05814v1) | Fubao Zhu, Zhanyuan Jia et al. | Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy. |
| 2025-12-05 | [Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling](http://arxiv.org/abs/2512.05809v1) | Saurav Jha, M. Jehanzeb Mirza et al. | Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney. |
| 2025-12-05 | [SRG/eROSITA-SDSS view on the relation between X-ray and UV emission for quasars](http://arxiv.org/abs/2512.05807v1) | S. A. Prokhorenko, S. Yu. Sazonov et al. | Motivated by the idea of using quasars as standardizable candles for cosmology, we examine the relation between X-ray (at 2 keV, $L_{\rm 2keV}$) and ultraviolet (at 2500 Angstrom, $L_{\rm 2500}$) monochromatic luminosities of quasars using a sample of 2414 X-ray sources from the SRG/eROSITA all-sky survey cross-matched with the Sloan Digital Sky Survey data release 16 quasar catalogue (SDSS DR16Q), at redshifts between 0.5 and 2.5. These objects are bright both in X-rays and in the optical, so that the sample is characterized by nearly 100% statistical completeness. We have developed a new method for determining the $L_{\rm 2keV}-L_{\rm 2500}$ relation, which consistently takes into account (i) X-ray and UV flux limited object selection, (ii) X-ray and UV variability of quasars, and (iii) the decreasing space density of quasars with increasing luminosity. Assuming a linear relation between $l_{\rm X}\equiv\log(L_{\rm 2keV}/[{\rm erg\,s^{-1}\,Hz^{-1}}])$ and $l_{\rm UV}\equiv\log(L_{\rm 2500}/[{\rm erg\,s^{-1}\,Hz^{-1}}])$, we find the slope, $Œ≥=0.69\pm0.02$ (hereafter all uncertainties are quoted at the 68% confidence level), and normalization, $l_{\rm X}=26.45\pm0.02$ at $l_{\rm UV}=30.5$, of the $L_{\rm 2keV}$ ($L_{\rm 2500}$) dependence. These values are not substantially different from the results of previous studies. A key novel aspect of our work is allowance for intrinsic scatter (which adds to the dispersion induced by quasar variability and flux measurement uncertainties) of the $L_{\rm 2keV}-L_{\rm 2500}$ relation in both variables, i.e. in X-ray and UV luminosity. The intrinsic X-ray scatter ($œÉ^2_{\rm intX}=0.066\pm0.005$) strongly dominates over the UV one ($œÉ^2_{\rm intUV}=0.001^{+0.003}_{-0.001}$). Further studies should seek to explain this behaviour in terms of accretion onto supermassive black holes and orientation of quasars with respect to the observer. |
| 2025-12-04 | [Measurement of the branching fractions and longitudinal polarisations of $B^0_{(s)} \to K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}$ decays](http://arxiv.org/abs/2512.05102v1) | LHCb collaboration, R. Aaij et al. | A time- and flavour-integrated amplitude analysis of $B^0$ and $B^0_s$ decays to the $(K^+œÄ^-)(K^-œÄ^+)$ final state in the $K^*(892)^0 \kern 0.18em \overline{\kern -0.18em K}{}^{*}(892)^0$ region is presented, using $pp$ collision data recorded with the LHCb detector in 2011--2018, corresponding to an integrated luminosity of $9\,\text{fb}^{-1}$. The branching fractions of the $B^0$ and $B^0_s$ decays are measured relative to the $B^0 \to D^-œÄ^+$ and $B^0_s \to D^-_s œÄ^+$ modes, respectively. The corresponding longitudinal polarisation fractions are found to be $f_L^{d} = 0.600 \pm 0.022 \pm 0.017$ and $f_L^{s} = 0.159 \pm 0.010 \pm 0.007$, where the uncertainties are statistical and systematic, respectively. The theory-motivated $L_{K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}}$ observable is found to be $L_{K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}} = 4.92 \pm 0.55 \pm 0.47 \pm 0.02 \pm 0.10$, where the uncertainties are statistical, systematic, due to uncertainty of external mass and lifetime measurements, and due to knowledge of the fragmentation fraction ratio, respectively. This confirms the previously reported tension between experimental determinations and theoretical predictions of longitudinal polarisation in $B \to VV$ decays. |
| 2025-12-04 | [PSR J0952-0607: Tightening a Record-High Neutron Star Mass](http://arxiv.org/abs/2512.05099v1) | Roger W. Romani, Maya Beleznay et al. | We report on new orbit-minimum photometry and revised radial-velocity fitting that provide an improved measurement of the mass of the neutron star (NS) in pulsar PSR~J0952$-$0607 at $M_NS = 2.35\pm 0.11 M_\odot$. With its fast spin and unusually low magnetic field, this NS has evidently experienced unusual evolution, likely connected with its high mass, which is now $2.5œÉ$ above that of the heaviest pulsar with a white dwarf companion, as measured by Shapiro delay techniques. By tightening the mass measurement, we also raise the maximum (commonly called Tolman-Oppenheimer-Volkoff) NS mass to $M_{\rm TOV} > 2.27\,M_\odot$$(2.12\,M_\odot)$ at $1œÉ$$(3œÉ)$ confidence, which improves bounds on the dense-matter equation of state. While the statistical error decreases and systematic issues should be modest, uncertainties remain; we comment briefly on these factors and prospects for further improvement. |
| 2025-12-04 | [Model-Free Assessment of Simulator Fidelity via Quantile Curves](http://arxiv.org/abs/2512.05024v1) | Garud Iyengar, Yu-Shiou Willy Lin et al. | Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs. |
| 2025-12-04 | [Internal superfluid response and torque evolution in the giant glitch of PSR J1718-3718](http://arxiv.org/abs/2512.04972v1) | Peng Liu, Zhonghao Tu et al. | We investigate the post-glitch rotational evolution of pulsars by analyzing the 2007 giant glitch of PSR J1718$-$3718 using a vortex creep model that incorporates both inward and outward nonlinear vortex motion, along with a time-varying external torque. A comprehensive fitting framework is developed, constrained by prior knowledge of moment of inertia participation from previous glitch studies. We apply a Markov Chain Monte Carlo approach to quantify uncertainties and parameter correlations. The model reproduces the observed timing data and yields physically consistent values for moment of inertia fractions and creep timescales. Our results indicate that inward creep and a long-term change in external torque dominate the observed increase in spin-down rate, pointing to structural changes within the star-likely triggered by a crustquake that initiated both vortex motion and a change in the moment of inertia. We estimate that the glitch involved approximately $2.4 \times 10^{12}$ inward-moving vortices and $\sim 142$ crustal plates with a typical size of $\sim 0.03$ km. This study demonstrates that detailed post-glitch modeling of sparse timing data can simultaneously constrain internal superfluid dynamics and external torque evolution, providing a quantitative framework to probe the structural properties of neutron star interiors. |
| 2025-12-04 | [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](http://arxiv.org/abs/2512.04938v1) | Raquel Norel, Michele Merler et al. | Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally. |
| 2025-12-04 | [First observation and measurement of the ${}^{198}\text{Hg}$ bosonic transition in an optical lattice clock](http://arxiv.org/abs/2512.04920v1) | Clara Zyskind, Thomas Laupr√™tre et al. | We report the first observation of the magnetic-field-induced (5d10 6s2)1S0-(5d10 6s6p)3P0 transition in a bosonic isotope of mercury, 198Hg, realized in an optical lattice clock. We characterize this new isotope, determining key features such as the quadratic Zeeman shift, the probe light shift, and the magic frequency. We also report a first comparison between the 198Hg optical lattice clock and 87Sr. In this comparison, the 198Hg clock has a relative frequency stability of 6x10-16/sqrt(tau/s) and a total relative systematic uncertainty of 6.9x10-16. This comparison yields the first direct determination of the 198Hg/87Sr optical frequency ratio: 198Hg/87Sr = 2.629 315 734 684 118 1, with the same relative uncertainty. |
| 2025-12-04 | [Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty](http://arxiv.org/abs/2512.04918v1) | Kailiang Liu, Ying Chen et al. | Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling. |
| 2025-12-04 | [Bayesian stepwise estimation of qubit rotations](http://arxiv.org/abs/2512.04898v1) | Mylenne Manrique, Marco Barbieri et al. | This work investigates Bayesian stepwise estimation (Se) for measuring the two parameters of a unitary qubit rotation. While asymptotic analysis predicts a precision advantage for SE over joint estimation (JE) in regimes where the quantum Fisher information matrix is near-singular ("sloppy" models), we demonstrate that this advantage is mitigated within a practical Bayesian framework with limited resources. We experimentally implement a SE protocol using polarisation qubits, achieving uncertainties close to the classical Van Trees bounds. However, comparing the total error to the ultimate quantum Van Trees bound for JE reveals that averaging over prior distributions erases the asymptotic SE advantage. Nevertheless, the stepwise strategy retains a significant practical benefit as it operates effectively with simple, fixed measurements, whereas saturating the JE bound typically requires complex, parameter-dependent operations. |
| 2025-12-04 | [Searching for new physics with $^{136}$Xe double beta decay spectrum in PandaX-4T](http://arxiv.org/abs/2512.04849v1) | PandaX Collaboration, Zhe Yuan et al. | The continuous spectrum of double beta decay ($Œ≤Œ≤$) provides a sensitive probe to test the predictions of the Standard Model and to search for signatures of new physics beyond it. We present a comprehensive analysis of the $^{136}$Xe $Œ≤Œ≤$ spectrum utilizing $37.8 \pm 0.6$ kg$\cdot$yr of $^{136}$Xe exposure from the PandaX-4T experiment. The analysis yields the most precise measurement to date of the $^{136}$Xe two-neutrino double beta decay ($2ŒΩŒ≤Œ≤$) half-life, $(2.14 \pm 0.05) \times 10^{21}$ years, the uncertainty of which is reduced by a factor of two compared to our previous result. We measure the parameter $Œæ_{31}^{2ŒΩ}$, defined as the ratio between the subleading and leading components of the $^{136}$Xe $2ŒΩŒ≤Œ≤$ nuclear matrix element, to be $0.59^{+0.41}_{-0.38}$, which is consistent with theoretical predictions. We also search for Majoron-emitting modes of $^{136}$Xe $Œ≤Œ≤$, establishing the most stringent limit for the spectral index $n=7$. |
| 2025-12-04 | [aim-resolve: Automatic Identification and Modeling for Bayesian Radio Interferometric Imaging](http://arxiv.org/abs/2512.04840v1) | Richard Fuchs, Jakob Knollm√ºller et al. | Modern radio interferometers deliver large volumes of data containing high-sensitivity sky maps over wide fields-of-view. These large area observations can contain various and superposed structures such as point sources, extended objects, and large-scale diffuse emission. To fully realize the potential of these observations, it is crucial to build appropriate sky emission models which separate and reconstruct the underlying astrophysical components. We introduce aim-resolve, an automatic and iterative method that combines the Bayesian imaging algorithm resolve with deep learning and clustering algorithms in order to jointly solve the reconstruction and source extraction problem. The method identifies and models different astrophysical components in radio observations while providing uncertainty quantification of the results. By using different model descriptions for point sources, extended objects, and diffuse background emission, the method efficiently separates the individual components and improves the overall reconstruction. We demonstrate the effectiveness of this method on synthetic image data containing multiple different sources. We further show the application of aim-resolve to an L-band (856 - 1712 MHz) MeerKAT observation of the radio galaxy ESO 137-006 and other radio galaxies in that environment. We observe a reasonable object identification for both applications, yielding a clean separation of the individual components and precise reconstructions of point sources and extended objects along with detailed uncertainty quantification. In particular, the method enables the creation of catalogs containing source positions and brightnesses and the corresponding uncertainties. The full decoupling of sky emission model and instrument response makes the method applicable to a wide variety of instruments or wavelength bands. |
| 2025-12-03 | [The effect of baryons on the positions and velocities of satellite galaxies in the MTNG simulation](http://arxiv.org/abs/2512.04080v1) | Sergio Contreras, Raul E. Angulo et al. | Mock galaxy catalogues are often constructed from dark-matter-only simulations based on the galaxy-halo connection. Although modern mocks can reproduce galaxy clustering to some extent, the absence of baryons affects the spatial and kinematic distributions of galaxies in ways that remain insufficiently quantified. We compare the positions and velocities of satellite galaxies in the MTNG hydrodynamic simulation with those in its dark-matter-only counterpart, assessing how baryonic effects influence galaxy clustering and contrasting them with the impact of galaxy selection, i.e. the dependence of clustering on sample definition. Using merger trees from both runs, we track satellite subhaloes until they become centrals, allowing us to match systems even when their z=0 positions differ. We then compute positional and velocity offsets as functions of halo mass and distance from the halo centre, and use these to construct a subhalo catalogue from the dark-matter-only simulation that reproduces the galaxy distribution in the hydrodynamic run. Satellites in the hydrodynamic simulation lie 3-4% closer to halo centres than in the dark-matter-only case, with an offset that is nearly constant with halo mass and increases toward smaller radii. Satellite velocities are also systematically higher in the dark-matter-only run. At scales of 0.1 Mpc/h, these spatial and kinematic differences produce 10-20% variations in clustering amplitude -corresponding to 1-3$œÉ$ assuming DESI-like errors- though the impact decreases at larger scales. These baryonic effects are relevant for cosmological and lensing analyses and should be accounted for when building high-fidelity mocks. However, they remain smaller than the differences introduced by galaxy selection, which thus represents the dominant source of uncertainty when constructing mocks based on observable quantities. |
| 2025-12-03 | [Learning Steerable Clarification Policies with Collaborative Self-play](http://arxiv.org/abs/2512.04068v1) | Jonathan Berant, Maximillian Chen et al. | To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time. |
| 2025-12-03 | [On topological and algebraic structures of categorical random variables](http://arxiv.org/abs/2512.04020v1) | Inocencio Ortiz, Santiago G√≥mez-Guerrero et al. | Based on entropy and symmetrical uncertainty (SU), we define a metric for categorical random variables and show that this metric can be promoted into an appropriate quotient space of categorical random variables. Moreover, we also show that there is a natural commutative monoid structure in the same quotient space, which is compatible with the topology induced by the metric, in the sense that the monoid operation is continuous. |
| 2025-12-03 | [Physics-Embedded Gaussian Process for Traffic State Estimation](http://arxiv.org/abs/2512.04004v1) | Yanlin Chen, Kehua Chen et al. | Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE. |
| 2025-12-03 | [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](http://arxiv.org/abs/2512.03992v1) | Zexin Lin, Hawen Wan et al. | Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments. |
| 2025-12-03 | [Applied Neural Network-Based Active Control for Vortex-Induced Vibrations Suppression in a Two-Degree-of-Freedom Cylinder](http://arxiv.org/abs/2512.03990v1) | Soha Ilbeigi, Ashkan Bagherzadeh et al. | Vortex-Induced Vibrations (VIVs) of cylindrical structures present significant challenges in various engineering applications, including marine risers, tall buildings, and renewable energy systems. Hence, it is vital to control Vortex-Induced Vibrations of cylindrical structures. For this purpose, in this study a novel approach is introduced to VIV control, based on a model-based active control strategy integrated with a Neural Network (NN) in the presence of uncertainty modeling. The proposed method utilizes a closed-loop control system, where feedback from the system's dynamic state is used to generate adaptive control commands, enabling the system to respond to changing flow conditions and nonlinearities. Then, the controllability analysis is conducted to assess the efficiency of the control strategy in mitigating VIV. Two control approaches are implemented: simple learning and composite learning. Both strategies significantly enhance vibration suppression, achieving up to 99% reduction in vibrations despite uncertainties in the system. The results demonstrate the potential of the proposed method to enhance the efficiency, stability, and lifespan of structures subject to VIV. |
| 2025-12-03 | [Performance and efficiency of a transformer-based quark/gluon jet tagger in the ATLAS experiment](http://arxiv.org/abs/2512.03949v1) | ATLAS Collaboration | A deep-learning approach based on the transformer architecture is developed to distinguish between jets originating from quarks and gluons. The algorithm operates on jets with transverse momentum $p_{\text{T}} > 20$ and pseudorapidity $|Œ∑| < 4.5$ and takes as input several properties derived from the jet constituents, using information from the ATLAS detector's tracker and calorimeter. The algorithm's performance is evaluated by analyzing dijet data events from proton-proton collisions at $\sqrt{s} = 13$ and $13.6$ TeV during Run 2 and Run 3 of the Large Hadron Collider. Two methods are used to obtain distributions from quark- or gluon-initiated jets in data: a matrix method fully based on Monte Carlo simulation and a new approach named `jet topics' which has less dependence on the modelling of the physics process under study. The quark and gluon identification efficiencies measured in data for the 50% quark-identification-efficiency working point vary from the simulated ones for quark-initiated (gluon-initiated) jets by factors of 0.88-1.30 (0.61-1.05) with uncertainties of 10%-70% (10%-95%). The uncertainties estimated with the jet topics method are smaller than those estimated with the matrix method, with up to 20% less systematic uncertainty in some phase-space regions. The advances in jet identification reported here provide a robust tool for precision Standard Model measurements and searches for new physics at the LHC. |
| 2025-12-03 | [Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response](http://arxiv.org/abs/2512.03936v1) | Aron Distelzweig, Yiwei Wang et al. | Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks. |
| 2025-12-03 | [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](http://arxiv.org/abs/2512.03928v1) | Michele Alessi, Alessio Ansuini et al. | We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors. |
| 2025-12-03 | [Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing](http://arxiv.org/abs/2512.03925v1) | David Ribes, Tatiana Gonzalez Grandon | Uncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited. |
| 2025-12-02 | [Radiative decays of the $Œ£_c$, $Œû'_c$ and $Œ©_c$ charmed baryons](http://arxiv.org/abs/2512.03008v1) | A. D√°vila-Rivera, H. Garc√≠a-Tecocoatzi et al. | In this work, we study the radiative decays of the $Œ£_c$, $Œû'_c$ and $Œ©_c$ charmed baryons, which belong to the flavor sextet ($\bf {6}_{\rm F}$), within the constituent quark model formalism. The electromagnetic transitions are calculated from the ground and $P$-wave states to ground states, as well as from the second shell states to both the ground and $P$-wave final states. These decays play a crucial role in confirming the existence of certain resonances when strong decays are not allowed. %A relevant case is the $Œ©_c^{0}$ spin excitation with $\bf J^P ={\frac{3}{2}}^{+}$ denoted as $Œ©_c^{*0}$. This state cannot decay strongly, but has a nonvanishing predicted electromagnetic decay width in the $Œ©_c^{0} Œ≥$ channel. The $Œ©_c^{*0}$ was precisely observed in this channel. Moreover, electromagnetic decay widths are particularly useful for identifying resonances when states have the same mass and total decay width. A relevant case is the $Œ©_c (3327)$ state, whose branching ratios between the strong decay channels are comparable; thus, the radiative decay widths may help assign this state. We also make the assignment of the recently discovered $Œû'_c(2923)^{+}$ baryon, which is consistent with being the isospin partner of the $Œû'_c(2923)^{0}$. This study presents, for the first time, the calculation of electromagnetic decays for $D_œÅ$-wave states, $œÅ-Œª$ mixed states, and $œÅ$-mode radially excited states in the charm sector. Throughout our calculations, we account for uncertainties arising from both experimental and model-dependent errors. |
| 2025-12-02 | [MIRI spectrophotometry of GN-z11: Detection and nature of an optical red continuum component](http://arxiv.org/abs/2512.02997v1) | A. Crespo G√≥mez, L. Colina et al. | We present new MIRI F560W, F770W and F1000W imaging of GN-z11, extending the previous rest-frame coverage from 0.38 to 0.86$Œº$m. We report significant detections (14$œÉ$) in the F560W and F770W images, and a marginal detection (3.2$œÉ$) in F1000W. Here, we analyse its SED combining new MIRI imaging data with archival NIRSpec/Prism and MRS spectroscopy, and NIRCam imaging. The continuum emission shows a flat energy distribution, in f$_ŒΩ$, up to 0.5$Œº$m, compatible with the presence of a mixed stellar population of young (4$\pm$1 Myr) and mature (63$\pm$23 Myr) stars that also account for the [O III], H$Œ≤$ and H$Œ±$ emission lines. The continuum at rest-frame 0.66$Œº$m shows a 36$\pm$3% flux excess above the predicted flux for a mixed stellar population, pointing to the presence of an additional source contributing at these wavelengths. This excess increases to 91$\pm$28% at rest-frame 0.86$Œº$m, although with a large uncertainty due to the marginal detection in the F1000W filter. We consider that hot dust emission in the dusty torus around a type 2 AGN could be responsible for the observed excess. Alternatively, this excess could be due to hot dust emission or to a photoluminiscence dust process (Extended Red Emission, ERE) under the extreme UV radiation field, as observed in some local metal-poor galaxies and in young compact starbursts. The presence of a type 1 AGN is not supported by the observed SED since high-z QSOs contribute at wavelengths above rest-frame 1$Œº$m, and an additional ad-hoc red source would be required to explain the observed flux excess at 0.66 and 0.86$Œº$m. Additional deep MIRI imaging covering the rest-frame near-IR are needed to confirm the flux detection at 10$Œº$m with higher significance, and to discriminate between the different hot dust emission in the extreme starburst and AGN scenarios with MIRI imaging at longer wavelengths. |
| 2025-12-02 | [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2512.02982v1) | Xiang Xu, Ao Liang et al. | Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation. |
| 2025-12-02 | [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](http://arxiv.org/abs/2512.02981v1) | Zhongyu Yang, Yingfang Yuan et al. | Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness. |
| 2025-12-02 | [Hypothesis Testing for Generalized Thurstone Models](http://arxiv.org/abs/2512.02912v1) | Anuran Makur, Japneet Singh | In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $Œò((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets. |
| 2025-12-02 | [Statistical-Symbolic Verification of Perception-Based Autonomous Systems using State-Dependent Conformal Prediction](http://arxiv.org/abs/2512.02893v1) | Yuang Geng, Thomas Waite et al. | Reachability analysis has been a prominent way to provide safety guarantees for neurally controlled autonomous systems, but its direct application to neural perception components is infeasible due to imperfect or intractable perception models. Typically, this issue has been bypassed by complementing reachability with statistical analysis of perception error, say with conformal prediction (CP). However, existing CP methods for time-series data often provide conservative bounds. The corresponding error accumulation over time has made it challenging to combine statistical bounds with symbolic reachability in a way that is provable, scalable, and minimally conservative. To reduce conservatism and improve scalability, our key insight is that perception error varies significantly with the system's dynamical state. This article proposes state-dependent conformal prediction, which exploits that dependency in constructing tight high-confidence bounds on perception error. Based on this idea, we provide an approach to partition the state space, using a genetic algorithm, so as to optimize the tightness of conformal bounds. Finally, since using these bounds in reachability analysis leads to additional uncertainty and branching in the resulting hybrid system, we propose a branch-merging reachability algorithm that trades off uncertainty for scalability so as to enable scalable and tight verification. The evaluation of our verification methodology on two complementary case studies demonstrates reduced conservatism compared to the state of the art. |
| 2025-12-02 | [Discovery and rectification of an error in high resistance traceability at NPL: a case study in how metrology works](http://arxiv.org/abs/2512.02887v1) | Stephen Giblin | We broach a seldom-discussed topic in precision metrology; how subtle errors in calibration processes are discovered and remedied. We examine a case study at the National Physical Laboratory (NPL), UK, involving the calibration of DC standard resistors of value 100 MOhm and 1 GOhm. Results from the period 2001 to 2015 were in error by approximately 0.7 parts per million (ppm), with quoted uncertainties (k=2) of 0.4 ppm and 1.6 ppm respectively. Inter-comparisons did not detect the error, mainly because the uncertainty due to the transportation drift of the comparison standards was too large to resolve it. Likewise, research into single-electron current standards did not detect the error because at this resistance value it was on the borderline of statistical significance. The key event was a comparison between PTB (Germany) and NPL (UK) of a new small-current measuring instrument, the ultrastable low-noise current amplifier (ULCA). At that time, the transport stability of the ULCA was not well established. Nevertheless, calibrations of the ULCA at NPL using a 100 MOhm resistor were sufficiently discrepant with the PTB calibrations to motivate a thorough investigation into the NPL traceability chain, which uncovered the error. This instructive episode illustrates a positive interplay between calibration and research activities and shows that cutting-edge calibration uncertainties must be supported by a vigorous research programme. It is also important for NMIs to maintain a comfortable buffer (at least a factor of 10) between their claimed uncertainty and the uncertainty that their customers require, so that small errors can be resolved without significant impact on measurement stakeholders. |
| 2025-12-02 | [Insights into Extragalactic Background Light constraints with MAGIC archival data](http://arxiv.org/abs/2512.02880v1) | R. Grau, A. Moralejo | The Extragalactic Background Light (EBL) is the accumulated light emitted throughout the history of the universe, spanning the UV, optical, and IR spectral ranges. Stars and dust in galaxies are expected to be the main source of the EBL. However, recent direct measurements performed beyond Pluto's orbit (less affected by foregrounds than those performed from the Earth) hint at an EBL level in the optical band larger than the one expected from the integrated contribution of known galaxy populations. One approach that could solve this controversy uses Very High Energy (VHE) photons coming from sources at cosmological distances. These photons can interact with the EBL producing electron-positron pairs, a process that leaves an imprint on the observed VHE spectrum. This technique, however, requires assumptions on the intrinsic spectrum of the source, which can compromise the robustness of EBL constraints. In this contribution, we used Monte Carlo simulations and archival data of the MAGIC telescopes to study the impact that the assumptions adopted in the literature have on the estimate of the EBL density, and how using more generic ones would modify the results. Our results show how the EBL density constraints obtained highly depend on the intrinsic spectral shape assumed for the source. We have studied two different methods to reduce the assumptions on the intrinsic spectral shape to get more robust results. This will be especially important for upcoming with new VHE facilities, where systematic uncertainties are expected to play a more significant role compared to statistical ones. |
| 2025-12-02 | [The future of AI in critical mineral exploration](http://arxiv.org/abs/2512.02879v1) | Jef Caers | The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage |
| 2025-12-02 | [Correcting for sampling variability in maximum likelihood-based one-sample log-rank tests](http://arxiv.org/abs/2512.02878v1) | Moritz Fabian Danzer, Rene Schmidt | Single-arm studies in the early development phases of new treatments are not uncommon in the context of rare diseases or in paediatrics. If an assessment of efficacy is to be made at the end of such a study, the observed endpoints can be compared with reference values that can be derived from historical data. For a time-to-event endpoint, a statistical comparison with a reference curve can be made using the one-sample log-rank test. In order to ensure the interpretability of the results of this test, the role of the reference curve is crucial. This quantity is often estimated from a historical control group using a parametric procedure. Hence, it should be noted that it is subject to estimation uncertainty. However, this aspect is not taken into account in the one-sample log-rank test statistic. We analyse this estimation uncertainty for the common situation that the reference curve is estimated parametrically using the maximum likelihood method, and indicate how the variance estimation of the one-sample log-rank test can be adapted in order to take this variability into account. The resulting test procedures are illustrated using a data example and analysed in more detail using simulations. |
| 2025-12-01 | [Fault-tolerant mutual-visibility: complexity and solutions for grid-like networks](http://arxiv.org/abs/2512.01978v1) | Serafino Cicerone, Gabriele Di Stefano et al. | Networks are often modeled using graphs, and within this setting we introduce the notion of $k$-fault-tolerant mutual visibility. Informally, a set of vertices $X \subseteq V(G)$ in a graph $G$ is a $k$-fault-tolerant mutual-visibility set ($k$-ftmv set) if any two vertices in $X$ are connected by a bundle of $k+1$ shortest paths such that: ($i$) each shortest path contains no other vertex of $X$, and ($ii$) these paths are internally disjoint. The cardinality of a largest $k$-ftmv set is denoted by $\mathrm{f}Œº^{k}(G)$. The classical notion of mutual visibility corresponds to the case $k = 0$.   This generalized concept is motivated by applications in communication networks, where agents located at vertices must communicate both efficiently (i.e., via shortest paths) and confidentially (i.e., without messages passing through the location of any other agent). The original notion of mutual visibility may fail in unreliable networks, where vertices or links can become unavailable.   Several properties of $k$-ftmv sets are established, including a natural relationship between $\mathrm{f}Œº^{k}(G)$ and $œâ(G)$, as well as a characterization of graphs for which $\mathrm{f}Œº^{k}(G)$ is large. It is shown that computing $\mathrm{f}Œº^{k}(G)$ is NP-hard for any positive integer $k$, whether $k$ is fixed or not. Exact formulae for $\mathrm{f}Œº^{k}(G)$ are derived for several specific graph topologies, including grid-like networks such as cylinders and tori, and for diameter-two networks defined by Hamming graphs and by the direct product of complete graphs. |
| 2025-12-01 | [AI-Driven Optimization under Uncertainty for Mineral Processing Operations](http://arxiv.org/abs/2512.01977v1) | William Xu, Amir Eskanlou et al. | The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware. |
| 2025-12-01 | [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](http://arxiv.org/abs/2512.01975v1) | Xu Zhang, Jin Yuan et al. | Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input. |
| 2025-12-01 | [Bounded treewidth, multiple context-free grammars, and downward closures](http://arxiv.org/abs/2512.01973v1) | C. Aiswarya, Pascal Baumann et al. | The reachability problem in multi-pushdown automata (MPDA) has many applications in static analysis of recursive programs. An example is safety verification of multi-threaded recursive programs with shared memory. Since these problems are undecidable, the literature contains many decidable (and efficient) underapproximations of MPDA.   A uniform framework that captures many of these underapproximations is that of bounded treewidth (tw): To each execution of the MPDA, we associate a graph; then we consider the subset of all graphs that have a wt at most $k$, for some constant $k$. In fact, bounding tw is a generic approach to obtain classes of systems with decidable reachability, even beyond MPDA underapproximations. The resulting systems are also called MSO-definable bounded-tw systems.   While bounded tw is a powerful tool for reachability and similar types of analysis, the word languages (i.e. action sequences corresponding to executions) of these systems remain far from understood.   For the slight restriction of bounded special tw, or "bounded-stw" (which is equivalent to bounded tw on MPDA, and even includes all bounded-tw systems studied in the literature), this work reveals a connection with multiple context-free languages (MCFL), a concept from computational linguistics. We show that the word languages of MSO-definable bounded-stw systems are exactly the MCFL.   We exploit this connection to provide an optimal algorithm for computing downward closures (dcl) for MSO-definable bounded-stw systems. Computing dcl is a notoriously difficult task that has many applications in the verification of complex systems: As an example application, we show that in programs with dynamic spawning of MSO-definable bounded-stw processes, safety verification has the same complexity as in the case of processes with sequential recursive processes. |
| 2025-12-01 | [Towards Precision Gluon Densities at Small $x$: From Resummation to Collider Observables](http://arxiv.org/abs/2512.01961v1) | Francesco Giovanni Celiberto, Marco Bonvini | Accurate gluon densities at small $x$ are essential for reducing theoretical uncertainties in collider predictions, yet remain one of the least constrained ingredients in global analyses. We report recent advances in the resummation of small-$x$ logarithms in the gluon sector, focusing on collinear distributions and their interplay with transverse-momentum-dependent formulations. Particular attention is paid to the impact of gluon-proton spin correlations and to the emergence of unintegrated gluon densities derived from resummed dynamics. These developments aim to fill the current precision gap in the small-$x$ region and enable robust applications to LHC and future-collider observables. |
| 2025-12-01 | [Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model](http://arxiv.org/abs/2512.01924v1) | Kentaro Fujii, Shingo Murata | Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions. |
| 2025-12-01 | [Uncertainty quantification in load profiles with rising EV and PV adoption: the case of residential, industrial, and office buildings](http://arxiv.org/abs/2512.01914v1) | Aiko Fias, Md Umar Hashmi et al. | The integration of photovoltaic (PV) generation and electric vehicle (EV) charging intro- duces significant uncertainty in electricity consumption patterns, particularly at the distribution level. This paper presents a comparative study for selecting metrics for uncertainty quantification (UQ) for net load profiles of residential, industrial, and office buildings under increased DER penetration. A variety of statistical metrics is evaluated for their usefulness in quantifying un- certainty, including, but not limited to, standard deviation, entropy, ramps, and distance metrics. The proposed metrics are classified into baseline-free, with baseline and error-based. These UQ metrics are evaluated for increased penetration of EV and PV. The results highlight suitable metrics to quantify uncertainty per consumer type and demonstrate how net load uncertainty is affected by EV and PV adoption. Additionally, it is observed that joint consideration of EV and PV can reduce overall uncertainty due to compensatory effects of EV charging and PV generation due to temporal alignment during the day. Uncertainty reduction is observed across all datasets and is most pronounced for the office building dataset. |
| 2025-12-01 | [NeuroHJR: Hamilton-Jacobi Reachability-based Obstacle Avoidance in Complex Environments with Physics-Informed Neural Networks](http://arxiv.org/abs/2512.01897v1) | Granthik Halder, Rudrashis Majumder et al. | Autonomous ground vehicles (AGVs) must navigate safely in cluttered environments while accounting for complex dynamics and environmental uncertainty. Hamilton-Jacobi Reachability (HJR) offers formal safety guarantees through the computation of forward and backward reachable sets, but its application is hindered by poor scalability in environments with numerous obstacles. In this paper, we present a novel framework called NeuroHJR that leverages Physics-Informed Neural Networks (PINNs) to approximate the HJR solution for real-time obstacle avoidance. By embedding system dynamics and safety constraints directly into the neural network loss function, our method bypasses the need for grid-based discretization and enables efficient estimation of reachable sets in continuous state spaces. We demonstrate the effectiveness of our approach through simulation results in densely cluttered scenarios, showing that it achieves safety performance comparable to that of classical HJR solvers while significantly reducing the computational cost. This work provides a new step toward real-time, scalable deployment of reachability-based obstacle avoidance in robotics. |
| 2025-12-01 | [Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets](http://arxiv.org/abs/2512.01888v1) | Adrienne M. Propp, Mauro Perego et al. | Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics. |
| 2025-12-01 | [First detections of methanol maser lines from a rare transition family](http://arxiv.org/abs/2512.01875v1) | Bradley R. Johnson, Simon P. Ellingsen et al. | We report the first observations in a rare family of class II methanol maser transitions in both CH$_3$OH and $^{13}$CH$_3$OH toward three southern high-mass star formation regions, along with the first maser detected in the $^{13}$CH$_3$OH line. The $8_2 \rightarrow 9_1 A^{-}$ methanol transition was observed in both CH$_3$OH and $^{13}$CH$_3$OH (at 28.9 GHz and 41.9 GHz respectively) toward three sources; G358.93-0.03, NGC6334I and G345.01+1.79, all of which are star formation regions with recent maser flaring events. We report the first maser detection of the 41.9 GHz line in $^{13}$CH$_3$OH toward G358.93-0.03 and the first confirmed maser detection of the 28.9 GHz line in CH$_3$OH toward NGC6334I. Additionally we report a maser detection of the 28.9 GHz line in CH$_3$OH toward G358.93-0.03, meaning that with our detection of the 41.9 GHz line, this is the first isotopic detection of these lines toward G358.93-0.03. The newly detected maser transitions are associated with the primary millimetre continuum sources (MM1) in both G358.93-0.03 and NGC6334I, within the varying positional uncertainties. |
| 2025-11-28 | [Epistemic and Aleatoric Uncertainty Quantification in Weather and Climate Models](http://arxiv.org/abs/2511.23448v1) | Laura A. Mansfield, Hannah M. Christensen | Representing and quantifying uncertainty in physical parameterisations is a central challenge in weather and climate modelling, and approaches are often developed separately for different timescales. Here, we introduce a unified framework for analysing uncertainty in parameterisations across weather and climate regimes. Using the Lorenz 1996 system as a testbed for simplified chaotic dynamics, we quantify uncertainties in a subgrid-scale parameterisation using a Bayesian Neural Network (BNN). This allows us to disentangle aleatoric uncertainty, arising from internal variability in the training data, and epistemic uncertainties, arising from poorly constrained parameters during training. At runtime, we sample uncertainties in line with stochastic approaches in weather models and perturbed-parameter methods in climate models. On weather timescales, aleatoric uncertainty dominates, underscoring the value of stochastic parameterisations. On longer, climate timescales and under changing forcings, accounting for both types of uncertainty is necessary for well-calibrated ensembles, with epistemic uncertainty widening the range of explored climate states, and aleatoric uncertainty promoting transitions between them. Constraining parameter uncertainty with short simulations reduces epistemic uncertainty and improves long-term model behaviour under perturbed forcings. This framework links concepts from machine learning with traditional uncertainty quantification in Earth system modelling, offering a pathway toward seamless treatment of uncertainty in weather and climate prediction. |
| 2025-11-28 | [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](http://arxiv.org/abs/2511.23440v1) | Bernhard Klein, Falk Selker et al. | Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems. |
| 2025-11-28 | [Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets](http://arxiv.org/abs/2511.23433v1) | Maria Alejandra Valdez Cabrera, Amy D Willis et al. | Connected acyclic graphs (trees) are data objects that hierarchically organize categories. Collections of trees arise in a diverse variety of fields, including evolutionary biology, public health, machine learning, social sciences and anatomy. Summarizing a collection of trees by a single representative is challenging, in part due to the dimension of both the sample and parameter space. We frame consensus tree estimation as a structured feature-selection problem, where leaves and edges are the features. We introduce a partial order on leaf-labeled trees, use it to define true and false discoveries for a candidate summary tree, and develop an estimation algorithm that controls the false discovery rate at a nominal level for a broad class of non-parametric generative models. Furthermore, using the partial order structure, we assess the stability of each feature in a selected tree. Importantly, our method accommodates unequal leaf sets and non-binary trees, allowing the estimator to reflect uncertainty by collapsing poorly supported structure instead of forcing full resolution. We apply the method to study the archaeal origin of eukaryotic cells and to quantify uncertainty in deep branching orders. While consensus tree construction has historically been viewed as an estimation task, reframing it as feature selection over a partially ordered set allows us to obtain the first estimator with finite-sample and model-free guarantees. More generally, our approach provides a foundation for integrating tools from multiple testing into tree estimation. |
| 2025-11-28 | [From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products](http://arxiv.org/abs/2511.23407v1) | Jan Baumg√§rtner, Malte Hansjosten et al. | To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts. |
| 2025-11-28 | [Bounded-Error Quantum Simulation via Hamiltonian and Lindbladian Learning](http://arxiv.org/abs/2511.23392v1) | Tristan Kraft, Manoj K. Joshi et al. | Analog Quantum Simulators offer a route to exploring strongly correlated many-body dynamics beyond classical computation, but their predictive power remains limited by the absence of quantitative error estimation. Establishing rigorous uncertainty bounds is essential for elevating such devices from qualitative demonstrations to quantitative scientific tools. Here we introduce a general framework for bounded-error quantum simulation, which provides predictions for many-body observables with experimentally quantifiable uncertainties. The approach combines Hamiltonian and Lindbladian Learning--a statistically rigorous inference of the coherent and dissipative generators governing the dynamics--with the propagation of their uncertainties into the simulated observables, yielding confidence bounds directly derived from experimental data. We demonstrate this framework on trapped-ion quantum simulators implementing long-range Ising interactions with up to 51 ions, and validate it where classical comparison is possible. We analyze error bounds on two levels. First, we learn an open-system model from experimental data collected in an initial time window of quench dynamics, simulate the corresponding master equation, and quantitatively verify consistency between theoretical predictions and measured dynamics at long times. Second, we establish error bounds directly from experimental measurements alone, without relying on classical simulation--crucial for entering regimes of quantum advantage. The learned models reproduce the experimental evolution within the predicted bounds, demonstrating quantitative reliability and internal consistency. Bounded-error quantum simulation provides a scalable foundation for trusted analog quantum computation, bridging the gap between experimental platforms and predictive many-body physics. The techniques presented here directly extend to digital quantum simulation. |
| 2025-11-28 | [The Kinematic Properties of T≈ªO Candidate HV 11417 with Gaia DR3](http://arxiv.org/abs/2511.23368v1) | Anna J. G. O'Grady | HV 11417 is a candidate Thorne-≈ªytkow Object, a red supergiant with a neutron star core, located within the Small Magellanic Cloud (SMC). Previous studies have questioned, using Gaia DR2 data, whether HV 11417 was truly located at the distance of the SMC or was instead a foreground star. However, the proper motion measurement uncertainties for HV 11417 in DR2 were high. In this work, we use Gaia DR3 data to show that HV 11417 is very likely to be a true member of the SMC. We further analyze the kinematics of HV 11417 relative to its local environment, and compare it to populations of massive and evolved stars in the SMC. We find HV 11417 has a local transverse velocity of $52\pm15$ km/s, and thus qualifies as a runaway star (v$_\mathrm{loc}\geq$ 30 km/s). This runaway classification does not conclusively prove its nature as a T≈ªO, particularly given results from recent T≈ªO models, but does indicate that HV 11417 experienced a kinematic disruption in its evolution. |
| 2025-11-28 | [Data-driven Reachability Verification with Probabilistic Guarantees under Koopman Spectral Uncertainty](http://arxiv.org/abs/2511.23322v1) | Jianqiang Ding, Shankar A. Deka | Providing rigorous reachability guarantees for unknown complex systems is a crucial and challenging task. In this paper, we present a novel data-driven framework that addresses this challenge by leveraging Koopman operator theory. Instead of operating in the state space, the proposed method encodes model uncertainty from finite data directly into Koopman spectral representation with quantifiable error bounds. Leveraging this spectral information, we systematically determine time intervals within which trajectories from the initial set are guaranteed, with a prescribed probability, to reach the target set. This enables the rigorous reachability verification without explicit computation of reachable sets, thereby offering a significant advantage in scalability and applicability. We finally validate the effectiveness of the proposed framework through case studies on representative dynamical systems. |
| 2025-11-28 | [Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering](http://arxiv.org/abs/2511.23304v1) | Zijian Fu, Changsheng Lv et al. | In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released. |
| 2025-11-28 | [Intrinsic $k_T$ and soft gluons in Monte Carlo event generators](http://arxiv.org/abs/2511.23291v1) | Louis Moureaux, Aleksandra Lelek et al. | Experimental measurements of the transverse momentum of Drell-Yan lepton pairs are sensitive to non-perturbative physics associated with the intrinsic parton transverse momentum $k_T$. We discuss recent determinations of intrinsic $k_T$ in the context of transverse momentum dependent (TMD) parton branching calculations and collinear parton-shower Monte Carlo generators. We illustrate the influence of the soft-gluon resolution scale and the non-perturbative Sudakov region on the intrinsic $k_T$ extraction. We emphasize the relevance of the correct treatment of correlated uncertainties between different transverse momentum bins in TMD fits and present an application to the determination of the intrinsic $k_T$ in the forward rapidity region. |
| 2025-11-28 | [Quark mixing from muon collider neutrinos](http://arxiv.org/abs/2511.23288v1) | David Marzocca, Francesco Montagno et al. | A high energy muon collider naturally produces a collimated beam of neutrinos for a fixed-target experiment at a dedicated far-forward facility. The high intensity and energy of the beam makes it ideally suited for astonishingly precise measurements of neutrino scattering on nucleons in the deeply inelastic regime, enabling the determination of the Cabibbo--Kobayashi--Maskawa~(CKM) quark mixing matrix. We assess the floor to the attainable sensitivity set by irreducible sources of uncertainties from the imperfect knowledge of the parton distribution (PDF) and fragmentation functions, showing that a strong improvement is possible well above current standards. As a by-product, our analysis also outlines extraordinary perspectives for a combined determination of the PDF. The results demonstrate the potential of a parasitic neutrino experiment at the muon collider, motivating detailed future studies. |
| 2025-11-26 | [Uncertainty Quantification for Visual Object Pose Estimation](http://arxiv.org/abs/2511.21666v1) | Lorenzo Shaikewitz, Charis Georgiou et al. | Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets. |
| 2025-11-26 | [Updated bounds on the (1,2) neutrino oscillation parameters after first JUNO results](http://arxiv.org/abs/2511.21650v1) | Francesco Capozzi, Eligio Lisi et al. | Within the standard $3ŒΩ$ framework, we discuss updated bounds on the leading oscillation parameters related to the $(ŒΩ_1,\,ŒΩ_2)$ states, namely, the squared mass difference $Œ¥m^2=m^2_2-m^2_1$ and the mixing parameter $\sin^2Œ∏_{12}$. A previous global analysis of 2024 oscillation data estimated $Œ¥m^2$ and $\sin^2Œ∏_{12}$ with fractional $1œÉ$ errors of about $2.3\%$ and $4.5\%$, respectively. First we update the analysis by applying the latest SNO+ constraints, that slightly shift the $(Œ¥m^2,\,\sin^2Œ∏_{12})$ best fits. Then we apply the constraints placed by the first JUNO results, that significantly reduce the uncertainties of both parameters. Our updated global bounds (as of 2025) can be summarized as: $Œ¥m^2/10^{-5}{\rm eV}^2 = 7.48\pm 0.10$ and $\sin^2Œ∏_{12}=0.3085\pm0.0073$ (with correlation $œÅ=-0.20$), corresponding to $1œÉ$ uncertainties as small as $1.3\%$ and $2.4\%$, respectively. We also comment on minor physical and statistical effects that, in the future, may contribute to lift the current mass-ordering degeneracy of $(Œ¥m^2,\,Œ∏_{12})$ estimates. |
| 2025-11-26 | [Active Learning for GCN-based Action Recognition](http://arxiv.org/abs/2511.21625v1) | Hichem Sahbi | Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work. |
| 2025-11-26 | [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](http://arxiv.org/abs/2511.21607v1) | Zarin Tahia Hossain, Mostafa Milani | Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines. |
| 2025-11-26 | [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](http://arxiv.org/abs/2511.21570v1) | Maria Perez-Ortiz | In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century. |
| 2025-11-26 | [Enhanced antineutrino emission from $Œ≤$ decay in core-collapse supernovae with self-consistent weak decay rates](http://arxiv.org/abs/2511.21567v1) | T. Dasher, A. Ravliƒá et al. | Nuclear weak-interaction rates are known to exert a prominent effect in the late-stages of stellar collapse. Despite their importance, most studies to date on core-collapse supernovae (CCSNe) have focused primarily on the effects of electron captures, generally neglecting $Œ≤$ decay contributions. In this Letter, we present the first CCSNe simulation incorporating global $Œ≤$ decay rates from a microscopic theory. These are enabled by a large-scale evaluation of both electron capture and $Œ≤$ decay rates, obtained self-consistently utilizing the relativistic energy density functional theory and finite-temperature quasiparticle random-phase approximation. We find a significant enhancement of antineutrino emissivity by more than 4 orders of magnitude due to the inclusion of $Œ≤$ decay rates, as well as 3 orders of magnitude for antineutrino luminosity. It is expected that these new rates could help us constrain the model uncertainties related to weak-interaction processes, improving the prediction of antineutrino signal during the final stages of stellar death and potentially influencing the late-stage evolution of massive stars. |
| 2025-11-26 | [A decoupled alignment kernel for peptide membrane permeability predictions](http://arxiv.org/abs/2511.21566v1) | Ali Amirahmadi, G√∂k√ße Geylan et al. | Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics. |
| 2025-11-26 | [Cosmological Implications of the Extended Uncertainty Principle: Energy Conditions, Stability, and Late Time Acceleration](http://arxiv.org/abs/2511.21546v1) | Maryam Roushan, Narges Rashidi | We study the cosmological consequences of the Extended Uncertainty Principle (EUP) by deriving modified Friedmann equations through thermodynamic arguments. The evolution of the effective equation of state induced by EUP corrections is analyzed and characterized using the Chevallier-Polarski-Linder (CPL) parametrization. We then examine the fulfillment of classical energy conditions, including the null, weak, strong, and dominant conditions. The dynamical and thermodynamic stability of the model is investigated, showing that the EUP cosmology admits a late-time de Sitter attractor. Finally, we evaluate the effective speed of sound associated with the model and discuss implications for perturbative stability. Our findings indicate that EUP-induced corrections can produce a consistent late-time acceleration without requiring a cosmological constant. |
| 2025-11-26 | [Testing modified gravity with 3x2pt analyses in galaxy mocks](http://arxiv.org/abs/2511.21468v1) | Marc Alemany-Gotor, Cristian Viglione et al. | Stage-IV surveys will enable unprecedented tests of gravity on cosmological scales. However, assuming General Relativity in the analysis of large-scale structure could introduce systematic biases if gravity deviates from GR at these scales. Modified gravity theories, such as the Hu-Sawicki formulation of $f(R)$ gravity, offer an alternative explanation for cosmic acceleration without invoking a cosmological constant, while remaining consistent with Solar System tests through screening mechanisms. In this work, we quantify the cosmological parameter biases that arise when using a combination of galaxy clustering and weak-lensing data-vectors, the so-called 3x2pt analysis, from an $f(R)$ galaxy mock under the incorrect assumption of GR, using for the first time high-fidelity full-sky galaxy mock catalogues. We employ a pair of twin simulations: one with GR and one with Hu--Sawicki $f(R)$ gravity with $|f_{R0}| = 10^{-5}$. The mocks are built using an HOD method to populate the dark matter haloes with galaxies, calibrated against SDSS observations at low redshift. Using conservative scale cuts to minimise modelling uncertainties, we perform 3x2pt analyses and infer cosmological parameters through nested sampling, validating our pipeline with the GR mock. Our results show that when analysing the $f(R)$ galaxy mock assuming GR, the recovered cosmological parameters are very significantly biased, even when considering conservative scale cuts: the Figure of Bias reaches $\sim12œÉ$ for both $\{Œ©_{\rm m}, œÉ_8\}$ and $S_8$. These biases persist even when marginalising over the galaxy bias and baryonic feedback, demonstrating that nuisance parameters cannot absorb the effects of modified gravity. We conclude that incorrectly assuming GR in a universe governed by $f(R)$ gravity leads to severe and detectable biases in cosmological inference for Stage-IV surveys. |
| 2025-11-26 | [The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods](http://arxiv.org/abs/2511.21363v1) | Kevin Iselborn, David Dembinsky et al. | The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes. |
| 2025-11-25 | [Carrier transport and electrical bandgaps in epitaxial CrN layers](http://arxiv.org/abs/2511.20625v1) | Duc V. Dinh, Jens Herfort et al. | The transport properties and electrical bandgap of nominally undoped ~75-nm-thick CrN layers simultaneously grown on AlN(0001) and AlN(11\bar{2}2) templates using plasma-assisted molecular beam epitaxy are investigated. The layers grown on AlN(0001) and AlN(11\bar{2}2) exhibit (111) and (113) surface orientations, respectively. All layers exhibit antiferromagnetism with a N√©el temperature of ~280 K, observed by temperature-dependent magnetic and electrical measurements. Hall-effect measurements demonstrate n-type semiconducting behavior across a wide temperature range from 4 to 920 K. At low temperatures (4 - 260 K), the data show parallel conduction channels from a metallic impurity band and the conduction band. The carrier mobility exhibits a temperature dependence consistent with a nondegenerate semiconductor, governed by ionized-impurity scattering below 400 K and phonon scattering above 400 K. An analysis of the temperature-dependent carrier density between 300 and 920 K yields two activation energies associated with intrinsic conduction: 0.15 eV (with an uncertainty of -0.02/+0.10 eV), which we attribute to the fundamental bandgap, and 0.50 eV (with an uncertainty of -0.05/+0.15 eV) representing a higher energy transition. |
| 2025-11-25 | [Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning](http://arxiv.org/abs/2511.20613v1) | Panayiotis Danassis, Naman Goel | The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios. |
| 2025-11-25 | [Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition](http://arxiv.org/abs/2511.20612v1) | Yujin Kim, Sarah Dean | Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD |
| 2025-11-25 | [Quantum Key Distribution: Bridging Theoretical Security Proofs, Practical Attacks, and Error Correction for Quantum-Augmented Networks](http://arxiv.org/abs/2511.20602v1) | Nitin Jha, Abhishek Parakh et al. | Quantum Key Distribution (QKD) is revolutionizing cryptography by promising information-theoretic security through the immutable laws of quantum mechanics. Yet, the challenge of transforming these idealized security models into practical, resilient systems remains a pressing issue, especially as quantum computing evolves. In this review, we critically dissect and synthesize the latest advancements in QKD protocols and their security vulnerabilities, with a strong emphasis on rigorous security proofs. We actively categorize contemporary QKD schemes into three key classes: uncertainty principle-based protocols (e.g., BB84), hybrid architectures that enable secure direct communication (eg, three-stage protocol), and continuous-variable frameworks. We further include two modern classes of QKD protocols, namely Twin-field QKD and Device-Independent QKD, both of which were developed to have practical implementations over the last decade. Moreover, we highlight important experimental breakthroughs and innovative mitigation strategies, including the deployment of advanced Quantum Error Correction Codes (QECCs), that significantly enhance channel fidelity and system robustness. By mapping the current landscape, from sophisticated quantum attacks to state-of-the-art error correction methods, this review fills an important gap in the literature. To bring everything together, the relevance of this review concerning quantum augmented networks (QuANets) is also presented. This allows the readers to gain a comprehensive understanding of the security promises of quantum key distribution from theoretical proofs to experimental validations. |
| 2025-11-25 | [Variational bagging: a robust approach for Bayesian uncertainty quantification](http://arxiv.org/abs/2511.20594v1) | Shitao Fan, Ilsang Ohn et al. | Variational Bayes methods are popular due to their computational efficiency and adaptability to diverse applications. In specifying the variational family, mean-field classes are commonly used, which enables efficient algorithms such as coordinate ascent variational inference (CAVI) but fails to capture parameter dependence and typically underestimates uncertainty. In this work, we introduce a variational bagging approach that integrates a bagging procedure with variational Bayes, resulting in a bagged variational posterior for improved inference. We establish strong theoretical guarantees, including posterior contraction rates for general models and a Bernstein-von Mises (BVM) type theorem that ensures valid uncertainty quantification. Notably, our results show that even when using a mean-field variational family, our approach can recover off-diagonal elements of the limiting covariance structure and provide proper uncertainty quantification. In addition, variational bagging is robust to model misspecification, with covariance structures matching those of the target covariance. We illustrate our variational bagging method in numerical studies through applications to parametric models, finite mixture models, deep neural networks, and variational autoencoders (VAEs). |
| 2025-11-25 | [PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic](http://arxiv.org/abs/2511.20586v1) | Koffi Ismael Ouattara, Ioannis Krontiris et al. | Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \emph{Trust Nodes} and \emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle. |
| 2025-11-25 | [An improved time delay from VLA and ATCA monitoring of the gravitational lens system PKS 1830-211](http://arxiv.org/abs/2511.20582v1) | A. D. Biggs | We have measured a time delay of 25.3 +/- 2.0 d (1-sigma confidence) in the Einstein ring gravitational lens system PKS 1830-211 from an analysis of archival VLA and ATCA monitoring data observed between 1997 and 2004. A small portion of the ATCA data was previously used to determine a time delay and our result is consistent with the previous value, but with an uncertainty that is smaller by more than a factor of two. The long time-baseline of the monitoring reveals that the flux density ratio is smoothly varying on a time-scale of years, an effect which we attribute to millilensing by massive objects (>>1 M_sun) in the lensing galaxy. Image A is unpolarized in the VLA monitoring, but VLBI observations show that this is partly due to beam dilution by an unpolarized counter-jet that is only present in that image. Based on the identification of this feature as a counter-jet, we conclude that its unexpected prominence in image A is a consequence of lensing and that more detailed modelling is required in order to reconcile the VLBI morphology of each image. |
| 2025-11-25 | [NNLO QCD predictions for $WŒ≥Œ≥$ production at the LHC](http://arxiv.org/abs/2511.20581v1) | Paolo Garbarino, Massimiliano Grazzini et al. | Triboson production processes play a crucial role in probing the electroweak sector of the Standard Model, as they involve quartic gauge-boson couplings already at the tree level. With these measurements entering the precision era at the Large Hadron Collider (LHC), accurate theoretical predictions become indispensable. We present the computation of the next-to-next-to-leading-order (NNLO) QCD radiative corrections to the production of a $W$ boson in association with two photons ($WŒ≥Œ≥$) at the LHC. The calculation is exact, except for the finite part of the two-loop contribution, which is included in the leading-colour approximation. Predictions for the fiducial cross section and selected kinematic distributions are provided at a centre-of-mass energy of $\sqrt{s}=13$ TeV, under standard experimental selection cuts. In line with observations for other multiboson processes involving direct photons, we find sizable NNLO corrections that enhance the next-to-leading-order predictions by about $23\%$, with residual perturbative uncertainties that can be roughly estimated to be at the $5\%$ level. |
| 2025-11-25 | [Active learning with physics-informed neural networks for optimal sensor placement in deep tunneling through transversely isotropic elastic rocks](http://arxiv.org/abs/2511.20574v1) | Alec Tristani, Chlo√© Arson | This paper presents a deep learning strategy to simultaneously solve Partial Differential Equations (PDEs) and back-calculate their parameters in the context of deep tunnel excavation. A Physics-Informed Neural Network (PINN) model is trained with synthetic data that emulates in situ displacement measurements in the host rock and at the cavity wall, obtained from extensometers and convergence monitoring. As acquiring field observations can be costly, a sequential training approach based on active learning is implemented to determine the most informative locations for new sensors. In particular, Monte Carlo dropout is used to quantify epistemic uncertainty and query measurements in regions where the model is least confident. This approach reduces the amount of required field data and optimizes sensor placement. The PINN is tested to reconstruct the displacement field around a deep tunnel of circular section excavated in transversely isotropic elastic rock and to determine rock constitutive and stress-field parameters. Results demonstrate excellent performance on small, scattered, and noisy datasets, achieving high precision for the Young's moduli, shear modulus, horizontal-to-vertical far-field stress ratio, and the orientation of the bedding planes. The proposed framework shall ultimately support decision-making for optimal subsurface monitoring and for adaptive tunnel design and control. |
| 2025-11-25 | [Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics](http://arxiv.org/abs/2511.20570v1) | Tasha Kim, Oiwi Parker Jones | Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action. |
| 2025-11-24 | [Gamma-ray Time Delay and Magnification Ratio in the Gravitationally-Lensed Blazar PKS 1830-211](http://arxiv.org/abs/2511.19419v1) | S. Buson, M. De Toma et al. | We present the characterization of macrolensing properties of the gravitationally lensed system PKS 1830-211, utilizing data from the Fermi Large Area Telescope. While at gamma-rays we can not spatially resolve the lensed images, a macrolensing-induced time pattern is expected in the blazar's lightcurve, resulting from the delay between variable gamma-ray components originating from its two brightest lensed images. Compared to our previous study, here we employ high-quality lightcurves coupled with prolonged outburst activity, and improved time-series techniques. Analyzing six independent data segments, we identified a delay of 20.26 +/- 0.62 days (statistical and stochastic uncertainty), with a chance detection probability at the 2.5 x $10^{-5}$ level (post-trial). We also present a novel approach to the magnification ratio estimate based on a comparison with simulated data. Our work suggests that the gamma-ray flux ratio between the two main lens components is $Œº_Œ≥ \lesssim$ 1.8. We do not observe convincing evidence of microlensing effects, as previously claimed. The measured gamma-ray time delay is in 2-sigma tension with radio-based estimates, suggesting either distinct emission sites, underestimated radio uncertainties, or gamma-ray production in a region opaque to radio. Our study highlights the potential of well sampled lightcurves and advanced time-series techniques to distinguish true lensing-induced delays from stochastic variability. When combined with improved radio constraints and refined lens models, PKS 1830-211 and similar sources constitute promising systems for time-delay cosmography, offering new insights into both jet structure and cosmological parameters. |
| 2025-11-24 | [Puzzling Isotonic Odd-Even Staggering of Charge Radii in Deformed Rare Earth Nuclei](http://arxiv.org/abs/2511.19395v1) | Endre Takacs, Hunter Staiger et al. | The nuclear charge radius is a fundamental observable that encodes key aspects of nuclear structure, deformation, and pairing. Isotonic (constant neutron number) systematics in the deformed rare-earth region have long suggested that odd-$Z$ nuclei are more compact than their even-$Z$ neighbors - except for Lu, whose recommended radius appeared anomalously large relative to Yb and Hf. We report a high-precision determination of the natural-abundance-averaged Lu-Yb charge-radius difference using extreme-ultraviolet spectroscopy of highly charged Na-like and Mg-like ions, supported by high-accuracy relativistic atomic-structure calculations - a recently introduced method with the unique ability to measure inter-element charge radius differences. Combined with muonic-atom and optical isotope-shift data, our result resolves the longstanding Lu inversion anomaly and reestablishes a pronounced odd-even staggering along the $N=94$ isotonic chain. The magnitude of this staggering is unexpectedly large, far exceeding that observed in semi-magic nuclei and in deformed isotopic sequences. State-of-the-art nuclear density functional theory calculations, including quantified uncertainties, fail to reproduce this enhancement, possibly indicating missing structural effects in current models. Our work demonstrates the power of highly charged ions for precise, element-crossing charge-radius measurements and provides stringent new constraints for future theoretical and experimental studies of nuclear-size systematics. |
| 2025-11-24 | [Guesswork in the gap: the impact of uncertainty in the compact binary population on source classification](http://arxiv.org/abs/2511.19393v1) | Utkarsh Mali, Reed Essick | The nature of the compact objects within the supposed "lower mass gap" remains uncertain. Observations of GW190814 and GW230529 highlight the challenges gravitational waves face in distinguishing neutron stars from black holes. Interpreting these systems is especially difficult because classifications depend simultaneously on measurement noise, compact binary population models, and equation of state (EOS) constraints on the maximum neutron star mass. We analyze 66 confident events from GWTC-3 to quantify how the probability of a component being a neutron star, P(NS), varies across the population. The effects are substantial, the dominant drivers of classification are the pairing preferences of neutron stars with other compact objects, and the neutron star spin distributions. The data reveals that P(NS) varies between 1% - 67% for GW230529's primary and between 51% - 100% for GW190425's primary. By contrast, P(NS) for GW190814's secondary varies by <10%, demonstrating robustness from its high signal-to-noise ratio and small mass ratio. Analysis using EOS information tends to affect P(NS) through the inferred maximum neutron star mass rather than the maximum spin. As it stands, P(NS) remains sensitive to numerous population parameters, limiting its reliability and potentially leading to ambiguous classifications of future GW events. |
| 2025-11-24 | [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](http://arxiv.org/abs/2511.19390v1) | Rudy Morel, Francesco Pio Ramunno et al. | Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability. |
| 2025-11-24 | [Normative active inference: A numerical proof of principle for a computational and economic legal analytic approach to AI governance](http://arxiv.org/abs/2511.19334v1) | Axel Constant, Mahault Albarracin et al. | This paper presents a computational account of how legal norms can influence the behavior of artificial intelligence (AI) agents, grounded in the active inference framework (AIF) that is informed by principles of economic legal analysis (ELA). The ensuing model aims to capture the complexity of human decision-making under legal constraints, offering a candidate mechanism for agent governance in AI systems, that is, the (auto)regulation of AI agents themselves rather than human actors in the AI industry. We propose that lawful and norm-sensitive AI behavior can be achieved through regulation by design, where agents are endowed with intentional control systems, or behavioral safety valves, that guide real-time decisions in accordance with normative expectations. To illustrate this, we simulate an autonomous driving scenario in which an AI agent must decide when to yield the right of way by balancing competing legal and pragmatic imperatives. The model formalizes how AIF can implement context-dependent preferences to resolve such conflicts, linking this mechanism to the conception of law as a scaffold for rational decision-making under uncertainty. We conclude by discussing how context-dependent preferences could function as safety mechanisms for autonomous agents, enhancing lawful alignment and risk mitigation in AI governance. |
| 2025-11-24 | [Revisiting model-independent constraints on spatial curvature and cosmic ladders calibration: updated and forecast analyses](http://arxiv.org/abs/2511.19332v1) | Arianna Favale, Adri√† G√≥mez-Valent et al. | Model-independent approaches have gained increasing attention as powerful tools to investigate persistent tensions between cosmological observations and the predictions of $Œõ$CDM. Notably, recent DESY5 Type Ia Supernovae (SNIa) and DESI Baryon Acoustic Oscillation (BAO) data challenge the validity of the cosmological constant, and they remain in tension with SH0ES local distance ladder measurements under standard pre-recombination physics. Building on our previous work, MNRAS 523 (2023) 3, 3406-3422, we present a follow-up analysis of the model-independent calibration of the local and inverse distance ladders using cosmic chronometers (CCH) data and Gaussian Processes. We jointly constrain the SNIa absolute magnitude, $M$, the comoving sound horizon at the baryon-drag epoch, $r_d$, and the spatial curvature parameter, $Œ©_k$, using CCH with DESY5 and DESI DR1/DR2. We find this data combination compatible with a flat universe at $\sim1.7œÉ$, with $Œ©_k=-0.143\pm0.085$, showing weaker compatibility than with Pantheon+, while the ladder calibrators read $M=-19.324_{-0.095}^{+0.092}$ and $r_d=(144.00^{+5.38}_{-4.88}$) Mpc. Although current uncertainties limit the precision of our constraints and prevent us from arbitrating the Hubble tension, it is nevertheless instructive to explore the constraining power of our methodology with future SNIa, CCH, and BAO from surveys such as LSST, Euclid, and DESI. We present the first forecast analysis for the triad $(M,Œ©_k,r_d)$, finding that, in an optimistic scenario, upcoming data will improve agnostic constraints on $M$ by $\sim$54% and on $r_d$ by $\sim$66%, enabling a $\sim2$% determination of $H_0$. Precision on $Œ©_k$ will increase by $\sim50$%. Our analysis outlines which improvements in future data - whether in quality, quantity, or redshift coverage - are likely to most effectively tighten these constraints.[abridged] |
| 2025-11-24 | [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](http://arxiv.org/abs/2511.19301v1) | Johannes Meier, Florian G√ºnther et al. | Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.   To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset. |
| 2025-11-24 | [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](http://arxiv.org/abs/2511.19274v1) | Mingyang Chen, Jiawei Du et al. | Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences. |
| 2025-11-24 | [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](http://arxiv.org/abs/2511.19263v1) | Lucas Li, Jean-Baptiste Puel et al. | Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction. |
| 2025-11-24 | [Integrating Complex Covariate Transformations in Generalized Additive Models](http://arxiv.org/abs/2511.19234v1) | Claudia Collarin, Matteo Fasiolo et al. | Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory. |
| 2025-11-21 | [Scaling Conditional Autoencoders for Portfolio Optimization via Uncertainty-Aware Factor Selection](http://arxiv.org/abs/2511.17462v1) | Ryan Engel, Yu Chen et al. | Conditional Autoencoders (CAEs) offer a flexible, interpretable approach for estimating latent asset-pricing factors from firm characteristics. However, existing studies usually limit the latent factor dimension to around K=5 due to concerns that larger K can degrade performance. To overcome this challenge, we propose a scalable framework that couples a high-dimensional CAE with an uncertainty-aware factor selection procedure. We employ three models for quantile prediction: zero-shot Chronos, a pretrained time-series foundation model (ZS-Chronos), gradient-boosted quantile regression trees using XGBoost and RAPIDS (Q-Boost), and an I.I.D bootstrap-based sample mean model (IID-BS). For each model, we rank factors by forecast uncertainty and retain the top-k most predictable factors for portfolio construction, where k denotes the selected subset of factors. This pruning strategy delivers substantial gains in risk-adjusted performance across all forecasting models. Furthermore, due to each model's uncorrelated predictions, a performance-weighted ensemble consistently outperforms individual models with higher Sharpe, Sortino, and Omega ratios. |
| 2025-11-21 | [A Framework for Adaptive Stabilisation of Nonlinear Stochastic Systems](http://arxiv.org/abs/2511.17436v1) | Seth Siriya, Jingge Zhu et al. | We consider the adaptive control problem for discrete-time, nonlinear stochastic systems with linearly parameterised uncertainty. Assuming access to a parameterised family of controllers that can stabilise the system in a bounded set within an informative region of the state space when the parameter is well-chosen, we propose a certainty equivalence learning-based adaptive control strategy, and subsequently derive stability bounds on the closed-loop system that hold for some probabilities. We then show that if the entire state space is informative, and the family of controllers is globally stabilising with appropriately chosen parameters, high probability stability guarantees can be derived. |
| 2025-11-21 | [Delegation and Lobbying](http://arxiv.org/abs/2511.17391v1) | Thomas Groll, Sharyn O'Halloran | This chapter examines the link between delegation and lobbying, two themes central to political economy. Delegation models explore how legislatures manage uncertainty and control bureaucratic agents, while lobbying models analyze how organized interests influence policy through contributions, information, and advocacy. We review the growing body of research that integrates these literatures, showing how the prospect of lobbying affects legislative incentives to delegate and how the structure of delegated authority shapes lobbying strategies. We highlight common-agency frameworks that capture the recursive relationship between delegation and lobbying and empirical studies documenting how venue choice, information provision, and interest group mobilization mediate delegation outcomes. We also review applications to agency oversight and fiscal policy. Finally, we present a model of regulatory rule-making that embeds lobbying directly into the delegation decision, offering predictions for both theory and empirical analysis. |
| 2025-11-21 | [Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions](http://arxiv.org/abs/2511.17380v1) | Zheng Wang, Yi Zhang et al. | Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts. |
| 2025-11-21 | [POPxf: An Exchange Format for Polynomial Observable Predictions](http://arxiv.org/abs/2511.17348v1) | Ilaria Brivio, Ken Mimasu et al. | We introduce the Polynomial Observable Prediction Exchange Format, POPxf, a structured, machine-readable data format for the publication and exchange of semi-analytical theoretical predictions in high energy physics. The format is designed to encode observables that can be expressed in terms of polynomials in model parameters, with particular emphasis on Effective Field Theory applications. All relevant assumptions and metadata are recorded explicitly, and the treatment of uncertainties and correlations is flexible enough to capture parameter-dependent effects. The format aims to improve reproducibility, facilitate global fits and reinterpretations, and streamline the use of theoretical predictions across the particle physics community. |
| 2025-11-21 | [Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks](http://arxiv.org/abs/2511.17345v1) | Hichem Sahbi | Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work. |
| 2025-11-21 | [Measurements of differential charged-current cross sections on argon for electron neutrinos with final-state protons in MicroBooNE](http://arxiv.org/abs/2511.17342v1) | MicroBooNE collaboration, P. Abratenko et al. | This work presents single-differential electron-neutrino charged-current cross sections on argon measured using the MicroBooNE detector at the Fermi National Accelerator Laboratory. The analysis uses data recorded when the Neutrinos at the Main Injector beam was operating in both neutrino and antineutrino modes, with exposures of $2 \times 10^{20}$ and $5 \times 10^{20}$ protons on target, respectively. A selection algorithm targeting electron-neutrino charged-current interactions with at least one proton, one electron, and no pions in the final topology is used to measure differential cross sections as a function of outgoing electron energy, total visible energy, and opening angle between the electron and the most energetic proton. The interaction rate as a function of proton multiplicity is also reported. The total cross section is measured as [4.1 $\pm$ 0.4 (stat.) $\pm$ 1.2 (syst.)]$\,$$\times \,10^{-39} \mathrm{cm}^{2}/\,\mathrm{nucleon}$. The unfolded cross-section measurements are compared to predictions from neutrino event generators commonly employed in the field. Good agreement is seen across all variables within uncertainties. |
| 2025-11-21 | [On the baryon budget in the X-ray-emitting circumgalactic medium of Milky Way-mass galaxies](http://arxiv.org/abs/2511.17313v1) | Yi Zhang, Soumya Shreeram et al. | Recent observations with SRG/eROSITA have revealed the average X-ray surface brightness profile of the X-ray-emitting circumgalactic medium (CGM) around Milky Way (MW)-mass galaxies, offering valuable insights into the baryon mass in these systems. However, the estimation of the baryon mass depends critically on several assumptions regarding the gas density profile, temperature, metallicity, and the underlying halo mass distribution. Here, we assess how these assumptions affect the inferred baryon mass of the X-ray-emitting CGM in MW-mass galaxies, based on the stacked eROSITA signal. We find that variations in temperature profiles and uncertainties in the halo mass introduce the dominant sources of uncertainty, resulting in X-ray-emitting baryon mass estimates that vary by nearly a factor of four ($0.8-3.5\times10^{11} M_\odot$). Assumptions about metallicity contribute an additional uncertainty of approximately $50\%$. We emphasize that accurate X-ray spectral constraints on gas temperature and metallicity, along with careful modeling of halo mass uncertainty, are essential for accurately estimating the baryon mass for MW-mass galaxies. Future X-ray microcalorimeter missions will be crucial for determining the hot CGM properties and closing the baryon census at the MW-mass scale. |
| 2025-11-21 | [MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning](http://arxiv.org/abs/2511.17299v1) | Tom√°≈° Musil, Matƒõj Petrl√≠k et al. | Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research. |
| 2025-11-21 | [Far-infrared to centimeter emission of very nearby galaxies with archival data](http://arxiv.org/abs/2511.17268v1) | L. Correia, C. Bot et al. | Compared to the well-studied infrared and radio domains, galaxy emission in the millimeter (mm) - centimeter (cm) range has been less observed. In this domain, galaxy emission consists of thermal dust, free-free and synchrotron emissions with a possible additional contribution from anomalous microwave emission (AME) peaking near 1 cm.The aim of this study is to accurately characterize the integrated spectral energy distribution (SED) of galaxies in the mm-cm range. We used COBE-DIRBE, IRAS, Planck, and WMAP all-sky surveys, brought to the same resolution of $1^\circ$, to cover 18 photometric bands from 97$Œº$m to 1.3 cm. Given the low angular resolution and mixing with foreground and background emission that hampers the detection of the galaxy, our sample consists of 6 of the brightest, nearby galaxies: LMC, SMC, M31, M33, NGC 253 and NGC 4945. We subtract Milky Way dust emission, distant unresolved galaxies, and foreground point sources in the fields. We fit each integrated SED with a model of thermal dust, free-free, synchrotron, AME and Cosmic Microwave Background (CMB) temperature fluctuations. The integrated SEDs of our sample of galaxies are well fitted by the model within the uncertainties, although degeneracies between the different components contributing to the mm-cm emission complicate the estimation of their individual contributions. We do not clearly detect AME in any of our target galaxies, and AME emissivity upper limits are weak compared to Galactic standards, suggesting that the signal of AME might be diluted at the scale of a whole galaxy. We infer positive CMB fluctuations in the background of 5 out of our 6 galaxies. This effect might be related to the degeneracy between the dust emissivity index and CMB fluctuations in the background, or linked to the specific spatial distribution of CMB fluctuations coupled with the low resolution and small number statistics. |
| 2025-11-20 | [A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability](http://arxiv.org/abs/2511.16631v1) | Haihao Shi, Zhenyang Huang et al. | We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $œÑ(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $œÑ(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $œÑ(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae. |
| 2025-11-20 | [Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring](http://arxiv.org/abs/2511.16628v1) | Tammam Bakeer, Max Herbers et al. | In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.   The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics. |
| 2025-11-20 | [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](http://arxiv.org/abs/2511.16625v1) | Elias Hossain, Md Mehedi Hasan Nipu et al. | We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems. |
| 2025-11-20 | [KrkNLO matching and phenomenology for vector boson processes](http://arxiv.org/abs/2511.16605v1) | Pratixan Sarmah, Andrzej Si√≥dmok et al. | The combination of NLO matrix elements with parton showers is indispensable for LHC physics. Differences between matching methods introduce matching uncertainties, corresponding to formally higher-order terms. We recently presented the process-independent generalisation of the KrkNLO method for NLO matching, which employs a modified PDF factorisation scheme to achieve NLO accuracy. With this factorisation scheme, the method can be used for colour-singlet final-states, and was previously implemented in the Herwig Monte Carlo Event Generator and applied to the diphoton-production process.   Here we present the extension of the implementation of the KrkNLO method within Herwig to support the full class of applicable processes, using an external matrix-element library. We re-validate the implementation, and use it to study the NLO matching uncertainty for four vector-boson production processes at the LHC: $W$, $ZŒ≥$, $WW$ and $ZZ$. We demonstrate that the KrkNLO method effectively eliminates the negative-weight problem in NLO event generation, across the four processes studied. We provide detailed comparisons between KrkNLO and variants of the MC@NLO method with different shower starting-scale choices, across processes and throughout phase-space, including double-differential observables. For each process, we compare the predictions to LHC data from ATLAS. |
| 2025-11-20 | [Bayesian polarization calibration and imaging in very long baseline interferometry](http://arxiv.org/abs/2511.16556v1) | Jong-Seo Kim, Jakob Roth et al. | Extracting polarimetric information from very long baseline interferometry (VLBI) data is demanding but vital for understanding the synchrotron radiation process and the magnetic fields of celestial objects, such as active galactic nuclei (AGNs). However, conventional CLEAN-based calibration and imaging methods provide suboptimal resolution without uncertainty estimation of calibration solutions, while requiring manual steering from an experienced user. We present a Bayesian polarization calibration and imaging method using Bayesian imaging software resolve for VLBI data sets, that explores the posterior distribution of antenna-based gains, polarization leakages, and polarimetric images jointly from pre-calibrated data. We demonstrate our calibration and imaging method with observations of the quasar 3C273 with the VLBA at 15 GHz and the blazar OJ287 with the GMVA+ALMA at 86 GHz. Compared to the CLEAN method, our approach provides physically realistic images that satisfy positivity of flux and polarization constraints and can reconstruct complex source structures composed of various spatial scales. Our method systematically accounts for calibration uncertainties in the final images and provides uncertainties of Stokes images and calibration solutions. The automated Bayesian approach for calibration and imaging will be able to obtain high-fidelity polarimetric images using high-quality data from next-generation radio arrays. The pipeline developed for this work is publicly available. |
| 2025-11-20 | [YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras](http://arxiv.org/abs/2511.16521v1) | Fan Yang, Sosuke Yamao et al. | Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications. |
| 2025-11-20 | [General-purpose Data-driven Wall Model for Low-speed Flows Part I: Baseline Model](http://arxiv.org/abs/2511.16511v1) | Yuenong Ling, Imran Hayat et al. | We present a general-purpose wall model for large-eddy simulation. The model builds on the building-block flow principle, leveraging essential physics from simple flows to train a generalizable model applicable across complex geometries and flow conditions. The model addresses key limitations of traditional equilibrium wall models (EQWM) and improves upon shortcomings of earlier building-block-based approaches. The model comprises four components: (i) a baseline wall model, (ii) an error model, (iii) a classifier, and (iv) a confidence score. The baseline model predicts the wall-shear stress, while the error model estimates epistemic errors and aleatoric errors, both used for uncertainty quantification. In Part I of this work, we present the baseline model, while the remaining three components are introduced in Part II. The baseline model is designed to capture a broad range of flow phenomena, including turbulence over curved walls and zero, adverse, and favorable mean pressure gradients, as well as flow separation and laminar flow. The problem is formulated as a regression task to predict wall shear stress using a neural network. Model inputs are localized in space and dimensionless, with their selection guided by information-theoretic criteria. Training data include, among other cases, a newly generated direct numerical simulation dataset of turbulent boundary layers under favorable and adverse PG conditions. Validation is carried out through both a priori and a posteriori tests. The a priori evaluation spans 140 diverse high-fidelity numerical datasets and experiments (67 training cases included), covering turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries, among others. We demonstrate that the baseline wall model outperforms the EQWM in 90% of test scenarios, while maintaining errors below 20% for 98% of the cases. |
| 2025-11-20 | [Quantum corrections in general relativity explored through a GUP-inspired maximal acceleration analysis](http://arxiv.org/abs/2511.16502v1) | Christian Corda, Carlo Cafaro et al. | A maximun acceleration analysis by Pati dating back to 1992 is here improved by replacing the traditional Heisenberg Uncertainty Principle (HUP) with the Generalized Uncertainty Principle (GUP), which predicts the existence of a minimum length in Nature. This new approach allows one to find a numerical value for the maximum acceleration existing in Nature for a physical particle that turns out to be a_{max}\simeq4\frac{c^{2}}{l_{P}}, that is, a function of two fundamental physical quantities such as the speed of light c and the Planck length l_{p}. An application of this result to black hole (BH) physics allows one to estimate a new quantum limit to general relativity. It is indeed shown that, for every real Schwarzschild BH, the maximum gravitational acceleration occurs, without becoming infinite, when the Schwarzschild radial coordinate reaches the gravitational radius. This means that quantum corrections to general relativity become necessary not at the Planck scale, as the majority of researchers in the field think, but at the Schwarzschild scale, in agreement with recent interesting results in the literature. In other words, the quantum nature of physics, which in this case manifests itself through the GUP, appears to prohibit the existence of real singularities, in this current case forbiddiing the gravitational acceleration of a Schwarzschild BH from becoming infinite. |
| 2025-11-20 | [Integrating Deep Learning and Spatial Statistics in Marine Ecosystem Monitoring](http://arxiv.org/abs/2511.16447v1) | Gian Mario Sangiovanni, Gianluca Mastrantonio et al. | In ecology, photogrammetry is a crucial method for efficiently collecting non-destructive samples of natural environments. When estimating the spatial distribution of animals, detecting objects in large-scale images becomes crucial. Object detection models enable large-scale analysis but introduce uncertainty because detection probability depends on various factors. To address detection bias, we model the distribution of a species of benthic animals (holothurians) in an area of the Italian Tyrrhenian coast near Giglio Island using a Thinned Log-Gaussian Cox Process (LGCP). We assume that a "true" intensity function accurately describes the distribution, while the observed process, resulting from independent thinning, is represented by a degraded intensity. The detection function controls the thinning mechanism, influenced by the object's location and other detection-related features. We use manual identification of holothurians as our benchmark. We compare automatic detection with this benchmark, an unthinned LGCP, and the thinned model to highlight the improvements gained from the proposed approach.Our method allows researchers to use photogrammetry, automatically identify objects of interest, and correct biases and approximations caused by the observation process. |
| 2025-11-20 | [Black-Box Uncertainty Estimation for Deep Learning Models in Atomistic Simulations](http://arxiv.org/abs/2511.16439v1) | Idan Fonea, Amir Peles et al. | We analyze an ensemble-based approach for uncertainty quantification (UQ) in atomistic neural networks. This method generates an epistemic uncertainty signal without requiring changes to the underlying multi-headed regression neural network architecture, making it suitable for sealed or black-box models. We apply this method to molecular systems, specifically sodium (Na) and aluminum (Al), under various temperature conditions. By scaling the uncertainty signal, we account for heteroscedasticity in the data. We demonstrate the robustness of the scaled UQ signal for detecting out-of-distribution (OOD) behavior in several scenarios. This UQ signal also correlates with model convergence during training, providing an additional tool for optimizing the training process. |
| 2025-11-19 | [Resolving Ratio Redundancy in Chemical Freeze-out Studies with Principal Component Analysis and Bayesian Calibration](http://arxiv.org/abs/2511.15707v1) | Nachiketa Sarkar | We introduce a Principal Component Analysis (PCA)--Bayesian framework for extracting chemical freeze-out conditions in relativistic heavy-ion collisions that resolves long-standing ambiguities in hadron-ratio--based analyses. By constructing all possible hadron-yield ratios from a chosen set of species and transforming them into an orthogonal PCA basis, the method removes linear redundancies and eliminates the information loss and systematic uncertainties associated with ratio selection. Energy-wise Bayesian calibration of the Hadron Resonance Gas (HRG) model is then performed directly in this decorrelated space, with a Gaussian Process emulator enabling fast and accurate model evaluations. A detailed Sobol sensitivity analysis, together with the PCA loading structure, identifies the most informative ratio combinations and reveals a transition from chemical-potential--dominated to temperature-controlled freeze-out with increasing $\sqrt{s_{NN}}$. The calibrated model reproduces all measured ratios, and the extracted freeze-out parameters are consistent with previous HRG determinations. |
| 2025-11-19 | [Terrestrial Matter Effects on Reactor Antineutrino Oscillations: Constant vs. Fluctuated Density Profiles](http://arxiv.org/abs/2511.15702v1) | Yu-Feng Li, Andong Wang et al. | The JUNO Collaboration has recently released its first reactor antineutrino oscillation result, achieving unprecedented precision in the measurement of $Œîm^2_{21}$ and $\sin^2Œ∏_{12}$. We emphasize that the accurate determination and modeling of the terrestrial matter density profile are fundamental for extracting the oscillation parameters and probing the neutrino mass ordering. This paper presents a realistic piecewise-constant model for the shallow crustal density profile along the baselines from Taishan and Yangjiang to the experimental hall, based on geological and petrophysical information. The uncertainty in the density profiles arises from variations in the density and length of each segment, both of which are conservatively estimated to be 10\%. A careful comparison of constant and fluctuated density profiles is provided and the implications for the precision measurement of oscillation parameters are discussed. Finally, we also discuss the possibility of tomography of the shallow Earth's crust in future reactor neutrino experiments. |
| 2025-11-19 | [Further Reduction of the PDF Uncertainty in the High-Mass Drell-Yan Spectrum Utilizing Neutral and Charged Current Inputs](http://arxiv.org/abs/2511.15683v1) | Yao Fu, Raymond Brock et al. | Uncertainties in the parametrization of Parton Distribution Functions are a serious limiting systematic uncertainty in Large Hadron Collider searches for Beyond the Standard Model physics. This is especially true for measurements at high scales induced by quark and anti-quark collisions, where Drell-Yan continuum backgrounds are dominant. In Phys. Rev. D99, 054004 (2019) we presented a unique strategy for improving uncertainties using neutral current Drell-Yan backgrounds and here we update that strategy and include charged current Drell-Yan final states in the program and demonstrate significant improvements. Through a judicious selection of measurable kinematical quantities can reduce the assigned systematic PDF uncertainties by significant factors in limit-setting or discovery for neutral and charged, high mass Intermediate Vector Bosons. This approach will be take advantage of the huge statistical precision of future High Luminosity, Large Hadron Collider Standard Model datasets and could also improve uncertainties in the high statistics results from LHC Run 3. |
| 2025-11-19 | [Branching fraction measurement of the $\mathitŒõ \to p Œº^- \overlineŒΩ_Œº$ decay](http://arxiv.org/abs/2511.15681v1) | LHCb collaboration, R. Aaij et al. | A measurement of the branching fraction for the decay $\mathitŒõ \to p Œº^- \overlineŒΩ_Œº$ is presented using $\textit{pp}$ collision data collected by the LHCb experiment at a centre-of-mass energy of 13 TeV. The analysis is based on data recorded between 2016 and 2018, corresponding to an integrated luminosity of $5.4 \ \text{fb}^{-1}$. The result is obtained using $\mathitŒõ \to p œÄ^-$ decays as a normalisation channel. The measured branching fraction is $B(\mathitŒõ \to p Œº^- \overlineŒΩ_Œº)= (1.462 \pm 0.016 \pm 0.100 \pm 0.011 ) \times 10^{-4}$, where the uncertainties are statistical, systematic, and due to the limited knowledge of the normalisation mode branching fraction, respectively. This result improves the precision of the branching fraction measurement by a factor of two compared to the previous best measurement and sets a more stringent bound on lepton flavour universality in $s \to u$ quark transitions. It is consistent with previous measurements, and the extracted lepton flavour universality test observable, $R^{Œºe} = \frac{Œì(\mathitŒõ \to p Œº^- \overlineŒΩ_Œº)}{Œì(\mathitŒõ \to p e^- \overlineŒΩ_e)} = 0.175 \pm 0.012$, agrees with the Standard Model prediction. |
| 2025-11-19 | [From Qubits to Couplings: A Hybrid Quantum Machine Learning Framework for LHC Physics](http://arxiv.org/abs/2511.15672v1) | Marwan Ait Haddou, Mohamed Belfkir et al. | In this paper, we propose a new Hybrid Quantum Machine Learning (HyQML) framework to improve the sensitivity of double Higgs boson searches in the $HH \to b\bar{b}Œ≥Œ≥$ final state at $\sqrt{s}$ = 13.6 TeV. The proposed model combines parameterized quantum circuits with a classical neural network meta-model, enabling event-level features to be embedded in a quantum feature space while maintaining the optimization stability of classical learning. The hybrid model outperforms both a state-of-the-art XGBoost model and a purely quantum implementation by a factor of two, achieving an expected 95% CL upper limit on the non-resonant double Higgs boson production cross-section of $1.9\timesœÉ_{\text{SM}}$ and $2.1\timesœÉ_{\text{SM}}$ under background normalization uncertainties of 10% and 50%, respectively. In addition, expected constraints on the Higgs boson self-coupling $Œ∫_Œª$ and quartic vector-boson-Higgs coupling $Œ∫_{2V}$ are found to be improved compared to the classical and purely quantum models. |
| 2025-11-19 | [Fant√¥mas: An analysis of parton distributions in a pion with B√©zier parametrizations](http://arxiv.org/abs/2511.15657v1) | Lucas Kotz, Aurore Courtoy et al. | We systematically explore the parametrization dependence of the Parton Distribution Functions (PDFs) to better quantify the true uncertainty from global QCD analyses. To achieve this, we employ a novel technique that automates the generation of polynomial parametrizations for PDFs using B√©zier curves. This technique is implemented in a C++ module, named Fant√¥mas, which is now included within the xFitter program. As an example, we examine the charged pion PDF. Our analysis reveals that the sea and gluon distributions in the pion are strongly correlated, and PDF solutions featuring a vanishing gluon and a large quark sea are still experimentally allowed. |
| 2025-11-19 | [INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases](http://arxiv.org/abs/2511.15656v1) | Edward Vendrow, Julia Chae et al. | Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis. |
| 2025-11-19 | [Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning](http://arxiv.org/abs/2511.15633v1) | Tao Hu, Lan Li et al. | Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation. |
| 2025-11-19 | [When to Think and When to Look: Uncertainty-Guided Lookback](http://arxiv.org/abs/2511.15613v1) | Jing Bi, Filippos Bellos et al. | Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets. |
| 2025-11-19 | [Assessing (H)EFT theory errors by pitting EoM against Field Redefinitions](http://arxiv.org/abs/2511.15609v1) | Rodrigo Alonso, Christoph Englert et al. | Truncations of effective field theory expansions are technically necessary but inherently intertwined with the redundancies of general field redefinitions. This can be viewed as a juxtaposition of power-counting and theoretical uncertainties, which seek to estimate neglected higher-dimensional interactions through approaches based on community consensus. One can then understand the invariance of physics under field redefinitions as a data-informed validation of different power-counting schemes, or as a means of assigning theoretical errors in comparison with algebraic, equation of motion-based replacements. Such an approach generalises widely accepted procedures for estimating theoretical uncertainties within the SM to non-renormalisable interactions. We perform a case study for a representative example in Higgs Effective Field theory, focusing on universal Higgs properties tensioned against process-dependent sensitivity expectations. |
| 2025-11-18 | [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](http://arxiv.org/abs/2511.14755v1) | Albert Lin, Alessandro Pinto et al. | As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. |
| 2025-11-18 | [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](http://arxiv.org/abs/2511.14751v1) | Yutian Chen, Yuheng Qiu et al. | We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. |
| 2025-11-18 | [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](http://arxiv.org/abs/2511.14749v1) | Alexander Vedernikov, Puneet Kumar et al. | Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06. |
| 2025-11-18 | [Systematic Study of the Self-Renormalized Nucleon Gluon PDF in Large-Momentum Effective Theory](http://arxiv.org/abs/2511.14708v1) | Alex NieMiera, William Good et al. | We present a systematic study of the nucleon gluon parton distribution function (PDF) using the self-renormalized large-momentum effective theory (LaMET) approach in lattice QCD. This work extends previous gluon-PDF extractions by performing a detailed analysis of key systematic effects, including gauge-link smearing, lattice spacing, pion mass, and nucleon boost momentum. The self-renormalization framework mitigates ultraviolet divergences associated with Wilson-line self-energy and renormalon contributions by combining lattice matrix elements with perturbative short-distance information, thereby preserving the correct infrared structure. Calculations are performed on $N_f=2+1+1$ HISQ ensembles generated by the MILC Collaboration at three lattice spacings and two pion masses, with boosted nucleon states reaching momenta up to 2.2~GeV. We determine renormalization factors from zero-momentum matrix elements and apply hybrid renormalization to suppress discretization artifacts. After extrapolating large-separation behavior and performing Fourier transforms, we reconstruct quasi-PDFs and match them to lightcone PDFs using next-to-leading order Wilson coefficients. Our results demonstrate that smearing and lattice-spacing effects are under control, and pion-mass and lattice-spacing dependence is mild relative to the current $O(10^6)$ statistics; however, momentum dependence remains a significant source of uncertainty. Future work including even larger boost momenta will be essential to reduce systematics in lattice determinations of the gluon PDF and to advance toward precision QCD phenomenology at the LHC and the future Electron-Ion Collider. |
| 2025-11-18 | [Compensating random transition-detection blackouts in Markov networks](http://arxiv.org/abs/2511.14679v1) | Alexander M. Maier, Benjamin H√§sler et al. | In Markov networks, measurement blackouts with unknown frequency compromise observations such that thermodynamic quantities can no longer be inferred reliably. In particular, the observed currents neither discern equilibrium from non-equilibrium nor can they be used in extant estimators of entropy production. Our strategy to eliminate these effects is based on formally attributing the blackouts to a second channel connecting states. The unknown frequency of blackouts and the true underlying transition rates can be determined from the short-time limit of observed waiting-time distributions. A post-modification of observed trajectory data yields a virtual effective dynamics from which the lower bound on entropy production based on thermodynamic uncertainty relations can be recovered fully. Moreover, the post-processed data can be used in waiting-time based estimators. Crucially, our strategy does neither require the blackouts to occur homogeneously nor symmetrically under time-reversal. |
| 2025-11-18 | [High-resolution weak lensing mass mapping from DES-Y3 data using diffusion-based prior](http://arxiv.org/abs/2511.14667v1) | Supranta S. Boruah, Michael Jacob et al. | High-resolution mapping of cosmic mass distribution is essential for a variety of astrophysical applications including understanding cosmic structure formation, and galaxy formation and evolution. However dark matter is not directly observed and therefore we need advanced methods for solving inverse problems to reconstruct the underlying cosmic matter distribution. Here, we train a generative diffusion model and use it in the Diffusion Posterior Sampling (DPS) framework to reconstruct mass maps from Dark Energy Survey-Year 3 (DES-Y3) weak gravitational lensing data at high (1 arcminute) resolution. We show that the standard DPS results are biased, but they can be easily corrected by scaling the log-likelihood score during the diffusion process, yielding unbiased results with proper uncertainty quantification. The resulting mass maps reveal cosmic structures with enhanced detail, opening the door for improved astrophysical studies using the obtained mass maps. |
| 2025-11-18 | [Automated Prediction of Thermodynamic Properties via Bayesian Free-Energy Reconstruction from Molecular Dynamics](http://arxiv.org/abs/2511.14655v1) | Ekaterina Spirande, Timofei Miryashkin et al. | Accurate free-energy calculations are essential for predicting thermodynamic properties and phase stability, but existing methods are limited: phonon-based approaches neglect anharmonicity and liquids, while molecular dynamics (MD) is computationally demanding, neglects low-temperature quantum effects, and often requires manual planning and post-processing of simulations. We present a unified workflow that reconstructs the Helmholtz free-energy surface from MD data using Gaussian Process Regression (GPR), augmented with zero-point energy corrections from harmonic/quasi-harmonic theory. The framework propagates statistical uncertainties, mitigates finite-size effects, and employs active learning to optimize sampling in the volume-temperature space. It applies seamlessly to both crystalline and liquid phases. We demonstrate the methodology by computing heat capacities, thermal expansion, isothermal and adiabatic bulk moduli, and melting properties for nine elemental FCC and BCC metals using 20 classical and machine-learned interatomic potentials, with all predictions accompanied by quantified confidence intervals. Automated, general, and uncertainty-aware, the workflow advances high-throughput thermodynamics and provides a systematic benchmark for interatomic potentials. |
| 2025-11-18 | [Forecasting Synchrotron Spectral Parameters with QUIJOTE-MFI2 in combination with Planck and WMAP](http://arxiv.org/abs/2511.14572v1) | Ana Almeida, Jos√© Alberto Rubi√±o-Mart√≠n et al. | We present a parametric component separation forecast for the QUIJOTE-MFI2 instrument (10-20 GHz), assessing its impact on constraining polarised synchrotron emission at $1^\circ$ FWHM and $N_{\rm side}=64$. Using simulated sky maps based on power-law and curved synchrotron spectra, we show that adding QUIJOTE-MFI2 to existing WMAP+$Planck$+MFI data yields statistically unbiased parameter estimates with substantial uncertainty reductions: improvement factors reach $\sim$10 for the synchrotron spectral index ($Œ≤_s$), $\sim$5 for the curvature parameter ($C_s$), and $\sim$43 for polarisation amplitudes in bright regions. Deep QUIJOTE cosmological fields enable $Œ≤_s$ constraints even in intrinsically low SNR regions where WMAP+$Planck$ alone remain prior-dominated. Current combined sensitivities are insufficient to detect a synchrotron curvature of $C_s=-0.052$ on a pixel-by-pixel basis, but a $2œÉ$ detection is achievable for $|C_s|\gtrsim 0.14$ in the brightest regions of the Galactic plane. In those deep cosmological fields, combining QUIJOTE-MFI2 with WMAP and $Planck$ reduces the median synchrotron residual at 100 GHz by a factor 2.2 (to 0.022 $Œº$K$_{\rm CMB}$) and the total residual by a factor 1.8 (to 0.030 $Œº$K$_{\rm CMB}$). These results demonstrate that QUIJOTE-MFI2 will provide critical low-frequency information for modelling Galactic synchrotron emission, offering valuable complementary constraints for future CMB surveys such as LiteBIRD and the Simons Observatory. |
| 2025-11-18 | [Uncertainty analysis of URANS simulations coupled with an anisotropic pressure fluctuation model](http://arxiv.org/abs/2511.14564v1) | Ali Eidi, Richard P. Dwight | Accurate prediction of pressure and velocity fluctuations in turbulent flows is essential for understanding flow-induced vibration and structural fatigue. This study investigates the role of turbulence model parameter uncertainty in such predictions using a combination of global sensitivity analysis, surrogate modeling, and Bayesian inference. The methodology is applied to two fluid-only flow cases: turbulent channel flow and turbulent annular flow. In the channel flow case, calibrated parameter distributions lead to improved agreement with reference data. In the annular case, limited parameter identifiability is observed, though predictions remain consistent with high-fidelity trends. The results demonstrate both the potential and limitations of model calibration strategies in wall-bounded turbulent flows. |
| 2025-11-18 | [A Bayesian INLA-SPDE Approach to Spatio-Temporal Point-Grid Fusion with Change-of-Support and Misaligned Covariates](http://arxiv.org/abs/2511.14535v1) | Weiyue Zheng, Andrew Elliott et al. | We propose a spatio-temporal data-fusion framework for point data and gridded data with variables observed on different spatial supports. A latent Gaussian field with a Mat√©rn-SPDE prior provides a continuous space representation, while source-specific observation operators map observations to both point measurements and gridded averages, addressing change-of-support and covariate misalignment. Additionally incorporating temporal dependence enables prediction at unknown locations and time points. Inference and prediction are performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equations approach, which delivers fast computation with uncertainty quantification. Our contributions are: a hierarchical model that jointly fuses multiple data sources of the same variable under different spatial and temporal resolutions and measurement errors, and a practical implementation that incorporates misaligned covariates via the same data fusion framework allowing differing covariate supports. We demonstrate the utility of this framework via simulations calibrated to realistic sensor densities and spatial coverage. Using the simulation framework, we explore the stability and performance of the approach with respect to the number of time points and data/covariate availability, demonstrating gains over single-source models through point and gridded data fusion. We apply our framework to soil moisture mapping in the Elliot Water catchment (Angus, Scotland). We fuse in-situ sensor data with aligned and misaligned covariates, satellite data and elevation data to produce daily high resolution maps with uncertainty. |
| 2025-11-17 | [Stellar Flares in the TESS Light Curves of Planet-hosting M dwarfs](http://arxiv.org/abs/2511.13716v1) | Benjamin K. Capistrant, Jason Dittmann | M dwarfs are magnetically active stars that frequently produce flares, which have implications for both stellar evolution and exoplanet studies. Flare occurrence rates and activity levels of M dwarfs correlate with stellar characteristics such as age, mass, and rotation period. We search TESS observations of a known active population of M dwarfs as well as a volume-limited sample of M dwarfs within 15 parsecs. We detect flares in the light curves of these stars, including 276 of 538 M dwarfs within 15 pc, and calculate cumulative flare frequency distributions (FFDs) for each star. Based on flaring behavior, we categorize stars into relatively higher and lower activity groups and fit power laws to their FFDs to compare the power law exponent ($Œ±$) across activity levels. We find $Œ±=1.99 \pm 0.07$ for the combined FFD of the lower activity M dwarfs, compared to averages of $Œ±= 1.94 \pm 0.58$ for highly active stars with 10-100 detected flares, and $Œ±= 2.03 \pm 0.43$ for those with > 100 detected flares, suggesting little evolution in the power law distribution of flares as M dwarfs transition from high to low activity states. The uncertainties for the active star groups reflect the standard deviation of $Œ±$ values across individual stars within each subset. Because stellar flares and associated stellar activity complicate exoplanet observations, we also examine the subset of M dwarfs with JWST transmission spectroscopy follow-up observations in Cycles 1-3. The flares we detect for these targets are consistent with the broader 15 pc sample, providing context for interpreting planetary atmosphere retrievals from JWST spectra. |
| 2025-11-17 | [The Scatter of the Many Outweighs the Scatter of the Few: Systematic Error Asymmetry in Steeply-Falling Mass Functions for High-Redshift JWST Galaxies](http://arxiv.org/abs/2511.13708v1) | Jay R. Krishnan, Kevork N. Abazajian | The discovery of massive, high redshift galaxies with JWST has been argued to challenge $Œõ$CDM: such systems would require extremely rare halos and baryon-to-stellar-mass conversion efficiencies unphysically approaching--or exceeding--100%. If confirmed at galaxy formation forbidden efficiencies, these galaxies could signal new physics beyond standard cosmological structure formation. We develop a galaxy model framework that ties the linear power spectrum to the inferred efficiencies of galaxy growth in order to test the structure formation models. In addition, we incorporate multiple sources of error, including (i) observational sample variance, (ii) asymmetric scatter induced by the steepness of the high-mass halo tail, and (iii) systematic uncertainties in stellar mass estimates. We find that the inferred efficiency of star formation is dominated by systematic uncertainties on the spectral energy distribution inferred stellar mass of the JWST detected galaxies. The systematic uncertainty augments the asymmetry in scatter that largely brings the inferred efficiencies to be in line with that expected from early galaxy formation models. Our framework can be used to test $Œõ$CDM as errors are reduced and further detections are made. |
| 2025-11-17 | [Scientific Data Compression and Super-Resolution Sampling](http://arxiv.org/abs/2511.13675v1) | Minh Vu, Andrey Lokhov | Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity. |
| 2025-11-17 | [Sense and Sensitivity - I. Uncertainty analysis of the gas-phase chemistry in AGB outflows](http://arxiv.org/abs/2511.13638v1) | M. Van de Sande, M. Gueguen et al. | Chemical reaction networks are central to all chemical models. Each rate coefficient has an associated uncertainty, which is generally not taken into account when calculating the chemistry. We performed the first uncertainty analysis of a chemical model of C-rich and O-rich AGB outflows using the Rate22 reaction network. Quantifying the error on the model predictions enables us to determine the need for adding complexity to the model. Using a Monte Carlo sampling method, we quantified the impact of the uncertainties on the chemical kinetic data on the predicted fractional abundances and column densities. The errors are caused by a complex interplay of reactions forming and destroying each species. Parent species show an error on their envelope sizes, which is not caused by the uncertainty on their photodissociation rate, but rather the chemistry reforming the parent after its photodissociation. Using photodissociation models to estimate the envelope size might be an oversimplification. The error on the CO envelope impacts retrieved mass-loss rates by up to a factor of two. For daughter species, the error on the peak fractional abundance ranges from a factor of a few to three orders of magnitude, and is on average about 10\% of its value. This error is positively correlated with the error on the column density. The standard model suffices for many species, e.g., the radial distribution of cyanopolyynes and hydrocarbon radicals around IRC +10216. However, including spherical asymmetries, dust-gas chemistry, and photochemistry induced by a close-by stellar companion are still necessary to explain certain observations. |
| 2025-11-17 | [Variable-temperature attenuator calibration method for on-wafer microwave noise characterization of low-noise amplifiers](http://arxiv.org/abs/2511.13619v1) | Anthony J. Ardizzi, Jiayin Zhang et al. | Low-noise cryogenic microwave amplifiers are widely used in applications such as radio astronomy and quantum computing. On-wafer noise characterization of cryogenic low-noise transistors is desirable because it facilitates more rapid characterization of devices prior to packaging, but obtaining accurate noise measurements is difficult due to the uncertainty arising from the input loss and temperature gradients prior to the device-under-test (DUT). Here, we report a calibration method that enables the simultaneous determination of the backend noise temperature and effective-noise-ratio at the input plane of the DUT. The method is based on measuring the S-parameters and noise power of a series of attenuators at two or more distinct physical temperatures. We validate our method by measuring the noise temperature of InP HEMTs in 4-8 GHz. The calibration method can be generalized to measure the microwave noise temperature of any two-port device so long as a series of attenuators can be measured at two or more distinct physical temperatures. |
| 2025-11-17 | [A Gentle Introduction to Conformal Time Series Forecasting](http://arxiv.org/abs/2511.13608v1) | M. Stocker, W. Ma≈Çgorzewicz et al. | Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions. |
| 2025-11-17 | [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](http://arxiv.org/abs/2511.13586v1) | Yinuo Xu, Yan Cui et al. | Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.   To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.   To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction. |
| 2025-11-17 | [Coclique level structure for stochastic chemical reaction networks](http://arxiv.org/abs/2511.13569v1) | Simone Bruno, Yi Fu et al. | Continuous time Markov chains are commonly used as models for the stochastic behavior of chemical reaction networks. More precisely, these Stochastic Chemical Reaction Networks (SCRNs) are frequently used to gain a mechanistic understanding of how chemical reaction rate parameters impact the stochastic behavior of these systems. One property of interest is mean first passage times (MFPTs) between states. However, deriving explicit formulas for MFPTs can be highly complex. In order to address this problem, we first introduce the concept of coclique level structure and develop theorems to determine whether certain SCRNs have this feature by studying associated graphs. Additionally, we develop an algorithm to identify, under specific assumptions, all possible coclique level structures associated with a given SCRN. Finally, we demonstrate how the presence of such a structure in a SCRN allows us to derive closed form formulas for both upper and lower bounds for the MFPTs. Our methods can be applied to SCRNs taking values in a generic finite state space and can also be applied to models with non-mass-action kinetics. We illustrate our results with examples from the biological areas of epigenetics, neurobiology and ecology. |
| 2025-11-17 | [In-memory phononic learning toward cognitive mechanical intelligence](http://arxiv.org/abs/2511.13543v1) | Yuning Zhang, K. W. Wang | Modern autonomous systems are driving the critical need for next-generation adaptive materials and structures with embodied intelligence, i.e., the embodiment of memory, perception, learning, and decision-making within the mechanical domain. A fundamental challenge is the seamless and efficient integration of memory with information processing in a physically interpretable way that enables cognitive learning and decision-making under uncertainty. Prevailing paradigms, from intricate logic cascades to black-box morphological computing or physical neural networks, are seriously limited by trade-offs among efficiency, scalability, interpretability, transparency, and reliance on additional electronics. Here, we introduce in-memory phononic learning, a paradigm-shifting framework that unifies nonvolatile mechanical memory with wave-based perception within a phononic metastructure. Our system encodes spatial information into stable structural states as mechanical memory that directly programs its elastic wave-propagation landscape. This memory/wave-dynamics coupling enables effective sensory perception, decomposing complex patterns into informative geometric features through frequency-selective wave localization. Learning is created by optimizing input waveforms to selectively probe these features for memory-pattern classification, with decisions inferred directly from the output wave energy, thereby completing the entire information loop mechanically through an efficient and physically transparent mechanism without hidden architectures or electronics. This work transcends the paradigm of 'materials that compute' to cognitive matter capable of interpreting dynamic environments, paving the way for future intelligent structural-material systems with low power consumption, more direct interaction with surroundings, and enhanced cybersecurity and resilience in harsh conditions. |
| 2025-11-17 | [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](http://arxiv.org/abs/2511.13533v1) | Jeffrey Wen, Rizwan Ahmad et al. | In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data. |
| 2025-11-17 | [Simultaneous variances of Pauli strings, weighted independence numbers, and a new kind of perfection of graphs](http://arxiv.org/abs/2511.13531v1) | Zhen-Peng Xu, Jie Wang et al. | A set of Pauli stings is well characterized by the graph that encodes its commutatitivity structure, i.e., by its frustration graph. This graph provides a natural interface between graph theory and quantum information, which we explore in this work. We investigate all aspects of this interface for a special class of graphs that bears tight connections between the groundstate structures of a spin systems and topological structure of a graph. We call this class $\hbar$-perfect, as it extends the class of perfect and $h$-perfect graphs.   Having an $\hbar$-perfect graph opens up several applications: we find efficient schemes for entanglement detection, a connection to the complexity of shadow tomography, tight uncertainty relations and a construction for computing good lower on bounds ground state energies. Conversely this also induces quantum algorithms for computing the independence number. Albeit those algorithms do not immediately promise an advantage in runtime, we show that an approximate Hamilton encoding of the independence number can be achieved with an amount of qubits that typically scales logarithmically in the number of vertices. We also we also determine the behavior of $\hbar$-perfectness under basic graph operations and evaluate their prevalence among all graphs. |
| 2025-11-17 | [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](http://arxiv.org/abs/2511.13514v1) | Pragatheeswaran Vipulananthan, Kamal Premaratne et al. | Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{√∂}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process. |
| 2025-11-17 | [The Liquid Buffer: Multi-Year Storage for Defossilization and Energy Security under Climate Uncertainty](http://arxiv.org/abs/2511.13513v1) | Leonard G√∂ke, Jan Wohland et al. | The climate-driven uncertainty of renewable generation and electricity demand challenges energy security in net-zero energy systems. By introducing a scalable stochastic model that implicitly accounts for 51'840 climate years, this paper identifies multi-year storage of liquid hydrocarbons as a key option for managing climate uncertainty and ensuring energy security. In Europe, multi-year storage reduces system costs by 4.1%, fossil imports by 86%, and curtailment by 60%. The benefit of multi-year storage is that a renewable surplus in one year is not curtailed but converted to synthetic oil, with hydrogen as an intermediate product, and stored to balance a future deficit. We find that the required energy capacity for liquid hydrocarbons is 525 TWh, a quarter of the European Union's current oil and gas reserves, complemented by 116 TWh for hydrogen storage. Security of supply remains high and unserved energy only amounts to 0.0035 per thousand, well below the common target of 0.02 per thousand. |
| 2025-11-17 | [The bulk metal content of WASP-80 b from joint interior-atmosphere retrievals: Breaking degeneracies and exploring biases with panchromatic spectra](http://arxiv.org/abs/2511.13483v1) | Lorena Acu√±a-Aguirre, Laura Kreidberg et al. | WASP-80 b is an unusually low-density exoplanet in tension with the metal-rich composition expected for a planet of its mass. We aim to derive precise constraints on WASP-80 b's bulk metal mass fraction, atmospheric composition, and thermal structure. We conducted a suite of retrievals using three approaches: traditional interior-only, atmosphere-only, and joint interior-atmosphere retrievals. We coupled the open-source models GASTLI and petitRADTRANS, which describe planetary structure and thermal evolution, and atmospheric chemistry and clouds, respectively. Our retrievals combine mass and age with panchromatic spectra from JWST and HST in both transmission (0.5-4 $Œº$m) and emission (1-12 $Œº$m) as observational constraints. We identify two fiducial scenarios. In the first, WASP-80 b has an internal temperature consistent with its age in the absence of external heating sources, and its atmosphere is in chemical equilibrium, with an atmospheric metallicity M/H = 2.75$^{+0.88}_{-0.56}$x solar, a bulk metal mass fraction $Z_{planet}=0.12\pm0.02$, and a core mass $M_{core}=3.49^{+3.49}_{-1.59} \ M_{\oplus}$. In the second scenario, WASP-80 b may be inflated by an additional heat source - possibly induced by magnetic fields - with an atmospheric metallicity M/H = 10.00$^{+8.20}_{-4.75}$x solar, $Z_{planet}=0.28\pm0.11$, and $M_{core}=31.8^{+21.3}_{-17.5} \ M_{\oplus}$. The super-solar M/H and sub-solar C/O ratios in both scenarios suggest late pebble or planetesimal accretion, while additional heating is required to reconcile the data with the more massive core predicted by the core accretion paradigm. In general, joint retrievals are inherently affected by a degeneracy between atmospheric chemistry and internal structure. Together with flexible cloud treatment and an unweighted likelihood, this leads to larger uncertainties in bulk and atmospheric compositions than previously claimed. |
| 2025-11-17 | [Probing parameters estimation with Gaussian non-commutative measurements](http://arxiv.org/abs/2511.13451v1) | Alice P. G. Hall, Carlos H. S. Vieira et al. | Gaussian quantum states and channels are pivotal across many branches of quantum science and their applications, including the processing and storage of quantum information, the investigation of thermodynamics in the quantum regime, and quantum computation. The great advantage is that Gaussian states are experimentally accessible via their first and second statistical moments. In this work, we investigate parameter estimation for Gaussian states, in which the probe-state preparation stage involves two noncommutative Gaussian measurements on the position and momentum observables, introducing tunable parameters. The influence of these noncommutative Gaussian measurements is investigated through the quantum Fisher information (QFI). We showed that the QFI for characterizing Gaussian channels can be increased by adjusting the uncertainty parameters in the preparation of the probe state. Furthermore, if the probe is initially in a thermal state, probe-state preparation may generate quantum coherence in its energy basis. We showed that not only does the amount of coherence affect the improvement of the QFI, but also the rate of change of the coherence with respect to the parameter to be estimated. The proposed probe-state protocol is applied to two paradigmatic single-mode Gaussian channels, the attenuator and amplification channels, which are building blocks of Gaussian quantum information. Our results contribute to the use of coherence in quantum metrology and are experimentally feasible in quantum-optical devices. |
| 2025-11-17 | [The Intrinsic Angular - Momentum of Particles and the Resolution of the Spin-Statistics Theorem](http://arxiv.org/abs/2511.13360v1) | Enrico Santamato, Francesco De Martini | The traditional Standard Quantum Mechanics (SQM) theory is unable to solve the Spin-s problem, i.e., to justify the utterly important "Pauli Exclusion Principle". A complete and straightforward solution of the Spin-Statistics problem is presented based on the "Weyl Integrable Quantum Mechanics" (WIQM) theory. This theory provides a Weyl-gauge invariant formulation of the Standard Quantum Mechanics and reproduces successfully, with no restrictions, the full set of the quantum mechanical processes, including the formulation of Dirac's or Schr√∂dinger's equation, of Heisenberg's uncertainty relations, and of the nonlocal EPR correlations. etc. When the Weyl Integrable Quantum Mechanics is applied to a system made of many identical particles with spin, an additional constant property of all elementary particles enters naturally into play: the "intrinsic helicity", or the "intrinsic angular - momentum". This additional particle property, not considered by Standard Quantum Mechanics, determines the correct Spin-Statistics Connection (SSC) observed in Nature. All this leads to the consideration of a novel, most complete (in the EPR sense) quantum mechanical theory. |
| 2025-11-17 | [Basis Immunity: Isotropy as a Regularizer for Uncertainty](http://arxiv.org/abs/2511.13334v1) | Florent Segonne | Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.   In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.   Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.   This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts. |
| 2025-11-17 | [Sub-Solar Mass Intermediate Mass Ratio Inspirals: Waveform Systematics and Detection Prospects with Gravitational Waves](http://arxiv.org/abs/2511.13324v1) | Devesh Giri, Bhooshan Gadre | We investigate the detectability and waveform systematics of sub-solar mass intermediate mass-ratio inspirals (SSM-IMRIs), characterized by mass ratios $q \sim 10^2-10^4$. Using the black hole perturbation theory surrogate model \textsc{BHPTNRSur1dq1e4} as a reference, we assess the performance of the \textsc{IMRPhenomX} phenomenological family in the high-mass-ratio regime. We find that the inclusion of higher-order gravitational wave modes is critical; their exclusion may degrade the signal-to-noise ratio by factors of $\sim3-5$ relative to quadrupole-only templates. With optimal mode inclusion, SSM-IMRIs are observable out to luminosity distances of $\sim575$ Mpc ($z\sim0.12$) with Advanced LIGO and $\sim10.5$ Gpc ($z\sim1.4$) with the Einstein Telescope. However, we identify substantial systematic uncertainties in current phenomenological approximants. Matches between \textsc{IMRPhenomX} and the reference surrogate model \textsc{BHPTNRSur1dq1e4} degrade to values as low as 0.2 for edge-on inclinations, and fitting factors consistently fall below 0.9, indicating a significant loss of effectualness in template-bank searches. Bayesian parameter estimation reveals that these modeling discrepancies induce systematic biases that exceed statistical errors by multiple standard deviations, underscoring the necessity for waveform models calibrated to perturbation theory in the intermediate mass-ratio regime for robust detection and inference. |
| 2025-11-14 | [Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems](http://arxiv.org/abs/2511.11567v1) | Allen Emmanuel Binny, Anushri Dixit | Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. |
| 2025-11-14 | [Drone Swarm Energy Management](http://arxiv.org/abs/2511.11557v1) | Michael Z. Zgurovsky, Pavlo O. Kasyanov et al. | This note presents an analytical framework for decision-making in drone swarm systems operating under uncertainty, based on the integration of Partially Observable Markov Decision Processes (POMDP) with Deep Deterministic Policy Gradient (DDPG) reinforcement learning. The proposed approach enables adaptive control and cooperative behavior of unmanned aerial vehicles (UAVs) within a cognitive AI platform, where each agent learns optimal energy management and navigation policies from dynamic environmental states. We extend the standard DDPG architecture with a belief-state representation derived from Bayesian filtering, allowing for robust decision-making in partially observable environments. In this paper, for the Gaussian case, we numerically compare the performance of policies derived from DDPG to optimal policies for discretized versions of the original continuous problem. Simulation results demonstrate that the POMDP-DDPG-based swarm control model significantly improves mission success rates and energy efficiency compared to baseline methods. The developed framework supports distributed learning and decision coordination across multiple agents, providing a foundation for scalable cognitive swarm autonomy. The outcomes of this research contribute to the advancement of energy-aware control algorithms for intelligent multi-agent systems and can be applied in security, environmental monitoring, and infrastructure inspection scenarios. |
| 2025-11-14 | [Testing the cosmological Euler equation: viscosity, equivalence principle, and gravity beyond general relativity](http://arxiv.org/abs/2511.11554v1) | Ziyang Zheng, Malte Schneider et al. | We investigate how the cosmological Euler equation can be tested in the presence of viscous dark matter, violations of the equivalence principle (EP), and modifications of gravity, while relying on minimal theoretical assumptions. Extending the previous analysis, we generalize the observable $E_P$, which quantifies EP violation, to $\tilde{E}_P$, discuss the degeneracy between bulk and shear viscosities and EP-violating effects, and explicitly show that the EP can still be tested in the small-viscosity limit. In addition, we identify a model-independent observable, $C_{\rm vis,0}$, which characterizes the present-day dark matter viscosity and can be measured from relativistic galaxy number counts by cross-correlating two galaxy populations. We perform forecasts for three forthcoming Stage-IV surveys: DESI, Euclid, and SKA Phase 2 (SKA2), and find that $C_{\rm vis,0}$ can be tightly constrained, at the level of $\mathcal{O}(10^{-6})$ or better in all cases. Among these surveys, SKA2 provides the tightest constraint, with a $1œÉ$ uncertainty of $1.08 \times 10^{-7}$ on $C_{\rm vis,0}$. |
| 2025-11-14 | [Looking at infrared background radiation anisotropies with Spitzer II. Small scale anisotropies and their implications for new and upcoming space surveys](http://arxiv.org/abs/2511.11501v1) | Aidan J. Kaminsky, Alexander Kashlinsky et al. | Spitzer-based cosmic infrared background (CIB) fluctuations at arcminute-to-degree scales indicate the presence of new populations, whereas sub-arcminute power arises from known $z\lesssim 6$ galaxies. We reconstruct the evolution of the near-IR CIB anisotropies on sub-arcminute scales by known galaxy populations. This method is based on, and significantly advanced over, the empirical reconstruction by \cite{Helgason2012} which is combined with the halo model connecting galaxies to their host dark matter (DM) halos. The modeled CIB fluctuations from known galaxies produce the majority of the observed small-scale signal down to statistical uncertainties of $< 10\%$ and we constrain the evolution of the halo mass regime hosting such galaxies. Thus the large-scale CIB fluctuations from new populations are produced by sources with negligible small-scale power. This appears to conflict with the presented Intra-halo light (IHL) models, but is accounted for if the new sources are at high $z$. Our analysis spanning several Spitzer datasets allows us to narrow the estimated contributions of remaining known galaxies to the CIB anisotropies to be probed potentially from surveys by new and upcoming space missions such as Euclid, SPHEREx, and Roman. Of these, the Roman surveys have the best prospects for measuring the source-subtracted CIB and probing the nature of the underlying new populations at $Œª<2\ Œº$m, followed by Euclid's surveys, while for SPHEREx the source-subtracted CIB signal from them appears significantly overwhelmed by the CIB from remaining known galaxies. |
| 2025-11-14 | [Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images](http://arxiv.org/abs/2511.11486v1) | Roman Kinakh, Gonzalo R. R√≠os-Mu√±oz et al. | Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows. |
| 2025-11-14 | [Risk-Aware Deep Reinforcement Learning for Dynamic Portfolio Optimization](http://arxiv.org/abs/2511.11481v1) | Emmanuel Lwele, Sabuni Emmanuel et al. | This paper presents a deep reinforcement learning (DRL) framework for dynamic portfolio optimization under market uncertainty and risk. The proposed model integrates a Sharpe ratio-based reward function with direct risk control mechanisms, including maximum drawdown and volatility constraints. Proximal Policy Optimization (PPO) is employed to learn adaptive asset allocation strategies over historical financial time series. Model performance is benchmarked against mean-variance and equal-weight portfolio strategies using backtesting on high-performing equities. Results indicate that the DRL agent stabilizes volatility successfully but suffers from degraded risk-adjusted returns due to over-conservative policy convergence, highlighting the challenge of balancing exploration, return maximization, and risk mitigation. The study underscores the need for improved reward shaping and hybrid risk-aware strategies to enhance the practical deployment of DRL-based portfolio allocation models. |
| 2025-11-14 | [Higher-order QCD corrections to top-quark pair production in association with a jet](http://arxiv.org/abs/2511.11431v1) | Simon Badger, Matteo Bechetti et al. | The production of a top-quark pair, the heaviest known elementary particle, in association with a light jet is a key process for studying the properties of the Standard Model of Particle Physics. Due to its significance as a signal process with considerable sensitivity to the top-quark mass and as a background process for new physics searches, it is crucial to predict differential cross sections with high precision. In this article, we present, for the first time, predictions for various kinematical observables at next-to-next-to-leading order in Quantum Chromodynamics. The perturbative behavior is analyzed, and uncertainties arising from missing higher-order contributions are substantially reduced. The necessary two-loop amplitudes have been evaluated in the leading-color approximation, and we provide estimates for the impact of the missing contributions. |
| 2025-11-14 | [Risk averse deterministic Kalman filters for uncertain dynamical systems](http://arxiv.org/abs/2511.11350v1) | Karl Kunisch, Jesper Schr√∂der | Taking a deterministic viewpoint this work investigates extensions of the Kalman-Bucy filter for state reconstruction to systems containing parametric uncertainty in the state operator. The emphasis lies on risk averse designs reducing the probability of large reconstruction errors. In a theoretical analysis error bounds in terms of the variance of the uncertainties are derived. The article concludes with a numerical implementation of two examples allowing for a comparison of risk neutral and risk averse estimators. |
| 2025-11-14 | [DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding](http://arxiv.org/abs/2511.11313v1) | Tanveer Hannan, Dimitrios Mallios et al. | Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material. |
| 2025-11-14 | [Simulating an Autonomous System in CARLA using ROS 2](http://arxiv.org/abs/2511.11310v1) | Joseph Abdo, Aditya Shibu et al. | Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360¬∞ light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS). |
| 2025-11-13 | [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](http://arxiv.org/abs/2511.10619v1) | Avrim Blum, Marten Garicano et al. | The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Œ©(k)$ and $Œ©(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied. |
| 2025-11-13 | [Optimizing the flight path for a scouting Uncrewed Aerial Vehicle](http://arxiv.org/abs/2511.10598v1) | Raghav Adhikari, Sachet Khatiwada et al. | Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty. |
| 2025-11-13 | [Carbox: an end-to-end differentiable astrochemical simulation framework](http://arxiv.org/abs/2511.10558v1) | Gijs Vermari√´n, Tommaso Grassi et al. | Since the first observations of interstellar molecules, astrochemical simulations have been employed to model and understand its formation and destruction path- ways. With the advent of high-resolution telescopes such as JWST and ALMA, the number of detected molecules has increased significantly, thereby creating a need for increasingly complex chemical reaction networks. To model such complex systems, we have developed Carbox, a new astrochemical simulation code that leverages the modern high-performance transformation framework Jax. With Jax enabling computational efficiency and differentiability, Carbox can easily utilize GPU acceleration, be used to study sensitivity and uncertainty, and interface with advances in Scientific Machine Learning. All of these features are crucial for modeling the molecules observed by current and next-generation telescopes. |
| 2025-11-13 | [VELOCE III. Reconstructing Radial Velocity Curves of Classical Cepheids](http://arxiv.org/abs/2511.10534v1) | Giordano Viviani, Richard I. Anderson | We present a novel framework for accurately reconstructing radial velocity (RV) curves of classical Cepheids (Cepheids) from sparsely sampled time-series data suitable for application in large spectroscopic surveys. The framework provides a set of priors for the principal components of RV curves established based on high-precision measurements from the VELOCE project; template RV curves of Cepheids can be readily extracted from our results. We demonstrate the ability of our framework to estimate unbiased pulsation average velocities, $v_Œ≥$, to within $20-30$m/s, and peak-to-peak amplitudes, $P2P$, to within $\sim 2\%$. Subsampling the initial data set, we show that $v_Œ≥$ and $P2P$ can be determined to within $\sim 0.35$ km/s and $\sim 6-7\%$, respectively, from as few as three observations. We fitted existing time-series RV data of Cepheids in the LMC and SMC using this framework and obtained typical RMSE of $0.5-2.0$ km/s. The typical total uncertainty on $v_Œ≥$ achieved for the SMC Cepheids is $\sim 0.85$ km/s, providing sensitivity to spectroscopic binaries (SB). We identified 8 SB1 systems; two and one of which are new detections in the LMC and SMC, respectively. This yields a single-lined SB fraction of $\sim 25\%$ and $29\%$ in the two galaxies, similar to the Milky Way's SB fraction of $29\%$ established as part of VELOCE. Despite their relatively small number, LMC Cepheids reproduce the known line-of-sight component of the LMC's large-scale rotation, which differs in the extremes by more than $80$km/s. The kinematics of the SMC are more complex and not sufficiently sampled by the available Cepheids. Our framework is designed to yield accurate $v_Œ≥$ and $P2P$ of Cepheids observed by large spectroscopic surveys, such as 4MOST, SDSS-V, and others, and will unlock new insights into the kinematics and multiplicity of evolved intermediate-mass stellar populations. |
| 2025-11-13 | [Evaluation of Grid-based Uncertainty Propagation for Collaborative Self-Calibration in Indoor Positioning Systems](http://arxiv.org/abs/2511.10526v1) | Andrea Jung, Paul Schwarzbach et al. | Radio-based localization systems conventionally require stationary reference points (e.g. anchors) with precisely surveyed positions, making deployment time-consuming and costly. This paper presents an empirical evaluation of collaborative self-calibration for Ultra-Wideband (UWB) networks, extending a discrete Bayesian approach based on grid-based uncertainty propagation. The enhanced algorithm reduces measurement availability requirements while maintaining positioning accuracy through probabilistic state estimation. We validate the approach using real-world data from controlled indoor UWB network experiments with 12 nodes in a static environment. Experimental evaluation demonstrates 0.28~m mean ranging error under line-of-sight conditions and 1.11~m overall ranging error across mixed propagation scenarios, achieving sub-meter positioning accuracy. Results demonstrate the algorithm's robustness to measurement noise and partial connectivity scenarios typical in industrial deployments. The findings contribute to automated UWB network initialization for indoor positioning applications, reducing infrastructure dependency compared to manual anchor calibration procedures. |
| 2025-11-13 | [Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling](http://arxiv.org/abs/2511.10501v1) | Georgios Chalkiadakis, Charilaos Akasiadis et al. | This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability. |
| 2025-11-13 | [Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance](http://arxiv.org/abs/2511.10475v1) | √áaƒürƒ± Eser, Zeynep Sonat Baltacƒ± et al. | Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM. |
| 2025-11-13 | [Riccati-ZORO: An efficient algorithm for heuristic online optimization of internal feedback laws in robust and stochastic model predictive control](http://arxiv.org/abs/2511.10473v1) | Florian Messerer, Yunfan Gao et al. | We present Riccati-ZORO, an algorithm for tube-based optimal control problems (OCP). Tube OCPs predict a tube of trajectories in order to capture predictive uncertainty. The tube induces a constraint tightening via additional backoff terms. This backoff can significantly affect the performance, and thus implicitly defines a cost of uncertainty. Optimizing the feedback law used to predict the tube can significantly reduce the backoffs, but its online computation is challenging.   Riccati-ZORO jointly optimizes the nominal trajectory and uncertainty tube based on a heuristic uncertainty cost design. The algorithm alternates between two subproblems: (i) a nominal OCP with fixed backoffs, (ii) an unconstrained tube OCP, which optimizes the feedback gains for a fixed nominal trajectory. For the tube optimization, we propose a cost function informed by the proximity of the nominal trajectory to constraints, prioritizing reduction of the corresponding backoffs. These ideas are developed in detail for ellipsoidal tubes under linear state feedback. In this case, the decomposition into the two subproblems yields a substantial reduction of the computational complexity with respect to the state dimension from $\mathcal{O}(n_x^6)$ to $\mathcal{O}(n_x^3)$, i.e., the complexity of a nominal OCP.   We investigate the algorithm in numerical experiments, and provide two open-source implementations: a prototyping version in CasADi and a high-performance implementation integrated into the acados OCP solver. |
| 2025-11-13 | [Correlations of ALMA CO(2-1) with JWST mid-infrared fluxes down to scale of $\lesssim$100 parsec in nearby star-forming galaxies from PHANGS](http://arxiv.org/abs/2511.10464v1) | Tao Jing, Cheng Li | We investigate the correlations of CO (2-1) emission (${I_{\rm CO}}$) with PAH (${I_{\rm F770W, PAH}}$ and ${I_{\rm F1130W}}$) and dust (${I_{\rm F2100W}}$) emission down to scales of $\lesssim$ 100 pc, by applying \raddest, a novel regression technique recently developed by T. Jing & C. Li (2025) that effectively handles uncertainties and outliers in datasets, to 19 nearby star-forming galaxies in the PHANGS sample. We find that for the majority of the data points in all galaxies, the scaling of ${I_{\rm CO}}$ with ${I_{\rm F770W, PAH}}$, ${I_{\rm F1130W}}$, and ${I_{\rm F2100W}}$ can be well described by log-linear relations, though with substantial dependence on ionization conditions (i.e., HII-like, composite-like, and AGN-like). Under given ionization conditions, significant galaxy-to-galaxy variations are identified, and are primarily attributed to variations of intercept $b$, which exhibits clear bimodality. This bimodality is related to the overall host galaxy star formation strength. The differences in slope $k$ and intrinsic scatter $œÉ$ across different MIR bands (${I_{\rm F770W, PAH}}$, ${I_{\rm F1130W}}$, and ${I_{\rm F2100W}}$) are minor compared to their galaxy-to-galaxy variations. All parameters ($k$, $b$, and $œÉ$) depend on the spatial scale of measurement, suggesting that the coupling among CO, PAH, and dust is regulated by different mechanisms at varying scales. We identify non-log-linear behaviors in the brightest regions, where deviations are primarily characterized by flattening of slope. No significant (3$œÉ$) correlations are found between global properties and the best-fit parameters. We discuss the comparison to previous studies and plausible physics behind the statistical results obtained in this work. |
| 2025-11-13 | [A scalable and accurate framework for self-calibrating null depth retrieval using neural posterior estimation](http://arxiv.org/abs/2511.10455v1) | Baoyi Zeng, Marc-Antoine Martinod et al. | Accurate null depth retrieval is critical in nulling interferometry. However, achieving accurate null depth calibration is challenging due to various noise sources, instrumental imperfections, and the complexity of real observational environments. These challenges necessitate advanced calibration techniques that can efficiently handle such uncertainties while maintaining a high accuracy. This paper aims to incorporate machine-learning techniques with a Bayesian inference to improve the accuracy and efficiency of null depth retrieval in nulling interferometry. Specifically, it explores the use of neural posterior estimation (NPE) to develop models that overcome the computational limitations of conventional methods, such as numerical self-calibration (NSC), providing a more robust solution for accurate null depth calibration. An NPE-based model was developed, with a simulator that incorporates real data to better represent specific conditions. The model was tested on both synthetic and observational data from the LBTI nuller for evaluation. The NPE model successfully demonstrated improved efficiency, achieving results comparable to current methods in use. It achieved a null depth retrieval accuracy down to a few $10^{-4}$ on real observational data, matching the performance of conventional approaches while offering significant computational advantages, reducing the data retrieval time to one-quarter of the time required by self-calibration methods. The NPE model presents a practical and scalable solution for null depth calibration in nulling interferometry, offering substantial improvements in efficiency over existing methods with a better precision and application to other interferometric techniques. |
| 2025-11-10 | [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](http://arxiv.org/abs/2511.07403v1) | Hunar Batra, Haoqin Tu et al. | Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning. |
| 2025-11-10 | [The position of SN 1987A](http://arxiv.org/abs/2511.07394v1) | C. Tegkelidis, J. Larsson et al. | The accurate positional measurement of Supernova (SN) 1987A is important for determining the kick velocity of its compact object and the velocities of the ejecta and various shock components. In this work, we perform absolute astrometry to determine the position of SN 1987A. We used multi-epoch Hubble Space Telescope imaging to model the early ejecta and the equatorial ring (ER). We combined our measurements and obtained the celestial coordinates in the International Celestial Reference System (ICRS) by registering the observations onto Gaia Data Release 3. The final average position of the different measurements is ${\alpha = 5^{\mathrm{h}}~ 35^{\rm{m}}~ 27^{\rm{s}}.9884(30)}$, ${\delta = -69^{\circ}~ 16'~ 11''.1134(136)}$ (ICRS J2016). The early ejecta position is located 14 mas south and 16 mas east of the ER center, with the offset being significant at 96% confidence. The offset may be due to instrument and/or filter-dependent systematics and registration uncertainties, though an intrinsic explosion offset relative to the ER remains possible. Image registration with proper motion corrections yields similar astrometry and a source proper motion of ${\mu_{\rm east} (\equiv \rm{PM_{\alpha }*}) = 1.60 \pm 0.15 ~\rm{mas ~ yr^{-1}}}$ and ${\mu_{\rm{north}} (\equiv \rm{PM_{\delta}}) = 0.44 \pm 0.09~\rm{mas ~ yr^{-1}}}$, in agreement with the typical local motion of the Large Magellanic Cloud. The absolute positional uncertainty of 21 mas adds a systematic uncertainty to the sky-plane kick velocity of ${123}~(t/40~\rm{yr})^{-1}~\rm{km~s}^{-1}$, where $t$ is the time since the explosion. Comparing the location of the compact source observed with JWST to our updated position implies a sky-plane kick of ${399\pm148~\mathrm{km~s^{-1}}}$ and a 3D kick of ${472\pm126~\mathrm{km~s^{-1}}}$, which is consistent with previous estimates. |
| 2025-11-10 | [Policy Learning for Perturbance-wise Linear Quadratic Control Problem](http://arxiv.org/abs/2511.07388v1) | Haoran Zhang, Wenhao Zhang et al. | We study finite horizon linear quadratic control with additive noise in a perturbancewise framework that unifies the classical model, a constraint embedded affine policy class, and a distributionally robust formulation with a Wasserstein ambiguity set. Based on an augmented affine representation, we model feasibility as an affine perturbation and unknown noise as distributional perturbation from samples, thereby addressing constrained implementation and model uncertainty in a single scheme. First, we construct an implementable policy gradient method that accommodates nonzero noise means estimated from data. Second, we analyze its convergence under constant stepsizes chosen as simple polynomials of problem parameters, ensuring global decrease of the value function. Finally, numerical studies: mean variance portfolio allocation and dynamic benchmark tracking on real data, validating stable convergence and illuminating sensitivity tradeoffs across horizon length, trading cost intensity, state penalty scale, and estimation window. |
| 2025-11-10 | [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](http://arxiv.org/abs/2511.07377v1) | June Moh Goo, Zichao Zeng et al. | LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems. |
| 2025-11-10 | [Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning](http://arxiv.org/abs/2511.07368v1) | Dake Bu, Wei Huang et al. | Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RLVR and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of exploration and entropy stability in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.   To reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to chain-of-thought reweighting. We show that several phenomena recently observed in empirical studies arise naturally in this setting: (1) RLVR induces a squeezing effect, reducing reasoning entropy and forgetting some correct paths; (2) population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and (3) certain rare, high-uncertainty reasoning paths by the base model are responsible for solving hard problem instances.   Together, these explain why exploration -- even when confined to the base model's reasoning scope -- remains essential: it preserves access to rare but crucial reasoning traces needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further show that exploration strategies such as rejecting easy instances and KL regularization help preserve rare reasoning traces. Empirical simulations corroborate our theoretical results. |
| 2025-11-10 | [Offset-Free Robust Nonlinear Control Using Data-Driven Model: A Nonlinear Multi-Model Computationally Efficient Approach](http://arxiv.org/abs/2511.07255v1) | Carine Menezes Rebello, Erbet Almeida Costa et al. | Robust model predictive control (MPC) aims to preserve performance under model-plant mismatch, yet robust formulations for nonlinear MPC (NMPC) with data-driven surrogates remain limited. This work proposes an offset-free robust NMPC scheme based on symbolic regression (SR). Using a compact NARX structure, we identify interpretable surrogate models that explicitly represent epistemic (structural) uncertainty at the operating-zone level, and we enforce robustness by embedding these models as hard constraints. gest margins. The single-zone variant (RNMPC$_{SZ}$) was investigated. Synthetic data were generated within one operating zone to identify SR models that are embedded as constraints, while the nominal predictor remains fixed, and the multi-zone variant (RNMPC$_{MZ}$), zone-specific SR models from multiple operating zones are jointly enforced in the constraint set; at each set-point change, the nominal predictor is re-scheduled to the SR model of the newly active zone. In both cases, robustness is induced by the intersection of the admissible sets defined by the enforced SR models, without modifying the nominal cost or introducing ancillary tube dynamics. The approach was validated on a simulated pilot-scale electric submersible pump (ESP) system with pronounced nonlinearities and dynamically varying safety envelopes (downthrust and upthrust). RNMPC$_{SZ}$ and RNMPC$_{MZ}$ maintained disturbance tracking and rejection, and by intersecting models in the constraints, they increased margins and eliminated violations (especially near the upthrust), with a slight increase in settling time. Including up to four models per zone did not increase the time per iteration, maintaining real-time viability; RNMPC$_{MZ}$ presented the lar |
| 2025-11-10 | [Interpolation Problem for Multidimensional Stationary Processes with Missing Observations](http://arxiv.org/abs/2511.07240v1) | Oleksandr Masyutka, Mikhail Moklyachuk et al. | The problem of the mean-square optimal linear estimation of linear functionals which depend on the unknown values of a multidimensional continuous time stationary stochastic process is considered. Estimates are based on observations of the process with an additive stationary stochastic noise process at points which do not belong to some finite intervals of a real line. The problem is investigated in the case of spectral certainty, where the spectral densities of the processes are exactly known. Formulas for calculating the mean-square errors and spectral characteristics of the optimal linear estimates of functionals are proposed under the condition of spectral certainty. The minimax (robust) method of estimation is applied in the case spectral uncertainty, where spectral densities of the processes are not known exactly while some sets of admissible spectral densities of the processes are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible spectral densities |
| 2025-11-10 | [Extrapolation Problem for Multidimensional Stationary Sequences with Missing Observations](http://arxiv.org/abs/2511.07228v1) | Oleksandr Masyutka, Mikhail Moklyachuk et al. | This paper focuses on the problem of the mean square optimal estimation of linear functionals which depend on the unknown values of a multidimensional stationary stochastic sequence.   Estimates are based on observations of the sequence with an additive stationary noise sequence.   The aim of the paper is to develop methods of finding the optimal estimates of the functionals in the case of missing observations.   The problem is investigated in the case of spectral certainty where the spectral densities of the sequences are exactly known.   Formulas for calculating the mean-square errors and the spectral characteristics of the optimal linear estimates of functionals are derived under the condition of spectral certainty.   The minimax (robust) method of estimation is applied in the case of spectral uncertainty, where spectral densities of the sequences are not known exactly while sets of admissible spectral densities are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible densities. |
| 2025-11-10 | [Prospects for geoneutrino detection with JUNO](http://arxiv.org/abs/2511.07227v1) | Thomas Adam, Shakeel Ahmad et al. | Geoneutrinos, which are antineutrinos emitted during the decay of long-lived radioactive elements inside Earth, serve as a unique tool for studying the composition and heat budget of our planet. The Jiangmen Underground Neutrino Observatory (JUNO) experiment in China, which has recently completed construction, is expected to collect a sample comparable in size to the entire existing world geoneutrino dataset in less than a year. This paper presents an updated estimation of sensitivity to geoneutrinos of JUNO using the best knowledge available to date about the experimental site, the surrounding nuclear reactors, the detector response uncertainties, and the constraints expected from the TAO satellite detector. To facilitate comparison with present and future geological models, our results cover a wide range of predicted signal strengths. Despite the significant background from reactor antineutrinos, the experiment will measure the total geoneutrino flux with a precision comparable to that of existing experiments within its first few years, ultimately achieving a world-leading precision of about 8% over ten years. The large statistics of JUNO will also allow separation of the Uranium-238 and Thorium-232 contributions with unprecedented precision, providing crucial constraints on models of formation and composition of Earth. Observation of the mantle signal above the lithospheric flux will be possible but challenging. For models with the highest predicted mantle concentrations of heat-producing elements, a 3-sigma detection over six years requires knowledge of the lithospheric flux to within 15%. Together with complementary measurements from other locations, the geoneutrino results of JUNO will offer cutting-edge, high-precision insights into the interior of Earth, of fundamental importance to both the geoscience and neutrino physics communities. |
| 2025-11-10 | [Resilient by Design - Active Inference for Distributed Continuum Intelligence](http://arxiv.org/abs/2511.07202v1) | Praveen Kumar Donta, Alfreds Lapkovskis et al. | Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free-energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework. |
| 2025-11-07 | [The $ZŒ±^2$ correction to superallowed beta decays in effective field theory and implications for $|V_{ud}|$](http://arxiv.org/abs/2511.05446v1) | Zehua Cao, Richard J. Hill et al. | Superallowed ($0^+\rightarrow0^+$) beta decays currently provide the most precise extraction of quark mixing in the Standard Model. Their interpretation as a measurement of $|V_{ud}|$ relies on a reliable first-principles computation of QED radiative corrections expressed as a series in $Z\alpha$ and $\alpha$. In this work, we provide the first model-independent result for two-loop, $O(Z\alpha^2)$, long-distance radiative corrections where the nuclei are treated as heavy point-like particles.   We use renormalization group analysis to obtain new results at $O(Z\alpha^3)$ for the coefficient of double-logarithms in the ratio of the maximal beta energy to the inverse nuclear size, $\Em/R^{-1}$. We use the Kinoshita-Lee-Nauenberg theorem to obtain new results at $O(Z^2\alpha^3)$ for the coefficient of logarithms in the ratio of maximal beta energy to the electron mass, $\log(2\Em/\me)$. We identify a structure-dependent, and therefore short-distance, contribution to the traditional $Z\alpha^2$ correction that should be revisited.. We provide the first comprehensive update to the long-distance corrections in almost forty years and comment on the impact of our findings for extractions of $|V_{ud}|$. We find that shifts in the long-distance corrections are $2.5\times$ larger than past estimates of their uncertainty, $1.5\times$ larger than the statistical uncertainty from the combined fit of superallowed decays, and about $1/2$ the size of estimated systematic error, which stems dominantly from nuclear structure effects. |
| 2025-11-07 | [Adversarially Robust Multitask Adaptive Control](http://arxiv.org/abs/2511.05444v1) | Kasra Fallah, Leonardo F. Toso et al. | We study adversarially robust multitask adaptive linear quadratic control; a setting where multiple systems collaboratively learn control policies under model uncertainty and adversarial corruption. We propose a clustered multitask approach that integrates clustering and system identification with resilient aggregation to mitigate corrupted model updates. Our analysis characterizes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior affect the expected regret of certainty-equivalent (CE) control across LQR tasks. We establish non-asymptotic bounds demonstrating that the regret decreases inversely with the number of honest systems per cluster and that this reduction is preserved under a bounded fraction of adversarial systems within each cluster. |
| 2025-11-07 | [Neutrino-Nucleus Scattering Cross Sections at Medium Energies](http://arxiv.org/abs/2511.05413v1) | Vishvas Pandey | The weak interactions of neutrinos with other Standard Model particles are well described within the Standard Model of particle physics. However, modern accelerator-based neutrino experiments employ nuclei as targets, where neutrinos interact with bound nucleons, turning a seemingly simple electroweak process into a complex many-body problem in nuclear physics. At the time of writing this Encyclopedia of Particle Physics chapter, neutrino-nucleus interactions remain one of the leading sources of systematic uncertainty in accelerator-based neutrino oscillation measurements.   This chapter provides a pedagogical overview of neutrino interactions with nuclei in the medium-energy regime, spanning a few hundred MeV to several GeV. It introduces the fundamental electroweak formalism, outlines the dominant interaction mechanisms - including quasielastic scattering, resonance production, and deep inelastic scattering - and discusses how nuclear effects such as Fermi motion, nucleon-nucleon correlations, meson-exchange currents, and final-state interactions modify observable cross sections. The chapter also presents a brief survey of the foundational and most widely used theoretical models for neutrino-nucleus cross sections, together with an overview of current and upcoming accelerator-based neutrino oscillation experiments that are shaping the field.   Rather than targeting experts, this chapter serves as a primer for advanced undergraduates, graduate students, and early-career researchers entering the field. It provides a concise foundation for understanding neutrino-nucleus scattering, its relevance to oscillation experiments, and its broader connections to both particle and nuclear physics. |
| 2025-11-07 | [Bayesian learning for accurate and robust biomolecular force fields](http://arxiv.org/abs/2511.05398v1) | Vojtech Kostal, Brennon L. Shanks et al. | Molecular dynamics is a valuable tool to probe biological processes at the atomistic level - a resolution often elusive to experiments. However, the credibility of molecular models is limited by the accuracy of the underlying force field, which is often parametrized relying on ad hoc assumptions. To address this gap, we present a Bayesian framework for learning physically grounded parameters directly from ab initio molecular dynamics data. By representing both model parameters and data probabilistically, the framework yields interpretable, statistically rigorous models in which uncertainty and transferability emerge naturally from the learning process. This approach provides a transparent, data-driven foundation for developing predictive molecular models and enhances confidence in computational descriptions of biophysical systems. We demonstrate the method using 18 biologically relevant molecular fragments that capture key motifs in proteins, nucleic acids, and lipids, and, as a proof of concept, apply it to calcium binding to troponin - a central event in cardiac regulation. |
| 2025-11-07 | [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](http://arxiv.org/abs/2511.05397v1) | Samarth Chopra, Alex McMoil et al. | While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: https://everydayvla.github.io/ |
| 2025-11-07 | [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](http://arxiv.org/abs/2511.05396v1) | Yiting He, Zhishuai Liu et al. | Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments. |
| 2025-11-07 | [Quantum-Uncertainty-Governed Spin Dynamics in s-d Coupled Systems](http://arxiv.org/abs/2511.05388v1) | Jie Zheng, Jiyong Kang et al. | We investigate quantum fluctuation effects arising from the Heisenberg uncertainty principle governing angular momentum operators in the full dynamical evolution of disentanglement-entanglement-disentanglement between itinerant electrons and localized magnetic moments under the s-d exchange interaction. Beyond the conventional deterministic spin-transfer torque, we identify an intrinsic channel for the transfer of spin quantum fluctuations. By extending the Landau-Lifshitz-Gilbert equation to include both quantum and thermal stochastic fields, we reveal a temperature regime where quantum fluctuations dominate spin dynamics. Furthermore, voltage-controlled magnetic anisotropy can exponentially amplify these quantum fluctuation signals, enabling their binary detection via tunneling magnetoresistance in magnetic tunnel junctions. These results establish a microscopic framework for quantum fluctuation-driven spin dynamics and provide a fundamental route toward spin-based quantum true random number generation. |
| 2025-11-07 | [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](http://arxiv.org/abs/2511.05330v1) | Jan-Hendrik Ewering, Robin E. Herrmann et al. | Embedding non-restrictive prior knowledge, such as energy conservation laws, in learning-based approaches is a key motive to construct physically consistent models from limited data, relevant for, e.g., model-based control. Recent work incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to obtain uncertainty-quantifying models that adhere to the underlying physical principles. However, these works rely on velocity or momentum data, which is rarely available in practice. In this paper, we consider dynamics learning with non-conservative Hamiltonian GPs, and address the more realistic problem setting of learning from input-output data. We provide a fully Bayesian scheme for estimating probability densities of unknown hidden states, of GP hyperparameters, as well as of structural hyperparameters, such as damping coefficients. Considering the computational complexity of GPs, we take advantage of a reduced-rank GP approximation and leverage its properties for computationally efficient prediction and training. The proposed method is evaluated in a nonlinear simulation case study and compared to a state-of-the-art approach that relies on momentum measurements. |
| 2025-11-07 | [Economic uncertainty and exchange rates linkage revisited: modelling tail dependence with high frequency data](http://arxiv.org/abs/2511.05315v1) | Nourhaine Nefzi, Abir Abid | The aim of this paper is to dig deeper into understanding the exchange rates and uncertainty dependence. Using the novel Baker et al. (2020)'s daily Twitter Uncertainty Index and BRICS exchange rates, we investigate their extreme tail dependence within an original time-varying copula framework. Our analysis makes several noteworthy results. Evidence for Indian, Russian and South African currencies indicates an elliptical copulas' dominance implying neither asymmetric features nor extreme movements in their dependence structure with the global economic uncertainty. Importantly, Brazilian and Chinese currencies tail dependence is upward trending suggesting a safe-haven role in times of high global economic uncertainty including the recent COVID-19 pandemic. In such circumstances, these markets offer opportunities to significant gains through portfolio diversification. |
| 2025-11-07 | [Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage](http://arxiv.org/abs/2511.05266v1) | Gabriel Serr√£o Seabra, Nikolaj T. M√ºcke et al. | Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment. |
| 2025-11-06 | [On the Exoplanet Yield of Gaia Astrometry](http://arxiv.org/abs/2511.04673v1) | Caleb Lammers, Joshua N. Winn | We re-examine the expected yield of Gaia astrometric planet detections using updated models for giant-planet occurrence, the local stellar population, and Gaia's demonstrated astrometric precision. Our analysis combines a semi-analytic model that clarifies key scaling relations with more realistic Monte Carlo simulations. We predict $7{,}500 \pm 2{,}100$ planet discoveries in the 5-year dataset (DR4) and $120{,}000 \pm 22{,}000$ over the full 10-year mission (DR5), with the dominant error arising from uncertainties in giant-planet occurrence. We evaluate the sensitivity of these forecasts to the detection threshold and the desired precision for measurements of planet masses and orbital parameters. Roughly $1{,}900 \pm 540$ planets in DR4 and $38{,}000 \pm 7{,}300$ planets in DR5 should have masses and orbital periods determined to better than $20$%. Most detections will be super-Jupiters ($3$ - $13 M_{\rm J}$) on $2$ - $5$AU orbits around GKM-type stars ($0.4$ - $1.3 M_\odot$) within $500$ pc. Unresolved binary stars will lead to spurious planet detections, but we estimate that genuine planets will outnumber them by a factor of $5$ or more. An exception is planets around M-dwarfs with $a < 1$AU, for which the false-positive rate is expected to be about $50$%. To support community preparation for upcoming data releases, we provide mock catalogs of Gaia exoplanets and planet-impostor binaries. |
| 2025-11-06 | [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](http://arxiv.org/abs/2511.04659v1) | Huaguan Chen, Wei Han et al. | Extreme precipitation nowcasting demands high spatiotemporal fidelity and extended lead times, yet existing approaches remain limited. Numerical Weather Prediction (NWP) and its deep-learning emulations are too slow and coarse for rapidly evolving convection, while extrapolation and purely data-driven models suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce a gray-box, fully three-dimensional nowcasting framework that directly processes volumetric radar reflectivity and couples physically constrained neural operators with datadriven learning. The model learns vertically varying 3D advection fields under a conservative advection operator, parameterizes spatially varying diffusion, and introduces a Brownian-motion--inspired stochastic term to represent unresolved motions. A residual branch captures small-scale convective initiation and microphysical variability, while a diffusion-based stochastic module estimates uncertainty. The framework achieves more accurate forecasts up to three-hour lead time across precipitation regimes and ranked first in 57\% of cases in a blind evaluation by 160 meteorologists. By restoring full 3D dynamics with physical consistency, it offers a scalable and robust pathway for skillful and reliable nowcasting of extreme precipitation. |
| 2025-11-06 | [Where to Experiment? Site Selection Under Distribution Shift via Optimal Transport and Wasserstein DRO](http://arxiv.org/abs/2511.04658v1) | Adam Bouyamourn | How should researchers select experimental sites when the deployment population differs from observed data? I formulate the problem of experimental site selection as an optimal transport problem, developing methods to minimize downstream estimation error by choosing sites that minimize the Wasserstein distance between population and sample covariate distributions. I develop new theoretical upper bounds on PATE and CATE estimation errors, and show that these different objectives lead to different site selection strategies. I extend this approach by using Wasserstein Distributionally Robust Optimization to develop a site selection procedure robust to adversarial perturbations of covariate information: a specific model of distribution shift. I also propose a novel data-driven procedure for selecting the uncertainty radius the Wasserstein DRO problem, which allows the user to benchmark robustness levels against observed variation in their data. Simulation evidence, and a reanalysis of a randomized microcredit experiment in Morocco (Cr\'epon et al.), show that these methods outperform random and stratified sampling of sites when covariates have prognostic R-squared > .5, and alternative optimization methods i) for moderate-to-large size problem instances ii) when covariates are moderately informative about treatment effects, and iii) under induced distribution shift. |
| 2025-11-06 | [Photodetection of Squeezed Light: a Whittaker-Shannon Analysis](http://arxiv.org/abs/2511.04657v1) | Jasper Kranias, Christian Drago et al. | The Whittaker-Shannon decomposition provides a temporally localized description of squeezed light, making it applicable in the CW limit and leading to a definition of squeezing strength based on the number of photon pairs at a time. We show examples of its usefulness by calculating quadrature variance in a homodyne detection scheme, coincidence detection probabilities in the continuous-wave limit, and analyzing the Hong-Ou-Mandel effect for strongly squeezed light. Quadrature uncertainty falls farther below the shot noise limit when squeezing is strong, but effects due to correlations between photon pairs are most significant with weak squeezing. Our analysis extends previous results to more general scenarios, and we leverage the Whittaker-Shannon formalism to interpret them based on the temporal properties of photon pairs. |
| 2025-11-06 | [Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for Supply Chains of Hardware OEMs](http://arxiv.org/abs/2511.04616v1) | Elvis Agbenyega, Cody Quick | Resilient supply chains are critical, especially for Original Equipment Manufacturers (OEMs) that power today's digital economy. Safety Stock dimensioning-the computation of the appropriate safety stock quantity-is one of several mechanisms to ensure supply chain resiliency, as it protects the supply chain against demand and supply uncertainties. Unfortunately, the major approaches to dimensioning safety stock heavily assume that demand is normally distributed and ignore future demand variability, limiting their applicability in manufacturing contexts where demand is non-normal, intermittent, and highly skewed. In this paper, we propose a data-driven approach that relaxes the assumption of normality, enabling the demand distribution of each inventory item to be analytically determined using Kernel Density Estimation. Also, we extended the analysis from historical demand variability to forecasted demand variability. We evaluated the proposed approach against a normal distribution model in a near-world inventory replenishment simulation. Afterwards, we used a linear optimization model to determine the optimal safety stock configuration. The results from the simulation and linear optimization models showed that the data-driven approach outperformed traditional approaches. In particular, the data-driven approach achieved the desired service levels at lower safety stock levels than the conventional approaches. |
| 2025-11-06 | [Analyzing the topological structure of composite dynamical systems](http://arxiv.org/abs/2511.04603v1) | Michael Robinson, Michael L. Szulczewski et al. | This chapter explores dynamical structural equation models (DSEMs) and their nonlinear generalizations into sheaves of dynamical systems. It demonstrates these two disciplines on part of the food web in the Bering Sea. The translation from DSEMs to sheaves passes through a formal construction borrowed from electronics called a netlist that specifies how data route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations can be tested for consistency, how missing data can be inferred, and how uncertainty about the observations can be quantified. Sheaf modeling provides a coherent mathematical framework for studying the interaction of various dynamical subsystems that together determine a larger system. |
| 2025-11-06 | [Machine Learning for Electron-Scale Turbulence Modeling in W7-X](http://arxiv.org/abs/2511.04567v1) | Ionut-Gabriel Farcas, Don Lawrence Carl Agapito Fernando et al. | Constructing reduced models for turbulent transport is essential for accelerating profile predictions and enabling many-query tasks such as uncertainty quantification, parameter scans, and design optimization. This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model predicts the ETG heat flux as a function of three plasma parameters: the normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of normalized electron temperature and density radial gradients ($\eta_e$), and the electron-to-ion temperature ratio ($\tau$). We first construct models across seven radial locations using regression and an active machine-learning-based procedure. This process initializes models using low-cardinality sparse-grid training data and then iteratively refines their training sets by selecting the most informative points from a pre-existing simulation database. We evaluate the prediction capabilities of our models using out-of-sample datasets with over $393$ points per location, and $95\%$ prediction intervals are estimated via bootstrapping to assess prediction uncertainty. We then investigate the construction of generalized reduced models, including a generic, position-independent model, and assess their heat flux prediction capabilities at three additional locations. Our models demonstrate robust performance and predictive accuracy comparable to the original reference simulations, even when applied beyond the training domain. |
| 2025-11-06 | [Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI](http://arxiv.org/abs/2511.04564v1) | Yoh-ichi Mototake, Makoto Sasaki | Physics-informed machine learning (PIML) integrates partial differential equations (PDEs) into machine learning models to solve inverse problems, such as estimating coefficient functions (e.g., the Hamiltonian function) that characterize physical systems. This framework enables data-driven understanding and prediction of complex physical phenomena. While coefficient functions in PIML are typically estimated on the basis of predictive performance, physics as a discipline does not rely solely on prediction accuracy to evaluate models. For example, Kepler's heliocentric model was favored owing to small discrepancies in planetary motion, despite its similar predictive accuracy to the geocentric model. This highlights the inherent uncertainties in data-driven model inference and the scientific importance of selecting physically meaningful solutions. In this paper, we propose a framework to quantify and analyze such uncertainties in the estimation of coefficient functions in PIML. We apply our framework to reduced model of magnetohydrodynamics and our framework shows that there are uncertainties, and unique identification is possible with geometric constraints. Finally, we confirm that we can estimate the reduced model uniquely by incorporating these constraints. |
| 2025-11-06 | [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](http://arxiv.org/abs/2511.04556v1) | Zihang Ding, Kun Zhang | Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource. |
| 2025-11-06 | [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](http://arxiv.org/abs/2511.04534v1) | Jonas E. Katona, Emily K. de Jong et al. | Reduced-order models (ROMs) can efficiently simulate high-dimensional physical systems, but lack robust uncertainty quantification methods. Existing approaches are frequently architecture- or training-specific, which limits flexibility and generalization. We introduce a post hoc, model-agnostic framework for predictive uncertainty quantification in latent space ROMs that requires no modification to the underlying architecture or training procedure. Using conformal prediction, our approach estimates statistical prediction intervals for multiple components of the ROM pipeline: latent dynamics, reconstruction, and end-to-end predictions. We demonstrate the method on a latent space dynamical model for cloud microphysics, where it accurately predicts the evolution of droplet-size distributions and quantifies uncertainty across the ROM pipeline. |
| 2025-11-05 | [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](http://arxiv.org/abs/2511.03724v1) | Richard Dewey, Janos Botyanszki et al. | AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players. |
| 2025-11-05 | [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](http://arxiv.org/abs/2511.03695v1) | Lipeng Zu, Hansong Zhou et al. | Offline reinforcement learning (RL) enables training from fixed data without online interaction, but policies learned offline often struggle when deployed in dynamic environments due to distributional shift and unreliable value estimates on unseen state-action pairs. We introduce Behavior-Adaptive Q-Learning (BAQ), a framework designed to enable a smooth and reliable transition from offline to online RL. The key idea is to leverage an implicit behavioral model derived from offline data to provide a behavior-consistency signal during online fine-tuning. BAQ incorporates a dual-objective loss that (i) aligns the online policy toward the offline behavior when uncertainty is high, and (ii) gradually relaxes this constraint as more confident online experience is accumulated. This adaptive mechanism reduces error propagation from out-of-distribution estimates, stabilizes early online updates, and accelerates adaptation to new scenarios. Across standard benchmarks, BAQ consistently outperforms prior offline-to-online RL approaches, achieving faster recovery, improved robustness, and higher overall performance. Our results demonstrate that implicit behavior adaptation is a principled and practical solution for reliable real-world policy deployment. |
| 2025-11-05 | [Final state sensitivity and fractal basin boundaries from coupled Chialvo neurons](http://arxiv.org/abs/2511.03671v1) | Bennett Lamb, Brandon B. Le | We investigate and quantify the basin geometry and extreme final state uncertainty of two identical electrically asymmetrically coupled Chialvo neurons. The system's diverse behaviors are presented, along with the mathematical reasoning behind its chaotic and nonchaotic dynamics as determined by the structure of the coupled equations. The system is found to be multistable with two qualitatively different attractors. Although each neuron is individually nonchaotic, the chaotic basin takes up the vast majority of the coupled system's state space, but the nonchaotic basin stretches to infinity due to chance synchronization. The boundary between the basins is found to be fractal, leading to extreme final state sensitivity. This uncertainty and its potential effect on the synchronization of biological neurons may have significant implications for understanding human behavior and neurological disease. |
| 2025-11-05 | [ALMA and JWST Imaging of $z\ >\ 6$ Quasars: No Spatial Position Offset Observed Between Quasars and Their Host Galaxies](http://arxiv.org/abs/2511.03659v1) | Aurora Wilde, Marcel Neeleman et al. | We present a study determining the spatial offset between the position of the supermassive black hole (as traced through their broad line regions) and the host galaxy in six $z > 6$ quasars. We determined the host galaxy's position from $\lesssim$$0.1^{\prime\prime}$ ($\lesssim$ 600 pc) resolution Atacama Large Millimeter/sub-millimeter Array (ALMA) [CII] 158 $\mu m$ and corresponding dust continuum imaging. We determined the quasar's position from $\lesssim$ 400 pc resolution James Webb Space Telescope Near-Infrared Camera (JWST NIRCam) imaging. We estimated the observational uncertainties on the quasar's position using astrometric data from the Global Astrometric Interferometer for Astrophysics (GAIA) of field stars within the NIRCam images. We find that all six quasars are found within the central $\sim 400$ pc of their host galaxy dust continuum and [CII] emission. Apparent offsets seen in rest-frame optical JWST observations are not detected in our ALMA data, suggesting they likely result from dust obscuration rather than a true physical separation between the SMBH and its host galaxy. Kinematic modeling of these data further reveals that none of the galaxies show evidence for recent merger activity, and most of the galaxies can be accurately modeled using a simple disk model. The lack of an offset supports theoretical models that predict that positional offset within these galaxies are either short-lived or intrinsically rare. |
| 2025-11-05 | [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](http://arxiv.org/abs/2511.03652v1) | Azizollah Taheri, Derya Aksaray | This paper addresses a motion planning problem to achieve spatio-temporal-logical tasks, expressed by syntactically co-safe linear temporal logic specifications (scLTL\next), in uncertain environments. Here, the uncertainty is modeled as some probabilistic knowledge on the semantic labels of the environment. For example, the task is "first go to region 1, then go to region 2"; however, the exact locations of regions 1 and 2 are not known a priori, instead a probabilistic belief is available. We propose a novel automata-theoretic approach, where a special product automaton is constructed to capture the uncertainty related to semantic labels, and a reward function is designed for each edge of this product automaton. The proposed algorithm utilizes value iteration for online replanning. We show some theoretical results and present some simulations/experiments to demonstrate the efficacy of the proposed approach. |
| 2025-11-05 | [Geometrically robust least squares through manifold optimization](http://arxiv.org/abs/2511.03644v1) | Jeremy Coulson, Alberto Padoan et al. | This paper presents a methodology for solving a geometrically robust least squares problem, which arises in various applications where the model is subject to geometric constraints. The problem is formulated as a minimax optimization problem on a product manifold, where one variable is constrained to a ball describing uncertainty. To handle the constraint, an exact penalty method is applied. A first-order gradient descent ascent algorithm is proposed to solve the problem, and its convergence properties are illustrated by an example. The proposed method offers a robust approach to solving a wide range of problems arising in signal processing and data-driven control. |
| 2025-11-05 | [Quantifying Weighted Morphological Content of Large-Scale Structures via Simulation-Based Inference](http://arxiv.org/abs/2511.03636v1) | M. H. Jalali Kanafi, S. M. S. Movahed | In this work, we perform a simulation-based forecasting analysis to compare the constraining power of two higher-order summary statistics of the large-scale structure (LSS), the Minkowski Functionals (MFs) and the Conditional Moments of Derivative (CMD), with a particular focus on their sensitivity to nonlinear and anisotropic features in redshift-space. Our analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations at redshift $z=0.5$, employing a likelihood-free inference framework implemented via neural posterior estimation. At the fiducial cosmology of the Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs components, improving the constraint precision by ${\sim}(44\%,\,52\%)$, ${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$, respectively. A joint configuration combining the MFs and CMD further enhances the precision by approximately ${\sim}27\%$ compared to the standard MFs alone, highlighting the complementary anisotropy-sensitive information captured by the CMD in contrast to the scalar morphological content encapsulated by the MFs. We further extend the forecasting analysis to a continuous range of cosmological parameter values and multiple smoothing scales. Our results show that, although the absolute forecast uncertainty for each component of summary statistics depends on the underlying parameter values and the adopted smoothing scale, the relative constraining power among the summary statistics remains nearly constant throughout. |
| 2025-11-05 | [LiveTradeBench: Seeking Real-World Alpha with Large Language Models](http://arxiv.org/abs/2511.03628v1) | Haofei Yu, Fenghai Li et al. | Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty. |
| 2025-11-05 | [Tensor-Efficient High-Dimensional Q-learning](http://arxiv.org/abs/2511.03595v1) | Junyi Wu, Dan Li | High-dimensional reinforcement learning faces challenges with complex calculations and low sample efficiency in large state-action spaces. Q-learning algorithms struggle particularly with the curse of dimensionality, where the number of state-action pairs grows exponentially with problem size. While neural network-based approaches like Deep Q-Networks have shown success, recent tensor-based methods using low-rank decomposition offer more parameter-efficient alternatives. Building upon existing tensor-based methods, we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor decomposition via improved block coordinate descent on discretized state-action spaces, incorporating novel exploration and regularization mechanisms. The key innovation is an exploration strategy that combines approximation error with visit count-based upper confidence bound to prioritize actions with high uncertainty, avoiding wasteful random exploration. Additionally, we incorporate a frequency-based penalty term in the objective function to encourage exploration of less-visited state-action pairs and reduce overfitting to frequently visited regions. Empirical results on classic control tasks demonstrate that TEQL outperforms conventional matrix-based methods and deep RL approaches in both sample efficiency and total rewards, making it suitable for resource-constrained applications, such as space and healthcare where sampling costs are high. |
| 2025-11-05 | [Approaches to the Inverse Fourier Transformation with Limited and Discrete Data](http://arxiv.org/abs/2511.03593v1) | Yu-Fei Ling, Min-Huan Chu et al. | We investigate several approaches to address the inverse problem that arises in the limited inverse Fourier transform (L-IDFT) of quasi-distributions. The methods explored include Tikhonov regularization, the Backus-Gilbert method, the Bayesian approach with Gaussian Random Walk (GRW) prior, and the feedforward artificial neural networks (ANNs). We evaluate the performance of these methods using both mock data generated from toy models and real lattice data from quasi distribution, and further compare them with the physics-driven $\lambda$-extrapolation approach. Our results demonstrate that the L-IDFT constitutes a moderately tractable inverse problem Except for the Backus-Gilbert method, all the other approaches are capable of correctly reconstructing the quasi-distributions in momentum space. In particular, the Bayesian approach with GRW and the feedforward ANNs yield more stable and accurate reconstructions. Based on these investigations, we conclude that, for a given L-IDFT problem, selecting an appropriate reconstruction method according to the input data and carefully assessing the potential systematic uncertainties are essential for obtaining reliable results. |
| 2025-11-04 | [Intercomparison of a High-Resolution Regional Climate Model Ensemble for Catchment-Scale Water Cycle Processes under Human Influence](http://arxiv.org/abs/2511.02799v1) | J. L. Roque, F. Da Silva Lopes et al. | Understanding regional hydroclimatic variability and its drivers is essential for anticipating the impacts of climate change on water resources and sustainability. Yet, considerable uncertainty remains in the simulation of the coupled land atmosphere water and energy cycles, largely due to structural model limitations, simplified process representations, and insufficient spatial resolution. Within the framework of the Collaborative Research Center 1502 DETECT, this study presents a coordinated intercomparison of regional climate model simulations designed for water cycle process analysis over Europe. We analyze the performance of simulations using the ICON and TSMP1 model systems and covering the period from 1990 to 2020, comparing against reference datasets (E-OBS, GPCC, and GLEAM). We focus on 2 m air temperature, precipitation and evapotranspiration over four representative basins, the Ebro, Po, Rhine, and Tisa, within the EURO CORDEX domain.   Our analysis reveals systematic cold biases across all basins and seasons, with ICON generally outperforming TSMP1. Precipitation biases exhibit substantial spread, particularly in summer, reflecting the persistent challenge of accurately simulating precipitation. ICON tends to underestimate evapotranspiration, while TSMP1 performs better some seasons. Sensitivity experiments further indicate that the inclusion of irrigation improves simulation performance in the Po basin, which is intensively irrigated, and that higher-resolution sea surface temperature forcing data improves the overall precipitation representation. This baseline evaluation provides a first assessment of the DETECT multimodel ensemble and highlights key structural differences influencing model skill across hydroclimatic regimes. |
| 2025-11-04 | [Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](http://arxiv.org/abs/2511.02748v1) | Farhad Rezazadeh, Hatim Chergui et al. | We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative "what-if" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening. |
| 2025-11-04 | [Bayesian full waveform inversion with learned prior using deep convolutional autoencoder](http://arxiv.org/abs/2511.02737v1) | Shuhua Hu, Mrinal K Sen et al. | Full waveform inversion (FWI) can be expressed in a Bayesian framework, where the associated uncertainties are captured by the posterior probability distribution (PPD). In practice, solving Bayesian FWI with sampling-based methods such as Markov chain Monte Carlo (MCMC) is computationally demanding because of the extremely high dimensionality of the model space. To alleviate this difficulty, we develop a deep convolutional autoencoder (CAE) that serves as a learned prior for the inversion. The CAE compresses detailed subsurface velocity models into a low-dimensional latent representation, achieving more effective and geologically consistent model reduction than conventional dimension reduction approaches. The inversion procedure employs an adaptive gradient-based MCMC algorithm enhanced by automatic differentiation-based FWI to compute gradients efficiently in the latent space. In addition, we implement a transfer learning strategy through online fine-tuning during inversion, enabling the framework to adapt to velocity structures not represented in the original training set. Numerical experiments with synthetic data show that the method can reconstruct velocity models and assess uncertainty with improved efficiency compared to traditional MCMC methods. |
| 2025-11-04 | [Measuring the expansion history of the Universe with DESI Cosmic Chronometers](http://arxiv.org/abs/2511.02730v1) | S. I. Loubser | Studying large samples of massive, passively evolving galaxies (called cosmic chronometers, CC) provides us with the unique ability to measure the Universe's expansion history without assuming a cosmological model. The Dark Energy Spectroscopic Instrument (DESI) DR1 is currently the largest, publicly available, homogeneous set of galaxies with reliable spectroscopic redshifts, and covers a wide range in redshift. We extracted all massive galaxies (stellar mass $\log M_{\star}/M_{\odot} > 10.75$, and velocity dispersion $\sigma > 280$ km s$^{-1}$), with no emission in [OII] $\lambda$ 3727 $\r{A}$, with reliable redshifts as well as reliable D4000$_{\rm n}$ measurements from DR1. From this sample of 360 000 massive, passive galaxies, we used D4000$_{\rm n}$ and the method of cosmic chronometers to get three new direct, independent measurements of $H(z)=$ 88.48 $\pm\ 0.57(\rm stat) \pm 12.32(\rm syst)$, $H(z)=$ 119.45 $\pm\ 6.39(\rm stat) \pm 16.64(\rm syst)$, and $H(z)= 108.28 \pm 10.07(\rm stat) \pm 15.08(\rm syst)$ $\rm km\ s^{-1}\ Mpc^{-1}$ at $z=0.46$, $z=0.67$, and $z=0.83$, respectively. This sample, which covers $0.3 < z < 1.0$, is the largest CC sample to date, and we reach statistical uncertainties of 0.65$\%$, 5.35$\%$, and 9.30$\%$ on our three measurements. Our measurements show no significant tension with the $\textit{Planck}$ $\Lambda$CDM cosmology. In our analysis, we also illustrate that even amongst samples of massive, passive galaxies, the effect of downsizing can clearly be seen. |
| 2025-11-04 | [Mass ratio estimates for overcontact binaries using the derivatives of light curves. II. Systems with deep eclipses](http://arxiv.org/abs/2511.02629v1) | Shinjirou Kouzuma | This is the second paper that proposes a simple method for estimating mass ratios using the derivatives of light curves for overcontact binaries. In the first paper (Kouzuma 2023, ApJ, 958, 84) , we presented a method to estimate the mass ratios for systems exhibiting a double-peak feature in the second derivatives of their light curves around eclipses. This second paper focuses on overcontact systems that are not addressed in the first paper, that is, systems lacking a double peak in the second derivative. A sample of synthetic light curves for overcontact binaries consists of 89670, covering a parameter space typical of overcontact systems. On the basis of a recent study that proposed a new classification scheme using light-curve derivatives up to the fourth order, the sample light curves were classified. We found that time intervals between two local extrema in the derivatives are associated with the mass ratio in systems that exhibit a high degree of eclipse obscuration. Using regression analysis for the identified associations, we derived empirical formulae to estimate the mass ratio and its associated uncertainty. The application of our proposed method to real overcontact binary data demonstrated its effectiveness in providing reliable estimates for both values. |
| 2025-11-04 | [Model Parameter Reconstruction of Electroweak Phase Transition with TianQin and LISA: Insights from the Dimension-Six Model](http://arxiv.org/abs/2511.02612v1) | Aidi Yang, Chikako Idegawa et al. | We investigate the capability of TianQin and LISA to reconstruct the model parameters in the Lagrangian of new physics scenarios that can generate a strong first-order electroweak phase transition. Taking the dimension-six Higgs operator extension of the Standard Model as a representative scenario for a broad class of new physics models, we establish the mapping between the model parameter $\Lambda$ and the observable spectral features of the stochastic gravitational wave background. We begin by generating simulated data incorporating Time Delay Interferometry channel noise, astrophysical foregrounds, and signals from the dimensional-six model. The data are then compressed and optimized, followed by geometric parameter inference using both Fisher matrix analysis and Bayesian nested sampling with PolyChord, which efficiently handles high-dimensional, multimodal posterior distributions. Finally, machine learning techniques are employed to achieve precise reconstruction of the model parameter $\Lambda$. For benchmark points producing strong signals, parameter reconstruction with both TianQin and LISA yields relative uncertainties of approximately $20$--$30\%$ in the signal amplitude and sub-percent precision in the model parameter $\Lambda$. TianQin's sensitivity is limited to stronger signals within its optimal frequency band, whereas LISA can reconstruct parameters across a broader range of signal strengths. Our results demonstrate that reconstruction precision depends on signal strength, astrophysical foregrounds, and instrumental noise characteristics. |
| 2025-11-04 | [Trustworthy Quantum Machine Learning: A Roadmap for Reliability, Robustness, and Security in the NISQ Era](http://arxiv.org/abs/2511.02602v1) | Ferhat Ozgur Catak, Jungwon Seo et al. | Quantum machine learning (QML) is a promising paradigm for tackling computational problems that challenge classical AI. Yet, the inherent probabilistic behavior of quantum mechanics, device noise in NISQ hardware, and hybrid quantum-classical execution pipelines introduce new risks that prevent reliable deployment of QML in real-world, safety-critical settings. This research offers a broad roadmap for Trustworthy Quantum Machine Learning (TQML), integrating three foundational pillars of reliability: (i) uncertainty quantification for calibrated and risk-aware decision making, (ii) adversarial robustness against classical and quantum-native threat models, and (iii) privacy preservation in distributed and delegated quantum learning scenarios. We formalize quantum-specific trust metrics grounded in quantum information theory, including a variance-based decomposition of predictive uncertainty, trace-distance-bounded robustness, and differential privacy for hybrid learning channels. To demonstrate feasibility on current NISQ devices, we validate a unified trust assessment pipeline on parameterized quantum classifiers, uncovering correlations between uncertainty and prediction risk, an asymmetry in attack vulnerability between classical and quantum state perturbations, and privacy-utility trade-offs driven by shot noise and quantum channel noise. This roadmap seeks to define trustworthiness as a first-class design objective for quantum AI. |
| 2025-11-04 | [Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis](http://arxiv.org/abs/2511.02509v1) | R. Alberich, N. A. Cruz et al. | High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures. |
| 2025-11-04 | [Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification](http://arxiv.org/abs/2511.02469v1) | Kaito Takano, Masanori Hirano et al. | Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts. |
| 2025-11-04 | [Decentralized Voltage Control of AC Microgrids with Constant Power Loads using Control Barrier Functions](http://arxiv.org/abs/2511.02438v1) | Grigoris Michos, George C. Konstantopoulos | This paper proposes a novel nonlinear decentralized voltage controller for constrained regulation of meshed AC Microgrid networks with high penetration of constant power loads. Perceiving the load demand as an unknown disturbance, the network model is reformulated in a cascaded structure composed of a nominal, i.e. uncertainty-free, and an error subsystem. The latter captures the distance between the true and the nominal state trajectories, for which we prove boundedness via a suitable control barrier function. Under sufficient conditions, we prove asymptotic stability of the cascaded dynamics with respect to an equilibrium set and also provide an estimate of the region of attraction. In addition, it is rigorously shown that the proposed nonlinear control law also enforces constrained regulation around a rated voltage value, without the need of saturation devices. The operation of the closed-loop system is illustrated in a simulation scenario, demonstrating bounded operation and convergence to a neighbourhood of the desired reference vector. |
| 2025-10-31 | [WallGo investigates: Theoretical uncertainties in the bubble wall velocity](http://arxiv.org/abs/2510.27691v1) | Jorinde van de Vis, Philipp Schicho et al. | We examine theoretical uncertainties in state-of-the-art calculations of the bubble wall velocity during first-order cosmological phase transitions. By utilising the software WallGo for two extensions of the Standard Model, we find several $\mathcal{O}(1)$ uncertainties arising from the number of particles taken out of equilibrium, the logarithmically and power enhanced collision integrals, the treatment of thermal masses, the nucleation temperature, the $\tanh$ ansatz, and the perturbative order of the effective potential. However, we show that the linearisation of the Boltzmann equations is generally a good approximation with much smaller associated errors. We further clarify the limitations of the quasiparticle approximation in regions with negative mass squared. This study provides a detailed uncertainty budget and highlights where future efforts should be directed to improve the reliability of wall velocity and hence gravitational wave predictions. |
| 2025-10-31 | [Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in Creative Work](http://arxiv.org/abs/2510.27681v1) | Sean Kelley, David De Cremer et al. | As AI becomes more deeply embedded in knowledge work, building assistants that support human creativity and expertise becomes more important. Yet achieving synergy in human-AI collaboration is not easy. Providing AI with detailed information about a user's demographics, psychological attributes, divergent thinking, and domain expertise may improve performance by scaffolding more effective multi-turn interactions. We implemented a personalized LLM-based assistant, informed by users' psychometric profiles and an AI-guided interview about their work style, to help users complete a marketing task for a fictional startup. We randomized 331 participants to work with AI that was either generic (n = 116), partially personalized (n = 114), or fully personalized (n=101). Participants working with personalized AI produce marketing campaigns of significantly higher quality and creativity, beyond what AI alone could have produced. Compared to generic AI, personalized AI leads to higher self-reported levels of assistance and feedback, while also increasing participant trust and confidence. Causal mediation analysis shows that personalization improves performance indirectly by enhancing collective memory, attention, and reasoning in the human-AI interaction. These findings provide a theory-driven framework in which personalization functions as external scaffolding that builds common ground and shared partner models, reducing uncertainty and enhancing joint cognition. This informs the design of future AI assistants that maximize synergy and support human creative potential while limiting negative homogenization. |
| 2025-10-31 | [Deep learning denoising unlocks quantitative insights in operando materials microscopy](http://arxiv.org/abs/2510.27667v1) | Samuel Degnan-Morgenstern, Alexander E. Cohen et al. | Operando microscopy provides direct insight into the dynamic chemical and physical processes that govern functional materials, yet measurement noise limits the effective resolution and undermines quantitative analysis. Here, we present a general framework for integrating unsupervised deep learning-based denoising into quantitative microscopy workflows across modalities and length scales. Using simulated data, we demonstrate that deep denoising preserves physical fidelity, introduces minimal bias, and reduces uncertainty in model learning with partial differential equation (PDE)-constrained optimization. Applied to experiments, denoising reveals nanoscale chemical and structural heterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron phosphate (LFP), enables automated particle segmentation and phase classification in optical microscopy of graphite electrodes, and reduces noise-induced variability by nearly 80% in neutron radiography to resolve heterogeneous lithium transport. Collectively, these results establish deep denoising as a powerful, modality-agnostic enhancement that advances quantitative operando imaging and extends the reach of previously noise-limited techniques. |
| 2025-10-31 | [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](http://arxiv.org/abs/2510.27598v1) | Yunze Wu, Dayuan Fu et al. | AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark. |
| 2025-10-31 | [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](http://arxiv.org/abs/2510.27598v2) | Yunze Wu, Dayuan Fu et al. | AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark. |
| 2025-10-31 | [Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs](http://arxiv.org/abs/2510.27558v1) | Sushil Samuel Dinesh, Shinkyu Park | This paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models. |
| 2025-10-31 | [EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities](http://arxiv.org/abs/2510.27545v1) | Travis Davies, Yiqi Huang et al. | Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts. |
| 2025-10-31 | [Risk-constrained stochastic scheduling of multi-market energy storage systems](http://arxiv.org/abs/2510.27528v1) | Gabriel D. Patr√≥n, Di Zhang et al. | Energy storage can promote the integration of renewables by operating with charge and discharge policies that balance an intermittent power supply. This study investigates the scheduling of energy storage assets under energy price uncertainty, with a focus on electricity markets. A two-stage stochastic risk-constrained approach is employed, whereby electricity price trajectories or specific power markets are observed, allowing for recourse in the schedule. Conditional value-at-risk is used to quantify tail risk in the optimization problems; this allows for the explicit specification of a probabilistic risk limit. The proposed approach is tested in an integrated hydrogen system (IHS) and a battery energy storage system (BESS). In the joint design and operation context for the IHS, the risk constraint results in larger installed unit capacities, increasing capital cost but enabling more energy inventory to buffer price uncertainty. As shown in both case studies, there is an operational trade-off between risk and expected reward; this is reflected in higher expected costs (or lower expected profits) with increasing levels of risk aversion. Despite the decrease in expected reward, both systems exhibit substantial benefits of increasing risk aversion. This work provides a general method to address uncertainties in energy storage scheduling, allowing operators to input their level of risk tolerance on asset decisions. |
| 2025-10-31 | [Euclid: Systematic uncertainties from the halo mass conversion on galaxy cluster number count data analyses](http://arxiv.org/abs/2510.27505v1) | T. Gayoux, P. -S. Corasaniti et al. | The large catalogues of galaxy clusters expected from the Euclid survey will enable cosmological analyses of cluster number counts that require accurate cosmological model predictions. One possibility is to use parametric fits calibrated against $N$-body simulations, that capture the cosmological parameter dependence of the halo mass function. Several studies have shown that this can be obtained through a calibration against haloes with spherical masses defined at the virial overdensity. In contrast, if different mass definitions are used for the HMF and the scaling relation, a mapping between them is required. Here, we investigate the impact of such a mapping on the cosmological parameter constraints inferred from galaxy cluster number counts. Using synthetic data from $N$-body simulations, we show that the standard approach, which relies on assuming a concentration-mass relation, can introduce significant systematic bias. In particular, depending on the mass definition and the relation assumed, this can lead to biased constraints at more than 2$\sigma$ level. In contrast, we find that in all the cases we have considered, the mass conversion based on the halo sparsity statistics result in a systematic bias smaller than the statistical error. |
| 2025-10-31 | [Evaluation of Reference Equations of State for Density Prediction in Regasified LNG Mixtures Using High-Precision Experimental Data](http://arxiv.org/abs/2510.27502v1) | Daniel Lozano-Mart√≠n, Dirk Tuma et al. | This study evaluates the performance of three reference equations of state (EoS), AGA8-DC92, GERG-2008, and SGERG-88, in predicting the density of regasified liquefied natural gas (RLNG) mixtures. A synthetic nine-component RLNG mixture was gravimetrically prepared. High-precision density measurements were obtained using a single-sinker magnetic suspension densimeter over a temperature range of (250 to 350) K and pressures up to 20 MPa. The experimental data were compared with EoS predictions to evaluate their accuracy. AGA8-DC92 and GERG-2008 showed excellent agreement with the experimental data, with deviations within their stated uncertainty. In contrast, SGERG-88 exhibited significantly larger deviations for this RLNG mixture, particularly at low temperatures of (250 to 260) K, where discrepancies reached up to 3 %. Even at 300 K, deviations larger than 0.4 % were observed at high pressures, within the model's uncertainty, but notably higher than those of the other two EoSs. The analysis was extended to three conventional 11-component natural gas mixtures (labeled G420 NG, G431 NG, and G432 NG), previously studied by our group using the same methodology. While SGERG-88 showed reduced accuracy for the RLNG mixture, it performed reasonably well for these three mixtures, despite two of them have a very similar composition to the RLNG. This discrepancy is attributed to the lower CO2 and N2 content typical in RLNG mixtures, demonstrating the sensitivity of EoS performance to minor differences in composition. These findings highlight the importance of selecting appropriate EoS models for accurate density prediction in RLNG applications. |
| 2025-10-31 | [Differential Set Selection via Confidence-Guided Entropy Minimization](http://arxiv.org/abs/2510.27479v1) | Mar√≠a del Carmen Romero, Mariana del Fresno et al. | This paper addresses the challenge of identifying a minimal subset of discrete, independent variables that best predicts a binary class. We propose an efficient iterative method that sequentially selects variables based on which one provides the most statistically significant reduction in conditional entropy, using confidence bounds to account for finite-sample uncertainty. Tests on simulated data demonstrate the method's ability to correctly identify influential variables while minimizing spurious selections, even with small sample sizes, offering a computationally tractable solution to this NP-complete problem. |
| 2025-10-30 | [Boosting the cosmic 21-cm signal with exotic Lyman-$Œ±$ from dark matter](http://arxiv.org/abs/2510.26791v1) | Dominic Agius, Tracy Robyn Slatyer | The 21-cm signal from the epoch of cosmic dawn ($z \sim 10-30$) offers a powerful probe of new physics. One standard mechanism for constraining decaying dark matter from 21-cm observations relies on heating of the intergalactic medium by the decay products, an effect whose observability is entangled with the uncertain Lyman-$\alpha$ fluxes and X-ray heating from the first stars. In this Letter, we explore a novel mechanism, where the Lyman-$\alpha$ photons produced from dark matter decay initiate early Wouthuysen-Field coupling of the spin temperature to the gas temperature, thereby boosting the 21-cm signal. This mechanism provides constraints on dark matter that are less dependent on uncertainties associated with star formation than constraints on exotic heating. We study this effect for decaying dark matter with masses $m_{\chi}\sim20.4-27.2$ eV, where diphoton decay efficiently produces Lyman-series photons. We present forecasts for the Hydrogen Epoch of Reionization Array and the Square Kilometre Array, showing their potential to probe an unconstrained parameter space for light decaying DM, including axion-like particles. |
| 2025-10-30 | [Moments of parton distributions functions of the pion from lattice QCD using gradient flow](http://arxiv.org/abs/2510.26738v1) | Anthony Francis, Patrick Fritzsch et al. | We present a nonperturbative determination of the pion valence parton distribution function (PDF) moment ratios $\left\langle x^{n-1} \right\rangle / \left\langle x \right\rangle$ up to $n=6$, using the gradient flow in lattice QCD. As a testing ground, we employ SU($3$) isosymmetric gauge configurations generated by the OpenLat initiative with a pseudoscalar mass of $m_\pi \simeq 411~\text{MeV}$. Our analysis uses four lattice spacings and a nonperturbatively improved action, enabling full control over the continuum extrapolation, and the limit of vanishing flow time, $t\to0$. The flowed ratios exhibit O($a^2$) scaling across the ensembles, and the continuum-extrapolated results, matched to the $\overline {\text{MS}}$ scheme at $\mu = 2$ GeV using next-to-next-to-leading order matching coefficients, show only mild residual flow-time dependence. The resulting ratios, computed with a relatively small number of configurations, are consistent with phenomenological expectations for the pion's valence distribution, with statistical uncertainties that are competitive with modern global fits. These findings demonstrate that the gradient flow provides an efficient and systematically improvable method to access partonic quantities from first principles. Future extensions of this work will target lighter pion masses toward the physical point, and applications to nucleon structure such as the proton PDFs and the gluon and sea-quark distributions. |
| 2025-10-30 | [Time-Optimal Model Predictive Control for Linear Systems with Multiplicative Uncertainties](http://arxiv.org/abs/2510.26712v1) | Renato Quartullo, Andrea Garulli et al. | This paper presents a time-optimal Model Predictive Control (MPC) scheme for linear discrete-time systems subject to multiplicative uncertainties represented by interval matrices. To render the uncertainty propagation computationally tractable, the set-valued error system dynamics are approximated using a matrix-zonotope-based bounding operator. Recursive feasibility and finite-time convergence are ensured through an adaptive terminal constraint mechanism. A key advantage of the proposed approach is that all the necessary bounding sets can be computed offline, substantially reducing the online computational burden. The effectiveness of the method is illustrated via a numerical case study on an orbital rendezvous maneuver between two satellites. |
| 2025-10-30 | [The ODYSSEUS Survey. Spatial correlation of magnetospheric inclinations points to parsec-scale star-cloud connection](http://arxiv.org/abs/2510.26687v1) | Caeley V. Pittman, Catherine C. Espaillat et al. | The properties of stars and planets are shaped by the initial conditions of their natal clouds. However, the spatial scales over which the initial conditions can exert a significant influence are not well constrained. We report the first evidence for parsec-scale spatial correlations of stellar magnetospheric inclinations ($i_{\rm mag}$), observed in the Lupus low-mass star forming region. Applying consensus clustering with a hierarchical density-based clustering algorithm, we demonstrate that the detected spatial dependencies are stable against perturbations by measurement uncertainties. The $i_{\rm mag}$ correlation scales are on the order of ~3 pc, which aligns with the typical scales of the Lupus molecular cloud filaments. Our results reveal a connection between large-scale forces -- in the form of expanding shells from the Upper Scorpius and Upper-Centaurus-Lupus regions -- and sub-au scale system configurations. We find that Lupus has a non-uniform $i_{\rm mag}$ distribution and suggest that this results from the preferential elongation of protostellar cores along filamentary axes. Non-uniformity would have significant implications for exoplanet occurrence rate calculations, so future work should explore the longevity of these biases driven by the star-cloud connection. |
| 2025-10-30 | [Refined additive uncertainty principle](http://arxiv.org/abs/2510.26664v1) | Ivan Bortnovskyi, June Duvivier et al. | Signal recovery from incomplete or partial frequency information is a fundamental problem in harmonic analysis and applied mathematics, with wide-ranging applications in communications, imaging, and data science. Historically, the classical uncertainty principles, such as those by Donoho and Stark, have provided essential bounds relating the sparsity of a signal and its Fourier transform, ensuring unique recovery under certain support size constraints.   Recent advances have incorporated additive combinatorial notions, notably additive energy, to refine these uncertainty principles and capture deeper structural properties of signal supports. Building upon this line of work, we present a strengthened additive energy uncertainty principle for functions $f:\mathbb{Z}_N^d\to\mathbb{C}$, introducing explicit correction terms that measure how far the supports are from highly structured extremal sets like subgroup cosets.   We have two main results. Our first theorem introduces a correction term which strictly improves the additive energy uncertainty principle from Aldahleh et al., provided that the classical uncertainty principle is not satisfied with equality. Our second theorem uses the improvement to obtain a better recovery condition. These theorems deliver strictly improved bounds over prior results whenever the product of the support sizes differs from the ambient dimension, offering a more nuanced understanding of the interplay between additive structure and Fourier sparsity. Importantly, we leverage these improvements to establish sharper sufficient conditions for unique and exact recovery of signals from partially observed frequencies, explicitly quantifying the role of additive energy in recoverability. |
| 2025-10-30 | [Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets](http://arxiv.org/abs/2510.26602v1) | Tanmay Mishra, Dakota Hamilton et al. | The increasing integration of renewable energy sources and distributed energy resources (DER) into modern power systems introduces significant uncertainty, posing challenges for maintaining grid flexibility and reliability. Hybrid energy systems (HES), composed of controllable generators, flexible loads, and battery storage, offer a decentralized solution to enhance flexibility compared to single centralized resources. This paper presents a two-level framework to enable HES participation in frequency regulation markets. The upper level performs a chance-constrained optimization to choose capacity bids based on historical regulation signals. At the lower level, a real-time control strategy disaggregates the regulation power among the constituent resources. This real-time control strategy is then benchmarked against an offline optimal dispatch to evaluate flexibility performance. Additionally, the framework evaluates the profitability of overbidding strategies and identifies thresholds beyond which performance degradation may lead to market penalties or disqualification. The proposed framework also compare the impact of imbalance of power capacities on performance and battery state of charge (SoC) through asymmetric HES configurations. |
| 2025-10-30 | [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](http://arxiv.org/abs/2510.26601v1) | Anirban Ray, Vera Galinova et al. | Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions. |
| 2025-10-30 | [FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles](http://arxiv.org/abs/2510.26588v1) | Gang Li, Chunlei Zhai et al. | Visual navigation algorithms for quadrotors often exhibit a large variation in performance when transferred across different vehicle platforms and scene geometries, which increases the cost and risk of field deployment. To support systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity, configurable benchmarking framework that measures how platform kinodynamics and scenario structure jointly affect navigation robustness. FLYINGTRUST models vehicle capability with two compact, physically interpretable indicators: maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The benchmark pairs a diverse scenario library with a heterogeneous set of real and virtual platforms and prescribes a standardized evaluation protocol together with a composite scoring method that balances scenario importance, platform importance and performance stability. We use FLYINGTRUST to compare representative optimization-based and learning-based navigation approaches under identical conditions, performing repeated trials per platform-scenario combination and reporting uncertainty-aware metrics. The results reveal systematic patterns: navigation success depends predictably on platform capability and scene geometry, and different algorithms exhibit distinct preferences and failure modes across the evaluated conditions. These observations highlight the practical necessity of incorporating both platform capability and scenario structure into algorithm design, evaluation, and selection, and they motivate future work on methods that remain robust across diverse platforms and scenarios. |
| 2025-10-30 | [A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains](http://arxiv.org/abs/2510.26541v1) | Aidan Furlong, Robert Salko et al. | The use of ML in engineering has grown steadily to support a wide array of applications. Among these methods, deep neural networks have been widely adopted due to their performance and accessibility, but they require large, high-quality datasets. Experimental data are often sparse, noisy, or insufficient to build resilient data-driven models. Transfer learning, which leverages relevant data-abundant source domains to assist learning in data-scarce target domains, has shown efficacy. Parameter transfer, where pretrained weights are reused, is common but degrades under large domain shifts. Domain-adversarial neural networks (DANNs) help address this issue by learning domain-invariant representations, thereby improving transfer under greater domain shifts in a semi-supervised setting. However, DANNs can be unstable during training and lack a native means for uncertainty quantification. This study introduces a fully-supervised three-stage framework, the staged Bayesian domain-adversarial neural network (staged B-DANN), that combines parameter transfer and shared latent space adaptation. In Stage 1, a deterministic feature extractor is trained on the source domain. This feature extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a Bayesian neural network is built on the adapted feature extractor for fine-tuning on the target domain to handle conditional shifts and yield calibrated uncertainty estimates. This staged B-DANN approach was first validated on a synthetic benchmark, where it was shown to significantly outperform standard transfer techniques. It was then applied to the task of predicting critical heat flux in rectangular channels, leveraging data from tube experiments as the source domain. The results of this study show that the staged B-DANN method can improve predictive accuracy and generalization, potentially assisting other domains in nuclear engineering. |
| 2025-10-30 | [An extraction of the Collins-Soper kernel from a joint analysis of experimental and lattice data](http://arxiv.org/abs/2510.26489v1) | Artur Avkhadiev, Valerio Bertone et al. | We present a first joint extraction of the Collins-Soper kernel (CSK) combining experimental and lattice QCD data in the context of an analysis of transverse-momentum-dependent distributions (TMDs). Based on a neural-network parametrization, we perform a Bayesian reweighting of an existing fits of TMDs using lattice data, as well as a joint TMD fit to lattice and experimental data. We consistently find that the inclusion of lattice information shifts the central value of the CSK by approximately 10% and reduces its uncertainty by 40-50%, highlighting the potential of lattice inputs to improve TMD extractions. |
| 2025-10-29 | [An OPF-based Control Framework for Hybrid AC-MTDC Power Systems under Uncertainty](http://arxiv.org/abs/2510.25671v1) | Hongjin Du, Rahul Rane et al. | The increasing integration of renewable energy, particularly offshore wind, introduces significant uncertainty into hybrid AC-HVDC systems due to forecast errors and power fluctuations. Conventional control strategies typically rely on fixed setpoints and neglect frequency deviations, which can compromise system stability under rapid renewable variations. To address this challenge, this paper presents a forecast-integrated, optimal power flow (OPF)-based adaptive control framework. Wind speed forecasts generated using a Random Forest model are incorporated into a time-coupled OPF to determine baseline converter setpoints in anticipation of wind fluctuations, which are further adjusted in real time based on actual operating conditions. An adaptive droop control scheme is developed that jointly considers DC voltage and AC frequency deviations. The effectiveness of the proposed control framework is validated through hardware-in-the-loop (HIL) simulations, demonstrating its capability to ensure stable and robust operation of hybrid AC-HVDC systems under high penetration of renewable energy. |
| 2025-10-29 | [Observing Orbital Decay in the Ultracompact Hot Subdwarf Binary System ZTFJ213056.71+442046.5](http://arxiv.org/abs/2510.25653v1) | Paul Teckenburg, Thomas Kupfer et al. | Ultracompact Galactic binary systems (UCBs) emit low-frequency gravitational waves (GWs). The emission of GWs is causing these systems to lose angular momentum, which is detectable by observing an orbital period decay. ZTFJ213056.71+442046.5 (ZTFJ2130) is an UCB with a period of 39.3401(1) minutes consisting of a Roche lobe-filling hot subdwarf and a white dwarf companion. We attempt to measure the orbital decay rate $\dot{P}$ caused by GW emission of ZTFJ2130 and predict the expected GW signal for LISA. High-speed photometry was conducted using the FLI Kepler KL4040FI CMOS camera, mounted to the 1.2-meter Oskar L\"uhning telescope at the Hamburg Observatory as well as the Hamamatsu ORCA-Quest 2 qCMOS camera at the 1.23-meter telescope at CAHA in Spain. ZTFJ2130 was observed on six nights between August 2024 and September 2025. The obtained lightcurves combined with previous high-cadence observations were used to conduct an O-C timing analysis. Additionally, we employed the LISA data analysis tool ldasoft to model the expected GW data. We measure a period change of $(-1.97\pm0.05)\times10^{-12}\,\mathrm{ss^{-1}}$. Assuming only GW emission, this result was used to calculate a chirp mass of $(0.408\pm0.006)\,\mathrm{M_{\odot}}$. From ldasoft we predict that LISA will be able to measure the chirp mass with an uncertainty of 5%. We measure $\dot{P}$ with an uncertainty of only 2% and show that modern (q)CMOS detectors are well suited to provide precise timing measurements, enabling the measurement of the orbital decay of UCBs with high precision with modest size telescopes. The derived orbital decay is fully consistent with predictions from spectral and lightcurve modeling. We show that future observations with LISA can potentially provide a deviation from only gravitational wave effects, e.g. due to accretion, if the effect is sufficiently large. |
| 2025-10-29 | [Extra dip in ultrahigh energy neutrino spectrum from generalized uncertainty principle](http://arxiv.org/abs/2510.25635v1) | J. Barranco, Emiliano Dur√°n | We revisited the scenario of a resonant enhancement in the oscillation probability due to an interaction potential between neutrinos and dark matter with the novelty of the inclusion of the generalized uncertainty principle. It is shown that a new resonant conversion appears at higher energies. This effect could be tested with future neutrino data as new dips in the ultrahigh energy neutrino flux. |
| 2025-10-29 | [Predictability of Storms in an Idealized Climate Revealed by Machine Learning](http://arxiv.org/abs/2510.25633v1) | Wuqiushi Yao, Or Hadas et al. | The midlatitude climate and weather are shaped by storms, yet the factors governing their predictability remain insufficiently understood. Here, we use a Convolutional Neural Network (CNN) to predict and quantify uncertainty in the intensity growth and trajectory of over 200,000 storms simulated in a 200-year aquaplanet GCM. This idealized framework provides a controlled climate background for isolating factors that govern predictability. Results show that storm intensity is less predictable than trajectory. Strong baroclinicity accelerates storm intensification and reduces its predictability, consistent with theory. Crucially, enhanced jet meanders further degrade forecast skill, revealing a synoptic source of uncertainty. Using sensitivity maps from explainable AI, we find that the error growth rate is nearly doubled by the more meandering structure. These findings highlight the potential of machine learning for advancing understanding of predictability and its governing mechanisms. |
| 2025-10-29 | [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](http://arxiv.org/abs/2510.25599v1) | Christopher B√ºlte, Yusuf Sale et al. | Regression tasks, notably in safety-critical domains, require proper uncertainty quantification, yet the literature remains largely classification-focused. In this light, we introduce a family of measures for total, aleatoric, and epistemic uncertainty based on proper scoring rules, with a particular emphasis on kernel scores. The framework unifies several well-known measures and provides a principled recipe for designing new ones whose behavior, such as tail sensitivity, robustness, and out-of-distribution responsiveness, is governed by the choice of kernel. We prove explicit correspondences between kernel-score characteristics and downstream behavior, yielding concrete design guidelines for task-specific measures. Extensive experiments demonstrate that these measures are effective in downstream tasks and reveal clear trade-offs among instantiations, including robustness and out-of-distribution detection performance. |
| 2025-10-29 | [General model for estimating range variances of terrestrial laser scanners based on (un-)scaled intensity values](http://arxiv.org/abs/2510.25587v1) | Omar AbdelGafar, Selin Palaz et al. | Recent advancements in technology have established terrestrial laser scanners (TLS) as a powerful instrument in geodetic deformation analysis. As TLS becomes increasingly integrated into this field, it is essential to develop a comprehensive stochastic model that accurately captures the measurement uncertainties. A key component of this model is the construction of a complete and valid variance-covariance matrix (VCM) for TLS polar measurements, which requires the estimation of variances for range, vertical, and horizontal angles, as well as their correlations. While angular variances can be obtained from manufacturer specifications, the range variance varies with different intensity measurements. As a primary contribution, this study presents an effective methodology for measuring and estimating TLS range variances using both raw and scaled intensity values. A two-dimensional scanning approach is applied to both controlled targets and arbitrary objects using TLS instruments that provide raw intensity values (e.g., Z+F~Imager~5016A) and those that output scaled intensities (e.g., Leica~ScanStation~P50). The methodology is further evaluated using field observations on a water dam surface. Overall, this work introduces a comprehensive workflow for modeling range uncertainties in high-end TLS systems. |
| 2025-10-29 | [Robust variable selection for spatial point processes observed with noise](http://arxiv.org/abs/2510.25550v1) | Dominik Sturm, Ivo F. Sbalzarini | We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process. |
| 2025-10-29 | [A Unified Photometric Redshift Calibration for Weak Lensing Surveys using the Dark Energy Spectroscopic Instrument](http://arxiv.org/abs/2510.25419v1) | Johannes U. Lange, Diana Blanco et al. | The effective redshift distribution $n(z)$ of galaxies is a critical component in the study of weak gravitational lensing. Here, we introduce a new method for determining $n(z)$ for weak lensing surveys based on high-quality redshifts and neural network-based importance weights. Additionally, we present the first unified photometric redshift calibration of the three leading stage-III weak lensing surveys, the Dark Energy Survey (DES), the Hyper Suprime-Cam (HSC) survey and the Kilo-Degree Survey (KiDS), with state-of-the-art spectroscopic data from the Dark Energy Spectroscopic Instrument (DESI). We verify our method using a new, data-driven approach and obtain $n(z)$ constraints with statistical uncertainties of order $\sigma_{\bar z} \sim 0.01$ and smaller. Our analysis is largely independent of previous photometric redshift calibrations and, thus, provides an important cross-check in light of recent cosmological tensions. Overall, we find excellent agreement with previously published results on the DES Y3 and HSC Y1 data sets while there are some differences on the mean redshift with respect to the previously published KiDS-1000 results. We attribute the latter to mismatches in photometric noise properties in the COSMOS field compared to the wider KiDS SOM-gold catalog. At the same time, the new $n(z)$ estimates for KiDS do not significantly change estimates of cosmic structure growth from cosmic shear. Finally, we discuss how our method can be applied to future weak lensing calibrations with DESI data. |
| 2025-10-29 | [Model-Free Robust Beamforming in Satellite Downlink using Reinforcement Learning](http://arxiv.org/abs/2510.25393v1) | Alea Schr√∂der, Steffen Gracla et al. | Satellite-based communications are expected to be a substantial future market in 6G networks. As satellite constellations grow denser and transmission resources remain limited, frequency reuse plays an increasingly important role in managing inter-user interference. In the multi-user downlink, precoding enables the reuse of frequencies across spatially separated users, greatly improving spectral efficiency. The analytical calculation of suitable precodings for perfect channel information is well studied, however, their performance can quickly deteriorate when faced with, e.g., outdated channel state information or, as is particularly relevant for satellite channels, when position estimates are erroneous. Deriving robust precoders under imperfect channel state information is not only analytically intractable in general but often requires substantial relaxations of the optimization problem or heuristic constraints to obtain feasible solutions. Instead, in this paper we flexibly derive robust precoding algorithms from given data using reinforcement learning. We describe how we adapt the applied Soft Actor-Critic learning algorithm to the problem of downlink satellite beamforming and show numerically that the resulting precoding algorithm adjusts to all investigated scenarios. The considered scenarios cover both single satellite and cooperative multi-satellite beamforming, using either global or local channel state information, and two error models that represent increasing levels of uncertainty. We show that the learned algorithms match or markedly outperform two analytical baselines in sum rate performance, adapting to the required level of robustness. We also analyze the mechanisms that the learned algorithms leverage to achieve robustness. The implementation is publicly available for use and reproduction of the results. |
| 2025-10-29 | [Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods](http://arxiv.org/abs/2510.25386v1) | Kumar Manas, Mert Keser et al. | This survey provides an analysis of current methodologies integrating legal and logical specifications into the perception, prediction, and planning modules of automated driving systems. We systematically explore techniques ranging from logic-based frameworks to computational legal reasoning approaches, emphasizing their capability to ensure regulatory compliance and interpretability in dynamic and uncertain driving environments. A central finding is that significant challenges arise at the intersection of perceptual reliability, legal compliance, and decision-making justifiability. To systematically analyze these challenges, we introduce a taxonomy categorizing existing approaches by their theoretical foundations, architectural implementations, and validation strategies. We particularly focus on methods that address perceptual uncertainty and incorporate explicit legal norms, facilitating decisions that are both technically robust and legally defensible. The review covers neural-symbolic integration methods for perception, logic-driven rule representation, and norm-aware prediction strategies, all contributing toward transparent and accountable autonomous vehicle operation. We highlight critical open questions and practical trade-offs that must be addressed, offering multidisciplinary insights from engineering, logic, and law to guide future developments in legally compliant autonomous driving systems. |
| 2025-10-28 | [ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking](http://arxiv.org/abs/2510.24698v1) | Baixuan Li, Dingchu Zhang et al. | Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption. |
| 2025-10-28 | [Towards constraining cosmological parameters with SPT-3G observations of 25% of the sky](http://arxiv.org/abs/2510.24669v1) | A. Vitrier, K. Fichman et al. | The South Pole Telescope (SPT), using its third-generation camera, SPT-3G, is conducting observations of the cosmic microwave background (CMB) in temperature and polarization across approximately 10 000 deg$^2$ of the sky at 95, 150, and 220 GHz. This comprehensive dataset should yield stringent constraints on cosmological parameters. In this work, we explore its potential to address the Hubble tension by forecasting constraints from temperature, polarization, and CMB lensing on Early Dark Energy (EDE) and the variation in electron mass in spatially flat and curved universes. For this purpose, we investigate first whether analyzing the distinct SPT-3G observation fields independently, as opposed to as a single, unified region, results in a loss of information relevant to cosmological parameter estimation. We develop a realistic temperature and polarization likelihood pipeline capable of analyzing these fields in these two ways, and subsequently forecast constraints on cosmological parameters. Our findings indicate that any loss of constraining power from analyzing the fields separately is primarily concentrated at low multipoles ($\ell$ < 50) and the overall impact on the relative uncertainty on standard $\Lambda$CDM parameters is minimal (< 3%). Our forecasts suggest that SPT-3G data should improve by more than a factor of 200 and 3000 the Figure of Merit (FoM) of the EDE and the varying electron mass models, respectively, when combined with Planck data. The likelihood pipeline developed and used in this work is made publicly available online. |
| 2025-10-28 | [Fast Bayesian Multilevel Quasi-Monte Carlo](http://arxiv.org/abs/2510.24604v1) | Aleksei G. Sorokin, Pieterjan Robbe et al. | Existing multilevel quasi-Monte Carlo (MLQMC) methods often rely on multiple independent randomizations of a low-discrepancy (LD) sequence to estimate statistical errors on each level. While this approach is standard, it can be less efficient than simply increasing the number of points from a single LD sequence. However, a single LD sequence does not permit statistical error estimates in the current framework. We propose to recast the MLQMC problem in a Bayesian cubature framework, which uses a single LD sequence and quantifies numerical error through the posterior variance of a Gaussian process (GP) model. When paired with certain LD sequences, GP regression and hyperparameter optimization can be carried out at only $\mathcal{O}(n \log n)$ cost, where $n$ is the number of samples. Building on the adaptive sample allocation used in traditional MLQMC, where the number of samples is doubled on the level with the greatest expected benefit, we introduce a new Bayesian utility function that balances the computational cost of doubling against the anticipated reduction in posterior uncertainty. We also propose a new digitally-shift-invariant (DSI) kernel of adaptive smoothness, which combines multiple higher-order DSI kernels through a weighted sum of smoothness parameters, for use with fast digital net GPs. A series of numerical experiments illustrate the performance of our fast Bayesian MLQMC method and error estimates for both single-level problems and multilevel problems with a fixed number of levels. The Bayesian error estimates obtained using digital nets are found to be reliable, although, in some cases, mildly conservative. |
| 2025-10-28 | [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](http://arxiv.org/abs/2510.24595v1) | Azadeh Pourkabirian, Kai Li et al. | Hybrid precoding is an indispensable technique to harness the full potential of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In this paper, we propose a new hybrid precoding approach that combines digital and analog precoding to optimize data transmission over multiple antennas. This approach steers signals in specific directions, leading to maximizing sum-rate and suppressing side-lobe interference. When dealing with complex signals, changes in phase are naturally associated with changes in angle, and these variations are inherently correlated. The correlation between the angle and phase is essential for accurately determining the channel characteristics. An important aspect of this approach is that we model the angle and phase as correlated variables following a bivariate Gaussian distribution, and for the first time, we define a joint angle and phase entropy to measure the uncertainty of angle and phase variations in wireless channels. This entropy is crucial to adapt the proposed precoding method with variations. Simulation result validate the accuracy of our analytical findings, demonstrating 18.31% increase in sum-rate and an 11.47% improvement in robustness compared to other state-of-the-art methods. |
| 2025-10-28 | [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](http://arxiv.org/abs/2510.24577v1) | He Yang, Fei Ren et al. | We are very delighted to see the fast development of physics-informed extreme learning machine (PIELM) in recent years for higher computation efficiency and accuracy in physics-informed machine learning. As a summary or review on PIELM is currently not available, we would like to take this opportunity to show our perspective and experience for this promising research direction. We can see many efforts are made to solve PDEs with sharp gradients, nonlinearities, high-frequency behavior, hard constraints, uncertainty, multiphysics coupling. Despite the success, many urgent challenges remain to be tackled, which also provides us opportunities to develop more robust, interpretable, and generalizable PIELM frameworks with applications in science and engineering. |
| 2025-10-28 | [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](http://arxiv.org/abs/2510.24561v1) | Qingyue Zhang, Chang Chu et al. | With the widespread adoption of LLMs, LoRA has become a dominant method for PEFT, and its initialization methods have attracted increasing attention. However, existing methods have notable limitations: many methods do not incorporate target-domain data, while gradient-based methods exploit data only at a shallow level by relying on one-step gradient decomposition, which remains unsatisfactory due to the weak empirical performance of the one-step fine-tuning model that serves as their basis, as well as the fact that these methods either lack a rigorous theoretical foundation or depend heavily on restrictive isotropic assumptions. In this paper, we establish a theoretical framework for data-aware LoRA initialization based on asymptotic analysis. Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher-gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information. By solving this problem, we obtain an optimal initialization strategy for LoRA. Building on this theoretical framework, we develop an efficient algorithm, LoRA-DA, which estimates the terms in the optimization problem from a small set of target domain samples and obtains the optimal LoRA initialization. Empirical results across multiple benchmarks demonstrate that LoRA-DA consistently improves final accuracy over existing initialization methods. Additional studies show faster, more stable convergence, robustness across ranks, and only a small initialization overhead for LoRA-DA. The source code will be released upon publication. |
| 2025-10-28 | [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](http://arxiv.org/abs/2510.24554v1) | Vignesh Kottayam Viswanathan, Yifan Bai et al. | In this work, we present a hierarchical framework designed to support robotic inspection under environment uncertainty. By leveraging a known environment model, existing methods plan and safely track inspection routes to visit points of interest. However, discrepancies between the model and actual site conditions, caused by either natural or human activities, can alter the surface morphology or introduce path obstructions. To address this challenge, the proposed framework divides the inspection task into: (a) generating the initial global view-plan for region of interests based on a historical map and (b) local view replanning to adapt to the current morphology of the inspection scene. The proposed hierarchy preserves global coverage objectives while enabling reactive adaptation to the local surface morphology. This enables the local autonomy to remain robust against environment uncertainty and complete the inspection tasks. We validate the approach through deployments in real-world subterranean mines using quadrupedal robot. |
| 2025-10-28 | [CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?](http://arxiv.org/abs/2510.24505v1) | Qing Zong, Jiayu Liu et al. | Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability. |
| 2025-10-28 | [Sample-efficient and Scalable Exploration in Continuous-Time RL](http://arxiv.org/abs/2510.24482v1) | Klemens Iten, Lenart Treven et al. | Reinforcement learning algorithms are typically designed for discrete-time dynamics, even though the underlying real-world control systems are often continuous in time. In this paper, we study the problem of continuous-time reinforcement learning, where the unknown system dynamics are represented using nonlinear ordinary differential equations (ODEs). We leverage probabilistic models, such as Gaussian processes and Bayesian neural networks, to learn an uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily maximizes a weighted sum of the extrinsic reward and model epistemic uncertainty. This yields a scalable and sample-efficient approach to continuous-time model-based RL. We show that COMBRL achieves sublinear regret in the reward-driven setting, and in the unsupervised RL setting (i.e., without extrinsic rewards), we provide a sample complexity bound. In our experiments, we evaluate COMBRL in both standard and unsupervised RL settings and demonstrate that it scales better, is more sample-efficient than prior methods, and outperforms baselines across several deep RL tasks. |
| 2025-10-28 | [Collaborating Unmanned Aerial Vehicle and Ground Sensors for Urban Signalized Network Traffic Monitoring](http://arxiv.org/abs/2510.24460v1) | Jiarong Yao, Chaopeng Tan et al. | Reliable estimation of network-wide traffic states is essential for urban traffic management. Unmanned Aerial Vehicles (UAVs), with their airborne full-sample continuous trajectory observation, bring new opportunities for traffic state estimation. In this study, we will explore the optimal UAV deployment problem in road networks in conjunction with ground sensors, including connected vehicle (CV) and loop detectors, to achieve more reliable estimation of vehicle path reconstruction as well as movement-based arrival rates and queue lengths. Oriented towards reliable estimation of traffic states, we propose an index, feasible domain size, as the uncertainty measurement, and transform the optimal UAV deployment problem into minimizing the observation uncertainty of network-wide traffic states. Given the large-scale and nonlinear nature of the problem, an improved quantum genetic algorithm (IQGA) that integrates two customized operators is proposed to enhance neighbor searching and solution refinement, thereby improving the observability of UAV pairs. Evaluation was conducted on an empirical network with 18 intersections. Results demonstrated that a UAV fleet size of 7 is sufficient for traffic monitoring, with more than 60\% of network-wide observation uncertainty reduced. Through horizontal comparison with three baselines, the optimal UAV location scheme obtained by the proposed method can reach an improvement of up to 7.23\% and 5.02\% in the estimation accuracy of arrival rate and queue length, respectively. The proposed IQGA is also shown to be faster in solution convergence than the classic QGA by about 9.22\% with better exploration ability in optimum searching. |
| 2025-10-27 | [Lightweight Robust Direct Preference Optimization](http://arxiv.org/abs/2510.23590v1) | Cheol Woo Kim, Shresth Verma et al. | Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants. |
| 2025-10-27 | [Cosmic magnification on multi-catalogue Herschel submillimetre galaxies](http://arxiv.org/abs/2510.23582v1) | R. Fernandez-Fernandez, M. M. Cueli et al. | {Submillimetre galaxies (SMGs) are excellent background sources for magnification-bias studies, but the limited sky coverage in the submillimetre (sub-mm) band constrains their statistical power. Beyond H-ATLAS, Herschel produced additional sub-mm catalogues, though not optimised for spatial statistical lensing analyses.} {Our goal is to refine cosmological constraints from SMG magnification bias by exploiting the full sub-mm sky surveyed by Herschel.} {We expanded the SMG sample by incorporating other Herschel catalogues overlapping SDSS spectroscopic lenses. Random catalogues were generated via kernel density estimation to compute cross-correlations, and Markov Chain Monte Carlo methods were applied to infer astrophysical and cosmological parameters for each catalogue and for the combined dataset.} {We report the first detection of magnification bias in SMGs beyond H-ATLAS, reinforcing the robustness of this observable. Individual Herschel catalogues yield reasonable central values for $\Omega_m$ and $\sigma_8$, although with large uncertainties. The combined analysis, dominated by the more powerful H-ATLAS sample, gives results consistent with $\Lambda$CDM: $\Omega_m = 0.30^{+0.05}_{-0.07}$, $\sigma_8 = 0.80 (+/- 0.07)$, and $h < 0.80$, in better agreement with \textit{Planck} 2018 than previous non-tomographic studies.} {SMGs are promising tracers for magnification bias, but the narrow sub-mm coverage remains a major limitation. Wider surveys optimised for lensing would enable cross-correlations on larger scales, yielding tighter cosmological constraints.} |
| 2025-10-27 | [UrbanVLA: A Vision-Language-Action Model for Urban Micromobility](http://arxiv.org/abs/2510.23576v1) | Anqi Li, Zhiyong Wang et al. | Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties. |
| 2025-10-27 | [COMAP Pathfinder -- Season 2 results IV. A stack on eBOSS/DESI quasars](http://arxiv.org/abs/2510.23568v1) | D. A. Dunne, K. A. Cleary et al. | We present a stack of data from the second season of the CO Mapping Array Project (COMAP) Pathfinder on the positions of quasars from eBOSS and DESI. COMAP is a Line Intensity Mapping (LIM) experiment targeting dense molecular gas via CO(1--0) emission at $z\sim3$. COMAP's Season 2 represents a $3\times$ increase in map-level sensitivity over the previous Early Science data release. We do not detect any CO emission in the stack, instead finding an upper limit of $10.0\times 10^{10}\ \mathrm{K\ km\ s^{-1}\ pc^2}$ at 95\% confidence within an $\sim 18\ \mathrm{cMpc}$ box. We compare this upper limit to models of the CO emission stacked on quasars and find a tentative ($\sim 3 \sigma$) tension between the limit and the brightest stack models after accounting for a suite of additional sources of experimental attenuation and uncertainty, including quasar velocity uncertainty, pipeline signal loss, cosmic variance, and interloper emission in the LIM data. The COMAP-eBOSS/DESI stack is primarily a measurement of the CO luminosity in the quasars' wider environment and is therefore potentially subject to environmental effects such as feedback. With our current simple models of the galaxy-halo connection, we are thus unable to confidently rule out any models of cosmic CO with the stack alone. Conversely, the stack's sensitivity to these large-scale environmental effects has the potential to make it a powerful tool for galaxy formation science, once we are able to constrain the average CO luminosity via the auto power spectrum (a key goal of COMAP). |
| 2025-10-27 | [Dark Energy Survey Year 6 Results: Redshift Calibration of the Weak Lensing Source Galaxies](http://arxiv.org/abs/2510.23566v1) | B. Yin, A. Amon et al. | Determining the distribution of redshifts for galaxies in wide-field photometric surveys is essential for robust cosmological studies of weak gravitational lensing. We present the methodology, calibrated redshift distributions, and uncertainties of the final Dark Energy Survey Year 6 (Y6) weak lensing galaxy data, divided into four redshift bins centered at $\langle z \rangle = [0.414, 0.538, 0.846, 1.157]$. We combine independent information from two methods on the full shape of redshift distributions: optical and near-infrared photometry within an improved Self-Organizing Map $p(z)$ (SOMPZ) framework, and cross-correlations with spectroscopic galaxy clustering measurements (WZ), which we demonstrate to be consistent both in terms of the redshift calibration itself and in terms of resulting cosmological constraints within 0.1$\sigma$. We describe the process used to produce an ensemble of redshift distributions that account for several known sources of uncertainty. Among these, imperfection in the calibration sample due to the lack of faint, representative spectra is the dominant factor. The final uncertainty on mean redshift in each bin is $\sigma_{\langle z\rangle} = [0.012, 0.008,0.009, 0.024]$. We ensure the robustness of the redshift distributions by leveraging new image simulations and a cross-check with galaxy shape information via the shear ratio (SR) method. |
| 2025-10-27 | [Dark Energy Survey Year 6 Results: Clustering-redshifts and importance sampling of Self-Organised-Maps $n(z)$ realizations for $3\times2$pt samples](http://arxiv.org/abs/2510.23565v1) | W. d'Assignies, G. M. Bernstein et al. | This work is part of a series establishing the redshift framework for the $3\times2$pt analysis of the Dark Energy Survey Year 6 (DES Y6). For DES Y6, photometric redshift distributions are estimated using self-organizing maps (SOMs), calibrated with spectroscopic and many-band photometric data. To overcome limitations from color-redshift degeneracies and incomplete spectroscopic coverage, we enhance this approach by incorporating clustering-based redshift constraints (clustering-z, or WZ) from angular cross-correlations with BOSS and eBOSS galaxies, and eBOSS quasar samples. We define a WZ likelihood and apply importance sampling to a large ensemble of SOM-derived $n(z)$ realizations, selecting those consistent with the clustering measurements to produce a posterior sample for each lens and source bin. The analysis uses angular scales of 1.5-5 Mpc to optimize signal-to-noise while mitigating modeling uncertainties, and marginalizes over redshift-dependent galaxy bias and other systematics informed by the N-body simulation Cardinal. While a sparser spectroscopic reference sample limits WZ constraining power at $z>1.1$, particularly for source bins, we demonstrate that combining SOMPZ with WZ improves redshift accuracy and enhances the overall cosmological constraining power of DES Y6. We estimate an improvement in $S_8$ of approximately 10\% for cosmic shear and $3\times2$pt analysis, primarily due to the WZ calibration of the source samples. |
| 2025-10-27 | [Towards Stochastic (N-1)-Secure Redispatch](http://arxiv.org/abs/2510.23551v1) | Oleksii Molodchyk, Hendrik Dr√∂gehorn et al. | The intermittent nature of renewable power availability is one of the major sources of uncertainty in power systems. While markets can guarantee that the demand is covered by the available generation, transmission system operators have to often intervene via economic redispatch to ensure that the physical constraints of the network are satisfied. To account for uncertainty, the underlying optimal power flow (OPF) routines have to be modified. Recently, polynomial chaos expansion (PCE) has been suggested in the literature as a tool for stochastic OPF problems. However, the usage of PCE-based methods in security-constrained OPF for (N-1)-secure operations has not yet been explored. In this paper, we propose a procedure that iteratively solves a PCE-overloaded stochastic OPF problem by including line outage constraints until an (N-1)-secure solution is achieved. We demonstrate the efficacy of our method by comparing it with a Monte-Carlo simulation on a 118-bus example system. |
| 2025-10-27 | [Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with Application to the Richards Equation](http://arxiv.org/abs/2510.23550v1) | Yumo Yang, Anass Ben Bouazza et al. | The estimation of unknown parameters in nonlinear partial differential equations (PDEs) offers valuable insights across a wide range of scientific domains. In this work, we focus on estimating plant root parameters in the Richards equation, which is essential for understanding the soil-plant system in agricultural studies. Since conventional methods are computationally intensive and often yield unstable estimates, we develop a new Gaussian process collocation method for efficient Bayesian inference. Unlike existing Gaussian process-based approaches, our method constructs an approximate posterior distribution using samples drawn from a Gaussian process model fitted to the observed data, which does not require any structural assumption about the underlying PDE. Further, we propose to use an importance sampling procedure to correct for the discrepancy between the approximate and true posterior distributions. As an alternative, we also devise a prior-guided Bayesian optimization algorithm leveraging the approximate posterior. Simulation studies demonstrate that our method yields robust estimates under various settings. Finally, we apply our method on a real agricultural data set and estimate the plant root parameters with uncertainty quantification. |
| 2025-10-27 | [An Error-Based Safety Buffer for Safe Adaptive Control (Extended Version)](http://arxiv.org/abs/2510.23491v1) | Peter A. Fisher, Johannes Autenrieb et al. | We consider the problem of adaptive control of a class of feedback linearizable plants with matched parametric uncertainties whose states are accessible, subject to state constraints, which often arise due to safety considerations. In this paper, we combine adaptation and control barrier functions into a real-time control architecture that guarantees stability, ensures control performance, and remains safe even with the parametric uncertainties. Two problems are considered, differing in the nature of the parametric uncertainties. In both cases, the control barrier function is assumed to have an arbitrary relative degree. In addition to guaranteeing stability, it is proved that both the control objective and safety objective are met with near-zero conservatism. No excitation conditions are imposed on the command signal. Simulation results demonstrate the non-conservatism of all of the theoretical developments. |
| 2025-10-27 | [Human-AI Collaborative Uncertainty Quantification](http://arxiv.org/abs/2510.23476v1) | Sima Noorani, Shayan Kiyani et al. | AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions. |
| 2025-10-24 | [From nuclear star clusters to Little Red Dots: black hole growth, mergers, and tidal disruptions](http://arxiv.org/abs/2510.21709v1) | Konstantinos Kritos, Joseph Silk | Little Red Dots, discovered by the James Webb Space Telescope, are hypothesized to be active galactic nuclei containing a supermassive black hole, possibly surrounded by a dense stellar cluster, large amounts of gas, and likely by a population of stellar-mass black holes. We develop a simple nuclear star cluster model to evolve the rapid mass growth of black hole seeds into the supermassive regime. The combined processes of tidal disruption events, black hole captures, and gas accretion are accounted for self-consistently in our model. Given the observed number density of Little Red Dots, and under reasonable assumptions, we predict at least a few tens of tidal disruption events and at least a few black hole captures at $z=4$-$6$, with a tidal disruption event rate an order of magnitude larger than the black hole capture rate. We also estimate the uncertainties in these estimates. Finally, we comment on the low x-ray luminosity of Little Red Dots. |
| 2025-10-24 | [Connecting Chemical Enrichment with Resolved Star Formation Histories](http://arxiv.org/abs/2510.21707v1) | Christopher T. Garling, Alex M. Garcia et al. | We present a new framework for modeling the chemical enrichment histories of galaxies by integrating the chemical evolution with resolved star formation histories (SFHs) derived from color-magnitude diagrams. This novel approach links the time evolution of the metallicity of the star-forming ISM to the cumulative stellar mass formed in the galaxy, enabling a physically motivated, self-consistent description of chemical evolution. We apply this methodology to four isolated, gas-rich Local Group dwarf galaxies -- WLM, Aquarius, Leo A, and Leo P -- using deep HST and JWST imaging. For WLM, Aquarius, and Leo A, we independently validate our metallicity evolution results using ages and metallicities of individual red giant stars with spectroscopic measurements, finding good agreement. We quantify systematic uncertainties by repeating our analysis with multiple stellar evolution and bolometric correction libraries. We then compare the observed chemical enrichment histories to predictions from the TNG50 and FIREbox cosmological hydrodynamic simulations and the Galacticus semi-analytic model. We find that the enrichment history of WLM is best reproduced by the FIREbox simulation, while TNG50 and Galacticus predict higher metallicities at early times. Our results suggest that differences in stellar feedback and metal recycling prescriptions drive significant variation in the predicted chemical enrichment of dwarf galaxies, particularly at early times. This work demonstrates the power of combining resolved SFHs with physically motivated chemical evolution models to constrain galaxy formation physics and highlights the need for further observational and theoretical studies of metal retention and recycling in low-mass dwarf galaxies. |
| 2025-10-24 | [On Uncertainty Calibration for Equivariant Functions](http://arxiv.org/abs/2510.21691v1) | Edward Berman, Jacob Ginesin et al. | Data-sparse settings such as robotic manipulation, molecular physics, and galaxy morphology classification are some of the hardest domains for deep learning. For these problems, equivariant networks can help improve modeling across undersampled parts of the input space, and uncertainty estimation can guard against overconfidence. However, until now, the relationships between equivariance and model confidence, and more generally equivariance and model calibration, has yet to be studied. Since traditional classification and regression error terms show up in the definitions of calibration error, it is natural to suspect that previous work can be used to help understand the relationship between equivariance and calibration error. In this work, we present a theory relating equivariance to uncertainty estimation. By proving lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under various equivariance conditions, we elucidate the generalization limits of equivariant models and illustrate how symmetry mismatch can result in miscalibration in both classification and regression. We complement our theoretical framework with numerical experiments that clarify the relationship between equivariance and uncertainty using a variety of real and simulated datasets, and we comment on trends with symmetry mismatch, group size, and aleatoric and epistemic uncertainties. |
| 2025-10-24 | [Comparing the data reduction pipelines of FRIPON, DFN, WMPL, and AMOS: Geminids Case Study](http://arxiv.org/abs/2510.21690v1) | P. M. Shober, J. Vaubaillon et al. | Methods. We processed a dataset of 584 Geminid fireballs observed by FRIPON between 2016 and 2023. The single-station astrometric data is converted into the Global Fireball Exchange (GFE) standard format for uniform processing. We assess variations in trajectory, velocity, radiant, and orbital element calculations across the pipelines and compare them to previously published Geminid measurements.   Results. The radiant and velocity solutions provided by the four data reduction pipelines are all within the range of previously published values. However, there are some nuances. Particularly, the radiants estimated by WMPL, DFN, and AMOS are nearly identical. Whereas FRIPON reports a systematic shift in right ascension (-0.3 degrees), caused by improper handling of the precession. Additionally, the FRIPON data reduction pipeline also tends to overestimate the initial velocity (+0.3 km s-1) due to the deceleration model used as the velocity solver. The FRIPON velocity method relies on having a well-constrained deceleration profile; however, for the Geminids, many are low-deceleration events, leading to an overestimation of the initial velocity. On the other end of the spectrum, the DFN tends to predict lower velocities, particularly for poorly observed events. However, this velocity shift vanishes for the DFN when we only consider Geminids with at least three observations or more. The primary difference identified in the analysis concerns the velocity uncertainties. Despite all four pipelines achieving similar residuals between their trajectories and observations, their velocity uncertainties vary systematically, with WMPL outputting the smallest values, followed by AMOS, FRIPON, and DFN. |
| 2025-10-24 | [Energy storage in a continuous-variable quantum battery with nonlinear coupling](http://arxiv.org/abs/2510.21672v1) | C. A. Downing, M. S. Ukhtary | In the quantum world, the process of energy storage can be enhanced thanks to various nonclassical phenomena. This inspiring fact suggests quantum batteries as plausible sources of power for future quantum devices, at least in principle. However, thermodynamically not all of the energy stored in a quantum battery is useful for doing work. By considering a class of models based upon quantum continuous variables, here we show how the maximum extractable energy from a bosonic quantum battery can be intimately related to Heisenberg's uncertainty principle. We found that realizing minimum uncertainty essentially guarantees that all of the energy stored in a Gaussian quantum battery can be withdrawn and used to do work. For a standard system where the charger and battery are coupled linearly, this criterion is satisfied rather trivially. However, our theoretical results demonstrate that - for a quantum battery with nonlinear coupling - a state of minimum uncertainty can also be achieved nontrivially via the generation of quantum squeezing. We characterize the charging performance of our proposed continuous variable quantum batteries in detail, and we hope that our theory may be useful in the design of a new generation of efficient quantum batteries harnessing bosonic excitations, such as those built with photonic architectures. |
| 2025-10-24 | [FlowSynth: Instrument Generation Through Distributional Flow Matching and Test-Time Search](http://arxiv.org/abs/2510.21667v1) | Qihui Yang, Randal Leistikow et al. | Virtual instrument generation requires maintaining consistent timbre across different pitches and velocities, a challenge that existing note-level models struggle to address. We present FlowSynth, which combines distributional flow matching (DFM) with test-time optimization for high-quality instrument synthesis. Unlike standard flow matching that learns deterministic mappings, DFM parameterizes the velocity field as a Gaussian distribution and optimizes via negative log-likelihood, enabling the model to express uncertainty in its predictions. This probabilistic formulation allows principled test-time search: we sample multiple trajectories weighted by model confidence and select outputs that maximize timbre consistency. FlowSynth outperforms the current state-of-the-art TokenSynth baseline in both single-note quality and cross-note consistency. Our approach demonstrates that modeling predictive uncertainty in flow matching, combined with music-specific consistency objectives, provides an effective path to professional-quality virtual instruments suitable for real-time performance. |
| 2025-10-24 | [Modest-Align: Data-Efficient Alignment for Vision-Language Models](http://arxiv.org/abs/2510.21606v1) | Jiaxiang Liu, Yuan Wang et al. | Cross-modal alignment aims to map heterogeneous modalities into a shared latent space, as exemplified by models like CLIP, which benefit from large-scale image-text pretraining for strong recognition capabilities. However, when operating in resource-constrained settings with limited or low-quality data, these models often suffer from overconfidence and degraded performance due to the prevalence of ambiguous or weakly correlated image-text pairs. Current contrastive learning approaches, which rely on single positive pairs, further exacerbate this issue by reinforcing overconfidence on uncertain samples. To address these challenges, we propose Modest-Align, a lightweight alignment framework designed for robustness and efficiency. Our approach leverages two complementary strategies -- Random Perturbation, which introduces controlled noise to simulate uncertainty, and Embedding Smoothing, which calibrates similarity distributions in the embedding space. These mechanisms collectively reduce overconfidence and improve performance on noisy or weakly aligned samples. Extensive experiments across multiple benchmark datasets demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval tasks, achieving competitive results with over 100x less training data and 600x less GPU time than CLIP. Our method offers a practical and scalable solution for cross-modal alignment in real-world, low-resource scenarios. |
| 2025-10-24 | [An unsupervised tour through the hidden pathways of deep neural networks](http://arxiv.org/abs/2510.21582v1) | Diego Doimo | The goal of this thesis is to improve our understanding of the internal mechanisms by which deep artificial neural networks create meaningful representations and are able to generalize. We focus on the challenge of characterizing the semantic content of the hidden representations with unsupervised learning tools, partially developed by us and described in this thesis, which allow harnessing the low-dimensional structure of the data. Chapter 2. introduces Gride, a method that allows estimating the intrinsic dimension of the data as an explicit function of the scale without performing any decimation of the data set. Our approach is based on rigorous distributional results that enable the quantification of uncertainty of the estimates. Moreover, our method is simple and computationally efficient since it relies only on the distances among nearest data points. In Chapter 3, we study the evolution of the probability density across the hidden layers in some state-of-the-art deep neural networks. We find that the initial layers generate a unimodal probability density getting rid of any structure irrelevant to classification. In subsequent layers, density peaks arise in a hierarchical fashion that mirrors the semantic hierarchy of the concepts. This process leaves a footprint in the probability density of the output layer, where the topography of the peaks allows reconstructing the semantic relationships of the categories. In Chapter 4, we study the problem of generalization in deep neural networks: adding parameters to a network that interpolates its training data will typically improve its generalization performance, at odds with the classical bias-variance trade-off. We show that wide neural networks learn redundant representations instead of overfitting to spurious correlation and that redundant neurons appear only if the network is regularized and the training error is zero. |
| 2025-10-24 | [Surrogate-based quantification of policy uncertainty in generative flow networks](http://arxiv.org/abs/2510.21523v1) | Ram√≥n Nartallo-Kaluarachchi, Robert Manson-Sawko et al. | Generative flow networks are able to sample, via sequential construction, high-reward, complex objects according to a reward function. However, such reward functions are often estimated approximately from noisy data, leading to epistemic uncertainty in the learnt policy. We present an approach to quantify this uncertainty by constructing a surrogate model composed of a polynomial chaos expansion, fit on a small ensemble of trained flow networks. This model learns the relationship between reward functions, parametrised in a low-dimensional space, and the probability distributions over actions at each step along a trajectory of the flow network. The surrogate model can then be used for inexpensive Monte Carlo sampling to estimate the uncertainty in the policy given uncertain rewards. We illustrate the performance of our approach on a discrete and continuous grid-world, symbolic regression, and a Bayesian structure learning task. |
| 2025-10-24 | [EXKALIBUR: Towards a Kaonic Atoms Periodic Table to test Fundamental Interactions](http://arxiv.org/abs/2510.21519v1) | Simone Manti, Leonardo Abbene et al. | Kaonic atoms, formed when a negatively charged kaon replaces an electron, provide a unique laboratory to test fundamental interactions at low energies. EXKALIBUR (EXtensive Kaonic Atoms research: from LIthium and Beryllium to URanium) is a program to perform systematic, high-precision X-ray spectroscopy of selected kaonic atoms across the periodic table at the DA$\Phi$NE accelerator at the National Laboratory of Frascati (INFN-LNF). Here, we outline its detector-driven strategy: Silicon Drift Detectors for 10-40 keV transitions in light targets (Li, Be, B, O), CdZnTe detectors for 40-300 keV lines in intermediate-$Z$ systems (Mg, Al, Si, S), and a High-Purity Germanium detector for high-$Z$ atoms (Se, Zr, Ta, Mo, W, Pb), complemented by VOXES, a high-resolution crystal spectrometer for sub-eV studies. EXKALIBUR plans to (i) reduce the charged-kaon mass uncertainty below 10 keV, (ii) produce a database of nuclear shifts and widths to constrain multi-nucleon K$^{-}$-nucleus interaction models, and (iii) provide precision data for testing bound-state QED in strong fields. We summarize the planned measurements and expected sensitivities within DA$\Phi$NE luminosities. |
| 2025-10-23 | [Simulation-calibrated Bayesian inference for progenitor properties of the microquasar SS 433](http://arxiv.org/abs/2510.20811v1) | Nathan Steinle, Matthew Mould et al. | SS\,433 is one of the most extreme Galactic X-ray binaries, exhibiting semi-relativistic jets and super-critical accretion, and harboring a compact object, likely a black hole. Despite decades of observation and modeling, the precise nature of its progenitor binary remains uncertain. To estimate the zero-age main sequence (ZAMS) properties of binaries that evolve into SS\,433-like systems, we apply simulation-based calibration to Bayesian inference and convolve a multivariate Gaussian likelihood constructed from six measured binary parameters of SS\,433 with the isolated binary evolution model \textsc{COSMIC}. Employing the dynamic nested sampler of \texttt{dynesty}, we perform posterior inference over a ten-dimensional progenitor parameter space defined by the masses, orbital parameters, mass transfer possibilities, and natal kick velocity. We find that SS\,433-like systems arise from specific regions of binary evolution parameter space depending on key assumptions, such as the mass transfer rate and uncertainty taken from observations. Our simulation-based calibration framework, implemented with a suite of machine learning algorithms and scored by a heuristic reliability metric, allows us to iteratively build posterior distributions of the progenitors of SS\,433-like systems. This analysis reveals 90\% confidence intervals for the ZAMS primary mass $(8, 11)$ M$_\odot$, secondary mass $(32, 40)$ M$_\odot $, orbital period $(136, 2259)$ days, eccentricity $(0.26, 0.6)$, common envelope evolution efficiency $(0.44, 0.76)$, accreted fraction in stable mass transfer $(0.22, 0.6)$, and black hole natal kick velocity magnitude $(5, 68)$ km/s. These results demonstrate the feasibility of direct probabilistic inference of X-ray binary progenitors to offer new insights into the evolution of high-accretion-rate systems such as SS\,433. |
| 2025-10-23 | [Quantum black holes: inside and outside](http://arxiv.org/abs/2510.20799v1) | Wei-Chen Lin, Dong-han Yeom et al. | For a unitary description of an evaporating black hole, one usually chooses the time slices that cover only outside of the event horizon, which is mostly problem-free because the event horizon is not encountered. However, is there any justification for avoiding time slices that cover inside the event horizon? To answer the question, we investigate the Wheeler-DeWitt equation, where the time slices can cover both inside and outside the event horizon. We find that one can reasonably construct a wave packet that covers outside, but the wave function must be annihilated near the event horizon. This observation strongly suggests that we cannot choose a coherent state for a spacelike hypersurface that crosses the event horizon. To explain the unitary time evolution, we must keep the slices as coherent states; hence, they must always be outside the event horizon. In contrast, inside the horizon, we cannot have a single coherent state of a classical spacetime. Hence, the interior must be a superposition of several coherent states, which implies that there exists a horizon-scale uncertainty and a black hole should be viewed as a highly quantum macroscopic object. We provide a synthetic approach to understanding the information loss paradox from this perspective. |
| 2025-10-23 | [Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks](http://arxiv.org/abs/2510.20795v1) | Juan Alejandro Pinto Castro, H√©ctor J. Hort√∫a et al. | Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology. |
| 2025-10-23 | [Reinforcement Learning and Consumption-Savings Behavior](http://arxiv.org/abs/2510.20748v1) | Brandon Kaplowitz | This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a "scarring" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict. |
| 2025-10-23 | [Bayesian Prediction under Moment Conditioning](http://arxiv.org/abs/2510.20742v1) | Nicholas G. Polson, Daniel Zantedeschi | Prediction is a central task of statistics and machine learning, yet many inferential settings provide only partial information, typically in the form of moment constraints or estimating equations. We develop a finite, fully Bayesian framework for propagating such partial information through predictive distributions. Building on de Finetti's representation theorem, we construct a curvature-adaptive version of exchangeable updating that operates directly under finite constraints, yielding an explicit discrete-Gaussian mixture that quantifies predictive uncertainty. The resulting finite-sample bounds depend on the smallest eigenvalue of the information-geometric Hessian, which measures the curvature and identification strength of the constraint manifold. This approach unifies empirical likelihood, Bayesian empirical likelihood, and generalized method-of-moments estimation within a common predictive geometry. On the operational side, it provides computable curvature-sensitive uncertainty bounds for constrained prediction; on the theoretical side, it recovers de Finetti's coherence, Doob's martingale convergence and local asymptotic normality as limiting cases of the same finite mechanism. Our framework thus offers a constructive bridge between partial information and full Bayesian prediction. |
| 2025-10-23 | [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](http://arxiv.org/abs/2510.20725v1) | Jasmine Bayrooti, Sattar Vakili et al. | Thompson sampling (TS) is a powerful and widely used strategy for sequential decision-making, with applications ranging from Bayesian optimization to reinforcement learning (RL). Despite its success, the theoretical foundations of TS remain limited, particularly in settings with complex temporal structure such as RL. We address this gap by establishing no-regret guarantees for TS using models with Gaussian marginal distributions. Specifically, we consider TS in episodic RL with joint Gaussian process (GP) priors over rewards and transitions. We prove a regret bound of $\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis addresses several challenges, including the non-Gaussian nature of value functions and the recursive structure of Bellman updates, and extends classical tools such as the elliptical potential lemma to multi-output settings. This work advances the understanding of TS in RL and highlights how structural assumptions and model uncertainty shape its performance in finite-horizon Markov Decision Processes. |
| 2025-10-23 | [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](http://arxiv.org/abs/2510.20666v1) | Mariona Jaramillo-Civill, Luis Gonz√°lez-Gudi√±o et al. | Global Navigation Satellite System (GNSS) signals are vulnerable to jamming, particularly in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed the received signal strength (RSS) field due to limited spatial context. We propose a hybrid Bayesian mixture-of-experts framework that fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling. The PL expert ensures physical consistency, while the CNN leverages building-height maps to capture urban propagation effects. Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field. Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points, while uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive. |
| 2025-10-23 | [Blue supergiants and the zero point of the Tully-Fisher relation: a path to a new independent test of the Hubble constant](http://arxiv.org/abs/2510.20652v1) | Rolf-Peter Kudritzki, Fabio Bresolin et al. | Blue supergiant distances of nearby galaxies obtained with the flux-weighted gravity-luminosity relationship are used for a measurement of the zero points of Tully-Fisher relationships at different photometric passbands. The Cousins I-band and the infrared WISE bands W1 and W2 are investigated. The results are compared with previous work using Cepheid and Tip-of-the-Red-Giant-Branch distances. No significant differences were encountered. This supports the large values of the Hubble constant greater than 73km/s/Mpc found with the Tully-Fisher distance ladder work over the last decade. Applying blue supergiant distances on the I-band Tully-Fisher relation observations yields a Hubble constant H0 = 76.2+/-6.2 km/s/Mpc. The large uncertainty is caused by the still relatively small blue supergiant galaxies sample size but will be reduced in future work. |
| 2025-10-23 | [Strong Lensing Model and Dust Extinction Maps of the Host Galaxy of Type Ia Supernova H0pe](http://arxiv.org/abs/2510.20561v1) | A. Galan, S. Schuldt et al. | Strong gravitational lensing by massive galaxy clusters offers particularly rare opportunities to observe multiple images of distant ($z\gtrsim2$) Type Ia supernovae (SNe) and resolve the properties of their host galaxies. A recent outstanding example is the Type Ia SN "H0pe" ($z=1.78$), discovered in James Webb Space Telescope (JWST) NIRCam images when it was still triply imaged by the galaxy cluster PLCK G165.7+67.0 (G165, $z=0.35$). In this work we build a new strong lensing model of G165, first by using only the position of multiple images of background galaxies. We then increase significantly the number of constraints around the position of SN H0pe by modeling the extended surface brightness of the SN host galaxy. The average uncertainty on mass model parameters is reduced by more than an order of magnitude. We also study the spatial distribution of dust in the arc to estimate the dust extinction at the position of SN H0pe. We find good statistical agreement of the extinction estimate at $\lesssim1\sigma$ with three fully independent methods based on spectral energy distribution fitting. Moreover, our extended-image lens model of G165 allows us to map the dust distribution of the host galaxy from the image plane to the source plane. Supernova H0pe exploded in a region with a relatively high extinction of $A_V \approx 0.9\ {\rm mag}$ at around $\sim 1\ {\rm kpc}$ from its host center. This work shows that extended image modeling in lensing clusters simultaneously reduces the uncertainty on lens model parameters and enables spatially resolved analyses of lensed transients host galaxies. Such modeling advances are expected to play an important role in future cosmological analyses using strongly lensed SNe. |
| 2025-10-23 | [Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy](http://arxiv.org/abs/2510.20551v1) | Jacob Ayers, Richard Hahnloser et al. | Conditional differential entropy provides an intuitive measure for relatively ranking time-series complexity by quantifying uncertainty in future observations given past context. However, its direct computation for high-dimensional processes from unknown distributions is often intractable. This paper builds on the information theoretic prediction error bounds established by Fang et al. \cite{fang2019generic}, which demonstrate that the conditional differential entropy \textbf{$h(X_k \mid X_{k-1},...,X_{k-m})$} is upper bounded by a function of the determinant of the covariance matrix of next-step prediction errors for any next step prediction model. We add to this theoretical framework by further increasing this bound by leveraging Hadamard's inequality and the positive semi-definite property of covariance matrices.   To see if these bounds can be used to rank the complexity of time series, we conducted two synthetic experiments: (1) controlled linear autoregressive processes with additive Gaussian noise, where we compare ordinary least squares prediction error entropy proxies to the true entropies of various additive noises, and (2) a complexity ranking task of bio-inspired synthetic audio data with unknown entropy, where neural network prediction errors are used to recover the known complexity ordering.   This framework provides a computationally tractable method for time-series complexity ranking using prediction errors from next-step prediction models, that maintains a theoretical foundation in information theory. |
| 2025-10-22 | [How Accurate Are DFT Forces? Unexpectedly Large Uncertainties in Molecular Datasets](http://arxiv.org/abs/2510.19774v1) | Domantas Kuryla, Fabian Berger et al. | Training of general-purpose machine learning interatomic potentials (MLIPs) relies on large datasets with properties usually computed with density functional theory (DFT). A pre-requisite for accurate MLIPs is that the DFT data are well converged to minimize numerical errors. A possible symptom of errors in DFT force components is nonzero net force. Here, we consider net forces in datasets including SPICE, Transition1x, ANI-1x, ANI-1xbb, AIMNet2, QCML, and OMol25. Several of these datasets suffer from significant nonzero DFT net forces. We also quantify individual force component errors by comparison to recomputed forces using more reliable DFT settings at the same level of theory, and we find significant discrepancies in force components averaging from 1.7 meV/{\AA} in the SPICE dataset to 33.2 meV/{\AA} in the ANI-1x dataset. These findings underscore the importance of well converged DFT data as increasingly accurate MLIP architectures become available. |
| 2025-10-22 | [Beta-decay Half Lives beyond $^{54}$Ca: A Systematic Survey of Decay Properties approaching the Neutron Dripline](http://arxiv.org/abs/2510.19757v1) | W. -J. Ong, Z. Y. Xu et al. | In an experiment performed at the Facility for Rare Isotope Beams (FRIB) using the FRIB Decay Station initiator (FDSi), 15 new half lives of isotopes near $^{54}$Ca were measured. A new method of extracting lifetimes from experimental data, taking into account the unknown $\beta$-delayed neutron emission branches of very neutron-rich nuclei, was developed to enable systematic uncertainty analysis. The experiment observed a dramatic change in the half-life systematics for the isotopes with neutron number N =34. Beyond N =34, the decline of nuclear lifetime is much slower, leading to longer than anticipated lifetimes for near-dripline nuclei. State-of-the-art shell-model calculations can explain the experimental results for Z$>$19 nuclei, revealing the imprint of shell effects and the need for modification of single-particle neutron states. The results from a newly developed QRPA model with potential for making global predictions were also tested against the experimental results and good agreement was found. |
| 2025-10-22 | [CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees](http://arxiv.org/abs/2510.19754v1) | Aman Bilkhoo, Milad Kazemi et al. | Counterfactual explanations (CFXs) provide human-understandable justifications for model predictions, enabling actionable recourse and enhancing interpretability. To be reliable, CFXs must avoid regions of high predictive uncertainty, where explanations may be misleading or inapplicable. However, existing methods often neglect uncertainty or lack principled mechanisms for incorporating it with formal guarantees. We propose CONFEX, a novel method for generating uncertainty-aware counterfactual explanations using Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX explanations are designed to provide local coverage guarantees, addressing the issue that CFX generation violates exchangeability. To do so, we develop a novel localised CP procedure that enjoys an efficient MILP encoding by leveraging an offline tree-based partitioning of the input space. This way, CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty and optimality. We evaluate CONFEX against state-of-the-art methods across diverse benchmarks and metrics, demonstrating that our uncertainty-aware approach yields robust and plausible explanations. |
| 2025-10-22 | [BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models](http://arxiv.org/abs/2510.19749v1) | Catherine Villeneuve, Benjamin Akera et al. | Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts. |
| 2025-10-22 | [Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+Œ¥}$](http://arxiv.org/abs/2510.19734v1) | Bhavya Agrawalla, Krishnakumar Balasubramanian et al. | Stochastic Gradient Descent (SGD) has become a cornerstone method in modern data science. However, deploying SGD in high-stakes applications necessitates rigorous quantification of its inherent uncertainty. In this work, we establish \emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online least-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in a \emph{growing-dimensional regime}. Existing approaches to high-dimensional inference for projection parameters, such as~\cite{chang2023inference}, rely on inverting empirical covariance matrices and require at least $t \gtrsim d^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees, rendering them computationally expensive and restrictive in the allowable dimensional scaling. In contrast, we show that a CLT holds for SGD iterates when the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta > 0$, significantly extending the dimensional regime permitted by prior works while improving computational efficiency. The proposed online SGD-based procedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$ memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime of covariance-inversion methods. To render the theory practically applicable, we further develop an \emph{online variance estimator} for the asymptotic variance appearing in the CLT and establish \emph{high-probability deviation bounds} for this estimator. Collectively, these results yield the first fully online and data-driven framework for constructing confidence intervals for SGD iterates in the near-optimal scaling regime $t \gtrsim d^{1+\delta}$. |
| 2025-10-22 | [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](http://arxiv.org/abs/2510.19670v1) | Hasan Akgul, Mari Eplik et al. | We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments. |
| 2025-10-22 | [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](http://arxiv.org/abs/2510.19669v1) | Xiang Liu, Xuming Hu et al. | Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\%, establishing a practical path toward compute-efficient reasoning. |
| 2025-10-22 | [Engineering the shapes of quark-gluon plasma droplets by comparing anisotropic flow in small symmetric and asymmetric collision systems](http://arxiv.org/abs/2510.19645v1) | STAR Collaboration | The observation of collective flow phenomena in small collision systems challenges our understanding of quark-gluon plasma (QGP) formation and evolution. This complexity lies in the initial geometries, which are influenced by both nucleon configuration and subnucleonic fluctuations, introducing uncertainties in interpreting flow patterns. We disentangle these contributions through comparative measurements of elliptic ($v_2$) and triangular ($v_3$) flow in asymmetric $d$+Au and symmetric $^{16}$O+$^{16}$O collisions at $\sqrt{s_{NN}}=200$ GeV, which produce medium of comparable sizes but with vastly different initial geometries. The larger $v_2$ in $d$+Au reflects its dominant elliptic geometry, while the similar $v_3$ in both systems is better explained by considering subnucleonic fluctuations. These contrasting flow patterns are quantitatively described by a state-of-the-art hydrodynamic model tuned to large-system Au+Au data, indicating efficient transformation of initial geometries to final-state anisotropies. These results provide evidence for droplet formation in small systems with transport properties that are similar to those observed in large collision systems, consistent with QGP-like behavior. |
| 2025-10-22 | [A Climate-Aware Deep Learning Framework for Generalizable Epidemic Forecasting](http://arxiv.org/abs/2510.19611v1) | Jinpyo Hong, Rachel E. Baker | Precise outbreak forecasting of infectious diseases is essential for effective public health responses and epidemic control. The increased availability of machine learning (ML) methods for time-series forecasting presents an enticing avenue to enhance outbreak forecasting. Though the COVID-19 outbreak demonstrated the value of applying ML models to predict epidemic profiles, using ML models to forecast endemic diseases remains underexplored. In this work, we present ForecastNet-XCL (an ensemble model based on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to addresses this gap by creating accurate multi-week RSV forecasts up to 100 weeks in advance based on climate and temporal data, without access to real-time surveillance on RSV. The framework combines high-resolution feature learning with long-range temporal dependency capturing mechanisms, bolstered by an autoregressive module trained on climate-controlled lagged relations. Stochastic inference returns probabilistic intervals to inform decision-making. Evaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed statistical baselines, individual neural nets, and conventional ensemble methods in both within- and cross-state scenarios, sustaining accuracy over extended forecast horizons. Training on climatologically diverse datasets enhanced generalization furthermore, particularly in locations having irregular or biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and uncertainty-aware design make it a deployable early-warning tool amid escalating climate pressures and constrained surveillance resources. |
| 2025-10-22 | [CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization](http://arxiv.org/abs/2510.19597v1) | Zhou Lei, Pan Gang et al. | Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment. |
| 2025-10-21 | [NNLO QCD$\otimes$QED corrections to unpolarized and polarized SIDIS](http://arxiv.org/abs/2510.18872v1) | Saurav Goyal, Roman N. Lee et al. | We present the first computation of next-to-next-to-leading order (NNLO) pure QED and mixed QCD$\otimes$QED corrections to unpolarized and polarized semi-inclusive deep-inelastic scattering (SIDIS). Building on our previous NNLO QCD results, these corrections are crucial for improving the theoretical precision. The coefficient functions are derived within the QCD factorization framework using dimensional regularization, with consistent renormalization and mass factorization. A detailed phenomenological analysis shows that the NNLO QED and QCD$\otimes$QED terms enhance perturbative stability and reduce scale uncertainties. These results are essential for high-precision SIDIS predictions at future facilities such as the Electron-Ion Collider. |
| 2025-10-21 | [Reexamining Evidence of a Pair-Instability Mass Gap in the Binary Black Hole Population](http://arxiv.org/abs/2510.18867v1) | Anarya Ray, Vicky Kalogera | The fourth gravitational wave transient catalog~(GWTC-4) has enabled empirical probes of the theorized pair-instability gap in the higher end of the binary black hole~(BBH) mass-spectrum. In this letter, using flexibly parametrized models, we show that at present there is no evidence of a sharp drop-off in the spectrum of black hole masses near $~40-50M_{\odot}$. We simultaneously characterize the transition in the distribution of BBH mass-ratios, effective aligned and effective precessing spins using our flexible models. From the transitions in our inferred spin and mass-ratio distributions, we find that the high-mass broad-spin sub-population has a significant fraction~($52^{+18}_{-23}\%$) of systems with mass ratios in the range $0.6-1$. This suggests that alternatives to the hypothesis of 2G+1G hierarchical systems dominating BBH formation above $\sim 40-50 M_{\odot}$ are more consistent with the GWTC-4 detection sample. By comparing with the predictions of star cluster simulations, we further show that contributions from (2G+2G) systems are not abundant enough to alleviate this discrepancy. We also demonstrate the effects of strong model assumptions on this inference, which can lead to biased astrophysical interpretation from restricted priors. We note that our results do not exclude that a high-mass gap may be identified as our sample size increases. We constrain the lower bound on the location of a possible PISN cutoff still allowed within measurement uncertainties to be $(57^{+17}_{-10}M_{\odot})$ and discuss its implications on the S factor of $^{12}\mathrm{C}(\alpha, \gamma)^{16}O$ at 300 kev. |
| 2025-10-21 | [Degeneracy-Aware Pulsar Parameter Estimation from Light Curves via Deep Learning and Test-Time Optimization](http://arxiv.org/abs/2510.18807v1) | Abu Bucker Siddik, Diane Oyen et al. | Probing properties of neutron stars from photometric observations of these objects helps us answer crucial questions at the forefront of multi-messenger astronomy, such as, what is behavior of highest density matter in extreme environments and what is the procedure of generation and evolution of magnetic fields in these astrophysical environments? However, uncertainties and degeneracies-where different parameter sets produce similar light curves-make this task challenging. We propose a deep learning framework for inferring pulsar parameters from observed light curves. Traditional deep learning models are not designed to produce multiple degenerate solutions for a given input. To address this, we introduce a custom loss function that incorporates a light curve emulator as a forward model, along with a dissimilarity loss that encourages the model to capture diverse, degenerate parameter sets for a given light curve. We further introduce a test-time optimization scheme that refines predicted parameters by minimizing the discrepancy between the observed light curve and those reconstructed by the forward model from predicted parameters during inference. The model is trained using a suite of state-of-the-art simulated pulsar light curves. Finally, we demonstrate that the parameter sets predicted by our approach reproduce light curves that are consistent with the true observation. |
| 2025-10-21 | [Gravitational-wave and electromagnetic detections in the context of the CosmoDC2 LSST synthetic catalog](http://arxiv.org/abs/2510.18727v1) | Ranier Menote, Valerio Marra et al. | We release CosmoDC2_BCO, a synthetic catalog of gravitational-wave events and electromagnetic counterparts associated with galaxies from CosmoDC2. The catalog provides intrinsic and extrinsic source parameters, signal-to-noise ratios, parameter uncertainties, sky localization areas, and kilonova apparent magnitudes in LSST filters. Our results show that third-generation detector networks substantially increase detection rates and improve parameter estimation. Second-generation detectors, when combined with third-generation ones, significantly enhance sky localization and distance precision, particularly for BNS mergers. Assuming a simplified Target of Opportunity strategy, we estimate that an LSST-like survey, partnered with the CE+ET+LVK network at 70% duty cycle, could detect about 5000 kilonovae with GW counterparts over a 10-year period on a 16000 deg^2 footprint, predominantly from low-mass BNS mergers that produce long-lived supermassive neutron star remnants. While this is a substantial number, it represents only a small fraction of the total neutron star mergers expected to be observed by third-generation networks. These projections rely on several simplifying assumptions-including the adopted merger rate, the kilonova luminosity distribution, and the configuration and scheduling of future surveys-which introduce notable uncertainties. Therefore, the estimated detection numbers should be interpreted with appropriate caution. |
| 2025-10-21 | [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](http://arxiv.org/abs/2510.18713v1) | Joongkyu Lee, Seouh-won Yi et al. | We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size. |
| 2025-10-21 | [Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations](http://arxiv.org/abs/2510.18697v1) | Phuoc Nguyen, Francesco Verdoja et al. | A fundamental aspect for building intelligent autonomous robots that can assist humans in their daily lives is the construction of rich environmental representations. While advances in semantic scene representations have enriched robotic scene understanding, current approaches lack a connection between spatial features and dynamic events; e.g., connecting the blue mug to the event washing a mug. In this work, we introduce the event-grounding graph (EGG), a framework grounding event interactions to spatial features of a scene. This representation allows robots to perceive, reason, and respond to complex spatio-temporal queries. Experiments using real robotic data demonstrate EGG's capability to retrieve relevant information and respond accurately to human inquiries concerning the environment and events within. Furthermore, the EGG framework's source code and evaluation dataset are released as open-source at: https://github.com/aalto-intelligent-robotics/EGG. |
| 2025-10-21 | [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](http://arxiv.org/abs/2510.18648v1) | Miro Miranda, Marcela Charfuelan et al. | Water is essential for agricultural productivity. Assessing water shortages and reduced yield potential is a critical factor in decision-making for ensuring agricultural productivity and food security. Crop simulation models, which align with physical processes, offer intrinsic explainability but often perform poorly. Conversely, machine learning models for crop yield modeling are powerful and scalable, yet they commonly operate as black boxes and lack adherence to the physical principles of crop growth. This study bridges this gap by coupling the advantages of both worlds. We postulate that the crop yield is inherently defined by the water availability. Therefore, we formulate crop yield as a function of temporal water scarcity and predict both the crop drought stress and the sensitivity to water scarcity at fine-scale resolution. Sequentially modeling the crop yield response to water enables accurate yield prediction. To enforce physical consistency, a novel physics-informed loss function is proposed. We leverage multispectral satellite imagery, meteorological data, and fine-scale yield data. Further, to account for the uncertainty within the model, we build upon a deep ensemble approach. Our method surpasses state-of-the-art models like LSTM and Transformers in crop yield prediction with a coefficient of determination ($R^2$-score) of up to 0.82 while offering high explainability. This method offers decision support for industry, policymakers, and farmers in building a more resilient agriculture in times of changing climate conditions. |
| 2025-10-21 | [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](http://arxiv.org/abs/2510.18631v1) | Carlo Proietti, Antonio Yuste-Ginel | Modelling qualitative uncertainty in formal argumentation is essential both for practical applications and theoretical understanding. Yet, most of the existing works focus on \textit{abstract} models for arguing with uncertainty. Following a recent trend in the literature, we tackle the open question of studying plausible instantiations of these abstract models. To do so, we ground the uncertainty of arguments in their components, structured within rules and premises. Our main technical contributions are: i) the introduction of a notion of expressivity that can handle abstract and structured formalisms, and ii) the presentation of both negative and positive expressivity results, comparing the expressivity of abstract and structured models of argumentation with uncertainty. These results affect incomplete abstract argumentation frameworks, and their extension with dependencies, on the abstract side, and ASPIC+, on the structured side. |
| 2025-10-21 | [Deep Q-Learning Assisted Bandwidth Reservation for Multi-Operator Time-Sensitive Vehicular Networking](http://arxiv.org/abs/2510.18553v1) | Abdullah Al-Khatib, Albert Gergus et al. | Very few available individual bandwidth reservation schemes provide efficient and cost-effective bandwidth reservation that is required for safety-critical and time-sensitive vehicular networked applications. These schemes allow vehicles to make reservation requests for the required resources. Accordingly, a Mobile Network Operator (MNO) can allocate and guarantee bandwidth resources based on these requests. However, due to uncertainty in future reservation time and bandwidth costs, the design of an optimized reservation strategy is challenging. In this article, we propose a novel multi-objective bandwidth reservation update approach with an optimal strategy based on Double Deep Q-Network (DDQN). The key design objectives are to minimize the reservation cost with multiple MNOs and to ensure reliable resource provisioning in uncertain situations by solving scenarios such as underbooked and overbooked reservations along the driving path. The enhancements and advantages of our proposed strategy have been demonstrated through extensive experimental results when compared to other methods like greedy update or other deep reinforcement learning approaches. Our strategy demonstrates a 40% reduction in bandwidth costs across all investigated scenarios and simultaneously resolves uncertain situations in a cost-effective manner. |
| 2025-10-21 | [Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data](http://arxiv.org/abs/2510.18548v1) | Ying Yao, Daniel J. Graham | Accurate annual average daily traffic (AADT) data are vital for transport planning and infrastructure management. However, automatic traffic detectors across national road networks often provide incomplete coverage, leading to underrepresentation of minor roads. While recent machine learning advances have improved AADT estimation at unmeasured locations, most models produce only point predictions and overlook estimation uncertainty. This study addresses that gap by introducing an interval prediction approach that explicitly quantifies predictive uncertainty. We integrate a Quantile Random Forest model with Principal Component Analysis to generate AADT prediction intervals, providing plausible traffic ranges bounded by estimated minima and maxima. Using data from over 2,000 minor roads in England and Wales, and evaluated with specialized interval metrics, the proposed method achieves an interval coverage probability of 88.22%, a normalized average width of 0.23, and a Winkler Score of 7,468.47. By combining machine learning with spatial and high-dimensional analysis, this framework enhances both the accuracy and interpretability of AADT estimation, supporting more robust and informed transport planning. |
| 2025-10-20 | [Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks](http://arxiv.org/abs/2510.17798v1) | Samuel Talkington, Cameron Khanpour et al. | This paper presents probabilistic bounds for the spectrum of the admittance matrix and classical linear power flow models under uncertain network parameters; for example, probabilistic line contingencies. Our proposed approach imports tools from probability theory, such as concentration inequalities for random matrices with independent entries. It yields error bounds for common approximations of the AC power flow equations under parameter uncertainty, including the DC and LinDistFlow approximations. |
| 2025-10-20 | [QUIJOTE scientific results XIX. New constraints on the synchrotron spectral index using a semi-blind component separation method](http://arxiv.org/abs/2510.17761v1) | Debabrata Adak, J. A. Rubi√±o-Mart√≠n et al. | We introduce a novel approach to estimate the spectral index, $\beta_s$, of polarised synchrotron emission, combining the moment expansion of CMB foregrounds and the constrained-ILC method. We reconstruct the maps of the first two synchrotron moments, combining multi-frequency data, and apply the `T-T plot' technique between two moment maps to estimate the synchrotron spectral index. This approach offers a new technique for mapping the foreground spectral parameters, complementing the model-based parametric component separation methods. Applying this technique, we derive a new constraint on the spectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey 11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and Ka bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar Spur regions, we obtain an inverse-variance-weighted mean synchrotron index of $\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter, consistent with previous results based on parametric methods using the same dataset. We find that the inverse-variance-weighted mean spectral index, including both statistical and systematic uncertainties, is $\beta_s^{\rm plane} = -3.05 \pm 0.01$ in the Galactic plane and $\beta_s^{\rm high\text{-}lat} = -3.13 \pm 0.02$ at high latitudes, indicating a moderate steepening of the spectral index from low to high Galactic latitudes. Our analysis indicates that, within the current upper limit on the AME polarisation fraction, our results are not subject to any appreciable bias. Furthermore, we infer the spectral index over the entire QUIJOTE survey region, partitioning the sky into 21 patches. This technique can be further extended to constrain the synchrotron spectral curvature by reconstructing higher-order moments when better-quality data become available. |
| 2025-10-20 | [Enabling Fine-Grained Operating Points for Black-Box LLMs](http://arxiv.org/abs/2510.17727v1) | Ege Beyazit, KL Navaneet et al. | Black-box Large Language Models (LLMs) provide practical and accessible alternatives to other machine learning methods, as they require minimal labeled data and machine learning expertise to develop solutions for various decision making problems. However, for applications that need operating with constraints on specific metrics (e.g., precision $\geq$ 95%), decision making with black-box LLMs remains unfavorable, due to their low numerical output cardinalities. This results in limited control over their operating points, preventing fine-grained adjustment of their decision making behavior. In this paper, we study using black-box LLMs as classifiers, focusing on efficiently improving their operational granularity without performance loss. Specifically, we first investigate the reasons behind their low-cardinality numerical outputs and show that they are biased towards generating rounded but informative verbalized probabilities. Then, we experiment with standard prompt engineering, uncertainty estimation and confidence elicitation techniques, and observe that they do not effectively improve operational granularity without sacrificing performance or increasing inference cost. Finally, we propose efficient approaches to significantly increase the number and diversity of available operating points. Our proposed approaches provide finer-grained operating points and achieve comparable to or better performance than the benchmark methods across 11 datasets and 3 LLMs. |
| 2025-10-20 | [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](http://arxiv.org/abs/2510.17690v1) | Xihong Su | This dissertation makes three main contributions. First, We identify a new connection between policy gradient and dynamic programming in MMDPs and propose the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov policy that maximizes the discounted return averaged over the uncertain models. CADP adjusts model weights iteratively to guarantee monotone policy improvements to a local maximum. Second, We establish sufficient and necessary conditions for the exponential ERM Bellman operator to be a contraction and prove the existence of stationary deterministic optimal policies for ERM-TRC and EVaR-TRC. We also propose exponential value iteration, policy iteration, and linear programming algorithms for computing optimal stationary policies for ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the optimal risk-averse value functions. The proposed Q-learning algorithms compute the optimal stationary policy for ERM-TRC and EVaR-TRC. |
| 2025-10-20 | [LILO: Bayesian Optimization with Interactive Natural Language Feedback](http://arxiv.org/abs/2510.17671v1) | Katarzyna Kobalczyk, Zhiyuan Jerry Lin et al. | For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space. Unlike preferential BO, which only accepts restricted feedback formats and requires customized models for each domain-specific problem, our approach leverages LLMs to turn varied types of textual feedback into consistent utility signals and to easily include flexible user priors without manual kernel design. At the same time, our method maintains the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes. |
| 2025-10-20 | [Space-Time Rate-Splitting Multiple Access for Multibeam LEO Satellite Networks](http://arxiv.org/abs/2510.17625v1) | Jaehyup Seong, Byungju Lee et al. | This paper proposes a novel space-time rate-splitting multiple access (ST-RSMA) framework for multibeam low Earth orbit (LEO) satellite communications (SATCOM) systems, where space-time coding is integrated into the common stream transmission. This design enables full diversity gain in the common stream transmission for all users, regardless of the uncertainty of the channel state information (CSI) and network load conditions, thereby overcoming the performance limitations of conventional RSMA that employs a single beamforming vector for all users. To further enhance performance, we develop a weighted minimum mean square error (WMMSE)-based algorithm tailored to ST-RSMA that jointly optimizes the power allocation for the common stream and the power/beamforming vectors for private streams, aiming to maximize the minimum user rate. Numerical results show that ST-RSMA significantly outperforms conventional RSMA and other multiple access techniques, offering a robust and scalable solution for LEO SATCOM. |
| 2025-10-20 | [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](http://arxiv.org/abs/2510.17614v1) | Praphul Singh, Corey Barrett et al. | Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection. |
| 2025-10-20 | [Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials](http://arxiv.org/abs/2510.17550v1) | Andrew Hall, Duncan Wilson et al. | The main objective of dose finding trials is to find an optimal dose amongst a candidate set for further research. The trial design in oncology proceeds in stages with a decision as to how to treat the next group of patients made at every stage until a final sample size is reached or the trial stopped early.   This work applies a Bayesian decision-theoretic approach to the problem, proposing a new utility function based on both efficacy and toxicity and grounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed framework seeks to better capture real clinical judgements by allowing attitudes to risk to vary when the judgements are of gains or losses, which are defined with respect to an intermediate outcome known as a reference point. We call this method Reference Dependent Decision Theoretic dose finding (R2DT).   A simulation study demonstrates that the framework can perform well and produce good operating characteristics. The simulation results demonstrate that R2DT is better at detecting the optimal dose in scenarios where candidate doses are around minimum acceptable efficacy and maximum acceptable toxicity thresholds.   The proposed framework shows that a flexible utility function, which better captures clinician beliefs, can lead to trials with good operating characteristics, including a high probability of finding the optimal dose. Our work demonstrates proof-of-concept for this framework, which should be evaluated in a broader range of settings. |
| 2025-10-20 | [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](http://arxiv.org/abs/2510.17548v1) | Nisrine Rair, Alban Goupil et al. | Language models are often evaluated with scalar metrics like accuracy, but such measures fail to capture how models internally represent ambiguity, especially when human annotators disagree. We propose a topological perspective to analyze how fine-tuned models encode ambiguity and more generally instances.   Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from topological data analysis, reveals that fine-tuning restructures embedding space into modular, non-convex regions aligned with model predictions, even for highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$ prediction purity, yet alignment with ground-truth labels drops in ambiguous data, surfacing a hidden tension between structural confidence and label uncertainty.   Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry directly uncovering decision regions, boundary collapses, and overconfident clusters. Our findings position Mapper as a powerful diagnostic tool for understanding how models resolve ambiguity. Beyond visualization, it also enables topological metrics that may inform proactive modeling strategies in subjective NLP tasks. |
| 2025-10-20 | [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](http://arxiv.org/abs/2510.17496v1) | Giacomo Camposampiero, Michael Hersche et al. | We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes. |
| 2025-10-17 | [A Unifying Convexification Framework for Chance-Constrained Programs via Bilinear Extended Formulations over a Simplex](http://arxiv.org/abs/2510.15861v1) | Danial Davarnia, Hamed Rahimian | Chance-constrained programming is a widely used framework for decision-making under uncertainty, yet its mixed-integer reformulations involve nonconvex mixing sets with a knapsack constraint, leading to weak relaxations and computational challenges. Most existing approaches for strengthening the relaxations of these sets rely primarily on extensions of a specific class of valid inequalities, limiting both convex hull coverage and the discovery of fundamentally new structures. In this paper, we develop a novel convexification framework that reformulates chance-constrained sets as bilinear sets over a simplex in a lifted space and employs a step-by-step aggregation procedure to derive facet-defining inequalities in the original space of variables. Our approach generalizes and unifies established families of valid inequalities in the literature while introducing new ones that capture substantially larger portions of the convex hull. Main contributions include: (i) the development of a new aggregation-based convexification technique for bilinear sets over a simplex in a lower-dimensional space; (ii) the introduction of a novel bilinear reformulation of mixing sets with a knapsack constraint -- arising from single-row relaxations of chance constraints -- over a simplex, which enables the systematic derivation of strong inequalities in the original variable space; and (iii) the characterization of facet-defining inequalities within a unified framework that contains both existing and new families. Preliminary computational experiments demonstrate that our inequalities describe over 90\% of the facet-defining inequalities of the convex hull of benchmark instances, significantly strengthening existing relaxations and advancing the polyhedral understanding of chance-constrained programs. |
| 2025-10-17 | [Quantum Monte Carlo Calculations of Light Nuclei with Fully Propagated Theoretical Uncertainties](http://arxiv.org/abs/2510.15860v1) | Ryan Curry, Kai Hebeler et al. | We report on the first quantum Monte Carlo calculations of helium isotopes with fully propagated theoretical uncertainties from the interaction to the many-body observables. To achieve this, we build emulators for solutions to the Faddeev equations for the binding energy and Gamow-Teller matrix element of $^3\text{H}$, as well as for auxiliary-field diffusion Monte Carlo calculations of the $^4\text{He}$ charge radius, employing local two- and three-body interactions up to next-to-next-to-leading order in chiral effective field theory. We use these emulators to determine the posterior distributions for all low-energy couplings that appear in the interaction up to this order using Bayesian inference while accounting for theoretical uncertainties. We then build emulators for auxiliary-field diffusion Monte Carlo for helium isotopes and propagate the full posterior distributions to these systems. Our approach serves as a framework for $\textit{ab initio}$ studies of atomic nuclei with consistently treated and correlated theoretical uncertainties. |
| 2025-10-17 | [Bio-inspired Microgrid Management based on Brain's Sensorimotor Gating](http://arxiv.org/abs/2510.15847v1) | Panos C. Papageorgiou, Anastasios E. Giannopoulos et al. | Microgrids are emerging as key enablers of resilient, sustainable, and intelligent power systems, but they continue to face challenges in dynamic disturbance handling, protection coordination, and uncertainty. Recent efforts have explored Brain Emotional Learning (BEL) controllers as bio-inspired solutions for microgrid control. Building on this growing trajectory, this article introduces a new paradigm for Neuro-Microgrids, inspired by the brain's sensorimotor gating mechanisms, specifically the Prepulse Inhibition (PPI) and Prepulse Facilitation (PPF). Sensorimotor gating offers a biological model for selectively suppressing or amplifying responses depending on contextual relevance. By mapping these principles onto the hierarchical control architecture of microgrids, we propose a Sensorimotor Gating-Inspired Neuro-Microgrid (SG-NMG) framework. In this architecture, PPI-like control decisions correspond to protective damping in primary and secondary management of microgrids, whereas PPF-like decisions correspond to adaptive amplification of corrective control actions. The framework is presented through analytical workflow design, neuro-circuitry analogies, and integration with machine learning methods. Finally, open challenges and research directions are outlined, including the mathematical modeling of gating, digital twin validation, and cross-disciplinary collaboration between neuroscience and industrial power systems. The resulting paradigm highlights sensorimotor gating as a promising framework for designing self-protective, adaptive, and resilient microgrids. |
| 2025-10-17 | [From Localization to Discovery: Bayesian Ranking of Electromagnetic Counterparts to Gravitational-Wave Events](http://arxiv.org/abs/2510.15836v1) | Kendall Ackley | The robust association of electromagnetic candidates discovered during follow-up of gravitational-wave alerts is challenging, not only due to the large sky areas and broad distance uncertainties, but also due to the tens to hundreds of unrelated optical transients that are observed per event. We present a Bayesian ranking method to identify electromagnetic counterparts to GW events using only location information. The framework combines three-dimensional gravitational wave skymaps with host-galaxy information, a morphology-aware host association, empirical offset priors, and peculiar velocity corrections. We apply the method to GW170817 where it ranks AT2017gfo as the top candidate and correctly selects NGC\,4993 as the host. The approach is directly applicable to transient candidates with only location information and enables more efficient follow-up with prioritized candidates and leads to more reliable counterpart identification in current and future observing runs. |
| 2025-10-17 | [The kinetic Sunyaev Zeldovich effect as a benchmark for AGN feedback models in hydrodynamical simulations: insights from DESI + ACT](http://arxiv.org/abs/2510.15822v1) | Leah Bigwood, Masaya Yamamoto et al. | Baryonic feedback remains one of the largest uncertainties in cosmological hydrodynamical simulations, with different prescriptions producing divergent predictions for the fraction of gas expelled from halos, the radial extent of the gas expulsion and the impact on large scale matter clustering. We present the first systematic study of the kinetic Sunyaev-Zel'dovich (kSZ) effect across a wide range of simulations (FLAMINGO, ANTILLES, BAHAMAS, SIMBA, FABLE and their variants), and compare them directly to DESI Year 1 + ACT kSZ measurements. We ensure a like-for-like comparison with observations by developing a robust methodology that accounts for the halo mass selection using galaxy-galaxy lensing, cosmic variance, miscentering and satellites, establishing the kSZ effect as a new benchmark for the simulations. We find that fiducial feedback models are disfavoured by >3 sigma, while simulations with more powerful AGN feedback within the FLAMINGO and BAHAMAS suites, as well as SIMBA, reproduce the observed kSZ signal within <2 sigma. We use the ANTILLES simulation suite to demonstrate that the amplitude of the kSZ effect is a strong predictor of matter power spectrum suppression, competitive with baryon fraction metrics. These results establish the kSZ as a critical probe for evaluating feedback physics and for advancing the fidelity of cosmological simulations. |
| 2025-10-17 | [Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction](http://arxiv.org/abs/2510.15780v1) | Alireza Moradi, Mathieu Tanneau et al. | Accurate forecasting is critical for reliable power grid operations, particularly as the share of renewable generation, such as wind and solar, continues to grow. Given the inherent uncertainty and variability in renewable generation, probabilistic forecasts have become essential for informed operational decisions. However, such forecasts frequently suffer from calibration issues, potentially degrading decision-making performance. Building on recent advances in Conformal Predictions, this paper introduces a tailored calibration framework that constructs context-aware calibration sets using a novel weighting scheme. The proposed framework improves the quality of probabilistic forecasts at the site and fleet levels, as demonstrated by numerical experiments on large-scale datasets covering several systems in the United States. The results demonstrate that the proposed approach achieves higher forecast reliability and robustness for renewable energy applications compared to existing baselines. |
| 2025-10-17 | [Multiphysics inversion with variable complexity of receiver-function, surface-wave dispersion and magnetotelluric data reduces uncertainty for lithosphere structure](http://arxiv.org/abs/2510.15779v1) | P. Shahsavari, J. Dettmer et al. | We present a probabilistic multiphysics inversion based on Bayesian inference with trans-dimensional models. We jointly consider magnetotelluric, receiver function, and Rayleigh-wave dispersion data to infer one-dimensional lithospheric structure in the vicinity of Athabasca, Canada. The location is on the North American Craton with a cover of sediments from the Western Canada Sedimentary Basin. The trans-dimensional model uses layer nodes that include parameters that are activated or deactivated based on data information. Furthermore, the number of nodes is based on data information. Hence, the parameterization uncertainty is included in the uncertainty estimates. Furthermore, the layer nodes permit trans-dimensional decoupling such that some discontinuities may be represented by only some of the parameters. In probabilistic multiphysics inversion, it is important that the various data types are weighed objectively. Here, the weights are the data covariance matrices of the various data types. We apply empirical estimation of data covariance matrices and employ hierarchical scaling parameters to reduce the dependence on some assumptions required by the empirical approach. Hence, we account for noise variances and covariances, which is crucial for successful probabilistic multiphysics inversion. The parameter estimates and data covariance matrices are obtained with the reversible-jump Markov chain Monte Carlo algorithm with parallel tempering to enhance the efficiency. Since covariance matrix estimation changes data weights, the estimation process is carried out while samples are not recorded for inference. The results at the Athabasca site fit the data and produce plausible data covariance matrices for the data weights. |
| 2025-10-17 | [Integrating Conductor Health into Dynamic Line Rating and Unit Commitment under Uncertainty](http://arxiv.org/abs/2510.15740v1) | Geon Roh, Jip Kim | Dynamic line rating (DLR) enables greater utilization of existing transmission lines by leveraging real-time weather data. However, the elevated temperature operation (ETO) of conductors under DLR is often overlooked, despite its long-term impact on conductor health. This paper addresses this issue by 1) quantifying depreciation costs associated with ETO and 2) proposing a Conductor Health-Aware Unit Commitment (CHA-UC) that internalizes these costs in operational decisions. The CHA-UC incorporates a robust linear approximation of conductor temperature and integration of expected depreciation costs due to hourly ETO into the objective function. Case studies on the Texas 123-bus backbone test system using NOAA weather data demonstrate that the proposed CHA-UC model reduces the total cost by 0.8% and renewable curtailment by 84%compared to static line rating (SLR), while conventional DLR operation without risk consideration resulted in higher costs due to excessive ETO. Further analysis of the commitment decisions and the line temperature statistics confirms that the CHA-UC achieves safer line flows by shifting generator commitments. Finally, we examine the emergent correlation between wind generation and DLR forecast errors, and show that CHA-UC adaptively manages this effect by relaxing flows for risk-hedging conditions while tightening flows for risk-amplifying ones. |
| 2025-10-17 | [Robust Insurance Pricing and Liquidity Management](http://arxiv.org/abs/2510.15709v1) | Shunzhi Pang | With the rise of emerging risks, model uncertainty poses a fundamental challenge in the insurance industry, making robust pricing a first-order question. This paper investigates how insurers' robustness preferences shape competitive equilibrium in a dynamic insurance market. Insurers optimize their underwriting and liquidity management strategies to maximize shareholder value, leading to equilibrium outcomes that can be analytically derived and numerically solved. Compared to a benchmark without model uncertainty, robust insurance pricing results in significantly higher premiums and equity valuations. Notably, our model yields three novel insights: (1) The minimum, maximum, and admissible range of aggregate capacity all expand, indicating that insurers' liquidity management becomes more conservative. (2) The expected length of the underwriting cycle increases substantially, far exceeding the range commonly reported in earlier empirical studies. (3) While the capacity process remains ergodic in the long run, the stationary density becomes more concentrated in low-capacity states, implying that liquidity-constrained insurers require longer to recover. Together, these findings provide a potential explanation for recent skepticism regarding the empirical evidence of underwriting cycles, suggesting that such cycles may indeed exist but are considerably longer than previously assumed. |
| 2025-10-17 | [Two-Stage Data-Driven Contextual Robust Optimization: An End-to-End Learning Approach for Online Energy Applications](http://arxiv.org/abs/2510.15696v1) | Carlos Gamboa, Alexandre Street et al. | Traditional end-to-end contextual robust optimization models are trained for specific contextual data, requiring complete retraining whenever new contextual information arrives. This limitation hampers their use in online decision-making problems such as energy scheduling, where multiperiod optimization must be solved every few minutes. In this paper, we propose a novel Data-Driven Contextual Uncertainty Set, which gives rise to a new End-to-End Data-Driven Contextual Robust Optimization model. For right-hand-side uncertainty, we introduce a reformulation scheme that enables the development of a variant of the Column-and-Constraint Generation (CCG) algorithm. This new CCG method explicitly considers the contextual vector within the cut expressions, allowing previously generated cuts to remain valid for new contexts, thereby significantly accelerating convergence in online applications. Numerical experiments on energy and reserve scheduling problems, based on classical test cases and large-scale networks (with more than 10,000 nodes), demonstrate that the proposed method reduces computation times and operational costs while capturing context-dependent risk structures. The proposed framework (model and method), therefore, offers a unified, scalable, and prescriptive approach to robust decision-making under uncertainty, effectively bridging data-driven learning and optimization. |
| 2025-10-16 | [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](http://arxiv.org/abs/2510.14959v1) | Lizhi Yang, Blake Werner et al. | Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter. |
| 2025-10-16 | [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](http://arxiv.org/abs/2510.14926v1) | Khalak Mahadeviya, Saulo V. Moreira et al. | We investigate current fluctuations in open quantum systems beyond the weak-coupling and Markovian regimes, focusing on a coherently driven qubit strongly coupled to a structured bosonic environment. By combining full counting statistics with the reaction coordinate mapping, we develop a framework that enables the calculation of steady-state current fluctuations and their temporal correlations in the strong-coupling regime. Our analysis reveals that, unlike in weak coupling, both the average current and its fluctuations exhibit nonmonotonic dependence on the system-environment interaction strength. Notably, we identify a regime where current noise is suppressed below the classical thermodynamic uncertainty bound, coinciding with enhanced anticorrelations in quantum jump trajectories and faster system relaxation. We further show that these features are linked to nonclassical properties of the reaction coordinate mode, such as non-Gaussianity and quantum coherence. Our results provide new insights and design principles for controlling current fluctuations in quantum devices operating beyond the standard weak-coupling paradigm. |
| 2025-10-16 | [Astrophysical uncertainties challenge 21-cm forecasts: A primordial black hole case study](http://arxiv.org/abs/2510.14877v1) | Dominic Agius, Rouven Essig et al. | The 21-cm signal is a powerful probe of the early Universe's thermal history and could provide a unique avenue for constraining exotic physics. Previous studies have forecasted stringent constraints on energy injections from exotic sources that heat, excite, and ionize the background gas and thereby modify the 21-cm signal. In this work, we quantify the substantial impact that astrophysical uncertainties have on the projected sensitivity to exotic energy injection. In particular, there are significant uncertainties in the minimum star-forming dark matter halo mass, the Lyman-$\alpha$ emission, and the X-ray emission, whose values characterize the fiducial astrophysical model when projecting bounds. As a case study, we investigate the energy injection of accreting primordial black holes of mass $\sim 1~M_\odot-10^3~M_\odot$, also taking into account uncertainties in the accretion model. We show that, depending on the chosen fiducial model and accretion uncertainties, the sensitivity of future 21-cm data could constrain the abundance of primordial black holes to be either slightly stronger, or significantly weaker, than current limits from the Cosmic Microwave Background. |
| 2025-10-16 | [A new photometric ephemeris for the 2M1510 AB double brown dwarf eclipsing binary system](http://arxiv.org/abs/2510.14801v1) | Seb T. Millward, Vedad Kunovac | Eclipsing brown dwarfs are important calibrators of sub-stellar evolution models used to infer the characteristics of directly imaged brown dwarfs and giant exoplanets. Only two double brown dwarf eclipsing binary systems are known, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a poorly constrained orbital period. Here we analyse TESS full-frame image (FFI) photometry of this faint ($T=15.9$) binary and detect a significant (${>}10\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with previous data. We refine the orbital period to $20.897782 \pm 0.000036$ d, reducing its present-day uncertainty from 18 h to 8 min. Our work is crucial for scheduling follow-up observations of this system for detailed study with other photometric facilities. We also find that a recent orbital solution from Doppler data is inconsistent with existing photometry. A timing offset in the Doppler data may have produced a spurious signal mimicking retrograde apsidal precession, from which the claimed circumbinary planet 2M1510 ABb was inferred. From our best attempt at correcting the data we were unable to reconcile the radial velocity data with the photometry, suggesting that the radial velocity uncertainties are underestimated, and that the circumbinary planet 2M1510 ABb may be a false positive. |
| 2025-10-16 | [Measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays with the LHCb Upgrade I detector](http://arxiv.org/abs/2510.14732v1) | LHCb collaboration, R. Aaij et al. | A measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays is reported, based on a data sample of proton-proton collisions collected with the LHCb Upgrade I detector in 2024 at a centre-of-mass energy of $13.6\,$TeV, corresponding to an integrated luminosity of $6.2\,\mathrm{fb}^{-1}$. The $D^0 \to K^0_{\rm S} \pi^+ \pi^-$ decay is used as calibration channel to cancel residual detection and production asymmetries. The time-integrated $C\!P$ asymmetry for the $D^0 \to K^0_{\rm S} K^0_{\rm S}$ mode is measured to be $$ {\cal A}^{C\!P} (D^0 \to K^0_{\rm S} K^0_{\rm S}) = (1.86 \pm 1.04\pm 0.41)\%, $$ where the first uncertainty is statistical, and the second is systematic. This is the most precise determination of this quantity to date. |
| 2025-10-16 | [Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock](http://arxiv.org/abs/2510.14715v1) | Guobin Mou | Recent long-term radio monitoring of tidal disruption events (TDEs) suggests that radio afterglows are common. Most studies argue that these afterglows may arise from forward shocks (FS) produced by the interaction between the TDE outflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical models do not model the evolution of relativistic electrons in space, which introduces uncertainties. Here we conducted hydrodynamic simulations to study the hydrodynamic evolution of relativistic electrons, and calculated the synchrotron spectra via radiative transfer. We focus on the FS scenario with non-relativistic outflows, and various parameters of the outflow and CNM are explored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a Galactic center - like CNM can produce mJy-level radio afterglows at a distance of 100 Mpc. The self-absorption frequency exhibits a slow decline at early times and a rapid decrease at late times. We derived the temporal evolution of the high-frequency radio flux, revealing its characteristic rise and decline pattern. We also find that: (1) the radio spectra for narrow outflows are clearly anisotropic along different sight lines; (2) the FS parameters inferred from radio spectra using conventional analytical formulas deviate significantly from those in simulations, in which the inferred shock radii are half of those from simulations, and the inferred energies are an order of magnitude lower. |
| 2025-10-16 | [Fast and Scalable Score-Based Kernel Calibration Tests](http://arxiv.org/abs/2510.14711v1) | Pierre Glaser, David Widmann et al. | We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a non-parametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a conditional goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the properties of our test on various synthetic settings. |
| 2025-10-16 | [Practical, Utilitarian Algorithm Configuration](http://arxiv.org/abs/2510.14683v1) | Devon Graham, Kevin Leyton-Brown | Utilitarian algorithm configuration identifies a parameter setting for a given algorithm that maximizes a user's utility. Utility functions offer a theoretically well-grounded approach to optimizing decision-making under uncertainty and are flexible enough to capture a user's preferences over algorithm runtimes (e.g., they can describe a sharp cutoff after which a solution is no longer required, a per-hour cost for compute, or diminishing returns from algorithms that take longer to run). COUP is a recently-introduced utilitarian algorithm configuration procedure which was designed mainly to offer strong theoretical guarantees about the quality of the configuration it returns, with less attention paid to its practical performance. This paper closes that gap, bringing theoretically-grounded, utilitarian algorithm configuration to the point where it is competitive with widely used, heuristic configuration procedures that offer no performance guarantees. We present a series of improvements to COUP that improve its empirical performance without degrading its theoretical guarantees and demonstrate their benefit experimentally. Using a case study, we also illustrate ways of exploring the robustness of a given solution to the algorithm selection problem to variations in the utility function. |
| 2025-10-16 | [Precision of an autonomous demon exploiting nonthermal resources and information](http://arxiv.org/abs/2510.14578v1) | Juliette Monsel, Matteo Acciai et al. | Quantum-dot systems serve as nanoscale heat engines exploiting thermal fluctuations to perform a useful task. Here, we investigate a multi-terminal triple-dot system, operating as a refrigerator that extracts heat from a cold electronic contact. In contrast to standard heat engines, this system exploits a nonthermal resource. This has the intriguing consequence that cooling can occur without extracting energy from the resource on average -- a seemingly demonic action -- while, however, requiring the resource to fluctuate. Using full counting statistics and stochastic trajectories, we analyze the performance of the device in terms of the cooling-power precision, employing performance quantifiers motivated by the thermodynamic and kinetic uncertainty relations. We focus on two regimes with large output power, which are based on two operational principles: exploiting information on one hand and the nonthermal properties of the resource on the other. We show that these regimes significantly differ in precision. In particular, the regime exploiting the nonthermal properties of the resource can have cooling-power fluctuations that are suppressed with respect to the input fluctuations by an order of magnitude. We also substantiate the interpretation of the two different working principles by analyzing cross-correlations between input and output heat currents and information flow. |
| 2025-10-16 | [Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory](http://arxiv.org/abs/2510.14555v1) | Amal Sakr, Andrea Araldo et al. | The introduction of new services, such as Mobile Edge Computing (MEC), requires a massive investment that cannot be assumed by a single stakeholder, for instance the Infrastructure Provider (InP). Service Providers (SPs) however also have an interest in the deployment of such services. We hence propose a co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form the so-called grand coalition composed of all the stakeholders with the aim of sharing costs and revenues and maximizing their payoffs. The challenge comes from the fact that future revenues are uncertain. We devise in this case a novel stochastic coalitional game formulation which builds upon robust game theory and derive a lower bound on the probability of the stability of the grand coalition, wherein no player can be better off outside of it. In the presence of some correlated fluctuations of revenues however, stability can be too conservative. In this case, we make use also of profitability, in which payoffs of players are non-negative, as a necessary condition for co-investment. The proposed framework is showcased for MEC deployment, where computational resources need to be deployed in nodes at the edge of a telecommunication network. Numerical results show high lower bound on the probability of stability when the SPs' revenues are of similar magnitude and the investment period is sufficiently long, even with high levels of uncertainty. In the case where revenues are highly variable however, the lower bound on stability can be trivially low whereas co-investment is still profitable. |
| 2025-10-15 | [$\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF](http://arxiv.org/abs/2510.13805v1) | David Gebauer, Anik Halder et al. | We present $\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework for analysing a higher-order weak lensing statistic, the integrated 3-point correlation function (i3PCF). Our approach forward-models the cosmic shear field using the $\texttt{CosmoGridV1}$ suite of N-body simulations, including a comprehensive set of systematic effects such as intrinsic alignment, baryonic feedback, photometric redshift uncertainty, shear calibration bias, and shape noise. Using this, we have produced a set of DES Y3-like synthetic measurements for 2-point shear correlation functions $\xi_{\pm}$ (2PCFs) and i3PCFs $\zeta_{\pm}$ across 6 cosmological and 11 systematic parameters. Having validated these measurements against theoretical predictions and thoroughly examined for potential systematic biases, we have found that the impact of source galaxy clustering and reduced shear on the i3PCF is negligible for Stage-III surveys. Furthermore, we have tested the Gaussianity assumption for the likelihood of our data vector and found that while the sampling distribution of the 2PCF can be well approximated by a Gaussian function, the likelihood of the combined 2PCF + i3PCF data vector including filter sizes of $90'$ and larger can deviate from this assumption. Our SBI pipeline employs masked autoregressive flows to perform neural likelihood estimation and is validated to give statistically accurate posterior estimates. On mock data, we find that including the i3PCF yields a substantial $63.8\%$ median improvement in the figure of merit for $\Omega_m - \sigma_8 - w_0$. These findings are consistent with previous works on the i3PCF and demonstrate that our SBI framework can achieve the accuracy and realism needed to analyse the i3PCF in wide-area weak lensing surveys. |
| 2025-10-15 | [Splitting Isotope Shift in the $1s2p\,^3\!P_{0,1,2}$ Fine-Structure Triplet in $^{12,13,14}$C$^{4+}$: Experiment and Theory](http://arxiv.org/abs/2510.13779v1) | Patrick M√ºller, Kristian K√∂nig et al. | We report measurements and theoretical calculations of the fine-structure splittings in all three $1s2s\,^3\!S_1\rightarrow\,1s2p\,^3\!P_{0,1,2}$ transitions in the heliumlike systems of the isotopes $^{12,13,14}$C. The metastable triplet state was efficiently populated in an electron beam ion source and the C$^{4+}$ ions were electrostatically accelerated to 50\,keV to perform collinear laser spectroscopy. From the determined transition frequencies, the splitting isotope shift (SIS), i.e., the difference in fine-structure splittings between different isotopes of the same element, was extracted. In the SIS, theoretical uncertainties due to higher-order quantum electrodynamic corrections are strongly suppressed since they are independent of both nuclear mass and the fine-structure quantum number $J$ in lowest order. Comparison with theory provides an important test of experimental accuracy, particularly in the $^{13}$C$^{4+}$ case, for which the nuclear spin leads to hyperfine-induced fine-structure mixing. At the same time, the even-even isotopes $^{12,14}$C$^{4+}$ without nuclear spin can be used to confirm theory. Theoretical values of the SIS are given for all the heliumlike ions with $2\le Z\le 10$. |
| 2025-10-15 | [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](http://arxiv.org/abs/2510.13750v1) | Zhiqi Huang, Vivek Datla et al. | We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment. |
| 2025-10-15 | [VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions](http://arxiv.org/abs/2510.13705v1) | Fan Chang, Yijia Fang | In this paper, we uncover a new uncertainty principle that governs the complexity of Boolean functions. This principle manifests as a fundamental trade-off between two central measures of complexity: a combinatorial complexity of its supported set, captured by its Vapnik-Chervonenkis dimension ($\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial degree over various fields. We establish two primary inequalities that formalize this trade-off:$\mathrm{VC}(f)+\deg(f)\ge n,$ and $\mathrm{VC}(f)+\deg_{\mathbb{F}_2}(f)\ge n$. In particular, these results recover the classical uncertainty principle on the discrete hypercube, as well as the Sziklai--Weiner's bound in the case of $\mathbb{F}_2$. |
| 2025-10-15 | [Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping](http://arxiv.org/abs/2510.13672v1) | Marc√≠lio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo | Dengue remains one of Brazil's major epidemiological challenges, marked by strong intra-urban inequalities and the influence of climatic and socio-environmental factors. This study analyzed confirmed dengue cases in Recife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model implemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal component. Covariates included population density, household size, income, drainage channels, lagged precipitation, and mean temperature. Population density and household size had positive effects on dengue risk, while income and channel presence were protective. Lagged precipitation increased risk, and higher temperatures showed an inverse association, suggesting thermal thresholds for vector activity. The model achieved good fit (DIC=65817; WAIC=64506) and stable convergence, with moderate residual spatial autocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019. Spatio-temporal estimates revealed persistent high-risk clusters in northern and western Recife, overlapping with areas of higher density and social vulnerability. Beyond reproducing historical patterns, the Bayesian model supports probabilistic forecasting and early warning systems. Compared with classical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty and spatial-temporal dependence, offering credible interval inference for decision-making in urban health management. |
| 2025-10-15 | [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](http://arxiv.org/abs/2510.13643v1) | Akib Mohammed Khan, Bartosz Krawczyk | Foundation models such as DINOv2 have shown strong performance in few-shot anomaly detection, yet two key questions remain unexamined: (i) how susceptible are these detectors to adversarial perturbations; and (ii) how well do their anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a training-free deep nearest-neighbor detector over DINOv2 features, we present one of the first systematic studies of adversarial attacks and uncertainty estimation in this setting. To enable white-box gradient attacks while preserving test-time behavior, we attach a lightweight linear head to frozen DINOv2 features only for crafting perturbations. Using this heuristic, we evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible perturbations can flip nearest-neighbor relations in feature space to induce confident misclassification. Complementing robustness, we probe reliability and find that raw anomaly scores are poorly calibrated, revealing a gap between confidence and correctness that limits safety-critical use. As a simple, strong baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly scores for uncertainty estimation. The resulting calibrated posteriors yield significantly higher predictive entropy on adversarially perturbed inputs than on clean ones, enabling a practical flagging mechanism for attack detection while reducing calibration error (ECE). Our findings surface concrete vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an evaluation protocol and baseline for robust, uncertainty-aware anomaly detection. We argue that adversarial robustness and principled uncertainty quantification are not optional add-ons but essential capabilities if anomaly detection systems are to be trustworthy and ready for real-world deployment. |
| 2025-10-15 | [Radiative decays of the $Œ©(2012)$ as a hadronic molecule](http://arxiv.org/abs/2510.13623v1) | Qing-Hua Shen, Jun-Xu Lu et al. | We present a theoretical investigation of the radiative decay process $\Omega(2012) \to \gamma \Omega$, where the $\Omega(2012)$ resonance with spin-parity $J^P=\frac{3}{2}^-$, is treated as a dynamically generated state from $\bar{K}\Xi(1530)$ and $\eta \Omega$ in $s$-wave and $\bar{K}\Xi$ in $d$-wave. The radiative decay width of the $\Omega(2012)$ is calculated using a triangular loop mechanism, where the $\Omega(2012)$ couples to the $\bar{K} \Xi(1530)$ channel. Subsequently, the final state interactions between $\Xi(1530)$ and $\bar{K}$ transition to a photon and $\Omega$ through the exchange of a $\Xi$ baryon. Our calculations yield a radiative decay width of $13.2 ^{+4.5}_{-3.9}$ KeV, with uncertainties arising from the model parameters. This result provides valuable insights into the nature of the $\Omega(2012)$ resonance and its decay dynamics. It is expected that the calculations presented here could be verified by future experiments, which would open a new door for studying the still elusive nature of the $\Omega(2012)$. |
| 2025-10-15 | [Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity](http://arxiv.org/abs/2510.13609v1) | Ahmad Awad, Erik Scharw√§chter | Soil organic carbon (SOC) sequestration projects require unbiased, precise and cost-effective Monitoring, Reporting, and Verification (MRV) systems that balance sampling costs against uncertainty deductions imposed by regulatory frameworks. Design-based estimators guarantee unbiasedness but cannot exploit auxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can improve precision but require independent validation for each project. Model-assisted estimation offers a robust compromise, combining model predictions with probability sampling to retain design-based guarantees while improving precision. We evaluate the scientific integrity and efficiency of the simple regression estimator (SRE), a well-known model-assisted estimator, via an extensive simulation study. Our simulations span diverse SOC stock variances, sample sizes, and model performances. We assess three core properties: empirical bias, empirical confidence interval coverage, and precision gain relative to the design-based Horvitz-Thompson estimator (HTE). Results show negligible bias and valid coverage probabilities for n > 40, regardless of SOC stock variance. Below this threshold, variance approximations and normality assumptions yield unreliable uncertainty estimates. With correlated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains over HTE. With uncorrelated variables, no gains are observed, but performance converges to HTE for n >= 40. Model-assisted estimation can enhance project economics without compromising scientific rigor. Regulators should permit such estimators while mandating minimum sample size thresholds. Project proponents should routinely employ such estimators when correlated ancillary variables exist. The industry should prioritize the retrieval of high-quality, project-specific covariates to maximize precision gains and thereby the project economics. |
| 2025-10-15 | [Unsupervised Constitutive Model Discovery from Sparse and Noisy Data](http://arxiv.org/abs/2510.13559v1) | Vahab Knauf Narouie, Jorge-Humberto Urrea-Quintero et al. | Recently, unsupervised constitutive model discovery has gained attention through frameworks based on the Virtual Fields Method (VFM), most prominently the EUCLID approach. However, the performance of VFM-based approaches, including EUCLID, is affected by measurement noise and data sparsity, which are unavoidable in practice. The statistical finite element method (statFEM) offers a complementary perspective by providing a Bayesian framework for assimilating noisy and sparse measurements to reconstruct the full-field displacement response, together with quantified uncertainty. While statFEM recovers displacement fields under uncertainty, it does not strictly enforce consistency with constitutive relations or aim to yield interpretable constitutive models. In this work, we couple statFEM with unsupervised constitutive model discovery in the EUCLID framework, yielding statFEM--EUCLID. The framework is demonstrated for isotropic hyperelastic materials. The results show that this integration reduces sensitivity to noise and data sparsity, while ensuring that the reconstructed fields remain consistent with both equilibrium and constitutive laws. |
| 2025-10-15 | [Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage](http://arxiv.org/abs/2510.13514v1) | Andreas C. Makrides, Adam Suski et al. | The transition to a fully decarbonised electricity system depends on integrating new technologies that ensure reliability alongside sustainability. However, missing risk markets hinder investment in reliability-enhancing technologies by exposing investors to revenue uncertainty. This study provides the first quantitative assessment of how missing risk markets affect investment decisions in power systems that depend on long-duration energy storage (LDES) for reliability. We develop a two-stage stochastic equilibrium model with risk-averse market participants, which independently sizes power and energy capacity. We apply the method to a case study of a deeply decarbonised power system in Great Britain. The results show that incomplete risk markets reduce social welfare, harm reliability, and discourage investment in LDES and other technologies with volatile revenue streams. Revenue volatility leads to substantial risk premiums and higher financing costs for LDES, creating a barrier to its large-scale deployment. These findings demonstrate the importance of policy mechanisms that hedge revenue risk to lower the cost of capital and accelerate investment in reliability-enhancing, zero-carbon technologies |
| 2025-10-14 | [What If : Understanding Motion Through Sparse Interactions](http://arxiv.org/abs/2510.12777v1) | Stefan Andreas Baumann, Nick Stracke et al. | Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer. |
| 2025-10-14 | [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](http://arxiv.org/abs/2510.12768v1) | Fengzhi Guo, Chih-Chuan Hsu et al. | Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints. |
| 2025-10-14 | [(Non-Parametric) Bootstrap Robust Optimization for Portfolios and Trading Strategies](http://arxiv.org/abs/2510.12725v1) | Daniel Cunha Oliveira, Grover Guzman et al. | Robust optimization provides a principled framework for decision-making under uncertainty, with broad applications in finance, engineering, and operations research. In portfolio optimization, uncertainty in expected returns and covariances demands methods that mitigate estimation error, parameter instability, and model misspecification. Traditional approaches, including parametric, bootstrap-based, and Bayesian methods, enhance stability by relying on confidence intervals or probabilistic priors but often impose restrictive assumptions. This study introduces a non-parametric bootstrap framework for robust optimization in financial decision-making. By resampling empirical data, the framework constructs flexible, data-driven confidence intervals without assuming specific distributional forms, thus capturing uncertainty in statistical estimates, model parameters, and utility functions. Treating utility as a random variable enables percentile-based optimization, naturally suited for risk-sensitive and worst-case decision-making. The approach aligns with recent advances in robust optimization, reinforcement learning, and risk-aware control, offering a unified perspective on robustness and generalization. Empirically, the framework mitigates overfitting and selection bias in trading strategy optimization and improves generalization in portfolio allocation. Results across portfolio and time-series momentum experiments demonstrate that the proposed method delivers smoother, more stable out-of-sample performance, offering a practical, distribution-free alternative to traditional robust optimization methods. |
| 2025-10-14 | [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](http://arxiv.org/abs/2510.12699v1) | Sunny Yu, Ahmad Jabbar et al. | Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs. |
| 2025-10-14 | [EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels](http://arxiv.org/abs/2510.12687v1) | Kunyu Peng, Di Wen et al. | Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM. |
| 2025-10-14 | [Measurement of the tau anomalous magnetic moment using Ultra-peripheral collisions with the ALICE detector in Run 3 Pb-Pb data](http://arxiv.org/abs/2510.12661v1) | Roman Laviƒçka, Paul Alois B√ºhler | The anomalous magnetic moment of the tau lepton ($a_{\tau}$) is a sensitive probe for the search for deviations from the Standard Model predictions and thus for new physics. This study investigates the feasibility of measuring $a_{\tau}$ using ultra-peripheral collisions (UPCs) at the LHC, where photon-photon interactions ($\gamma\gamma \to \tau^+ \tau^-$) produce tau lepton pairs. We focus on events recorded by the ALICE detector during Run 3 Pb-Pb data-taking. Events are selected in the decay channel where one tau decays into an electron and neutrinos, and the other decays into a charged pion, or three charged pions, and neutrinos. These samples are enhanced with decays into muons, which are inseparable in the ALICE detector. The clean environment of UPCs minimizes hadronic background, while the advanced particle identification capabilities of the ALICE Time Projection Chamber (TPC) and Time-of-Flight (TOF) systems allow for efficient separation of electrons, pions, and background particles. In this talk, prospects for measuring this process by ALICE in Run 3, which benefits from high statistics and improved systematics uncertainties, will be discussed. Results will provide tighter constraints on $a_{\tau}$, contributing to the broader effort to test the Standard Model's robustness and explore physics beyond it. |
| 2025-10-14 | [Moment-based Posterior Sampling for Multi-reference Alignment](http://arxiv.org/abs/2510.12651v1) | Axel Janson, Joakim And√©n | We propose a Bayesian approach to the problem of multi-reference alignment -- the recovery of signals from noisy, randomly shifted observations. While existing frequentist methods accurately recover the signal at arbitrarily low signal-to-noise ratios, they require a large number of samples to do so. In contrast, our proposed method leverages diffusion models as data-driven plug-and-play priors, conditioning these on the sample power spectrum (a shift-invariant statistic) enabling both accurate posterior sampling and uncertainty quantification. The use of an appropriate prior significantly reduces the required number of samples, as illustrated in simulation experiments with comparisons to state-of-the-art methods such as expectation--maximization and bispectrum inversion. These findings establish our approach as a promising framework for other orbit recovery problems, such as cryogenic electron microscopy (cryo-EM). |
| 2025-10-14 | [Designing Tools with Control Confidence](http://arxiv.org/abs/2510.12630v1) | Ajith Anil Meera, Abian Torres et al. | Prehistoric humans invented stone tools for specialized tasks by not just maximizing the tool's immediate goal-completion accuracy, but also increasing their confidence in the tool for later use under similar settings. This factor contributed to the increased robustness of the tool, i.e., the least performance deviations under environmental uncertainties. However, the current autonomous tool design frameworks solely rely on performance optimization, without considering the agent's confidence in tool use for repeated use. Here, we take a step towards filling this gap by i) defining an optimization framework for task-conditioned autonomous hand tool design for robots, where ii) we introduce a neuro-inspired control confidence term into the optimization routine that helps the agent to design tools with higher robustness. Through rigorous simulations using a robotic arm, we show that tools designed with control confidence as the objective function are more robust to environmental uncertainties during tool use than a pure accuracy-driven objective. We further show that adding control confidence to the objective function for tool design provides a balance between the robustness and goal accuracy of the designed tools under control perturbations. Finally, we show that our CMAES-based evolutionary optimization strategy for autonomous tool design outperforms other state-of-the-art optimizers by designing the optimal tool within the fewest iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence. |
| 2025-10-14 | [Learning-To-Measure: In-context Active Feature Acquisition](http://arxiv.org/abs/2510.12624v1) | Yuta Kobayashi, Zilin Jing et al. | Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness. |
| 2025-10-14 | [Enhancing Robust Multi-Market Participation of Renewable-Based VPPs through Flexible Resources](http://arxiv.org/abs/2510.12589v1) | Hadi Nemati, √Ålvaro Ortega et al. | In the transition toward a sustainable power system, renewable-based Virtual Power Plants (RVPPs) have emerged as a promising solution to the challenges of integrating renewable energy sources into electricity markets. Their viability, however, depends on effective market participation strategies and the ability to manage uncertainties while leveraging flexible resources. This paper analyzes the impact of different flexible resources - such as concentrated solar power plants, hydro plants, biomass plants, and flexible demand - on the participation of RVPPs in energy and reserve markets. Multiple sources of uncertainty in generation, consumption, and electricity prices are addressed using a two-stage robust optimization approach. The contribution of different technologies to RVPP profitability is evaluated through a marginal contribution method, ensuring fair allocation of profits among them according to their actual role in energy and reserve provision across markets. Simulations for an RVPP in southern Spain demonstrate how strategic decisions and the availability of flexible resources influence viability, market participation, and unit scheduling. |
| 2025-10-13 | [BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe](http://arxiv.org/abs/2510.11719v1) | M. Grayling, S. Thorp et al. | We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia supernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged, gravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for magnifications and time-delays across multiple images while marginalising over an achromatic, Gaussian process-based treatment of microlensing, to allow for time-dependent deviations from a typical SN Ia SED caused by gravitational lensing by stars in the lensing system. BayeSN-TD is able to robustly infer time delays and produce well-calibrated uncertainties, even when applied to simulations based on a different SED model and incorporating chromatic microlensing, strongly validating its suitability for time-delay cosmography. We then apply BayeSN-TD to publicly available photometry of the glSN Ia SN H0pe, inferring time delays between images BA and BC of $\Delta T_{BA}=121.9^{+9.5}_{-7.5}$ days and $\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days along with absolute magnifications $\beta$ for each image, $\beta_A = 2.38^{+0.72}_{-0.54}$, $\beta_B=5.27^{+1.25}_{-1.02}$ and $\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and magnifications with existing lens models of this system, we infer $H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous analysis of this system; incorporating additional constraints based on spectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While this is not yet precise enough to draw a meaningful conclusion with regard to the `Hubble tension', upcoming analysis of SN H0pe with more accurate photometry enabled by template images, and other glSNe, will provide stronger constraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses. |
| 2025-10-13 | [Bayesian Topological Convolutional Neural Nets](http://arxiv.org/abs/2510.11704v1) | Sarah Harkins Dayton, Hayden Everett et al. | Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification. |
| 2025-10-13 | [Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation](http://arxiv.org/abs/2510.11689v1) | Maggie Wang, Stephen Tian et al. | Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ . |
| 2025-10-13 | [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](http://arxiv.org/abs/2510.11654v1) | Daniel Berhane Araya, Duoduo Liao | Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches. |
| 2025-10-13 | [StatTestCalculator: A New General Tool for Statistical Analysis in High Energy Physics](http://arxiv.org/abs/2510.11637v1) | Emil Abasov, Lev Dudko et al. | We present StatTestCalculator (STC), a new open-source statistical analysis tool designed for analysis high energy physics experiments. STC provides both asymptotic calculations and Monte Carlo simulations for computing the exact statistical significance of a discovery or for setting upper limits on signal model parameters. We review the underlying statistical formalism, including profile likelihood ratio test statistics for discovery and exclusion hypotheses, and the asymptotic distributions that allow quick significance estimates. We explain the relevant formulas for the likelihood functions, test statistic distributions, and significance metrics (both with and without incorporating systematic uncertainties). The implementation and capabilities of STC are described, and we validate its performance against the widely-used CMS Combine tool. We find excellent agreement in both the expected discovery significances and upper limit calculations. STC is a flexible framework that can accommodate systematic uncertainties and user-defined statistical models, making it suitable for a broad range of analyses. |
| 2025-10-13 | [The Fractional Two-Sided Quaternionic Dunkl Transform and Heisenberg-Type Inequalities](http://arxiv.org/abs/2510.11597v1) | Mohamed Essenhajy | This report investigates the main definitions and fundamental properties of the fractional two-sided quaternionic Dunkl transform in two dimensions. We present key results concerning its structure and emphasize its connections to classical harmonic analysis. Special attention is given to inversion, boundedness, spectral behavior, and explicit formulas for structured functions such as radial or harmonic functions. Within this framework, we establish a generalized form of the classical Heisenberg-type uncertainty principle. Building on this foundation, we further extend the result by proving a higher-order Heisenberg-type inequality valid for arbitrary moments $p \geq 1$, with sharp constants characterized through generalized Hermite functions. Finally, by analyzing the interplay between the two-sided fractional quaternionic Dunkl transform and the two-sided fractional quaternionic Fourier transform, we derive a corresponding uncertainty principle for the latter. |
| 2025-10-13 | [Reproducibility: The New Frontier in AI Governance](http://arxiv.org/abs/2510.11595v1) | Israel Mason-Williams, Gabryel Mason-Williams | AI policymakers are responsible for delivering effective governance mechanisms that can provide safe, aligned and trustworthy AI development. However, the information environment offered to policymakers is characterised by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and creating deep uncertainty and divides on which risks should be prioritised from a governance perspective. We posit that the current publication speeds in AI combined with the lack of strong scientific standards, via weak reproducibility protocols, effectively erodes the power of policymakers to enact meaningful policy and governance protocols. Our paper outlines how AI research could adopt stricter reproducibility guidelines to assist governance endeavours and improve consensus on the AI risk landscape. We evaluate the forthcoming reproducibility crisis within AI research through the lens of crises in other scientific domains; providing a commentary on how adopting preregistration, increased statistical power and negative result publication reproducibility protocols can enable effective AI governance. While we maintain that AI governance must be reactive due to AI's significant societal implications we argue that policymakers and governments must consider reproducibility protocols as a core tool in the governance arsenal and demand higher standards for AI research. Code to replicate data and figures: https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance |
| 2025-10-13 | [Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization](http://arxiv.org/abs/2510.11539v1) | Denglin Cheng, Jiarong Kang et al. | Accurate state estimation is critical for legged and aerial robots operating in dynamic, uncertain environments. A key challenge lies in specifying process and measurement noise covariances, which are typically unknown or manually tuned. In this work, we introduce a bi-level optimization framework that jointly calibrates covariance matrices and kinematic parameters in an estimator-in-the-loop manner. The upper level treats noise covariances and model parameters as optimization variables, while the lower level executes a full-information estimator. Differentiating through the estimator allows direct optimization of trajectory-level objectives, resulting in accurate and consistent state estimates. We validate our approach on quadrupedal and humanoid robots, demonstrating significantly improved estimation accuracy and uncertainty calibration compared to hand-tuned baselines. Our method unifies state estimation, sensor, and kinematics calibration into a principled, data-driven framework applicable across diverse robotic platforms. |
| 2025-10-13 | [Characterizing planetary systems with SPIRou: questions about the magnetic cycle of 55 Cnc A and two new planets around B](http://arxiv.org/abs/2510.11523v1) | C. Moutou, P. Petit et al. | One of the first exoplanet hosts discovered thirty years ago, the star 55 Cnc has been constantly observed ever since. It is now known to host at least five planets with orbital periods ranging from 17 hours to 15 years. It is also one of the most extreme metal rich stars in the neighbourhood and it has a low-mass secondary star. In this article, we present data obtained at the Canada-France-Hawai'i Telescope with the SPIRou spectropolarimeter on both components of the 55 Cnc stellar system. We revisit the long-period radial-velocity signals of 55 Cnc A, with a focus on the role of the magnetic cycle, and propose the existence of a sixth planet candidate, whose period falls close to that of the magnetic cycle, or half of it. The other massive outer planet has a revised period of 13.15 years and a minimum mass of 3.8 MJup. Although some uncertainty remains on these outer planets, the characterization of the four inner planets is very robust through the combination of many different data sets, and all signals are consistent in the nIR and optical domains. In addition, the magnetic topology of the solar-type primary component of the system is observed by SPIRou at the minimum of its activity cycle, characterized by an amplitude ten times smaller than observed during its maximum in 2017. For the low-mass component 55 Cnc B, we report the discovery of two exoplanets in the system, with a period of 6.799+-0.0014 and 33.75+-0.04 days and a minimum mass of 3.5+-0.8 and 5.3+-1.4 MEarth, respectively. The secondary magnetic field is very weak and the current data set does not allow its precise characterization, setting an upper limit of 10 G. The system 55 Cnc stands out as the sixth binary system with planetary systems around both components, and the first one with non equal-mass stellar components. |
| 2025-10-13 | [Uncertainty Quantification for Retrieval-Augmented Reasoning](http://arxiv.org/abs/2510.11483v1) | Heydar Soudani, Hamed Zamani et al. | Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods. |
| 2025-10-10 | [Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference](http://arxiv.org/abs/2510.09574v1) | Daria de tinguy, Tim Verbelen et al. | Autonomous navigation in unfamiliar environments requires robots to simultaneously explore, localise, and plan under uncertainty, without relying on predefined maps or extensive training. We present a biologically inspired, Active Inference-based framework, Active Inference MAPping and Planning (AIMAPP). This model unifies mapping, localisation, and decision-making within a single generative model. Inspired by hippocampal navigation, it uses topological reasoning, place-cell encoding, and episodic memory to guide behaviour. The agent builds and updates a sparse topological map online, learns state transitions dynamically, and plans actions by minimising Expected Free Energy. This allows it to balance goal-directed and exploratory behaviours. We implemented a ROS-compatible navigation system that is sensor and robot-agnostic, capable of integrating with diverse hardware configurations. It operates in a fully self-supervised manner, is resilient to drift, and supports both exploration and goal-directed navigation without any pre-training. We demonstrate robust performance in large-scale real and simulated environments against state-of-the-art planning models, highlighting the system's adaptability to ambiguous observations, environmental changes, and sensor noise. The model offers a biologically inspired, modular solution to scalable, self-supervised navigation in unstructured settings. AIMAPP is available at https://github.com/decide-ugent/AIMAPP. |
| 2025-10-10 | [Yukawa interactions in Quantum Gravity](http://arxiv.org/abs/2510.09572v1) | Gustavo P. de Brito, Manuel Reichert et al. | We present the first complete next-to-leading-order analysis of a Yukawa system within the framework of asymptotically safe quantum gravity. Our results are obtained through a systematic resummation of higher-order operators, revealing two distinct resummation mechanisms -- one of which has not been explored previously. In addition, we introduce a novel approach to estimate systematic uncertainties by simulating the impact of neglected higher-order contributions.   We demonstrate that quantum gravity fluctuations anti-screen Yukawa interactions, thereby resolving previously inconclusive leading-order results. This anti-screening mechanism enables the generation of finite interactions from an asymptotically free Yukawa fixed point. Consequently, our findings provide strong evidence that non-vanishing Yukawa couplings are compatible with asymptotically safe quantum gravity, which is a necessary requirement for the Standard Model to emerge from an asymptotically safe ultraviolet completion. |
| 2025-10-10 | [Low-redshift constraints on structure growth from CMB lensing tomography](http://arxiv.org/abs/2510.09563v1) | Andrea Rubiola, Matteo Zennaro et al. | We present constraints on the amplitude of matter fluctuations from the clustering of galaxies and their cross-correlation with the gravitational lensing convergence of the cosmic microwave background (CMB), focusing on low redshifts ($z\lesssim0.3$), where potential deviations from a perfect cosmological constant dominating the growth of structure could be more prominent. Specifically, we make use of data from the 2MASS photometric survey (\tmpz) and the \wisc galaxy survey, in combination with CMB lensing data from \planck. Using a hybrid effective field theory (HEFT) approach to model galaxy bias we obtain constraints on the combination $S_8=\sigma_8\sqrt{\Omega_m/0.3}$, where $\sigma_8$ is the amplitude of matter fluctuations, and $\Omega_m$ is the non-relativistic matter fraction. Using a prior on $\Omega_m$ based on the baryon acoustic oscillation measurements of DESI, we find $S_8=0.79\pm0.06$, in reasonable agreement with CMB constraints. We also find that, in the absence of this prior, the data favours a value of $\Omega_m=0.245\pm0.024$, that is 2.8$\sigma$ lower than \planck. This result is driven by the broadband shape of the galaxy auto-correlation, and may be affected by theoretical uncertainties in the HEFT power spectrum templates. We further reconstruct the low-redshift growth history, finding it to be compatible with the \planck predictions, as well as existing constraints from lensing tomography. Finally, we study our constraints on the HEFT bias parameters of the galaxy samples studied, finding them to be in reasonable agreement with coevolution predictions. |
| 2025-10-10 | [A LENS on DUNE-PRISM: Characterizing a Neutrino Beam with Off-Axis Measurements](http://arxiv.org/abs/2510.09546v1) | Julia Gehrlein, Joachim Kopp et al. | Upcoming precision long-baseline neutrino oscillation experiments will be severely limited by the large systematic uncertainties associated with neutrino flux predictions and neutrino--nucleus cross sections. A promising remedy is the PRISM (Precision Reaction Independent Spectrum Measurement) technique, whereby the near detector measures the neutrino spectrum at different angles with respect to the beam axis. These measurements are then linearly combined into a prediction of the oscillated neutrino flux at the far detector. This prediction is data-driven, but still dependent on some theoretical knowledge about the neutrino flux. In this paper, we study to what extent off-axis measurements themselves can be used to directly constrain neutrino flux models. In particular, we use them to extract separately the fluxes and spectra of different meson species in the beam. We call this measurement LENS (Lateral Extraction of Neutrino Spectra). Second, we demonstrate how the thus improved flux model helps to further constrain the far detector flux prediction, thereby ultimately improving oscillation measurements. |
| 2025-10-10 | [Hierarchical Progressive Survey (HiPS) format: moving from visualisation to scientific analysis](http://arxiv.org/abs/2510.09533v1) | Fabrizio Giordano, Yago Ascasibar et al. | Context. In the current era of multi-wavelength and multi-messenger astronomy, international organisations are actively working on the definition of new standards for the publication of astronomical data, and substantial effort is devoted to make them available through public archives. Aims. We present a set of tools that allow user-friendly access and basic scientific analysis of observations in Hierarchical Progressive Survey (HiPS) format, and we use them to gauge the quality of representative skymaps at ultraviolet, optical, and infrared wavelengths. Methods. We apply a fully-automatic procedure to derive aperture photometry in 10 different bands for the 323 nearby galaxies in the Herschel Reference Sample (HRS), and compare its results with the rigorous analyses involving specialised knowledge and human intervention carried out by the HRS team. Results. Our experiment shows that 9 of the 10 skymaps considered preserve the original quality of the data, and the photometric fluxes returned by our pipeline are consistent with the HRS measurements within a few per cent. In the case of Herschel PACS maps at 100 {\mu}m, we uncovered a systematic error that we ascribe to an inconsistent combination of data products with different spatial resolution. For the remaining skymaps, the estimated statistical uncertainties provide a realistic indication of the differences with respect to the HRS catalogue. Conclusions. In principle, the currently available HiPS skymaps in Flexible Image Transport System (FITS) format allow to carry out broadband photometric analyses with an accuracy of the order of a few percent, but some level human intervention is still required. In addition to assessing data quality, we also propose a series of recommendations to realise the full potential of the HiPS format for the scientific analysis of large astronomical data sets. |
| 2025-10-10 | [Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study](http://arxiv.org/abs/2510.09498v1) | Rogier P. Krijnen, Akshay Joshi et al. | Fully capturing this behavior in traditional homogenized tissue testing requires the excitation of multiple deformation modes, i.e. combined triaxial shear tests and biaxial stretch tests. Inherently, such multimodal experimental protocols necessitate multiple tissue samples and extensive sample manipulations. Intrinsic inter-sample variability and manipulation-induced tissue damage might have an adverse effect on the inversely identified tissue behavior. In this work, we aim to overcome this gap by focusing our attention to the use of heterogeneous deformation profiles in a parameter estimation problem. More specifically, we adapt EUCLID, an unsupervised method for the automated discovery of constitutive models, towards the purpose of parameter identification for highly nonlinear, orthotropic constitutive models using a Bayesian inference approach and three-dimensional continuum elements. We showcase its strength to quantitatively infer, with varying noise levels, the material model parameters of synthetic myocardial tissue slabs from a single heterogeneous biaxial stretch test. This method shows good agreement with the ground-truth simulations and with corresponding credibility intervals. Our work highlights the potential for characterizing highly nonlinear and orthotropic material models from a single biaxial stretch test with uncertainty quantification. |
| 2025-10-10 | [FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents](http://arxiv.org/abs/2510.09483v1) | Lars Ohnemus, Nils Hantke et al. | Dynamic Scene Graphs (DSGs) provide a structured representation of hierarchical, interconnected environments, but current approaches struggle to capture stochastic dynamics, partial observability, and multi-agent activity. These aspects are critical for embodied AI, where agents must act under uncertainty and delayed perception. We introduce FOGMACHINE , an open-source framework that fuses DSGs with discrete-event simulation to model object dynamics, agent observations, and interactions at scale. This setup enables the study of uncertainty propagation, planning under limited perception, and emergent multi-agent behavior. Experiments in urban scenarios illustrate realistic temporal and spatial patterns while revealing the challenges of belief estimation under sparse observations. By combining structured representations with efficient simulation, FOGMACHINE establishes an effective tool for benchmarking, model training, and advancing embodied AI in complex, uncertain environments. |
| 2025-10-10 | [Failure Prediction at Runtime for Generative Robot Policies](http://arxiv.org/abs/2510.09459v1) | Ralf R√∂mer, Adrian Kobras et al. | Imitation learning (IL) with generative models, such as diffusion and flow matching, has enabled robots to perform complex, long-horizon tasks. However, distribution shifts from unseen environments or compounding action errors can still cause unpredictable and unsafe behavior, leading to task failure. Early failure prediction during runtime is therefore essential for deploying robots in human-centered and safety-critical environments. We propose FIPER, a general framework for Failure Prediction at Runtime for generative IL policies that does not require failure data. FIPER identifies two key indicators of impending failure: (i) out-of-distribution (OOD) observations detected via random network distillation in the policy's embedding space, and (ii) high uncertainty in generated actions measured by a novel action-chunk entropy score. Both failure prediction scores are calibrated using a small set of successful rollouts via conformal prediction. A failure alarm is triggered when both indicators, aggregated over short time windows, exceed their thresholds. We evaluate FIPER across five simulation and real-world environments involving diverse failure modes. Our results demonstrate that FIPER better distinguishes actual failures from benign OOD situations and predicts failures more accurately and earlier than existing methods. We thus consider this work an important step towards more interpretable and safer generative robot policies. Code, data and videos are available at https://tum-lsy.github.io/fiper_website. |
| 2025-10-10 | [Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior](http://arxiv.org/abs/2510.09401v1) | Matthew R. Williams, F. Hunter McGuire et al. | Parameter estimation and inference from complex survey samples typically focuses on global model parameters whose estimators have asymptotic properties, such as from fixed effects regression models. We present a motivating example of Bayesian inference for a multi-level or mixed effects model in which both the local parameters (e.g. group level random effects) and the global parameters may need to be adjusted for the complex sampling design. We evaluate the limitations of the survey-weighted pseudo-posterior and an existing automated post-processing method to incorporate the complex survey sample design for a wide variety of Bayesian models. We propose modifications to the automated process and demonstrate their improvements for multi-level models via a simulation study and a motivating example from the National Survey on Drug Use and Health. Reproduction examples are available from the authors and the updated R package is available via github:https://github.com/RyanHornby/csSampling |
| 2025-10-10 | [Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation](http://arxiv.org/abs/2510.09390v1) | Mert ƒ∞nan, Anthony Sicilia et al. | Establishing shared goals is a fundamental step in human-AI communication. However, ambiguities can lead to outputs that seem correct but fail to reflect the speaker's intent. In this paper, we explore this issue with a focus on the data visualization domain, where ambiguities in natural language impact the generation of code that visualizes data. The availability of multiple views on the contextual (e.g., the intended plot and the code rendering the plot) allows for a unique and comprehensive analysis of diverse ambiguity types. We develop a taxonomy of types of ambiguity that arise in this task and propose metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, we demonstrate that our ambiguity metrics better correlate with human annotations than uncertainty baselines. Our work also explores how multi-turn dialogue can reduce ambiguity, therefore, improve code accuracy by better matching user goals. We evaluate three pragmatic models to inform our dialogue strategies: Gricean Cooperativity, Discourse Representation Theory, and Questions under Discussion. A simulated user study reveals how pragmatic dialogues reduce ambiguity and enhance code accuracy, highlighting the value of multi-turn exchanges in code generation. |
| 2025-10-09 | [Scalable Offline Metrics for Autonomous Driving](http://arxiv.org/abs/2510.08571v1) | Animikh Aich, Adwait Kulkarni et al. | Real-World evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e., by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed. |
| 2025-10-09 | [Cleaning Galactic foregrounds with spatially varying spectral dependence from CMB observations with \texttt{fgbuster}](http://arxiv.org/abs/2510.08534v1) | Arianna Rizzieri, Cl√©ment Leloup et al. | In the context of maximum-likelihood parametric component separation for next-generation full-sky CMB polarization experiments, we study the impact of fitting different spectral parameters of Galactic foregrounds in distinct subsets of pixels on the sky, with the goal of optimizing the search for primordial B modes. Using both simulations and analytical arguments, we highlight how the post-component separation uncertainty and systematic foreground residuals in the cleaned CMB power spectrum depend on spatial variations in the spectral parameters. We show that allowing spectral parameters to vary across subsets of the sky pixels is essential to achieve competitive S/N on the reconstructed CMB after component separation while keeping residual foreground bias under control. Although several strategies exist to define pixel subsets for the spectral parameters, each with its advantages and limitations, we show using current foreground simulations in the context of next-generation space-borne missions that there are satisfactory configurations in which both statistical and systematic residuals become negligible. The exact magnitude of these residuals, however, depends on the mission's specific characteristics, especially its frequency coverage and sensitivity. We also show that the post-component separation statistical uncertainty is only weakly dependent on the properties of the foregrounds and propose a semi-analytical framework to estimate it. On the contrary, the systematic foreground residuals highly depend on both the properties of the foregrounds and the chosen spatial resolution of the spectral parameters. |
| 2025-10-09 | [Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression](http://arxiv.org/abs/2510.08512v1) | Nikolaos Stathoulopoulos, Christoforos Kanellakis et al. | Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans. |
| 2025-10-09 | [Constraining the new contributions to electron $g-2$ in a radiative neutrino mass model](http://arxiv.org/abs/2510.08504v1) | Bayu Dirgantara, J. Julio | We examine electron and muon anomalous magnetic dipole moments within a radiative neutrino mass model featuring TeV-scale scalar leptoquarks $S(3,1,-1/3)$ and $R(3,2,1/6)$. We utilize textures with decoupling electron and muon sectors, so that both electron and muon anomalous magnetic dipole moments could receive internal chiral enhancements from different heavy up-type quarks while in the same time evading the stringent $\mu\to e\gamma$ constraint. A successful fit to neutrino oscillation data requires the simultaneous presence of one- and two-loop neutrino mass contributions. This severely constrains the parameter space of the model, which results in a negligible new physics correction to the muon $g-2$. The electron $g-2$ discrepancy implied by the rubidium experiment, on the other hand, can be resolved within $2\sigma$ uncertainty provided that neutrino mass ordering is inverted. Lepton-flavor-violating tau decay rates, such as $\tau\to e\gamma$ and $\tau\to 3e$, are predicted to be within the sensitivities of next-generation experiments. |
| 2025-10-09 | [Diffusion-Based Probabilistic Modeling for Hourly Streamflow Prediction and Assimilation](http://arxiv.org/abs/2510.08488v1) | Wencong Yang, Haoyu Ji et al. | Hourly predictions are critical for issuing flood warnings as the flood peaks on the hourly scale can be distinctly higher than the corresponding daily ones. Currently a popular hourly data-driven prediction scheme is multi-time-scale long short-term memory (MTS-LSTM), yet such models face challenges in probabilistic forecasts or integrating observations when available. Diffusion artificial intelligence (AI) models represent a promising method to predict high-resolution information, e.g., hourly streamflow. Here we develop a denoising diffusion probabilistic model (h-Diffusion) for hourly streamflow prediction that conditions on either observed or simulated daily discharge from hydrologic models to generate hourly hydrographs. The model is benchmarked on the CAMELS hourly dataset against record-holding MTS-LSTM and multi-frequency LSTM (MF-LSTM) baselines. Results show that h-Diffusion outperforms baselines in terms of general performance and extreme metrics. Furthermore, the h-Diffusion model can utilize the inpainting technique and recent observations to accomplish data assimilation that largely improves flood forecasting performance. These advances can greatly reduce flood forecasting uncertainty and provide a unified probabilistic framework for downscaling, prediction, and data assimilation at the hourly scale, representing risks where daily models cannot. |
| 2025-10-09 | [Stochastic Volatility-in-mean VARs with Time-Varying Skewness](http://arxiv.org/abs/2510.08415v1) | Leonardo N. Ferreira, Haroon Mumtaz et al. | This paper introduces a Bayesian vector autoregression (BVAR) with stochastic volatility-in-mean and time-varying skewness. Unlike previous approaches, the proposed model allows both volatility and skewness to directly affect macroeconomic variables. We provide a Gibbs sampling algorithm for posterior inference and apply the model to quarterly data for the US and the UK. Empirical results show that skewness shocks have economically significant effects on output, inflation and spreads, often exceeding the impact of volatility shocks. In a pseudo-real-time forecasting exercise, the proposed model outperforms existing alternatives in many cases. Moreover, the model produces sharper measures of tail risk, revealing that standard stochastic volatility models tend to overstate uncertainty. These findings highlight the importance of incorporating time-varying skewness for capturing macro-financial risks and improving forecast performance. |
| 2025-10-09 | [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](http://arxiv.org/abs/2510.08389v1) | Rui Wang, Zeming Wei et al. | Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness. |
| 2025-10-09 | [The Two-Sided Clifford Dunkl Transform and Miyachi's Theorem](http://arxiv.org/abs/2510.08347v1) | Mohamed Essenhajy, Said Fahlaoui | Recent advances have extended the Dunkl transform to the setting of Clifford algebras. In particular, the two-sided quaternionic Dunkl transform has been introduced as a Dunkl analogue of the two-dimensional quaternionic Fourier transform. In this paper, we develop the two-sided Clifford Dunkl transform, defined using two square roots of -1 in Cl_{p,q}. We establish its fundamental properties, including the inversion and Plancherel formulas, and provide two explicit expressions for the associated translation operator. Moreover, we prove an analogue of Miyachi's theorem for this transform, thereby extending a classical result in harmonic analysis to the Clifford-Dunkl framework. |
| 2025-10-09 | [Design of chemical recycling processes for PUR foam under uncertainty](http://arxiv.org/abs/2510.08301v1) | Patrick Lotz, Luca Bosetti et al. | Optimization problems in chemical process design involve a significant number of discrete and continuous decisions. When taking into account uncertainties, the search space is very difficult to explore, even for experienced engineers. Moreover, it should be taken into account that while some decisions are fixed at the design stage, other parameters can be adapted to the realization of the uncertainty during the operation of the plant. This leads to a two-stage optimization problem which is difficult to solve. To address this challenge, we propose to combine commercial process simulation software with an evolutionary strategy. This approach is applied to designing a downstream process to isolate valuable products from pyrolysis oil produced by the catalytic pyrolysis of rigid polyurethane foam. The suggested algorithm consistently performed better than a manually designed robust process. Additionally, the analysis of different scenarios provided insight into promising changes in the overall layout of the recycling process. |
| 2025-10-09 | [Adversarial Thermodynamics](http://arxiv.org/abs/2510.08298v1) | Maite Arcos, Philippe Faist et al. | In thermodynamics, an agent's ability to extract work is fundamentally constrained by their environment. Traditional frameworks struggle to capture how strategic decision-making under uncertainty -- particularly an agent's tolerance for risk -- determines the trade-off between extractable work and probability of success in finite-scale experiments. Here, we develop a framework for non-equilibrium thermodynamics based on adversarial resource theories, in which work extraction is modelled as an adversarial game for an agent extracting work. Within this perspective, we recast the Szilard engine as a game isomorphic to Kelly gambling, an information-theoretic model of optimal betting under uncertainty -- but with a thermodynamic utility function. Extending the framework to finite-size regimes, we apply a risk-reward trade-off to find an interpretation of the Renyi-divergences, in terms of extractable work for a given failure probability. By incorporating risk sensitivity via utility functions, we show that the guaranteed amount of work a rational agent would accept instead of undertaking a risky protocol is given by a R\'enyi divergence. This provides a unified picture of thermodynamics and gambling, and highlights how generalized free energies emerge from an adversarial setup. |
| 2025-10-08 | [Probing evolution of Long GRB properties through their cosmic formation history aided by Machine Learning predicted redshifts](http://arxiv.org/abs/2510.07306v1) | Dhruv S. Bal, Aditya Narendra et al. | Gamma-ray Bursts (GRBs) are valuable probes of cosmic star formation reaching back into the epoch of reionization, and a large dataset with known redshifts ($z$) is an important ingredient for these studies. Usually, $z$ is measured using spectroscopy or photometry, but $\sim80\%$ of GRBs lack such data. Prompt and afterglow correlations can provide estimates in these cases, though they suffer from systematic uncertainties due to assumed cosmologies and due to detector threshold limits. We use a sample with $z$ estimated via machine learning models, based on prompt and afterglow parameters, without relying on cosmological assumptions. We then use an augmented sample of GRBs with measured and predicted redshifts, forming a larger dataset. We find that the predicted redshifts are a crucial step forward in understanding the evolution of GRB properties. We test three cases: no evolution, an evolution of the beaming factor, and an evolution of all terms captured by an evolution factor $(1+z)^\delta$. We find that these cases can explain the density rate in the redshift range between 1-2, but neither of the cases can explain the derived rate densities at smaller and higher redshifts, which may point towards an evolution term different than a simple power law. Another possibility is that this mismatch is due to the non-homogeneity of the sample, e.g., a non-collapsar origin of some long GRB within the sample. |
| 2025-10-08 | [On the Convergence of Moral Self-Correction in Large Language Models](http://arxiv.org/abs/2510.07290v1) | Guangliang Liu, Haitao Mao et al. | Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance. |
| 2025-10-08 | [Muonium HFS Uncertainty Revisited](http://arxiv.org/abs/2510.07281v1) | Michael I. Eides | Uncertainty of the quantum electrodynamics theoretical prediction for the hyperfine splitting in the ground state of muonium is considered. It is compared with the respective discussion in the two most recent CODATA adjustments of the fundamental physical constants. |
| 2025-10-08 | [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](http://arxiv.org/abs/2510.07210v1) | Donald Pfaffmann, Matthias Klusch et al. | We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proximal policy optimization and approximated online POMDP planning with heuristic confidence-based vertical pruning to reduce its execution time without compromising safety of driving. Our experimental performance analysis on the CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed that HyPlan may navigate safer than selected relevant baselines and perform significantly faster than considered alternative online POMDP planners. |
| 2025-10-08 | [A Broader View of Thompson Sampling](http://arxiv.org/abs/2510.07208v1) | Yanlin Qu, Hongseok Namkoong et al. | Thompson Sampling is one of the most widely used and studied bandit algorithms, known for its simple structure, low regret performance, and solid theoretical guarantees. Yet, in stark contrast to most other families of bandit algorithms, the exact mechanism through which posterior sampling (as introduced by Thompson) is able to "properly" balance exploration and exploitation, remains a mystery. In this paper we show that the core insight to address this question stems from recasting Thompson Sampling as an online optimization algorithm. To distill this, a key conceptual tool is introduced, which we refer to as "faithful" stationarization of the regret formulation. Essentially, the finite horizon dynamic optimization problem is converted into a stationary counterpart which "closely resembles" the original objective (in contrast, the classical infinite horizon discounted formulation, that leads to the Gittins index, alters the problem and objective in too significant a manner). The newly crafted time invariant objective can be studied using Bellman's principle which leads to a time invariant optimal policy. When viewed through this lens, Thompson Sampling admits a simple online optimization form that mimics the structure of the Bellman-optimal policy, and where greediness is regularized by a measure of residual uncertainty based on point-biserial correlation. This answers the question of how Thompson Sampling balances exploration-exploitation, and moreover, provides a principled framework to study and further improve Thompson's original idea. |
| 2025-10-08 | [Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks](http://arxiv.org/abs/2510.07199v1) | Shirin Shoushtari, Edward P. Chandler et al. | Poisson denoising plays a central role in photon-limited imaging applications such as microscopy, astronomy, and medical imaging. It is common to train deep learning models for denoising using the mean-squared error (MSE) loss, which corresponds to computing the posterior mean $\mathbb{E}[x \mid y]$. When the noise is Gaussian, Tweedie's formula enables approximation of the posterior distribution through its higher-order moments. However, this connection no longer holds for Poisson denoising: while $ \mathbb{E}[x \mid y] $ still minimizes MSE, it fails to capture posterior uncertainty. We propose a new strategy for Poisson denoising based on training a log-network. Instead of predicting the posterior mean $ \mathbb{E}[x \mid y] $, the log-network is trained to learn $\mathbb{E}[\log x \mid y]$, leveraging the logarithm as a convenient parameterization for the Poisson distribution. We provide a theoretical proof that the proposed log-network enables recovery of higher-order posterior moments and thus supports posterior approximation. Experiments on simulated data show that our method matches the denoising performance of standard MMSE models while providing access to the posterior. |
| 2025-10-08 | [Bayesian Portfolio Optimization by Predictive Synthesis](http://arxiv.org/abs/2510.07180v1) | Masahiro Kato, Kentaro Baba et al. | Portfolio optimization is a critical task in investment. Most existing portfolio optimization methods require information on the distribution of returns of the assets that make up the portfolio. However, such distribution information is usually unknown to investors. Various methods have been proposed to estimate distribution information, but their accuracy greatly depends on the uncertainty of the financial markets. Due to this uncertainty, a model that could well predict the distribution information at one point in time may perform less accurately compared to another model at a different time. To solve this problem, we investigate a method for portfolio optimization based on Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for meta-learning. We assume that investors have access to multiple asset return prediction models. By using BPS with dynamic linear models to combine these predictions, we can obtain a Bayesian predictive posterior about the mean rewards of assets that accommodate the uncertainty of the financial markets. In this study, we examine how to construct mean-variance portfolios and quantile-based portfolios based on the predicted distribution information. |
| 2025-10-08 | [Optimal network pricing with oblivious users: a new model and algorithm](http://arxiv.org/abs/2510.07157v1) | Yixuan Li, Andersen Ang et al. | Traffic modeling is important in modern society. In this work we propose a new model on the optimal network pricing (Onp) with the assumption of oblivious users, in which the users remain oblivious to real-time traffic conditions and others' behavior. Inspired by works on transportation research and network pricing for selfish traffic, we mathematically derive and prove a new formulation of Onp with decision-dependent modeling that relax certain existing modeling constraints in the literature. Then, we express the Onp formulation as a constrained nonconvex stochastic quadratic program with uncertainty, and we propose an efficient algorithm to solve the problem, utilizing graph theory, sparse linear algebra and stochastic approximation. Lastly, we showcase the effectiveness of the proposed algorithm and the usefulness of the new Onp formulation. The proposed algorithm achieves a 5x speedup by exploiting the sparsity structure of the model. |
| 2025-10-08 | [CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized Cluster Strong Lens Modeling](http://arxiv.org/abs/2510.07131v1) | Yushan Xie, Huanyuan Shan et al. | Strongly lensed supernovae (glSNe) provide a powerful, independent method to measure the Hubble constant, $H_{0}$, through time delays between their multiple images. The accuracy of this measurement depends critically on both the precision of time delay estimation and the robustness of lens modeling. In many current cluster-scale modeling algorithms, all multiple images used for modeling are simplified as point sources to reduce computational costs. In the first paper of the CURLING program, we demonstrated that such a point-like approximation can introduce significant uncertainties and biases in both magnification reconstruction and cosmological inference. In this study, we explore how such simplifications affect $H_0$ measurements from glSNe. We simulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at $z=0.336$, assuming time delays are measured from LSST-like light curves. The lens model is constructed using JWST-like imaging data, utilizing both Lenstool and a pixelated method developed in CURLING. Under a fiducial cosmology with $H_0=70\rm \ km \ s^{-1}\ Mpc^{-1}$, the Lenstool model yields $H_0=69.91^{+6.27}_{-5.50}\rm \ km\ s^{-1}\ Mpc^{-1}$, whereas the pixelated framework improves the precision by over an order of magnitude, $H_0=70.39^{+0.82}_{-0.60}\rm \ km \ s^{-1}\ Mpc^{-1}$. Our results indicate that in the next-generation observations (e.g., JWST), uncertainties from lens modeling dominate the error budget for $H_0$ inference, emphasizing the importance of incorporating the extended surface brightness of multiple images to fully leverage the potential of glSNe for cosmology. |
| 2025-10-08 | [Mitigating Increase-Decrease Gaming with Alternative Connection Agreements: A Defender-Attacker-Defender Game](http://arxiv.org/abs/2510.07102v1) | Bart van der Holst, Thomas Swarts et al. | Redispatch markets are widely used by system operators to manage network congestion. A well-known drawback, however, is that Flexibility Service Providers (FSPs) may strategically adjust their baselines in anticipation of redispatch actions, thereby aggravating congestion and raising system costs. To address this increase-decrease gaming, Distribution System Operators (DSOs) could use Alternative Connection Agreements (ACAs) to conditionally limit the available connection capacity of market participants in the day-ahead stage. In this paper, we present a novel Defender-Attacker-Defender game to investigate the potential of this approach in distribution networks under load and price uncertainty. We solve the resulting trilevel optimization model using a custom branch-and-bound algorithm, and we demonstrate that it efficiently solves the problem without exploring many nodes in the branch-and-bound search tree for most simulated scenarios. The case study demonstrates that applying ACAs can substantially lower redispatch costs (e.g. by 25%) for the DSO with only a limited impact on FSP profits. The effectiveness of the approach critically depends on how often the DSO can invoke ACAs and on the extent to which the DSO can anticipate strategic bidding behavior of the FSP. |
| 2025-10-07 | [Studying the gravitational-wave population without looking that FAR out](http://arxiv.org/abs/2510.06220v1) | Noah E. Wolfe, Matthew Mould et al. | From catalogs of gravitational-wave transients, the population-level properties of their sources and the formation channels of merging compact binaries can be constrained. However, astrophysical conclusions can be biased by misspecification or misestimation of the population likelihood. Despite detection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio (SNR), the current catalog is likely contaminated by noise transients. Further, computing the population likelihood becomes less accurate as the catalog grows. Current methods to address these challenges often scale poorly with the number of events and potentially become infeasible for future catalogs. Here, we evaluate a simple remedy: increasing the significance threshold for including events in population analyses. To determine the efficacy of this approach, we analyze simulated catalogs of up to 1600 gravitational-wave signals from black-hole mergers using full Bayesian parameter estimation with current detector sensitivities. We show that the growth in statistical uncertainty about the black-hole population, as we analyze fewer events but with higher SNR, depends on the source parameters of interest. When the SNR threshold is raised from 11 to 15 -- reducing our catalog size by two--thirds -- we find that statistical uncertainties on the mass distribution only grow by a few 10% and constraints on the spin distribution are essentially unchanged; meanwhile, uncertainties on the high-redshift cosmic merger rate more than double. Simultaneously, numerical uncertainty in the estimate of the population likelihood more than halves, allowing us to ensure unbiased inference without additional computational expense. Our results demonstrate that focusing on higher-significance events is an effective way to facilitate robust astrophysical inference with growing gravitational-wave catalogs. |
| 2025-10-07 | [Conformalized Gaussian processes for online uncertainty quantification over graphs](http://arxiv.org/abs/2510.06181v1) | Jinwen Xu, Qin Lu et al. | Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP. |
| 2025-10-07 | [Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques](http://arxiv.org/abs/2510.06136v1) | Jieyun Wang, Anna L. Smith | Latent space models assume that network ties are more likely between nodes that are closer together in an underlying latent space. Euclidean space is a popular choice for the underlying geometry, but hyperbolic geometry can mimic more realistic patterns of ties in complex networks. To identify the underlying geometry, past research has applied non-Euclidean extensions of multidimensional scaling (MDS) to the observed geodesic distances: the shortest path lengths between nodes. The difference in stress, a standard goodness-of-fit metric for MDS, across the geometries is then used to select a latent geometry with superior model fit (lower stress). The effectiveness of this method is assessed through simulations of latent space networks in Euclidean and hyperbolic geometries. To better account for uncertainty, we extend permutation-based hypothesis tests for MDS to the latent network setting. However, these tests do not incorporate any network structure. We propose a parametric bootstrap distribution of networks, conditioned on observed geodesic distances and the Gaussian Latent Position Model (GLPM). Our method extends the Davidson-MacKinnon J-test to latent space network models with differing latent geometries. We pay particular attention to large and sparse networks, and both the permutation test and the bootstrapping methods show an improvement in detecting the underlying geometry. |
| 2025-10-07 | [Multiprobe constraints on early and late time dark energy](http://arxiv.org/abs/2510.06114v1) | Alexander Reeves, Simone Ferraro et al. | We perform a multiprobe analysis combining cosmic microwave background (CMB) data from Planck and the Atacama Cosmology Telescope (ACT), ACT CMB lensing, and large-scale structure (LSS) measurements from the Dark Energy Spectroscopic Instrument (DESI), including DESI Legacy Imaging Survey (LS) galaxies and baryon acoustic oscillations (BAOs). We present the first $5\times2$pt analysis of ACT DR6 lensing, DESI LS, and Planck ISW. Within $\Lambda$CDM, this yields $S_8 = \sigma_8(\Omega_m/0.3)^{0.5} = 0.819 \pm 0.016$, in good agreement with primary CMB inferences and provides a sound-horizon-free Hubble constant constraint of $H_0 = 70.0 \pm 4.4$ km s$^{-1}$ Mpc$^{-1}$. Then, combining with CMB primary and BAO, we reconfirm a CMB-BAO discrepancy in the $\Omega_m$-$\frac{D_v}{r_d}$ plane, which is heightened when combining BAO with the $5\times2$pt data vector. We explore two dark-energy extensions that may reconcile this: an early-time modification, early dark energy (EDE), and late-time dynamical dark energy (DDE) parameterized by $w_0w_a$. For CMB primary+BAO+$5\times2$pt, we find a $3.3\sigma$ preference for DDE over $\Lambda$CDM, while EDE is modestly favoured at $2.3\sigma$. The models address different shortcomings of $\Lambda$CDM: DDE relaxes the neutrino mass bound ($M_\nu<0.17$eV vs. $<0.050$eV under $\Lambda$CDM), making it compatible with neutrino oscillation measurements, while EDE raises the Hubble constant to $H_0=70.5\pm1.2\,\mathrm{km\,s^{-1}\,Mpc^{-1}}$, easing the discrepancy with SH0ES. However, neither model resolves both issues simultaneously. Our analysis indicates that both DDE and EDE remain viable extensions of $\Lambda$CDM within current uncertainties and demonstrates the capacity of combined probes to place increasingly stringent constraints on cosmological parameters. |
| 2025-10-07 | [Mass loading of outflows from evolving Young Massive Clusters](http://arxiv.org/abs/2510.06100v1) | C. J. K. Larkin, C. Hawcroft et al. | Feedback from Young Massive Clusters (YMCs) is an important driver of galaxy evolution. In the first few Myr, mechanical feedback is dominated by collective effects of the massive stellar winds in the YMC. The mass-loss rates and terminal wind velocities of these stars change by orders of magnitude over pre-SN timescales as the massive stars evolve, and mass-loss rates of Cool Supergiant (CSG) stars in particular are uncertain by a factor $\sim~20$ or more. In this work we perform a first study of the time evolution of average cluster wind velocity $\bar{V}_{\mathrm{cl}}$ as a function of stellar metallicity $Z$, assuming single star evolution. We also check the validity of assuming Wolf-Rayet stars dominate the feedback effects of a YMC, as often done when interpreting X-ray and $\gamma$-ray observations, and test how sensitive $\bar{V}_{\mathrm{cl}}$ is to current uncertainties in mass-loss rates. We use pySTARBURST99 to calculate integrated properties of YMCs for $Z$ in the range of $0.0004-0.02$, representing a range of environments from IZw18 to the Galactic Centre. We find that $\bar{V}_{\mathrm{cl}}$ drops off rapidly for sub-LMC $Z$, and we recommend a value of $500-1000\,~\textrm{km~s}^{-1}$ be used in this regime. We show accounting only for WR stars can overestimate $\bar{V}_{\mathrm{cl}}$ by $500-2000\,~\textrm{km~s}^{-1}$ at $Z \geq Z_\text{LMC}$. We also find that different RSG mass-loss assumptions can change the inferred $\bar{V}_{\mathrm{cl}}$ by $\sim1000\,~\textrm{km~s}^{-1}$, highlighting the need for improved observational constraints for RSGs in YMCs. |
| 2025-10-07 | [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](http://arxiv.org/abs/2510.06096v1) | Matthieu Bou, Nyal Patel et al. | The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI. |
| 2025-10-07 | [Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data](http://arxiv.org/abs/2510.06080v1) | Nga Nguyen, Olivier Bonnefon et al. | Biological control strategies against mosquito-borne diseases--such as the sterile insect technique (SIT), RIDL, and Wolbachia-based releases--require reliable estimates of dispersal and survival of released males. We propose a mechanistic--statistical framework for mark--release--recapture (MRR) data linking an individual-based 2D diffusion model with its reaction--diffusion limit. Inference is based on solving the macroscopic system and embedding it in a Poisson observation model for daily trap counts, with uncertainty quantified via a parametric bootstrap. We validate identifiability using simulated data and apply the model to an urban MRR campaign in El Cano (Havana, Cuba) involving four weekly releases of sterile Aedes aegypti males. The best-supported model suggests a mean life expectancy of about five days and a typical displacement of about 180 m. Unlike empirical fits of survival or dispersal, our mechanistic approach jointly estimates movement, mortality, and capture, yielding biologically interpretable parameters and a principled framework for designing and evaluating SIT-based interventions. |
| 2025-10-07 | [Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition](http://arxiv.org/abs/2510.06033v1) | Jim Dai, Manxi Wu et al. | Stochastic processing networks (SPNs) have broad applications in healthcare, transportation, and communication networks. The control of SPN is to dynamically assign servers in batches under uncertainty to optimize long-run performance. This problem is challenging as the policy dimension grows exponentially with the number of servers, making standard reinforcement learning and policy optimization methods intractable at scale. We propose an atomic action decomposition framework that addresses this scalability challenge by breaking joint assignments into sequential single-server assignments. This yields policies with constant dimension, independent of the number of servers. We study two classes of atomic policies, the step-dependent and step-independent atomic policies, and prove that both achieve the same optimal long-run average reward as the original joint policies. These results establish that computing the optimal SPN control can be made scalable without loss of optimality using the atomic framework. Our results offer theoretical justification for the strong empirical success of the atomic framework in large-scale applications reported in previous articles. |
| 2025-10-07 | [Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers](http://arxiv.org/abs/2510.06025v1) | Kevin Raina, Tanya Schmah | Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST and CIFAR-10 In-Distributions, with 5000 training samples or less, show that the Bayesian methods outperform corresponding deterministic methods. |
| 2025-10-07 | [Uncertainty in Machine Learning](http://arxiv.org/abs/2510.06007v1) | Hans Weytjens, Wouter Verbeke | This book chapter introduces the principles and practical applications of uncertainty quantification in machine learning. It explains how to identify and distinguish between different types of uncertainty and presents methods for quantifying uncertainty in predictive models, including linear regression, random forests, and neural networks. The chapter also covers conformal prediction as a framework for generating predictions with predefined confidence intervals. Finally, it explores how uncertainty estimation can be leveraged to improve business decision-making, enhance model reliability, and support risk-aware strategies. |
| 2025-10-06 | [Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A Particle-Tracking Study](http://arxiv.org/abs/2510.05084v1) | Ethan Kahn | Electrospray thrusters are emerging as a leading propulsion technology for CubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power requirements. However, the divergent ion plumes can impinge on spacecraft surfaces, particularly body-mounted solar arrays, causing contamination and thrust efficiency losses. This study presents a validated particle-tracking simulation to quantify the effects of thruster placement on thrust efficiency and surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs a cosine power distribution ($k=1.8$) with half-angle $46^\circ$, validated against experimental data with errors below 7%. Results show that thrust efficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted configurations to 100% for side-mounted configurations with deployable arrays. CubeSat size significantly affects impingement: 3U platforms experience 46.4% contamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable solar arrays reduce contamination by 77% compared to body-mounted arrays, while side-mounted thrusters eliminate impingement entirely at the cost of only 1.6% efficiency loss. Corner-mounted configurations at $30^\circ$ cant provide intermediate performance with 88.9% efficiency and 11.1% contamination. These quantitative design guidelines enable mission planners to optimize thruster integration based on power budget and propellant mass constraints, with statistical uncertainty below 0.15% across all configurations. |
| 2025-10-06 | [The role of entropy production and thermodynamic uncertainty relations in the thermalization of open quantum systems](http://arxiv.org/abs/2510.05072v1) | √Ålvaro Tejero | The asymmetry between heating and cooling in open quantum systems is a hallmark of nonequilibrium dynamics, yet its thermodynamic origin has remained unclear. Here, we investigate the thermalization of a quantum system weakly coupled to a thermal bath, focusing on the entropy production rate and the quantum thermokinetic uncertainty relation (TKUR). We derive an analytical expression for the entropy production rate, showing that heating begins with a higher entropy production, which drives faster thermalization than cooling. The quantum TKUR links this asymmetry to heat current fluctuations, demonstrating that larger entropy production suppresses fluctuations, making heating more stable than cooling. Our results reveal the thermodynamic basis of asymmetric thermalization and highlight uncertainty relations as key to nonequilibrium quantum dynamics. |
| 2025-10-06 | [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](http://arxiv.org/abs/2510.05054v1) | Peter Van Katwyk, Karianne J. Bergen | Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework. |
| 2025-10-06 | [Inferring the spins of merging black holes in the presence of data-quality issues](http://arxiv.org/abs/2510.05029v1) | Rhiannon Udall, Sophie Bini et al. | Gravitational waves from black hole binary mergers carry information about the component spins, but inference is sensitive to analysis assumptions, which may be broken by terrestrial noise transients known as glitches. Using a variety of simulated glitches and gravitational wave signals, we study the conditions under which glitches can bias spin measurements. We confirm the theoretical expectation that inference and subtraction of glitches invariably leaves behind residual power due to statistical uncertainty, no matter the strength (signal-to-noise ratio; SNR) of the original glitch. Next we show that low-SNR glitches - including those below the threshold for flagging data-quality issues - can still significantly bias spin inference. Such biases occur for a range of glitch morphologies, even in cases where glitches and signals are not precisely aligned in phase. Furthermore, we find that residuals of glitch subtraction can result in biases as well. Our results suggest that joint inference of the glitch and gravitational wave parameters, with appropriate models and priors, is required to address these uncertainties inherent in glitch mitigation via subtraction. |
| 2025-10-06 | [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](http://arxiv.org/abs/2510.05027v1) | Ethan Davis | We introduce a framework for applying metaheuristic algorithms, such as ant colony optimization (ACO), to combinatorial optimization problems (COPs) like the traveling salesman problem (TSP). The framework consists of three sequential stages: broad exploration of the parameter space, exploitation of top-performing parameters, and uncertainty quantification (UQ) to assess the reliability of results. As a case study, we apply ACO to the TSPLIB berlin52 dataset, which has a known optimal tour length of 7542. Using our framework, we calculate that the probability of ACO finding the global optimum is approximately 1/40 in a single run and improves to 1/5 when aggregated over ten runs. |
| 2025-10-06 | [Risk-Adjusted Policy Learning and the Social Cost of Uncertainty: Theory and Evidence from CAP evaluation](http://arxiv.org/abs/2510.05007v1) | Giovanni Cerulli, Francesco Caracciolo | This paper develops a risk-adjusted alternative to standard optimal policy learning (OPL) for observational data by importing Roy's (1952) safety-first principle into the treatment assignment problem. We formalize a welfare functional that maximizes the probability that outcomes exceed a socially required threshold and show that the associated pointwise optimal rule ranks treatments by the ratio of conditional means to conditional standard deviations. We implement the framework using microdata from the Italian Farm Accountancy Data Network to evaluate the allocation of subsidies under the EU Common Agricultural Policy. Empirically, risk-adjusted optimal policies systematically dominate the realized allocation across specifications, while risk aversion lowers overall welfare relative to the risk-neutral benchmark, making transparent the social cost of insurance against uncertainty. The results illustrate how safety-first OPL provides an implementable, interpretable tool for risk-sensitive policy design, quantifying the efficiency-insurance trade-off that policymakers face when outcomes are volatile. |
| 2025-10-06 | [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](http://arxiv.org/abs/2510.05006v1) | Koen Vellenga, H. Joe Steinhauer et al. | Deep neural networks (DNNs) are increasingly applied to safety-critical tasks in resource-constrained environments, such as video-based driver action and intention recognition. While last layer probabilistic deep learning (LL-PDL) methods can detect out-of-distribution (OOD) instances, their performance varies. As an alternative to last layer approaches, we propose extending pre-trained DNNs with transformation layers to produce multiple latent representations to estimate the uncertainty. We evaluate our latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) approaches against eight PDL methods across four video-based driver action and intention recognition datasets, comparing classification performance, calibration, and uncertainty-based OOD detection. We also contribute 28,000 frame-level action labels and 1,194 video-level intention labels for the NuScenes dataset. Our results show that LUR and RLUR achieve comparable in-distribution classification performance to other LL-PDL approaches. For uncertainty-based OOD detection, LUR matches top-performing PDL methods while being more efficient to train and easier to tune than approaches that require Markov-Chain Monte Carlo sampling or repulsive training procedures. |
| 2025-10-06 | [Doubly Discordant SH0ES NGC4258 Cepheid Relations (HVI), and Impactful Extinction Laws](http://arxiv.org/abs/2510.04998v1) | Daniel Majaess | S$H_0$ES 2016-2022 $HVI$ data for classical Cepheids in the keystone galaxy NGC4258 yield doubly discordant Wesenheit Leavitt functions:~$\Delta W_{0,H-VI} = -0.13\pm0^{m}.02$ ($-0^{m}.17$ unweighted) and that is paired with a previously noted $\Delta W_{0,I-VI}\simeq-0^{m}.3$, which in concert with complimentary evidence suggest the 2016 S$H_0$ES NGC4258-anchored $H_0 \pm \sigma_{H_0}$ warrants scrutiny (i.e., $\sigma_{H_0}/{H_0}\gtrsim 6$\%). Cepheid distance uncertainties are further exacerbated by extinction law ambiguities endemic to such Leavitt relations (e.g., NGC4258), particularly for comparatively obscured variables (e.g., $\Delta d \gtrsim 4$\%, reddened Cepheid subsamples in the Milky Way, M31, NGC2442, NGC4424, NGC5643, NGC7250). Lastly, during the analysis it was identified that the 2022 S$H_0$ES database relays incorrect SMC Cepheid photometry. |
| 2025-10-06 | [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](http://arxiv.org/abs/2510.04996v1) | Wei Xiong, Chenlu Ye et al. | Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada. |
| 2025-10-06 | [Line emission search from DM annihilation in the Galactic Center with LST-1](http://arxiv.org/abs/2510.04977v1) | Abhishek Abhishek, Shotaro Abe et al. | Dark Matter remains a great mystery in modern physics. Among various candidates, the weakly interacting massive particles (WIMPs) scenario stands out and is under extensive study. The detection of the hypothetical gamma-ray emission from WIMP annihilation could act as a direct probe of electroweak-scale interactions, complementing DM collider searches and other direct DM detection techniques. At very high energies (VHE), WIMP self-annihilation is expected to produce gamma rays together with other Standard Model particles. The galactic center (GC), due to its relative proximity to the Earth and its high expected DM density, is a prime target for monoenergetic line searches. IACTs have placed strong constraints on the DM properties at the GC, with the MAGIC providing the most stringent limits from 20 TeV to 100 TeV, exploiting large zenith angle (LZA) observations. However, the limited field of view (FoV) of the MAGIC telescopes (< 3.5{\deg} ) prevented a detailed study of the extended region around the GC in which an enhanced DM density is expected. The LST-1 of the CTAO, located at the Roque de Los Muchachos Observatory (La Palma, Spain), close to the MAGIC site, has been observing the GC since 2021. With its wide FoV of 4.5{\deg}, LST-1 could contribute significantly to the WIMPs search at the GC. The observations are performed at LZA (ZA > 58{\deg}), which, while required due to the source's low altitude, also optimizes the detection of gamma rays up to 100 TeV and beyond. We present a study of the systematic uncertainties in WIMP line emission searches with LST-1. Our work examines the instrument response functions for LZA observations, background rejection in monoscopic mode, and includes updated results from simulations, highlighting new methods for spectral line searches. |
| 2025-10-03 | [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](http://arxiv.org/abs/2510.03181v1) | Ha Manh Bui, Felix Parker et al. | We study the Non-Stationary Reinforcement Learning (RL) under distribution shifts in both finite-horizon episodic and infinite-horizon discounted Markov Decision Processes (MDPs). In the finite-horizon case, the transition functions may suddenly change at a particular episode. In the infinite-horizon setting, such changes can occur at an arbitrary time step during the agent's interaction with the environment. While the Q-learning Upper Confidence Bound algorithm (QUCB) can discover a proper policy during learning, due to the distribution shifts, this policy can exploit sub-optimal rewards after the shift happens. To address this issue, we propose Density-QUCB (DQUCB), a shift-aware Q-learning~UCB algorithm, which uses a transition density function to detect distribution shifts, then leverages its likelihood to enhance the uncertainty estimation quality of Q-learning~UCB, resulting in a balance between exploration and exploitation. Theoretically, we prove that our oracle DQUCB achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the computational efficiency of model-free RL and outperforms QUCB baselines by having a lower regret across RL tasks, as well as a real-world COVID-19 patient hospital allocation task using a Deep-Q-learning architecture. |
| 2025-10-03 | [Calibrated Uncertainty Sampling for Active Learning](http://arxiv.org/abs/2510.03162v1) | Ha Manh Bui, Iliana Maifeld-Carucci et al. | We study the problem of actively learning a classifier with a low calibration error. One of the most popular Acquisition Functions (AFs) in pool-based Active Learning (AL) is querying by the model's uncertainty. However, we recognize that an uncalibrated uncertainty model on the unlabeled pool may significantly affect the AF effectiveness, leading to sub-optimal generalization and high calibration error on unseen data. Deep Neural Networks (DNNs) make it even worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty. Specifically, we utilize a kernel calibration error estimator under the covariate shift and formally show that AL with this AF eventually leads to a bounded calibration error on the unlabeled pool and unseen test data. Empirically, our proposed method surpasses other AF baselines by having a lower calibration and generalization error across pool-based AL settings. |
| 2025-10-03 | [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](http://arxiv.org/abs/2510.03155v1) | Stevens Johnson, Varun Puram et al. | Accurate modeling of neuronal action potential (AP) onset timing is crucial for understanding neural coding of danger signals. Traditional leaky integrate-and-fire (LIF) models, while widely used, exhibit high relative error in predicting AP onset latency, especially under strong or rapidly changing stimuli. Inspired by recent experimental findings and quantum theory, we present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats AP onset as a probabilistic event, represented by a Gaussian wave packet in time. This approach captures the biological variability and uncertainty inherent in neuronal firing. We systematically compare the relative error of AP onset predictions between the classical LIF and QI-LIF models using synthetic data from hippocampal and sensory neurons subjected to varying stimulus amplitudes. Our results demonstrate that the QI-LIF model significantly reduces prediction error, particularly for high-intensity stimuli, aligning closely with observed biological responses. This work highlights the potential of quantum-inspired computational frameworks in advancing the accuracy of neural modeling and has implications for quantum engineering approaches to brain-inspired computing. |
| 2025-10-03 | [Statistical framework for nuclear parameter uncertainties in nucleosynthesis modeling of r- and i-process](http://arxiv.org/abs/2510.03138v1) | S. Martinet, G. Goriely et al. | Propagating nuclear uncertainties to nucleosynthesis simulations is key to understand the impact of theoretical uncertainties on the predictions, especially for processes far from the stability region, where nuclear properties are scarcely known. While systematic (model) uncertainties have been thoroughly studied, the statistical (parameter) ones have been more rarely explored, as constraining them is more challenging. We present here a methodology to determine coherently parameter uncertainties by anchoring the theoretical uncertainties to the experimentally known nuclear properties through the use of the Backward Forward Monte Carlo method. We use this methodology for two nucleosynthesis processes: the intermediate neutron capture process (i-process) and the rapid neutron capture process (r-process). We determine coherently for the i-process the uncertainties from the (n,$\gamma$) rates while we explore the impact of nuclear mass uncertainties for the r-process. The effect of parameter uncertainties on the final nucleosynthesis is in the same order as model uncertainties, suggesting the crucial need for more experimental constraints on key nuclei of interest. We show how key nuclear properties, such as relevant (n,$\gamma$) rates impacting the i-process tracers, could enhance tremendously the prediction of stellar evolution models by experimentally constraining them. |
| 2025-10-03 | [A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control](http://arxiv.org/abs/2510.03100v1) | Tianhua Gao | Quadrotor stability under complex dynamic disturbances and model uncertainties poses significant challenges. One of them remains the underfitting problem in high-dimensional features, which limits the identification capability of current learning-based methods. To address this, we introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from which we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for geometric control. Specifically, the high-dimensional mapping for identification is axially ``sliced" into multiple low-dimensional submappings (``slices"). In this way, the complex high-dimensional problem is decomposed into a set of simple low-dimensional tasks addressed by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation without any pre-training or persistent excitation (PE) condition. To enhance the interpretability of the proposed approach, we prove that the full-state closed-loop system exhibits arbitrarily close to exponential stability despite multi-dimensional time-varying disturbances and model uncertainties. This result is novel as it demonstrates exponential convergence without requiring pre-training for unknown disturbances and specific knowledge of the model. |
| 2025-10-03 | [Polarization dependence of spin-electric transitions in molecular exchange qubits](http://arxiv.org/abs/2510.03099v1) | Filippo Troiani, Athanassios K. Boudalis | Quasi-optical experiments are emerging as a powerful technique to probe magnetic transitions in molecular spin systems. However, the simultaneous presence of the electric- and magnetic-dipole induced transitions poses the challenge of discriminating between these two contributions. Besides, the identification of the spin-electric transitions can hardly rely on the peak intensity, because of the current uncertainties on the value of the spin-electric coupling in most molecular compounds. Here, we compute the polarizations required for electric- and magnetic-dipole induced transitions through spin-Hamiltonian models of molecular spin triangles. We show that the polarization allows a clear discrimination between the two kinds of transitions. In addition, it allows one to identify the physical origin of the zero-field splitting in the ground multiplet, a debated issue with significant implications on the coherence properties of the spin qubit implemented in molecular spin triangles. |
| 2025-10-03 | [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](http://arxiv.org/abs/2510.03046v1) | Soohaeng Yoo Willow, Tae Hyeon Park et al. | Machine learning potentials (MLPs) have become essential for large-scale atomistic simulations, enabling ab initio-level accuracy with computational efficiency. However, current MLPs struggle with uncertainty quantification, limiting their reliability for active learning, calibration, and out-of-distribution (OOD) detection. We address these challenges by developing Bayesian E(3) equivariant MLPs with iterative restratification of many-body message passing. Our approach introduces the joint energy-force negative log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models uncertainty in both energies and interatomic forces, yielding superior accuracy compared to conventional NLL losses. We systematically benchmark multiple Bayesian approaches, including deep ensembles with mean-variance estimation, stochastic weight averaging Gaussian, improved variational online Newton, and laplace approximation by evaluating their performance on uncertainty prediction, OOD detection, calibration, and active learning tasks. We further demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by quantifying energy and force uncertainties. Using Bayesian active learning by disagreement (BALD), our framework outperforms random sampling and energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs achieve competitive accuracy with state-of-the-art models while enabling uncertainty-guided active learning, OOD detection, and energy/forces calibration. This work establishes Bayesian equivariant neural networks as a powerful framework for developing uncertainty-aware MLPs for atomistic simulations at scale. |
| 2025-10-03 | [Distributional Inverse Reinforcement Learning](http://arxiv.org/abs/2510.03013v1) | Feiyang Wu, Ye Zhao et al. | We propose a distributional framework for offline Inverse Reinforcement Learning (IRL) that jointly models uncertainty over reward functions and full distributions of returns. Unlike conventional IRL approaches that recover a deterministic reward estimate or match only expected returns, our method captures richer structure in expert behavior, particularly in learning the reward distribution, by minimizing first-order stochastic dominance (FSD) violations and thus integrating distortion risk measures (DRMs) into policy learning, enabling the recovery of both reward distributions and distribution-aware policies. This formulation is well-suited for behavior analysis and risk-aware imitation learning. Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks demonstrate that our method recovers expressive reward representations and achieves state-of-the-art imitation performance. |
| 2025-10-03 | [Real-Time Peer-to-Peer Energy Trading for Multi-Microgrids: Improved Double Auction Mechanism and Prediction-Free Online Trading Approach](http://arxiv.org/abs/2510.02985v1) | Kaidi Huang, Lin Cheng et al. | Peer-to-peer energy trading offers a promising solution for enhancing renewable energy utilization and economic benefits within interconnected microgrids. However, existing real-time P2P markets face two key challenges: high computational complexity in trading mechanisms, and suboptimal participant decision-making under diverse uncertainties. Existing prediction-based decision-making methods rely heavily on accurate forecasts, which are typically unavailable for microgrids, while prediction-free methods suffer from myopic behaviors. To address these challenges, this paper proposes an improved double auction mechanism combined with an adaptive step-size search algorithm to reduce computational burden, and a data-driven dual-reference online optimization (DDOO) framework to enhance participant decision-making. The improved mechanism simplifies bidding procedures, significantly reducing computational burden and ensuring rapid convergence to the market equilibrium. Additionally, the prediction-free DDOO framework mitigates myopic decision-making by introducing two informative reference signals. Case studies on a 20-microgrid system demonstrate the effectiveness and scalability of the proposed mechanism and approach. The improved mechanism significantly decreases the computational time while increasing local energy self-sufficiency periods from 0.01% to 29.86%, reducing reverse power flow periods from 24.51% to 3.96%, and lowering average operating costs by 19.20%. Compared with conventional approaches such as Lyapunov optimization and model predictive control, the DDOO framework achieves a 10%-13% reduction in operating costs with an optimality gap of only 5.76%. |
| 2025-10-03 | [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](http://arxiv.org/abs/2510.02976v1) | Alvaro Paz, Pauli Mustalahti et al. | This paper presents a framework for real-time optimal controlling of a heavy-duty skid-steered mobile platform for trajectory tracking. The importance of accurate real-time performance of the controller lies in safety considerations of situations where the dynamic system under control is affected by uncertainties and disturbances, and the controller should compensate for such phenomena in order to provide stable performance. A multiple-shooting nonlinear model-predictive control framework is proposed in this paper. This framework benefits from suitable algorithm along with readings from various sensors for genuine real-time performance with extremely high accuracy. The controller is then tested for tracking different trajectories where it demonstrates highly desirable performance in terms of both speed and accuracy. This controller shows remarkable improvement when compared to existing nonlinear model-predictive controllers in the literature that were implemented on skid-steered mobile platforms. |
| 2025-10-02 | [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](http://arxiv.org/abs/2510.02279v1) | Mykyta Ielanskyi, Kajetan Schweighofer et al. | Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings. |
| 2025-10-02 | [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](http://arxiv.org/abs/2510.02216v1) | Zeqi Ye, Minshuo Chen | Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance. |
| 2025-10-02 | [Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale](http://arxiv.org/abs/2510.02189v1) | Boris Kriuk | Arctic warming threatens over 100 billion in permafrost-dependent infrastructure across Northern territories, yet existing risk assessment frameworks lack spatiotemporal validation, uncertainty quantification, and operational decision-support capabilities. We present a hybrid physics-machine learning framework integrating 2.9 million observations from 171,605 locations (2005-2021) combining permafrost fraction data with climate reanalysis. Our stacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic Net) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal cross-validation preventing data leakage. To address machine learning limitations in extrapolative climate scenarios, we develop a hybrid approach combining learned climate-permafrost relationships (60%) with physical permafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over 10 years), we project mean permafrost fraction decline of -20.3 pp (median: -20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point loss. Infrastructure risk classification identifies 15% high-risk zones (25% medium-risk) with spatially explicit uncertainty maps. Our framework represents the largest validated permafrost ML dataset globally, provides the first operational hybrid physics-ML forecasting system for Arctic infrastructure, and delivers open-source tools enabling probabilistic permafrost projections for engineering design codes and climate adaptation planning. The methodology is generalizable to other permafrost regions and demonstrates how hybrid approaches can overcome pure data-driven limitations in climate change applications. |
| 2025-10-02 | [On the uncertainty in predicting the stochastic gravitational wave background from compact binary coalescences](http://arxiv.org/abs/2510.02163v1) | Michael Ebersold, Tania Regimbau | The stochastic gravitational-wave background from compact binary coalescences is expected to be the first detectable stochastic signal via cross-correlation searches with terrestrial detectors. It encodes the cumulative merger history of stellar-mass binaries across cosmic time, offering a unique probe of the high-redshift Universe. However, predicting the background spectrum is challenging due to numerous modeling choices, each with distinct uncertainties. In this work, we present a comprehensive forecast of the astrophysical gravitational-wave background from binary black holes, binary neutron stars, and neutron star-black hole systems. We systematically assess the impact of uncertainties in population properties, waveform features, and the modeling of the merger rate evolution. By combining all uncertainties, we derive credible bands for the background spectrum, spanning approximately an order of magnitude in the fractional energy density. These results provide thorough predictions to facilitate the interpretation of current upper limits and future detections. |
| 2025-10-02 | [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](http://arxiv.org/abs/2510.02109v1) | Jong Bum Won, Wesley De Neve et al. | Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast. |
| 2025-10-02 | [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](http://arxiv.org/abs/2510.02088v1) | Alexander Schmitz, Andreas Petersen et al. | The analysis of the size of nanoparticles is an essential task in plasma technology and dusty plasmas. Light scattering techniques, based on Mie theory, can be used as a non-invasive and in-situ diagnostic tool for this purpose. However, the standard back-calculation methods require expertise from the user. To address this, we introduce a neural network that performs the same task. We discuss how we set up and trained the network to analyze the size of plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up to several hundred nanometers, depending on the used wavelength. The diagnostic approach is kinetic, which means that the particles need to change in size due to growth or etching. An uncertainty analysis as well as a test with experimental data are presented. Our neural network achieves results that agree with those of prior fitting algorithms while offering higher methodical stability. The model also holds a major advantage in terms of computing speed and automation. |
| 2025-10-02 | [Event-triggered control and communication for single-master multi-slave teleoperation systems with Try-Once-Discard protocol](http://arxiv.org/abs/2510.02072v1) | Yuling Li, Chenxi Li et al. | Single-master multi-slave (SMMS) teleoperation systems can perform multiple tasks remotely in a shorter time, cover large-scale areas, and adapt more easily to single-point failures, thereby effectively encompassing a broader range of applications. As the number of slave manipulators sharing a communication network increases, the limitation of communication bandwidth becomes critical. To alleviate bandwidth usage, the Try-Once-Discard (TOD) scheduling protocol and event-triggered mechanisms are often employed separately. In this paper, we combine both strategies to optimize network bandwidth and energy consumption for SMMS teleoperation systems. Specifically, we propose event-triggered control and communication schemes for a class of SMMS teleoperation systems using the TOD scheduling protocol. Considering dynamic uncertainties, the unavailability of relative velocities, and time-varying delays, we develop adaptive controllers with virtual observers based on event-triggered schemes to achieve master-slave synchronization. Stability criteria for the SMMS teleoperation systems under these event-triggered control and communication schemes are established, demonstrating that Zeno behavior is excluded. Finally, experiments are conducted to validate the effectiveness of the proposed algorithms. |
| 2025-10-02 | [Strong-lensing rates of massive black hole binaries in LISA](http://arxiv.org/abs/2510.02061v1) | Juan Guti√©rrez, Macarena Lagos | Similarly to electromagnetic (EM) signals, gravitational lensing by intervening galaxies can also affect gravitational waves (GWs). In this paper, we estimate the strong-lensing rate of massive black hole mergers observed with LISA. Given the uncertainties in the source populations as well as in the population of galaxies at high redshift, we consider: six different source population models, including light and heavy seeds, as well as three lens population models, including redshift-independent and redshift-dependent evolution properties. Among all the scenarios explored, the expected number of strong lensed events detected in a 4-year observation time in LISA ranges between 0.13-231 with most of them having two (one) images detectable in the heavy (light) seed scenarios. The event numbers obtained correspond to 0.2%-0.9% of all detected unlensed events. Out of all the detectable strong-lensed events, up to 61% (in the light-seed scenario) and 1% (in the heavy-seed scenario) of them are above the detectability threshold solely due to strong lensing effects and would otherwise be undetectable. For detectable pairs of strong-lensed events by galaxy lenses, we also find between 72%-81% of them to have time delays from 1 week to 1 year. |
| 2025-10-02 | [Chemical transport by weakly nonlinear internal gravity waves in stars](http://arxiv.org/abs/2510.02031v1) | Yifeng Mao, Daniel Lecoanet | While it is well-known that internal gravity waves (IGWs) transport chemicals in the radiative zones of stars, there remains substantial uncertainty on the amount of, and physical mechanism behind, this transport. Most previous studies have relied on heuristic theories, or numerical simulations that may be hard to extrapolate to stellar parameters. In this work, we present the first rigorous asymptotic calculation of (passive) chemical transport by IGWs, in the limit of small wave amplitude. We find that the net transport by a coherent packet of waves scales like wave amplitude to the fourth power, and verify these analytic calculations with numerical simulations. Because the transport is equally likely to be positive as negative, the transport by a random superposition of waves is expected to scale as wave amplitude to the eighth power. These results show that closer comparisons between theoretical arguments and numerical calculations are essential for interpreting numerical simulations of chemical transport by IGWs, and making accurate predictions of this process for stellar evolution modeling. |
| 2025-10-02 | [A Large Sample of JWST/NIRSpec Brown Dwarfs: New Distant Discoveries](http://arxiv.org/abs/2510.02026v1) | Zhijun Tu, Shu Wang et al. | Brown dwarfs are essential probes of stellar and planetary formation, yet their low luminosities pose challenges for detection at large Galactic distances. The James Webb Space Telescope (JWST), with its unprecedented near-infrared sensitivity, enables the discovery and characterization of distant substellar objects, including those in the Milky Way's thick disk and halo. We conducted a systematic search using over 40,000 publicly available JWST/NIRSpec PRISM/CLEAR spectra and identified 68 brown dwarfs through spectral template matching and visual inspection. Among them, 12 are newly identified candidates, including 8 T dwarfs and 4 M/L dwarfs, most at distances exceeding 1 kpc. Remarkably, two sources -- JWST J001418.22-302223.2 and JWST J033240.07-274907.8 -- are found at distances greater than 5 kpc, making them the most distant brown dwarfs within the Milky Way. Spectral fits were performed using a nested sampling Monte Carlo algorithm with three model grids: Sonora Elf Owl, LOWZ, and SAND. The analysis reveals that cloud-free models are unable to reproduce L/T transition spectra, whereas the SAND model provides a more accurate representation of cloud effects in metal-poor environments. With the newly identified distant brown dwarfs, we also investigated the vertical metallicity gradient of brown dwarfs. Overall, the metallicities do not show an evident trend with Galactic height $|Z|$, due to the limited sample size and the uncertainties in metallicity measurements. |
| 2025-09-30 | [Uncertainty Quantification for Regression using Proper Scoring Rules](http://arxiv.org/abs/2509.26610v1) | Alexander Fishkov, Kajetan Schweighofer et al. | Quantifying uncertainty of machine learning model predictions is essential for reliable decision-making, especially in safety-critical applications. Recently, uncertainty quantification (UQ) theory has advanced significantly, building on a firm basis of learning with proper scoring rules. However, these advances were focused on classification, while extending these ideas to regression remains challenging. In this work, we introduce a unified UQ framework for regression based on proper scoring rules, such as CRPS, logarithmic, squared error, and quadratic scores. We derive closed-form expressions for the resulting uncertainty measures under practical parametric assumptions and show how to estimate them using ensembles of models. In particular, the derived uncertainty measures naturally decompose into aleatoric and epistemic components. The framework recovers popular regression UQ measures based on predictive variance and differential entropy. Our broad evaluation on synthetic and real-world regression datasets provides guidance for selecting reliable UQ measures. |
| 2025-09-30 | [Beyond Suboptimality: Resource-Rationality and Task Demands Shape the Complexity of Perceptual Representations](http://arxiv.org/abs/2509.26606v1) | Andrew Jun Lee, Daniel Turek et al. | Early theories of perception as probabilistic inference propose that uncertainty about the interpretation of sensory input is represented as a probability distribution over many interpretations -- a relatively complex representation. However, critics argue that persistent demonstrations of suboptimal perceptual decision-making indicate limits in representational complexity. We contend that suboptimality arises not from genuine limits, but participants' resource-rational adaptations to task demands. For example, when tasks are solvable with minimal attention to stimuli, participants may neglect information needed for complex representations, relying instead on simpler ones that engender suboptimality. Across three experiments, we progressively reduced the efficacy of resource-rational strategies on a carefully controlled decision task. Model fits favored simple representations when resource-rational strategies were effective, and favored complex representations when ineffective, suggesting that perceptual representations can be simple or complex depending on task demands. We conclude that resource-rationality is an epistemic constraint for experimental design and essential to a complete theory of perception. |
| 2025-09-30 | [Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning](http://arxiv.org/abs/2509.26605v1) | Ma√´l Macuglia, Paul Friedrich et al. | Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents. |
| 2025-09-30 | [The JWST EXCELS Survey: A spectroscopic investigation of the ionizing properties of star-forming galaxies at 1<z<8](http://arxiv.org/abs/2509.26591v1) | R. Begley, R. J. McLure et al. | Charting the Epoch of Reionization demands robust assessments of what drives the production of ionizing photons in high-redshift star-forming galaxies (SFGs), and requires better predictive capabilities from current observations. Using a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution spectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical analysis of their ionizing photon production efficiencies ($\xi_\rm{ion}$). We consider $\xi_\rm{ion}$, measured with Balmer line measurements, in relation to a number of key galaxy properties including; nebular emission line strengths ($W_\lambda(\rm{H\alpha})$ and $W_\lambda$( [OIII])), UV luminosity ($M_\rm{UV}$) and UV slope ($\beta_\rm{UV}$), as well as dust attenuation ($E(B-V)_\rm{neb}$) and redshift. Implementing a Bayesian linear regression methodology, we fit $\xi_\rm{ion}$ against the principal observables while fully marginalising over all measurement uncertainties, mitigating against the impact of outliers and determining the intrinsic scatter. Significant relations between $\xi_\rm{ion}$ and $ W_\lambda(\rm{H\alpha})$, $W_\lambda$([OIII]) and $\beta_\rm{UV}$ are recovered. Moreover, the weak trends with $M_\rm{UV}$ and redshift can be fully explained by the remaining property dependencies. Expanding our analysis to multivariate regression, we determine that $W_\lambda(\rm{H\alpha})$ or $W_\lambda$([OIII]), along with $\beta_\rm{UV}$ and $E(B-V)_\rm{neb}$, are the most important observables for accurately predicting $\xi_\rm{ion,0}$. The latter identifies the most common outliers as SFGs with relatively high $E(B-V)_\rm{neb}\gtrsim0.5$, possibly indicative of obscured star-formation or strong differential attenuation. Combining these properties enable $\xi_\rm{ion,0}$ to be inferred with an accuracy of $\sim0.15\,$dex, with a population intrinsic scatter of $\sigma_\rm{int}\sim0.035\,$dex. |
| 2025-09-30 | [Signal-Aware Workload Shifting Algorithms with Uncertainty-Quantified Predictors](http://arxiv.org/abs/2509.26511v1) | Ezra Johnson, Adam Lechowicz et al. | A wide range of sustainability and grid-integration strategies depend on workload shifting, which aligns the timing of energy consumption with external signals such as grid curtailment events, carbon intensity, or time-of-use electricity prices. The main challenge lies in the online nature of the problem: operators must make real-time decisions (e.g., whether to consume energy now) without knowledge of the future. While forecasts of signal values are typically available, prior work on learning-augmented online algorithms has relied almost exclusively on simple point forecasts. In parallel, the forecasting research has made significant progress in uncertainty quantification (UQ), which provides richer and more fine-grained predictive information. In this paper, we study how online workload shifting can leverage UQ predictors to improve decision-making. We introduce $\texttt{UQ-Advice}$, a learning-augmented algorithm that systematically integrates UQ forecasts through a $\textit{decision uncertainty score}$ that measures how forecast uncertainty affects optimal future decisions. By introducing $\textit{UQ-robustness}$, a new metric that characterizes how performance degrades with forecast uncertainty, we establish theoretical performance guarantees for $\texttt{UQ-Advice}$. Finally, using trace-driven experiments on carbon intensity and electricity price data, we demonstrate that $\texttt{UQ-Advice}$ consistently outperforms robust baselines and existing learning-augmented methods that ignore uncertainty. |
| 2025-09-30 | [Nondestructive characterization of laser-cooled atoms using machine learning](http://arxiv.org/abs/2509.26479v1) | G. De Sousa, M. Doris et al. | We develop machine learning techniques for estimating physical properties of laser-cooled potassium-39 atoms in a magneto-optical trap using only the scattered light -- i.e., fluorescence -- that is intrinsic to the cooling process. In-situ snap-shot images of fluorescing atomic ensembles directly reveal the spatial structure of these millimeter-scale objects but contain no obvious information regarding internal properties such as the temperature. We first assembled and labeled a balanced dataset sampling $8\times10^3$ different experimental parameters that includes examples with: large and dense atomic ensembles, a complete absence of atoms, and everything in between. We describe a range of models trained to predict atom number and temperature solely from fluorescence images. These run the gamut from a poorly performing linear regression model based only on integrated fluorescence to deep neural networks that give number and temperature with fractional uncertainties of $0.1$ and $0.2$ respectively. |
| 2025-09-30 | [Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification](http://arxiv.org/abs/2509.26457v1) | Artur Barros, Carlos Caetano et al. | Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Œ≤ spectrum](http://arxiv.org/abs/2509.26390v1) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete \beta decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full \beta decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of \beta particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision \beta decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {\sigma}) enhancement in \beta emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for \beta decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Œ≤ spectrum](http://arxiv.org/abs/2509.26390v2) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete $\beta$ decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full $\beta$ decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of $\beta$ particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision $\beta$ decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {$\sigma$}) enhancement in $\beta$ emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for $\beta$ decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition](http://arxiv.org/abs/2509.26385v1) | Zejin Gao, Ksheera Sagar et al. | We consider the problem of fully Bayesian posterior estimation and uncertainty quantification in undirected Gaussian graphical models via Markov chain Monte Carlo (MCMC) under recently-developed element-wise graphical priors, such as the graphical horseshoe. Unlike the conjugate Wishart family, these priors are non-conjugate; but have the advantage that they naturally allow one to encode a prior belief of sparsity in the off-diagonal elements of the precision matrix, without imposing a structure on the entire matrix. Unfortunately, for a graph with $p$ nodes and with $n$ samples, the state-of-the-art MCMC approaches for the element-wise priors achieve a per iteration complexity of $O(p^4),$ which is prohibitive when $p\gg n$. In this regime, we develop a suitably reparameterized MCMC with per iteration complexity of $O(p^3)$, providing a one-order of magnitude improvement, and consequently bringing the computational cost at par with the conjugate Wishart family, which is also $O(p^3)$ due to a use of the classical Bartlett decomposition, but this decomposition does not apply outside the Wishart family. Importantly, the proposed benefit is obtained solely due to our reparameterization in an MCMC scheme targeting the true posterior, that reverses the recently developed telescoping block decomposition of Bhadra et al. (2024), in a suitable sense. There is no variational or any other approximate Bayesian computation scheme considered in this paper that compromises targeting the true posterior. Simulations and the analysis of a breast cancer data set confirm both the correctness and better algorithmic scaling of the proposed reverse telescoping sampler. |
| 2025-09-30 | [Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography](http://arxiv.org/abs/2509.26382v1) | Shijie Lin, Bin Hu et al. | Time-delay cosmography, by monitoring the multiply imaged gravitational lenses in the time domain, offers a promising and independent method for measuring cosmological distances. However, in addition to the main deflector that produces the multiple images, the large-scale structure along the line-of-sight (LoS) will also deflect the traveling light rays, known as weak lensing (WL). Due to resolution limitations, accurately measuring WL on arcsecond scales is highly challenging. In this work, we evaluate the LoS effects on both lensing images and time-delay measurements using a more straightforward, high-resolution N-body simulation that provides a more realistic matter distribution compared to the traditional, computationally cheaper halo rendering method. We employ the multi-plane ray tracing technique, which is traditionally utilized to compute WL effects at the arcminute scale, extending its application to the strong lensing regime at the arcsecond scale. We focus on the quadruple-image system and present the following findings: 1. In addition to a constant external convergence, large-scale structures within a region approximately 2 arcminutes in angular size act as external perturbers, inducing inhomogeneous fluctuations on the arcsecond scale; 2. These fluctuations cannot be fully accounted for by external shear alone, necessitating the inclusion of external flexion; 3. While incorporating flexion provides a reasonably good fit to the lensing image, the time-delay distance still exhibits a $6.2$\textperthousand~bias and a $2.5\%$ uncertainty. This underscores the limitations of the single-plane approximation, as time-delay errors accumulate along the LoS. |
| 2025-09-29 | [Safe Planning in Unknown Environments using Conformalized Semantic Maps](http://arxiv.org/abs/2509.25124v1) | David Smith Sundarsingh, Yifei Li et al. | This paper addresses semantic planning problems in unknown environments under perceptual uncertainty. The environment contains multiple unknown semantically labeled regions or objects, and the robot must reach desired locations while maintaining class-dependent distances from them. We aim to compute robot paths that complete such semantic reach-avoid tasks with user-defined probability despite uncertain perception. Existing planning algorithms either ignore perceptual uncertainty - thus lacking correctness guarantees - or assume known sensor models and noise characteristics. In contrast, we present the first planner for semantic reach-avoid tasks that achieves user-specified mission completion rates without requiring any knowledge of sensor models or noise. This is enabled by quantifying uncertainty in semantic maps - constructed on-the-fly from perceptual measurements - using conformal prediction in a model- and distribution-free manner. We validate our approach and the theoretical mission completion rates through extensive experiments, showing that it consistently outperforms baselines in mission success rates. |
| 2025-09-29 | [New insights from the flavor dependence of quark transverse momentum distributions in the pion](http://arxiv.org/abs/2509.25098v1) | Lorenzo Rossi, Alessandro Bacchetta et al. | We update our previous extraction of transverse momentum distributions of unpolarized quarks in the pion by implementing a more comprehensive description of theoretical uncertainties and, for the first time, by exploring possible differences among quark flavors. We extract such distributions from all available data for unpolarized pion-nucleus Drell-Yan processes, where the cross section is differential in the transverse momentum of the final lepton pair. The cross section involves transverse momentum distributions in the nucleon, that we consistently take from our previous studies. |
| 2025-09-29 | [Curriculum Imitation Learning of Distributed Multi-Robot Policies](http://arxiv.org/abs/2509.25097v1) | Jes√∫s Roche, Eduardo Sebasti√°n et al. | Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements. |
| 2025-09-29 | [Finite-Size Security Bounds in Semi-Quantum Key Distribution: Spectral, Operator-Theoretic, and Entropic Perspectives](http://arxiv.org/abs/2509.25078v1) | Zahidur Rezwan Ratul | We study Semi-Quantum Key Distribution (SQKD) with a focus on finite-size security bounds, developed through three complementary perspectives. (i) Spectral disturbance: wrong-basis L\"uders updates produce closed-form spectra and purity loss, which serve as basis-independent indicators of disturbance. (ii) Operator-theoretic reduction: in Z/Z-sifted rounds, intercept-resend attacks can be represented as an effective depolarizing channel, characterized by a fidelity-QBER relation. (iii) Entropic trade-offs: Maassen-Uffink and memory-assisted uncertainty relations certify security through X tests and reflection rounds, even when the sifted QBER is low. The exposition provides step-by-step derivations supported by physically interpretable figures, and the framework concludes with finite-size estimates based on concentration inequalities that are suited for practical parameter estimation. |
| 2025-09-29 | [Confidence-Guided Error Correction for Disordered Speech Recognition](http://arxiv.org/abs/2509.25048v1) | Abner Hernandez, Tom√°s Arias Vergara et al. | We investigate the use of large language models (LLMs) as post-processing modules for automatic speech recognition (ASR), focusing on their ability to perform error correction for disordered speech. In particular, we propose confidence-informed prompting, where word-level uncertainty estimates are embedded directly into LLM training to improve robustness and generalization across speakers and datasets. This approach directs the model to uncertain ASR regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare our approach to both transcript-only fine-tuning and post hoc confidence-based filtering. Evaluations show that our method achieves a 10% relative WER reduction compared to naive LLM correction on the Speech Accessibility Project spontaneous speech and a 47% reduction on TORGO, demonstrating the effectiveness of confidence-aware fine-tuning for impaired speech. |
| 2025-09-29 | [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](http://arxiv.org/abs/2509.25034v1) | Heming Fu, Guojun Xiong et al. | As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management. |
| 2025-09-29 | [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](http://arxiv.org/abs/2509.25031v1) | Sophia V. Kuhn, Rafael Bischof et al. | Aging infrastructure portfolios pose a critical resource allocation challenge: deciding which structures require intervention and which can safely remain in service. Structural assessments must balance the trade-off between cheaper, conservative analysis methods and accurate but costly simulations that do not scale portfolio-wide. We propose Bayesian neural network (BNN) surrogates for rapid structural pre-assessment of worldwide common bridge types, such as reinforced concrete frame bridges. Trained on a large-scale database of non-linear finite element analyses generated via a parametric pipeline and developed based on the Swiss Federal Railway's bridge portfolio, the models accurately and efficiently estimate high-fidelity structural analysis results by predicting code compliance factors with calibrated epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware triage: flagging likely critical structures and providing guidance where refined analysis is pertinent. We demonstrate the framework's effectiveness in a real-world case study of a railway underpass, showing its potential to significantly reduce costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios. |
| 2025-09-29 | [The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools](http://arxiv.org/abs/2509.25028v1) | Eric Browne | Randomness plays a pivotal yet paradoxical role in computational music creativity: it can spark novelty, but unchecked chance risks incoherence. This paper presents a thematic review of contemporary AI music systems, examining how designers incorporate randomness and uncertainty into creative practice. I draw on the concept of structured uncertainty to analyse how stochastic processes are constrained within musical and interactive frameworks. Through a comparative analysis of six systems - Musika (Pasini and Schl\"uter, 2022), MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) - we identify recurring design patterns that support musical coherence, user control, and co-creativity. To my knowledge, this is the first thematic review examining randomness in AI music through structured uncertainty, offering practical insights for designers and artists aiming to support expressive, collaborative, or improvisational interactions. |
| 2025-09-29 | [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](http://arxiv.org/abs/2509.25017v1) | Spyros Kondylatos, Gustau Camps-Valls et al. | Wildfires are among the most severe natural hazards, posing a significant threat to both humans and natural ecosystems. The growing risk of wildfires increases the demand for forecasting models that are not only accurate but also reliable. Deep Learning (DL) has shown promise in predicting wildfire danger; however, its adoption is hindered by concerns over the reliability of its predictions, some of which stem from the lack of uncertainty quantification. To address this challenge, we present an uncertainty-aware DL framework that jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance short-term wildfire danger forecasting. In the next-day forecasting, our best-performing model improves the F1 Score by 2.3% and reduces the Expected Calibration Error by 2.1% compared to a deterministic baseline, enhancing both predictive skill and calibration. Our experiments confirm the reliability of the uncertainty estimates and illustrate their practical utility for decision support, including the identification of uncertainty thresholds for rejecting low-confidence predictions and the generation of well-calibrated wildfire danger maps with accompanying uncertainty layers. Extending the forecast horizon up to ten days, we observe that aleatoric uncertainty increases with time, showing greater variability in environmental conditions, while epistemic uncertainty remains stable. Finally, we show that although the two uncertainty types may be redundant in low-uncertainty cases, they provide complementary insights under more challenging conditions, underscoring the value of their joint modeling for robust wildfire danger prediction. In summary, our approach significantly improves the accuracy and reliability of wildfire danger forecasting, advancing the development of trustworthy wildfire DL systems. |
| 2025-09-29 | [Addressing Methodological Uncertainty in MCDM with a Systematic Pipeline Approach to Data Transformation Sensitivity Analysis](http://arxiv.org/abs/2509.24996v1) | Juan B. Cabral, Alvaro Roy Schachner | Multicriteria decision-making methods exhibit critical dependence on the choice of normalization techniques, where different selections can alter 20-40% of the final rankings. Current practice is characterized by the ad-hoc selection of methods without systematic robustness evaluation. We present a framework that addresses this methodological uncertainty through automated exploration of the scaling transformation space. The implementation leverages the existing Scikit-Criteria infrastructure to automatically generate all possible methodological combinations and provide robust comparative analysis. |
| 2025-09-26 | [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](http://arxiv.org/abs/2509.22633v1) | Gen Li, Yuling Yan | Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters. |
| 2025-09-26 | [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](http://arxiv.org/abs/2509.22612v1) | Jonne S√§lev√§, Duygu Ataman et al. | In this paper, we introduce a set of resampling-based methods for quantifying uncertainty and statistical precision of evaluation metrics in multilingual and/or multitask NLP benchmarks. We show how experimental variation in performance scores arises from both model- and data-related sources, and that accounting for both of them is necessary to avoid substantially underestimating the overall variability over hypothetical replications. Using multilingual question answering, machine translation, and named entity recognition as example tasks, we also demonstrate how resampling methods are useful for computing sampling distributions for various quantities used in leaderboards such as the average/median, pairwise differences between models, and rankings. |
| 2025-09-26 | [Likelihood-free inference for gravitational-wave data analysis and public alerts](http://arxiv.org/abs/2509.22561v1) | Ethan Marx, Deep Chatterjee et al. | Rapid and reliable detection and dissemination of source parameter estimation data products from gravitational-wave events, especially sky localization, is critical for maximizing the potential of multi-messenger astronomy. Machine learning based detection and parameter estimation algorithms are emerging as production ready alternatives to traditional approaches. Here, we report validation studies of AMPLFI, a likelihood-free inference solution to low-latency parameter estimation of binary black holes. We use simulated signals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run (O3) to compare sky localization performance with BAYESTAR, the algorithm currently in production for rapid sky localization of candidates from matched-filter pipelines. We demonstrate sky localization performance, measured by searched area and volume, to be equivalent with BAYESTAR. We show accurate reconstruction of source parameters with uncertainties for use distributing low-latency coarse-grained chirp mass information. In addition, we analyze several candidate events reported by the LVK in the third gravitational-wave transient catalog (GWTC-3) and show consistency with the LVK's analysis. Altogether, we demonstrate AMPLFI's ability to produce data products for low-latency public alerts. |
| 2025-09-26 | [Event Generator Tuning as a Robustness Test](http://arxiv.org/abs/2509.22526v1) | Jean Wolfs, Chris M. Marshall | Neutrino oscillation experiments use Monte Carlo event generators to predict neutrino-nucleus interactions. Cross section uncertainties are typically implemented by varying the parameters of the model(s) used in the generator. We study the performance of two commonly-used model configurations of the GENIE generator (G18_10a_02_11a and AR23_0i_00_000) and their uncertainties by tuning parameters to cross section data, and then comparing the resulting tuned prediction to a suite of other measurements from T2K, MicroBooNE, and MINERvA. This reveals whether the model can simultaneously describe several datasets, as well as whether the uncertainties are adequately robust. We find that G18 and especially AR23 are reasonable in predicting lower-energy measurements from T2K and MicroBooNE, but unable to describe MINERvA data, and discuss the implications for short-baseline oscillation searches. We attempt to replicate a tuning procedure developed by MicroBooNE using several different measurements, and find substantially different results depending on which measurement is used, and that the MicroBooNE tune does not agree with other measurements. We conclude that the SBN experiment should not tune its generator to external data. |
| 2025-09-26 | [A high-stability optical clock based on a continuously ground-state cooled Al$^+$ ion without compromising its accuracy](http://arxiv.org/abs/2509.22525v1) | Fabian Dawel, Lennart Pelzer et al. | Single ion optical clocks have shown systematic frequency uncertainties below $10^{-18}$, but typically require more than one week of averaging to achieve a corresponding statistical uncertainty. This time can be reduced with longer probe times, but comes at the cost of a higher time-dilation shift due to motional heating of the ions in the trap. We show that sympathetic ground-state cooling using electromagnetically-induced transparency (EIT) of an \Al clock ion via a co-trapped \Ca ion during clock interrogation suppresses the heating of the ions. \Al can be kept close to the motional ground state, independent from the chosen interrogation time, at a relative time dilation shift of $(-1.69\pm0.20)\times10^{-18}$. The \Ca cooling light introduces an additional light shift on the \Al clock transition of $(-9.27\pm 1.03)\times10^{-18}$. We project that the uncertainty of this light shift can be further reduced by nearly an order of magnitude. This sympathetic cooling enables seconds of interrogation time with $10^{-19}$ motional and cooling laser-induced uncertainties for \Al and can be employed in other ion clocks as well. |
| 2025-09-26 | [Metric response of relative entropy: a universal indicator of quantum criticality](http://arxiv.org/abs/2509.22515v1) | Pritam Sarkar, Diptiman Sen et al. | The information-geometric origin of fidelity susceptibility and its utility as a universal probe of quantum criticality in many-body settings have been widely discussed. Here we explore the metric response of quantum relative entropy (QRE), by tracing out all but $n$ adjacent sites from the ground state of spin chains of finite length $N$, as a parameter of the corresponding Hamiltonian is varied. The diagonal component of this metric defines a susceptibility of the QRE that diverges at quantum critical points (QCPs) in the thermodynamic limit. We study two spin-$1/2$ models as examples, namely the integrable transverse field Ising model (TFIM) and a non-integrable Ising chain with three-spin interactions. We demonstrate distinct scaling behaviors for the peak of the QRE susceptibility as a function of $N$: namely a square logarithmic divergence in TFIM and a power-law divergence in the non-integrable chain. This susceptibility encodes uncertainty of entanglement Hamiltonian gradients and is also directly connected to other information measures such as Petz-R\'enyi entropies. We further show that this susceptibility diverges even at finite $N$ if the subsystem size, $n$, exceeds a certain value when the Hamiltonian is tuned to its classical limits due to the rank of the RDMs being finite; unlike the divergence associated with the QCPs which require $N \rightarrow \infty$. |
| 2025-09-26 | [A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random](http://arxiv.org/abs/2509.22499v1) | Yunshu Zhang, Chan Park et al. | Instrumental variable (IV) methods offer a valuable approach to account for outcome data missing not-at-random. A valid missing data instrument is a measured factor which (i) predicts the nonresponse process and (ii) is independent of the outcome in the underlying population. For point identification, all existing IV methods for missing data including the celebrated Heckman selection model, a priori restrict the extent of selection bias on the outcome scale, therefore potentially understating uncertainty due to missing data. In this work, we introduce an IV framework which allows the degree of selection bias on the outcome scale to remain completely unrestricted. The new approach instead relies for identification on (iii) a key multiplicative selection model, which posits that the instrument and any hidden common correlate of selection and the outcome, do not interact on the multiplicative scale. Interestingly, we establish that any regular statistical functional of the missing outcome is nonparametrically identified under (i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald ratio estimand in causal inference. For estimation and inference, we characterize the influence function for any functional defined on a nonparametric model for the observed data, which we leverage to develop semiparametric multiply robust IV estimators. Several extensions of the methods are also considered, including the important practical setting of polytomous and continuous instruments. Simulation studies illustrate the favorable finite sample performance of proposed methods, which we further showcase in an HIV study nested within a household health survey study we conducted in Mochudi, Botswana, in which interviewer characteristics are used as instruments to correct for selection bias due to dependent nonresponse in the HIV component of the survey study. |
| 2025-09-26 | [On an optimization framework for damage localization in structures](http://arxiv.org/abs/2509.22492v1) | Owais Saleem, Tim Suchan et al. | Efficient structural damage localization remains a challenge in structural health monitoring (SHM), particularly when the problem is coupled with uncertainty of conditions and complexity of structures. Traditional methods simply based on experimental data processing are often not sufficiently reliable, while complex models often struggle with computational inefficiency given the tremendous amount of model parameters. This paper focuses on closing the gap between data-driven SHM and physics-based model updating by offering a solution for real-world infrastructure. We first concentrate on fusing multi-source damage-sensitive features (DSF) based on experimental modal data into spatially mapped belief masses to pre-screen candidate damage locations. The resulting candidate damage locations are integrated into an inverse Finite Element method (FEM) model calibration process. We propose an optimization framework to identify the most probable damage scenario with single and multi-damage cases. We present the corresponding numerical results in this paper, which open the door to extend the application of the framework to a complex real bridge structure. |
| 2025-09-26 | [B√©zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](http://arxiv.org/abs/2509.22476v1) | Chen Li, Meilong Xu et al. | Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, B\'ezier Meets Diffusion, for cross-domain image generation. First, we introduce a B\'ezier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance. |
| 2025-09-26 | [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](http://arxiv.org/abs/2509.22469v1) | Ben Rossano, Jaein Lim et al. | This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation. |
| 2025-09-25 | [Efficient Digital Methods to Quantify Sensor Output Uncertainty](http://arxiv.org/abs/2509.21311v1) | Orestis Kaparounakis, Phillip Stanley-Marbell | Accurate characterization of sensor output uncertainty is important for reliable data interpretation in many applications. Here, we investigate the impact of transducer-level measurement uncertainty on overall sensor measurement accuracy due to limited-precision information about sensor components. We explain our method using thermopile-based sensors as an example class of sensors. We show how sensor calibration and conversion equations, which are an essential part of all sensing systems, propagate uncertainties resulting from the quantization of calibration parameters, to the final, compensated sensor output. The experimental results show that the epistemic uncertainty of calibration-related quantities leads to absolute error in the sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for one commonly-used thermopile sensor. In one instance of using the epistemic uncertainty information in edge detection, we show reduction of false-positives edges to zero for the conventional Canny operator, while maintaining accuracy. We show these ideas are practical and possible on actual embedded sensor systems by prototyping them on two commercially-available uncertainty tracking hardware platforms, one with average power dissipation 16.7 mW and 42.9x speedup compared to the equal-confidence Monte Carlo computation (the status quo), and the other with average power dissipation 147.15 mW and 94.4x speedup, paving the way for use in real time. |
| 2025-09-25 | [Emission line tracers of galactic outflows driven by stellar feedback in simulations of isolated disk galaxies](http://arxiv.org/abs/2509.21295v1) | Elliot L. Howatson, Alexander J. Richings et al. | Hydrodynamic simulations can connect outflow observables to the physical conditions of outflowing gas. Here, we use simulations of isolated disk galaxies ranging from dwarf mass ($M_{200} = 10^{10}\mathrm{M}_{\odot}$) to Milky Way mass ($M_{200} = 10^{12}\mathrm{M}_{\odot}$), based on the FIRE-2 subgrid models to investigate multiphase galactic outflows. We use the CHIMES non-equilibrium chemistry module to create synthetic spectra of common outflow tracers ([CII]$_{158\rm{\mu m}}$, $\mathrm{CO}_{J(1-0)}$, H$\alpha$ and $[\mathrm{OIII}]_{5007\text{A}}$). Using our synthetic spectra we measure the mass outflow rate, kinetic power and momentum flux using observational techniques. In [CII]$_{158\rm{\mu m}}$ we measure outflow rates of $10^{-4}$ to $1$ $\mathrm{M_{\odot}yr^{-1}}$ across an SFR range of $10^{-3}$ to $1$ $\text{M}_{\odot}\text{yr}^{-1}$, which is in reasonable agreement with observations. The significant discrepancy is in $\mathrm{CO}_{J(1-0)}$, with the simulations lying $\approx1$ dex below the observational sample. We test observational assumptions used to derive outflow properties from synthetic spectra. We find the greatest uncertainty lies in measurements of electron density, as estimates using the SII doublet can overestimate the actual electron density by up to 2 dex, which changes mass outflow rates by up to 4 dex. We also find that molecular outflows are especially sensitive to the conversion factor between CO luminosity and H2 mass, with outflow rates changing by up to 4 dex in our least massive galaxy. Comparing the outflow properties derived from the synthetic spectra to those derived directly from the simulation, we find that [CII]$_{158\rm{\mu m}}$ probes outflows at greater distances from the disk, whilst we find that molecular gas does not survive at large distances within outflows within our modestly star-forming disk galaxies simulated in this work. |
| 2025-09-25 | [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](http://arxiv.org/abs/2509.21210v1) | Ali Kafili Gavgani, Amin Talaeizadeh et al. | Conventional multi-rotors are under-actuated systems, hindering them from independently controlling attitude from position. In this study, we present several distinct configurations that incorporate additional control inputs for manipulating the angles of the propeller axes. This addresses the mentioned limitations, making the systems "omniorientational". We comprehensively derived detailed dynamic models for all introduced configurations and validated by a methodology using Simscape Multibody simulations. Two controllers are designed: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the "Power Consumption Factor" defined in this study. Moreover, the controllers effectively managed harsh disturbances and uncertainties. Simulations compare and analyze the proposed configurations and controllers, majorly considering their power consumption. Furthermore, we conduct a qualitative comparison to evaluate the impact of different types of uncertainties on the control system, highlighting areas for potential model or hardware improvements. The analysis in this study provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design. This research aligns with the project SAC-1, one of the objectives of Sharif AgRoLab. |
| 2025-09-25 | [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](http://arxiv.org/abs/2509.21201v1) | Yang Fu, Peng Qin et al. | The vision of 6G networks aims to enable edge inference by leveraging ubiquitously deployed artificial intelligence (AI) models, facilitating intelligent environmental perception for a wide range of applications. A critical operation in edge inference is for an edge node (EN) to aggregate multi-view sensory features extracted by distributed agents, thereby boosting perception accuracy. Over-the-air computing (AirComp) emerges as a promising technique for rapid feature aggregation by exploiting the waveform superposition property of analog-modulated signals, which is, however, incompatible with existing digital communication systems. Meanwhile, hybrid reconfigurable intelligent surface (RIS), a novel RIS architecture capable of simultaneous signal amplification and reflection, exhibits potential for enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital AirComp (HRD-AirComp) scheme, which employs vector quantization to map high-dimensional features into discrete codewords that are digitally modulated into symbols for wireless transmission. By judiciously adjusting the AirComp transceivers and hybrid RIS reflection to control signal superposition across agents, the EN can estimate the aggregated features from the received signals. To endow HRD-AirComp with a task-oriented design principle, we derive a surrogate function for inference accuracy that characterizes the impact of feature quantization and over-the-air aggregation. Based on this surrogate, we formulate an optimization problem targeting inference accuracy maximization, and develop an efficient algorithm to jointly optimize the quantization bit allocation, agent transmission coefficients, EN receiving beamforming, and hybrid RIS reflection beamforming. Experimental results demonstrate that the proposed HRD-AirComp outperforms baselines in terms of both inference accuracy and uncertainty. |
| 2025-09-25 | [The depletion of the asteroid belt and the impact history of the Earth](http://arxiv.org/abs/2509.21194v1) | Julio A. Fernandez | We have evaluated the rate at which the asteroid belt is losing material, and how it splits between macroscopic bodies and meteoritic dust. The mass loss process is due to the injection of asteroid fragments into unstable dynamical regions, associated to mean-motion resonances with Jupiter, Saturn and Mars or secular resonances, from where they are scattered either to the region of the terrestrial planets or to the vicinity of Jupiter's orbit. Asteroid fragments that do not escape from the belt are ground down by mutual collisions to meteoritic dust. Under the assumption that 25\% of the zodiacal dust mass is of asteroidal origin, we find that the asteroid belt is currently losing a fraction of about $\mu_o \simeq 8.8 \times 10^{-5}$ Ma$^{-1}$ of its collisionally-active mass (without the primordial objects Ceres, Vesta and Pallas), about 20\% as macroscopic bodies, and 80\% as dust particles that feed the zodiacal dust cloud. Extrapolation of the current mass loss rate to the past suggests only a moderate increase of the asteroid belt mass and the mass loss rate around 3.0 - 3.5 Ga ago (by about 50\% and a factor of two respectively). Yet, should the computed $\mu_o$ be somewhat underestimated owing to the different uncertainties associated to its computation, the extrapolation to the past would lead to quite different results. For instance, a moderate increase in the computed $\mu_o$, say by a factor of three, would lead to an exponential increase of the asteroid mass and mass loss rate about 3.5 Ga ago. A greater asteroid mass loss rate in the past should be correlated with a more intense impact rate of the Earth, Moon and the other terrestrial planets, which is indeed what suggests the geologic record (Hartmann 2007). |
| 2025-09-25 | [$Œõ_{c}(2910)$ and $Œõ_{c}(2940)$ productions in $p \bar{p}$ annihilation and $K^{-}p$ scattering processes](http://arxiv.org/abs/2509.21176v1) | Quan-Yun Guo, Dian-Yong Chen | In this work, we investigate the productions of $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ in the $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes by utilizing an effective Lagrangian approach, where both $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ are considered as $D^{\ast}N$ molecular states with $J^{P}=1/2^{-}$ and $3/2^{-}$, respectively. The cross sections and $D^0 p$ invariant mass distributions at exemplified center-of-mass energy for the involved processes are estimated. At $\sqrt{s}=10$ $\mathrm{GeV}$, our estimations indicate that the cross sections for $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes are $(2.1^{+5.6}_{-1.6})$ nb, $(2.2^{+5.9}_{-1.7})$ nb, and $(0.6^{+1.6}_{-0.4})$ nb, respectively, where the uncertainties are resulted from the variation of model parameter. Furthermore, Our estimations of the $D^{0}p$ invariant mass spectra reveal that the peak structure around 2.9 GeV primarily originates from $\Lambda_c(2910)$ across all three processes, however the contributions from $\Lambda_c(2940)$ makes the structure asymmetric. It is expected that the total cross sections and the $D^0 p$ invariant mass distributions estimated in the present work can be tested by future experiments at $\mathrm{\bar{P}ANDA}$ and J-PARC. |
| 2025-09-25 | [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](http://arxiv.org/abs/2509.21161v1) | Giuseppe Serra, Florian Buettner | Continual Learning (CL) is recently gaining increasing attention for its ability to enable a single model to learn incrementally from a sequence of new classes. In this scenario, it is important to keep consistent predictive performance across all the classes and prevent the so-called Catastrophic Forgetting (CF). However, in safety-critical applications, predictive performance alone is insufficient. Predictive models should also be able to reliably communicate their uncertainty in a calibrated manner - that is, with confidence scores aligned to the true frequencies of target events. Existing approaches in CL address calibration primarily from a data-centric perspective, relying on a single temperature shared across all tasks. Such solutions overlook task-specific differences, leading to large fluctuations in calibration error across tasks. For this reason, we argue that a more principled approach should adapt the temperature according to the distance to the current task. However, the unavailability of the task information at test time/during deployment poses a major challenge to achieve the intended objective. For this, we propose Distance-Aware Temperature Scaling (DATS), which combines prototype-based distance estimation with distance-aware calibration to infer task proximity and assign adaptive temperatures without prior task information. Through extensive empirical evaluation on both standard benchmarks and real-world, imbalanced datasets taken from the biomedical domain, our approach demonstrates to be stable, reliable and consistent in reducing calibration error across tasks compared to state-of-the-art approaches. |
| 2025-09-25 | [Normalizing Flows are Capable Visuomotor Policy Learning Models](http://arxiv.org/abs/2509.21073v1) | Simon Kristoffersson Lind, Jialong Li et al. | The field of general purpose robotics has recently embraced powerful probabilistic models, such as diffusion models, to model and learn complex behaviors. However, these models often come with significant trade-offs, namely high computational costs for inference and a fundamental inability to quantify output uncertainty. We argue that a model's trustworthiness, a critical factor for reliable, general-purpose robotics, is inherently linked to its ability to provide confidence measures.   In this work, we introduce Normalizing Flows Policy, a novel visuomotor policy learning model based on Normalizing Flows. We show that Normalizing Flows are a natural and powerful alternative to diffusion models, providing both a statistically sound measure of confidence and a highly efficient inference process. Through comprehensive experiments across four distinct simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves performance comparable to, and often surpassing, Diffusion Policy, and it does so not only with improved sample efficiency but also with up to 30 times faster inference. Additionally, our ablation study validates several key architectural and training techniques that enable Normalizing Flows to perform well in this domain. |
| 2025-09-25 | [A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data](http://arxiv.org/abs/2509.21041v1) | Maeve Upton, Eamonn Organ et al. | Accurate estimation of solar irradiance is essential for reliable modelling of solar photovoltaic (PV) power production. In Ireland's highly variable maritime climate, where ground-based measurement stations are sparsely distributed, selecting an appropriate solar irradiance dataset presents a significant challenge. This study introduces a novel Bayesian spatio-temporal modelling framework for predicting solar irradiance at hourly and sub-hourly (10-minute) resolutions across Ireland. Cross-validation demonstrates that our model is statistically robust across all temporal resolutions with hourly showing highest prediction precision whereas 10-minute resolution encounters higher errors but better uncertainty quantification. In separate evaluations, we compare our model against alternative data sources, including reanalysis datasets and nearest-station interpolation, and find that it consistently provides superior site-specific accuracy. At the hourly scale, our model outperforms ERA5 in agreement with ground-based observations. At the sub-hourly scale, 10-minute resolution estimates provide solar PV power outputs consistent with residential and industrial solar PV installations in Ireland. Beyond surpassing existing datasets, our model delivers full uncertainty quantification, scalability and the capacity for real-time implementation, offering a powerful tool for solar energy prediction and the estimation of losses due to overload clipping from inverter undersizing. |
| 2025-09-25 | [Study on Locomotive Epidemic Dynamics in a Stochastic Spatio-Temporal Simulation Model on a Multiplex Network](http://arxiv.org/abs/2509.21017v1) | H. M. Shadman Tabib, Jaber Ahmed Deedar et al. | This study presents an integrated approach to understanding epidemic dynamics through a stochastic spatio-temporal simulation model on a multiplex network, blending physical and informational layers. The physical layer maps the geographic movement of individuals, while the information layer tracks the spread of knowledge and health behavior via social interactions. We explore the interplay between physical mobility, information flow, and epidemic outcomes by simulating disease spread within this dual-structured network. Our model employs stochastic elements to mirror human behavior, mobility, and information dissemination uncertainties. Through simulations, we assess the impact of network structure, mobility patterns, and information spread speed on epidemic dynamics. The findings highlight the crucial role of effective communication in curbing disease transmission, even in highly mobile societies. Additionally, our agent-based simulation allows for real-time scenario analysis through a user interface, offering insights into leveraging physical and informational networks for epidemic control. This research sheds light on designing strategic interventions in complex social systems to manage disease outbreaks. |
| 2025-09-24 | [Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing](http://arxiv.org/abs/2509.20349v1) | Ramona Rubini, Siavash Khodakarami et al. | Accurate time-series forecasting for complex physical systems is the backbone of modern industrial monitoring and control. While deep learning models excel at capturing complex dynamics, currently, their deployment is limited due to physical inconsistency and robustness, hence constraining their reliability in regulated environments. We introduce process-informed forecasting (PIF) models for temperature in pharmaceutical lyophilization. We investigate a wide range of models, from classical ones such as Autoregressive Integrated Moving Average Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning architectures, including Kolmogorov-Arnold Networks (KANs). We compare three different loss function formulations that integrate a process-informed trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a Residual-Based Attention (RBA) mechanism. We evaluate all models not only for accuracy and physical consistency but also for robustness to sensor noise. Furthermore, we test the practical generalizability of the best model in a transfer learning scenario on a new process. Our results show that PIF models outperform their data-driven counterparts in terms of accuracy, physical plausibility and noise resilience. This work provides a roadmap for developing reliable and generalizable forecasting solutions for critical applications in the pharmaceutical manufacturing landscape. |
| 2025-09-24 | [Quantum speed limits based on Jensen-Shannon and Jeffreys divergences for general physical processes](http://arxiv.org/abs/2509.20347v1) | Jucelino Ferreira de Sousa, Diego Paiva Pires | We discuss quantum speed limits (QSLs) for finite-dimensional quantum systems undergoing a general physical process. These QSLs were obtained using two families of entropic measures, namely the square root of the Jensen-Shannon divergence, which in turn defines a faithful distance of quantum states, and the square root of the quantum Jeffreys divergence. The results apply to both closed and open quantum systems, and are evaluated in terms of the Schatten speed of the evolved state, as well as cost functions that depend on the smallest and largest eigenvalues of both initial and instantaneous states of the quantum system. To illustrate our findings, we focus on the unitary and nonunitary dynamics of mixed single-qubit states. In the first case, we obtain speed limits $\textit{\`{a} la}$ Mandelstam-Tamm that are inversely proportional to the variance of the Hamiltonian driving the evolution. In the second case, we set the nonunitary dynamics to be described by the noisy operations: depolarizing channel, phase damping channel, and generalized amplitude damping channel. We provide analytical results for the two entropic measures, present numerical simulations to support our results on the speed limits, comment on the tightness of the bounds, and provide a comparison with previous QSLs. Our results may find applications in the study of quantum thermodynamics, entropic uncertainty relations, and also complexity of many-body systems. |
| 2025-09-24 | [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](http://arxiv.org/abs/2509.20293v1) | Benjamin Feuer, Chiung-Yi Tseng et al. | LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md |
| 2025-09-24 | [4D Driving Scene Generation With Stereo Forcing](http://arxiv.org/abs/2509.20251v1) | Hao Lu, Zhuang Ma et al. | Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}. |
| 2025-09-24 | [$S_8$ from Tully-Fisher, fundamental plane, and supernova distances agree with Planck](http://arxiv.org/abs/2509.20235v1) | Richard Stiskalek | Peculiar velocity measurements constrain the parameter combination $f\sigma_8$, the product of the linear growth rate $f$ and the fluctuation amplitude $\sigma_8$. Under the approximation that $f$ is a monotonic function of $\Omega_{\rm m}$, this can be related to $S_8 \equiv \sigma_8 \sqrt{\Omega_{\rm m}/0.3}$, enabling direct comparison with weak lensing and cosmic microwave background results. We exploit this by using three classes of direct-distance tracers -- the Tully-Fisher relation, the fundamental plane, and Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical forward model jointly calibrates each distance indicator and a linear theory reconstruction of the local Universe. This is the first consistent Bayesian analysis to combine all three major classes of distance indicators within a common framework, enabling cross-checks of systematics across diverse galaxy populations. All three tracers yield consistent values of $S_8$ that are also in agreement with Planck. Our joint constraint is $S_8 = 0.819 \pm 0.030$, with the uncertainty dominated by the 2M++ galaxy field. These results demonstrate that peculiar velocity surveys provide a robust, consistent measurement of $S_8$, and support concordance with the cosmic microwave background. |
| 2025-09-24 | [InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection](http://arxiv.org/abs/2509.20140v1) | Zongyi Li, Junchuan Zhao et al. | Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence/Arousal/Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis. |
| 2025-09-24 | [Causal Understanding by LLMs: The Role of Uncertainty](http://arxiv.org/abs/2509.20088v1) | Oscar Lithgow-Serrano, Vani Kanjirangat et al. | Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining. |
| 2025-09-24 | [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](http://arxiv.org/abs/2509.20077v1) | Xun Li, Rodrigo Santa Cruz et al. | To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution. We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments. |
| 2025-09-24 | [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](http://arxiv.org/abs/2509.20065v1) | Maggie Mi, Aline Villavicencio et al. | Language models often struggle with idiomatic, figurative, or context-sensitive inputs, not because they produce flawed outputs, but because they misinterpret the input from the outset. We propose an input-only method for anticipating such failures using token-level likelihood features inspired by surprisal and the Uniform Information Density hypothesis. These features capture localized uncertainty in input comprehension and outperform standard baselines across five linguistically challenging datasets. We show that span-localized features improve error detection for larger models, while smaller models benefit from global patterns. Our method requires no access to outputs or hidden activations, offering a lightweight and generalizable approach to pre-generation error prediction. |
| 2025-09-24 | [Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors](http://arxiv.org/abs/2509.20059v1) | Koki Kanzaki, Koya Sato | This paper proposes a high-accuracy radio map construction method tailored for environments where location information is affected by bursty errors. Radio maps are an effective tool for visualizing wireless environments. Although extensive research has been conducted on accurate radio map construction, most existing approaches assume noise-free location information during sensing. In practice, however, positioning errors ranging from a few to several tens of meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring such errors during inference can lead to significant degradation in radio map accuracy. This study highlights that these errors often tend to be biased when using mobile devices as sensors. We introduce a novel framework that models these errors together with spatial correlation in radio propagation by embedding them as tunable parameters in the marginal log-likelihood function. This enables ex-post calibration of location uncertainty during radio map construction. Numerical results based on practical human mobility data demonstrate that the proposed method can limit RMSE degradation to approximately 0.25-0.29 dB, compared with Gaussian process regression using noise-free location data, whereas baseline methods suffer performance losses exceeding 1 dB. |
| 2025-09-23 | [Recalibration of the H$Œ±$ surface brightness-radius relation for planetary nebulae using Gaia DR3: new distances and the Milky Way oxygen radial gradient](http://arxiv.org/abs/2509.19239v1) | Oscar Cavichia, Hektor Monteiro et al. | The spatial distribution of chemical elements in the Galactic disk provides key constraints on models of galaxy evolution. However, studies using planetary nebulae (PNe) as tracers have been historically limited by large uncertainties in their distances. To overcome the long-standing distance uncertainties, we recalibrated the H$\alpha$ surface brightness-radius relation (Frew et al. 2016) with Gaia DR3 parallaxes, deriving statistical distances for 1,200 PNe and Bayesian distances for 419 objects with reliable parallaxes. Adopting Bayesian values preferentially, we determined the O/H radial gradient for 230 disk PNe. We tested three models: a single linear gradient, a segmented fit with one break, and a segmented fit with two breaks. Although model selection is statistically inconclusive, segmented fits indicate a change in slope near the solar radius ($R \sim 8$ kpc), with a flatter or slightly positive gradient inward and a steeper negative gradient outward. This feature may reflect changes in star formation efficiency driven by the Galactic bar or the corotation resonance of the spiral arms. Comparison with other tracers - Cepheids, red giants, and open clusters - shows qualitative consistency. The two-dimensional O/H distribution in the Galactic plane supports the adopted distances and reveals modest azimuthal asymmetry, with enhanced abundances near the bar at positive longitudes, and a bimodal abundance structure between the inner and outer solar regions. Our results provide new constraints on the chemical evolution of the Milky Way, the impact of non-axisymmetric structures, and the possible existence of distinct radial abundance regimes across the Galactic disk. |
| 2025-09-23 | [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](http://arxiv.org/abs/2509.19233v1) | Milad Leyli-abadi, Antoine Marot et al. | In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page. |
| 2025-09-23 | [A decentralized future for the open-science databases](http://arxiv.org/abs/2509.19206v1) | Gaurav Sharma, Viorel Munteanu et al. | Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come. |
| 2025-09-23 | [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](http://arxiv.org/abs/2509.19185v1) | Mohammed Mehedi Hasan, Hao Li et al. | Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents. |
| 2025-09-23 | [Bayesian Neural Networks versus deep ensembles for uncertainty quantification in machine learning interatomic potentials](http://arxiv.org/abs/2509.19180v1) | Riccardo Farris, Emanuele Telari et al. | Neural-network-based machine learning interatomic potentials have emerged as powerful tools for predicting atomic energies and forces, enabling accurate and efficient simulations in atomistic modeling. A key limitation of traditional deep learning approaches, however, is their inability to provide reliable estimates of predictive uncertainty. Such uncertainty quantification is critical for assessing model reliability, especially in materials science, where often the model is applied on out-of-distribution data. Different strategies have been proposed to address this challenge, with deep ensembles and Bayesian neural networks being among the most widely used. In this work, we introduce an implementation of Bayesian neural networks with variational inference in the aenet-PyTorch framework. To evaluate their applicability to machine learning interatomic potentials, we systematically compare the performance of variational BNNs and deep ensembles on a dataset of 7,815 TiO$_{2}$ structures. The models are trained on both the full dataset and a subset to assess how variations in data representation influence predictive accuracy and uncertainty estimation. This analysis provides insights into the strengths and limitations of each approach, offering practical guidance for the development of uncertainty-aware machine learning interatomic potentials. |
| 2025-09-23 | [Intrinsic Heisenberg Lower Bounds on Schwarzschild and Weyl-Class Spacelike Slices](http://arxiv.org/abs/2509.19099v1) | Thomas Sch√ºrmann | We establish a coordinate-invariant Heisenberg-type lower bound for quantum states strictly localized in geodesic balls of radius $r_g$ on horizon-regular spacelike slices of static, spherically symmetric, asymptotically flat (AF) black-holes. Via a variance-eigenvalue equivalence the momentum uncertainty reduces to the first Dirichlet eigenvalue of the Laplace-Beltrami operator, yielding a slice-uniform Hardy baseline $\sigma_p r_g \ge \hbar/2$ under mild convexity assumptions on the balls; the bound is never attained and admits a positive gap both on compact interior regions and uniformly far out. For the Schwarzschild Painlev\'e-Gullstrand (PG) slice, whose induced 3-geometry is Euclidean, one recovers the exact Euclidean scale $\sigma_p r_g \ge \pi\hbar$, which is optimal among all admissible slices. The entire construction extends across the black-hole horizon, and it transfers to the static axisymmetric Weyl class, where the Hardy floor, strict gap, and AF $\pi$-scale persist (a global PG-like optimum need not exist). |
| 2025-09-23 | [Hybrid Adaptive Robust Stochastic Optimization Model for the Design of a Photovoltaic Battery Energy Storage System](http://arxiv.org/abs/2509.19054v1) | Alba Lun Mora Pous, Fernando Garcia-Mu√±oz et al. | Future energy projections and their inherent uncertainty play a key role in the design of photovoltaic-battery energy storage systems (PV-BESS) for household use. In this study, both stochastic and robust optimization techniques are simultaneously integrated into a Hybrid Adaptive Robust-Stochastic Optimization (HARSO) model. Uncertainty in future PV generation is addressed using a stochastic approach, while uncertainty in power demand is handled through robust optimization. To solve the tri-level structure emerging from the hybrid approach, a Column-and-Constraint Generation (CCG) algorithm is implemented. The model also accounts for battery degradation by considering multiple commercially available battery chemistries, enabling a more realistic evaluation of long-term system costs and performance. To demonstrate its applicability, the model is applied to a case study involving the optimal design of a PV-BESS system for a household in Spain. The empirical analysis includes both first-life (FL) and second-life (SL) batteries with different chemistries, providing a comprehensive evaluation of design alternatives under uncertainty. Results indicate that the optimal solution is highly dependent on the level of robustness considered, leading to a shift in design strategy. Under less conservative settings, robustness is achieved by increasing battery capacity, while higher levels of conservatism favor expanding PV capacity to meet demand. |
| 2025-09-23 | [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](http://arxiv.org/abs/2509.18998v1) | Christina Schenk, Jacobo Ayensa Jim√©nez et al. | Computational models provide crucial insights into complex biological processes such as cancer evolution, but their mechanistic nature often makes them nonlinear and parameter-rich, complicating calibration. We systematically evaluate parameter probability distributions in cell migration models using Bayesian calibration across four complementary strategies: parametric and surrogate models, each with and without explicit model discrepancy. This approach enables joint analysis of parameter uncertainty, predictive performance, and interpretability. Applied to a real data experiment of glioblastoma progression in microfluidic devices, surrogate models achieve higher computational efficiency and predictive accuracy, whereas parametric models yield more reliable parameter estimates due to their mechanistic grounding. Incorporating model discrepancy exposes structural limitations, clarifying where model refinement is necessary. Together, these comparisons offer practical guidance for calibrating and improving computational models of complex biological systems. |
| 2025-09-23 | [Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints](http://arxiv.org/abs/2509.18988v1) | Ziliang Lyu, Miroslav Krstic et al. | This paper considers the problem of adaptively overriding unsafe actions of a nominal controller in the presence of high-relative-degree nonovershooting constraints and parametric uncertainties. To prevent the design from being coupled with high-order derivatives of the parameter estimation error, we adopt a modular design approach in which the controller and the parameter identifier are designed separately. The controller module ensures that any safety violations caused by parametric uncertainties remain bounded, provided that the parameter estimation error and its first-order derivative are either bounded or square-integrable. The identifier module, in turn, guarantees that these requirements on the parameter estimation error are satisfied. Both theoretical analysis and simulation results demonstrate that the closed-loop safety violation is bounded by a tunable function of the initial estimation error. Moreover, as time increases, the parameter estimate converges to the true value, and the amount of safety violation decreases accordingly. |
| 2025-09-23 | [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](http://arxiv.org/abs/2509.18954v1) | Minoo Dolatabadi, Fardin Ayar et al. | LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map, which may not always be available, or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty. In this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness. |
| 2025-09-22 | [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](http://arxiv.org/abs/2509.18090v1) | Jiahe Li, Jiawei Zhang et al. | Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR. |
| 2025-09-22 | [Robust, Online, and Adaptive Decentralized Gaussian Processes](http://arxiv.org/abs/2509.18011v1) | Fernando Llorente, Daniel Waxman et al. | Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling. |
| 2025-09-22 | [Cosmic inventory of the background fields of relativistic particles in the Universe](http://arxiv.org/abs/2509.17954v1) | Jonathan Biteau | The extragalactic background is composed of the emission from all astrophysical sources, both resolved and unresolved, in addition to any diffuse components. In the last decade, there has been significant progress in our understanding of the cosmic history of extragalactic emissions associated with stellar evolution and accretion onto supermassive black holes, largely enabled by the extensive body of multi-wavelength data. The brightness of the extragalactic sky is now measured in photons, neutrinos, and cosmic rays, using observatories on the ground, in the sea, and in the ice, satellites in Earth orbit, and probes at the edge of the solar system. This wealth of disparate data is essential to unraveling the mysteries of the source populations that contribute to the extragalactic background.   In this contribution, we present an open database containing the most comprehensive collection of measurements of the extragalactic background spectrum to date. The combination of multi-messenger measurements over 27 frequency decades allows us to estimate the energy density of most extragalactic background components with an uncertainty of less than 30%. We explore the consistency of this cosmic inventory of the observed fields of relativistic particles populating the Universe with the cosmic history of star formation and accretion around supermassive black holes. Models incorporating these cosmic histories, as well as the redshift-dependent luminosity functions of extragalactic sources, currently match the electromagnetic component of the extragalactic background spectrum over 14 frequency decades, from the near UV to sub-TeV gamma rays. The knowledge gained from synthetic population models in the electromagnetic bands may become a crucial tool for understanding the origin of the most energetic extragalactic messengers, neutrinos and ultrahigh-energy cosmic rays. |
| 2025-09-22 | [Deep Hierarchical Learning with Nested Subspace Networks](http://arxiv.org/abs/2509.17874v1) | Paulius Rauba, Mihaela van der Schaar | Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models. |
| 2025-09-22 | [Modeling Scintillation Photon Transport and Reconstruction Algorithms for the Time-of-Flight Detector in the T2K Neutrino Experiment](http://arxiv.org/abs/2509.17860v1) | C. Alt, A. Blanchet et al. | The T2K ND280 upgrade aims to reduce the systematic uncertainty of the CP-violating phase, $\delta_{CP}$, to reject non-CP violation hypothesis at $3\sigma$ confidence level. A crucial component of the ND280 upgrade, alongside the Super Fine Grained Detector (SuperFGD) and two High-Angle Time Projection Chambers (TPCs), is the Time-of-Flight (ToF) detector, which significantly enhances background rejection and particle identification capabilities. The ToF detector features six modules in a cube configuration, each with 20 plastic scintillator bars measuring $\text{220}\times\text{12}\times\text{1}\,\text{cm}^3$ and is equipped with Silicon Photomultiplier (SiPM) arrays at both ends to capture scintillation light. This letter outlines the modelling of the detector response and the signal reconstruction process. |
| 2025-09-22 | [How Realistic are Idealized Copper Surfaces? A Machine Learning Study of Rough Copper-Water Interfaces](http://arxiv.org/abs/2509.17833v1) | Linus C. Erhard, Johannes Sch√∂rghuber et al. | Copper is a highly promising catalyst for the electrochemical CO$_2$ reduction reaction (CO2RR) since it is the only pure metal that can form highly added-value products such as ethylene and ethanol. Since the CO2RR takes place in aqueous solution, the detailed atomic structure of the water-copper interface is essential for unraveling the key reaction mechanisms. In this study, we investigate copper-water interfaces exhibiting nanometer-scale roughnesses. We introduce two molecular dynamics protocols to create rough copper surfaces, which are subsequently brought into contact with water. From these interfaces, we sample additional training configurations from machine-learning-interatomic-potential-driven molecular dynamics simulations containing hundreds of thousands of atoms. An active learning workflow is developed to identify regions with high spatially resolved uncertainty and convert them into DFT-feasible cells through a modified amorphous matrix embedding approach. Finally, we analyze the local environments at the interface using unsupervised machine-learning techniques. Unique environments emerge on the rough copper surfaces absent from model systems, including stacking-fault-induced configurations and undercoordinated corner atoms. Notably, corner atoms consistently feature chemisorbed water molecules in our simulations, indicating their potential importance in catalytic processes. |
| 2025-09-22 | [On Fast Attitude Filtering Based on Matrix Fisher Distribution with Stability Guarantee](http://arxiv.org/abs/2509.17827v1) | Shijie Wang, Haichao Gui et al. | This paper addresses two interrelated problems of the nonlinear filtering mechanism and fast attitude filtering with the matrix Fisher distribution (MFD) on the special orthogonal group. By analyzing the distribution evolution along Bayes' rule, we reveal two essential properties that enhance the performance of Bayesian attitude filters with MFDs, particularly in challenging conditions, from a theoretical viewpoint.   Benefiting from the new understanding of the filtering mechanism associated with MFDs, two closed-form filters with MFDs is then proposed. These filters avoid the burdensome computations in previous MFD-based filters by introducing linearized error systems with right-invariant errors but retaining the two advantageous properties. Moreover, we leverage the two properties and closed-form filtering iteration to prove the almost-global exponential stability of the proposed filter with right-invariant error for the single-axis rotation, which, to our knowledge, is not achieved by existing directional statistics-based filters. Numerical simulations demonstrate that the proposed filters are significantly more accurate than the classic invariant Kalman filter. Besides, they are also as accurate as recent MFD-based Bayesian filters in challenging circumstances with large initial error and measurement uncertainty but consumes far less computation time (about 1/5 to 1/100 of previous MFD-based attitude filters). |
| 2025-09-22 | [RoboSeek: You Need to Interact with Your Objects](http://arxiv.org/abs/2509.17783v1) | Yibo Peng, Jiahao Yang et al. | Optimizing and refining action execution through   exploration and interaction is a promising way for robotic   manipulation. However, practical approaches to interaction driven robotic learning are still underexplored, particularly for   long-horizon tasks where sequential decision-making, physical   constraints, and perceptual uncertainties pose significant chal lenges. Motivated by embodied cognition theory, we propose   RoboSeek, a framework for embodied action execution that   leverages interactive experience to accomplish manipulation   tasks. RoboSeek optimizes prior knowledge from high-level   perception models through closed-loop training in simulation   and achieves robust real-world execution via a real2sim2real   transfer pipeline. Specifically, we first replicate real-world   environments in simulation using 3D reconstruction to provide   visually and physically consistent environments., then we train   policies in simulation using reinforcement learning and the   cross-entropy method leveraging visual priors. The learned   policies are subsequently deployed on real robotic platforms   for execution. RoboSeek is hardware-agnostic and is evaluated   on multiple robotic platforms across eight long-horizon ma nipulation tasks involving sequential interactions, tool use, and   object handling. Our approach achieves an average success rate   of 79%, significantly outperforming baselines whose success   rates remain below 50%, highlighting its generalization and   robustness across tasks and platforms. Experimental results   validate the effectiveness of our training framework in complex,   dynamic real-world settings and demonstrate the stability of the   proposed real2sim2real transfer mechanism, paving the way for   more generalizable embodied robotic learning. Project Page:   https://russderrick.github.io/Roboseek/ |
| 2025-09-22 | [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](http://arxiv.org/abs/2509.17726v1) | Javier Bisbal, Patrick Winter et al. | Accurate anatomical labeling of intracranial arteries is essential for cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming and subject to interoperator variability. We present a deep learning-based framework for automated artery labeling from 3D Time-of-Flight Magnetic Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating uncertainty quantification to enhance interpretability and reliability. We evaluated three convolutional neural network architectures: (1) a UNet with residual encoder blocks, reflecting commonly used baselines in vascular labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and spatial attention mechanisms for enhanced curvilinear structure recognition; and (3) nnUNet, a self-configuring framework that automates preprocessing, training, and architectural adaptation based on dataset characteristics. Among these, nnUNet achieved the highest labeling performance (average Dice score: 0.922; average surface distance: 0.387 mm), with improved robustness in anatomically complex vessels. To assess predictive confidence, we implemented test-time augmentation (TTA) and introduced a novel coordinate-guided strategy to reduce interpolation errors during augmented inference. The resulting uncertainty maps reliably indicated regions of anatomical ambiguity, pathological variation, or manual labeling inconsistency. We further validated clinical utility by comparing flow velocities derived from automated and manual labels in co-registered 4D Flow MRI datasets, observing close agreement with no statistically significant differences. Our framework offers a scalable, accurate, and uncertainty-aware solution for automated cerebrovascular labeling, supporting downstream hemodynamic analysis and facilitating clinical integration. |
| 2025-09-22 | [Enhanced fill probability estimates in institutional algorithmic bond trading using statistical learning algorithms with quantum computers](http://arxiv.org/abs/2509.17715v1) | Axel Ciceri, Austin Cottrell et al. | The estimation of fill probabilities for trade orders represents a key ingredient in the optimization of algorithmic trading strategies. It is bound by the complex dynamics of financial markets with inherent uncertainties, and the limitations of models aiming to learn from multivariate financial time series that often exhibit stochastic properties with hidden temporal patterns. In this paper, we focus on algorithmic responses to trade inquiries in the corporate bond market and investigate fill probability estimation errors of common machine learning models when given real production-scale intraday trade event data, transformed by a quantum algorithm running on IBM Heron processors, as well as on noiseless quantum simulators for comparison. We introduce a framework to embed these quantum-generated data transforms as a decoupled offline component that can be selectively queried by models in low-latency institutional trade optimization settings. A trade execution backtesting method is employed to evaluate the fill prediction performance of these models in relation to their input data. We observe a relative gain of up to ~ 34% in out-of-sample test scores for those models with access to quantum hardware-transformed data over those using the original trading data or transforms by noiseless quantum simulation. These empirical results suggest that the inherent noise in current quantum hardware contributes to this effect and motivates further studies. Our work demonstrates the emerging potential of quantum computing as a complementary explorative tool in quantitative finance and encourages applied industry research towards practical applications in trading. |
| 2025-09-19 | [CAPOS: The bulge Cluster APOGEE Survey VI. Characterizing multiple stellar populations and chemical abundances in the bulge globular cluster NGC 6569](http://arxiv.org/abs/2509.16168v1) | Nicol√°s Barrera, Sandro Villanova et al. | Context. The CAPOS project aims to obtain accurate mean abundances and radial velocities for many elements, and it explores the multiple population (MP) phenomenon in Galactic bulge globular clusters (BGCs). NGC 6569 is one of the CAPOS targets. Aims. This study provides a detailed high-resolution spectroscopic analysis of NGC 6569 to derive precise abundances for elements of different nucleosynthetic origins and to unveil its MPs by focusing on key spectral features. Methods. We analyzed APOGEE-2 near-infrared spectra of 11 giant members with the BACCHUS code, deriving abundances for 12 elements (C, N, O, Mg, Si, Ca, Ti, Fe, Ni, Al, Ce, Nd). Isochrone fitting with Gaia+2MASS photometry was used to estimate atmospheric parameters, cluster distance, and extinction. Results. We obtained [Fe/H] = -0.91 $\pm$ 0.06, consistent with APOGEE pipeline values; the scatter lies within uncertainties. The cluster shows [$\alpha$/Fe] = 0.36 $\pm$ 0.06 dex, similar to other GCs. Al appears homogeneous, while N is strongly enriched ([N/Fe] = 0.68 $\pm$ 0.34) with a spread of 0.90 dex, yielding two populations anticorrelated in C. The n-capture elements Ce and Nd are overabundant compared to the Sun but consistent with GCs of similar metallicity, with $\langle$[Ce/Nd]$\rangle$ = -0.17 $\pm$ 0.12. We also measure RV = -49.75 $\pm$ 3.68 km s$^{-1}$, consistent with previous works, while d$_\odot$ = 12.4 $\pm$ 1.45 kpc and E(B-V) = 0.68 are both higher than literature values. Conclusions. MPs in NGC 6569 are confirmed through a clear C-N anticorrelation. The cluster shows [$\alpha$/Fe] enhancement from Type II SNe and no Mg-Al-Si anticorrelation, suggesting rapid and homogeneous formation. The $\langle$[Ce/Nd]$\rangle$ ratio points to contributions from r-process events such as neutron star mergers, while overall Ce and Nd abundances are reported here for the first time in this cluster. |
| 2025-09-19 | [Lensed stars in galaxy-galaxy strong lensing -- a JWST prediction for the Cosmic Horseshoe](http://arxiv.org/abs/2509.16154v1) | Sung Kei Li, Luke Weisenbach et al. | We explore for the first time the possibility of detecting lensed star transients in galaxy-galaxy strong lensing systems upon repeated, deep imaging using the {\it James-Webb Space Telescope} ({\it JWST}). Our calculation predicts that the extremely high recent star formation rate of $\sim 100\,M_{\odot}\textrm{yr}^{-1}$ over the last 50 Myr (not accounting for image multiplicity) in the ``Cosmic Horseshoe'' lensed system ($z = 2.381$) generates many young, bright stars, of which their large abundance is expected to lead to a detection rate of $\sim 60$ transients per pointing in {\it JWST} observations with a $5\sigma$ limiting magnitude of $\sim 29\,m_{AB}$. With the high expected detection rate and little room for uncertainty for the lens model compared with cluster lenses, our result suggests that the Cosmic Horseshoe could be an excellent tool to test the nature of Dark Matter based on the spatial distribution of transients, and can be used to constrain axion mass if Dark Matter is constituted of ultra-light axions. We also argue that the large distance modulus of $\sim46.5\,$mag at $z \approx 2.4$ can act as a filter to screen out less massive stars as transients and allow one to better constrain the high-mass end of the stellar initial mass function based on the transient detection rate. Follow-up {\it JWST} observations of the Cosmic Horseshoe with would allow one to better probe the nature of Dark Matter and the star formation properties, such as the initial mass function at the cosmic noon, via lensed star transients. |
| 2025-09-19 | [First evidence of $CP$ violation in beauty baryon to charmonium decays](http://arxiv.org/abs/2509.16103v1) | LHCb collaboration, R. Aaij et al. | A study of the difference in the $CP$ asymmetries between ${\Lambda}^0_b \rightarrow J / \psi p \pi^-$ and ${\Lambda}^0_b \rightarrow J / \psi p K^-$ decays, $\Delta {\cal A}_{CP}$, is performed using proton-proton collision data collected by the LHCb experiment in the years 2015--2018, corresponding to an integrated luminosity of $6 {\rm fb}^{-1}$. This quantity is measured to be $ \Delta {\cal A}_{CP}=(4.03\pm 1.18\pm 0.23)\%$, where the first uncertainty is statistical and the second is systematic. When combined with the previous LHCb result, a value of $\Delta {\cal A}_{CP} = (4.31 \pm 1.06 \pm 0.28)\%$ is obtained, corresponding to a significance of $3.9\sigma$ against the $CP$ symmetry hypothesis. Studies of triple-product asymmetries, which provide an additional probe of $CP$ violation, show no significant deviation from $CP$ symmetry. |
| 2025-09-19 | [Compose by Focus: Scene Graph-based Atomic Skills](http://arxiv.org/abs/2509.16053v1) | Han Qi, Changhe Chen et al. | A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine "focused" scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks. |
| 2025-09-19 | [Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation](http://arxiv.org/abs/2509.16007v1) | James E. Warner, Geoffrey F. Bomarito et al. | Multifidelity uncertainty propagation combines the efficiency of low-fidelity models with the accuracy of a high-fidelity model to construct statistical estimators of quantities of interest. It is well known that the effectiveness of such methods depends crucially on the relative correlations and computational costs of the available computational models. However, the question of how to automatically tune low-fidelity models to maximize performance remains an open area of research. This work investigates automated model tuning, which optimizes model hyperparameters to minimize estimator variance within a target computational budget. Focusing on multifidelity trajectory simulation estimators, the cost-versus-precision tradeoff enabled by this approach is demonstrated in a practical, online setting where upfront tuning costs cannot be amortized. Using a real-world entry, descent, and landing example, it is shown that automated model tuning largely outperforms hand-tuned models even when the overall computational budget is relatively low. Furthermore, for scenarios where the computational budget is large, model tuning solutions can approach the best-case multifidelity estimator performance where optimal model hyperparameters are known a priori. Recommendations for applying model tuning in practice are provided and avenues for enabling adoption of such approaches for budget-constrained problems are highlighted. |
| 2025-09-19 | [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](http://arxiv.org/abs/2509.15987v1) | Aur√©lien Cecille, Stefan Duffner et al. | Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines. |
| 2025-09-19 | [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](http://arxiv.org/abs/2509.15981v1) | Yujie Zhu, Charles A. Hepburn et al. | In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD. |
| 2025-09-19 | [Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems](http://arxiv.org/abs/2509.15961v1) | Nicole Aretz, Thomas Lynn et al. | We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition. |
| 2025-09-19 | [Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration](http://arxiv.org/abs/2509.15959v1) | Zhuoyue Zhang, Haitong Xu | Autonomous navigation in maritime domains is accelerating alongside advances in artificial intelligence, sensing, and connectivity. Opaque decision-making and poorly calibrated human-automation interaction remain key barriers to safe adoption. This article synthesizes 100 studies on automation transparency for Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA), human factors, interface design, and regulation. We (i) map the Guidance-Navigation-Control stack to shore-based operational modes -- remote supervision (RSM) and remote control (RCM) -- and identify where human unsafe control actions (Human-UCAs) concentrate in handover and emergency loops; (ii) summarize evidence that transparency features (decision rationales, alternatives, confidence/uncertainty, and rule-compliance indicators) improve understanding and support trust calibration, though reliability and predictability often dominate trust; (iii) distill design strategies for transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI presentation (textual/graphical overlays, color coding, conversational and immersive UIs), and engineer-facing processes (resilient interaction design, validation, and standardization). We integrate methods for Human-UCA identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and operator workload monitoring, and outline regulatory and rule-based implications including COLREGs formalization and route exchange. We conclude with an adaptive transparency framework that couples operator state estimation with explainable decision support to reduce cognitive overload and improve takeover timeliness. The review highlights actionable figure-of-merit displays (e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs (rule traceability, confidence), and training pipelines (HIL/MIL, simulation) as near-term levers for safer MASS operations. |
| 2025-09-19 | [Quantum Metric Spaces: Replacing Fuzzy Metrics with the Hilbert Space Structure of Quantum States](http://arxiv.org/abs/2509.15945v1) | Nicola Fabiano | Fuzzy metric spaces, grounded in t-norms and membership functions, have been widely proposed to model uncertainty in machine learning, decision systems, and artificial intelligence. Yet these frameworks treat uncertainty as an external layer of imprecision imposed upon classical, point-like entities - a conceptual mismatch for domains where indeterminacy is intrinsic, such as quantum systems or cognitive representations. We argue that fuzzy metrics are unnecessary for modeling such uncertainty: instead, the well-established structure of complex Hilbert spaces - the foundational language of quantum mechanics for over a century - provides a natural, rigorous, and non-contradictory metric space where the ``points'' are quantum states themselves. The distance between states is given by the Hilbert norm, which directly encodes state distinguishability via the Born rule. This framework inherently captures the non-classical nature of uncertainty without requiring fuzzy logic, t-norms, or membership degrees. We demonstrate its power by modeling AI concepts as Gaussian wavefunctions and classifying ambiguous inputs via quantum overlap integrals. Unlike fuzzy methods, our approach naturally handles interference, distributional shape, and concept compositionality through the geometry of state vectors. We conclude that fuzzy metric spaces, while historically useful, are obsolete for representing intrinsic uncertainty - superseded by the more robust, predictive, and ontologically coherent framework of quantum state geometry. |
| 2025-09-18 | [Explaining deep learning for ECG using time-localized clusters](http://arxiv.org/abs/2509.15198v1) | Ahc√®ne Boubekki, Konstantinos Patlatzoglou et al. | Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns. |
| 2025-09-18 | [Parallel Simulation of Contact and Actuation for Soft Growing Robots](http://arxiv.org/abs/2509.15180v1) | Yitian Gao, Lucas Chen et al. | Soft growing robots, commonly referred to as vine robots, have demonstrated remarkable ability to interact safely and robustly with unstructured and dynamic environments. It is therefore natural to exploit contact with the environment for planning and design optimization tasks. Previous research has focused on planning under contact for passively deforming robots with pre-formed bends. However, adding active steering to these soft growing robots is necessary for successful navigation in more complex environments. To this end, we develop a unified modeling framework that integrates vine robot growth, bending, actuation, and obstacle contact. We extend the beam moment model to include the effects of actuation on kinematics under growth and then use these models to develop a fast parallel simulation framework. We validate our model and simulator with real robot experiments. To showcase the capabilities of our framework, we apply our model in a design optimization task to find designs for vine robots navigating through cluttered environments, identifying designs that minimize the number of required actuators by exploiting environmental contacts. We show the robustness of the designs to environmental and manufacturing uncertainties. Finally, we fabricate an optimized design and successfully deploy it in an obstacle-rich environment. |
| 2025-09-18 | [To CLEAN or not to CLEAN: Data Processing in the ngVLA era](http://arxiv.org/abs/2509.15176v1) | Hendrik M√ºller | Radio interferometric imaging has long relied on the CLEAN algorithm, valued for its speed, robustness, and integration with calibration pipelines. However, next-generation facilities such as the ngVLA, SKA, and ALMAs Wideband Sensitivity Upgrade will produce data volumes and dynamic ranges that exceed the scalability of traditional methods. CLEAN remains dominant due to its simplicity and accumulated expertise, yet its assumption of modeling the sky as point sources limits its ability to recover extended emission and hampers automation. We review CLEANs limitations and survey alternatives, including multiscale extensions, compressive sensing, Regularized Maximum Likelihood, Bayesian inference, and AI-driven approaches. Forward-modeling methods enable higher fidelity, flexible priors, and uncertainty quantification, albeit at greater computational cost. Hybrid approaches such as Autocorr-CLEAN, CG-CLEAN, and PolyCLEAN retain CLEANs workflow while incorporating modern optimization. We argue hybrids are best suited for the near term, while Bayesian and AI-based frameworks represent the long-term future of interferometric imaging. |
| 2025-09-18 | [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](http://arxiv.org/abs/2509.15147v1) | Viktor Kovalchuk, Nikita Kotelevskii et al. | Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training. |
| 2025-09-18 | [A model-independent measurement of the CKM angle $Œ≥$ in the decays $B^\pm\to[K^+K^-œÄ^+œÄ^-]_D h^\pm$ and $B^\pm\to[œÄ^+œÄ^-œÄ^+œÄ^-]_D h^\pm$ ($h = K, œÄ$)](http://arxiv.org/abs/2509.15139v1) | LHCb collaboration, R. Aaij et al. | A model-independent determination of the CKM angle $\gamma$ is presented, using the $B^\pm\to[K^+K^-\pi^+\pi^-]_D h^\pm$ and $B^\pm\to[\pi^+\pi^-\pi^+\pi^-]_D h^\pm$ decays, with $h=K,\pi$. This measurement is the first phase-space-binned study of these decay modes, and uses a sample of proton-proton collision data collected by the LHCb experiment, corresponding to an integrated luminosity of $9$fb$^{-1}$. The phase-space bins are optimised for sensitivity to $\gamma$, and in each bin external inputs from the BESIII experiment are used to constrain the charm strong-phase parameters. The result of this binned analysis is $\gamma = (53.9_{-8.9}^{+9.5})^\circ$, where the uncertainty includes both statistical and systematic contributions. Furthermore, when combining with existing phase-space-integrated measurements of the same decay modes, a value of $\gamma = (52.6_{-6.4}^{+8.5})^\circ$ is obtained, which is one of the most precise determinations of $\gamma$ to date. |
| 2025-09-18 | [Sequential sample size calculations and learning curves safeguard the robust development of a clinical prediction model for individuals](http://arxiv.org/abs/2509.15134v1) | Amardeep Legha, Joie Ensor et al. | When prospectively developing a new clinical prediction model (CPM), fixed sample size calculations are typically conducted before data collection based on sensible assumptions. But if the assumptions are inaccurate the actual sample size required to develop a reliable model may be very different. To safeguard against this, adaptive sample size approaches have been proposed, based on sequential evaluation of a models predictive performance. Aim: illustrate and extend sequential sample size calculations for CPM development by (i) proposing stopping rules based on minimising uncertainty (instability) and misclassification of individual-level predictions, and (ii) showcasing how it safeguards against inaccurate fixed sample size calculations. Using the sequential approach repeats the pre-defined model development strategy every time a chosen number (e.g., 100) of participants are recruited and adequately followed up. At each stage, CPM performance is evaluated using bootstrapping, leading to prediction and classification stability statistics and plots, alongside optimism-adjusted measures of calibration and discrimination. Our approach is illustrated for development of acute kidney injury using logistic regression CPMs. The fixed sample size calculation, based on perceived sensible assumptions suggests recruiting 342 patients to minimise overfitting; however, the sequential approach reveals that a much larger sample size of 1100 is required to minimise overfitting (targeting population-level stability). If the stopping rule criteria also target small uncertainty and misclassification probability of individual predictions, the sequential approach suggests an even larger sample size (n=1800). Our sequential sample size approach allows users to dynamically monitor individual-level prediction and classification instability and safeguard against using inaccurate assumptions. |
| 2025-09-18 | [LOFAR 58 MHz Legacy Survey of the 3CRR Catalog](http://arxiv.org/abs/2509.15115v1) | J. M. Boxelaar, F. De Gasperin et al. | The Low Frequency Array (LOFAR) is uniquely able to perform deep, 15" resolutions imaging at frequencies below 100 MHz. Observations in this regime, using the Low Band Antenna (LBA) system, are significantly affected by instrumental and ionospheric distortions. Recent developments in calibration techniques have enabled routine production of high-fidelity images at these challenging frequencies. The aim of this paper is to obtain images of the radio sources included in the Third Cambridge catalog, second revised version (3CRR), at an observing frequency of 58 MHz, with an angular resolution of 15"and sensitivity to both compact and diffuse radio emission. This work also aims to produce accurate flux measurements for all sources. This dataset is designed to serve as a reference for low-frequency radio galaxy studies and future spectral aging analyses. We deliver 58. MHz radio images for the complete 3CRR sample including flux density measurements. We determined that the LBA has an accurate flux density scale with an average flux uncertainty of 10%. This is an important confirmation for any future works using the LOFAR LBA system. With these results we characterize the bright radio galaxy population with new high-resolution low-frequency images. We also provide high-resolution models of these sources which will be useful for calibrating future surveys. This legacy survey significantly expands the available high-resolution data at low frequencies and is the first fully imaged high-resolution sample at ultra low frequencies (< 100 MHz). It lays the foundation for future studies of radio galaxy physics, low-energy cosmic-ray populations, and the interplay between radio jets and their environments. |
| 2025-09-18 | [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](http://arxiv.org/abs/2509.15107v1) | Amy Rafferty, Rishi Ramaesh et al. | Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks. |
| 2025-09-18 | [Precise measurement of the $t\bar{t}$ production cross-section and lepton differential distributions in $eŒº$ dilepton events from $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector](http://arxiv.org/abs/2509.15066v1) | ATLAS Collaboration | The inclusive top quark pair ($t\bar{t}$) cross-section $\sigma_{t\bar{t}}$ has been measured in $\sqrt{s}=13$ TeV proton-proton collisions, using 140 fb$^{-1}$ of data collected by the ATLAS experiment at the Large Hadron Collider. Using events with an opposite-charge $e\mu$ pair and $b$-tagged jets, the cross-section is measured to be: $\sigma_{t\bar{t}} = 829.3 \pm 1.3\,\mathrm{(stat)}\ \pm 8.0\,\mathrm{(syst)}\ \pm 7.3\,\mathrm{(lumi)}\ \pm 1.9\,\mathrm{(beam)}\,\mathrm{pb},$ where the uncertainties reflect the limited size of the data sample, experimental and theoretical systematic effects, the integrated luminosity, and the proton beam energy, giving a total uncertainty of 1.3%. The result is used to determine the top quark pole mass via the dependence of the predicted cross-section on $m_t^\mathrm{pole}$, giving $m_t^\mathrm{pole}=172.8^{+1.5}_{-1.7}$ GeV. The same event sample is used to measure absolute and normalised differential cross-sections for the $t\bar{t}\rightarrow e\mu\nu\bar{\nu}b\bar{b}$ process as a function of single-lepton and dilepton kinematic variables. Complementary measurements of $e\mu b\bar{b}$ production, treating both $t\bar{t}$ and $Wt$ events as signal, are also provided. Both sets of differential cross-sections are compared to the predictions of various Monte Carlo event generators, demonstrating that the state-of-the-art generators Powheg MiNNLO and Powheg $bb4l$ describe the data better than Powheg hvq. |
| 2025-09-18 | [Constraining Cosmology with Double-Source-Plane Strong Gravitational Lenses From the AGEL Survey](http://arxiv.org/abs/2509.15012v1) | Duncan J. Bowden, Nandini Sahu et al. | Double-source-plane strong gravitational lenses (DSPLs), with two sources at different redshifts, are independent cosmological probes of the dark energy equation of state parameter $w$ and the matter density parameter $\Omega_{\rm m}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer cosmological constraints from this system for flat $\Lambda$CDM and flat $w$CDM cosmologies. From the joint posterior of $w$ and $\Omega_{\rm m}$ in the flat $w$CDM cosmology, we extract the following median values and 1$\sigma$ uncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\Omega_{\rm m} = 0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with two previously analyzed DSPLs, we present the joint constraint on these parameters from a sample of three, the largest galaxy-scale DSPL sample used for cosmological measurement to date. The combined precision of $w$ from three DSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave background (CMB) measurements improves the precision of $w$ from CMB-only constraints by 39%, demonstrating the complementarity of DSPLs with the CMB. Despite their promising constraining power, DSPLs are limited by sample size, with only a handful discovered so far. Although ongoing and near-future wide-area sky surveys will increase the number of known DSPLs by up to two orders of magnitude, these systems will still require dedicated high-resolution imaging and spectroscopic follow-ups like those presented in this paper. Our ASTRO 3D Galaxy Evolution with Lenses (AGEL) collaboration is undertaking such follow-up campaigns for several newly discovered DSPLs and will provide cosmological measurements from larger samples of DSPLs in the future. |
| 2025-09-17 | [Large deviations for probability graphons](http://arxiv.org/abs/2509.14204v1) | Pierfrancesco Dionigi, Giulio Zucal | We establish a large deviation principle (LDP) for probability graphons, which are symmetric functions from the unit square into the space of probability measures. This notion extends classical graphons and provides a flexible framework for studying the limit behavior of large dense weighted graphs. In particular, our result generalizes the seminal work of Chatterjee and Varadhan (2011), who derived an LDP for Erd\H{o}s-R\'enyi random graphs via graphon theory. We move beyond their binary (Bernoulli) setting to encompass arbitrary edge-weight distributions. Specifically, we analyze the distribution on probability graphons induced by random weighted graphs in which edges are sampled independently from a common reference probability measure supported on a compact Polish space. We prove that this distribution satisfies an LDP with a good rate function, expressed as an extension of the Kullback-Leibler divergence between probability graphons and the reference measure. This theorem can also be viewed as a Sanov-type result in the graphon setting. Our work provides a rigorous foundation for analyzing rare events in weighted networks and supports statistical inference in structured random graph models under distributional edge uncertainty. |
| 2025-09-17 | [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](http://arxiv.org/abs/2509.14167v1) | Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes et al. | Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains. |
| 2025-09-17 | [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](http://arxiv.org/abs/2509.14151v1) | Rongyu Zhang, Jiaming Liu et al. | Vision-centric Bird's Eye View (BEV) perception holds considerable promise for autonomous driving. Recent studies have prioritized efficiency or accuracy enhancements, yet the issue of domain shift has been overlooked, leading to substantial performance degradation upon transfer. We identify major domain gaps in real-world cross-domain scenarios and initiate the first effort to address the Domain Adaptation (DA) challenge in multi-view 3D object detection for BEV perception. Given the complexity of BEV perception approaches with their multiple components, domain shift accumulation across multi-geometric spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain adaptation. In this paper, we introduce an innovative geometric-aware teacher-student framework, BEVUDA++, to diminish this issue, comprising a Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model. Specifically, RDT effectively blends target LiDAR with dependable depth predictions to generate depth-aware information based on uncertainty estimation, enhancing the extraction of Voxel and BEV features that are essential for understanding the target domain. To collaboratively reduce the domain shift, GCS maps features from multiple spaces into a unified geometric embedding space, thereby narrowing the gap in data distribution between the two domains. Additionally, we introduce a novel Uncertainty-guided Exponential Moving Average (UEMA) to further reduce error accumulation due to domain shifts informed by previously obtained uncertainty guidance. To demonstrate the superiority of our proposed method, we execute comprehensive experiments in four cross-domain scenarios, securing state-of-the-art performance in BEV 3D object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night adaptation. |
| 2025-09-17 | [Safe Sliding Mode Control in Position for Double Integrator Systems](http://arxiv.org/abs/2509.14121v1) | Marco A. Gomez, Christopher D. Cruz-Ancona | We address the problem of robust safety control design for double integrator systems. We show that, when the constraints are defined only on position states, it is possible to construct a safe sliding domain from the dynamic of a simple integrator that is already safe. On this domain, the closed-loop trajectories remain robust and safe against uncertainties and disturbances. Furthermore, we design a controller gain that guarantees convergence to the safe sliding domain while avoiding the given unsafe set. The concept is initially developed for first-order sliding mode and is subsequently generalized to an adaptive framework, ensuring that trajectories remain confined to a predefined vicinity of the sliding domain, outside the unsafe region. |
| 2025-09-17 | [Online Bayesian Risk-Averse Reinforcement Learning](http://arxiv.org/abs/2509.14077v1) | Yuhao Wang, Enlu Zhou | In this paper, we study the Bayesian risk-averse formulation in reinforcement learning (RL). To address the epistemic uncertainty due to a lack of data, we adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the parameter uncertainty of the unknown underlying model. We derive the asymptotic normality that characterizes the difference between the Bayesian risk value function and the original value function under the true unknown distribution. The results indicate that the Bayesian risk-averse approach tends to pessimistically underestimate the original value function. This discrepancy increases with stronger risk aversion and decreases as more data become available. We then utilize this adaptive property in the setting of online RL as well as online contextual multi-arm bandits (CMAB), a special case of online RL. We provide two procedures using posterior sampling for both the general RL problem and the CMAB problem. We establish a sub-linear regret bound, with the regret defined as the conventional regret for both the RL and CMAB settings. Additionally, we establish a sub-linear regret bound for the CMAB setting with the regret defined as the Bayesian risk regret. Finally, we conduct numerical experiments to demonstrate the effectiveness of the proposed algorithm in addressing epistemic uncertainty and verifying the theoretical properties. |
| 2025-09-17 | [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](http://arxiv.org/abs/2509.14054v1) | Weihao Yan, Christoph Brune et al. | Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications. |
| 2025-09-17 | [Network representations reveal structured uncertainty in music](http://arxiv.org/abs/2509.14053v1) | Lluc Bono Rossell√≥, Robert Jankowski et al. | Music, as a structured yet perceptually rich experience, can be modeled as a network to uncover how humans encode and process auditory information. While network-based representations of music are increasingly common, the impact of feature selection on structural properties and cognitive alignment remains underexplored. In this study, we evaluated eight network models, each constructed from symbolic representations of piano compositions using distinct combinations of pitch, octave, duration, and interval, designed to be representative of existing approaches in the literature. By comparing these models through topological metrics, entropy analysis, and divergence with respect to inferred cognitive representations, we assessed both their structural and perceptual efficiency. Our findings reveal that simpler, feature-specific models better match human perception, whereas complex, multidimensional representations introduce cognitive inefficiencies. These results support the view that humans rely on modular, parallel cognitive networks--an architecture consistent with theories of predictive processing and free energy minimization. Moreover, we find that musical networks are structurally organized to guide attention toward transitions that are both uncertain and inferable. The resulting structure concentrates uncertainty in a few frequently visited nodes, creating local entropy gradients that alternate between stable and unpredictable regions, thereby enabling the expressive dynamics of tension and release that define the musical experience. These findings show that network structures make the organization of uncertainty in music observable, offering new insight into how patterned flows of expectation shape perception, and open new directions for studying how musical structures evolve across genres, cultures, and historical periods through the lens of network science. |
| 2025-09-17 | [Day-Ahead Transmission Grid Topology Optimization Considering Renewable Energy Sources' Uncertainty](http://arxiv.org/abs/2509.13994v1) | Giacomo Bastianel, Dirk Van Hertem et al. | The increasing renewable penetration introduces significant uncertainty in power system operations. At the same time, the existing transmission grid is often already congested, and urgently needed reinforcements are frequently delayed due to several constraints. To address these challenges, adjusting the grid topology based on congestion patterns is considered a non-costly remedy to guarantee efficient power transmission. Based on this idea, this paper proposes a grid topology optimization model combining optimal transmission switching and busbar splitting for AC and hybrid AC/DC grids. The methodology incorporates RES forecast uncertainty through a scenario-based stochastic optimization approach, using real offshore wind data and K-means clustering to generate representative forecast error scenarios. The proposed model includes several formulations to be compared with a plain optimal power flow (OPF) model: hourly optimizing the topology, one topology for 24 hours, or a limited number of switching actions over a day. The grid topology optimization model is formulated as a Mixed-Integer Quadratic Convex Problem, optimized based on the day-ahead (D-1) RES forecast and validated for AC-feasibility via an AC-OPF formulation. Based on the generation setpoints of the feasibility check, a redispatch simulation based on the measured (D) RES realization is then computed. The methodology is tested on an AC 30-bus test case and a hybrid AC/DC 50-bus test case, for a 24-hours (30-bus) and a 14-days (both test cases) time series. The results highlight the economic benefits brought by grid topology optimization for congested test cases with high penetration of RES. In addition, the results demonstrate that accounting for RES uncertainty with at least 6 to 8 scenarios leads to lower or comparable total costs to deterministic day-ahead forecasts, even when limiting the frequency of topological actions. |
| 2025-09-17 | [Improved systematic evaluation of a strontium optical clock with uncertainty below $1\times 10^{-18}$](http://arxiv.org/abs/2509.13991v1) | Zhi-Peng Jia, Jie Li et al. | We report a systematic uncertainty of $9.2\times 10^{-19}$ for the USTC Sr1 optical lattice clock, achieving accuracy at the level required for the roadmap of the redefinition of the SI second. A finite-element model with {\it in situ}-validated, spatially-resolved chamber emissivity reduced blackbody radiation shift uncertainty to $6.3\times 10^{-19}$. Concurrently, an externally mounted lattice cavity combined with a larger beam waist suppressed density shifts. Enhanced lattice depth modulation consolidated lattice light shift uncertainty to $6.3\times 10^{-19}$ by enabling simultaneous determination of key polarizabilities and magic wavelength. Magnetic shifts were resolved below $10^{-18}$ via precision characterization of the second-order Zeeman coefficient. Supported by a crystalline-coated ultra-low-expansion cavity-stabilized laser and refined temperature control suppressing BBR fluctuations, the clock also achieves a frequency stability better than $1\times10^{-18}$ at 30,000-s averaging time. These developments collectively establish a new benchmark in USTC Sr1 clock performance and pave the way for high-accuracy applications in metrology and fundamental physics. |
| 2025-09-17 | [Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game](http://arxiv.org/abs/2509.13985v1) | Yixun Wen, Yulong Gao et al. | Generalized Nash equilibrium problem (GNEP) is fundamental for practical applications where multiple self-interested agents work together to make optimal decisions. In this work, we study GNEP with shared distributionally robust chance constraints (DRCCs) for incorporating inevitable uncertainties. The DRCCs are defined over the Wasserstein ball, which can be explicitly characterized even with limited sample data. To determine the equilibrium of the GNEP, we propose an exact approach to transform the original computationally intractable problem into a deterministic formulation using the Nikaido-Isoda function. Specifically, we show that when all agents' objectives are quadratic in their respective variables, the equilibrium can be obtained by solving a typical mixed-integer nonlinear programming (MINLP) problem, where the integer and continuous variables are decoupled in both the objective function and the constraints. This structure significantly improves computational tractability, as demonstrated through a case study on the charging station pricing problem. |
| 2025-09-16 | [Axion-photon conversion in transient compact stars: Systematics, constraints, and opportunities](http://arxiv.org/abs/2509.13322v1) | Damiano F. G. Fiorillo, √Ångel Gil Muyor et al. | We study magnetic conversion of ultra-relativistic axion-like particles (ALPs) into photons in compact-star environments, focusing on the hot, transient conditions of core-collapse supernova (SN) remnants and neutron-star mergers (NSMs). We address previously overlooked uncertainties, particularly the suppression caused by ejected matter near the stellar surface, a region crucial to the conversion process. We derive analytical expressions for the transition rate; they reveal the influence of key parameters and their uncertainties. We update constraints using historical gamma-ray data from SN~1987A and find $g_{a\gamma}<5\times10^{-12}~{\rm GeV}^{-1}$ for $m_a\lesssim10^{-9}$ meV. We also forecast sensitivities for a future Galactic SN and for NSMs, assuming observations with Fermi-LAT or similar gamma-ray instruments. We distinguish ALPs -- defined as coupling only to photons and produced via Primakoff scattering -- from axions, which also couple to nucleons and emerge through nuclear bremsstrahlung. We omit pionic axion production due to its large uncertainties and inconsistencies, though it could contribute comparably to bremsstrahlung under optimistic assumptions. For the compact sources, we adopt time-averaged one-zone models, guided by numerical simulations, to enable clear and reproducible parametric studies. |
| 2025-09-16 | [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](http://arxiv.org/abs/2509.13313v1) | Xixi Wu, Kuan Li et al. | Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents. |
| 2025-09-16 | [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](http://arxiv.org/abs/2509.13305v1) | Kuan Li, Zhongwang Zhang et al. | Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap. |
| 2025-09-16 | [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](http://arxiv.org/abs/2509.13262v1) | Zhizhong Zhao, Ke Chen | Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet existing methods are either computationally intensive, such as Bayesian or ensemble methods, or provide only partial, task-specific estimates, such as single-forward-pass techniques. In this paper, we propose a post-hoc single-forward-pass framework that jointly captures aleatoric and epistemic uncertainty without modifying or retraining pretrained models. Our method applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs) on each side. We prove that, under ideal conditions, the total MAR equals the harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency Discrepancy Score} (SDS) for fine-grained epistemic estimation across regression and classification. For regression, side-specific quantile regression yields prediction intervals with improved empirical coverage, which are further calibrated via SDS. For classification, when calibration data are available, we apply SPA-based calibration identities to adjust the softmax outputs and then compute predictive entropy on these calibrated probabilities. Extensive experiments on diverse regression and classification benchmarks demonstrate that our framework matches or exceeds several state-of-the-art UQ methods while incurring minimal overhead.   Our source code is available at https://github.com/zzz0527/SPC-UQ. |
| 2025-09-16 | [SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty](http://arxiv.org/abs/2509.13189v1) | Zhao Feng, Bicheng Yan et al. | We present a direct inverse modeling method named SURGIN, a SURrogate-guided Generative INversion framework tailed for subsurface multiphase flow data assimilation. Unlike existing inversion methods that require adaptation for each new observational configuration, SURGIN features a zero-shot conditional generation capability, enabling real-time assimilation of unseen monitoring data without task-specific retraining. Specifically, SURGIN synergistically integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a score-based generative model (SGM), framing the conditional generation as a surrogate prediction-guidance process in a Bayesian perspective. Instead of directly learning the conditional generation of geological parameters, an unconditional SGM is first pretrained in a self-supervised manner to capture the geological prior, after which posterior sampling is performed by leveraging a differentiable U-FNO surrogate to enable efficient forward evaluations conditioned on unseen observations. Extensive numerical experiments demonstrate SURGIN's capability to decently infer heterogeneous geological fields and predict spatiotemporal flow dynamics with quantified uncertainty across diverse measurement settings. By unifying generative learning with surrogate-guided Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and uncertainty quantification in parametric functional spaces. |
| 2025-09-16 | [Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments](http://arxiv.org/abs/2509.13176v1) | Qiushi Bu, Wen Su et al. | We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility. |
| 2025-09-16 | [Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression](http://arxiv.org/abs/2509.13150v1) | Shima Mohammadi, Mohsen Jenadeleh et al. | Nowadays, image compression solutions are increasingly designed to operate within high-fidelity quality ranges, where preserving even the most subtle details of the original image is essential. In this context, the ability to detect and quantify subtle compression artifacts becomes critically important, as even slight degradations can impact perceptual quality in professional or quality sensitive applications, such as digital archiving, professional editing and web delivery. However, the performance of current objective image quality assessment metrics in this range has not been thoroughly investigated. In particular, it is not well understood how reliably these metrics estimate distortions at or below the threshold of Just Noticeable Difference (JND). This study directly addresses this issue by proposing evaluation methodologies for assessing the performance of objective quality metrics and performing a comprehensive evaluation using the JPEG AIC-3 dataset which is designed for high-fidelity image compression. Beyond conventional criteria, the study introduces Z-RMSE to incorporate subjective score uncertainty and applies novel statistical tests to assess significant differences between metrics. The analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity subsets, examines the impact of cropping in subjective tests, and a public dataset with benchmarks and evaluation tools is released to support further research. |
| 2025-09-16 | [An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios](http://arxiv.org/abs/2509.13132v1) | Zhihao Zhang, Chengyang Peng et al. | Autonomous driving in dense, dynamic environments requires decision-making systems that can exploit both spatial structure and long-horizon temporal dependencies while remaining robust to uncertainty. This work presents a novel framework that integrates multi-channel bird's-eye-view occupancy grids with transformer-based sequence modeling for tactical driving in complex roundabout scenarios. To address the imbalance between frequent low-risk states and rare safety-critical decisions, we propose the Uncertainty-Weighted Decision Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate per-token predictive entropy, which is then used as a weight in the student model's loss function. This mechanism amplifies learning from uncertain, high-impact states while maintaining stability across common low-risk transitions. Experiments in a roundabout simulator, across varying traffic densities, show that UWDT consistently outperforms other baselines in terms of reward, collision rate, and behavioral stability. The results demonstrate that uncertainty-aware, spatial-temporal transformers can deliver safer and more efficient decision-making for autonomous driving in complex traffic environments. |
| 2025-09-16 | [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](http://arxiv.org/abs/2509.13084v1) | Yunyao Lu, Yihang Wu et al. | Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments. |
| 2025-09-16 | [Physics potential of the IceCube Upgrade for atmospheric neutrino oscillations](http://arxiv.org/abs/2509.13066v1) | IceCube Collaboration | The IceCube Upgrade is an extension of the existing IceCube Neutrino Observatory and will be deployed in the 2025-2026 austral summer. It will significantly improve the sensitivity of the detector to atmospheric neutrino oscillations. The existing 86-string IceCube array contains a dense in-fill known as DeepCore which is optimized to measure neutrinos with energies down to a few GeV. The IceCube Upgrade will consist of seven new densely-instrumented strings placed within the DeepCore volume to further enhance the performance in the GeV energy range. The additional strings will feature new optical modules, each containing multiple PMTs, in contrast to the existing modules that each contain a single PMT. This will more than triple the number of PMT channels with respect to the current IceCube configuration, allowing for improved detection efficiency and reconstruction performance at GeV energies. We describe necessary updates to simulation, event selection, and reconstruction to accommodate the higher data rates observed by the upgraded detector and the addition of multi-PMT modules. We determine the expected sensitivity of the IceCube Upgrade to the atmospheric neutrino oscillation parameters sin$^2\theta_{23}$ and $\Delta m^2_{32}$, the appearance of tau neutrinos and the neutrino mass ordering. The IceCube Upgrade will provide neutrino oscillation measurements that are of similar precision to those from accelerator experiments, while providing complementarity by probing higher energies and longer baselines, and with different sources of systematic uncertainties. |
| 2025-09-15 | [Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks](http://arxiv.org/abs/2509.12199v1) | Asif Iqbal, Subhabrata Majumdar et al. | Precise determination of galaxy cluster masses is crucial for establishing reliable mass-observable scaling relations in cluster cosmology. We employ graph neural networks (GNNs) to estimate galaxy cluster masses from radially sampled profiles of the intra-cluster medium (ICM) inferred from X-ray observations. GNNs naturally handle inputs of variable length and resolution by representing each ICM profile as a graph, enabling accurate and flexible modeling across diverse observational conditions. We trained and tested GNN model using state-of-the-art hydrodynamical simulations of galaxy clusters from The Three Hundred Project. The mass estimates using our method exhibit no systematic bias compared to the true cluster masses in the simulations. Additionally, we achieve a scatter in recovered mass versus true mass of about 6\%, which is a factor of six smaller than obtained from a standard hydrostatic equilibrium approach. Our algorithm is robust to both data quality and cluster morphology and it is capable of incorporating model uncertainties alongside observational uncertainties. Finally, we apply our technique to XMM-Newton observed galaxy cluster samples and compare the GNN derived mass estimates with those obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our results provide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZ derived masses, with higher mass clusters exhibiting a greater degree of deviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$, albeit with significant dispersion due to its mass dependence. This work takes a significant step towards establishing unbiased observable mass scaling relations by integrating X-ray, SZ and optical datasets using deep learning techniques, thereby enhancing the role of galaxy clusters in precision cosmology. |
| 2025-09-15 | [Approaches to Analysis and Design of AI-Based Autonomous Vehicles](http://arxiv.org/abs/2509.12169v1) | Tao Yan, Zheyu Zhang et al. | Artificial intelligence (AI) models are becoming key components in an autonomous vehicle (AV), especially in handling complicated perception tasks. However, closing the loop through AI-based feedback may pose significant risks on reliability of autonomous driving due to very limited understanding about the mechanism of AI-driven perception processes. To overcome it, this paper aims to develop tools for modeling, analysis, and synthesis for a class of AI-based AV; in particular, their closed-loop properties, e.g., stability, robustness, and performance, are rigorously studied in the statistical sense. First, we provide a novel modeling means for the AI-driven perception processes by looking at their error characteristics. Specifically, three fundamental AI-induced perception uncertainties are recognized and modeled by Markov chains, Gaussian processes, and bounded disturbances, respectively. By means of that, the closed-loop stochastic stability (SS) is established in the sense of mean square, and then, an SS control synthesis method is presented within the framework of linear matrix inequalities (LMIs). Besides the SS properties, the robustness and performance of AI-based AVs are discussed in terms of a stochastic guaranteed cost, and criteria are given to test the robustness level of an AV when in the presence of AI-induced uncertainties. Furthermore, the stochastic optimal guaranteed cost control is investigated, and an efficient design procedure is developed innovatively based on LMI techniques and convex optimization. Finally, to illustrate the effectiveness, the developed results are applied to an example of car following control, along with extensive simulation. |
| 2025-09-15 | [Superresolving Non-linear PDE Dynamics with Reduced-Order Autodifferentiable Ensemble Kalman Filtering For Turbulence Modeling and Flow Regulation](http://arxiv.org/abs/2509.12149v1) | Mrigank Dhingra, Omer San | Accurately reconstructing and forecasting high-resolution (HR) states from computationally cheap low-resolution (LR) observations is central to estimation-and-control of spatio-temporal PDE systems. We develop a unified superresolution pipeline based on the reduced-order autodifferentiable Ensemble Kalman filter (ROAD-EnKF). The method learns a low-dimensional latent dynamics model and a nonlinear decoder from latent variables to HR fields; the learned pair is embedded in an EnKF, enabling simultaneous state estimation and control-oriented forecasting with quantified uncertainty. We evaluate on three benchmarks: 1-D viscous Burgers equation (shock formation), Kuramoto-Sivashinsky (KS) equation (chaotic dynamics), and 2-D Navier-Stokes-Kraichnan turbulence (NSKT) (vortex decaying dynamics at Re 16,000). LR data are obtained by factors of 4-8 downsampling per spatial dimension and are corrupted with noise. On Burgers and KS, the latent models remain stable far beyond the observation window, accurately predicting shock propagation and chaotic attractor statistics up to 150 steps. On 2-D NSKT, the approach preserves the kinetic-energy spectrum and enstrophy budget of the HR data, indicating suitability for control scenarios that depend on fine-scale flow features. These results position ROAD-EnKF as a principled and efficient framework for physics-constrained superresolution, bridging LR sensing and HR actuation across diverse PDE regimes. |
| 2025-09-15 | [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](http://arxiv.org/abs/2509.12094v1) | Tianqi Zhao, Russa Biswas et al. | Graph machine learning models often achieve similar overall performance yet behave differently at the node level, failing on different subsets of nodes with varying reliability. Standard evaluation metrics such as accuracy obscure these fine grained differences, making it difficult to diagnose when and where models fail. We introduce NodePro, a node profiling framework that enables fine-grained diagnosis of model behavior by assigning interpretable profile scores to individual nodes. These scores combine data-centric signals, such as feature dissimilarity, label uncertainty, and structural ambiguity, with model-centric measures of prediction confidence and consistency during training. By aligning model behavior with these profiles, NodePro reveals systematic differences between models, even when aggregate metrics are indistinguishable. We show that node profiles generalize to unseen nodes, supporting prediction reliability without ground-truth labels. Finally, we demonstrate the utility of NodePro in identifying semantically inconsistent or corrupted nodes in a structured knowledge graph, illustrating its effectiveness in real-world settings. |
| 2025-09-15 | [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](http://arxiv.org/abs/2509.12043v1) | Mayur Patil, Qadeer Ahmed et al. | Traffic flow forecasting is essential for managing congestion, improving safety, and optimizing various transportation systems. However, it remains a prevailing challenge due to the stochastic nature of urban traffic and environmental factors. Better predictions require models capable of accommodating the traffic variability influenced by multiple dynamic and complex interdependent factors. In this work, we propose a Graph Neural Network (GNN) framework to address the stochasticity by leveraging adaptive adjacency matrices using log-normal distributions and Coefficient of Variation (CV) values to reflect real-world travel time variability. Additionally, weather factors such as temperature, wind speed, and precipitation adjust edge weights and enable GNN to capture evolving spatio-temporal dependencies across traffic stations. This enhancement over the static adjacency matrix allows the model to adapt effectively to traffic stochasticity and changing environmental conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP) framework to provide reliable uncertainty quantification, achieving target coverage while maintaining acceptable prediction intervals. Experimental results demonstrate that the proposed model, in comparison with baseline methods, showed better prediction accuracy and uncertainty bounds. We, then, validate this method by constructing traffic scenarios in SUMO and applying Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under Test (VUT) to reflect real-world variability. The simulated mean travel time of the VUT falls within the intervals defined by INRIX historical data, verifying the model's robustness. |
| 2025-09-15 | [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](http://arxiv.org/abs/2509.12034v1) | Emmanuel Adjei Domfeh, Christopher L. Dancy | In high-stakes disaster scenarios, timely and informed decision-making is critical yet often challenged by uncertainty, dynamic environments, and limited resources. This paper presents a systematic review of Human-AI collaboration patterns that support decision-making across all disaster management phases. Drawing from 51 peer-reviewed studies, we identify four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Within these, we analyze sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. Our review highlights how AI systems may enhance situational awareness, improves response efficiency, and support complex decision-making, while also surfacing critical limitations in scalability, interpretability, and system interoperability. We conclude by outlining key challenges and future research directions, emphasizing the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and equitable recovery outcomes. |
| 2025-09-15 | [Learning from Uncertain Similarity and Unlabeled Data](http://arxiv.org/abs/2509.11984v1) | Meng Wei, Zhongnian Li et al. | Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches. |
| 2025-09-15 | [Probabilistic modelling of atmosphere-surface coupling with a copula Bayesian network](http://arxiv.org/abs/2509.11975v1) | Laura Mack, Marvin K√§hnert et al. | Land-atmosphere coupling is an important process for correctly modelling near-surface temperature profiles, but it involves various uncertainties due to subgrid-scale processes, such as turbulent fluxes or unresolved surface heterogeneities, suggesting a probabilistic modelling approach. We develop a copula Bayesian network (CBN) to interpolate temperature profiles, acting as alternative to T2m-diagnostics used in numerical weather prediction (NWP) systems. The new CBN results in (1) a reduction of the warm bias inherent to NWP predictions of wintertime stable boundary layers allowing cold temperature extremes to be better represented, and (2) consideration of uncertainty associated with subgrid-scale spatial variability. The use of CBNs combines the advantages of uncertainty propagation inherent to Bayesian networks with the ability to model complex dependence structures between random variables through copulas. By combining insights from copula modelling and information entropy, criteria for the applicability of CBNs in the further development of parameterizations in NWP models are derived. |
| 2025-09-15 | [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](http://arxiv.org/abs/2509.11966v1) | Sangjoon Park, Yeonjong Shin et al. | Poroelasticity -- coupled fluid flow and elastic deformation in porous media -- often involves spatially variable permeability, especially in subsurface systems. In such cases, simulations with random permeability fields are widely used for probabilistic analysis, uncertainty quantification, and inverse problems. These simulations require repeated forward solves that are often prohibitively expensive, motivating the development of efficient surrogate models. However, efficient surrogate modeling techniques for poroelasticity with random permeability fields remain scarce. In this study, we propose a surrogate modeling framework based on the deep operator network (DeepONet), a neural architecture designed to learn mappings between infinite-dimensional function spaces. The proposed surrogate model approximates the solution operator that maps random permeability fields to transient poroelastic responses. To enhance predictive accuracy and stability, we integrate three strategies: nondimensionalization of the governing equations, input dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step training procedure that decouples the optimization of branch and trunk networks. The methodology is evaluated on two benchmark problems in poroelasticity: soil consolidation and ground subsidence induced by groundwater extraction. In both cases, the DeepONet achieves substantial speedup in inference while maintaining high predictive accuracy across a wide range of permeability statistics. These results highlight the potential of the proposed approach as a scalable and efficient surrogate modeling technique for poroelastic systems with random permeability fields. |
| 2025-09-15 | [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](http://arxiv.org/abs/2509.11964v1) | Junyoung Kim, Minsik Jeon et al. | Semantic mapping aims to construct a 3D semantic representation of the environment, providing essential knowledge for robots operating in complex outdoor settings. While Bayesian Kernel Inference (BKI) addresses discontinuities of map inference from sparse sensor data, existing semantic mapping methods suffer from various sources of uncertainties in challenging outdoor environments. To address these issues, we propose an uncertainty-aware semantic mapping framework that handles multiple sources of uncertainties, which significantly degrade mapping performance. Our method estimates uncertainties in semantic predictions using Evidential Deep Learning and incorporates them into BKI for robust semantic inference. It further aggregates noisy observations into coherent Gaussian representations to mitigate the impact of unreliable points, while employing geometry-aligned kernels that adapt to complex scene structures. These Gaussian primitives effectively fuse local geometric and semantic information, enabling robust, uncertainty-aware mapping in complex outdoor scenarios. Comprehensive evaluation across diverse off-road and urban outdoor environments demonstrates consistent improvements in mapping quality, uncertainty calibration, representational flexibility, and robustness, while maintaining real-time efficiency. |
| 2025-09-12 | [The CHARA Array Polarization Model and Prospects for Spectropolarimetry](http://arxiv.org/abs/2509.10451v1) | Linling Shuai, John D. Monnier et al. | Polarimetric data provide key insights into infrared emission mechanisms in the inner disks of YSOs and the details of dust formation around AGB stars. While polarization measurements are well-established in radio interferometry, they remain challenging at visible and near-infrared due to the significant time-variable birefringence introduced by the complex optical beamtrain. In this study, we characterize instrumental polarization effects within the optical path of the CHARA Array, focusing on the H-band MIRC-X and K-band MYSTIC beam combiners. Using Jones matrix formalism, we developed a comprehensive model describing diattenuation and retardance across the array. By applying this model to an unpolarized calibrator, we derived the instrumental parameters for both MIRC-X and MYSTIC. Our results show differential diattenuation consistent with >= 97% reflectivity per aluminum-coated surface at 45 deg incidence. The differential retardance exhibits small wavelength-dependent variations, in some cases larger than we expected. Notably, telescope W2 exhibits a significantly larger phase shift in the Coude path, attributable to a fixed aluminum mirror (M4) used in place of deformable mirrors present on the other telescopes during the observing run. We also identify misalignments in the LiNbO_3 birefringent compensator plates on S1 (MIRC-X) and W2 (MYSTIC). After correcting for night-to-night offsets, we achieve calibration accuracies of $\pm$ 3.4% in visibility ratio and $\pm$ 1.4 deg in differential phase for MIRC-X, and $\pm$ 5.9% and $\pm$ 2.4 deg, respectively, for MYSTIC. Given that the differential intrinsic polarization of spatially resolved sources, such as AGB stars and YSOs, typically greater than these instrumental uncertainties, our results demonstrate that CHARA is now capable of achieving high-accuracy measurements of intrinsic polarization in astrophysical targets. |
| 2025-09-12 | [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](http://arxiv.org/abs/2509.10419v1) | Francesco Vitale, Tommaso Zoppi et al. | Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability. |
| 2025-09-12 | [A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives](http://arxiv.org/abs/2509.10393v1) | Cl√©mentine Chazal, Heishiro Kanagawa et al. | Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called 'gradient discrepancy', and in particular a 'kernel gradient discrepancy' (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel charasterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained. To illustrate this point several novel algorithms are proposed, including a natural generalisation of Stein variational gradient descent, with applications to mean-field neural networks and prediction-centric uncertainty quantification presented. On the theoretical side, our principal contribution is to establish sufficient conditions for desirable properties of KGD, such as continuity and convergence control. |
| 2025-09-12 | [Vendi Information Gain for Active Learning and its Application to Ecology](http://arxiv.org/abs/2509.10390v1) | Quan Nguyen, Adji Bousso Dieng | While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG achieves impressive predictive accuracy close to full supervision using less than 10% of the labels. It consistently outperforms standard baselines across metrics and batch sizes, collecting more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments. |
| 2025-09-12 | [Theory uncertainties of the irreducible background to VBF Higgs production](http://arxiv.org/abs/2509.10368v1) | Xuan Chen, Silvia Ferrario Ravasio et al. | Higgs boson production through gluon fusion in association with two jets is an irreducible background to Higgs boson production through vector boson fusion, one of the most important channels for analyzing and understanding the Higgs boson properties at the Large Hadron Collider. Despite a range of available simulation tools, precise predictions for the corresponding final states are notoriously hard to achieve. Using state-of-the-art fixed-order calculations as the baseline for a comparison, we perform a detailed study of similarities and differences in existing event generators. We provide consistent setups for the simulations that can be used to obtain identical parametric precision in various programs used by experiments. We find that NLO calculations for the two-jet final state are essential to achieve reliable predictions. |
| 2025-09-12 | [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](http://arxiv.org/abs/2509.10348v1) | Yehudit Aperstein, Amit Tzahar et al. | Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings. |
| 2025-09-12 | [OpenCSP: A Deep Learning Framework for Crystal Structure Prediction from Ambient to High Pressure](http://arxiv.org/abs/2509.10293v1) | Yinan Wang, Xiaoyang Wang et al. | High-pressure crystal structure prediction (CSP) underpins advances in condensed matter physics, planetary science, and materials discovery. Yet, most large atomistic models are trained on near-ambient, equilibrium data, leading to degraded stress accuracy at tens to hundreds of gigapascals and sparse coverage of pressure-stabilized stoichiometries and dense coordination motifs. Here, we introduce OpenCSP, a machine learning framework for CSP tasks spanning ambient to high-pressure conditions. This framework comprises an open-source pressure-resolved dataset alongside a suite of publicly available atomistic models that are jointly optimized for accuracy in energy, force, and stress predictions. The dataset is constructed via randomized high-pressure sampling and iteratively refined through an uncertainty-guided concurrent learning strategy, which enriches underrepresented compression regimes while suppressing redundant DFT labeling. Despite employing a training corpus one to two orders of magnitude smaller than those of leading large models, OpenCSP achieves comparable or superior performance in high-pressure enthalpy ranking and stability prediction. Across benchmark CSP tasks spanning a wide pressure window, our models match or surpass MACE-MPA-0, MatterSim v1 5M, and GRACE-2L-OAM, with the largest gains observed at elevated pressures. These results demonstrate that targeted, pressure-aware data acquisition coupled with scalable architectures enables data-efficient, high-fidelity CSP, paving the way for autonomous materials discovery under ambient and extreme conditions. |
| 2025-09-12 | [Astroclimes -- measuring the abundance of CO$_2$ and CH$_4$ in the Earth's atmosphere using astronomical observations](http://arxiv.org/abs/2509.10258v1) | Marcelo Aron Fetzner Keniger, David Armstrong et al. | Monitoring the abundance of greenhouse gases (GHGs) such as carbon dioxide (CO$_2$) and methane (CH$_4$) is necessary to quantify their impact on global warming and climate change. Although a number of satellites and ground-based networks measure the total column volume mixing ratio (VMR) of these gases, they rely on sunlight, and column measurements at night are comparatively scarce. We present a new algorithm, Astroclimes, that hopes to complement and extend nighttime CO$_2$ and CH4 column measurements. Astroclimes can measure the abundance of GHGs on Earth by generating a model telluric transmission spectra and fitting it to the spectra of telluric standard stars in the near-infrared taken by ground-based telescopes. A Markov Chain Monte Carlo (MCMC) analysis on an extensive dataset from the CARMENES spectrograph showed that Astroclimes was able to recover the long term trend known to be present in the molecular abundances of both CO$_2$ and CH$_4$, but not their seasonal cycles. Using the Copernicus Atmosphere Monitoring Service (CAMS) global greenhouse gas reanalysis model (EGG4) as a benchmark, we identified an overall vertical shift in our data and quantified the long term scatter in our retrievals. The scatter on a 1 hour timescale, however, is much lower, and is on par with the uncertainties on individual measurements. Although currently the precision of the method is not in line with state of the art techniques using dedicated instrumentation, it shows promise for further development. |
| 2025-09-12 | [Learning Constraint Surrogate Model for Two-stage Stochastic Unit Commitment](http://arxiv.org/abs/2509.10246v1) | Amir Bahador Javadi, Amin Kargarian et al. | The increasing penetration of renewable energy sources introduces significant uncertainty in power system operations, making traditional deterministic unit commitment approaches computationally expensive. This paper presents a machine learning surrogate modeling approach designed to reformulate the feasible design space of the two-stage stochastic unit commitment (TSUC) problem, reducing its computational complexity. The proposed method uses a support vector machine (SVM) to construct a surrogate model based on the governing equations of the learner. This model replaces the original 2|L| * |S| transmission line flow constraints, where |S| is the number of uncertainty scenarios and |L| is the number of transmission lines with |S| much less than |L|, with a significantly reduced set of 1 * |S| linear inequality constraints. The approach is theoretically grounded in the polyhedral structure of the feasible region under the DC power flow approximation, enabling the transformation of 2|L| line flow limit constraints into a single linear constraint. The surrogate model is trained using data generated from computationally efficient DC optimal power flow simulations. Simulation results on the IEEE 57-bus and 118-bus systems demonstrate SVM halfspace constraint accuracy of 99.72% and 99.88%, respectively, with TSUC computational time reductions of 46% and 31% and negligible generation cost increases (0.63% and 0.88% on average for IEEE 57- and 118-bus systems, respectively). This shows the effectiveness of the proposed approach for practical power system operations under renewable energy uncertainty. |
| 2025-09-12 | [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](http://arxiv.org/abs/2509.10227v1) | √Ångel Ladr√≥n, Miguel S√°nchez-Dom√≠nguez et al. | Fatigue life prediction is essential in both the design and operational phases of any aircraft, and in this sense safety in the aerospace industry requires early detection of fatigue cracks to prevent in-flight failures. Robust and precise fatigue life predictors are thus essential to ensure safety. Traditional engineering methods, while reliable, are time consuming and involve complex workflows, including steps such as conducting several Finite Element Method (FEM) simulations, deriving the expected loading spectrum, and applying cycle counting techniques like peak-valley or rainflow counting. These steps often require collaboration between multiple teams and tools, added to the computational time and effort required to achieve fatigue life predictions. Machine learning (ML) offers a promising complement to traditional fatigue life estimation methods, enabling faster iterations and generalization, providing quick estimates that guide decisions alongside conventional simulations.   In this paper, we present a ML-based pipeline that aims to estimate the fatigue life of different aircraft wing locations given the flight parameters of the different missions that the aircraft will be operating throughout its operational life. We validate the pipeline in a realistic use case of fatigue life estimation, yielding accurate predictions alongside a thorough statistical validation and uncertainty quantification. Our pipeline constitutes a complement to traditional methodologies by reducing the amount of costly simulations and, thereby, lowering the required computational and human resources. |
| 2025-09-11 | [Cosmology inference with perturbative forward modeling at the field level: a comparison with joint power spectrum and bispectrum analyses](http://arxiv.org/abs/2509.09673v1) | Kazuyuki Akitsu, Marko Simonoviƒá et al. | We extend field-level inference to jointly constrain the cosmological parameters $\{A,\omega_{\rm cdm},H_0\}$, in both real and redshift space. Our analyses are based on mock data generated using a perturbative forward model, with noise drawn from a Gaussian distribution with a constant power spectrum. This idealized setting, where the field-level likelihood is exactly Gaussian, allows us to precisely quantify the information content in the nonlinear field on large scales. We find that field-level inference accurately recovers all cosmological parameters in both real and redshift space, with uncertainties consistent with perturbation theory expectations. We show that these error bars are comparable to those obtained from a joint power spectrum and bispectrum analysis using the same perturbative model. Finally, we perform several tests using the Gaussian field-level likelihood to fit the mock data where the true noise model is non-Gaussian, and find significant biases in the inferred cosmological parameters. These results highlight that the success of field-level inference critically depends on using the correct likelihood, which may be the primary challenge for applying this method to smaller scales even in the perturbative regime. |
| 2025-09-11 | [Measuring Epistemic Humility in Multimodal Large Language Models](http://arxiv.org/abs/2509.09658v1) | Bingkui Tong, Jiaer Xia et al. | Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench. |
| 2025-09-11 | [Reconstructing the origin of black hole mergers using sparse astrophysical models](http://arxiv.org/abs/2509.09647v1) | V. Gayathri, Giuliano Iorio et al. | The astrophysical origin of binary black hole mergers discovered by LIGO and Virgo remains uncertain. Efforts to reconstruct the processes that lead to mergers typically rely on either astrophysical models with fixed parameters, or continuous analytical models that can be fit to observations. Given the complexity of astrophysical formation mechanisms, these methods typically cannot fully take into account model uncertainties, nor can they fully capture the underlying processes. Here, we present a merger population analysis that can take a discrete set of simulated model distributions as its input to interpret observations. The analysis can take into account multiple formation scenarios as fractional contributors to the total set of observations, and can naturally account for model uncertainties. We apply this technique to investigate the origin of black hole mergers observed by LIGO Virgo. Specifically, we consider a model of AGN assisted black hole merger distributions, exploring a range of AGN parameters along with several {{SEVN}} population synthesis models that vary in common envelope efficiency parameter ($\alpha$) and metallicity ($Z$). We estimate the posterior distributions for AGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs. The inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN sub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN sub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$. |
| 2025-09-11 | [Constraints on Ultra-heavy DM from TeV-PeV gamma-ray diffuse measurements](http://arxiv.org/abs/2509.09609v1) | Manuel Rocamora, Pedro De La Torre Luque et al. | Recent experiments have measured the Galactic $\gamma$-ray diffuse emission up to PeV energies, opening a window to study acceleration of Galactic cosmic rays and their propagation up to the cosmic-ray knee. Furthermore, these observations provide a powerful tool to set strong constraints into very-heavy dark matter particles, with masses in the TeV-PeV range. In this paper, we explore the potential of the newest observations of diffuse emissions at the Galactic plane from HAWC and LHAASO to probe this kind of dark matter over a wide mass range. Here, we model secondary emissions (inverse-Compton) from the electrons and positrons produced in the annihilation/decay of dark matter, on top of their prompt $\gamma$-ray emission, including the effects of absorption of high-energy photons via pair production. Furthermore, we show that including the astrophysical backgrounds (namely diffuse emission from cosmic-ray collisions or emission from unresolved sources) can significantly improve these limits. We find that the new measurements provided, specially by LHAASO with the combination of the WCDA and KM2A detectors, allow us to set strong constraints in decaying dark matter, being competitive and even improving the strongest constraints at the moment. We also highlight that these regions lead to constraints that are less affected by uncertainties from the dark matter distribution and discuss how CTA north and SWGO will be able to improve limits in this mass range. |
| 2025-09-11 | [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](http://arxiv.org/abs/2509.09599v1) | Ira J. S. Shokar, Rich R. Kerswell et al. | We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events. |
| 2025-09-11 | [ObjectReact: Learning Object-Relative Control for Visual Navigation](http://arxiv.org/abs/2509.09594v1) | Sourav Garg, Dustin Craggs et al. | Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/ |
| 2025-09-11 | [Unified Framework for Hybrid Aleatory and Epistemic Uncertainty Propagation via Decoupled Multi-Probability Density Evolution Method](http://arxiv.org/abs/2509.09535v1) | Yi Luo, Meng-Ze Lyu et al. | This paper presents a unified framework for uncertainty propagation in dynamical systems involving hybrid aleatory and epistemic uncertainties. The framework accommodates precise probabilistic, imprecise probabilistic, and non-probabilistic representations, including the distribution-free probability-box (p-box). A central aspect of the framework involves transforming the original uncertainty inputs into an augmented random space, yielding the primary challenge of determining the conditional probability density function (PDF) of the response quantity of interest given epistemic uncertainty parameters. The recently proposed decoupled multi-probability density evolution method (decoupled M-PDEM) is employed to numerically solve the conditional PDF for complex dynamical systems. Several numerical examples illustrate the applicability, efficiency, and accuracy of the proposed framework. These include a linear single-degree-of-freedom (SDOF) system subject to Gaussian white noise with its natural frequency modeled as a p-box, a 10-DOF hysteretic structure subject to imprecise seismic loads, and a crash box model with mixed random and interval system parameters. |
| 2025-09-11 | [Mapping of discrete range modulated proton radiograph to water-equivalent path length using machine learning](http://arxiv.org/abs/2509.09514v1) | Atiq Ur Rahman, Chun-Chieh Wang et al. | Objective. Proton beams enable localized dose delivery. Accurate range estimation is essential, but planning still relies on X-ray CT, which introduces uncertainty in stopping power and range. Proton CT measures water equivalent thickness directly but suffers resolution loss from multiple Coulomb scattering. We develop a data driven method that reconstructs water equivalent path length (WEPL) maps from energy resolved proton radiographs, bypassing intermediate reconstructions. Approach. We present a machine learning pipeline for WEPL from high dimensional radiographs. Data were generated with the TOPAS Monte Carlo toolkit, modeling a clinical nozzle and a patient CT. Proton energies spanned 70-230 MeV across 72 projection angles. Principal component analysis reduced input dimensionality while preserving signal. A conditional GAN with gradient penalty was trained for WEPL prediction using a composite loss (adversarial, MSE, SSIM, perceptual) to balance sharpness, accuracy, and stability. Main results. The model reached a mean relative WEPL deviation of 2.5 percent, an SSIM of 0.97, and a proton radiography gamma index passing rate of 97.1 percent (2 percent delta WEPL, 3 mm distance-to-agreement) on a simulated head phantom. Results indicate high spatial fidelity and strong structural agreement. Significance. WEPL can be mapped directly from proton radiographs with deep learning while avoiding intermediate steps. The method mitigates limits of analytic techniques and may improve treatment planning. Future work will tune the number of PCA components, include detector response, explore low dose settings, and extend multi angle data toward full proton CT reconstruction; it is compatible with clinical workflows. |
| 2025-09-11 | [Spin Constraints on 4U 1630-47 via combined Continuum Fitting and Reflection methods: a comparative study using Frequentist and Bayesian statistics](http://arxiv.org/abs/2509.09481v1) | Debtroy Das, Honghui Liu et al. | We present a comprehensive Bayesian spectral analysis of the black hole X-ray binary 4U 1630-47 during its 2022 outburst, using simultaneous \textit{NICER} and \textit{NuSTAR} observations. Using the traditional frequentist approach, we build our model combining reflection spectroscopy with continuum fitting techniques and analyse the data. In the Bayesian framework, we jointly constrain the black hole's spin, mass, inclination, and distance within a unified framework. Employing nested sampling, we capture parameter degeneracies and rigorously propagate both statistical and systematic uncertainties. Our results yield robust and precise spin measurements from both approaches. Our Bayesian analysis fetches spin $a_*= 0.93_{-0.04}^{+0.05}$, mass $M_{\rm BH} = 9.0_{-2.0}^{+2.0} \, M_\odot$, distance $d_{\rm BH} = 10.5_{-1.2}^{+1.3}$~kpc, and inclination angle $i=53.8_{-1.3}^{+1.3}$~deg. It also demonstrates the power of Bayesian inference in fetching valuable insights into the complex physics of black hole accretion and enabling high-confidence measurements of fundamental parameters. |
| 2025-09-11 | [Dark Vector Boson Bremsstrahlung: New Form Factors for a Broader Class of Models](http://arxiv.org/abs/2509.09437v1) | Felix Kling, Peter Reimitz et al. | We explore the sensitivity of collider experiments to a broad class of GeV-scale dark vector models of new physics via production in proton and neutron bremsstrahlung and initial state radiation. This is achieved using a new physically motivated model for timelike vector form factors with generic charges for both protons and neutrons, which is fit to a variety of timelike and spacelike data with quantified uncertainties. The production model for both proton and neutron bremsstrahlung is applied to re-cast and extend the reach of existing FASER data to GeV-mass dark photons, $U(1)_B$, $U(1)_{B-L}$, and photophobic vectors, as well as forecasts for millicharged particles at FORMOSA. |
| 2025-09-10 | [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](http://arxiv.org/abs/2509.08777v1) | Eric Slyman, Mehrab Tanjim et al. | Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation. |
| 2025-09-10 | [FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model](http://arxiv.org/abs/2509.08742v1) | Yanlong Wang, Jian Xu et al. | Financial time series forecasting is both highly significant and challenging. Previous approaches typically standardized time series data before feeding it into forecasting models, but this encoding process inherently leads to a loss of important information. Moreover, past time series models generally require fixed numbers of variables or lookback window lengths, which further limits the scalability of time series forecasting. Besides, the interpretability and the uncertainty in forecasting remain areas requiring further research, as these factors directly impact the reliability and practical value of predictions. To address these issues, we first construct a diverse financial image-text dataset (FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization (UARPO) method to enable the model not only output predictions but also analyze the uncertainty of those predictions. We then proposed FinZero, a multimodal pre-trained model finetuned by UARPO to perform reasoning, prediction, and analytical understanding on the FVLDB financial time series. Extensive experiments validate that FinZero exhibits strong adaptability and scalability. After fine-tuning with UARPO, FinZero achieves an approximate 13.48\% improvement in prediction accuracy over GPT-4o in the high-confidence group, demonstrating the effectiveness of reinforcement learning fine-tuning in multimodal large model, including in financial time series forecasting tasks. |
| 2025-09-10 | [Quantifying model prediction sensitivity to model-form uncertainty](http://arxiv.org/abs/2509.08708v1) | Teresa Portone, Rebekah D. White et al. | Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration). |
| 2025-09-10 | [Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of X-ray instruments used in the large, multi-mission observational campaign](http://arxiv.org/abs/2509.08649v1) | XRISM collaboration | Accurate X-ray spectroscopic measurements are fundamental for deriving basic physical parameters of the most abundant baryon components in the Universe. The plethora of X-ray observatories currently operational enables a panchromatic view of the high-energy emission of celestial sources. However, uncertainties in the energy-dependent calibration of the instrument transfer functions (e.g. the effective area, energy redistribution, or gain) can limit - and historically, did limit - the accuracy of X-ray spectroscopic measurements.   We revised the status of the cross-calibration among the scientific payload on board four operation missions: Chandra, NuSTAR, XMM-Newton, and the recently launched XRISM. XRISM carries the micro-calorimeter Resolve, which yields the best energy resolution at energies above 2 keV. For this purpose, we used the data from a 10-day-long observational campaign targeting the nearby active galactic nucleus NGC 3783, carried out in July 2024.   We present a novel model-independent method for assessing the cross-calibration status that is based on a multi-node spline of the spectra with the highest-resolving power (XRISM/Resolve in our campaign). We also estimated the impact of the intrinsic variability of NGC 3783 on the cross-calibration status due to the different time coverages of participating observatories and performed an empirical reassessment of the Resolve throughput at low energies.   Based on this analysis, we derived a set of energy-dependent correction factors of the observed responses, enabling a statistically robust analysis of the whole spectral dataset. They will be employed in subsequent papers describing the astrophysical results of the campaign. |
| 2025-09-10 | [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](http://arxiv.org/abs/2509.08638v1) | Rebecca Martin, Jay Patrikar et al. | Specialized machine learning models, regardless of architecture and training, are susceptible to failures in deployment. With their increasing use in high risk situations, the ability to audit these models by determining their operational design domain (ODD) is crucial in ensuring safety and compliance. However, given the high-dimensional input spaces, this process often requires significant human resources and domain expertise. To alleviate this, we introduce \coolname, an LLM-Agent centric framework for automated generation of semantically relevant test cases to search for failure modes in specialized black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit a uncertainty-aware failure distribution model on a learned text-embedding manifold by projecting the high-dimension input space to low-dimension text-embedding latent space. The LLM-Agent is tasked with iteratively building the failure landscape by leveraging tools for generating test-cases to probe the model-under-test (MUT) and recording the response. The agent also guides the search using tools to probe uncertainty estimate on the low dimensional manifold. We demonstrate this process in a simple case using models trained with missing digits on the MNIST dataset and in the real world setting of vision-based intruder detection for aerial vehicles. |
| 2025-09-10 | [Isotope shift spectroscopy in mercury vapors: a valid alternative to ytterbium for new physics search](http://arxiv.org/abs/2509.08622v1) | Stefania Gravina, Antonio Castrillo et al. | Precision isotope shift metrology in the deep-UV region has been performed for all bosonic isotopes of mercury with a zero nuclear spin, by using the technique of frequency-comb referenced, wavelength-modulated, saturated absorption spectroscopy. The absolute center frequencies of the 6s$^2$ $^1$S$_0$ $\rightarrow$ 6s6p $^3$P$_1$ intercombination line have been measured with precision in the range of 2.5 - 5.9 10$^{-12}$, in temperature-stabilized mercury vapor samples with natural abundances. Frequency shifts in four isotope pairs have been determined with unprecedented accuracy, the global uncertainty being improved by a factor greater than 20 with respect to the best experimental data of the past literature. Our data set, when combined with previous measurements on the 6s6p $^3$P$_2$$\rightarrow$6s7s $^3$S$_1$ transition at 546 nm, allows us to build a King plot that reveals a nonlinearity with a statistical significance of 4.6$\sigma$. |
| 2025-09-10 | [No-Knowledge Alarms for Misaligned LLMs-as-Judges](http://arxiv.org/abs/2509.08593v1) | Andr√©s Corrada-Emmanuel | If we use LLMs as judges to evaluate the complex decisions of other LLMs, who or what monitors the judges? Infinite monitoring chains are inevitable whenever we do not know the ground truth of the decisions by experts and we do not want to trust them. One way to ameliorate our evaluation uncertainty is to exploit the use of logical consistency between disagreeing experts. By observing how LLM judges agree and disagree while grading other LLMs, we can compute the only possible evaluations of their grading ability. For example, if two LLM judges disagree on which tasks a third one completed correctly, they cannot both be 100\% correct in their judgments. This logic can be formalized as a Linear Programming problem in the space of integer response counts for any finite test. We use it here to develop no-knowledge alarms for misaligned LLM judges. The alarms can detect, with no false positives, that at least one member or more of an ensemble of judges are violating a user specified grading ability requirement. |
| 2025-09-10 | [Interpretability as Alignment: Making Internal Understanding a Design Principle](http://arxiv.org/abs/2509.08592v1) | Aadit Sengupta, Pratinav Seth et al. | Large neural models are increasingly deployed in high-stakes settings, raising concerns about whether their behavior reliably aligns with human values. Interpretability provides a route to internal transparency by revealing the computations that drive outputs. We argue that interpretability especially mechanistic approaches should be treated as a design principle for alignment, not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer intuitive but correlational explanations, while mechanistic techniques like circuit tracing or activation patching yield causal insight into internal failures, including deceptive or misaligned reasoning that behavioral methods like RLHF, red teaming, or Constitutional AI may overlook. Despite these advantages, interpretability faces challenges of scalability, epistemic uncertainty, and mismatches between learned representations and human concepts. Our position is that progress on safe and trustworthy AI will depend on making interpretability a first-class objective of AI research and development, ensuring that systems are not only effective but also auditable, transparent, and aligned with human intent. |
| 2025-09-10 | [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](http://arxiv.org/abs/2509.08578v1) | Hong Liu | Timely and robust influenza incidence forecasting is critical for public health decision-making. To address this, we present MAESTRO, a Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves robustness by adaptively fusing multi-modal inputs-including surveillance, web search trends, and meteorological data-and leveraging a comprehensive spectro-temporal architecture. The model first decomposes time series into seasonal and trend components. These are then processed through a hybrid feature enhancement pipeline combining Transformer-based encoders, a Mamba state-space model for long-range dependencies, multi-scale temporal convolutions, and a frequency-domain analysis module. A cross-channel attention mechanism further integrates information across the different data modalities. Finally, a temporal projection head performs sequence-to-sequence forecasting, with an optional estimator to quantify prediction uncertainty. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO shows strong competitive performance, demonstrating a superior model fit and relative accuracy, achieving a state-of-the-art R-square of 0.956. Extensive ablations confirm the significant contributions of both multi-modal fusion and the spectro-temporal components. Our modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens.Our publicly available pipeline presents a powerful, unified framework, demonstrating the critical synergy of advanced spectro-temporal modeling and multi-modal data fusion for robust epidemiological forecasting. |
| 2025-09-10 | [Accelerating first-principles molecular-dynamics thermal conductivity calculations for complex systems](http://arxiv.org/abs/2509.08573v1) | Sandro Wieser, YuJie Cen et al. | Atomistic simulations of heat transport in complex materials are costly and hard to converge. This has led to the development of several noise reduction techniques applicable to equilibrium molecular-dynamics simulations. We analyze the performance of those strategies, taking InAs nanowires as our benchmark due to the diverse structures and complex phonon spectra of these quasi-1D systems. We demonstrate how, for low-thermal-conductivity systems, cepstral analysis can reduce computational demands while still delivering accurate results that do not require discarding arbitrary parts of the dataset. However, issues with this approach are revealed when treating high-thermal-conductivity systems, where the thermal conductivity is significantly underestimated. We discuss alternative methods to be used in that situation, relying on uncertainty propagation from independent simulations. We show that the contributions of the covariance matrix have to be included for a quantitative assessment of the error. The combination of these strategies with machine-learning interatomic potentials (MLIPs) provides an accelerated, robust workflow applicable to a diverse set of systems, as our examples using a highly transferable MACE potential illustrate. |
| 2025-09-09 | [Advanced Weights for IXPE Polarization Analysis](http://arxiv.org/abs/2509.07981v1) | Jack T. Dinsmore, Roger W. Romani | As the Imaging X-ray Polarimetry Explorer (IXPE) measures increasingly faint sources, the need for precise polarimetry extraction becomes paramount. In addition to previously described neural-net (NN) weights, we introduce here point-spread function weights and particle background weights, which can be critical for faint sources. In some cases these can be augmented by time/phase and energy weights. We provide a publicly available analysis tool to incorporate these new weights, validate our method on simulated data, and test it on archival IXPE observations. Together these weights decrease the area of the polarization uncertainty contour by a factor of two and will be essential for background-limited IXPE observations. |
| 2025-09-09 | [Early warning for lensed gravitational wave counterparts from time delays of their host galaxies observed in the optical](http://arxiv.org/abs/2509.07967v1) | Sourabh Magare, Anupreeta More et al. | Gravitational lensing of gravitational waves (GWs) can be leveraged to provide early-warning times of $\mathcal{O}({\rm hours})$ to $\mathcal{O}({\rm days})$ before the merger of Binary Neutron Stars (BNSs) and Neutron Star Black Holes (NSBHs). This in turn could enable electromagnetic (EM) telescopes to capture emissions surrounding the time of the merger. In this work, we assess the practicability of lensing-driven early-warning by analysing optical images of the lensed host galaxy to predict the arrival time of subsequent BNS/NSBH signals following the observation of the first signal. We produce mock lenses with image quality and resolution similar to images taken with the Hubble Space Telescope (HST) and the ground-based Hyper Suprime-Cam (HSC) on the Subaru telescope. We compare the time delay uncertainties between these two cases for typical lensed image configurations and multiplicity. These include doubles and quads, and among quads: the fold, cusp, cross image configurations. We find that time delay uncertainties for doubles are comparable for both HST and HSC mocks. On the other hand, quads tend to provide accurate time-delay predictions (typical relative error $\sim0.1$) with HST. Analysis of a real lens led to a difference in time-delay estimates of $\mathcal{O}(\rm days)$ between the predictions derived from HST and HSC data. Our work therefore strongly advocates the need for high-resolution EM observations of lensed host galaxies to feasibly enable lensing-driven early-warning. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: Redshift Calibration of the MagLim++ Lens Sample](http://arxiv.org/abs/2509.07964v1) | G. Giannini, A. Alarcon et al. | In this work, we derive and calibrate the redshift distribution of the MagLim++ lens galaxy sample used in the Dark Energy Survey Year 6 (DES Y6) 3x2pt cosmology analysis. The 3x2pt analysis combines galaxy clustering from the lens galaxy sample and weak gravitational lensing. The redshift distributions are inferred using the SOMPZ method - a Self-Organizing Map framework that combines deep-field multi-band photometry, wide-field data, and a synthetic source injection (Balrog) catalog. Key improvements over the DES Year 3 (Y3) calibration include a noise-weighted SOM metric, an expanded Balrog catalogue, and an improved scheme for propagating systematic uncertainties, which allows us to generate O($10^8$) redshift realizations that collectively span the dominant sources of uncertainty. These realizations are then combined with independent clustering-redshift measurements via importance sampling. The resulting calibration achieves typical uncertainties on the mean redshift of 1-2%, corresponding to a 20-30% average reduction relative to DES Y3. We compress the $n(z)$ uncertainties into a small number of orthogonal modes for use in cosmological inference. Marginalizing over these modes leads to only a minor degradation in cosmological constraints. This analysis establishes the MagLim++ sample as a robust lens sample for precision cosmology with DES Y6 and provides a scalable framework for future surveys. |
| 2025-09-09 | [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](http://arxiv.org/abs/2509.07961v1) | Valen Tagliabue, Leonard Dung | We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration. |
| 2025-09-09 | [Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation](http://arxiv.org/abs/2509.07957v1) | Shunlei Li, Longsen Gao et al. | Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: improved mitigation of spatially varying observational systematics with masking](http://arxiv.org/abs/2509.07943v1) | M. Rodr√≠guez-Monroy, N. Weaverdyck et al. | As photometric surveys reach unprecedented statistical precision, systematic uncertainties increasingly dominate large-scale structure probes relying on galaxy number density. Defining the final survey footprint is critical, as it excludes regions affected by artefacts or suboptimal observing conditions. For galaxy clustering, spatially varying observational systematics, such as seeing, are a leading source of bias. Template maps of contaminants are used to derive spatially dependent corrections, but extreme values may fall outside the applicability range of mitigation methods, compromising correction reliability. The complexity and accuracy of systematics modelling depend on footprint conservativeness, with aggressive masking enabling simpler, robust mitigation. We present a unified approach to define the DES Year 6 joint footprint, integrating observational systematics templates and artefact indicators that degrade mitigation performance. This removes extreme values from an initial seed footprint, leading to the final joint footprint. By evaluating the DES Year 6 lens sample MagLim++ plus plus on this footprint, we enhance the Iterative Systematics Decontamination (ISD) method, detecting non-linear systematic contamination and improving correction accuracy. While the mask's impact on clustering is less significant than systematics decontamination, it remains non-negligible, comparable to statistical uncertainties in certain w(theta) scales and redshift bins. Supporting coherent analyses of galaxy clustering and cosmic shear, the final footprint spans 4031.04 deg2, setting the basis for DES Year 6 1x2pt, 2x2pt, and 3x2pt analyses. This work highlights how targeted masking strategies optimise the balance between statistical power and systematic control in Stage-III and -IV surveys. |
| 2025-09-09 | [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](http://arxiv.org/abs/2509.07925v1) | Tuo Wang, Adithya Kulkarni et al. | Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ. |
| 2025-09-09 | [Forecasting dementia incidence](http://arxiv.org/abs/2509.07874v1) | J√©r√¥me R. Simons, Yuntao Chen et al. | This paper estimates the stochastic process of how dementia incidence evolves over time. We proceed in two steps: first, we estimate a time trend for dementia using a multi-state Cox model. The multi-state model addresses problems of both interval censoring arising from infrequent measurement and also measurement error in dementia. Second, we feed the estimated mean and variance of the time trend into a Kalman filter to infer the population level dementia process. Using data from the English Longitudinal Study of Aging (ELSA), we find that dementia incidence is no longer declining in England. Furthermore, our forecast is that future incidence remains constant, although there is considerable uncertainty in this forecast. Our two-step estimation procedure has significant computational advantages by combining a multi-state model with a time series method. To account for the short sample that is available for dementia, we derive expressions for the Kalman filter's convergence speed, size, and power to detect changes and conclude our estimator performs well even in short samples. |
| 2025-09-09 | [Are Humans as Brittle as Large Language Models?](http://arxiv.org/abs/2509.07869v1) | Jiahui Li, Sean Papay et al. | The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs. |
| 2025-09-09 | [Jet cone size dependence of single inclusive jet suppression due to jet quenching in Pb+Pb collisions at $\sqrt{s_{\rm NN}}=5.02$ TeV](http://arxiv.org/abs/2509.07842v1) | Qing-Fei Han, Man Xie et al. | Jet suppression in high-energy heavy-ion collisions results from jet energy loss and transverse-momentum broadening during jet propagation through the quark-gluon plasma (QGP). The jet cone size ($R$) dependence of this suppression offers crucial insights into the energy loss mechanisms and QGP transport properties. In our study, we implement a comprehensive approach within the perturbative QCD parton model that incorporates both elastic and inelastic energy loss mechanisms. For elastic processes the contribution from recoiling thermal partons reduces the net in-cone energy loss for a given jet radius. For inelastic processes, we account for the angular distribution of radiated gluons, the thermalization of soft gluons, and transverse-momentum broadening. Using this framework, we calculate the jet nuclear modification factors ($R_{AA}$) and their double ratios $R_{AA}(R=0.2-1.0)/R_{AA}(R=0.2)$, and systematically compare with ALICE, ATLAS and CMS data in 0-10\% and 30-50\% Pb+Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02~TeV. Numerical results show that $R_{AA}$ increases with the cone size $R$ because the in-cone energy loss decreases at larger radii. Specifically, as the radius $R$ grows, the probability for elastically scattered partons to escape the jet cone and the likelihood for radiated gluons to fall outside the cone both decrease, resulting in a net reduction of energy loss. The $R_{AA}$ double ratios are approximately unity for small radii ($R=0.4$ relative to $R=0.2$) and at high $p_{\rm T}\gtrsim200$ GeV$/c$, in agreement with the data within uncertainties. |
| 2025-09-08 | [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](http://arxiv.org/abs/2509.06938v1) | Praneet Suresh, Jack Stanley et al. | As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk. |
| 2025-09-08 | [Black-hole mass estimation through accretion disk spectral fitting for high-redshift blazars](http://arxiv.org/abs/2509.06933v1) | G. Kyriopoulos, M. Petropoulou et al. | High-redshift ($z>2$) blazars, with relativistic jets aligned toward us, probe the most powerful end of the active galactic nuclei (AGN) population. We aim at determining the black hole masses and mass accretion rates of high-$z$ blazars in a common framework that utilizes a Markov Chain Monte Carlo (MCMC) fitting method and the Shakura-Sunayev multi-temperature accretion disk model, accounting also for attenuation due to neutral hydrogen gas in the intergalactic medium (IGM). We compiled a sample of 23 high-redshift blazars from the literature with publicly available infrared-to-ultraviolet photometric data. We performed a Bayesian fit to the spectral energy distribution (SED) of the accretion disk, accounting for upper limits, and determined the black hole masses and mass accretion rates with their uncertainties. We also examined the impact of optical-ultraviolet attenuation due to gas in the IGM. We find that neglecting IGM attenuation in SED fits leads to systematically larger black-hole mass estimates and correspondingly lower Eddington ratios, with the bias becoming more severe at higher redshift. Our MCMC fits yield median black-hole masses in the range $\sim (10^{8}-10^{10})\,M_{\odot}$ and a broad distribution of median Eddington ratios ($\lambda_{\rm Edd}\sim 0.04 - 1$). Comparison with previous literature shows no clear method-dependent systematic offsets, although individual mass estimates can differ by up to a factor of a few. We also demonstrate that assumptions about black-hole spin introduce a systematic degeneracy. This work is to our knowledge the first systematic study to model the accretion-disk emission of a large sample of high-$z$ blazars within a single, consistent statistical framework. Our results emphasize the importance of accounting for IGM attenuation and of using uniform fitting methods when comparing disk-based black hole estimates across samples. |
| 2025-09-08 | [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](http://arxiv.org/abs/2509.06902v1) | Aivin V. Solatorio | Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty. |
| 2025-09-08 | [Stochastic modelling of cosmic-ray sources for Galactic diffuse emissions](http://arxiv.org/abs/2509.06857v1) | Anton Stall, Philipp Mertsch | Galactic diffuse emissions in gamma rays and neutrinos arise from interactions of cosmic rays with the interstellar medium and probe the cosmic-ray intensity away from the Solar system. Model predictions for those are influenced by the properties of cosmic-ray sources, and understanding the impact of cosmic-ray sources on Galactic diffuse emissions is key for interpreting measurements by LHAASO, Tibet AS-gamma, IceCube, and the upcoming SWGO. We consider supernova remnants as prototypical cosmic-ray sources and study the impact of their discreteness on the Galactic diffuse emissions in different source injection and near-source transport models in a stochastic Monte Carlo study. Three lessons exemplify the results of our simulations: First, the distributions of Galactic diffuse emission intensities can be described by a mixture model of stable laws and Gaussian distributions. Second, the maximal deviations caused by discrete sources across the sky depend on energy, reaching typically tens of percent in burst-like and energy-dependent escape scenarios but order unity or larger in a time-dependent diffusion scenario. Third, the additional model uncertainty from source stochasticity is subdominant in burst-like and energy-dependent escape scenarios, but becomes sizeable above some tens of TeV in the time-dependent diffusion scenario, where it can help reconcile model predictions with LHAASO measurements. With increased spatial resolution, especially at energies beyond tens of TeV, measurements of Galactic diffuse emissions can be expected to constrain source models and locate cosmic ray sources. |
| 2025-09-08 | [Adversarial Obstacle Placement with Spatial Point Processes for Optimal Path Disruption](http://arxiv.org/abs/2509.06837v1) | Li Zhou, Elvan Ceyhan et al. | We investigate the Optimal Obstacle Placement (OOP) problem under uncertainty, framed as the dual of the Optimal Traversal Path problem in the Stochastic Obstacle Scene paradigm. We consider both continuous domains, discretized for analysis, and already discrete spatial grids that form weighted geospatial networks using 8-adjacency lattices. Our unified framework integrates OOP with stochastic geometry, modeling obstacle placement via Strauss (regular) and Mat\'ern (clustered) processes, and evaluates traversal using the Reset Disambiguation algorithm. Through extensive Monte Carlo experiments, we show that traversal cost increases by up to 40% under strongly regular placements, while clustered configurations can decrease traversal costs by as much as 25% by leaving navigable corridors compared to uniform random layouts. In mixed (with both true and false obstacles) scenarios, increasing the proportion of true obstacles from 30% to 70% nearly doubles the traversal cost. These findings are further supported by statistical analysis and stochastic ordering, providing rigorous insights into how spatial patterns and obstacle compositions influence navigation under uncertainty. |
| 2025-09-08 | [Uncertainty Principle from Operator Asymmetry](http://arxiv.org/abs/2509.06760v1) | Xingze Qiu | The uncertainty principle is fundamentally rooted in the algebraic asymmetry between observables. We introduce a new class of uncertainty relations grounded in the resource theory of asymmetry, where incompatibility is quantified by an observable's intrinsic, state-independent capacity to break the symmetry associated with another. This ``operator asymmetry,'' formalized as the asymmetry norm, leads to a variance-based uncertainty relation for pure states that can be tighter than the standard Robertson bound, especially in the near-compatible regime. Most significantly, this framework resolves a long-standing open problem in quantum information theory: the formulation of a universally valid, product-form uncertainty relation for the Wigner-Yanase skew information. We demonstrate the practical power of our framework by deriving tighter quantum speed limits for the dynamics of nearly conserved quantities, which are crucial for understanding non-equilibrium phenomena such as prethermalization and many-body localization. This work provides both a new conceptual lens for understanding quantum uncertainty and a powerful toolkit for its application. |
| 2025-09-08 | [Nested Optimal Transport Distances](http://arxiv.org/abs/2509.06702v1) | Ruben Bontorno, Songyan Hou | Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches. |
| 2025-09-08 | [Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties](http://arxiv.org/abs/2509.06697v1) | Tanujit Chakraborty, Donia Besher et al. | Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \texttt{narfima} \textbf{R} package provides an implementation of our approach. |
| 2025-09-08 | [Port-Hamiltonian Neural Networks: From Theory to Simulation of Interconnected Stochastic Systems](http://arxiv.org/abs/2509.06674v1) | Luca Di Persio, Matthias Ehrhardt et al. | This work introduces a new framework integrating port-Hamiltonian systems (PHS) and neural network architectures. This framework bridges the gap between deterministic and stochastic modeling of complex dynamical systems. We introduce new mathematical formulations and computational methods that expand the geometric structure of PHS to account for uncertainty, environmental noise, and random perturbations. Building on these advances, we introduce stochastic port-Hamiltonian neural networks (pHNNs), which facilitate the accurate learning and prediction of non-autonomous and interconnected stochastic systems. Our proposed framework generalizes passivity concepts to the stochastic regime, ensuring stability while maintaining the system's energy-consistent structure. Extensive simulations, including those involving damped mass-spring systems, Duffing oscillators, and robotic control tasks, demonstrate the capability of pHNNs to capture complex dynamics with high fidelity, even under noise and uncertainty. This unified approach establishes a foundation for the robust, data-driven modeling and control of nonlinear stochastic systems. |
| 2025-09-08 | [The complementary of CTAO, direct detection and collider searches for dark matter in Effective Field Theories and Simplified models](http://arxiv.org/abs/2509.06628v1) | Igor Reis, Andre Scaffidi et al. | This paper explores the sensitivity of the Cherenkov Telescope Array Observatory to dark matter annihilation in the Galactic Center, within the frameworks of Effective Field Theory and Simplified Models. We present sensitivity forecasts, utilizing an up-to-date instrument configuration and incorporating the latest models for Galactic Diffuse Emission. A key aspect of our work is the inclusion of updated dark matter density profiles, J-factors, and velocity dispersion distributions derived from the FIRE-2 cosmological hydrodynamical simulations, which significantly impact the expected indirect detection signals. Furthermore, we update the constraints from direct detection experiments (Xenon1T and LZ) taking into account the astrophysical uncertainties informed by the FIRE-2 simulations, and also investigate limits coming from collider searches (ATLAS and CMS). Our analysis reveals improved constraints on the effective suppression scale ($M_*$) in the Effective Field Theory framework and on the mediator mass ($M_{med}$) in Simplified Models compared to previous studies, highlighting the complementarity of the Cherenkov Telescope Array Observatory with direct and collider searches in probing a wide range of dark matter scenarios. We discuss the implications of these results for various dark matter interaction types, including scalar, pseudoscalar, vector, and axial-vector mediators, and emphasize the importance of considering realistic astrophysical inputs in interpreting dark matter search results across different experimental fronts. |
| 2025-09-05 | [Testing Magnetic Field Configurations in Spider Pulsar PSR J1723-2837 with IXPE](http://arxiv.org/abs/2509.05240v1) | Michela Negro, Haocheng Zhang et al. | We present the first X-ray polarimetry observations of a redback millisecond pulsar binary, \src, with the Imaging X-ray Polarimetry Explorer (IXPE). Redbacks are compact binaries in which a rotation-powered millisecond pulsar interacts with a non-degenerate companion via an intrabinary shock, forming ideal laboratories for probing pulsar winds and relativistic shock physics, where ordered magnetic fields and particle acceleration shape the observed radiation. We conduct a spectro-polarimetric analysis combining IXPE data with archival Chandra, XMM-Newton, NuSTAR, and Swift observations. We explore two limiting magnetic field configurations, parallel and perpendicular to the bulk flow, and simulate their expected polarization signatures using the {\tt 3DPol} radiative transport code. To account for the rapid rotation of the polarization angle predicted by these models, we implement a phase-dependent Stokes alignment procedure that preserves the polarization degree while correcting for phase-rotating PA. We also devise a new maximum-likelihood fitting strategy to determine the phase-dependence of the polarization angle by minimizing the polarization degree uncertainty. This technique shows a hint the binary may be rotating clockwise relative to the celestial north pole. We find no significant detection of polarization in the IXPE data, with PD<51% at 99% confidence level. Our results excludes the high-polarization degree scenario predicted by the perpendicular field model during the brightest orbital phase bin. Simulations show that doubling the current exposure would make the parallel configuration detectable. The new PA rotation technique is also applicable to IXPE data of many sources whose intrinsic PA variation is apriori not known but is strictly periodic. |
| 2025-09-05 | [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](http://arxiv.org/abs/2509.05201v1) | Nariman Niknejad, Gokul S. Sankar et al. | This paper presents a robust model predictive control (MPC) framework that explicitly addresses the non-Gaussian noise inherent in deep learning-based perception modules used for state estimation. Recognizing that accurate uncertainty quantification of the perception module is essential for safe feedback control, our approach departs from the conventional assumption of zero-mean noise quantification of the perception error. Instead, it employs set-based state estimation with constrained zonotopes to capture biased, heavy-tailed uncertainties while maintaining bounded estimation errors. To improve computational efficiency, the robust MPC is reformulated as a linear program (LP), using a Minkowski-Lyapunov-based cost function with an added slack variable to prevent degenerate solutions. Closed-loop stability is ensured through Minkowski-Lyapunov inequalities and contractive zonotopic invariant sets. The largest stabilizing terminal set and its corresponding feedback gain are then derived via an ellipsoidal approximation of the zonotopes. The proposed framework is validated through both simulations and hardware experiments on an omnidirectional mobile robot along with a camera and a convolutional neural network-based perception module implemented within a ROS2 framework. The results demonstrate that the perception-aware MPC provides stable and accurate control performance under heavy-tailed noise conditions, significantly outperforming traditional Gaussian-noise-based designs in terms of both state estimation error bounding and overall control performance. |
| 2025-09-05 | [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](http://arxiv.org/abs/2509.05186v1) | Benjamin J. Zhang, Siting Liu et al. | In-context operator networks (ICON) are a class of operator learning methods based on the novel architectures of foundation models. Trained on a diverse set of datasets of initial and boundary conditions paired with corresponding solutions to ordinary and partial differential equations (ODEs and PDEs), ICON learns to map example condition-solution pairs of a given differential equation to an approximation of its solution operator. Here, we present a probabilistic framework that reveals ICON as implicitly performing Bayesian inference, where it computes the mean of the posterior predictive distribution over solution operators conditioned on the provided context, i.e., example condition-solution pairs. The formalism of random differential equations provides the probabilistic framework for describing the tasks ICON accomplishes while also providing a basis for understanding other multi-operator learning methods. This probabilistic perspective provides a basis for extending ICON to \emph{generative} settings, where one can sample from the posterior predictive distribution of solution operators. The generative formulation of ICON (GenICON) captures the underlying uncertainty in the solution operator, which enables principled uncertainty quantification in the solution predictions in operator learning. |
| 2025-09-05 | [Orlicz spaces and the uncertainty principle](http://arxiv.org/abs/2509.05185v1) | A. Iosevich, I. Li et al. | Let $f$ be a finite signal. The classical uncertainty principle tells us that the product of the support of $f$ and the support of $\hat{f}$, the Fourier transform of $f$, must satisfy $|supp(f)|\cdot|supp(\hat{f})|\geq |G|$. Recently, Iosevich and Mayeli improved the uncertainty principle for signals with Fourier supported on generic sets. This was done by employing the Fourier restriction theory in $L^p$ spaces. In this paper, we extended the $(p,q)$-restriction setting to Orlicz spaces. Then we apply uncertainty principles to the problem of exact recovery, which again extends and recovers the result that Iosevich and Mayeli obtained in Lebesgue spaces. |
| 2025-09-05 | [Deep-Field Analytical Calibration](http://arxiv.org/abs/2509.05152v1) | Andy Park, Xiangchong Li et al. | The next generation of imaging surveys, including the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), Euclid, and the Nancy Grace Roman Space Telescope, will place unprecedented constraints on cosmology using weak gravitational lensing. To fully exploit their statistical power, shear measurement methods must achieve sub-percent accuracy while mitigating systematic biases from noise, the point-spread function (PSF), blending, and shear-dependent detection. The analytical calibration framework (\texttt{AnaCal}) has demonstrated such accuracy but requires adding noise to images, reducing their effective depth. We introduce Deep-Field Analytical Calibration (\textsc{deep-field~}\texttt{AnaCal}), an extension of \texttt{AnaCal} that leverages deep-field images to compute shear responses while preserving wide-field statistical power. We validate the method on isolated and blended galaxy simulations with LSST-like seeing and noise, showing it meets the stringent requirement of multiplicative bias $|m| < 3\times10^{-3}$ at 99.7% confidence. Relative to standard \texttt{AnaCal} on wide-field images, this method improves effective galaxy number density from $17$ to $30$ arcmin$^{-2}$ for simulated 10-year LSST data. Assuming deep fields with $10\times$ the exposure of wide fields, we find the pixel noise variance in shear estimation is reduced by $30%$ and the overall shear uncertainty by $\sim 25%$. Finally, we assess sample variance impacts using the LSST Deep Drilling Fields strategy, finding an equivalent calibration uncertainty of $\lesssim 0.3%$. These results establish \textsc{deep-field~}\texttt{AnaCal} as a promising approach for shear calibration in upcoming weak lensing surveys. |
| 2025-09-05 | [An emulator-based forecasting on astrophysics and cosmology with 21 cm and density cross-correlations during EoR](http://arxiv.org/abs/2509.05096v1) | Barun Maity | The 21 cm signal arising from fluctuations in the neutral hydrogen field, and its cross-correlation with other tracers of cosmic density, are promising probes of the high-redshift Universe. In this study, we assess the potential of the 21 cm power spectrum, along with its cross power spectrum with dark matter density and associated bias, to constrain both astrophysics during the reionization era and the underlying cosmology. Our methodology involves emulating these estimators using an Artificial Neural Network (ANN), enabling efficient exploration of the parameter space. Utilizing a photon-conserving semi-numerical reionization model, we construct emulators at a fixed redshift ($z = 7.0$) for $k$-modes relevant to upcoming telescopes such as SKA-Low. We generate $\sim7000$ training samples by varying both cosmological and astrophysical parameters along with initial conditions, achieving high accuracy when compared to true simulation outputs. While forecasting, the model involves five free parameters: three cosmological ($\Omega_m$, $h$, $\sigma_8$) and two astrophysical (ionizing efficiency, $\zeta$, and minimum halo mass, $M_{\mathrm{min}}$). Using a fiducial model at the mid-reionization stage, we create a mock dataset and perform forecasting with the trained emulators. Assuming a 5% observational uncertainty combined with emulator error, we find that the 21 cm and 21 cm-density cross power spectra can constrain the Hubble parameter ($h$) to better than 6% at a confidence interval of 95%, with tight constraints on the global neutral fraction ($Q_{\mathrm{HI}}$). The inclusion of bias information further improves constraints on $\sigma_8$ (< 10% at 95% confidence). Finally, robustness tests with two alternate ionization states and a variant with higher observational uncertainty show that the ionization fractions are still reliably recovered, even when cosmological constraints weaken. |
| 2025-09-05 | [Finding your MUSE: Mining Unexpected Solutions Engine](http://arxiv.org/abs/2509.05072v1) | Nir Sweed, Hanit Hakim et al. | Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research. |
| 2025-09-05 | [Deep Inverse Rosenblatt Transport for Structural Reliability Analysis](http://arxiv.org/abs/2509.05061v1) | Aryan Tyagi, Jan N. Fuhg | Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study. |
| 2025-09-05 | [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](http://arxiv.org/abs/2509.05060v1) | Patrick Amadeus Irawan, Ryandito Diandaru et al. | We introduce Entropy2Vec, a novel framework for deriving cross-lingual language representations by leveraging the entropy of monolingual language models. Unlike traditional typological inventories that suffer from feature sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in language models to capture typological relationships between languages. By training a language model on a single language, we hypothesize that the entropy of its predictions reflects its structural similarity to other languages: Low entropy indicates high similarity, while high entropy suggests greater divergence. This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values. Empirical evaluations demonstrate that Entropy2Vec embeddings align with established typological categories and achieved competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework. |
| 2025-09-05 | [Reconstruction of the Dipole Amplitude in the Dipole Picture as a mathematical Inverse Problem](http://arxiv.org/abs/2509.05005v1) | Henri H√§nninen, Antti Kykk√§nen et al. | We show that the inference problem of constraining the dipole amplitude with inclusive deep inelastic scattering data can be written into a discrete linear inverse problem, in an analogous manner as can be done for computed tomography. To this formulation of the problem, we apply standard inverse problems methods and algorithms to reconstruct known dipole amplitudes from simulated reduced cross section data with realistic precision. The main difference of this approach to previous works is that this implementation does not require any fit parametrization of the dipole amplitude. The freedom from parametrization also enables us for the first time to quantify the uncertainties of the inferred dipole amplitude in a novel more general framework. This mathematical approach to small-$x$ phenomenology opens a path to parametrization bias free inference of the dipole amplitude from HERA and Electron--Ion Collider data. |
| 2025-09-04 | [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](http://arxiv.org/abs/2509.04430v1) | Nikolay Kartashev, Ivan Rubachev et al. | Recent advancements in tabular deep learning have demonstrated exceptional practical performance, yet the field often lacks a clear understanding of why these techniques actually succeed. To address this gap, our paper highlights the importance of the concept of data uncertainty for explaining the effectiveness of the recent tabular DL methods. In particular, we reveal that the success of many beneficial design choices in tabular DL, such as numerical feature embeddings, retrieval-augmented models and advanced ensembling strategies, can be largely attributed to their implicit mechanisms for managing high data uncertainty. By dissecting these mechanisms, we provide a unifying understanding of the recent performance improvements. Furthermore, the insights derived from this data-uncertainty perspective directly allowed us to develop more effective numerical feature embeddings as an immediate practical outcome of our analysis. Overall, our work paves the way to foundational understanding of the benefits introduced by modern tabular methods that results in the concrete advancements of existing techniques and outlines future research directions for tabular DL. |
| 2025-09-04 | [Generation of Lognormal Synthetic Lyman-$Œ±$ Forest Spectra for $P_{1D}$ Analysis](http://arxiv.org/abs/2509.04405v1) | Meagan Herbold, Naim G√∂ksel Kara√ßaylƒ± et al. | The one-dimensional flux power spectrum (P1D) of the Lyman-$\alpha$ forest probes small-scale structure in the intergalactic medium (IGM) and is therefore sensitive to a variety of cosmological and astrophysical parameters. These include the amplitude and shape of the matter power spectrum, the thermal history of the IGM, the sum of neutrino masses, and potential small-scale fluctuations due to the nature of dark matter. However, P1D is also highly sensitive to observational and instrumental systematics, making accurate synthetic spectra essential for validating analyses and quantifying these effects, especially in high-volume surveys like the Dark Energy Spectroscopic Instrument (DESI). We present an efficient lognormal mock framework for generating one-dimensional Lyman-$\alpha$ forest spectra tailored for P1D analysis. Our method captures the redshift evolution of the mean transmitted flux and the scale-dependent shape and amplitude of the one-dimensional flux power spectrum by tuning Gaussian field correlations and transformation parameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \leq z \leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \leq k \leq 1.0$ s km$^{-1}$), our mocks recover the mean flux evolution with redshift to sub-percent accuracy, and the P1D at the percent level. Additionally, we discuss potential extensions of this framework, such as the incorporation of astrophysical contaminants, continuum uncertainties, and instrumental effects. Such improvements would expand its utility in ongoing and upcoming surveys and enable a broader range of validation efforts and systematics studies for P1D inference and precision cosmology. |
| 2025-09-04 | [Revealing the origin of supermassive black holes with Taiji-TianQin network](http://arxiv.org/abs/2509.04396v1) | Ping Shen, Wen-Biao Han et al. | The origin of supermassive black holes (SMBHs) is a pivotal problem in modern cosmology. This work explores the potential of the Taiji-TianQin space-borne gravitational-wave (GW) detector network to identify the formation channels of massive black hole binaries (MBHBs) at high redshifts ($z \gtrsim 10$). The network substantially improves detection capability, boosting the signal-to-noise ratio by a factor of 2.2-3.0 (1.06-1.14) relative to TianQin (Taiji) alone. It increases the detection rate of MBHBs formed from light seeds (LS) by more than 2.2 times and achieves over 96\% detection efficiency for those originating from heavy seeds (HS). Furthermore, the network enables component mass estimation with relative uncertainties as low as $\sim 10^{-4}$ at the $2\sigma$ level. These improvements facilitate the assembly of a well-constrained population sample, allowing robust measurement of the fractional contributions from different formation pathways. The network achieves high precision in distinguishing between LS and HS origins (7.4\% relative uncertainty at $2\sigma$) and offers moderate discrimination between delay and no-delay channels in HS-origin binaries (24\%). However, classification remains challenging for delay versus no-delay scenarios in LS-origin systems (58\%) due to significant population overlap. In conclusion, the Taiji-TianQin network will serve as a powerful tool for unveiling the origins of SMBHs through GW population studies. |
| 2025-09-04 | [Prominence: A discriminator of gravitational wave signals](http://arxiv.org/abs/2509.04384v1) | Jo√£o Gon√ßalves, Danny Marfatia et al. | The concept of prominence is familiar to signal engineers, topographers and mountaineers. We introduce Prominence $\mathcal P$ as a discriminator of gravitational wave (GW) signals. We treat black hole and neutron star binaries as astrophysical background sources, and show how $\mathcal P$ can be used to distinguish between GW spectra produced by first-order phase transitions, domain walls and cosmic strings, and combinations thereof. Prominence can also be used to discriminate between these and off-piste sources of GWs. The uncertainty in the measured energy density in GWs at Pulsar Timing Arrays needs to be smaller than $\sim 4\%$ for $\mathcal{P}$ to achieve discrimination at 3$\sigma$. LISA and ET data are expected to have sufficiently small uncertainties that Prominence can play a central role in their analysis. |
| 2025-09-04 | [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](http://arxiv.org/abs/2509.04363v1) | Paul Scherer, Andreas Kirsch et al. | Real-world experimental scenarios are characterized by the presence of heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated in batched settings. The bias--variance tradeoff can be used to write the expected mean squared error between a model distribution and a ground-truth random variable as the sum of an epistemic uncertainty term, the bias squared, and an aleatoric uncertainty term. We leverage this relationship to propose novel active learning strategies that directly reduce the bias between experimental rounds, considering model systems both with and without noise. Finally, we investigate methods to leverage historical data in a quadratic manner through the use of a novel cobias--covariance relationship, which naturally proposes a mechanism for batching through an eigendecomposition strategy. When our difference-based method leveraging the cobias--covariance relationship is utilized in a batched setting (with a quadratic estimator), we outperform a number of canonical methods including BALD and Least Confidence. |
| 2025-09-04 | [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](http://arxiv.org/abs/2509.04357v1) | Jiajun He, Naoki Sawada et al. | Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech. |
| 2025-09-04 | [History matching for functional data and its application to tsunami warnings in the Indian Ocean](http://arxiv.org/abs/2509.04342v1) | Ryuichi Kanai, Nicol√°s Hern√°ndez et al. | Traditional History Matching (HM) identifies implausible regions of the input parameter space by comparing scalar outputs of a computer model to observations. It offers higher computational efficiency than Bayesian calibration, making it suitable for high-dimensional problems. However, in real physical systems, outputs are often functional, such as time series or spatial fields, and conventional HM cannot fully exploit such information. We propose a novel method, Functional History Matching (FHM), which extends HM to handle functional data. FHM incorporates the Outer Product Emulator, an extension of the Gaussian Process emulator designed for time series, to enhance computational efficiency. FHM also leverages Random Projection to extract dynamic features from infinite-dimensional data, including derivatives. FHM supports uncertainty quantification essential for decision-making and naturally accommodates model discrepancies. To demonstrate its practical effectiveness, we apply FHM to a synthetic tsunami forecasting scenario in the Indian Ocean, assuming a realistic event in the Makran subduction zone. Wave elevation time series from offshore buoy data are used to predict wave elevations over the Indian coastline. Our results show that FHM significantly outperforms scalar-based HM in accuracy. FHM enables reliable forecasting from functional data within feasible computational constraints, offering a robust framework for early warning systems and beyond. |
| 2025-09-04 | [We Have It Covered: A Resampling-based Method for Uplift Model Comparison](http://arxiv.org/abs/2509.04315v1) | Yang Liu, Chaoyu Yuan | Uplift models play a critical role in modern marketing applications to help understand the incremental benefits of interventions and identify optimal targeting strategies. A variety of techniques exist for building uplift models, and it is essential to understand the model differences in the context of intended applications. The uplift curve is a widely adopted tool for assessing uplift model performance on the selection universe when observations are available for the entire population. However, when it is uneconomical or infeasible to select the entire population, it becomes difficult or even impossible to estimate the uplift curve without appropriate sampling design. To the best of our knowledge, no prior work has addressed uncertainty quantification of uplift curve estimates, which is essential for model comparisons. We propose a two-step sampling procedure and a resampling-based approach to compare uplift models with uncertainty quantification, examine the proposed method via simulations and real data applications, and conclude with a discussion. |
| 2025-09-04 | [Learning Optimal Crew Dispatch for Grid Restoration Following an Earthquake](http://arxiv.org/abs/2509.04308v1) | Farshad Amani, Faezeh Ardali et al. | Post-disaster crew dispatch is a critical but computationally intensive task. Traditional mixed-integer linear programming methods often require minutes to several hours to compute solutions, leading to delays that hinder timely decision-making in highly dynamic restoration environments. To address this challenge, we propose a novel learning-based framework that integrates transformer architectures with deep reinforcement learning (DRL) to deliver near real-time decision support without compromising solution quality. Crew dispatch is formulated as a sequential decision-making problem under uncertainty, where transformers capture high-dimensional system states and temporal dependencies, while DRL enables adaptive and scalable decision-making. Earthquake-induced distribution network damage is first characterized using established seismic standards, followed by a scenario generation and reduction pipeline that aggregates probable outcomes into a single geospatial impact map. Conditioned on this map, the proposed framework generates second-level dispatch strategies, trained offline on simulated and historical events and deployed online for rapid response. In addition to substantial runtime improvements, the proposed method enhances system resilience by enabling faster and more effective recovery and restoration. Case studies, particularly on the 2869-bus European gas and power network, demonstrate that the method substantially accelerates restoration while maintaining high-quality solutions, underscoring its potential for practical deployment in large-scale disaster response. |
| 2025-09-04 | [Energy Confinement Time Scaling for the Negative Triangularity Scenario in DIII-D](http://arxiv.org/abs/2509.04279v1) | P. Lunia | Results from the 2023 negative triangularity campaign on DIII-D demonstrate encouraging energy confinement properties, similar to or exceeding the scaling of the IPB98(y,2) law. This paper describes the procedure with which a new scaling law was regressed specifically from the data from the DIII-D campaign. Given the relatively small size of the single-machine dataset, measures were taken to minimize sampling bias and give a realistic estimate of the large uncertainties from the regression. The resulting power law shows a robustly stronger dependence on plasma current and more severe power degradation as compared to the H-mode scaling law. |
| 2025-09-03 | [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](http://arxiv.org/abs/2509.03516v1) | Ouxiang Li, Yuan Wang et al. | Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/. |
| 2025-09-03 | [Bayesian Multivariate Sparse Functional PCA](http://arxiv.org/abs/2509.03512v1) | Joseph Sartini, Scott Zeger et al. | Functional Principal Components Analysis (FPCA) provides a parsimonious, semi-parametric model for multivariate, sparsely-observed functional data. Frequentist FPCA approaches estimate principal components (PCs) from the data, then condition on these estimates in subsequent analyses. As an alternative, we propose a fully Bayesian inferential framework for multivariate, sparse functional data (MSFAST) which explicitly models the PCs and incorporates their uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate, densely-observed functional data. Like FAST, MSFAST represents PCs using orthonormal splines, samples the orthonormal spline coefficients using parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST extends FAST to multivariate, sparsely-observed data by (1) standardizing each functional covariate to mitigate poor posterior conditioning due to disparate scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing likelihood calculations over covariates; (4) updating parameterizations and priors for computational stability; (5) using a Procrustes-based posterior alignment procedure; and (6) providing efficient prediction routines. We evaluated MSFAST alongside existing implementations using simulations. MSFAST produces uniquely valid inferences and accurate estimates, particularly for smaller signals. MSFAST is motivated by and applied to a study of child growth, with an accompanying vignette illustrating the implementation step-by-step. |
| 2025-09-03 | [Quantitative Stability and Contraction Principles for Mean-Field G-SDEs](http://arxiv.org/abs/2509.03439v1) | Yunfan Zhao, Xiaojing Chen et al. | We study mean-field stochastic differential equations (SDEs) driven by G-Brownian motion, extending recent work on existence and uniqueness by developing a full quantitative stability framework. Our main contribution is the construction of an intrinsic stability modulus that provides explicit bounds on the sensitivity of solutions with respect to perturbations in initial data (and, indirectly, coefficients). Using Bihari-Osgood type inequalities under G-expectation, we establish sharp continuity estimates for the data-to-solution map and analyze the asymptotic properties of the stability modulus. In particular, we identify contraction behavior on short horizons, leading to a contraction principle that guarantees uniqueness and global propagation of stability. The results apply under non-Lipschitz, non-deterministic coefficients with square-integrable initial data, thereby significantly broadening the scope of mean-field G-SDEs. Beyond existence and uniqueness, our framework quantifies robustness of solutions under volatility uncertainty, opening new directions for applications in stochastic control, risk management, and mean-field models under ambiguity. |
| 2025-09-03 | [Bayesian analysis of properties of nuclear matter with the FOPI experimental data](http://arxiv.org/abs/2509.03406v1) | Guojun Wei, Manzi Nan et al. | Based on the ultra-relativistic quantum molecular dynamics (UrQMD) transport model, combined with experimental data of directed flow, elliptic flow, and nuclear stopping power measured by FOPI in $\rm ^{197}Au+^{197}Au$ collisions at beam energies ($E_{lab}$) of 0.25 and 0.4 GeV/nucleon, the incompressibility of the nuclear equation of state $K_0$, the nucleon effective mass $m^*$, and the in-medium correction factor ($F$, with respect to free-space values) on the nucleon-nucleon elastic cross sections are studied by Bayesian analysis. It is found that both $m^*$ and $F$ can be tightly constrained with the uncertainty $\le$ 15\%, however, $K_0$ cannot be constrained tightly. We deduce $m^*/m_0 = 0.78^{+0.09}_{-0.10}$ and $F = 0.75^{+0.08}_{-0.07}$ with experimental data at $E_{lab}$ = 0.25 GeV/nucleon, and the obtained values increased to $m^*/m_0 = 0.88^{+0.03}_{-0.03}$ and $F = 0.88^{+0.06}_{-0.07}$ at $E_{lab}$ = 0.4 GeV/nucleon. The obtained results are further verified with rapidity-dependent flow data. |
| 2025-09-03 | [Elemental and Isotopic Yields from T Coronae Borealis: Predictions and Uncertainties](http://arxiv.org/abs/2509.03395v1) | Emma Wallace, Christian Iliadis et al. | T Coronae Borealis (T CrB) is a symbiotic recurrent nova system expected to undergo its next outburst within the next two years. Recent hydrodynamic simulations have predicted the nucleosynthetic yields for both carbon-oxygen (CO) and oxygen-neon (ONe) white-dwarf models, but without accounting for thermonuclear reaction-rate uncertainties. We perform detailed Monte Carlo post-processing nucleosynthesis calculations based on updated thermonuclear reaction rates and uncertainties from the 2025 evaluation. We quantify the resulting abundance uncertainties and identify the key nuclear reactions that dominate them. Our results show that both the CO and ONe nova models robustly produce characteristic CNO isotopes. More pronounced abundance differences emerge for elements with A $\ge$ 20. Sulfur is the most robust observational discriminator between the CO and ONe nova models, with a model-to-model difference of a factor of $\approx$30 and minimal sensitivity to reaction rate uncertainties. Neon, silicon, and phosphorus exhibit even larger abundance differences (factors of $\approx$150-250), providing strong diagnostic potential. While their predicted yields are subject to larger uncertainties, these remain smaller than the model-to-model differences, allowing these elements to serve as useful, though less precise, tracers of white-dwarf composition. Chlorine, argon, and potassium also differ between models, but the 1$\sigma$-abundance ranges for the CO and ONe models overlap, reducing their present usefulness as composition tracers. We find that only nine nuclear reactions dominate the abundance uncertainties of the most diagnostically important isotopes, and their influence is largely independent of the underlying white-dwarf composition. These results provide guidance for future experimental efforts and for interpreting ejecta compositions in the next eruption of T CrB. |
| 2025-09-03 | [On the renormalization-group analysis of the SM: loops, uncertainties, and vacuum stability](http://arxiv.org/abs/2509.03369v1) | A. V. Bednyakov, A. S. Fedoruk et al. | Renormalization-group equations (RGE) is one of the key tools in studying high-energy behavior of the Standard Model (SM). We begin by reviewing one-loop RGE for the dimensionless couplings of the SM and proceed to the state-of-the-art results. Our study focuses on the RGE solutions at different loop orders. We compare not only the standard (``diagonal'') loop counting, when one considers gauge, Yukawa, and scalar self-coupling beta functions at the same order, but also ``non-diagonal'' ones, inspired by the so-called Weyl consistency conditions. We discuss the initial conditions for RGE (``matching'') for different loop configurations, and study the uncertainties of running coupling both related to the limited precision of the experimental input (``parametric'') and to the missing high-order corrections (``theoretical''). As an application of our analysis we also estimate the electroweak vacuum decay probability and study how the uncertainties in the running parameters affect the latter. We argue that the ``non-diagonal'' beta functions, if coupled with more consistent ``non-diagonal'' matching lead to larger theoretical uncertainty than the ``diagonal'' ones. |
| 2025-09-03 | [Search for Past Stellar Encounters and the Origin of 3I/ATLAS](http://arxiv.org/abs/2509.03361v1) | Yiyang Guo, Luyao Zhang et al. | 3I/ATLAS, the third discovered interstellar object, has a heliocentric speed of 58 km/s and exhibits cometary activity. To constrain the origin of 3I/ATLAS and its past dynamical evolution, we propagate the orbits of 3I/ATLAS and nearby stars to search for stellar encounters. Integrating orbits in the Galactic potential and propagating the astrometric and radial-velocity uncertainties of 30 million Gaia stars, we identify 25 encounters with median encounter distances less than 1 pc. However, because the encounter speeds between 3I/ATLAS and each encounter exceed 20 km/s, none is a plausible host under common ejection mechanisms. We infer stellar masses for most stars and quantify the gravitational perturbations exerted by each individual star or each binary system on 3I/ATLAS. The strongest gravitational scattering perturber is a wide M-dwarf binary. Among all past encounters, the binary's barycenter and 3I/ATLAS reach the small encounter distance of 0.242 pc and the encounter speed of 28.39 km/s,1.64 Myr ago. We further demonstrate that the cumulative influence of the stellar encounters on both the speed and direction of 3I/ATLAS is weak. Based on the present kinematics of 3I/ATLAS to assess its origin, we find that a thin-disk origin is strongly favored, because the thin disk both exhibits a velocity distribution closely matching that of 3I/ATLAS and provides the dominant local number density of stars. |
| 2025-09-03 | [Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner](http://arxiv.org/abs/2509.03348v1) | Yewen Li, Jingtong Gao et al. | Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers' bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges by focusing on trajectory-level accumulated rewards, as well as their explainable capability, i.e., planning a future trajectory of states and executing bids accordingly. However, diffusers struggle with generation uncertainty, particularly regarding dynamic legitimacy between adjacent states, which can lead to poor bids and further cause significant loss of ad impression opportunities when competing with other advertisers in a highly competitive auction environment. To address it, we propose a Causal auto-Bidding method based on a Diffusion completer-aligner framework, termed CBD. Firstly, we augment the diffusion training process with an extra random variable t, where the model observes t-length historical sequences with the goal of completing the remaining sequence, thereby enhancing the generated sequences' dynamic legitimacy. Then, we employ a trajectory-level return model to refine the generated trajectories, aligning more closely with advertisers' objectives. Experimental results across diverse settings demonstrate that our approach not only achieves superior performance on large-scale auto-bidding benchmarks, such as a 29.9% improvement in conversion value in the challenging sparse-reward auction setting, but also delivers significant improvements on the Kuaishou online advertising platform, including a 2.0% increase in target cost. |
| 2025-09-03 | [Bayesian Additive Regression Trees for functional ANOVA model](http://arxiv.org/abs/2509.03317v1) | Seokhun Park, Insung Kong et al. | Bayesian Additive Regression Trees (BART) is a powerful statistical model that leverages the strengths of Bayesian inference and regression trees. It has received significant attention for capturing complex non-linear relationships and interactions among predictors. However, the accuracy of BART often comes at the cost of interpretability. To address this limitation, we propose ANOVA Bayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART based on the functional ANOVA decomposition, which is used to decompose the variability of a function into different interactions, each representing the contribution of a different set of covariates or factors. Our proposed ANOVA-BART enhances interpretability, preserves and extends the theoretical guarantees of BART, and achieves superior predictive performance. Specifically, we establish that the posterior concentration rate of ANOVA-BART is nearly minimax optimal, and further provides the same convergence rates for each interaction that are not available for BART. Moreover, comprehensive experiments confirm that ANOVA-BART surpasses BART in both accuracy and uncertainty quantification, while also demonstrating its effectiveness in component selection. These results suggest that ANOVA-BART offers a compelling alternative to BART by balancing predictive accuracy, interpretability, and theoretical consistency. |
| 2025-09-03 | [Credible Uncertainty Quantification under Noise and System Model Mismatch](http://arxiv.org/abs/2509.03311v1) | Penggao Yan, Li-Ta Hsu | State estimators often provide self-assessed uncertainty metrics, such as covariance matrices, whose reliability is critical for downstream tasks. However, these self-assessments can be misleading due to underlying modeling violations like noise or system model mismatch. This letter addresses the problem of estimator credibility by introducing a unified, multi-metric evaluation framework. We construct a compact credibility portfolio that synergistically combines traditional metrics like the Normalized Estimation Error Squared (NEES) and the Noncredibility Index (NCI) with proper scoring rules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our key contributions are a novel energy distance-based location test to robustly detect system model misspecification and a method that leverages the asymmetric sensitivities of NLL and ES to distinguish optimism covariance scaling from system bias. Monte Carlo simulations across six distinct credibility scenarios demonstrate that our proposed method achieves high classification accuracy (80-100%), drastically outperforming single-metric baselines which consistently fail to provide a complete and correct diagnosis. This framework provides a practical tool for turning patterns of credibility indicators into actionable diagnoses of model deficiencies. |
| 2025-08-29 | [Sequential Fair Allocation With Replenishments: A Little Envy Goes An Exponentially Long Way](http://arxiv.org/abs/2508.21753v1) | Chido Onyeze, Sean R. Sinclair et al. | We study the trade-off between envy and inefficiency in repeated resource allocation settings with stochastic replenishments, motivated by real-world systems such as food banks and medical supply chains. Specifically, we consider a model in which a decision-maker faced with stochastic demand and resource donations must trade off between an equitable and efficient allocation of resources over an infinite horizon. The decision-maker has access to storage with fixed capacity $M$, and incurs efficiency losses when storage is empty (stockouts) or full (overflows). We provide a nearly tight (up to constant factors) characterization of achievable envy-inefficiency pairs. Namely, we introduce a class of Bang-Bang control policies whose inefficiency exhibits a sharp phase transition, dropping from $\Theta(1/M)$ when $\Delta = 0$ to $e^{-\Omega(\Delta M)}$ when $\Delta > 0$, where $\Delta$ is used to denote the target envy of the policy. We complement this with matching lower bounds, demonstrating that the trade-off is driven by supply, as opposed to demand uncertainty. Our results demonstrate that envy-inefficiency trade-offs not only persist in settings with dynamic replenishment, but are shaped by the decision-maker's available capacity, and are therefore qualitatively different compared to previously studied settings with fixed supply. |
| 2025-08-29 | [Uncertainties within Weather Regime definitions for the Euro-Atlantic sector in ERA5 and CMIP6](http://arxiv.org/abs/2508.21701v1) | Lotte Hompes, Swinda K. J. Falkena et al. | Certain Weather Regimes (WR) are associated with a higher risk of energy shortages, i.e. Blocking regimes for European winters. However, there are many uncertainties tied to the implementation of WRs and associated risks in the energy sector. Especially the impact of climate change is unknown.   We investigate these uncertainties by looking at three methodologically diverse Euro-Atlantic WR definitions. We carry out a thorough validation of these methods and analyse their methodological and spatio-temporal sensitivity using ERA5 data. Furthermore, we look into the suitability of CMIP6 models for WR based impact assessments.   Our sensitivity assessment showed that the persistence and occurrence of regimes are sensitive to small changes in the methodology. We show that the training period used has a very significant impact on the persistence and occurrence of the regimes found. For both WR4 and WR7, this results in instability of the regime patterns.   All CMIP6 models investigated show instability of the regimes. Meaning that the normalised distance between the CMIP6 model regimes and our baseline regimes exceeds 0.4 or are visually extremely dissimilar. Only the WR4 regimes clustered on historical CMIP6 model data consistently have a normalised distance to our baseline regimes smaller than 0.4 and are visually identifiable. The WR6 definition exceeds the normalised distance threshold for all investigated CMIP6 experiments. Though all CMIP6 model experiments clustered with the WR7 definition have a normalised distance to the baseline regimes below 0.4, visual inspection of the regimes indicates instability.   Great caution should be taken when applying WR's in impact models for the energy sector, due to this large instability and uncertainties associated with WR definitions. |
| 2025-08-29 | [Chance-Constrained DC Optimal Power Flow Using Constraint-Informed Statistical Estimation](http://arxiv.org/abs/2508.21687v1) | Tianyang Yi, D. Adrian Maldonado et al. | Chance-constrained optimization has emerged as a promising framework for managing uncertainties in power systems. This work advances its application to the DC Optimal Power Flow (DC-OPF) model, developing a novel approach to uncertainty modeling and estimation. Current methods typically tackle these problems by first modeling random nodal injections using high-dimensional statistical distributions that scale with the number of buses, followed by deriving deterministic reformulations of the probabilistic constraints. We propose an alternative methodology that exploits the constraint structure to inform the uncertainties to be estimated, enabling significant dimensionality reduction. Rather than learning joint distributions of net-load forecast errors across units, we instead directly model the one-dimensional aggregate system forecast error and two-dimensional line errors weighted by power transfer distribution factors. We evaluate our approach under both Gaussian and non-Gaussian distributions on synthetic and real-world datasets, demonstrating significant improvements in statistical accuracy and optimization performance compared to existing methods. |
| 2025-08-29 | [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](http://arxiv.org/abs/2508.21677v1) | Bernhard Wullt, Johannes K√∂hler et al. | Industrial manipulators are normally operated in cluttered environments, making safe motion planning important. Furthermore, the presence of model-uncertainties make safe motion planning more difficult. Therefore, in practice the speed is limited in order to reduce the effect of disturbances. There is a need for control methods that can guarantee safe motions that can be executed fast. We address this need by suggesting a novel model predictive control (MPC) solution for manipulators, where our two main components are a robust tube MPC and a corridor planning algorithm to obtain collision-free motion. Our solution results in a convex MPC, which we can solve fast, making our method practically useful. We demonstrate the efficacy of our method in a simulated environment with a 6 DOF industrial robot operating in cluttered environments with uncertainties in model parameters. We outperform benchmark methods, both in terms of being able to work under higher levels of model uncertainties, while also yielding faster motion. |
| 2025-08-29 | [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](http://arxiv.org/abs/2508.21648v1) | Farhad Abtahi, Mehdi Astaraki et al. | Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems. |
| 2025-08-29 | [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](http://arxiv.org/abs/2508.21620v1) | Agustinus Kristiadi | Decision theories offer principled methods for making choices under various types of uncertainty. Algorithms that implement these theories have been successfully applied to a wide range of real-world problems, including materials and drug discovery. Indeed, they are desirable since they can adaptively gather information to make better decisions in the future, resulting in data-efficient workflows. In scientific discovery, where experiments are costly, these algorithms can thus significantly reduce the cost of experimentation. Theoretical analyses of these algorithms are crucial for understanding their behavior and providing valuable insights for developing next-generation algorithms. However, theoretical analyses in the literature are often inaccessible to non-experts. This monograph aims to provide an accessible, self-contained introduction to the theoretical analysis of commonly used probabilistic decision-making algorithms, including bandit algorithms, Bayesian optimization, and tree search algorithms. Only basic knowledge of probability theory and statistics, along with some elementary knowledge about Gaussian processes, is assumed. |
| 2025-08-29 | [Universal Precision Limits in General Open Quantum Systems](http://arxiv.org/abs/2508.21567v1) | Tan Van Vu, Ryotaro Honma et al. | The intuition that the precision of observables is constrained by thermodynamic costs has recently been formalized through thermodynamic and kinetic uncertainty relations. While such trade-offs have been extensively studied in Markovian systems, corresponding constraints in the non-Markovian regime remain largely unexplored. In this Letter, we derive universal bounds on the precision of generic observables in open quantum systems coupled to environments of arbitrary strength and subjected to two-point measurements. By introducing an asymmetry term that quantifies the disparity between forward and backward processes, we show that the relative fluctuation of any time-antisymmetric current is constrained by both entropy production and this forward-backward asymmetry. For general observables, we prove that their relative fluctuation is always bounded from below by a generalized activity term. These results establish a comprehensive framework for understanding precision limits in broad classes of general open quantum systems. |
| 2025-08-29 | [Nuclear suppression in diffractive vector meson production within the color glass condensate framework](http://arxiv.org/abs/2508.21562v1) | Heikki M√§ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions within a Color Glass Condensate based framework. Using data from HERA and the LHC, we find that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ observables is challenging. Introducing a global $K$-factor to account for theoretical uncertainties improves the agreement with data and enhances the framework's predictive power. We present predictions for integrated $\mathrm{J}/\psi$ cross sections at different photon-nucleus energies and study their $A$-dependence relative to a no-saturation baseline, quantifying nuclear suppression and providing insights into the onset of saturation effects. |
| 2025-08-29 | [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](http://arxiv.org/abs/2508.21550v1) | Yujin Park, Haejun Chung et al. | Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking. |
| 2025-08-29 | [Adaptive extended Kalman filter and laser link acquisition in the detection of gravitational waves in space](http://arxiv.org/abs/2508.21538v1) | Jinke Yang, Yong Xie et al. | An alternative, new laser link acquisition scheme for the triangular constellation of spacecraft (SCs) in deep space in the detection of gravitational waves is considered. In place of a wide field CCD camera in the initial stage of laser link acquisition adopted in the conventional scheme, an extended Kalman filter based on precision orbit determination is incorporated in the point ahead angle mechanism (PAAM) to steer the laser beam in such a way to narrow the uncertainty cone and at the same time avoids the heating problem generated by the CCD camera.A quadrant photodetector (QPD) based on the Differential Power Sensing (DPS) technique, which offers a higher dynamic range than differential wavefront sensing (DWS), is employed as the readout of the laser beam spot. The conventional two stages (coarse acquisition and fine acquisition) are integrated into a single control loop. The payload structure of the ATP control loop is simplified and numerical simulations, based on a colored measurement noise model that closely mimics the prospective on-orbit conditions, demonstrate that the AEKF significantly reduces the initial uncertainty region by predicting the point ahead angle (PAA) even when the worst case scenario in SC position (navigation) error is considered. |
| 2025-08-28 | [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](http://arxiv.org/abs/2508.21007v1) | Mateusz Jaszczuk, Nadia Figueroa | With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming-based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system. |
| 2025-08-28 | [Practical indistinguishability in a gene regulatory network inference problem, a case study](http://arxiv.org/abs/2508.21006v1) | Cody E. FitzGerald, Shelley Reich et al. | Computationally inferring mechanistic insights from typical biological data is a challenging pursuit. Even the highest-quality experimental data come with challenges. There are always sources of noise, a limit to how often we can measure the system, and we can rarely measure all the relevant states that participate in the underlying complexity. There are usually sources of uncertainty in model development, which give rise to multiple competing model structures. To underscore the need for further analysis of structural uncertainty in modeling, we use a meta-analysis across six journals covering mathematical biology and show that a huge number of models for biological systems are developed each year, but model selection and comparison across model structures appear to be less common. We walk through a case study involving inference of regulatory network structure involved in a developmental decision in the nematode, \textit{Pristonchus pacificus}. We use real biological data and compare across 13,824 models--each corresponding to a different regulatory network structure, to determine which regulatory features are supported by the data across three experimental conditions. We find that the best-fitting models for each experimental condition share a combination of features and identify a regulatory network that is common across the model sets for each condition. This model can describe the data across the experimental conditions we considered and exhibits a high degree of positive regulation and interconnectivity between the key regulators, \textit{eud-1}, $textit{sult-1}, and \textit{nhr-40}. While the biological results are specific to the molecular biology of development in \textit{Pristonchus pacificus}, the general modeling framework and underlying challenges we faced doing this analysis are widespread across biology, chemistry, physics, and many other scientific disciplines. |
| 2025-08-28 | [Bottomonium transport in a strongly coupled quark-gluon plasma](http://arxiv.org/abs/2508.20995v1) | Biaogang Wu, Ralf Rapp | Quarkonium production in high-energy heavy-ion collisions remains a key probe of the quark-gluon plasma formed in these reactions, but the development of a fully integrated nonperturbative approach remains a challenge. Toward this end, we set up a semiclassical transport approach that combines nonperturbative reaction rates rooted in lattice-constrained $T$-matrix interactions with a viscous hydrodynamic medium evolution. Bottomonium suppression is computed along trajectories in the hydrodynamic evolution while regeneration is evaluated via a rate equation extended to a medium with spatial gradients. The much larger reaction rates compared to previous calculations markedly enhance both dissociation and regeneration processes. This, in particular, requires a reliable assessment of bottomonium equilibrium limits and of the non-thermal distributions of the bottom quarks transported through the expanding medium. Within current uncertainties our approach can describe the centrality dependence of bottomonium yields measured in Pb-Pb ($\sqrt{s_{_{\rm NN}}}$=5.02\,TeV) collisions at the LHC, while discrepancies are found at large transverse momenta. |
| 2025-08-28 | [Polynomial Chaos Expansion for Operator Learning](http://arxiv.org/abs/2508.20886v1) | Himanshu Sharma, Luk√°≈° Nov√°k et al. | Operator learning (OL) has emerged as a powerful tool in scientific machine learning (SciML) for approximating mappings between infinite-dimensional functional spaces. One of its main applications is learning the solution operator of partial differential equations (PDEs). While much of the progress in this area has been driven by deep neural network-based approaches such as Deep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recent work has begun to explore traditional machine learning methods for OL. In this work, we introduce polynomial chaos expansion (PCE) as an OL method. PCE has been widely used for uncertainty quantification (UQ) and has recently gained attention in the context of SciML. For OL, we establish a mathematical framework that enables PCE to approximate operators in both purely data-driven and physics-informed settings. The proposed framework reduces the task of learning the operator to solving a system of equations for the PCE coefficients. Moreover, the framework provides UQ by simply post-processing the PCE coefficients, without any additional computational cost. We apply the proposed method to a diverse set of PDE problems to demonstrate its capabilities. Numerical results demonstrate the strong performance of the proposed method in both OL and UQ tasks, achieving excellent numerical accuracy and computational efficiency. |
| 2025-08-28 | [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](http://arxiv.org/abs/2508.20812v1) | Lorenzo Busellato, Federico Cunico et al. | To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art. |
| 2025-08-28 | [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](http://arxiv.org/abs/2508.20806v1) | Moriba Jah, Van Haslett | Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified. |
| 2025-08-28 | [Time Series Embedding and Combination of Forecasts: A Reinforcement Learning Approach](http://arxiv.org/abs/2508.20795v1) | Marcelo C. Medeiros, Jeronymo M. Pinro | The forecasting combination puzzle is a well-known phenomenon in forecasting literature, stressing the challenge of outperforming the simple average when aggregating forecasts from diverse methods. This study proposes a Reinforcement Learning - based framework as a dynamic model selection approach to address this puzzle. Our framework is evaluated through extensive forecasting exercises using simulated and real data. Specifically, we analyze the M4 Competition dataset and the Survey of Professional Forecasters (SPF). This research introduces an adaptable methodology for selecting and combining forecasts under uncertainty, offering a promising advancement in resolving the forecasting combination puzzle. |
| 2025-08-28 | [Surfel-based 3D Registration with Equivariant SE(3) Features](http://arxiv.org/abs/2508.20789v1) | Xueyang Kang, Hang Zhao et al. | Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods. |
| 2025-08-28 | [A predictive solution of the EPR paradox](http://arxiv.org/abs/2508.20788v1) | Henryk Gzyl | In this work an incorrect argument in EPR's paper is corrected. A predictive approach to further confirm the validity of quantum theory is also proposed. The essence of the detail that EPR missed is that in a state of given total momentum (in their example the total momentum is zero), since the total momentum operator $\hat{\bp}=\hat{\bp}_1+\hat{\bp}_2$ does not commute with any of the position operators $\hat{\bx}_1$ and $\hat{\bx}_2,$ then in an eigenstate of the total momentum operator, the standard deviation on the measurement of the position of any of the two particles has to be infinite. Below we compute the (quantum) conditional expectation of the momentum of any of the particles (say, the first) given the value of a measurement of the total momentum. Since the predictor of the momentum of the particle depends on the total momentum, and the total momentum does not commute with the position operator of any of the particles, the uncertainty principle is not violated, and no contradiction appears. We also prove, using the basic tenets of quantum measurement theory, that after measurement of the total momentum, the probability density (computed as the absolute square of the wave function), coincides with the (quantum) conditional density given the total momentum. |
| 2025-08-28 | [Update of the CODE catalogue and some aspects of the dynamical status of Oort Cloud comets](http://arxiv.org/abs/2508.20780v1) | Piotr A. Dybczy≈Ñski, Ma≈Çgorzata Kr√≥likowska | Context. The outer Solar System is believed to host a vast reservoir of long-period comets (LPCs), but our understanding of their spatial distribution and dynamical history remains limited due to observational biases and uncertainties in orbital solutions for really observed comets. Aims. We aim to provide a comprehensive and dynamically homogeneous orbital database of LPCs to support the study of their origin, evolution, dynamical status, and 6D distribution of orbital elements. Methods. We updated the Catalogue of Cometary Orbits and their Dynamical Evolution (CODE catalogue) by computing original and future barycentric orbits, orbital parameters at previous and next perihelion, using full Monte Carlo swarms of real comets for the uncertainties estimation and taking into account the planetary, Galactic and passing stars perturbations according to the latest data and algorithms. Results. This update of the CODE catalogue focuses on the dynamical status of near-parabolic comets. Using current stellar data, we formulate new constraints for dynamically new comets. Now, the CODE database includes 983 orbital solutions for 369 comets with full uncertainty estimates and dynamical classifications, covering nearly all comets with original semi-major axes exceeding 10,000 au and discovered before 2022, as well as all LPCs discovered beyond 10 au from the Sun during during this period, and over 80% of the known LPCs with perihelion distances beyond 7 au. |
| 2025-08-27 | [A Partially Derivative-Free Proximal Method for Composite Multiobjective Optimization in the H√∂lder Setting](http://arxiv.org/abs/2508.20071v1) | V. S. Amaral, P. B. Assun√ß√£o et al. | This paper presents an algorithm for solving multiobjective optimization problems involving composite functions, where we minimize a quadratic model that approximates $F(x) - F(x^k)$ and that can be derivative-free. We establish theoretical assumptions about the component functions of the composition and provide comprehensive convergence and complexity analysis. Specifically, we prove that the proposed method converges to a weakly $\varepsilon$-approximate Pareto point in at most $\mathcal{O}\left(\varepsilon^{-\frac{\beta+1}{\beta}}\right)$ iterations, where $\beta$ denotes the H\"{o}lder exponent of the gradient. The algorithm incorporates gradient approximations and a scaling matrix $B_k$ to achieve an optimal balance between computational accuracy and efficiency. Numerical experiments on robust biobjective instances with Lipschitz and H\"{o}lder-gradient components illustrate the method's behavior. In these tests, the proposed approach was able to approximate the Pareto front under different levels of uncertainty and consistently recovered distinct solutions, even in challenging cases where the objectives have only H\"{o}lder continuous gradients. |
| 2025-08-27 | [Neural Conditional Simulation for Complex Spatial Processes](http://arxiv.org/abs/2508.20067v1) | Julia Walchessen, Andrew Zammit-Mangion et al. | A key objective in spatial statistics is to simulate from the distribution of a spatial process at a selection of unobserved locations conditional on observations (i.e., a predictive distribution) to enable spatial prediction and uncertainty quantification. However, exact conditional simulation from this predictive distribution is intractable or inefficient for many spatial process models. In this paper, we propose neural conditional simulation (NCS), a general method for spatial conditional simulation that is based on neural diffusion models. Specifically, using spatial masks, we implement a conditional score-based diffusion model that evolves Gaussian noise into samples from a predictive distribution when given a partially observed spatial field and spatial process parameters as inputs. The diffusion model relies on a neural network that only requires unconditional samples from the spatial process for training. Once trained, the diffusion model is amortized with respect to the observations in the partially observed field, the number and locations of those observations, and the spatial process parameters, and can therefore be used to conditionally simulate from a broad class of predictive distributions without retraining the neural network. We assess the NCS-generated simulations against simulations from the true conditional distribution of a Gaussian process model, and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick process model for spatial extremes. In the latter case, we show that it is more efficient and accurate to conditionally simulate using NCS than classical MCMC techniques implemented in standard software. We conclude that NCS enables efficient and accurate conditional simulation from spatial predictive distributions that are challenging to sample from using traditional methods. |
| 2025-08-27 | [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](http://arxiv.org/abs/2508.20066v1) | Zheng Li, Yanming Guo et al. | Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios. |
| 2025-08-27 | [Robust Paths: Geometry and Computation](http://arxiv.org/abs/2508.20039v1) | Hao Hao, Peter Zhang | Applying robust optimization often requires selecting an appropriate uncertainty set both in shape and size, a choice that directly affects the trade-off between average-case and worst-case performances. In practice, this calibration is usually done via trial-and-error: solving the robust optimization problem many times with different uncertainty set shapes and sizes, and examining their performance trade-off. This process is computationally expensive and ad hoc. In this work, we take a principled approach to study this issue for robust optimization problems with linear objective functions, convex feasible regions, and convex uncertainty sets. We introduce and study what we define as the robust path: a set of robust solutions obtained by varying the uncertainty set's parameters. Our central geometric insight is that a robust path can be characterized as a Bregman projection of a curve (whose geometry is defined by the uncertainty set) onto the feasible region. This leads to a surprising discovery that the robust path can be approximated via the trajectories of standard optimization algorithms, such as the proximal point method, of the deterministic counterpart problem. We give a sharp approximation error bound and show it depends on the geometry of the feasible region and the uncertainty set. We also illustrate two special cases where the approximation error is zero: the feasible region is polyhedrally monotone (e.g., a simplex feasible region under an ellipsoidal uncertainty set), or the feasible region and the uncertainty set follow a dual relationship. We demonstrate the practical impact of this approach in two settings: portfolio optimization and adversarial deep learning. |
| 2025-08-27 | [Pruning Strategies for Backdoor Defense in LLMs](http://arxiv.org/abs/2508.20032v1) | Santosh Chapagain, Shah Muhammad Hamdi et al. | Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks. |
| 2025-08-27 | [Tip of the Red Giant Branch Distances to NGC 1316, NGC 1380, NGC 1404, & NGC 4457: A Pilot Study of a Parallel Distance Ladder Using Type Ia Supernovae in Early-Type Host Galaxies](http://arxiv.org/abs/2508.20023v1) | Max J. B. Newman, Conor Larison et al. | Though type-Ia supernovae (SNe Ia) are found in all types of galaxies, recent local Hubble constant measurements have disfavored using SNe Ia in early-type or quiescent galaxies, aiming instead for better consistency with SNe Ia in star-forming, late-type host galaxies calibrated by Cepheid distances. Here we investigate the feasibility of a parallel distance ladder using SNe Ia exclusively in quiescent, massive ($\log M_*/M_{\odot} \geq 10$) host galaxies, calibrated by tip of the red giant branch (TRGB) distances. We present TRGB measurements to four galaxies: three measured from the Hubble Space Telescope with the ACS F814W filter, and one measured from the JWST NIRCam F090W filter. Combined with literature measurements, we define a TRGB calibrator sample of five high-mass, early-type galaxies that hosted well-measured SNe Ia: NGC 1316 (SN 2006dd), NGC 1380 (SN 1992A), NGC 1404 (SN 2007on, SN 2011iv), NGC 4457 (SN 2020nvb), and NGC 4636 (SN 2020ue). We jointly standardize these calibrators with a fiducial sample of 124 Hubble-flow SNe Ia from the Zwicky Transient Facility that are matched in host-galaxy and light-curve properties. Our results with this homogenized subsample show a Hubble residual scatter of under 0.11 mag, lower than usually observed in cosmological samples of the full SN~Ia distribution. We obtain a measurement of the Hubble constant, $H_0 = 75.3 \pm 2.9$ km s$^{-1}$ Mpc$^{-1}$, including statistical and estimated systematic uncertainties, and discuss the potential to further improve the precision of this approach. As calibrator and supernova samples grow, we advocate that future cosmological applications of SNe Ia use subsamples matched in host-galaxy and supernova properties across redshift. |
| 2025-08-27 | [Communicating astrobiology and the search for life elsewhere: speculations and promises of a developing scientific field in newspapers, press releases and papers](http://arxiv.org/abs/2508.19984v1) | Danilo Albergaria, Pedro Russo et al. | This study examines the communication of astrobiology and the Search for Life Elsewhere (SLE) in academic papers, press releases, and news articles over three decades. Through a quantitative content analysis, it investigates the prevalence of speculations and promises/expectations in these sources, aiming to understand how research results are portrayed and their potential impact on public perception and future research directions. Findings reveal that speculations and promises/expectations are more frequent in news articles and press releases compared to academic papers. Speculations about conditions for life and the existence of life beyond Earth are common, particularly in news articles covering exoplanet research, while promises of life detection are rare. Press releases tend to emphasize the significance of research findings and the progress of the field. Speculations and promises/expectations in news articles often occur without attribution to scientists and in quotes of authors of the studies, and slightly less so in quotes of outside experts. The study highlights the complex dynamics of science communication in astrobiology, where speculations and promises can generate public excitement and influence research funding, but also risk misrepresenting scientific uncertainty and creating unrealistic expectations. It underscores the need for responsible communication practices that acknowledge the speculative dimension of the field while fostering public engagement and informed decision-making. |
| 2025-08-27 | [Comment on Garc√≠a-Donato et al. (2025) "Model uncertainty and missing data: An objective Bayesian perspective"](http://arxiv.org/abs/2508.19939v1) | Joris Mulder | Garcia-Donato et al. (2025) present a methodology for handling missing data in a model selection problem using an objective Bayesian approach. The current comment discusses an alternative, existing objective Bayesian method for this problem. First, rather than using the g prior, O'Hagan's fractional Bayes factor (O'Hagan, 1995) is utilized based on a minimal fraction. Second, and more importantly due to the focus on missing data, Rubin's rules for multiple imputation can directly be used as the fractional Bayes factor can be written as a Savage-Dickey density ratio for a variable selection problem. The current comment derives the methodology for a variable selection problem. Moreover, its implied behavior is illustrated in a numerical experiment, showing competitive results as the method of Garcia-Donato et al. (2025). |
| 2025-08-27 | [Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition](http://arxiv.org/abs/2508.19933v1) | Sten Elling Tingstad Jacobsen, Bal√°zs Kulcs√°r et al. | The electrification and automation of mobility are reshaping how cities operate on-demand transport systems. Managing Electric Autonomous Mobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch, rebalancing, and charging decisions under multiple uncertainties, including travel demand, travel time, energy consumption, and charger availability. We address this challenge with a combined stochastic and robust model predictive control (MPC) framework. The framework integrates spatio-temporal Bayesian neural network forecasts with a multi-stage stochastic optimization model, formulated as a large-scale mixed-integer linear program. To ensure real-time applicability, we develop a tailored Nested Benders Decomposition that exploits the scenario tree structure and enables efficient parallelized solution. Stochastic optimization is employed to anticipate demand and infrastructure variability, while robust constraints on energy consumption and travel times safeguard feasibility under worst-case realizations. We evaluate the framework using high-fidelity simulations of San Francisco and Chicago. Compared with deterministic, reactive, and robust baselines, the combined stochastic and robust approach reduces median passenger waiting times by up to 36% and 95th-percentile delays by nearly 20%, while also lowering rebalancing distance by 27% and electricity costs by more than 35%. We also conduct a sensitivity analysis of battery size and vehicle efficiency, finding that energy-efficient vehicles maintain stable performance even with small batteries, whereas less efficient vehicles require larger batteries and greater infrastructure support. Our results emphasize the importance of jointly optimizing predictive control, vehicle capabilities, and infrastructure planning to enable scalable, cost-efficient EAMoD operations. |
| 2025-08-27 | [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](http://arxiv.org/abs/2508.19909v1) | Lechun You, Zhonghua Wu et al. | Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation. |
| 2025-08-26 | [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](http://arxiv.org/abs/2508.19191v1) | Yue Wang, Wenjie Deng et al. | Intraocular foreign body removal demands millimeter-level precision in confined intraocular spaces, yet existing robotic systems predominantly rely on manual teleoperation with steep learning curves. To address the challenges of autonomous manipulation (particularly kinematic uncertainties from variable motion scaling and variation of the Remote Center of Motion (RCM) point), we propose AutoRing, an imitation learning framework for autonomous intraocular foreign body ring manipulation. Our approach integrates dynamic RCM calibration to resolve coordinate-system inconsistencies caused by intraocular instrument variation and introduces the RCM-ACT architecture, which combines action-chunking transformers with real-time kinematic realignment. Trained solely on stereo visual data and instrument kinematics from expert demonstrations in a biomimetic eye model, AutoRing successfully completes ring grasping and positioning tasks without explicit depth sensing. Experimental validation demonstrates end-to-end autonomy under uncalibrated microscopy conditions. The results provide a viable framework for developing intelligent eye-surgical systems capable of complex intraocular procedures. |
| 2025-08-26 | [Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions](http://arxiv.org/abs/2508.19159v1) | Ersin Das, Rahal Nanayakkara et al. | Measurements and state estimates are often imperfect in control practice, posing challenges for safety-critical applications, where safety guarantees rely on accurate state information. In the presence of estimation errors, several prior robust control barrier function (R-CBF) formulations have imposed strict conditions on the input. These methods can be overly conservative and can introduce issues such as infeasibility, high control effort, etc. This work proposes a systematic method to improve R-CBFs, and demonstrates its advantages on a tracked vehicle that navigates among multiple obstacles. A primary contribution is a new optimization-based online parameter adaptation scheme that reduces the conservativeness of existing R-CBFs. In order to reduce the complexity of the parameter optimization, we merge several safety constraints into one unified numerical CBF via Poisson's equation. We further address the dual relative degree issue that typically causes difficulty in vehicle tracking. Experimental trials demonstrate the overall performance improvement of our approach over existing formulations. |
| 2025-08-26 | [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](http://arxiv.org/abs/2508.19150v1) | Juan Carlos Sabor√≠o, Marc Vinci et al. | Purposeful behavior in robotic assistants requires the integration of multiple components and technological advances. Often, the problem is reduced to recognizing explicit prompts, which limits autonomy, or is oversimplified through assumptions such as near-perfect information. We argue that a critical gap remains unaddressed -- specifically, the challenge of reasoning about the uncertain outcomes and perception errors inherent to human intention recognition. In response, we present a framework designed to be resilient to uncertainty and sensor noise, integrating real-time sensor data with a combination of planners. Centered around an intention-recognition POMDP, our approach addresses cooperative planning and acting under uncertainty. Our integrated framework has been successfully tested on a physical robot with promising results. |
| 2025-08-26 | [VPPE: Application of Scaled Vecchia Approximations to Parallel Partial Emulation](http://arxiv.org/abs/2508.19144v1) | Josh Seidman, Elaine T. Spiller | Computer models or simulators are widely used across scientific fields, but are computationally expensive limiting their use to explore possible scenarios/outcomes. Gaussian process emulators are statistical surrogates that can rapidly approximate the outputs of computer models at untested inputs and enable uncertainty quantification studies. The parallel partial emulation (PPE) was developed to model simulators with vector-valued outputs. While the PPE is adept at fitting simulator data with multidimensional outputs, the time to fit the PPE increases quickly as the number of training runs increases. The Scaled Vecchia approximation, a fast approximation to multivariate Gaussian likelihoods, makes fitting Gaussian process emulators with large training datasets tractable. Here we introduce the Vecchia Parallel Partial Emulation (VPPE) that utilizes the Scaled Vecchia approximation within the PPE framework to allow for parallel partial emulation with larger training datasets. The VPPE is applied to three computer experiments, a synthetic data set, a hydrology model, and a volcanic flow model, yielding comparable predictive accuracy to the PPE at a fraction of the runtime. |
| 2025-08-26 | [Using Machine Learning to Generate, Clarify, and Improve Economic Models](http://arxiv.org/abs/2508.19136v1) | Annie Liang | Machine learning algorithms can now outperform classic economic models in predicting quantities ranging from bargaining outcomes, to choice under uncertainty, to an individual's future jobs and wages. Yet this predictive accuracy comes at a cost: most machine learning algorithms function as black boxes, offering little insight into \emph{why} outcomes occur. This article asks whether machine learning can guide the development of new economic theories.   Economic models serve an important purpose beyond prediction -- they uncover the general mechanisms behind observed behaviors. A model that identifies the causal pathways of economic development is more valuable than one that merely predicts which countries will escape poverty, because it enables policymakers to encourage that development in countries where it might not have happened otherwise. Similarly, a model that predicts imperfectly across many domains can be more valuable than one that is highly accurate in a specific domain, since the former allows insights and data obtained from one setting to inform decisions and policy in another.   Applying machine learning algorithms off-the-shelf is unlikely to yield such models. But recent work shows that, when reconceived with the aims of an economic modeler in mind, machine learning methods can improve both prediction and understanding. These approaches range from adversarially training algorithms to expose the limits of existing models, to imposing economic theory as a constraint on algorithmic search. Advances in large language models complement these strategies and open new research directions. |
| 2025-08-26 | [Trustworthy Agents for Electronic Health Records through Confidence Estimation](http://arxiv.org/abs/2508.19096v1) | Yongwoo Song, Minbyul Jeong et al. | Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low. |
| 2025-08-26 | [Measurement of the branching fraction of $\psip \to œâŒ∑Œ∑$](http://arxiv.org/abs/2508.19092v1) | BESIII Collaboration, M. Ablikim et al. | Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to \omega \eta \eta $ is observed for the first time. The branching fraction of the $\psi(3686)\to\omega\eta\eta$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $\omega(1420)$ and $f_{0}(1710)$ resonances are observed in the $\omega\eta$ and $\eta\eta$ invariant-mass spectra, respectively. |
| 2025-08-26 | [SN2023syz and SN2025cbj: Two Type IIn Supernovae Associated with IceCube High-energy Neutrinos](http://arxiv.org/abs/2508.19080v1) | Ming-Xuan Lu, Yun-Feng Liang et al. | Type IIn supernovae (SNe IIn) are a subclass of core-collapse SNe in which strong interactions occur between the ejecta and dense circumstellar material, creating ideal conditions for the production of high-energy neutrinos. This makes them promising candidate sources of neutrinos. In this work, we conduct an association study between 163 SNe IIn observed by the Zwicky Transient Facility and 138 neutrino alert events detected by the IceCube neutrino observatory. After excluding alerts with poor localization, we find two SNe that are spatiotemporally coincident with neutrino events. IC231027A and IC250421A coincide with the positions of SN2023syz and SN2025cbj, respectively, within their localization uncertainties, and the neutrino arrival times are delayed by 38 days and 61 days relative to the discovery times of the corresponding SNe. Using Monte Carlo simulations, we estimate that the probability of such two coincidences occurring by chance in our sample is $p \sim 0.67\%$, suggesting a high likelihood that they arise from genuine associations, though the result is not yet statistical significant. Furthermore, model calculations show that the expected numbers of neutrino events from these SNe IIn could be consistent with the actual observations. Our study provides possible evidence that interacting SNe may be potential neutrino-emitting sources. |
| 2025-08-26 | [A Quick Estimation of Fr√©chet Quantizers for a Dynamic Solution to Flood Risk Management Problems](http://arxiv.org/abs/2508.19045v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization is a well-known quantitative tool for decision-making under uncertainty. It is broadly used in financial and investment planning, inventory control, and also natural disaster risk management. Theoretical solutions of multi-stage stochastic programs can be found explicitly only in very exceptional cases due to their variational form and interdependency of uncertainty in time. Nevertheless, numerical solutions are often inaccurate, as they rely on Monte-Carlo sampling, which requires the Law of Large Numbers to hold for the approximation quality. In this article, we introduce a new approximation scheme, which computes and groups together stage-wise optimal quantizers of conditional Fr\'echet distributions for optimal weighting of value functions in the dynamic programming. We consider optimality of scenario quantization methods in the sense of minimal Kantorovich-Wasserstein distance at each stage of the scenario tree. By this, we bound the approximation error with convergence guarantees. We also provide global solution guarantees under convexity and monotonicity conditions on the value function. We apply the developed methods to the governmental budget allocation problem for risk management of flood events in Austria. For this, we propose an extremely efficient way to approximate optimal quantizers for conditional Fr\'echet distributions. Our approach allows to enhance the overall efficiency of dynamic programming via the use of different parameter estimation methods for different groups of quantizers. The groups are distinguished by a particular risk threshold and are able to differentiate between higher- and lower-impact flood events. |
| 2025-08-26 | [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](http://arxiv.org/abs/2508.18937v1) | Wang Jiayin, Wei Yanran et al. | Autonomous control of the laparoscope in robot-assisted Minimally Invasive Surgery (MIS) has received considerable research interest due to its potential to improve surgical safety. Despite progress in pixel-level Image-Based Visual Servoing (IBVS) control, the requirement of continuous visibility and the existence of complex disturbances, such as parameterization error, measurement noise, and uncertainties of payloads, could degrade the surgeon's visual experience and compromise procedural safety. To address these limitations, this paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and uncertainty-adaptive framework for autonomous laparoscope control that guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian Process Regression (GPR) is utilized to perform hybrid (deterministic + stochastic) quantification of operational uncertainties including residual model uncertainties, stochastic uncertainties, and external disturbances. Based on uncertainty quantification, a novel safety aware trajectory optimization framework with probabilistic guarantees is proposed, where a uncertainty-adaptive safety Control Barrier Function (CBF) condition is given based on uncertainty propagation, and chance constraints are simultaneously formulated based on probabilistic approximation. This uncertainty aware formulation enables adaptive control effort allocation, minimizing unnecessary camera motion while maintaining robustness. The proposed method is validated through comparative simulations and experiments on a commercial surgical robot platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph node dissection. Compared with baseline methods, the framework maintains near-perfect target visibility (>99.9%), reduces tracking e |
| 2025-08-25 | [Flight-Ready Precise and Robust Carrier-Phase GNSS Navigation Software for Distributed Space Systems](http://arxiv.org/abs/2508.18246v1) | Samuel Y. W. Low, Toby Bell et al. | This paper presents the full requirements analysis, design, development, and testing of high-precision navigation flight software for Distributed Space Systems (DSS) using Carrier Phase Differential GNSS (CDGNSS). Five main contributions are made. First, a survey of flown and upcoming DSS missions with stringent precision requirements is conducted, from which a thorough requirements analysis is distilled to guide development and testing. Second, a real-time navigation functional architecture is designed, and adopts a sparse and regularized Consider Kalman Filter with options for numerical stability in-flight. The filter rigorously accounts for uncertainties in process noise, measurement noise, and biases. It tracks float ambiguities with integer resolution where possible. The covariance correlation structure is preserved under all navigation modes, including contingencies and outages. Third, a lightweight, memoryless Fault Detection, Isolation, and Recovery (FDIR) module is developed to guard against anomalous measurements, providing statistical screening and ensuring robust navigation. Fourth, the software architecture is proposed for ease of integration, with strategies presented for modularity and computational efficiency tailored to constrained flight systems. Fifth, a comprehensive test campaign is conducted, mapped to a requirements verification matrix, spanning unit, interface, software-in-the-loop, and real-time hardware-in-the-loop tests, emphasizing gradual test fidelity for efficient fault isolation. Finally, flight-like results are demonstrated using the VISORS mission, due to the generalizability of the VISORS navigation operations, and the stringency which demands sub-centimeter relative position and sub-millimeter-per-second velocity accuracy. This architecture aims to serve as a reference for next-generation DSS missions adopting CDGNSS. |
| 2025-08-25 | [Uncertain data assimilation for urban wind flow simulations with OpenLB-UQ](http://arxiv.org/abs/2508.18202v1) | Mingliang Zhong, Dennis Teutscher et al. | Accurate prediction of urban wind flow is essential for urban planning, pedestrian safety, and environmental management. Yet, it remains challenging due to uncertain boundary conditions and the high cost of conventional CFD simulations. This paper presents the use of the modular and efficient uncertainty quantification (UQ) framework OpenLB-UQ for urban wind flow simulations. We specifically use the lattice Boltzmann method (LBM) coupled with a stochastic collocation (SC) approach based on generalized polynomial chaos (gPC). The framework introduces a relative-error noise model for inflow wind speeds based on real measurements. The model is propagated through a non-intrusive SC LBM pipeline using sparse-grid quadrature. Key quantities of interest, including mean flow fields, standard deviations, and vertical profiles with confidence intervals, are efficiently computed without altering the underlying deterministic solver. We demonstrate this on a real urban scenario, highlighting how uncertainty localizes in complex flow regions such as wakes and shear layers. The results show that the SC LBM approach provides accurate, uncertainty-aware predictions with significant computational efficiency, making OpenLB-UQ a practical tool for real-time urban wind analysis. |
| 2025-08-25 | [New shell-model calculations of the $Œ¥_C$ correction to superallowed $0^+\rightarrow0^+$ nuclear $Œ≤$ decay and standard-model implications](http://arxiv.org/abs/2508.18189v1) | L. Xayavong, N. A. Smirnova et al. | Refined calculations of the radial mismatch correction, $\delta_{C2}$, to superallowed $0^+\rightarrow0^+$ nuclear $\beta$ decay are performed using the shell model with realistic Woods-Saxon radial wave functions. Two important improvements are introduced: i) charge radii used to constrain the length parameter are evaluated within a generalized formula, where proton occupation numbers are substituted by sums of spectroscopic factors, while radial wave functions are required to match separation energies with respect to the intermediate $(A-1)$-nucleon states by adjusting parameters such as the potential depth; ii) configuration mixing wave functions and energies for many-particle states are obtained through the diagonalization of well-established effective interactions in large configuration spaces without truncation. Furthermore, a variation of $\pm0.1$\,fm in the surface diffuseness parameter is now incorporated as a source of uncertainty. The present results are generally in fairly good agreement with those from previous studies. As an exception, the $\delta_{C2}$ value obtained for $^{18}$Ne is smaller by approximately a factor of two, principally due to the updated charge-radius treatment. A reduction is also observed in most cases with $A\ge38$, through the deviations generally remain within the newly assigned error bars. The smaller isospin-mixing counterpart, $\delta_{C1}$, is strongly interaction-dependent, roughly following an inverse-square law with respect to the energy separation between the lowest admixed levels. Therefore, an additional procedure to ensure isobaric displacements within the isospin multiplets appears to be indispensable. Our results for $\delta_{C2}$ lead to a new averaged $\overline{\mathcal{F}t}$ value of $3073.11(99)_{stat}(36)_{\delta_R'}(173)_{\delta_{NS}}$~s with $\chi^2/\nu=0.624$. The corresponding $|V_{ud}|$ value is 0.97359(33). |
| 2025-08-25 | [Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization](http://arxiv.org/abs/2508.18170v1) | Jonathan A. Moran, Pablo G. Morato | Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%. |
| 2025-08-25 | [Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation](http://arxiv.org/abs/2508.18142v1) | Tianjun Wei, Huizhong Guo et al. | User simulation is increasingly vital to develop and evaluate recommender systems (RSs). While Large Language Models (LLMs) offer promising avenues to simulate user behavior, they often struggle with the absence of specific domain alignment required for RSs and the efficiency demands of large-scale simulation. A vast yet underutilized resource for enhancing this alignment is the extensive user feedback inherent in RSs. However, directly leveraging such feedback presents two significant challenges. First, user feedback in RSs is often ambiguous and noisy, which negatively impacts effective preference alignment. Second, the massive volume of feedback largely hinders the efficiency of preference alignment, necessitating an efficient filtering mechanism to identify more informative samples. To overcome these hurdles, we introduce a novel data construction framework that leverages user feedback in RSs with advanced LLM capabilities to generate high-quality simulation data. Our framework unfolds in two key phases: (1) employing LLMs to generate cognitive decision-making processes on constructed simulation samples, reducing ambiguity in raw user feedback; (2) data distillation based on uncertainty estimation and behavior sampling to filter challenging yet denoised simulation samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using such high-quality dataset with corresponding decision-making processes. Extensive experiments verify that our framework significantly boosts the alignment with human preferences and in-domain reasoning capabilities of fine-tuned LLMs, and provides more insightful and interpretable signals when interacting with RSs. We believe our work will advance the RS community and offer valuable insights for broader human-centric AI research. |
| 2025-08-25 | [Forward-Backward Quantization of Scenario Processes in Multi-Stage Stochastic Optimization](http://arxiv.org/abs/2508.18112v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization lies at the core of decision-making under uncertainty. As the analytical solution is available only in exceptional cases, dynamic optimization aims to efficiently find approximations but often neglects non-Markovian time-interdependencies. Methods on scenario trees can represent such interdependencies but are subject to the curse of dimensionality. To ease this problem, researchers typically approximate the uncertainty by smaller but more accurate trees. In this article, we focus on multi-stage optimal tree quantization methods of time-interdependent stochastic processes, for which we develop novel bounds and demonstrate that the upper bound can be minimized via projected gradient descent incorporating the tree structure as linear constraints. Consequently, we propose an efficient quantization procedure, which improves forward-looking samples using a backward step on the tree.We apply the results to the multi-stage inventory control with time-interdependent demand. For the case with one product, we benchmark the approximation because the problem allows a solution in closed-form. For the multi-dimensional problem, our solution found by optimal discrete approximation demonstrates the importance of holding mitigation inventory in different phases of the product life cycle. |
| 2025-08-25 | [Measurement of the branching ratio of $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ isotopes through the nuclear muon capture reaction in the Super-Kamiokande detector](http://arxiv.org/abs/2508.18110v1) | Y. Maekawa, K. Abe et al. | The Super-Kamiokande detector has measured solar neutrinos for more than $25$ years. The sensitivity for solar neutrino measurement is limited by the uncertainties of energy scale and background modeling. Decays of unstable isotopes with relatively long half-lives through nuclear muon capture, such as $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$ and $\mathrm{^{13}B}$, are detected as background events for solar neutrino observations. In this study, we developed a method to form a pair of stopping muon and decay candidate events and evaluated the production rates of such unstable isotopes. We then measured their branching ratios considering both their production rates and the estimated number of nuclear muon capture processes as $Br(\mathrm{^{16}N})=(9.0 \pm 0.1)\%$, $Br(\mathrm{^{15}C})=(0.6\pm0.1)\%$, $Br(\mathrm{^{12}B})=(0.98 \pm 0.18)\%$, $Br(\mathrm{^{13}B})=(0.14 \pm 0.12)\%$, respectively. The result for $\mathrm{^{16}N}$ has world-leading precision at present and the results for $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ are the first branching ratio measurements for those isotopes. |
| 2025-08-25 | [The $Œõ_{b} \to Œõ$ transition form factors in perturbative QCD approach](http://arxiv.org/abs/2508.18069v1) | Lei Yang, Jia-Jie Han et al. | In this work, we investigate the $\Lambda_b \to \Lambda$ transition form factors in the perturbative QCD (PQCD) approach, incorporating higher-twist light-cone distribution amplitudes (LCDAs). The resulted form factors show that higher-twist LCDAs are dominant numerically. By combining our PQCD predictions at low-$q^2$ with lattice QCD results at high-$q^2$, $z$-series expansion fits are performed to parametrize the form factors over the full kinematic range. We also provide the prediction for physical observables in the rare decay $\Lambda_b \to \Lambda \mu^+ \mu^-$, including the differential branching fraction, dilepton longitudinal polarization fraction, and forward-backward asymmetries (lepton-side, hadron-side, and combined lepton-hadron). Our obtained form factors are consistent with those in other theoretical methods within the uncertainties. |
| 2025-08-25 | [Assessing the conditional calibration of interval forecasts using decompositions of the interval score](http://arxiv.org/abs/2508.18034v1) | Sam Allen, Julia Burnello et al. | Forecasts for uncertain future events should be probabilistic. Probabilistic forecasts are commonly issued as prediction intervals, which provide a measure of uncertainty in the unknown outcome whilst being easier to understand and communicate than full predictive distributions. The calibration of a $(1 - \alpha)$-level prediction interval can be assessed by checking whether the probability that the outcome falls within the interval is equal to $1 - \alpha$. However, such coverage checks are typically unconditional and therefore relatively weak. Although this is well known, there is a lack of methods to assess the conditional calibration of interval forecasts. In this work, we demonstrate how this can be achieved via decompositions of the well-known interval (or Winkler) score. We study notions of calibration for interval forecasts and then introduce a decomposition of the interval score based on isotonic distributional regression. This decomposition exhibits many desirable properties, both in theory and in practice, which allows users to accurately assess the conditional calibration of interval forecasts. This is illustrated on simulated data and in three applications to benchmark regression datasets. |
| 2025-08-25 | [Precision Measurements of the Electroweak Mixing Angle in the Region of the Z pole](http://arxiv.org/abs/2508.18022v1) | Arie Bodek, Hyon-San Seo et al. | We review the current status and techniques used in precision measurements of the effective leptonic weak mixing angle $\sin^2\theta^\ell_{\rm eff}$ (a fundamental parameter of the Standard Model (SM)) in the region of the Z pole with emphasis on hadron colliders. We also build on these techniques to extract the most precise single measurement to date of $\sin^2\theta^\ell_{\rm eff}$ from a new analysis of the published forward-backward asymmetry ($A_{\rm FB}$) in Drell-Yan dielpton production in proton-proton collisions at a center of mass energy of 13 TeV measured by the CMS collaboration at the large hadron collider. The uncertainty in $\sin^2\theta^\ell_{\rm eff}$ published by CMS is dominated by uncertainties in Parton Distribution Functions (PDFs), which are reduced by PDF profiling using the dilepton mass dependence of $A_{\rm FB}$. Our new extraction of $\sin^2\theta^\ell_{\rm eff}$ from the CMS values of $A_{\rm FB}$ includes profiling with additional new CMS measurements of the $W$-boson decay lepton asymmetry, and W/Z cross section ratio at 13 TeV. We obtain the most precise single measurement of $\sin^2\theta^\ell_{\rm eff}$ to date of 0.23153$\pm$0.00023, which is in excellent agreement with the SM prediction of 0.23161$\pm$0.00004. We also discuss outlook for future measurements at the LHC including more precise measurements of $\sin^2\theta^\ell_{\rm eff}$, a measurement of $\sin^2\theta^\ell_{\rm eff}$ for b-quarks in the initial state, and a measurement of the running of $\sin^2\theta^{\overline{\rm MS}}(\mu)$ up to 3 TeV. |
| 2025-08-22 | [A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer](http://arxiv.org/abs/2508.16569v1) | Yuhui Tao, Zhongwei Zhao et al. | The non-invasive assessment of increasingly incidentally discovered renal masses is a critical challenge in urologic oncology, where diagnostic uncertainty frequently leads to the overtreatment of benign or indolent tumors. In this study, we developed and validated RenalCLIP using a dataset of 27,866 CT scans from 8,809 patients across nine Chinese medical centers and the public TCIA cohort, a visual-language foundation model for characterization, diagnosis and prognosis of renal mass. The model was developed via a two-stage pre-training strategy that first enhances the image and text encoders with domain-specific knowledge before aligning them through a contrastive learning objective, to create robust representations for superior generalization and diagnostic precision. RenalCLIP achieved better performance and superior generalizability across 10 core tasks spanning the full clinical workflow of kidney cancer, including anatomical assessment, diagnostic classification, and survival prediction, compared with other state-of-the-art general-purpose CT foundation models. Especially, for complicated task like recurrence-free survival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726, representing a substantial improvement of approximately 20% over the leading baselines. Furthermore, RenalCLIP's pre-training imparted remarkable data efficiency; in the diagnostic classification task, it only needs 20% training data to achieve the peak performance of all baseline models even after they were fully fine-tuned on 100% of the data. Additionally, it achieved superior performance in report generation, image-text retrieval and zero-shot diagnosis tasks. Our findings establish that RenalCLIP provides a robust tool with the potential to enhance diagnostic accuracy, refine prognostic stratification, and personalize the management of patients with kidney cancer. |
| 2025-08-22 | [Exploring null-entropy events: What do we learn when nothing happens?](http://arxiv.org/abs/2508.16528v1) | Abhaya S. Hegde, Andr√© M. Timpanaro et al. | Fluctuation theorems establish that thermodynamic processes at the microscale can occasionally result in negative entropy production. At the microscale, another distinct possibility becomes more likely: processes where no entropy is produced overall. In this work, we explore the constraints imposed by such null-entropy events on the fluctuations of thermodynamic currents. By incorporating the probability of null-entropy events, we obtain tighter bounds on finite-time thermodynamic uncertainty relations derived from fluctuation theorems. We validate this framework using an example of a qudit SWAP engine. |
| 2025-08-22 | [Identifying Treatment Effect Heterogeneity with Bayesian Hierarchical Adjustable Random Partition in Adaptive Enrichment Trials](http://arxiv.org/abs/2508.16523v1) | Xianglin Zhao, Shirin Golchi et al. | Treatment effect heterogeneity refers to the systematic variation in treatment effects across subgroups. There is an increasing need for clinical trials that aim to investigate treatment effect heterogeneity and estimate subgroup-specific responses. While several statistical methods have been proposed to address this problem, existing partitioning-based methods often depend on auxiliary analysis, overlook model uncertainty, or impose inflexible borrowing strength. We propose the Bayesian Hierarchical Adjustable Random Partition (BHARP) model, a self-contained framework that applies a finite mixture model with an unknown number of components to explore the partition space accounting for model uncertainty. The BHARP model jointly estimates subgroup-specific effects and the heterogeneity patterns, and adjusts the borrowing strengths based on within-cluster cohesion without requiring manual calibration. Posterior sampling is performed via a custom reversible-jump Markov chain Monte Carlo sampler tailored to partitioning-based information borrowing in clinical trials. Simulation studies across a range of treatment effect heterogeneity patterns show that the BHARP model achieves better accuracy and precision compared to conventional and advanced methods. We showcase the utilities of the BHARP model in the context of a multi-arm adaptive enrichment trial investigating physical activity interventions in patients with type 2 diabetes. |
| 2025-08-22 | [Uncertainty Quantification and Propagation for ACORN, a geometric deep learning tracking pipeline for HEP experiments](http://arxiv.org/abs/2508.16518v1) | Lukas P√©ron, Paolo Calafiura et al. | We have developed an Uncertainty Quantification process for multistep pipelines and applied it to the ACORN particle tracking pipeline. All our experiments are made using the TrackML open dataset. Using the Monte Carlo Dropout method, we measure the data and model uncertainties of the pipeline steps, study how they propagate down the pipeline, and how they are impacted by the training dataset's size, the input data's geometry and physical properties. We will show that for our case study, as the training dataset grows, the overall uncertainty becomes dominated by aleatoric uncertainty, indicating that we had sufficient data to train the ACORN model we chose to its full potential. We show that the ACORN pipeline yields high confidence in the track reconstruction and does not suffer from the miscalibration of the GNN model. |
| 2025-08-22 | [Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean Modeling](http://arxiv.org/abs/2508.16489v1) | Yixuan Sun, Romain Egele et al. | Accurate simulations of the oceans are crucial in understanding the Earth system. Despite their efficiency, simulations at lower resolutions must rely on various uncertain parameterizations to account for unresolved processes. However, model sensitivity to parameterizations is difficult to quantify, making it challenging to tune these parameterizations to reproduce observations. Deep learning surrogates have shown promise for efficient computation of the parametric sensitivities in the form of partial derivatives, but their reliability is difficult to evaluate without ground truth derivatives. In this work, we leverage large-scale hyperparameter search and ensemble learning to improve both forward predictions, autoregressive rollout, and backward adjoint sensitivity estimation. Particularly, the ensemble method provides epistemic uncertainty of function value predictions and their derivatives, providing improved reliability of the neural surrogates in decision making. |
| 2025-08-22 | [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](http://arxiv.org/abs/2508.16476v1) | Maryam Ghasemzadeh, Anton van Beek | Multi-objective Bayesian optimization (MOBO) struggles with sparse (non-space-filling), scarce (limited observations) datasets affected by experimental uncertainty, where identical inputs can yield varying outputs. These challenges are common in physical and simulation experiments (e.g., randomized medical trials and, molecular dynamics simulations) and are therefore incompatible with conventional MOBO methods. As a result, experimental resources are inefficiently allocated, leading to suboptimal designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data Trust Region-based Optimization Algorithm), a novel sampling framework that integrates prior knowledge of experimental uncertainty to construct more accurate surrogate models while employing trust regions to focus sampling on promising areas of the design space. By strategically leveraging prior information and refining search regions, NOSTRA accelerates convergence to the Pareto frontier, enhances data efficiency, and improves solution quality. Through two test functions with varying levels of experimental uncertainty, we demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse, and scarce data. Specifically, we illustrate that, NOSTRA effectively prioritizes regions where samples enhance the accuracy of the identified Pareto frontier, offering a resource-efficient algorithm that is practical in scenarios with limited experimental budgets while ensuring efficient performance. |
| 2025-08-22 | [Scalable Bayesian inference on high-dimensional multivariate linear regression](http://arxiv.org/abs/2508.16446v1) | Xuan Cao, Kyoungjae Lee | We consider jointly estimating the coefficient matrix and the error precision matrix in high-dimensional multivariate linear regression models. Bayesian methods in this context often face computational challenges, leading to previous approaches that either utilize a generalized likelihood without ensuring the positive definiteness of the precision matrix or rely on maximization algorithms targeting only the posterior mode, thus failing to address uncertainty. In this work, we propose two Bayesian methods: an exact method and an approximate two-step method. We first propose an exact method based on spike and slab priors for the coefficient matrix and DAG-Wishart prior for the error precision matrix, whose computational complexity is comparable to the state-of-the-art generalized likelihood-based Bayesian method. To further enhance scalability, a two-step approach is developed by ignoring the dependency structure among response variables. This method estimates the coefficient matrix first, followed by the calculation of the posterior of the error precision matrix based on the estimated errors. We validate the two-step method by demonstrating (i) selection consistency and posterior convergence rates for the coefficient matrix and (ii) selection consistency for the directed acyclic graph (DAG) of errors. We demonstrate the practical performance of proposed methods through synthetic and real data analysis. |
| 2025-08-22 | [Dynamic Financial Analysis (DFA) of General Insurers under Climate Change](http://arxiv.org/abs/2508.16444v1) | Benjamin Avanzi, Yanfeng Li et al. | Climate change is expected to significantly affect the physical, financial, and economic environments over the long term, posing risks to the financial health of general insurers. While general insurers typically use Dynamic Financial Analysis (DFA) for a comprehensive view of financial impacts, traditional DFA as presented in the literature does not consider the impact of climate change. To address this gap, we introduce a climate-dependent DFA approach that integrates climate risk into DFA, providing a holistic assessment of the long-term impact of climate change on the general insurance industry. The proposed framework has three key features. First, it captures the long-term impact of climate change on the assets and liabilities of general insurers by considering both physical and economic dimensions across different climate scenarios within an interconnected structure. Second, it addresses the uncertainty of climate change impacts using stochastic simulations within climate scenario analysis that are useful for actuarial applications. Finally, the framework is tailored to the general insurance sector by addressing its unique characteristics. To demonstrate the practical application of our model, we conduct an extensive empirical study using Australian data to assess the long-term financial impact of climate change on the general insurance market under various climate scenarios. The results show that the interaction between economic growth and physical risk plays a key role in shaping general insurers' risk-return profiles. Limitations of our framework are thoroughly discussed. |
| 2025-08-22 | [Cost-optimized replacement strategies for water electrolysis systems affected by degradation](http://arxiv.org/abs/2508.16370v1) | Marie Arnold, Jonathan Brandt et al. | A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up. |
| 2025-08-22 | [Attenuation Models for Extensive Air Showers Derived from Simulations](http://arxiv.org/abs/2508.16360v1) | Fiona Ellwanger, Darko Veberiƒç | At ultra-high energies, the flux of cosmic rays is too low for direct measurements to be meaningful. When a cosmic ray enters the atmosphere, it initiates an extensive air shower, producing a cascade of secondary particles that propagate toward the ground. Large arrays of surface detectors are used to measure these secondary particles upon arrival.   The signal detected at a specific reference distance from the shower core serves as a proxy for the shower size and, consequently, as a reliable estimator of the energy of primary cosmic ray. However, shower development is influenced by attenuation effects: measured signals at the ground depend on the amount of traversed atmospheric density (column density) through which the shower evolves. Since the column density varies with the inclination of the shower, it is important to account for these attenuation effects to ensure accurate energy estimation.   In this study, we derive physics-and-geometry-based functional forms to describe attenuation and propose appropriate expansion terms using simple one-dimensional shower-development models, incorporating one or two main particle-cascade components. We then evaluate the applicability and effectiveness of these functional forms using a Monte-Carlo dataset that includes various primary cosmic-ray particles. By directly calibrating the the shower size derived from ground signals to the Monte-Carlo energy, we characterize attenuation behavior across different primary particles, assess the energy dependence of attenuation, and quantify systematic uncertainties introduced by different functional forms. |
| 2025-08-21 | [Bayesian Hierarchical Methods for Surveillance of Cervical Dystonia Treatments](http://arxiv.org/abs/2508.15762v1) | D. Baidoo, E. Kubuafor et al. | Cervical dystonia, a debilitating neurological disorder marked by involuntary muscle contractions and chronic pain, presents significant treatment challenges despite advances in botulinum toxin therapy. While botulinum toxin type B has emerged as one of the leading treatments, comparative efficacy across doses and the influence of demographic factors for personalized medicine remain understudied. This study aimed to: (1) compare the efficacy of different botulinum toxin type B doses using Bayesian methods, (2) evaluate demographic and clinical factors affecting treatment response, and (3) establish a probabilistic framework for personalized cervical dystonia management. We analyzed data from a multicenter randomized controlled trial involving 109 patients assigned to placebo, 5,000 units, or 10,000 units of botulinum toxin type B groups. The primary outcome was the Toronto Western Spasmodic Torticollis Rating Scale measured over 16 weeks. Bayesian hierarchical modeling assessed treatment effects while accounting for patient heterogeneity. Lower botulinum toxin type B doses (5,000 units) showed greater overall Toronto Western Spasmodic Torticollis Rating Scale score reductions (treatment effect: -2.39, 95% Probability Interval: -4.10 to -0.70). Male patients demonstrated better responses (5.2% greater improvement) than female patients. Substantial between-patient variability and site-specific effects were observed, highlighting the need for personalized protocols. The study confirms botulinum toxin type B's dose-dependent efficacy while identifying key modifiable factors in treatment response. Bayesian methods provided nuanced insights into uncertainty and heterogeneity, paving the way for personalized medicine in cervical dystonia management. |
| 2025-08-21 | [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](http://arxiv.org/abs/2508.15719v1) | Mohammed Elmusrati | Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning. |
| 2025-08-21 | [CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps](http://arxiv.org/abs/2508.15672v1) | Franz Hanke, Antonia Bieringer et al. | Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3 |
| 2025-08-21 | [Beyond the Nyquist frequency: Asteroseismic catalog of undersampled Kepler late subgiants and early red giants](http://arxiv.org/abs/2508.15654v1) | B. Liagre, R. A. Garc√≠a et al. | Subgiants and early red giants are crucial for studying the first dredge-up, a key evolutionary phase where the convective envelope deepens, mixing previously interior-processed material and bringing it to the surface. Yet, very few have been seismically characterized with Kepler because their oscillation frequencies are close to the 30 minute sampling frequency of the mission. We developed a new method as part of the new PyA2Z code to identify super-Nyquist oscillators and infer their global seismic parameters, $\nu_\mathrm{max}$ and large separation, $\Delta\nu$. Applying PyA2Z to 2 065 Kepler targets, we seismically characterize 285 super-Nyquist and 168 close-to-Nyquist stars with masses from 0.8 to 1.6 M$_\odot$. In combination with APOGEE spectroscopy, Gaia spectro-photometry, and stellar models, we derive stellar ages for the sample. There is good agreement between the predicted and actual positions of stars on the HR diagram (luminosity vs. effective temperature) as a function of mass and composition. While the timing of dredge-up is consistent with predictions, the magnitude and mass dependence show discrepancies with models, possibly due to uncertainties in model physics or calibration issues in observed abundance scales. |
| 2025-08-21 | [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2508.15652v1) | Ardian Selmonaj, Miroslav Strupl et al. | To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems. |
| 2025-08-21 | [Label Uncertainty for Ultrasound Segmentation](http://arxiv.org/abs/2508.15635v1) | Malini Shivaram, Gautam Rajendrakumar Gare et al. | In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging. |
| 2025-08-21 | [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](http://arxiv.org/abs/2508.15588v1) | Ahmed Nasir, Abdelhafid Zenati | The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden "skeleton" governing the system's behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the system's convergence properties and potential failure modes, such as unintended "trap" states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policy's safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone. |
| 2025-08-21 | [LoUQAL: Low-fidelity informed Uncertainty Quantification for Active Learning in the chemical configuration space](http://arxiv.org/abs/2508.15577v1) | Vivin Vinod, Peter Zaspel | Uncertainty quantification is an important scheme in active learning techniques, including applications in predicting quantum chemical properties. In quantum chemical calculations, there exists the notion of a fidelity, a less accurate computation is accessible at a cheaper computational cost. This work proposes a novel low-fidelity informed uncertainty quantification for active learning with applications in predicting diverse quantum chemical properties such as excitation energies and \textit{ab initio} potential energy surfaces. Computational experiments are carried out in order to assess the proposed method with results demonstrating that models trained with the novel method outperform alternatives in terms of empirical error and number of iterations required. The effect of the choice of fidelity is also studied to perform a thorough benchmark. |
| 2025-08-21 | [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](http://arxiv.org/abs/2508.15569v1) | Xin Du, Sikun Yang et al. | Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal prediction's rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the framework's effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification. |
| 2025-08-21 | [Uncertainty Relation for the Wigner-Yanase Skew Information and Quantum Sobolev Inequalities](http://arxiv.org/abs/2508.15554v1) | Laurent Lafleche | This note explores uncertainty inequalities for quantum analogues of the Fisher information including the Wigner-Yanase skew information, and their connection to the quantum Sobolev inequalities proved by the author in [Journal of Functional Analysis, 286 (10) 2024]. Some additional inequalities concerning commutators are derived and others are left as open problems. |
| 2025-08-20 | [Squeezed Diffusion Models](http://arxiv.org/abs/2508.14871v1) | Jyotirmai Singh, Samar Khanna et al. | Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes. |
| 2025-08-20 | [Calibration offset estimation in mobile hearing tests via categorical loudness scaling](http://arxiv.org/abs/2508.14824v1) | Chen Xu, Birger Kollmeier | Objective: To enable reliable smartphone-based hearing assessments by developing methods to estimate device calibration offsets using categorical loudness scaling (CLS). Design: Calibration offsets were simulated from a Gaussian distribution. Two prediction models - a Bayesian regression model and a nearest neighbor model - were trained on CLS-derived parameters and data from the Oldenburg Hearing Health Repository (OHHR). CLS was chosen because it provides level-independent measures (e.g., dynamic range) that remain robust despite calibration errors. Study Sample: The dataset comprised CLS results from N = 847 participants with a mean age of 70.0 years (SD = 8.7), including 556 male and 291 female listeners with diverse hearing profiles. Results: The Bayesian regression model achieved correlations of up to 0.81 between estimated and true calibration offsets, enabling accurate individual-level correction. Compared to threshold-based approaches, calibration uncertainty was reduced by factors between 0.41 and 0.79, demonstrating greater robustness in uncontrolled environments. Conclusions: CLS-based models can effectively compensate for missing calibration in mobile hearing assessments. This approach provides a practical alternative to threshold-based methods, supporting the use of smartphone-based tests outside laboratory settings and expanding access to reliable hearing healthcare in everyday and resource-limited contexts. |
| 2025-08-20 | [Equipartition and the temperature of maximum density of TIP4/2005 water](http://arxiv.org/abs/2508.14800v1) | Dilipkumar N. Asthagiri, Thomas L. Beck | We simulate TIP4P/2005 water in the temperature range of 257 K to 318 K with time steps of $\delta =$ 0.25, 0.50, and 2.0 fs. Within the computed statistical uncertainties, the density-temperature behavior obtained using 0.25 fs and 0.50 fs are in excellent agreement with each other but differ from those obtained using $\delta t = 2.0$ fs, a choice that leads to a breakdown of equipartition. The temperature of maximum density (TMD) is 277.15 K with $\delta t = $0.25 fs or 0.50 fs, but is shifted to 273.15 K for simulations with $\delta t = 2.0$ fs. This shift is comparable in magnitude to the shift in TMD due to nuclear quantum effects, emphasizing the care required in the parameterization and classical statistical mechanical simulation of a fluid that displays nontrivial nuclear quantum effects under ambient conditions. Enhancing the water-water dispersion interaction, as has been recommended for modeling disordered solvated proteins, degrades the description of the liquid-vapor phase envelope. |
| 2025-08-20 | [Safe and Transparent Robots for Human-in-the-Loop Meat Processing](http://arxiv.org/abs/2508.14763v1) | Sagar Parekh, Casey Grothoff et al. | Labor shortages have severely affected the meat processing sector. Automated technology has the potential to support the meat industry, assist workers, and enhance job quality. However, existing automation in meat processing is highly specialized, inflexible, and cost intensive. Instead of forcing manufacturers to buy a separate device for each step of the process, our objective is to develop general-purpose robotic systems that work alongside humans to perform multiple meat processing tasks. Through a recently conducted survey of industry experts, we identified two main challenges associated with integrating these collaborative robots alongside human workers. First, there must be measures to ensure the safety of human coworkers; second, the coworkers need to understand what the robot is doing. This paper addresses both challenges by introducing a safety and transparency framework for general-purpose meat processing robots. For safety, we implement a hand-detection system that continuously monitors nearby humans. This system can halt the robot in situations where the human comes into close proximity of the operating robot. We also develop an instrumented knife equipped with a force sensor that can differentiate contact between objects such as meat, bone, or fixtures. For transparency, we introduce a method that detects the robot's uncertainty about its performance and uses an LED interface to communicate that uncertainty to the human. Additionally, we design a graphical interface that displays the robot's plans and allows the human to provide feedback on the planned cut. Overall, our framework can ensure safe operation while keeping human workers in-the-loop about the robot's actions which we validate through a user study. |
| 2025-08-20 | [Distributional Adversarial Attacks and Training in Deep Hedging](http://arxiv.org/abs/2508.14757v1) | Guangyi He, Tobias Sutter et al. | In this paper, we study the robustness of classical deep hedging strategies under distributional shifts by leveraging the concept of adversarial attacks. We first demonstrate that standard deep hedging models are highly vulnerable to small perturbations in the input distribution, resulting in significant performance degradation. Motivated by this, we propose an adversarial training framework tailored to increase the robustness of deep hedging strategies. Our approach extends pointwise adversarial attacks to the distributional setting and introduces a computationally tractable reformulation of the adversarial optimization problem over a Wasserstein ball. This enables the efficient training of hedging strategies that are resilient to distributional perturbations. Through extensive numerical experiments, we show that adversarially trained deep hedging strategies consistently outperform their classical counterparts in terms of out-of-sample performance and resilience to model misspecification. Our findings establish a practical and effective framework for robust deep hedging under realistic market uncertainties. |
| 2025-08-20 | [Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds](http://arxiv.org/abs/2508.14661v1) | Alexander Raab, Stephan Weiss et al. | Aiming to enhance the consistency and thus long-term accuracy of Extended Kalman Filters for terrestrial vehicle localization, this paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the robot's pose in a space with reduced dimensionality, the approach ensures feasible estimates on generic smooth surfaces, without introducing artificial constraints or simplifications that may degrade a filter's performance. The accompanying measurement models are compatible with common loosely- and tightly-coupled sensor modalities and also implicitly account for the ground geometry. We extend the formulation by introducing a novel correction scheme that embeds additional domain knowledge into the sensor data, giving more accurate uncertainty approximations and further enhancing filter consistency. The proposed estimator is seamlessly integrated into a validated modular state estimation framework, demonstrating compatibility with existing implementations. Extensive Monte Carlo simulations across diverse scenarios and dynamic sensor configurations show that the M-ESEKF outperforms classical filter formulations in terms of consistency and stability. Moreover, it eliminates the need for scenario-specific parameter tuning, enabling its application in a variety of real-world settings. |
| 2025-08-20 | [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](http://arxiv.org/abs/2508.14654v1) | Peilin Ji, Xiao Xue et al. | In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response. |
| 2025-08-20 | [Experimental validation of universal filtering and smoothing for linear system identification using adaptive tuning](http://arxiv.org/abs/2508.14629v1) | Zihao Liu, Sima Abolghasemi et al. | In Kalman filtering, unknown inputs are often estimated by augmenting the state vector, which introduces reliance on fictitious input models. In contrast, minimum-variance unbiased methods estimate inputs and states separately, avoiding fictitious models but requiring strict sensor configurations, such as full-rank feedforward matrices or without direct feedthrough. To address these limitations, two universal approaches have been proposed to handle systems with or without direct feedthrough, including cases of rank-deficient feedforward matrices. Numerical studies have shown their robustness and applicability, however, they have so far relied on offline tuning, and performance under physical sensor noise and structural uncertainties has not yet been experimentally validated. Contributing to this gap, this paper experimentally validates the universal methods on a five-storey shear frame subjected to shake table tests and multi-impact events. Both typical and rank-deficient conditions are considered. Furthermore, a self-tuning mechanism is introduced to replace impractical offline tuning and enable real-time adaptability. The findings of this paper provide strong evidence of the robustness and adaptability of the methods for structural health monitoring applications, particularly when sensor networks deviate from ideal configurations. |
| 2025-08-20 | [A Simple and Scalable Kernel Density Approach for Reliable Uncertainty Quantification in Atomistic Machine Learning](http://arxiv.org/abs/2508.14613v1) | Daniel Willimetz, Luk√°≈° Grajciar | Machine learning models are increasingly used to predict material properties and accelerate atomistic simulations, but the reliability of their predictions depends on the representativeness of the training data. We present a scalable, GPU-accelerated uncertainty quantification framework based on $k$-nearest-neighbor kernel density estimation (KDE) in a PCA-reduced descriptor space. This method efficiently detects sparsely sampled regions in large, high-dimensional datasets and provides a transferable, model-agnostic uncertainty metric without requiring retraining costly model ensembles. The framework is validated across diverse case studies varying in: i) chemistry, ii) prediction models (including foundational neural network), iii) descriptors used for KDE estimation, and iv) properties whose uncertainty is sought. In all cases, the KDE-based score reliably flags extrapolative configurations, correlates well with conventional ensemble-based uncertainties, and highlights regions of reduced prediction trustworthiness. The approach offers a practical route for improving the interpretability, robustness, and deployment readiness of ML models in materials science. |
| 2025-08-20 | [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](http://arxiv.org/abs/2508.14597v1) | Nitish Kumar Mahala, Muzammil Khan et al. | Fire outbreaks pose critical threats to human life and infrastructure, necessitating high-fidelity early-warning systems that detect combustion precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal dynamics influenced by illumination variability, flow kinematics, and environmental noise, undermining the reliability of traditional detectors. To address these challenges without the logistical complexity of multi-sensor arrays, we propose an information-fusion framework by integrating smoke feature representations extracted from monocular imagery. Specifically, a Two-Phase Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke detection, leveraging a novel smoke segmentation dataset, constructed via optical flow-based motion encoding, is proposed. The optical flow estimation is performed with a four-color-theorem-inspired dual-phase level-set fractional-order variational model, which preserves motion discontinuities. The resulting color-encoded optical flow maps are fused with appearance cues via a Gaussian Mixture Model to generate binary segmentation masks of the smoke regions. These fused representations are fed into the novel Shifted-Windows Transformer, which is augmented with a multi-scale uncertainty estimation head and trained under a two-phase learning regimen. First learning phase optimizes smoke detection accuracy, while during the second phase, the model learns to estimate plausibility confidence in its predictions by jointly modeling aleatoric and epistemic uncertainties. Extensive experiments using multiple evaluation metrics and comparative analysis with state-of-the-art approaches demonstrate superior generalization and robustness, offering a reliable solution for early fire detection in surveillance, industrial safety, and autonomous monitoring applications. |
| 2025-08-19 | [BLIPs: Bayesian Learned Interatomic Potentials](http://arxiv.org/abs/2508.14022v1) | Dario Coscia, Pim de Haan et al. | Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool in simulation-based chemistry. However, like most deep learning models, MLIPs struggle to make accurate predictions on out-of-distribution data or when trained in a data-scarce regime, both common scenarios in simulation-based chemistry. Moreover, MLIPs do not provide uncertainty estimates by construction, which are fundamental to guide active learning pipelines and to ensure the accuracy of simulation results compared to quantum calculations. To address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian framework for training or fine-tuning MLIPs, built on an adaptive version of Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and minimal computational overhead for energy and forces prediction at inference time, while integrating seamlessly with (equivariant) message-passing architectures. Empirical results on simulation-based computational chemistry tasks demonstrate improved predictive accuracy with respect to standard MLIPs, and trustworthy uncertainty estimates, especially in data-scarse or heavy out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP yields consistent performance gains and calibrated uncertainties. |
| 2025-08-19 | [Data Compression with Noise Suppression for Inference under Noisy Covariance](http://arxiv.org/abs/2508.14021v1) | Sunao Sugiyama, Minsu Park | In many fields including cosmology, statistical inference often relies on Gaussian likelihoods whose covariance matrices are estimated from a finite number of simulations. This finite-sample estimation introduces noise into the covariance, which propagates to parameter estimates, a phenomenon known as the Dodelson-Schneider (DS) effect, leading to inflated uncertainties. While the Massively Optimized Parameter Estimation and Data compression (MOPED) algorithm offers lossless Fisher information-preserving compression, it does not mitigate the DS effect when the compression matrix itself is derived from noisy covariances. In this paper, we propose a modified compression scheme, powered MOPED ($p$-MOPED), which suppresses noise propagation by balancing information retention and covariance estimate noise reduction through a tunable power-law transformation of the sample correlation matrix. We test $p$-MOPED against standard and diagonal MOPED on toy models and on cosmological data from the Subaru Hyper Suprime-Cam Year 3 weak lensing survey. Our results demonstrate that $p$-MOPED consistently outperforms other approaches, especially in regimes with limited simulations, offering a robust compression strategy for high-dimensional data analyses under practical constraints. |
| 2025-08-19 | [Dark Energy Survey Year 3 Results: Cosmological constraints from second and third-order shear statistics](http://arxiv.org/abs/2508.14018v1) | R. C. H. Gomes, S. Sugiyama et al. | We present a cosmological analysis of the third-order aperture mass statistic using Dark Energy Survey Year 3 (DES Y3) data. We perform a complete tomographic measurement of the three-point correlation function of the Y3 weak lensing shape catalog with the four fiducial source redshift bins. Building upon our companion methodology paper, we apply a pipeline that combines the two-point function $\xi_{\pm}$ with the mass aperture skewness statistic $\langle M_{\rm ap}^3\rangle$, which is an efficient compression of the full shear three-point function. We use a suite of simulated shear maps to obtain a joint covariance matrix. By jointly analyzing $\xi_\pm$ and $\langle M_{\rm ap}^3\rangle$ measured from DES Y3 data with a $\Lambda$CDM model, we find $S_8=0.780\pm0.015$ and $\Omega_{\rm m}=0.266^{+0.039}_{-0.040}$, yielding 111% of figure-of-merit improvement in $\Omega_m$-$S_8$ plane relative to $\xi_{\pm}$ alone, consistent with expectations from simulated likelihood analyses. With a $w$CDM model, we find $S_8=0.749^{+0.027}_{-0.026}$ and $w_0=-1.39\pm 0.31$, which gives an improvement of $22\%$ on the joint $S_8$-$w_0$ constraint. Our results are consistent with $w_0=-1$. Our new constraints are compared to CMB data from the Planck satellite, and we find that with the inclusion of $\langle M_{\rm ap}^3\rangle$ the existing tension between the data sets is at the level of $2.3\sigma$. We show that the third-order statistic enables us to self-calibrate the mean photometric redshift uncertainty parameter of the highest redshift bin with little degradation in the figure of merit. Our results demonstrate the constraining power of higher-order lensing statistics and establish $\langle M_{\rm ap}^3\rangle$ as a practical observable for joint analyses in current and future surveys. |
| 2025-08-19 | [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](http://arxiv.org/abs/2508.13990v1) | Daniel Kl√∂tzl, Ozan Tastekin et al. | Multidimensional data is often associated with uncertainties that are not well-described by normal distributions. In this work, we describe how such distributions can be projected to a low-dimensional space using uncertainty-aware principal component analysis (UAPCA). We propose to model multidimensional distributions using Gaussian mixture models (GMMs) and derive the projection from a general formulation that allows projecting arbitrary probability density functions. The low-dimensional projections of the densities exhibit more details about the distributions and represent them more faithfully compared to UAPCA mappings. Further, we support including user-defined weights between the different distributions, which allows for varying the importance of the multidimensional distributions. We evaluate our approach by comparing the distributions in low-dimensional space obtained by our method and UAPCA to those obtained by sample-based projections. |
| 2025-08-19 | [GBEES-GPU: An efficient parallel GPU algorithm for high-dimensional nonlinear uncertainty propagation](http://arxiv.org/abs/2508.13986v1) | Benjamin L. Hanson, Carlos Rubio et al. | Eulerian nonlinear uncertainty propagation methods often suffer from finite domain limitations and computational inefficiencies. A recent approach to this class of algorithm, Grid-based Bayesian Estimation Exploiting Sparsity, addresses the first challenge by dynamically allocating a discretized grid in regions of phase space where probability is non-negligible. However, the design of the original algorithm causes the second challenge to persist in high-dimensional systems. This paper presents an architectural optimization of the algorithm for CPU implementation, followed by its adaptation to the CUDA framework for single GPU execution. The algorithm is validated for accuracy and convergence, with performance evaluated across distinct GPUs. Tests include propagating a three-dimensional probability distribution subject to the Lorenz '63 model and a six-dimensional probability distribution subject to the Lorenz '96 model. The results imply that the improvements made result in a speedup of over 1000 times compared to the original implementation. |
| 2025-08-19 | [Dynamics-independent bounds on state transformations and precision in open quantum systems](http://arxiv.org/abs/2508.13884v1) | Yoshihiko Hasegawa | We derive dynamics-independent upper bounds on achievable quantum state transformations. Modeling the evolution as a joint unitary on the system and its environment, we show that the R\'enyi divergence between the initial system state and any state reachable via the dynamics is bounded from above by a quantity determined solely by the eigenvalues of the initial system and environment density operators. As a consequence, we establish dynamics-independent lower bounds on the relative variance for arbitrary measurements, which parallel thermodynamic uncertainty relations. Moreover, we obtain dynamics- and measurement-independent lower bounds on the variance of parameter estimators. These results depend only on the initial eigenvalues of the system and environment and hold for any joint unitary, providing computable bounds for open quantum systems. |
| 2025-08-19 | [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](http://arxiv.org/abs/2508.13874v1) | Rouqaiah Al-Refai, Pankaja Priya Ramasamy et al. | The rapid advancement of authentication systems and their increasing reliance on biometrics for faster and more accurate user verification experience, highlight the critical need for a reliable framework to evaluate the suitability of biometric modalities for specific applications. Currently, the most widely known evaluation framework is a comparative table from 1998, which no longer adequately captures recent technological developments or emerging vulnerabilities in biometric systems. To address these challenges, this work revisits the evaluation of biometric modalities through an expert survey involving 24 biometric specialists. The findings indicate substantial shifts in property ratings across modalities. For example, face recognition, shows improved ratings due to technological progress, while fingerprint, shows decreased reliability because of emerging vulnerabilities and attacks. Further analysis of expert agreement levels across rated properties highlighted the consistency of the provided evaluations and ensured the reliability of the ratings. Finally, expert assessments are compared with dataset-level uncertainty across 55 biometric datasets, revealing strong alignment in most modalities and underscoring the importance of integrating empirical evidence with expert insight. Moreover, the identified expert disagreements reveal key open challenges and help guide future research toward resolving them. |
| 2025-08-19 | [OpenLB-UQ: An Uncertainty Quantification Framework for Incompressible Fluid Flow Simulations](http://arxiv.org/abs/2508.13867v1) | Mingliang Zhong, Adrian Kummerl√§nder et al. | Uncertainty quantification (UQ) is crucial in computational fluid dynamics to assess the reliability and robustness of simulations, given the uncertainties in input parameters. OpenLB is an open-source lattice Boltzmann method library designed for efficient and extensible simulations of complex fluid dynamics on high-performance computers. In this work, we leverage the efficiency of OpenLB for large-scale flow sampling with a dedicated and integrated UQ module. To this end, we focus on non-intrusive stochastic collocation methods based on generalized polynomial chaos and Monte Carlo sampling. The OpenLB-UQ framework is extensively validated in convergence tests with respect to statistical metrics and sample efficiency using selected benchmark cases, including two-dimensional Taylor--Green vortex flows with up to four-dimensional uncertainty and a flow past a cylinder. Our results confirm the expected convergence rates and show promising scalability, demonstrating robust statistical accuracy as well as computational efficiency. OpenLB-UQ enhances the capability of the OpenLB library, offering researchers a scalable framework for UQ in incompressible fluid flow simulations and beyond. |
| 2025-08-19 | [Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems](http://arxiv.org/abs/2508.13839v1) | Yue Xiu, Yang Zhao et al. | The cell-free integrated sensing and communication (CF-ISAC) architecture is a promising enabler for 6G, offering spectrum efficiency and ubiquitous coverage. However, real deployments suffer from hardware impairments, especially nonlinear distortion from power amplifiers (PAs), which degrades both communication and sensing. To address this, we propose a movable antenna (MA)-aided CF-ISAC system that mitigates distortion and enhances robustness. The PAs nonlinearities are modeled by a third-order memoryless polynomial, where the third-order distortion coefficients (3RDCs) vary across access points (APs) due to hardware differences, aging, and environmental conditions. We design a distributed distortion-aware worst-case robust optimization framework that explicitly incorporates uncertainty in 3RDCs. First, we analyze the worst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB) and communication rate. Then, to address the resulting non-convexity, we apply successive convex approximation (SCA) for estimating the 3RDCs. With these, we jointly optimize beamforming and MA positions under transmit power and sensing constraints. To efficiently solve this highly non-convex problem, we develop an MA-enabled self-attention convolutional graph neural network (SACGNN) algorithm. Simulations demonstrate that our method substantially enhances the communication-sensing trade-off under distortion and outperforms fixed-position antenna baselines in terms of robustness and capacity, thereby highlighting the advantages of MA-aided CF-ISAC systems. |
| 2025-08-19 | [Online Conformal Selection with Accept-to-Reject Changes](http://arxiv.org/abs/2508.13838v1) | Kangdao Liu, Huajun Xi et al. | Selecting a subset of promising candidates from a large pool is crucial across various scientific and real-world applications. Conformal selection offers a distribution-free and model-agnostic framework for candidate selection with uncertainty quantification. While effective in offline settings, its application to online scenarios, where data arrives sequentially, poses challenges. Notably, conformal selection permits the deselection of previously selected candidates, which is incompatible with applications requiring irreversible selection decisions. This limitation is particularly evident in resource-intensive sequential processes, such as drug discovery, where advancing a compound to subsequent stages renders reversal impractical. To address this issue, we extend conformal selection to an online Accept-to-Reject Changes (ARC) procedure: non-selected data points can be reconsidered for selection later, and once a candidate is selected, the decision is irreversible. Specifically, we propose a novel conformal selection method, Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC), which incorporates online Benjamini-Hochberg procedure into the candidate selection process. We provide theoretical guarantees that OCS-ARC controls the false discovery rate (FDR) at or below the nominal level at any timestep under both i.i.d. and exchangeable data assumptions. Additionally, we theoretically show that our approach naturally extends to multivariate response settings. Extensive experiments on synthetic and real-world datasets demonstrate that OCS-ARC significantly improves selection power over the baseline while maintaining valid FDR control across all examined timesteps. |
| 2025-08-18 | [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](http://arxiv.org/abs/2508.13144v1) | David Heineman, Valentin Hofmann et al. | Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances. |
| 2025-08-18 | [The ALPINE-CRISTAL-JWST survey: spatially resolved star formation relations at $z\sim5$](http://arxiv.org/abs/2508.13136v1) | C. Accard, M. B√©thermin et al. | Star formation governs galaxy evolution, shaping stellar mass assembly and gas consumption across cosmic time. The Kennicutt-Schmidt (KS) relation, linking star formation rate (SFR) and gas surface densities, is fundamental to understand star formation regulation, yet remains poorly constrained at $z > 2$ due to observational limitations and uncertainties in locally calibrated gas tracers. The [CII] $158 {\rm \mu m}$ line has recently emerged as a key probe of the cold ISM and star formation in the early Universe. We investigate whether the resolved [CII]-SFR and KS relations established at low redshift remain valid at $4 < z < 6$ by analysing 13 main-sequence galaxies from the ALPINE and CRISTAL surveys, using multi-wavelength data (HST, JWST, ALMA) at $\sim2$ kpc resolution. We perform pixel-by-pixel spectral energy distribution (SED) modelling with CIGALE on resolution-homogenised images. We develop a statistical framework to fit the [CII]-SFR relation that accounts for pixel covariance and compare our results to classical fitting methods. We test two [CII]-to-gas conversion prescriptions to assess their impact on inferred gas surface densities and depletion times. We find a resolved [CII]-SFR relation with a slope of $0.87 \pm 0.15$ and intrinsic scatter of $0.19 \pm 0.03$ dex, which is shallower and tighter than previous studies at $z\sim5$. The resolved KS relation is highly sensitive to the [CII]-to-gas conversion factor: using a fixed global $\alpha_{\rm [CII]}$ yields depletion times of $0.5$-$1$ Gyr, while a surface brightness-dependent $W_{\rm [CII]}$, places some galaxies with high gas density in the starburst regime ($<0.1$ Gyr). Future inputs from both simulations and observations are required to better understand how the [CII]-to-gas conversion factor depends on local ISM properties. We need to break this fundamental limit to properly study the KS relation at $z\gtrsim4$. |
| 2025-08-18 | [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](http://arxiv.org/abs/2508.13121v1) | Carlos Celemin | This work introduces an automated testing approach that employs agents controlling game characters to detect potential bugs within a game level. Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient search, the method determines the next sampling point by analyzing the data collected so far and calculates the data point that will maximize information acquisition. To support the BO process, we introduce a game testing-specific model built on top of a grid map, that features the smoothness and uncertainty estimation required by BO, however and most importantly, it does not suffer the scalability issues that traditional models carry. The experiments demonstrate that the approach significantly improves map coverage capabilities in both time efficiency and exploration distribution. |
| 2025-08-18 | [Reasonable uncertainty: Confidence intervals in empirical Bayes discrimination detection](http://arxiv.org/abs/2508.13110v1) | Jiaying Gu, Nikolaos Ignatiadis et al. | We revisit empirical Bayes discrimination detection, focusing on uncertainty arising from both partial identification and sampling variability. While prior work has mostly focused on partial identification, we find that some empirical findings are not robust to sampling uncertainty. To better connect statistical evidence to the magnitude of real-world discriminatory behavior, we propose a counterfactual odds-ratio estimand with a attractive properties and interpretation. Our analysis reveals the importance of careful attention to uncertainty quantification and downstream goals in empirical Bayes analyses. |
| 2025-08-18 | [Surrogate-based Bayesian calibration methods for climate models: a comparison of traditional and non-traditional approaches](http://arxiv.org/abs/2508.13071v1) | Maike F. Holthuijzen, Atlanta Chakraborty et al. | Parameter calibration is crucial for reducing uncertainty and improving simulation accuracy in physics-based models, yet computational constraints pose significant challenges. Bayesian calibration methods offer a principled framework for combining prior knowledge with data while rigorously quantifying uncertainty. In this work, we compare four emulator-based Bayesian calibration methods: Calibrate-Emulate-Sample (CES), History Matching (HM), Bayesian Optimal Experimental Design (BOED), and a novel Goal-Oriented BOED (GBOED) approach, using the Lorenz '96 multiscale system as a testbed. Our GBOED formulation explicitly targets calibration-relevant quantities and leverages information-theoretic criteria for data selection. We assess each method in terms of calibration accuracy, uncertainty quantification, computational cost, and convergence behavior. We evaluate each method's performance in balancing computational cost, implementation complexity, and uncertainty quantification (UQ), with additional insights into convergence behavior as model evaluations increase. We find CES offers excellent performance but at high computational expense, while GBOED achieves comparable accuracy using fewer model evaluations. Standard BOED underperforms with respect to calibration accuracy, and HM shows moderate effectiveness but can be useful as a precursor. Our results highlight trade-offs among Bayesian strategies and demonstrate the promise of goal-oriented design in calibration workflows. |
| 2025-08-18 | [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](http://arxiv.org/abs/2508.13057v1) | Adolfo Gonz√°lez, V√≠ctor Parada | Demand forecasting is essential for strategic planning in competitive environments, enabling resource optimization and improved responsiveness to market dynamics. However, multivariate time series modeling faces challenges due to data complexity, uncertainty, and frequent regime shifts. Traditional evaluation metrics can introduce biases and limit generalization. This work compares two custom evaluation functions: FMAE (Focused Mean Absolute Error), focused on minimizing absolute errors, and HEF (Hierarchical Evaluation Function), designed to weight global metrics and penalize large deviations. Experiments were conducted under different data splits (91:9, 80:20, 70:30) using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative accuracy, robustness, and computational efficiency. Results show that HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE), enhancing model robustness and explanatory power. These findings were confirmed via visualizations and statistical tests. Conversely, FMAE offers advantages in local metrics (MAE, MASE) and execution time, making it suitable for short-term scenarios. The study highlights a methodological trade-off: HEF is ideal for strategic planning, while FMAE is better suited for operational efficiency. A replicable framework is proposed for optimizing predictive models in dynamic environments. |
| 2025-08-18 | [A test of Ca ii H & K photometry for isolating massive globular clusters below the metallicity floor](http://arxiv.org/abs/2508.13031v1) | Bas van Heumen, William E. Harris et al. | The serendipitous discovery of the M31 globular cluster (GC) EXT8 has presented a significant challenge to current theories for GC formation. By finding other GCs similar to EXT8, it should become clear if and/or how EXT8 can fit into our current understanding of GC formation. We aim to test the potential of integrated-light narrow-band Ca II H & K photometry as a proxy for the metallicity of GCs to be able to provide effective candidate selection for massive GCs below the GC metallicity floor ([Fe/H] $\leq$ -2.5), such as EXT8. We investigate the behaviour of two colours involving the CaHK filter employed by the Pristine survey, CaHK-u and CaHK-g, as a function of metallicity through CFHT MegaCam imaging of EXT8 and a wide set of M31 GCs covering the metallicity range of -2.9 $\leq$ [Fe/H] $\leq$ +0.4. Additionally, we investigate if the CaHK colours are strongly influenced by horizontal branch morphology through available morphology measurements. In both of the CaHK colours, EXT8 and two other potential GCs below the metallicity floor can be selected from other metal-poor GCs ([Fe/H] $\leq$ -1.5) with CaHK-g showing the larger metallicity sensitivity. The RMS of linear fits to the metal-poor GCs show an uncertainty of 0.3 dex on metallicity estimations for both colours. Comparisons with u-g and g-z/F450W-F850L colours reinforce the notion that CaHK photometry can be used for effective candidate selection as they reduce false positive selection rates by at least a factor of 2. We find no strong influence of the horizontal branch morphology on the CaHK colours that would interfere with candidate selection, although the assessment is limited by quantity and quality of available data. |
| 2025-08-18 | [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](http://arxiv.org/abs/2508.13021v1) | Pengcheng Huang, Shuhao Liu et al. | Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler. |
| 2025-08-18 | [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](http://arxiv.org/abs/2508.12997v1) | Haishun Chen, Cai Xu et al. | Multi-view evidential learning aims to integrate information from multiple views to improve prediction performance and provide trustworthy uncertainty esitimation. Most previous methods assume that view-specific evidence learning is naturally reliable. However, in practice, the evidence learning process tends to be biased. Through empirical analysis on real-world data, we reveal that samples tend to be assigned more evidence to support data-rich classes, thereby leading to unreliable uncertainty estimation in predictions. This motivates us to delve into a new Biased Evidential Multi-view Learning (BEML) problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning (FAML). FAML first introduces an adaptive prior based on training trajectory, which acts as a regularization strategy to flexibly calibrate the biased evidence learning process. Furthermore, we explicitly incorporate a fairness constraint based on class-wise evidence variance to promote balanced evidence allocation. In the multi-view fusion stage, we propose an opinion alignment mechanism to mitigate view-specific bias across views, thereby encouraging the integration of consistent and mutually supportive evidence. Extensive experiments on five real-world multi-view datasets demonstrate that FAML achieves more balanced evidence allocation and improves both prediction performance and the reliability of uncertainty estimation compared to state-of-the-art methods. |
| 2025-08-18 | [Likelihood-Based Heterogeneity Inference Reveals Non-Stationary Effects in Biohybrid Cell-Cargo Transport](http://arxiv.org/abs/2508.12976v1) | Jan Albrecht, Lara S. Dautzenberg et al. | Variability of motility behavior in populations of microbiological agents is an ubiquitous phenomenon even in the case of genetically identical cells. Accordingly, passive objects introduced into such biological systems and driven by them will also exhibit heterogeneous motion patterns. Here, we study a biohybrid system of passive beads driven by active ameboid cells and use a likelihood approach to estimate the heterogeneity of the bead dynamics from their discretely sampled trajectories. We showcase how this approach can deal with information-scarce situations and provides natural uncertainty bounds for heterogeneity estimates. Using these advantages we particularly uncover that the heterogeneity in the system is time-dependent. |
| 2025-08-15 | [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](http://arxiv.org/abs/2508.11573v1) | Mogens Plessen | Automatic Section Control (ASC) is a long-standing trend for spraying in agriculture. It promises to minimise spray overlap areas. The core idea is to (i) switch off spray nozzles on areas that have already been sprayed, and (ii) to dynamically adjust nozzle flow rates along the boom bar that holds the spray nozzles when velocities of boom sections vary during turn maneuvers. ASC is not possible without sensors, in particular for accurate positioning data. Spraying and the movement of modern wide boom bars are highly dynamic processes. In addition, many uncertainty factors have an effect such as cross wind drift, boom height, nozzle clogging in open-field conditions, and so forth. In view of this complexity, the natural question arises if a simpler alternative exist. Therefore, an Automatic Multi-Sections Control method is compared to a proposed simpler one- or two-sections alternative that uses predictive spray switching. The comparison is provided under nominal conditions. Agricultural spraying is intrinsically linked to area coverage path planning and spray switching logic. Combinations of two area coverage path planning and switching logics as well as three sections-setups are compared. The three sections-setups differ by controlling 48 sections, 2 sections or controlling all nozzles uniformly with the same control signal as one single section. Methods are evaluated on 10 diverse real-world field examples, including non-convex field contours, freeform mainfield lanes and multiple obstacle areas. A preferred method is suggested that (i) minimises area coverage pathlength, (ii) offers intermediate overlap, (iii) is suitable for manual driving by following a pre-planned predictive spray switching logic for an area coverage path plan, and (iv) and in contrast to ASC can be implemented sensor-free and therefore at low cost. |
| 2025-08-15 | [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](http://arxiv.org/abs/2508.11547v1) | Martin Jirou≈°ek, Tom√°≈° B√°ƒça et al. | This paper addresses the problem of tracking the position of a cable-suspended payload carried by an unmanned aerial vehicle, with a focus on real-world deployment and minimal hardware requirements. In contrast to many existing approaches that rely on motion-capture systems, additional onboard cameras, or instrumented payloads, we propose a framework that uses only standard onboard sensors--specifically, real-time kinematic global navigation satellite system measurements and data from the onboard inertial measurement unit--to estimate and control the payload's position. The system models the full coupled dynamics of the aerial vehicle and payload, and integrates a linear Kalman filter for state estimation, a model predictive contouring control planner, and an incremental model predictive controller. The control architecture is designed to remain effective despite sensing limitations and estimation uncertainty. Extensive simulations demonstrate that the proposed system achieves performance comparable to control based on ground-truth measurements, with only minor degradation (< 6%). The system also shows strong robustness to variations in payload parameters. Field experiments further validate the framework, confirming its practical applicability and reliable performance in outdoor environments using only off-the-shelf aerial vehicle hardware. |
| 2025-08-15 | [Integrating Uncertainties for Koopman-Based Stabilization](http://arxiv.org/abs/2508.11533v1) | Yicheng Lin, Bingxian Wu et al. | Over the past decades, the Koopman operator has been widely applied in data-driven control, yet its theoretical foundations remain underexplored. This paper establishes a unified framework to address the robust stabilization problem in data-driven control via the Koopman operator, fully accounting for three uncertainties: projection error, estimation error, and process disturbance. It comprehensively investigates both direct and indirect data-driven control approaches, facilitating flexible methodology selection for analysis and control. For the direct approach, considering process disturbances, the lifted-state feedback controller, designed via a linear matrix inequality (LMI), robustly stabilizes all lifted bilinear systems consistent with noisy data. For the indirect approach requiring system identification, the feedback controller, designed using a nonlinear matrix inequality convertible to an LMI, ensures closed-loop stability under worst-case process disturbances. Numerical simulations via cross-validation validate the effectiveness of both approaches, highlighting their theoretical significance and practical utility. |
| 2025-08-15 | [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](http://arxiv.org/abs/2508.11460v1) | Aurora Grefsrud, Nello Blaser et al. | Rigorous statistical methods, including parameter estimation with accompanying uncertainties, underpin the validity of scientific discovery, especially in the natural sciences. With increasingly complex data models such as deep learning techniques, uncertainty quantification has become exceedingly difficult and a plethora of techniques have been proposed. In this case study, we use the unifying framework of approximate Bayesian inference combined with empirical tests on carefully created synthetic classification datasets to investigate qualitative properties of six different probabilistic machine learning algorithms for class probability and uncertainty estimation: (i) a neural network ensemble, (ii) neural network ensemble with conflictual loss, (iii) evidential deep learning, (iv) a single neural network with Monte Carlo Dropout, (v) Gaussian process classification and (vi) a Dirichlet process mixture model. We check if the algorithms produce uncertainty estimates which reflect commonly desired properties, such as being well calibrated and exhibiting an increase in uncertainty for out-of-distribution data points. Our results indicate that all algorithms are well calibrated, but none of the deep learning based algorithms provide uncertainties that consistently reflect lack of experimental evidence for out-of-distribution data points. We hope our study may serve as a clarifying example for researchers developing new methods of uncertainty estimation for scientific data-driven modeling. |
| 2025-08-15 | [Improving Air Shower Simulations by Tuning Pythia 8/Angantyr with Accelerator Data](http://arxiv.org/abs/2508.11458v1) | Michael Windau, Chlo√© Gaudu et al. | We present a combined analysis of the Pythia 8 event generator using accelerator data and evaluate its impact on air shower observables. Reliable simulations with event generators are essential for particle physics analyses, achievable through advanced tuning to experimental data. Pythia 8 has emerged as a promising high-energy interaction model for cosmic ray air shower simulations, offering well-documented parameter settings and a user-friendly interface to enable automatic tuning efforts. Using data from collider and fixed-target experiments, we first derive tunes for each domain separately, before tuning both domains simultaneously. To achieve this, we define a core set of observables and quantify their dependence on selected parameters. The tuning efforts are based on gradient descent and Bayesian methods, the latter providing a full uncertainty propagation of the parameters to the observables. Results for the impact of a combined analysis for the Pythia 8/Angantyr event generator on air shower observables, such as particle densities at ground level and energy deposit profiles, are presented. |
| 2025-08-15 | [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](http://arxiv.org/abs/2508.11453v1) | Jiayue Jin, Lang Qian et al. | Recent years have witnessed remarkable progress in autonomous driving, with systems evolving from modular pipelines to end-to-end architectures. However, most existing methods are trained offline and lack mechanisms to adapt to new environments during deployment. As a result, their generalization ability diminishes when faced with unseen variations in real-world driving scenarios. In this paper, we break away from the conventional "train once, deploy forever" paradigm and propose EvoPSF, a novel online Evolution framework for autonomous driving based on Planning-State Feedback. We argue that planning failures are primarily caused by inaccurate object-level motion predictions, and such failures are often reflected in the form of increased planner uncertainty. To address this, we treat planner uncertainty as a trigger for online evolution, using it as a diagnostic signal to initiate targeted model updates. Rather than performing blind updates, we leverage the planner's agent-agent attention to identify the specific objects that the ego vehicle attends to most, which are primarily responsible for the planning failures. For these critical objects, we compute a targeted self-supervised loss by comparing their predicted waypoints from the prediction module with their actual future positions, selected from the perception module's outputs with high confidence scores. This loss is then backpropagated to adapt the model online. As a result, our method improves the model's robustness to environmental changes, leads to more precise motion predictions, and therefore enables more accurate and stable planning behaviors. Experiments on both cross-region and corrupted variants of the nuScenes dataset demonstrate that EvoPSF consistently improves planning performance under challenging conditions. |
| 2025-08-15 | [Conformal Prediction Meets Long-tail Classification](http://arxiv.org/abs/2508.11345v1) | Shuqi Liu, Jianguo Huang et al. | Conformal Prediction (CP) is a popular method for uncertainty quantification that converts a pretrained model's point prediction into a prediction set, with the set size reflecting the model's confidence. Although existing CP methods are guaranteed to achieve marginal coverage, they often exhibit imbalanced coverage across classes under long-tail label distributions, tending to over cover the head classes at the expense of under covering the remaining tail classes. This under coverage is particularly concerning, as it undermines the reliability of the prediction sets for minority classes, even with coverage ensured on average. In this paper, we propose the Tail-Aware Conformal Prediction (TACP) method to mitigate the under coverage of the tail classes by utilizing the long-tail structure and narrowing the head-tail coverage gap. Theoretical analysis shows that it consistently achieves a smaller head-tail coverage gap than standard methods. To further improve coverage balance across all classes, we introduce an extension of TACP: soft TACP (sTACP) via a reweighting mechanism. The proposed framework can be combined with various non-conformity scores, and experiments on multiple long-tail benchmark datasets demonstrate the effectiveness of our methods. |
| 2025-08-15 | [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](http://arxiv.org/abs/2508.11340v1) | Yuanlin Liu, Zhihan Zhou et al. | Information on the number and category of cervical cells is crucial for the diagnosis of cervical cancer. However, existing classification methods capable of automatically measuring this information require the training dataset to be representative, which consumes an expensive or even unaffordable human cost. We herein propose active labeling that enables us to construct a representative training dataset using a much smaller human cost for data-efficient cervical cell classification. This cost-effective method efficiently leverages the classifier's uncertainty on the unlabeled cervical cell images to accurately select images that are most beneficial to label. With a fast estimation of the uncertainty, this new algorithm exhibits its validity and effectiveness in enhancing the representative ability of the constructed training dataset. The extensive empirical results confirm its efficacy again in navigating the usage of human cost, opening the avenue for data-efficient cervical cell classification. |
| 2025-08-15 | [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](http://arxiv.org/abs/2508.11338v1) | Prathamesh Devadiga, Yashmitha Shailesh | We introduce RegimeNAS, a novel differentiable architecture search framework specifically designed to enhance cryptocurrency trading performance by explicitly integrating market regime awareness. Addressing the limitations of static deep learning models in highly dynamic financial environments, RegimeNAS features three core innovations: (1) a theoretically grounded Bayesian search space optimizing architectures with provable convergence properties; (2) specialized, dynamically activated neural modules (Volatility, Trend, and Range blocks) tailored for distinct market conditions; and (3) a multi-objective loss function incorporating market-specific penalties (e.g., volatility matching, transition smoothness) alongside mathematically enforced Lipschitz stability constraints. Regime identification leverages multi-head attention across multiple timeframes for improved accuracy and uncertainty estimation. Rigorous empirical evaluation on extensive real-world cryptocurrency data demonstrates that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving an 80.3% Mean Absolute Error reduction compared to the best traditional recurrent baseline and converging substantially faster (9 vs. 50+ epochs). Ablation studies and regime-specific analysis confirm the critical contribution of each component, particularly the regime-aware adaptation mechanism. This work underscores the imperative of embedding domain-specific knowledge, such as market regimes, directly within the NAS process to develop robust and adaptive models for challenging financial applications. |
| 2025-08-15 | [Demonstrating the velocity response of a table-top EPR Speedmeter](http://arxiv.org/abs/2508.11316v1) | S. L. Kranzhoff, S. L. Danilishin et al. | The sensitivity of gravitational-wave interferometers is fundamentally limited by quantum noise, as dictated by the Heisenberg uncertainty principle, due to their continuous position measurement of the end mirrors. Speedmeter configurations, which measure mirror velocity rather than position, have been proposed as a means to suppress quantum back-action noise, but practical implementations remain at an early stage. In this work, we present a table-top realisation of the Einstein- Podolsky-Rosen (EPR) Speedmeter concept, employing an optical readout scheme based on two orthogonal polarisation modes that probe the interferometer with different effective bandwidths. Using a triangular cavity, we demonstrate that the differential optical response between the linear p- and s-polarised modes exhibits a speed-like frequency dependence: vanishing at DC and increasing linearly with signal frequency, up to the bandwidth of the slower mode. With this we show that an optical system equivalent to the EPR Speedmeter indeed performs a velocity readout of the end mirror. |
| 2025-08-14 | [The Dark Energy Bedrock All-Sky Supernova Program: Cross Calibration, Simulations, and Cosmology Forecasts](http://arxiv.org/abs/2508.10877v1) | Maria Acevedo, Nora R. Sherman et al. | Type Ia supernovae (SNe Ia) have been essential for probing the nature of dark energy; however, most SN analyses rely on the same low-redshift sample, which may lead to shared systematics. In a companion paper (Sherman et al., submitted), we introduce the Dark Energy Bedrock All-Sky Supernova (DEBASS) program, which has already collected more than 500 low-redshift SNe Ia on the Dark Energy Camera (DECam), and present an initial release of 77 SNe Ia within the Dark Energy Survey (DES) footprint observed between 2021 and 2024. Here, we examine the systematics, including photometric calibration and selection effects. We find agreement at the 10 millimagnitude level among the tertiary standard stars of DEBASS, DES, and Pan-STARRS1. Our simulations reproduce the observed distributions of DEBASS SN light-curve properties, and we measure a bias-corrected Hubble residual scatter of $0.08$ mag, which, while small, is found in 10% of our simulations. We compare the DEBASS SN distances to the Foundation sample and find consistency with a median residual offset of $0.016 \pm 0.019$ mag. Selection effects have negligible impacts on distances, but a different photometric calibration solution shifts the median residual $-0.015 \pm 0.019$ mag, highlighting calibration sensitivity. Using conservative simulations, we forecast that replacing historical low-redshift samples with the full DEBASS sample (>400 SNe Ia) will improve the statistical uncertainties on dark energy parameters $w_0$ and $w_a$ by 30% and 24% respectively, enhance the dark energy Figure of Merit by up to 60%, and enable a measurement of $f\sigma_8$ at the 25% level. |
| 2025-08-14 | [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](http://arxiv.org/abs/2508.10867v1) | Yizhi Zhou, Ziwei Kang et al. | Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial odometry (VIO) systems. Consistency is crucial for ensuring the estimation accuracy of a UWBaided VIO system. An inconsistent estimator can degrade localization performance, where the inconsistency primarily arises from two main factors: (1) the estimator fails to preserve the correct system observability, and (2) UWB anchor positions are assumed to be known, leading to improper neglect of calibration uncertainty. In this paper, we propose a consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system based on the Lie group. Our method incorporates the UWB anchor state into the system state, explicitly accounting for UWB calibration uncertainty and enabling the joint and consistent estimation of both robot and anchor states. Furthermore, observability consistency is ensured by leveraging the invariant error properties of the Lie group. We analytically prove that the CVIRO algorithm naturally maintains the system's correct unobservable subspace, thereby preserving estimation consistency. Extensive simulations and experiments demonstrate that CVIRO achieves superior localization accuracy and consistency compared to existing methods. |
| 2025-08-14 | [Comparison of Data Reduction Criteria for Online Gaussian Processes](http://arxiv.org/abs/2508.10815v1) | Thore Wietzke, Knut Graichen | Gaussian Processes (GPs) are widely used for regression and system identification due to their flexibility and ability to quantify uncertainty. However, their computational complexity limits their applicability to small datasets. Moreover in a streaming scenario, more and more datapoints accumulate which is intractable even for Sparse GPs. Online GPs aim to alleviate this problem by e.g. defining a maximum budget of datapoints and removing redundant datapoints. This work provides a unified comparison of several reduction criteria, analyzing both their computational complexity and reduction behavior. The criteria are evaluated on benchmark functions and real-world datasets, including dynamic system identification tasks. Additionally, acceptance criteria are proposed to further filter out redundant datapoints. This work yields practical guidelines for choosing a suitable criterion for an online GP algorithm. |
| 2025-08-14 | [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](http://arxiv.org/abs/2508.10798v1) | Troi Williams | Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks. |
| 2025-08-14 | [When Experts Disagree: Characterizing Annotator Variability for Vessel Segmentation in DSA Images](http://arxiv.org/abs/2508.10797v1) | M. Geshvadi, G. So et al. | We analyze the variability among segmentations of cranial blood vessels in 2D DSA performed by multiple annotators in order to characterize and quantify segmentation uncertainty. We use this analysis to quantify segmentation uncertainty and discuss ways it can be used to guide additional annotations and to develop uncertainty-aware automatic segmentation methods. |
| 2025-08-14 | [Accelerating Stochastic Energy System Optimization Models: Temporally Split Benders Decomposition](http://arxiv.org/abs/2508.10789v1) | Shima Sasanpour, Manuel Wetzel et al. | Stochastic programming can be applied to consider uncertainties in energy system optimization models for capacity expansion planning. However, these models become increasingly large and time-consuming to solve, even without considering uncertainties. For two-stage stochastic capacity expansion planning problems, Benders decomposition is often applied to ensure that the problem remains solvable. Since stochastic scenarios can be optimized independently within subproblems, their optimization can be parallelized. However, hourly-resolved capacity expansion planning problems typically have a larger temporal than scenario cardinality. Therefore, we present a temporally split Benders decomposition that further exploits the parallelization potential of stochastic expansion planning problems. A compact reformulation of the storage level constraint into linking variables ensures that long-term storage operation can still be optimized despite the temporal decomposition. We demonstrate this novel approach with model instances of the German power system with up to 87 million rows and columns. Our results show a reduction in computing times of up to 60% and reduced memory requirements. Additional enhancement strategies and the use of distributed memory on high-performance computers further improve the computing time by over 80%. |
| 2025-08-14 | [Estimating Covariance for Global Minimum Variance Portfolio: A Decision-Focused Learning Approach](http://arxiv.org/abs/2508.10776v1) | Juchan Kim, Inwoo Tae et al. | Portfolio optimization constitutes a cornerstone of risk management by quantifying the risk-return trade-off. Since it inherently depends on accurate parameter estimation under conditions of future uncertainty, the selection of appropriate input parameters is critical for effective portfolio construction. However, most conventional statistical estimators and machine learning algorithms determine these parameters by minimizing mean-squared error (MSE), a criterion that can yield suboptimal investment decisions. In this paper, we adopt decision-focused learning (DFL) - an approach that directly optimizes decision quality rather than prediction error such as MSE - to derive the global minimum-variance portfolio (GMVP). Specifically, we theoretically derive the gradient of decision loss using the analytic solution of GMVP and its properties regarding the principal components of itself. Through extensive empirical evaluation, we show that prediction-focused estimation methods may fail to produce optimal allocations in practice, whereas DFL-based methods consistently deliver superior decision performance. Furthermore, we provide a comprehensive analysis of DFL's mechanism in GMVP construction, focusing on its volatility reduction capability, decision-driving features, and estimation characteristics. |
| 2025-08-14 | [On data-driven robust distortion risk measures for non-negative risks with partial information](http://arxiv.org/abs/2508.10682v1) | Xiangyu Han, Yijun Hu et al. | In this paper, by proposing two new kinds of distributional uncertainty sets, we explore robustness of distortion risk measures against distributional uncertainty. To be precise, we first consider a distributional uncertainty set which is characterized solely by a ball determined by general Wasserstein distance centered at certain empirical distribution function, and then further consider additional constraints of known first moment and any other higher moment of the underlying loss distribution function. Under the assumption that the distortion function is strictly concave and twice differentiable, and that the underlying loss random variable is non-negative and bounded, we derive closed-form expressions for the distribution functions which maximize a given distortion risk measure over the distributional uncertainty sets respectively. Moreover, we continue to study the general case of a concave distortion function and unbounded loss random variables. Comparisons with existing studies are also made. Finally, we provide a numerical study to illustrate the proposed models and results. Our work provides a novel generalization of several known achievements in the literature. |
| 2025-08-14 | [A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners](http://arxiv.org/abs/2508.10679v1) | Jinhua He, Tingzhe Pan et al. | With the continuous increase in the penetration of renewable energy in the emerging power systems, the pressure on system peak regulation has been significantly intensified. Against this backdrop, demand side resources particularly air conditioning loads have garnered considerable attention for their substantial regulation potential and fast response capabilities, making them promising candidates for providing auxiliary peak shaving services. This study focuses on fixed frequency air conditioners (FFACs) and proposes an optimization model and solution method for their participation in demand response (DR) programs. First, a probabilistic response model for FFACs is developed based on the Markov assumption. Second, by sampling this probabilistic model, the aggregate power consumption of an FFAC cluster under decentralized control is obtained. Subsequently, a robust optimization model is formulated to maximize the profit of an aggregator managing the FFAC cluster during DR events, taking into account the aggregated response power. The model explicitly considers temperature uncertainty to ensure user comfort in a robust sense. Finally, leveraging the structure of the proposed model, it is reformulated as a mixed-integer linear programming (MILP) problem and solved using a commercial optimization solver. Simulation results validate the effectiveness of the proposed model and solution approach. |
| 2025-08-14 | [On the Derivation of Equations of Motion from Symmetries in Quantum-Mechanical Systems via Heisenberg's Uncertainty](http://arxiv.org/abs/2508.10661v1) | Enrique Casanova, Jos√© Rojas et al. | We propose the construction of equations of motion based on symmetries in quantum-mechanical systems, using Heisenberg's uncertainty principle as a minimal foundation. From canonical operators, two spaces of conjugate operators are constructed, along with a third space derived from the former, which includes the ``Symmetry-Dilation'' operator. When this operator commutes with the main equation of motion, it defines the set of observables compatible with a complete basis of operators (symmetry generators), organized into a Lie algebra dependent on Heisenberg's uncertainty principle within Minkowski spacetime. Furthermore, by requiring the dilation operator to commute with the central operator, the wavefunction is constrained, thereby constructing known structures. Specific cases are derived -- relativistic, non-relativistic, and a lesser-studied case: ``ultra-relativistic (Carroll-Schr\"odinger)''. Our work may open new avenues for understanding and classifying symmetries in quantum mechanics, as well as offer an alternative method for deriving equations of motion and applying them to complex scenarios involving exotic particles. |
| 2025-08-13 | [Data-driven analyses and model-independent fits for present $b\to s \ell \ell$ results](http://arxiv.org/abs/2508.09986v1) | T. Hurth, F. Mahmoudi et al. | We present a critical assessment of the present $B$ anomalies in the exclusive $b \to s \ell\ell$ mode based on the QCD factorisation approach (QCDf). In particular, we analyse the impact of different local form factor calculations and of the largest bin in the low-$q^2$ region.   We also present a model-independent analysis of the new results of the CMS experiment on the $B \to K^* \mu^+\mu^-$ angular observables and compare them with the corresponding LHCb data. In addition, we update the global fit by including all $b \to s$ observables incorporating the new data from CMS. In these analyses, we use 10% or higher guesstimates of the non-factorisable power corrections as additional uncertainties, serving as a placeholder for robust estimates of these contributions.   Updating earlier results, we also analyse the combined LHCb and CMS data on the $B \to K^* \mu^+\mu^-$ angular observables using data-driven approaches to find indications whether these tensions between the QCDf predictions and the present data are due to underestimated subleading hadronic contributions or due to new physics effects. |
| 2025-08-13 | [Laboratory Measurements of Ca XIX Dielectronic Recombination Satellites](http://arxiv.org/abs/2508.09975v1) | Filipe Grilo, Marc Botz et al. | We report measurements of the K$\alpha$ emission from the astrophysically very abundant Ca XIX (He-like ion) and its satellite lines resonantly excited by dielectronic recombination (DR). We achieve an electron-energy resolution of 8 eV in a cryogenic electron beam ion trap, and determine the energies of the exciting electrons and the emitted photons up to the KLn ($n\le 8$) manifold with $0.05\%$ and $0.1\%$ respective uncertainties. For the KLL satellites, energies agree very well with our predictions using the Flexible Atomic Code (FAC) and previous state-of-the-art calculations. Our calculations also agree with our experimental direct excitation cross-sections for K$\alpha$ within their $10\%$ uncertainty. We extract DR coefficient rates and find good agreement with values tabulated in the OPEN-ADAS database. As an application, we experimentally benchmark Ca XIX atomic data used to model high-temperature astrophysical plasmas by comparing FAC synthetic spectra with recent XRISM observations revealing the contributions of DR satellites to the Ca XIX lines. |
| 2025-08-13 | [Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems](http://arxiv.org/abs/2508.09908v1) | Haoshu Cheng, Martin Guay et al. | In this paper, we investigate the problem of tracking formations driven by bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty in the presence of multiple moving leaders. To estimate the leaders' velocities and accelerations, we first design a distributed observer for the leader system, utilizing a bearing-based localization condition in place of the conventional connectivity assumption. This observer, coupled with an adaptive mechanism, enables the synthesis of a novel distributed control law that guides the formation towards the target formation, without requiring prior knowledge of the system parameters. Furthermore, we establish a sufficient condition, dependent on the initial formation configuration, that ensures collision avoidance throughout the formation evolution. The effectiveness of the proposed approach is demonstrated through a numerical example. |
| 2025-08-13 | [Multi-head committees enable direct uncertainty prediction for atomistic foundation models](http://arxiv.org/abs/2508.09907v1) | Hubert Beck, Pavol Simko et al. | Machine learning potentials have become a standard tool for atomistic materials modelling. While models continue to become more generalisable, an open challenge relates to efficient uncertainty predictions for active learning and robust error analysis. In this work, we utilise MACE and its multi-head mechanism to implement a committee neural network potential for message-passing architectures, where the committee comprises multiple output modules attached to the same atomic environment descriptors. As with traditional committees of independent networks, the standard deviation of the predictions functions as an estimate of the model's uncertainty. We show for a range of datasets in custom-build models that the uncertainty of the force predictions correlates well with the true errors. We subsequently apply this concept to foundation models, specifically MACE-MP-0, where we train only the newly attached output heads while keeping the remaining part of the model fixed. We use this approach in an active learning workflow to condense the training set of the foundation model to just 5\% of its original size. The foundation model multi-head committee trained on the condensed training set enables reliable uncertainty estimation without any substantial decrease in prediction accuracy. |
| 2025-08-13 | [$gg \to ZH$ : updated predictions at NLO QCD](http://arxiv.org/abs/2508.09905v1) | Benjamin Campillo Aveleira, Long Chen et al. | We present state-of-the-art predictions for the inclusive cross section of gluon-initiated $ZH$ production, following the recommendations of the LHC Higgs Working Group. In particular, we include NLO QCD corrections, where the virtual corrections are obtained from the combination of a forward expansion and a high-energy expansion, and the real corrections are exact. The expanded results for the virtual corrections are compared in detail to full numerical results. The updated predictions show a reduction of the scale uncertainties to the level of 15%, and they include an estimate of the top-mass-scheme uncertainty. |
| 2025-08-13 | [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](http://arxiv.org/abs/2508.09732v1) | Romeo Valentin, Sydney M. Katz et al. | Recent advances in data-driven computer vision have enabled robust autonomous navigation capabilities for civil aviation, including automated landing and runway detection. However, ensuring that these systems meet the robustness and safety requirements for aviation applications remains a major challenge. In this work, we present a practical vision-based pipeline for aircraft pose estimation from runway images that represents a step toward the ability to certify these systems for use in safety-critical aviation applications. Our approach features three key innovations: (i) an efficient, flexible neural architecture based on a spatial Soft Argmax operator for probabilistic keypoint regression, supporting diverse vision backbones with real-time inference; (ii) a principled loss function producing calibrated predictive uncertainties, which are evaluated via sharpness and calibration metrics; and (iii) an adaptation of Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling runtime detection and rejection of faulty model outputs. We implement and evaluate our pose estimation pipeline on a dataset of runway images. We show that our model outperforms baseline architectures in terms of accuracy while also producing well-calibrated uncertainty estimates with sub-pixel precision that can be used downstream for fault detection. |
| 2025-08-13 | [CKFNet: Neural Network Aided Cubature Kalman filtering](http://arxiv.org/abs/2508.09727v1) | Jinhui Hu, Haiquan Zhao et al. | The cubature Kalman filter (CKF), while theoretically rigorous for nonlinear estimation, often suffers performance degradation due to model-environment mismatches in practice. To address this limitation, we propose CKFNet-a hybrid architecture that synergistically integrates recurrent neural networks (RNN) with the CKF framework while preserving its cubature principles. Unlike conventional model-driven approaches, CKFNet embeds RNN modules in the prediction phase to dynamically adapt to unmodeled uncertainties, effectively reducing cumulative error propagation through temporal noise correlation learning. Crucially, the architecture maintains CKF's analytical interpretability via constrained optimization of cubature point distributions. Numerical simulation experiments have confirmed that our proposed CKFNet exhibits superior accuracy and robustness compared to conventional model-based methods and existing KalmanNet algorithms. |
| 2025-08-13 | [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](http://arxiv.org/abs/2508.09639v1) | Akshat Dubey, Aleksandar An≈æel et al. | Explainable Artificial Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP), have become essential tools for interpreting complex ensemble tree-based models, especially in high-stakes domains such as healthcare analytics. However, SHAP values are usually treated as point estimates, which disregards the inherent and ubiquitous uncertainty in predictive models and data. This uncertainty has two primary sources: aleatoric and epistemic. The aleatoric uncertainty, which reflects the irreducible noise in the data. The epistemic uncertainty, which arises from a lack of data. In this work, we propose an approach for decomposing uncertainty in SHAP values into aleatoric, epistemic, and entanglement components. This approach integrates Dempster-Shafer evidence theory and hypothesis sampling via Dirichlet processes over tree ensembles. We validate the method across three real-world use cases with descriptive statistical analyses that provide insight into the nature of epistemic uncertainty embedded in SHAP explanations. The experimentations enable to provide more comprehensive understanding of the reliability and interpretability of SHAP-based attributions. This understanding can guide the development of robust decision-making processes and the refinement of models in high-stakes applications. Through our experiments with multiple datasets, we concluded that features with the highest SHAP values are not necessarily the most stable. This epistemic uncertainty can be reduced through better, more representative data and following appropriate or case-desired model development techniques. Tree-based models, especially bagging, facilitate the effective quantification of epistemic uncertainty. |
| 2025-08-13 | [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](http://arxiv.org/abs/2508.09623v1) | Akshay Thakur, Sawan Kumar et al. | Solving partial differential equations (PDEs) within the framework of probabilistic numerics offers a principled approach to quantifying epistemic uncertainty arising from discretization. By leveraging Gaussian process regression and imposing the governing PDE as a constraint at a finite set of collocation points, probabilistic numerics delivers mesh-free solutions at arbitrary locations. However, the high computational cost, which scales cubically with the number of collocation points, remains a critical bottleneck, particularly for large-scale or high-dimensional problems. We propose a scalable enhancement to this paradigm through two key innovations. First, we develop a stochastic dual descent algorithm that reduces the per-iteration complexity from cubic to linear in the number of collocation points, enabling tractable inference. Second, we exploit a clustering-based active learning strategy that adaptively selects collocation points to maximize information gain while minimizing computational expense. Together, these contributions result in an $h$-adaptive probabilistic solver that can scale to a large number of collocation points. We demonstrate the efficacy of the proposed solver on benchmark PDEs, including two- and three-dimensional steady-state elliptic problems, as well as a time-dependent parabolic PDE formulated in a space-time setting. |
| 2025-08-13 | [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](http://arxiv.org/abs/2508.09561v1) | Changyuan Zhao, Guangyuan Liu et al. | Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems. |
| 2025-08-12 | [Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs](http://arxiv.org/abs/2508.09135v1) | Wenxin Zhang, Mark van der Laan | Adaptive experimental designs have gained popularity in clinical trials and online experiments. Unlike traditional, fixed experimental designs, adaptive designs can dynamically adjust treatment randomization probabilities and other design features in response to data accumulated sequentially during the experiment. These adaptations are useful to achieve diverse objectives, including reducing uncertainty in the estimation of causal estimands or increasing participants' chances of receiving better treatments during the experiment. At the end of the experiment, it is often desirable to answer causal questions from the observed data. However, the adaptive nature of such experiments and the resulting dependence among observations pose significant challenges to providing valid statistical inference and efficient estimation of causal estimands. Building upon the Targeted Maximum Likelihood Estimator (TMLE) framework tailored for adaptive designs (van der Laan, 2008), we introduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a variety of causal estimands from adaptive experiment data. We establish asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed positivity and design stabilization assumptions for adaptive experiments. Motivated by efficiency results, we further propose a novel adaptive design aimed at minimizing the variance of estimators based on data generated under that design. Using the average treatment effect as a representative example, simulation studies show that ADL-TMLE demonstrates superior variance-reduction performance across different types of adaptive experiments, and that the proposed adaptive design attains lower variance than the standard efficiency-oriented adaptive design. Finally, we generalize this estimation and design framework to broader settings with longitudinal structures. |
| 2025-08-12 | [A New Method of Deriving Doppler Velocities for Solar Orbiter SPICE](http://arxiv.org/abs/2508.09121v1) | J. E. Plowman, D. M. Hassler et al. | This paper presents a follow-up to previous work on correcting PSF-induced Doppler artifacts in observations by the SPICE spectrograph on Solar Orbiter. In a previous paper, we demonstrated correction of these artifacts in the $y-\lambda$ plane with PSF Regularization, treating the forward problem with a method based on large sparse matrix inversion. It has since been found that similar apparent artifacts are also present in the $x-\lambda$ direction, i.e., across adjacent slit positions. This is difficult (although not impossible) to correct with the previous matrix inversion method due to the time variation between slit positions. We have therefore devised a new method which addresses both $x-\lambda$ and $y-\lambda$ artifacts simultaneously by applying wavelength dependent shifts at each $x-y$ plane of the spectral cube. This paper demonstrates the SPICE data issue, describes the new method, and shows a comparison with the previous one. We explore the time variation of the correction parameters for the SPICE data and show a clear orbit dependence. The results of the method are significantly higher quality derived Doppler signals, which we estimate at less than $\sim$ 5 km/s uncertainty for brighter lines in the absence of other systematics. Furthermore, we show the new SPICE polar observation results as a demonstration. The correction codes are written in Python, publicly available on GitHub, and can be directly applied to SPICE level 2 datasets. |
| 2025-08-12 | [Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring](http://arxiv.org/abs/2508.09085v1) | Zihan Fang, Zheng Lin et al. | Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness. |
| 2025-08-12 | [CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks](http://arxiv.org/abs/2508.09054v1) | Debdeep Mukherjee, Eduardo Di Santi et al. | Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability. |
| 2025-08-12 | [Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2508.09014v1) | Kaiwen Huang, Tao Zhou et al. | Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at https://github.com/taozh2017/UCSeg. |
| 2025-08-12 | [Current-Enhanced Excited States in Lattice QCD Three-Point Functions](http://arxiv.org/abs/2508.09006v1) | Lorenzo Barca | Excited-state contamination remains one of the leading sources of systematic uncertainty in the precise determination of hadron structure observables from lattice QCD. In this letter, we present a general argument, inspired by current-meson dominance and implemented through the variational method, to identify which excited states are enhanced by the choice of the inserted current and kinematics. The argument is supported by numerical evidence across multiple hadronic channels and provides both a conceptual understanding and practical guidance to account for excited-state effects in hadron three-point function analyses. |
| 2025-08-12 | [Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty](http://arxiv.org/abs/2508.08992v1) | Rui Wang, Qihan Lin et al. | Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT. |
| 2025-08-12 | [Mutually equi-biased bases](http://arxiv.org/abs/2508.08969v1) | Seyed Javad Akhtarshenas, Saman Karimi et al. | In the framework of mutually unbiased bases (MUBs), a measurement in one basis gives \emph{no information} about the outcomes of measurements in another basis. Here, we relax the no-information condition by allowing the $d$ outcomes to be predicted according to a predefined probability distribution $q=(q_0,\cdots,q_{d-1})$. The notion of mutual unbiasedness, however, is preserved by requiring that the extracted information is the same for any preparation and any measurement; regardless of which state from which basis is chosen to prepare the system, the outcomes of measuring the system with respect to the other basis generate the same probability distribution. In the light of this, we define the notion of \emph{mutually equi-biased bases} (MEBs) such that within each basis the states are equi-biased with respect to the states of the other basis and that the bases are mutually equi-biased with respect to each other. For $d=2,3$, we derive a complete set of $d+1$ MEBs. The mutual equi-biasedness imposes nontrivial constraints on the distribution $q$, leading for $d=3$ to the restriction $1/3\le\mu \le 1/2$ where $\mu=\sum_{k=0}^{2}q_k^2$. To capture the incompatibility of the measurements in MEBs, we derive an inequality for the probabilities of projective measurements in a qudit system, which yields an associated entropic uncertainty inequality. Finally, we construct a class of positive maps and their associated entanglement witnesses based on MEBs. While an entanglement witness constructed from MUBs is generally finer than one based on MEBs when both use the same number of bases, for certain values of the index $\mu$, employing a larger set of MEBs can yield a finer witness. We illustrate this behavior using isotropic states of a $3\times 3$ system. |
| 2025-08-12 | [A comprehensive analysis of Drell-Yan production uncertainties and mass effects at moderate and low dilepton masses](http://arxiv.org/abs/2508.08956v1) | Ekta Chaubey, Claude Duhr et al. | We present a thorough investigation of the sources of uncertainties to the Drell-Yan production using state-of-the-art predictions for both neutral and charged current channels, focusing on the low invariant mass region. Differential predictions for the invariant mass spectrum are provided at N$^3$LO supplemented with exact charm and bottom quark mass effects calculated at $\mathcal{O}(\alpha_s^2)$. The impact of PDF choices (including approximate N$^3$LO), scale variations, the variation of the strong coupling constant, and impact heavy quark mass effects on the distributions is studied in detail. We also comment on the correlation of high-energy astrophysical processes with the low-mass DY region. |
| 2025-08-12 | [Hi-fi functional priors by learning activations](http://arxiv.org/abs/2508.08880v1) | Marcin Sendera, Amin Sorkhei et al. | Function-space priors in Bayesian Neural Networks (BNNs) provide a more intuitive approach to embedding beliefs directly into the model's output, thereby enhancing regularization, uncertainty quantification, and risk-aware decision-making. However, imposing function-space priors on BNNs is challenging. We address this task through optimization techniques that explore how trainable activations can accommodate higher-complexity priors and match intricate target function distributions. We investigate flexible activation models, including Pade functions and piecewise linear functions, and discuss the learning challenges related to identifiability, loss construction, and symmetries. Our empirical findings indicate that even BNNs with a single wide hidden layer when equipped with flexible trainable activation, can effectively achieve desired function-space priors. |
| 2025-08-11 | [Autonomous Air-Ground Vehicle Operations Optimization in Hazardous Environments: A Multi-Armed Bandit Approach](http://arxiv.org/abs/2508.08217v1) | Jimin Choi, Max Z. Li | Hazardous environments such as chemical spills, radiological zones, and bio-contaminated sites pose significant threats to human safety and public infrastructure. Rapid and reliable hazard mitigation in these settings often unsafe for humans, calling for autonomous systems that can adaptively sense and respond to evolving risks. This paper presents a decision-making framework for autonomous vehicle dispatch in hazardous environments with uncertain and evolving risk levels. The system integrates a Bayesian Upper Confidence Bound (BUCB) sensing strategy with task-specific vehicle routing problems with profits (VRPP), enabling adaptive coordination of unmanned aerial vehicles (UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning. Using VRPP allows selective site visits under resource constraints by assigning each site a visit value that reflects sensing or cleaning priorities. Site-level hazard beliefs are maintained through a time-weighted Bayesian update. BUCB scores guide UAV routing to balance exploration and exploitation under uncertainty, while UGV routes are optimized to maximize expected hazard reduction under resource constraints. Simulation results demonstrate that our framework reduces the number of dispatch cycles to resolve hazards by around 30% on average compared to baseline dispatch strategies, underscoring the value of uncertainty-aware vehicle dispatch for reliable hazard mitigation. |
| 2025-08-11 | [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](http://arxiv.org/abs/2508.08206v1) | Amirhossein Taherpour, Abbas Taherpour et al. | We propose a joint learning framework for Byzantine-resilient spectrum sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic access under channel state information (CSI) uncertainty. The sensing stage performs logit-domain Bayesian updates with trimmed aggregation and attention-weighted consensus, and the base station (BS) fuses network beliefs with a conservative minimum rule, preserving detection accuracy under a bounded number of Byzantine users. Conditioned on the sensing outcome, we pose downlink design as sum mean-squared error (MSE) minimization under transmit-power and signal-leakage constraints and jointly optimize the BS precoder, IRS phase shifts, and user equalizers. With partial (or known) CSI, we develop an augmented-Lagrangian alternating algorithm with projected updates and provide provable sublinear convergence, with accelerated rates under mild local curvature. With unknown CSI, we perform constrained Bayesian optimization (BO) in a geometry-aware low-dimensional latent space using Gaussian process (GP) surrogates; we prove regret bounds for a constrained upper confidence bound (UCB) variant of the BO module, and demonstrate strong empirical performance of the implemented procedure. Simulations across diverse network conditions show higher detection probability at fixed false-alarm rate under adversarial attacks, large reductions in sum MSE for honest users, strong suppression of eavesdropper signal power, and fast convergence. The framework offers a practical path to secure opportunistic communication that adapts to CSI availability while coherently coordinating sensing and transmission through joint learning. |
| 2025-08-11 | [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](http://arxiv.org/abs/2508.08204v1) | Kyle Moore, Jesse Roberts et al. | There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis. |
| 2025-08-11 | [Atomic thermometry in optical lattice clocks](http://arxiv.org/abs/2508.08164v1) | Irene Goti, Tommaso Petrucciani et al. | Accurate measurement of atomic temperature is essential for a wide range of applications, from quantum sensing to precision metrology. In optical lattice clocks, precise characterization of atomic temperature is required to minimize systematic uncertainties at the $10^{-18}$ level. In this work, we investigate atomic temperature measurements in the ytterbium optical lattice clock developed at INRIM, IT-Yb1, employing sideband and Doppler spectroscopy across a wide range of trapping conditions. By implementing clock-line-mediated Sisyphus cooling, we reduce the atomic temperature and enable operation at shallower lattice depths down to $D = 50E_{R}$. We compare temperature estimates obtained from the harmonic oscillator model with those derived using a Born-Oppenheimer-based approach, which is expected to provide a more accurate description of atomic motion in both longitudinal and radial directions, especially for hotter atoms whose motion deviates from the harmonic regime. Discrepancies up to a factor of two in extracted temperatures are observed depending on the chosen model. We assess the impact of these modeling differences on the evaluation of lattice frequency shifts and find deviations up to $8\times10^{-17}$. Even though extended Sisyphus cooling reduces these inconsistencies to the $1\times10^{-18}$ level or below, residual biases may still limit the accuracy of optical lattice clocks. |
| 2025-08-11 | [Precision Test of Bound-State QED at Intermediate-Z with Kaonic Neon](http://arxiv.org/abs/2508.08161v1) | Manti S., Sgaramella F. et al. | We report Dirac-Fock calculations of transition energies for kaonic neon (KNe). For the most intense line, the 7-6 transition, the calculated energy is 9450.28 eV, which includes a bound-state QED (BSQED) contribution of 12.66 eV. This is in excellent agreement with the recent SIDDHARTHA-2 measurement at DA$\Phi$NE of 9450.23 $\pm$ 0.37 (stat.) $\pm$ 1.50 (syst.) eV. With the QED shift far exceeding experimental uncertainty, these results establish kaonic atoms as powerful platforms for precision tests of BSQED in intermediate-Z systems. |
| 2025-08-11 | [Robust Adaptive Discrete-Time Control Barrier Certificate](http://arxiv.org/abs/2508.08153v1) | Changrui Liu, Anil Alan et al. | This work develops a robust adaptive control strategy for discrete-time systems using Control Barrier Functions (CBFs) to ensure safety under parametric model uncertainty and disturbances. A key contribution of this work is establishing a barrier function certificate in discrete time for general online parameter estimation algorithms. This barrier function certificate guarantees positive invariance of the safe set despite disturbances and parametric uncertainty without access to the true system parameters. In addition, real-time implementation and inherent robustness guarantees are provided. Our approach demonstrates that, using the proposed robust adaptive CBF framework, the parameter estimation module can be designed separately from the CBF-based safety filter, simplifying the development of safe adaptive controllers for discrete-time systems. The resulting safety filter guarantees that the system remains within the safe set while adapting to model uncertainties, making it a promising strategy for real-world applications involving discrete-time safety-critical systems. |
| 2025-08-11 | [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](http://arxiv.org/abs/2508.08139v1) | Tianyi Zhou, Johanne Medina et al. | Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation. |
| 2025-08-11 | [OFAL: An Oracle-Free Active Learning Framework](http://arxiv.org/abs/2508.08126v1) | Hadi Khorsand, Vahid Pourahmadi | In the active learning paradigm, using an oracle to label data has always been a complex and expensive task, and with the emersion of large unlabeled data pools, it would be highly beneficial If we could achieve better results without relying on an oracle. This research introduces OFAL, an oracle-free active learning scheme that utilizes neural network uncertainty. OFAL uses the model's own uncertainty to transform highly confident unlabeled samples into informative uncertain samples. First, we start with separating and quantifying different parts of uncertainty and introduce Monte Carlo Dropouts as an approximation of the Bayesian Neural Network model. Secondly, by adding a variational autoencoder, we go on to generate new uncertain samples by stepping toward the uncertain part of latent space starting from a confidence seed sample. By generating these new informative samples, we can perform active learning and enhance the model's accuracy. Lastly, we try to compare and integrate our method with other widely used active learning sampling methods. |
| 2025-08-11 | [Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors](http://arxiv.org/abs/2508.08097v1) | Muhammad Asif, Zain Ali et al. | This work explores the integration of rate-splitting multiple access (RSMA), simultaneous wireless information and power transfer (SWIPT), and beyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the spectral-efficiency, energy-efficiency, coverage, and connectivity of future sixth-generation (6G) communication networks. Specifically, with a multiuser BD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding vectors, the common rate proportion of users, the power-splitting ratios, and scattering matrix of BD-RIS node, under the assumption of imperfect channel state information (CSI). Additionally, to better capture practical hardware behavior, we incorporate a nonlinear energy harvesting model under energy harvesting constraints. We design a robust optimization framework to maximize the system sum-rate, while explicitly accounting for the worst-case impact of CSI uncertainties. Further, we introduce an alternating optimization framework that partitions the problem into several blocks, which are optimized iteratively. More specifically, the transmit precoding vectors are optimized by reformulating the problem as a convex semidefinite programming through successive-convex approximation (SCA), whereas the power-splitting problem is solved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the scattering matrix of the BD-RIS, we first employ SCA to reformulate the problem into a convex form, and then design a manifold optimization strategy based on the Conjugate-Gradient method. Finally, numerical simulation results reveal that the proposed scheme provides significant performance improvements over existing benchmarks and demonstrates rapid convergence within a reasonable number of iterations. |
| 2025-08-11 | [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](http://arxiv.org/abs/2508.08075v1) | Meishen He, Wenjun Ma et al. | The Dempster-Shafer theory of evidence has been widely applied in the field of information fusion under uncertainty. Most existing research focuses on combining evidence within the same frame of discernment. However, in real-world scenarios, trained algorithms or data often originate from different regions or organizations, where data silos are prevalent. As a result, using different data sources or models to generate basic probability assignments may lead to heterogeneous frames, for which traditional fusion methods often yield unsatisfactory results. To address this challenge, this study proposes an open-world information fusion method, termed Full Negation Belief Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a criterion is introduced to determine whether a given fusion task belongs to the open-world setting. Then, by extending the frames, the method can accommodate elements from heterogeneous frames. Finally, a full negation mechanism is employed to transform the mass functions, so that existing combination rules can be applied to the transformed mass functions for such information fusion. Theoretically, the proposed method satisfies three desirable properties, which are formally proven: mass function invariance, heritability, and essential conflict elimination. Empirically, FNBT demonstrates superior performance in pattern classification tasks on real-world datasets and successfully resolves Zadeh's counterexample, thereby validating its practical effectiveness. |
| 2025-08-08 | [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](http://arxiv.org/abs/2508.06468v1) | Jesutofunmi Ajayi, Antonio Di Maio et al. | Network Slicing has emerged as a powerful technique to enable cost-effective, multi-tenant communications and services over a shared physical mobile network infrastructure. One major challenge of service provisioning in slice-enabled networks is the uncertainty in the demand for the limited network resources that must be shared among existing slices and potentially new Network Slice Requests. In this paper, we consider admission control of Network Slice Requests in an online setting, with the goal of maximizing the long-term revenue received from admitted requests. We model the Slice Admission Control problem as an Online Multidimensional Knapsack Problem and present two reservation-based policies and their algorithms, which have a competitive performance for Online Multidimensional Knapsack Problems. Through Monte Carlo simulations, we evaluate the performance of our online admission control method in terms of average revenue gained by the Infrastructure Provider, system resource utilization, and the ratio of accepted slice requests. We compare our approach with those of the online First Come First Serve greedy policy. The simulation's results prove that our proposed online policies increase revenues for Infrastructure Providers by up to 12.9 % while reducing the average resource consumption by up to 1.7% In particular, when the tenants' economic inequality increases, an Infrastructure Provider who adopts our proposed online admission policies gains higher revenues compared to an Infrastructure Provider who adopts First Come First Serve. |
| 2025-08-08 | [Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials](http://arxiv.org/abs/2508.06456v1) | Yonatan Kurniawan, Mingjian Wen et al. | Machine learning interatomic potentials (MLIPs) enable atomistic simulations with near first-principles accuracy at substantially reduced computational cost, making them powerful tools for large-scale materials modeling. The accuracy of MLIPs is typically validated on a held-out dataset of \emph{ab initio} energies and atomic forces. However, accuracy on these small-scale properties does not guarantee reliability for emergent, system-level behavior -- precisely the regime where atomistic simulations are most needed, but for which direct validation is often computationally prohibitive. As a practical heuristic, predictive precision -- quantified as inverse uncertainty -- is commonly used as a proxy for accuracy, but its reliability remains poorly understood, particularly for system-level predictions. In this work, we systematically assess the relationship between predictive precision and accuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes, focusing on ensemble-based uncertainty quantification methods for neural network potentials, including bootstrap, dropout, random initialization, and snapshot ensembles. We use held-out cross-validation for ID assessment and calculate cold curve energies and phonon dispersion relations for OOD testing. These evaluations are performed across various carbon allotropes as representative test systems. We find that uncertainty estimates can behave counterintuitively in OOD settings, often plateauing or even decreasing as predictive errors grow. These results highlight fundamental limitations of current uncertainty quantification approaches and underscore the need for caution when using predictive precision as a stand-in for accuracy in large-scale, extrapolative applications. |
| 2025-08-08 | [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](http://arxiv.org/abs/2508.06452v1) | Mattia Litrico, Mario Valerio Giuffrida et al. | Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance. |
| 2025-08-08 | [$b \to c$ semileptonic sum rule: Current status and prospects](http://arxiv.org/abs/2508.06322v1) | Motoi Endo, Syuhei Iguro et al. | The $b \to c$ semileptonic sum rules provide relations between the decay rates of $B \to D^{(*)} \tau\bar\nu$ and $\Lambda_b \to \Lambda_c \tau\bar\nu$. Starting from the heavy quark and zero-recoil limits, we revisit the derivation of the sum rule for total decay rates. We then examine deviations from the limits and investigate corrections arising from realistic hadron masses and higher-order contributions to form factors, taking account of uncertainties. We show that these corrections are negligible compared to current experimental uncertainties, indicating that the sum rule is useful for cross-checking experimental consistency and testing the validity of the Standard Model predictions. In future, precise determinations of the form factors particularly for the tensor operator will be necessary to compare the sum rule predictions with $\Lambda_b \to \Lambda_c \tau\bar\nu$ data from the LHCb experiment and the Tera-Z projects. |
| 2025-08-08 | [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](http://arxiv.org/abs/2508.06317v1) | Jian Hu, Zixu Cheng et al. | Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic matching, they often struggle with fine-grained temporal localisation. Recently, Group Relative Policy Optimisation (GRPO) reformulates the inference process as a reinforcement learning task, enabling fine-grained grounding and achieving strong in-domain performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To overcome both problems, we introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce. Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using GRPO rollouts, averages them to form a pseudo label, and estimates confidence from the variance across these rollouts. This confidence then weights the training rewards, guiding the model to focus on reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. Codes will be released once published. |
| 2025-08-08 | [A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions](http://arxiv.org/abs/2508.06303v1) | Gerhard Kirsten, Bilgesu Bilgin et al. | Computing with discrete representations of high-dimensional probability distributions is fundamental to uncertainty quantification, Bayesian inference, and stochastic modeling. However, storing and manipulating such distributions suffers from the curse of dimensionality, as memory and computational costs grow exponentially with dimension. Monte Carlo methods require thousands to billions of samples, incurring high computational costs and producing inconsistent results due to stochasticity. We present an efficient tensor train method for performing exact arithmetic operations on discretizations of continuous probability distributions while avoiding exponential growth. Our approach leverages low-rank tensor train decomposition to represent latent random variables compactly using Dirac deltas, enabling deterministic addition, subtraction and multiplication operations directly in the compressed format. We develop an efficient implementation using sparse matrices and specialized data structures that further enhances performance. Theoretical analysis demonstrates polynomial scaling of memory and computational complexity under rank assumptions, and shows how statistics of latent variables can be computed with polynomial complexity. Numerical experiments spanning randomized linear algebra to stochastic differential equations demonstrate orders-of-magnitude improvements in memory usage and computational time compared to conventional approaches, enabling tractable deterministic computations on discretized random variables in previously intractable dimensions. |
| 2025-08-08 | [Situationally-aware Path Planning Exploiting 3D Scene Graphs](http://arxiv.org/abs/2508.06283v1) | Saad Ejaz, Marco Giberna et al. | 3D Scene Graphs integrate both metric and semantic information, yet their structure remains underutilized for improving path planning efficiency and interpretability. In this work, we present S-Path, a situationally-aware path planner that leverages the metric-semantic structure of indoor 3D Scene Graphs to significantly enhance planning efficiency. S-Path follows a two-stage process: it first performs a search over a semantic graph derived from the scene graph to yield a human-understandable high-level path. This also identifies relevant regions for planning, which later allows the decomposition of the problem into smaller, independent subproblems that can be solved in parallel. We also introduce a replanning mechanism that, in the event of an infeasible path, reuses information from previously solved subproblems to update semantic heuristics and prioritize reuse to further improve the efficiency of future planning attempts. Extensive experiments on both real-world and simulated environments show that S-Path achieves average reductions of 5.7x in planning time while maintaining comparable path optimality to classical sampling-based planners and surpassing them in complex scenarios, making it an efficient and interpretable path planner for environments represented by indoor 3D Scene Graphs. |
| 2025-08-08 | [Thermodynamic uncertainty relation for feedback cooling](http://arxiv.org/abs/2508.06174v1) | Kousuke Kumasaki, Kaito Tojo et al. | Feedback cooling enables a system to achieve low temperatures through measurement-based control. Determining the thermodynamic cost required to achieve the ideal cooling efficiency within a finite time remains an important problem. In this work, we establish a thermodynamic uncertainty relation (TUR) for feedback cooling in classical underdamped Langevin systems, thereby deriving a trade-off between the cooling efficiency and the entropy reduction rate. The obtained TUR implies that simultaneous achievement of the ideal cooling efficiency and finite entropy reduction rate is asymptotically possible by letting the fluctuation of the reversible local mean velocity diverge. This is shown to be feasible by using a feedback control based on the Kalman filter. Our results clarify the thermodynamic costs of achieving the fundamental cooling limit of feedback control from the perspective of the TUR. |
| 2025-08-08 | [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](http://arxiv.org/abs/2508.06169v1) | Wenpeng Xing, Jie Chen et al. | Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts. |
| 2025-08-08 | [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](http://arxiv.org/abs/2508.06145v1) | Byeonghun Bang, Jongsuk Yoon et al. | The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information. |
| 2025-08-07 | [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](http://arxiv.org/abs/2508.05634v1) | Jianpeng Yao, Xiaopan Zhang et al. | Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/. |
| 2025-08-07 | [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](http://arxiv.org/abs/2508.05544v1) | Guang Yang, Xinyang Liu | Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications. |
| 2025-08-07 | [Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy](http://arxiv.org/abs/2508.05466v1) | Yun Li, Jicheng Shi et al. | This paper studies the finite-horizon robust optimal control of linear systems subject to model mismatch and additive stochastic disturbances. Utilizing the system level synthesis (SLS) parameterization, we propose a novel SLS design using output-feedback affine control policy and extend it to a distributionally robust setting to improve system resilience by minimizing the cost function while ensuring constraint satisfaction against the worst-case uncertainty distribution. The scopes of model mismatch and stochastic disturbances are quantified using the 1-norm and a Wasserstein metric-based ambiguity set, respectively. For the closed-loop dynamics, we analyze the distributional shift between the predicted output-input response -- computed using nominal parameters and empirical disturbance samples -- and the actual closed-loop distribution, highlighting its dependence on model mismatch and SLS parameterization. Assuming convex and Lipschitz continuous cost functions and constraints, we derive a tractable reformulation of the distributionally robust SLS (DR-SLS) problem by leveraging tools from robust control and distributionally robust optimization (DRO). Numerical experiments validate the performance and robustness of the proposed approach. |
| 2025-08-07 | [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](http://arxiv.org/abs/2508.05454v1) | Wei Li, Zixin Wang et al. | Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field. |
| 2025-08-07 | [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](http://arxiv.org/abs/2508.05441v1) | Zuyuan Zhang, Arnob Ghosh et al. | Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\alpha$ achieves explicit tail-risk control over the expected loss in the "worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability. |
| 2025-08-07 | [LLM-based Multi-Agent Copilot for Quantum Sensor](http://arxiv.org/abs/2508.05421v1) | Rong Sha, Binglin Wang et al. | Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\rm{8}}$ sub-$\rm{\mu}$K atoms without any human intervention within a few hours, representing $\sim$100$\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems. |
| 2025-08-07 | [Metallicity of Active Galactic Nuclei from ultraviolet and optical emission lines-II. Revisiting the $C43$ metallicity calibration and its implications](http://arxiv.org/abs/2508.05397v1) | O. L. Dors, C. B. Oliveira et al. | In this study, a new semi-empirical calibration is proposed between ultraviolet emission lines (\ion{C}{iii}]$\lambda1909$, \ion{C}{iv}$\lambda1549$, \ion{He}{ii}]$\lambda1640$) of type~2 AGNs and their metallicity ($Z$). This calibration is derived by comparing a large sample of 106 objects (data taken from the literature) located over a wide range of redshifts ($0 \: \lesssim \: z \: \lesssim \: 4.0$) with predictions from photoionization models that adopt a recent C/O-O/H relation derived via estimates using the $T_{\rm e}$ method, which is considered the most reliable method. We found that the new calibration produces $Z$ values in agreement (within an uncertainty of $\pm 0.1$ dex) with those from other calibrations and from estimates via the $T_{\rm e}$-method. We find also that AGN metallicities are already high at early epochs, with no evidence for monotonic evolution across the redshift range $0 \: \lesssim \: z \: \lesssim \: 12$. Notably, the highest metallicities in our sample, reaching up to $\rm 4\: Z_{\odot}$, are found in objects at $2 \lesssim z \lesssim 3$. This redshift range coincides with the peak of the cosmic star formation rate history, suggesting a strong connection between the major epoch of star formation, black hole growth, and rapid metal enrichment in the host galaxies of AGNs. Furthermore, our analysis reveals no significant correlation between AGN metallicity and radio properties (radio spectral index or radio luminosity) or host galaxy stellar mass. The lack of a clear mass-metallicity relation, consistent with findings for local AGNs, suggests that the chemical evolution of the nuclear gas is decoupled from the global properties of the host galaxy. |
| 2025-08-07 | [Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games](http://arxiv.org/abs/2508.05378v1) | Zhisen Jiang, Saverio Bolognani et al. | The integration of distributed energy resources into transmission grid operations presents a complex challenge, particularly in the context of reactive power procurement for voltage support. This paper addresses this challenge by formulating the voltage regulation problem as a Stackelberg game, where the Transmission System Operator (TSO) designs incentives to guide the reactive power responses of Distribution System Operators (DSOs). We utilize a gradient-based iterative algorithm that updates the incentives to ensure that DSOs adjust their reactive power injections to maintain voltage stability. We incorporate principles from online feedback optimization to enable real-time implementation, utilizing voltage measurements in both TSO's and DSOs' policies. This approach not only enhances the robustness against model uncertainties and changing operating conditions but also facilitates the co-design of incentives and automation. Numerical experiments on a 5-bus transmission grid demonstrate the effectiveness of our approach in achieving voltage regulation while accommodating the strategic interactions of self-interested DSOs. |
| 2025-08-07 | [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](http://arxiv.org/abs/2508.05342v1) | Shunlei Li, Longsen Gao et al. | Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. |
| 2025-08-07 | [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](http://arxiv.org/abs/2508.05310v1) | Jelle Luijkx, Zlatan Ajanoviƒá et al. | Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io. |
| 2025-08-06 | [Occupancy Learning with Spatiotemporal Memory](http://arxiv.org/abs/2508.04705v1) | Ziyang Leng, Jiawei Yang et al. | 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%. |
| 2025-08-06 | [Open Scene Graphs for Open-World Object-Goal Navigation](http://arxiv.org/abs/2508.04678v1) | Joel Loo, Zhanxin Wu et al. | How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments. |
| 2025-08-06 | [Stochastic Calculus for Pathwise Observables of Markov-Jump Processes: Unification of Diffusion and Jump Dynamics](http://arxiv.org/abs/2508.04647v1) | Lars Torbj√∏rn Stutzer, Cai Dieball et al. | Path-wise observables--functionals of stochastic trajectories--are at the heart of time-average statistical mechanics and are central to thermodynamic inequalities such as uncertainty relations, speed limits, and correlation-bounds. They provide a means of thermodynamic inference in the typical situation, when not all dissipative degrees of freedom in a system are experimentally accessible. So far, theories focusing on path-wise observables have been developing in two major directions, diffusion processes and Markov-jump dynamics, in a virtually disjoint manner. Moreover, even the respective results for diffusion and jump dynamics were derived with a patchwork of different approaches that are predominantly indirect. Stochastic calculus was recently shown to provide a direct approach to path-wise observables of diffusion processes, while a corresponding framework for jump dynamics remained elusive. In our work we develop, in an exact parallelism with continuous-space diffusion, a complete stochastic calculus for path-wise observables of Markov-jump processes. We formulate a "Langevin equation" for jump processes, define general path-wise observables, and establish their covariation structure, whereby we fully account for transients and time-inhomogeneous dynamics. We prove the known kinds of thermodynamic inequalities in their most general form and discus saturation conditions. We determine the response of path-wise observables to general (incl. thermal) perturbations and carry out the continuum limit to achieve the complete unification of diffusion and jump dynamics. Our results open new avenues in the direction of discrete-state analogs of generative diffusion models and the learning of stochastic thermodynamics from fluctuating trajectories. |
| 2025-08-06 | [Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials](http://arxiv.org/abs/2508.04633v1) | James P. Long, Abhishikta Roy et al. | In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model. |
| 2025-08-06 | [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](http://arxiv.org/abs/2508.04588v1) | Nicola Casali, Alessandro Brusaferri et al. | Accurate estimation of intravoxel incoherent motion (IVIM) parameters from diffusion-weighted MRI remains challenging due to the ill-posed nature of the inverse problem and high sensitivity to noise, particularly in the perfusion compartment. In this work, we propose a probabilistic deep learning framework based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling estimation of total predictive uncertainty and decomposition into aleatoric (AU) and epistemic (EU) components. The method was benchmarked against non probabilistic neural networks, a Bayesian fitting approach and a probabilistic network with single Gaussian parametrization. Supervised training was performed on synthetic data, and evaluation was conducted on both simulated and two in vivo datasets. The reliability of the quantified uncertainties was assessed using calibration curves, output distribution sharpness, and the Continuous Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper predictive distributions for the D and f parameters, although slight overconfidence was observed in D*. The Robust Coefficient of Variation (RCV) indicated smoother in vivo estimates for D* with MDNs compared to Gaussian model. Despite the training data covering the expected physiological range, elevated EU in vivo suggests a mismatch with real acquisition conditions, highlighting the importance of incorporating EU, which was allowed by DE. Overall, we present a comprehensive framework for IVIM fitting with uncertainty quantification, which enables the identification and interpretation of unreliable estimates. The proposed approach can also be adopted for fitting other physical models through appropriate architectural and simulation adjustments. |
| 2025-08-06 | [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](http://arxiv.org/abs/2508.04570v1) | A. Tarik Leblebici, Sumeyra Hassan et al. | This paper proposes a joint communication and indoor positioning (JCP) system based on visible light communication (VLC) designed for high-precision indoor environments. The framework supports 2D and 3D positioning using received signal strength (RSS) from pilot transmissions, enhanced by the radical axis theorem to improve accuracy under measurement uncertainties. Communication is achieved using spatial modulation (SM) with M-ary pulse amplitude modulation (PAM), where data is conveyed through the modulation symbol and the active light-emitting diode (LED) index, improving spectral efficiency while maintaining low complexity. A pilot-aided least squares (LS) estimator is employed for joint channel and dimming coefficient estimation, enabling robust symbol detection in multipath environments characterized by both line-of-sight (LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician fading. The proposed system incorporates a dimming control mechanism to meet lighting requirements while maintaining reliable communication and positioning performance. Simulation results demonstrate sub-centimeter localization accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below 10^{-6} for low-order PAM schemes. Additionally, comparative analysis across user locations reveals that positioning and communication performance improve significantly near the geometric center of the LED layout. These findings validate the effectiveness of the proposed system for future 6G indoor networks requiring integrated localization and communication under practical channel conditions. |
| 2025-08-06 | [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](http://arxiv.org/abs/2508.04537v1) | Alkesh K. Srivastava, Aamodh Suresh et al. | We address the challenge of multi-robot autonomous hazard mapping in high-risk, failure-prone, communication-denied environments such as post-disaster zones, underground mines, caves, and planetary surfaces. In these missions, robots must explore and map hazards while minimizing the risk of failure due to environmental threats or hardware limitations. We introduce a behavior-adaptive, information-theoretic planning framework for multi-robot teams grounded in the concept of Behavioral Entropy (BE), that generalizes Shannon entropy (SE) to capture diverse human-like uncertainty evaluations. Building on this formulation, we propose the Behavior-Adaptive Path Planning (BAPP) framework, which modulates information gathering strategies via a tunable risk-sensitivity parameter, and present two planning algorithms: BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for safe deployment under high risk. We provide theoretical insights on the informativeness of the proposed BAPP framework and validate its effectiveness through both single-robot and multi-robot simulations. Our results show that the BAPP stack consistently outperforms Shannon-based and random strategies: BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot survivability with minimal loss in information gain. In multi-agent deployments, BAPP scales effectively through spatial partitioning, mobile base relocation, and role-aware heterogeneity. These findings underscore the value of behavior-adaptive planning for robust, risk-sensitive exploration in complex, failure-prone environments. |
| 2025-08-06 | [Metric Learning in an RKHS](http://arxiv.org/abs/2508.04476v1) | Gokcan Tatli, Yi Chen et al. | Metric learning from a set of triplet comparisons in the form of "Do you think item h is more similar to item i or item j?", indicating similarity and differences between items, plays a key role in various applications including image retrieval, recommendation systems, and cognitive psychology. The goal is to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric learning using kernel methods and neural networks have shown great empirical promise. While previous works have addressed certain aspects of this problem, there is little or no theoretical understanding of such methods. The exception is the special (linear) case in which the RKHS is the standard Euclidean space $\mathbb{R}^d$; there is a comprehensive theory for metric learning in $\mathbb{R}^d$. This paper develops a general RKHS framework for metric learning and provides novel generalization guarantees and sample complexity bounds. We validate our findings through a set of simulations and experiments on real datasets. Our code is publicly available at https://github.com/RamyaLab/metric-learning-RKHS. |
| 2025-08-06 | [Case Studies of Generative Machine Learning Models for Dynamical Systems](http://arxiv.org/abs/2508.04459v1) | Nachiket U. Bapat, Randy C. Paffenroth et al. | Systems like aircraft and spacecraft are expensive to operate in the real world. The design, validation, and testing for such systems therefore relies on a combination of mathematical modeling, abundant numerical simulations, and a relatively small set of real-world experiments. Due to modeling errors, simplifications, and uncertainties, the data synthesized by simulation models often does not match data from the system's real-world operation. We consider the broad research question of whether this model mismatch can be significantly reduced by generative artificial intelligence models (GAIMs). Unlike text- or image-processing, where generative models have attained recent successes, GAIM development for aerospace engineering applications must not only train with scarce operational data, but their outputs must also satisfy governing equations based on natural laws, e.g., conservation laws. The scope of this paper primarily focuses on two case studies of optimally controlled systems that are commonly understood and employed in aircraft guidance, namely: minimum-time navigation in a wind field and minimum-exposure navigation in a threat field. We report GAIMs that are trained with a relatively small set, of the order of a few hundred, of examples and with underlying governing equations. By focusing on optimally controlled systems, we formulate training loss functions based on invariance of the Hamiltonian function along system trajectories. We investigate three GAIM architectures, namely: the generative adversarial network (GAN) and two variants of the variational autoencoder (VAE). We provide architectural details and thorough performance analyses of these models. The main finding is that our new models, especially the VAE-based models, are able to synthesize data that satisfy the governing equations and are statistically similar to the training data despite small volumes of training data. |
| 2025-08-06 | [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](http://arxiv.org/abs/2508.04457v1) | Simon Baur, Wojciech Samek et al. | Reliable uncertainty quantification is crucial for trustworthy decision-making and the deployment of AI models in medical imaging. While prior work has explored the ability of neural networks to quantify predictive, epistemic, and aleatoric uncertainties using an information-theoretical approach in synthetic or well defined data settings like natural image classification, its applicability to real life medical diagnosis tasks remains underexplored. In this study, we provide an extensive uncertainty quantification benchmark for multi-label chest X-ray classification using the MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for convolutional (ResNet) and transformer-based (Vision Transformer) architectures across a wide range of tasks. Additionally, we extend Evidential Deep Learning, HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting. Our analysis provides insights into uncertainty estimation effectiveness and the ability to disentangle epistemic and aleatoric uncertainties, revealing method- and architecture-specific strengths and limitations. |
| 2025-08-05 | [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2508.03692v1) | Ao Liang, Youquan Liu et al. | Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community. |
| 2025-08-05 | [La La LiDAR: Large-Scale Layout Generation from LiDAR Data](http://arxiv.org/abs/2508.03691v1) | Youquan Liu, Lingdong Kong et al. | Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation. |
| 2025-08-05 | [Streaming Generated Gaussian Process Experts for Online Learning and Control](http://arxiv.org/abs/2508.03679v1) | Zewen Yang, Dongfa Zhang et al. | Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \underline{s}treaming \underline{k}ernel-induced progressivel\underline{y} generated expert framework of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches. |
| 2025-08-05 | [Theoretical framework for lattice QCD computations of $B\to K \ell^+ \ell^-$ and $\bar{B}_s\to \ell^+\ell^- Œ≥$ decays rates, including contributions from "Charming Penguins"](http://arxiv.org/abs/2508.03655v1) | R. Frezzotti, G. Gagliardi et al. | We develop a strategy for computing the $B\to K\ell^+\ell^-$ and $\bar{B}_s\to\gamma\ell^+\ell^-$ decay amplitudes using lattice QCD (where $\ell^\pm$ are charged leptons). We focus on those terms which contain complex contributions to the amplitude, due to on-shell intermediate states propagating between the weak operator and electromagnetic current(s). Such terms, which are generally estimated using model calculations and represent significant uncertainties in the phenomenological predictions for these decays, cannot be computed using standard lattice QCD techniques. It has recently been shown that such contributions can be computed using spectral-density methods and our proposed strategy, which we discuss in detail, is built on this approach. The complex contributions include the ``charming penguins" (matrix elements of the current-current operators $O_1^{(c)}$ and $O_2^{(c)}$ defined in Eq. (6) below), in which the charm-quark loop can propagate long distances, particularly close to the region of charmonium resonances. They also include the contributions from the chromomagnetic operator ($O_8$ in standard notation, defined in Eq. (8) below). We discuss the renormalization of the ultra-violet divergences, and in particular those which arise due to ``contact" terms, and explain how those which appear as inverse powers of the lattice spacing can be subtracted non-perturbatively. We apply the spectral density methods in an instructive exploratory computation of the charming penguin diagram in $B\to K\ell^+\ell^-$ decays in which the virtual photon is emitted from the charm-quark loop (the diagram in Fig. 1(a) below) and discuss the prospects and strategies for the reliable determination of the amplitudes in future dedicated computations. |
| 2025-08-05 | [RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data](http://arxiv.org/abs/2508.03578v1) | Jonas Leo Mueller, Lukas Engel et al. | Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications. |
| 2025-08-05 | [Machine learning the single-$Œõ$ hypernuclei with neural-network quantum states](http://arxiv.org/abs/2508.03575v1) | Zi-Xiao Zhang, Yi-Long Yang et al. | Single-$\Lambda$ hypernuclei are the most straightforward extension of atomic nuclei. A thorough description of baryonic system beyond first-generation quark sector is indispensable for the maturation of nuclear $ab$ $initio$ methods. This study pioneers the application of neural-network quantum states to hypernuclei, with trainable parameters determined by variational Monte Carlo approach (VMC-NQS). In order to reduce the numerical uncertainty and treat the nucleons and hyperons in a unified manner, spinor grouping (SG) method is proposed to analytically integrate out isospin degrees of freedom. A novel spin purification scheme is developed to address the severe spin contamination occurring in standard energy minimization due to the weakly bound characteristic of light single-$\Lambda$ hypernuclei. The energy spectrum of $s$-shell hypernuclei is computed with one-thousandth level accuracy and benchmarked against existing stochastic variational results, showing superior performance. By comparing two different sets of Hamiltonian based on pionless effective field theory (pionless EFT), we choose an optimal model and further carry out calculations of selected $p$-shell charge-symmetric hypernuclei with mass number up to 13, exhibiting satisfactory consistency with experimental results. Our findings underscore the potential of VMC-NQS family in approaching exact solution of few-body systems and the accuracy of pionless EFT in modeling hypernuclei. This is crucial for understanding hyperon-nucleon-nucleon and hyperon-hyperon-nucleon interactions, providing a powerful tool for precisely predicting the properties of multi-strangeness hypernuclei. |
| 2025-08-05 | [Regulator and gauge dependence of the Abelian gauge coupling in asymptotically safe quantum gravity](http://arxiv.org/abs/2508.03563v1) | Maksym Riabokon, Marc Schiffer et al. | Both General Relativity and the Standard Model of particle physics are not UV complete. General Relativity is perturbatively non-renormalizable, while the Standard Model features Landau poles, where couplings are predicted to diverge at finite energies, e.g., in the Abelian gauge sector. Asymptotically safe quantum gravity may resolve both of these issues at the same time. In this paper, we assess the systematic uncertainties associated with this scenario, in particular with the gravitationally induced UV-completion of the Abelian gauge sector. Specifically, we study the dependence of this qualitative feature, namely the existence of a UV-complete gauge sector, on unphysical choices like the gauge, and the regulator function. Intriguingly, in some scenarios, we find simultaneous points of minimal sensitivity relative to both the regulator and gauge parameters, which allow for a UV completion. This provides further indications that the simultaneous UV-completion of quantum gravity and matter via an asymptotically safe fixed point is a robust physical feature, and that physical quantities, like scaling exponents, can become independent of unphysical choices. |
| 2025-08-05 | [An Evolutionary Analysis of Narrative Selection](http://arxiv.org/abs/2508.03540v1) | Federico Innocenti, Roberto Rozzi | We study the performance of different methods for processing information, incorporating narrative selection within an evolutionary model. All agents update their beliefs according to Bayes' Rule, but some strategically choose the narrative they use in updating according to heterogeneous criteria. We simulate the endogenous composition of the population, considering different laws of motion for the underlying state of the world. We find that conformists -- that is, agents that choose the narrative to conform to the average belief in the population -- have an evolutionary advantage over other agents across all specifications. The survival chances of the remaining types depend on the uncertainty regarding the state of the world. Agents who tend to develop mild beliefs perform better when the uncertainty is high, whereas agents who tend to develop extreme beliefs perform better when the uncertainty is low. |
| 2025-08-05 | [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](http://arxiv.org/abs/2508.03520v1) | Md Rakibul Hasan, Md Zakir Hossain et al. | Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems. |
| 2025-08-05 | [MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation](http://arxiv.org/abs/2508.03511v1) | Yazhou Zhu, Haofeng Zhang | Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: https://github.com/YazhouZhu19/MAUP. |
| 2025-08-04 | [DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting](http://arxiv.org/abs/2508.02616v1) | Ali Forootani, Mohammad Khosravi et al. | Time series forecasting plays a vital role across scientific, industrial, and environmental domains, especially when dealing with high-dimensional and nonlinear systems. While Transformer-based models have recently achieved state-of-the-art performance in long-range forecasting, they often suffer from interpretability issues and instability in the presence of noise or dynamical uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting framework that combines the representational power of Transformers with the theoretical rigor of Koopman operator theory. Our model features a modular encoder-propagator-decoder structure, where temporal dynamics are learned via a spectrally constrained, linear Koopman operator in a latent space. We impose structural guarantees-such as bounded spectral radius, Lyapunov based energy regularization, and orthogonal parameterization to ensure stability and interpretability. Comprehensive evaluations are conducted on both synthetic dynamical systems, real-world climate dataset (wind speed and surface pressure), financial time series (cryptocurrency), and electricity generation dataset using the Python package that is prepared for this purpose. Across all experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline Transformer models in terms of accuracy, robustness to noise, and long-term forecasting stability. These results establish DeepKoopFormer as a flexible, interpretable, and robust framework for forecasting in high dimensional and dynamical settings. |
| 2025-08-04 | [Quark mass corrections in di-Higgs production amplitude at high-energy](http://arxiv.org/abs/2508.02589v1) | Sebastian Jaskiewicz | A large theoretical uncertainty due to the choice of the top-quark mass renormalisation scheme is present in QCD predictions for Higgs boson pair production. In these proceedings, we report on the recent progress in tackling these uncertainties for the $gg\to HH$ amplitude in the high-energy limit $s,|t|,|u| \gg m_t^2 \gg m_H^2$. Using the Method of Regions and Soft-Collinear Effective Theory, the leading power in $m_t$ behaviour of the amplitude is understood to all orders in the strong coupling expansion, and leading logarithmic resummation leads to a significant reduction in the scheme choice uncertainty in the virtual amplitude for di-Higgs production at high energies. |
| 2025-08-04 | [Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification](http://arxiv.org/abs/2508.02566v1) | Javier Fumanal-Idocin, Raquel Fernandez-Peralta et al. | Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. Unlike classical methods that apply a uniform feature set, DFS customizes feature selection per sample, providing insight into the decision-making process for each case. DFS is especially significant in settings where decision transparency is key, i.e., clinical decisions; however, existing methods use opaque models, which hinder their applicability in real-life scenarios. This paper introduces a novel approach leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. We also show how this method provides a quantitative measure of uncertainty for each feature query and can make the feature selection process computationally lighter by constraining the feature search space. We also discuss when greedy selection of conditional mutual information is equivalent to selecting features that minimize the difference with respect to the global model predictions. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and RL methods, which are mostly considered opaque, compared to our explainable rule-based system. |
| 2025-08-04 | [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](http://arxiv.org/abs/2508.02528v1) | Jingsong Liu, Xiaofeng Deng et al. | Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis. |
| 2025-08-04 | [Quantitative and Predictive Folding Models from Limited Single-Molecule Data Using Simulation-Based Inference](http://arxiv.org/abs/2508.02509v1) | Lars Dingeldein, Aaron Lyons et al. | The study of biomolecular folding has been greatly advanced by single-molecule force spectroscopy (SMFS), which enables the observation of the dynamics of individual molecules. However, extracting quantitative models of fundamental properties such as folding landscapes from SNFS data is very challenging due to instrumental noise, linker artifacts, and the inherent stochasticity of the process, often requiring extensive datasets and complex calibration experiments. Here, we introduce a framework based on simulation-based inference (SBI) that overcomes these limitations by integrating physics-based modeling with deep learning. We apply this framework to analyze constant-force measurements of a DNA hairpin. From a single, short experimental trajectory of only two seconds, we successfully reconstruct the hairpin's free energy landscape and folding dynamics, obtaining results that are in close agreement with established deconvolution methods that require approximately 100 times more data. Furthermore, the Bayesian nature of this approach robustly quantifies uncertainties for inferred parameter values, including the free-energy profile, diffusion coefficients, and linker stiffness, without needing independent measurements of instrumental properties. The inferred model is predictive, generating simulated trajectories that quantitatively reproduce the thermodynamic and kinetic properties of the experimental data. This work establishes SBI as a highly efficient and powerful tool for analyzing single-molecule experiments. The ability to derive statistically robust models from minimal datasets is crucial for investigating complex biomolecular systems where extensive data collection is impractical or impossible. Consequently, our SBI framework enables the rigorous quantitative analysis of previously intractable biomolecular systems, paving the way for novel applications of SMFS. |
| 2025-08-04 | [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](http://arxiv.org/abs/2508.02503v1) | Maxime Bouscary, Saurabh Amin | LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\% to 92\% on the most complex problems. |
| 2025-08-04 | [Pre-discovery TESS Observations of Interstellar Object 3I/ATLAS](http://arxiv.org/abs/2508.02499v1) | Jorge Martinez-Palomera, Amy Tuson et al. | 3I/ATLAS, also known as C/2025 N1 (ATLAS), is the third known interstellar object to pass through our Solar System. We report serendipitous Transiting Exoplanet Survey Satellite (TESS) observations of 3I/ATLAS taken between 2025-05-07 and 2025-06-02,, 55 days prior to the discovery date (2025-07-01) and 14 days prior to the current earliest observation (2025-05-21). We retrieve the TESS pixel data, perform a robust background correction and use a data-driven approach to refine the object's ephemeris. We find a statistically significant offset between the target's observed and predicted positions and we show that this is dominated by uncertainty in the TESS World Coordinate System (WCS) rather than the ephemeris. 3I/ATLAS is too faint to be detected in the individual 200\,second TESS integrations, so we perform image stacking to improve detectability. After co-adding the TESS image data, we performed aperture and Pixel Response Function (PRF) photometry to create two light curves for 3I/ATLAS. Each light curve consists of 15 measurements with $\text{SNR}>3$, collected across two different TESS cameras during the 26\,days that the object was observed, but the PRF light curve is more robust against image noise. The PRF light curve in the TESS bandpass shows a gradual increase in brightness from $T_{\text{mag}} = 20.9 \pm 0.29$ to $T_{\text{mag}} = 19.57 \pm 0.15$. This is expected as 3I/ATLAS approaches the inner Solar System. This paper highlights the power of using TESS for Solar System science; by increasing the photometric observing baseline, future studies will be able to investigate the long-term behavior of 3I/ATLAS |
| 2025-08-04 | [Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning](http://arxiv.org/abs/2508.02495v1) | Kunyu Zhang, Lin Gu et al. | Many previous studies have proposed extracting image labels from clinical notes to create large-scale medical image datasets at a low cost. However, these approaches inherently suffer from label noise due to uncertainty from the clinical experts. When radiologists and physicians analyze medical images to make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or ``not excluded''. Unfortunately, current text-mining methods overlook these nuances, resulting in the creation of noisy labels. Existing methods for handling noisy labels in medical image analysis, which typically address the problem through post-processing techniques, have largely ignored the important issue of expert-driven uncertainty contributing to label noise. To better incorporate the expert-written uncertainty in clinical notes into medical image analysis and address the label noise issue, we first examine the impact of clinical expert uncertainty on label noise. We then propose a clinical expert uncertainty-aware benchmark, along with a label smoothing method, which significantly improves performance compared to current state-of-the-art approaches. |
| 2025-08-04 | [Uncertainty-Aware Perception-Based Control for Autonomous Racing](http://arxiv.org/abs/2508.02494v1) | Jelena Trisovic, Andrea Carron et al. | Autonomous systems operating in unknown environments often rely heavily on visual sensor data, yet making safe and informed control decisions based on these measurements remains a significant challenge. To facilitate the integration of perception and control in autonomous vehicles, we propose a novel perception-based control approach that incorporates road estimation, quantification of its uncertainty, and uncertainty-aware control based on this estimate. At the core of our method is a parametric road curvature model, optimized using visual measurements of the road through a constrained nonlinear optimization problem. This process ensures adherence to constraints on both model parameters and curvature. By leveraging the Frenet frame formulation, we embed the estimated track curvature into the system dynamics, allowing the controller to explicitly account for perception uncertainty and enhancing robustness to estimation errors based on visual input. We validate our approach in a simulated environment, using a high-fidelity 3D rendering engine, and demonstrate its effectiveness in achieving reliable and uncertainty-aware control for autonomous racing. |
| 2025-08-04 | [Gauge theory approach to describe ice crystals habit evolution in ice clouds](http://arxiv.org/abs/2508.02472v1) | Gianluca Di Natale, Francesco Pio De Cosmo et al. | Ice clouds, particularly cirrus clouds, significantly influence Earth's radiative balance but remain poorly characterized in current climate models. A major uncertainty arises from the variability of their microphysical properties, especially the evolution of ice crystal habits under depositional growth. We propose a heuristic method to describe habit evolution based on four fundamental shapes identified in the literature and from in situ observations: droxtals, plates, columns, and rosettes. These represent the primary forms that are relevant under depositional growth, excluding aggregation. In this study, we employ a non-Abelian gauge theory within a field-theoretical framework, imposing an SU(2) $\otimes$ U(1) symmetry on the fields associated with each habit probability growth. This symmetry enables the derivation of a modified system of coupled Fokker-Planck equations, capturing the stochastic growth dynamics of ice crystals while incorporating phenomenological mutual influences among habits. This framework outlines a novel theoretical direction for integrating symmetry principles and field-theoretical tools into the modelling of habit dynamics in ice clouds. |
| 2025-08-01 | [Numerical Uncertainty in Linear Registration: An Experimental Study](http://arxiv.org/abs/2508.00781v1) | Niusha Mirhakimi, Yohan Chatelain et al. | While linear registration is a critical step in MRI preprocessing pipelines, its numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA) simulations, we assessed the most commonly used linear registration tools within major software packages (SPM, FSL, and ANTs) across multiple image similarity measures, two brain templates, and both healthy control (HC, n=50) and Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the influence of linear registration tools and similarity measures on numerical stability. Among the evaluated tools and with default similarity measures, SPM exhibited the highest stability. FSL and ANTs showed greater and similar ranges of variability, with ANTs demonstrating particular sensitivity to numerical perturbations that occasionally led to registration failure. Furthermore, no significant differences were observed between healthy and PD cohorts, suggesting that numerical stability analyses obtained with healthy subjects may generalise to clinical populations. Finally, we also demonstrated how numerical uncertainty measures may support automated quality control (QC) of linear registration results. Overall, our experimental results characterize the numerical stability of linear registration experimentally and can serve as a basis for future uncertainty analyses. |
| 2025-08-01 | [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](http://arxiv.org/abs/2508.00754v1) | Yaxin Ma, Benjamin Colburn et al. | Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models. |
| 2025-08-01 | [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](http://arxiv.org/abs/2508.00750v1) | Prerana Ramkumar | Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications. |
| 2025-08-01 | [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](http://arxiv.org/abs/2508.00734v1) | Liuyun Xu, Seymour M. J. Spence | Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches. |
| 2025-08-01 | [Efficient Solution and Learning of Robust Factored MDPs](http://arxiv.org/abs/2508.00707v1) | Yannik Schnitzer, Alessandro Abate et al. | Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods. |
| 2025-08-01 | [Chemical abundances of seven stars in the GD-1 stream](http://arxiv.org/abs/2508.00671v1) | Jing-Kun Zhao, Guang-Wei Li et al. | We present the first detailed chemical abundances for seven GD-1 stream stars from Subaru/HDS spectroscopy. Atmospheric parameters were derived via color calibrations ($T\rm_{eff}$) and iterative spectroscopic analysis. LTE abundances for 14 elements ($\alpha$, odd-Z, iron-peak, n-capture) were measured. Six stars trace the main orbit, one resides in a `blob'. All exhibit tightly clustered metallicities ([Fe/H] = -2.38, {\bf intrinsic dispersion smaller than 0.05 dex, average uncertainty is about 0.13 dex}). While one star shows binary mass transfer signatures, the other six display consistent abundance patterns (dispersions $<$ uncertainties). Their iron-peak elements (Sc, Cr, Mn, Ni) match Milky Way halo stars. In contrast, Y and Sr are systematically lower than halo stars of similar [Fe/H]. Significantly, six stars show consistently enhanced [Eu/Fe] $\sim$ 0.60 ($\sigma$ = 0.08). A tight Ba-Eu correlation (r = 0.83, p=0.04) exists, with [Ba/Fe] = -0.03 $\pm$ 0.05, indicating a common r-process origin. This extreme chemical homogeneity strongly supports an origin from a single disrupted globular cluster. The lack of light-element anti-correlations may stem from our sample size or the progenitor's low mass. |
| 2025-08-01 | [Uncertainty Relation for Pseudo-Hermitian Quantum Systems](http://arxiv.org/abs/2508.00648v1) | Boubakeur Khantoul, Bilel Hamil et al. | This study investigates pseudo-Hermitian quantum mechanics, where the Hamiltonian satisfies a modified Hermiticity condition. We extend the uncertainty relation for such systems, demonstrating its equivalence to the standard Hermitian case within a pseudo-Hermitian inner product. Analytical solutions to the time-dependent Schr\"odinger equation with a linearly evolving potential are derived. Furthermore, we show that the uncertainty relation for position and momentum remains real and greater than 1/2, highlighting the significance of non-Hermitian systems in quantum mechanics. |
| 2025-08-01 | [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](http://arxiv.org/abs/2508.00643v1) | Albert Matveev, Sanmitra Ghosh et al. | Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification. |
| 2025-08-01 | [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](http://arxiv.org/abs/2508.00600v1) | Mingruo Yuan, Shuyi Zhang et al. | Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines. |
| 2025-08-01 | [Output-recurrent gated state space model for multiphase flows modeling and uncertainty quantification of exhaust vehicles](http://arxiv.org/abs/2508.00588v1) | Ruilin Chen, Ming Fang et al. | This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for complex multiphase flows modeling and uncertainty quantification of exhaust vehicles during motion. By establishing the state-space formulation of the gas-liquid Navier-Stokes equations applying semigroup theory and Galerkin projection, explicitly characterizing the dynamic coupling evolution between the velocity, pressure, and volume fraction fields. A novel Gated State Space Transition (GSST) unit is designed to learn parameterized transition and input matrices with adaptive timescales, enhancing physical interpretability and computational efficiency. The output recursion mechanism aligns with the numerical solution characteristics of state-space equations, mitigating long-term error accumulation and addressing training-inference pattern mismatch issues inherent in teacher forcing and scheduled sampling. Validations on the underwater cone-head and water-exit hemisphere-head vehicles demonstrate that: OR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and computational efficiency through its physics-informed adaptive state-space unit design and parallel matrix operations; The output recursion mechanism ensures more stable training, better generalization, and higher prediction accuracy than teacher forcing and scheduled sampling; OR-GSSM accurately captures the gas-phase expansion, gas-liquid mixing formation, backflow jet generation, bubble shedding, and entire water-exit process, etc, showcasing outstanding modeling capability; Its uncertainty quantification effectively characterizes flow features and uncertainty distributions, validating prediction reliability. The proposed method resolves the accuracy-real-time trade-off in traditional computational fluid dynamics, advancing machine learning for multiphase flow modeling and uncertainty quantification in exhaust vehicles. |
| 2025-07-31 | [Formal Bayesian Transfer Learning via the Total Risk Prior](http://arxiv.org/abs/2507.23768v1) | Nathan Wycoff, Ali Arab et al. | In analyses with severe data-limitations, augmenting the target dataset with information from ancillary datasets in the application domain, called source datasets, can lead to significantly improved statistical procedures. However, existing methods for this transfer learning struggle to deal with situations where the source datasets are also limited and not guaranteed to be well-aligned with the target dataset. A typical strategy is to use the empirical loss minimizer on the source data as a prior mean for the target parameters, which places the estimation of source parameters outside of the Bayesian formalism. Our key conceptual contribution is to use a risk minimizer conditional on source parameters instead. This allows us to construct a single joint prior distribution for all parameters from the source datasets as well as the target dataset. As a consequence, we benefit from full Bayesian uncertainty quantification and can perform model averaging via Gibbs sampling over indicator variables governing the inclusion of each source dataset. We show how a particular instantiation of our prior leads to a Bayesian Lasso in a transformed coordinate system and discuss computational techniques to scale our approach to moderately sized datasets. We also demonstrate that recently proposed minimax-frequentist transfer learning techniques may be viewed as an approximate Maximum a Posteriori approach to our model. Finally, we demonstrate superior predictive performance relative to the frequentist baseline on a genetics application, especially when the source data are limited. |
| 2025-07-31 | [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](http://arxiv.org/abs/2507.23756v1) | Diana Mortagua | This study centers on overcoming the challenge of selecting the best annotators for each query in Active Learning (AL), with the objective of minimizing misclassifications. AL recognizes the challenges related to cost and time when acquiring labeled data, and decreases the number of labeled data needed. Nevertheless, there is still the necessity to reduce annotation errors, aiming to be as efficient as possible, to achieve the expected accuracy faster. Most strategies for query-annotator pairs do not consider internal factors that affect productivity, such as mood, attention, motivation, and fatigue levels. This work addresses this gap in the existing literature, by not only considering how the internal factors influence annotators (mood and fatigue levels) but also presenting a new query-annotator pair strategy, using a Knowledge-Based Recommendation System (RS). The RS ranks the available annotators, allowing to choose one or more to label the queried instance using their past accuracy values, and their mood and fatigue levels, as well as information about the instance queried. This work bases itself on existing literature on mood and fatigue influence on human performance, simulating annotators in a realistic manner, and predicting their performance with the RS. The results show that considering past accuracy values, as well as mood and fatigue levels reduces the number of annotation errors made by the annotators, and the uncertainty of the model through its training, when compared to not using internal factors. Accuracy and F1-score values were also better in the proposed approach, despite not being as substantial as the aforementioned. The methodologies and findings presented in this study begin to explore the open challenge of human cognitive factors affecting AL. |
| 2025-07-31 | [DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction](http://arxiv.org/abs/2507.23736v1) | Kyle Naddeo, Nikolas Koutsoubis et al. | Access to medical imaging and associated text data has the potential to drive major advances in healthcare research and patient outcomes. However, the presence of Protected Health Information (PHI) and Personally Identifiable Information (PII) in Digital Imaging and Communications in Medicine (DICOM) files presents a significant barrier to the ethical and secure sharing of imaging datasets. This paper presents a hybrid de-identification framework developed by Impact Business Information Solutions (IBIS) that combines rule-based and AI-driven techniques, and rigorous uncertainty quantification for comprehensive PHI/PII removal from both metadata and pixel data.   Our approach begins with a two-tiered rule-based system targeting explicit and inferred metadata elements, further augmented by a large language model (LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of synthetic datasets simulating realistic clinical PHI/PII. For pixel data, we employ an uncertainty-aware Faster R-CNN model to localize embedded text, extract candidate PHI via Optical Character Recognition (OCR), and apply the NER pipeline for final redaction. Crucially, uncertainty quantification provides confidence measures for AI-based detections to enhance automation reliability and enable informed human-in-the-loop verification to manage residual risks.   This uncertainty-aware deidentification framework achieves robust performance across benchmark datasets and regulatory standards, including DICOM, HIPAA, and TCIA compliance metrics. By combining scalable automation, uncertainty quantification, and rigorous quality assurance, our solution addresses critical challenges in medical data de-identification and supports the secure, ethical, and trustworthy release of imaging data for research. |
| 2025-07-31 | [Distributed AI Agents for Cognitive Underwater Robot Autonomy](http://arxiv.org/abs/2507.23735v1) | Markus Buchholz, Ignacio Carlucho et al. | Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications. |
| 2025-07-31 | [High-resolution eikonal imaging and uncertainty quantification of the Kilauea caldera](http://arxiv.org/abs/2507.23692v1) | Angela F. Gao, John D. Wilding et al. | Images of the Earth's interior can provide us with insight into the underlying properties of the Earth, such as how seismic activity might emerge and the interplay between seismic and volcanic activity. Understanding these systems requires reliable high-resolution images to understand mechanisms and estimate physical quantities. However, reliable images are often difficult to obtain due to the non-linear nature of seismic wave propagation and the ill-posedness of the related inverse problem. Reconstructions rely on good initial estimates as well as hand-crafted priors, which can ultimately bias solutions. In our work, we present a 3D reconstruction of Kilauea's magmatic system at a previously unattained resolution. Our eikonal tomography procedure improves upon prior imaging results of Kilauea through increased resolution and per-pixel uncertainties estimated through variational inference. In particular, solving eikonal imaging using variational inference with stochastic gradient descent enables stable inversion and uncertainty quantification in the absence of strong prior knowledge of the velocity structure. Our work makes two key contributions: developing a stochastic eikonal tomography scheme with uncertainty quantification and illuminating the structure and melt quantity of the magmatic system that underlies Kilauea. |
| 2025-07-31 | [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](http://arxiv.org/abs/2507.23607v1) | Tien Huu Do, Antoine Masquelier et al. | Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models. |
| 2025-07-31 | [Branching ratios and CP asymmetries of $B^0 \to Œ∑_c f_0$ in the improved perturbative QCD formalism](http://arxiv.org/abs/2507.23578v1) | Min-Qi Li, Xin Liu et al. | Motivated by the idea of fragmented scalar glueball, we investigate the decays $B^0 \to \eta_c f_0$ within the improved perturbative QCD (iPQCD) framework by including the known next-to-leading order corrections. Here, $B^0$ and $f_0$ denote the neutral $B_{d,s}^0$ mesons and the light scalar mesons $f_0(500, 980, 1370, 1500)$ under the $q\bar q$ assignment. The {\it CP}-averaged branching ratios (BRs) and the {\it CP} asymmetries of $B^0 \to \eta_c f_0$ are evaluated with the $f_0(500)[f_0(1370)]-f_0(980)[f_0(1500)]$ mixing in quark-flavor basis. For effective comparisons with the near-future measurements, we further derive the $B^0 \to \eta_c f_0 (\to \pi^+ \pi^-/K^+ K^-)$ BRs under the narrow-width approximation. ${\rm BR}(B_s^0 \to \eta_c f_0(980) (\to \pi^+ \pi^-))= (2.87^{+1.38}_{-1.29}) \times 10^{-4}$ obtained in the iPQCD formalism agrees with the available measurements and predictions within uncertainties. Large BRs of $B_s^0 \to \eta_c f_0(1500) (\to \pi^+ \pi^-/K^+ K^-)$ and large direct {\it CP} asymmetries of $B^0 \to \eta_c f_0(1370, 1500)$ are accessible in the LHCb and Belle-II experiments. The experimental tests of these iPQCD predictions would help us to understand the nature of these light scalars more deeply and provide evidences to decipher $f_0(1500)$ as a primary or fragmented scalar glueball potentially. |
| 2025-07-31 | [Subthreshold parameters of $œÄœÄ$ scattering revisited](http://arxiv.org/abs/2507.23566v1) | Mari√°n Koles√°r, Jaroslav ≈ò√≠ha | Using the most recent experimental data and lattice QCD calculations of $\pi\pi$ scattering lengths, while employing dispersive representations of the amplitude based on Roy equations, we compute the subthreshold parameters of this process. We use Monte Carlo sampling to numerically model the probability distribution of the results based on all uncertainties in the inputs. We also investigate the dependence of the results on a theoretical correlation between the $\pi\pi$ scattering lengths $a^0_0$ and $a^2_0$, which was previously established in the framework of two-flavour chiral perturbation theory. |
| 2025-07-31 | [Latest neutrino results from the FASER experiment and their implications for forward hadron production](http://arxiv.org/abs/2507.23552v1) | FASER Collaboration, Roshan Mammen Abraham et al. | The muon puzzle -- an excess of muons relative to simulation predictions in ultra-high-energy cosmic-ray air showers -- has been reported by many experiments. This suggests that forward particle production in hadronic interactions is not fully understood. Some of the scenarios proposed to resolve this predict reduced production of forward neutral pions and enhanced production of forward kaons (or other particles). The FASER experiment at the LHC is located 480 m downstream of the ATLAS interaction point and is sensitive to neutrinos and muons, which are the decay products of forward charged pions and kaons. In this study, the latest measurements of electron and muon neutrino fluxes are presented using the data corresponding to 9.5 $\mathrm{fb^{-1}}$ and 65.6 $\mathrm{fb^{-1}}$ of proton-proton collisions with $\sqrt{s}=13.6~\mathrm{TeV}$ by the FASER$\nu$ and the FASER electronic detector, respectively. These fluxes are compared with predictions from recent hadronic interaction models, including EPOS-LHCr, SIBYLL 2.3e, and QGSJET 3. The predictions are generally consistent with the measured fluxes from FASER, although some discrepancies appear in certain energy bins. More precise flux measurements with additional data will follow soon, enabling validation of pion, kaon, and charm meson production with finer energy binning, reduced uncertainties, and multi-differential analyses. |
| 2025-07-31 | [A decomposition of Fisher's information to inform sample size for developing or updating fair and precise clinical prediction models -- Part 3: continuous outcomes](http://arxiv.org/abs/2507.23548v1) | Rebecca Whittle, Richard D Riley et al. | Clinical prediction models enable healthcare professionals to estimate individual outcomes using patient characteristics. Current sample size guidelines for developing or updating models with continuous outcomes aim to minimise overfitting and ensure accurate estimation of population-level parameters, but do not explicitly address the precision of predictions. This is a critical limitation, as wide confidence intervals around predictions can undermine clinical utility and fairness, particularly if precision varies across subgroups. We propose methodology for calculating the sample size required to ensure precise and fair predictions in models with continuous outcomes. Building on linear regression theory and the Fisher's unit information matrix, our approach calculates how sample size impacts the epistemic (model-based) uncertainty of predictions and allows researchers to either (i) evaluate whether an existing dataset is sufficiently large, or (ii) determine the sample size needed to target a particular confidence interval width around predictions. The method requires real or synthetic data representing the target population. To assess fairness,the approach can evaluate prediction precision across subgroups. Extensions to prediction intervals are included to additionally address aleatoric uncertainty. Our methodology provides a practical framework for examining required sample sizes when developing or updating prediction models with continuous outcomes, focusing on achieving precise and equitable predictions. It supports the development of more reliable and fair models, enhancing their clinical applicability and trustworthiness. |
| 2025-07-30 | [Solitons, chaos, and quantum phenomena: a deterministic approach to the Schr√∂dinger equation](http://arxiv.org/abs/2507.22868v1) | Dami√† Gomila | We show that the Schr\"odinger equation describes the ensemble mean dynamics of solitons in a Galilean invariant field theory where we interpret solitons as particles. On a zero background, solitons move classically, following Newton`s second law, however, on a non-zero amplitude chaotic background, their momentum and position fluctuate fulfilling an exact uncertainty relation, which give rise to the emergence of quantum phenomena. The Schrodinger equation for the ensemble of solitons is obtained from this exact uncertainty relation, and the amplitude of the background fluctuations is what corresponds to the value of $\hbar$. We confirm our analytical results running simulations of solitons moving against a potential barrier and comparing the ensemble probabilities with the predictions of the time dependent Schr\"odinger equation, providing a deterministic version of the quantum tunneling effect. We conclude with a discussion of how our theory does not present statistical independence between measurement and experiment outcome. |
| 2025-07-30 | [A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model](http://arxiv.org/abs/2507.22854v1) | Andris Ambainis, Joao F. Doriguello et al. | We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces. |
| 2025-07-30 | [Robust Contract with Career Concerns](http://arxiv.org/abs/2507.22852v1) | Tan Gan, Hongcheng Li | An employer contracts with a worker to incentivize efforts whose productivity depends on ability; the worker then enters a market that pays him contingent on ability evaluation. With non-additive monitoring technology, the interdependence between market expectations and worker efforts can lead to multiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy (1992)). We identify a sufficient and necessary criterion for the employer to face such strategic uncertainty--one linked to skill-effort complementarity, a pervasive feature of labor markets. To fully implement work, the employer optimally creates private wage discrimination to iteratively eliminate pessimistic market expectations and low worker efforts. Our result suggests that present contractual privacy, employers' coordination motives generate within-group pay inequality. The comparative statics further explain several stylized facts about residual wage dispersion. |
| 2025-07-30 | [An Uncertainty Principle for Probabilistic Computation in the Retina](http://arxiv.org/abs/2507.22785v1) | Jayanth R Taranath, Salim M'Jahad | We introduce a probabilistic model of early visual processing, beginning with the interaction between a light wavefront and the retina. We argue that perception originates not with deterministic transduction, but with probabilistic threshold crossings shaped by quantum photon arrival statistics and biological variability. We formalize this with an uncertainty relation, \( \Delta \alpha \cdot \Delta t \geq \eta \), through the transformation of light into symbolic neural code through the layered retinal architecture. Our model is supported by previous experimental results, which show intrinsic variability in retinal responses even under fixed stimuli. We contrast this with a classical null hypothesis of deterministic encoding and propose experiments to further test our uncertainty relation. By re-framing the retina as a probabilistic measurement device, we lay the foundation for future models of cortical dynamics rooted in quantum-like computation. We are not claiming that the brain could be working as a quantum-system, but rather putting forth the argument that the brain as a classical system could still implement quantum-inspired computations. We define quantum-inspired computation as a scheme that includes both probabilistic and time-sensitive computation, clearly separating it from classically implementable probabilistic systems. |
| 2025-07-30 | [A comparison of variable selection methods and predictive models for postoperative bowel surgery complications](http://arxiv.org/abs/2507.22771v1) | √ñzge ≈ûahin, Annemiek Kwast et al. | Accurate prediction of postoperative complications can support personalized perioperative care. However, in surgical settings, data collection is often constrained, and identifying which variables to prioritize remains an open question. We analyzed 767 elective bowel surgeries performed under an Enhanced Recovery After Surgery protocol at Medisch Spectrum Twente (Netherlands) between March 2020 and December 2023. Although hundreds of variables were available, most had substantial missingness or near-constant values and were therefore excluded. After data preprocessing, 34 perioperative predictors were selected for further analysis. Surgeries from 2020 to 2022 ($n=580$) formed the development set, and 2023 cases ($n=187$) provided temporal validation. We modeled two binary endpoints: any and serious postoperative complications (Clavien Dindo $\ge$ IIIa). We compared weighted logistic regression, stratified random forests, and Naive Bayes under class imbalance (serious complication rate $\approx$11\%; any complication rate $\approx$35\%). Probabilistic performance was assessed using class-specific Brier scores. We advocate reporting probabilistic risk estimates to guide monitoring based on uncertainty. Random forests yielded better calibration across outcomes. Variable selection modestly improved weighted logistic regression and Naive Bayes but had minimal effect on random forests. Despite single-center data, our findings underscore the value of careful preprocessing and ensemble methods in perioperative risk modeling. |
| 2025-07-30 | [Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models](http://arxiv.org/abs/2507.22766v1) | Felix Kronenwett, Georg Maier et al. | Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters. |
| 2025-07-30 | [Pionic gluons from global QCD analysis of experimental and lattice data](http://arxiv.org/abs/2507.22730v1) | William Good, Patrick C. Barry et al. | We perform the first global QCD analysis of parton distribution functions (PDFs) in the pion, with lattice-QCD data on gluonic pseudo--Ioffe-time distributions fitted simultaneously with experimental Drell-Yan and leading neutron electroproduction data. Inclusion of the lattice results with parametrized systematic corrections significantly reduces the uncertainties on the gluon PDF at parton momentum fractions $x \gtrsim 0.2$, revealing a higher gluon density in the pion at large $x$ than in the proton. The similar gluon momentum fractions in the pion and proton further suggests a relative suppression of the pion gluon density at small $x$. |
| 2025-07-30 | [Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection](http://arxiv.org/abs/2507.22693v1) | Naoki Aizawa, Keita Emura et al. | This study proposes an encrypted PID control system with a disturbance observer (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to achieve control performance while providing resistance to malleability-based attacks. The controller integrates a DOB with a PID structure to compensate for modeling uncertainties by estimating and canceling external disturbances. To enhance security, the system is designed to output error symbols when ciphertexts are falsified during decryption or evaluation, enabling real-time detection of malleability-based signal or parameter falsification. To validate the proposed method, we conduct stage positioning control experiments and attack detection tests using an industrial linear stage. The results show that the encrypted DOB-based PID controller outperforms a conventional encrypted PID controller in terms of tracking accuracy. Furthermore, the system successfully detects two types of malleability-based attacks: one that destabilizes the control system, and another that degrades its performance. The primary contributions of this study are: (i) the implementation of a KHE-based encrypted DOB-PID controller, (ii) the improvement of control performance under uncertainties, and (iii) the experimental demonstration of attack detection capabilities in encrypted control systems. |
| 2025-07-30 | [Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach](http://arxiv.org/abs/2507.22671v1) | Sami Saeed Alghamdi, Christopher Bull et al. | Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools. |
| 2025-07-30 | [Robust Voting under Uncertainty](http://arxiv.org/abs/2507.22655v1) | Satoshi Nakada, Shmuel Nitzan et al. | This paper proposes normative criteria for voting rules under uncertainty about individual preferences. The criteria emphasize the importance of responsiveness, i.e., the probability that the social outcome coincides with the realized individual preferences. Given a convex set of probability distributions of preferences, denoted by $P$, a voting rule is said to be $P$-robust if, for each probability distribution in $P$, at least one individual's responsiveness exceeds one-half. Our main result establishes that a voting rule is $P$-robust if and only if there exists a nonnegative weight vector such that the weighted average of individual responsiveness is strictly greater than one-half under every extreme point of $P$. In particular, if the set $P$ includes all degenerate distributions, a $P$-robust rule is a weighted majority rule without ties. |
| 2025-07-29 | [Planning Persuasive Trajectories Based on a Leader-Follower Game Model](http://arxiv.org/abs/2507.22022v1) | Chaozhe R. He, Yichen Dong et al. | We propose a framework that enables autonomous vehicles (AVs) to proactively shape the intentions and behaviors of interacting human drivers. The framework employs a leader-follower game model with an adaptive role mechanism to predict human interaction intentions and behaviors. It then utilizes a branch model predictive control (MPC) algorithm to plan the AV trajectory, persuading the human to adopt the desired intention. The proposed framework is demonstrated in an intersection scenario. Simulation results illustrate the effectiveness of the framework for generating persuasive AV trajectories despite uncertainties. |
| 2025-07-29 | [Uncertainty Estimation of the Optimal Decision with Application to Cure Process Optimization](http://arxiv.org/abs/2507.21995v1) | Yezhuo Li, Qiong Zhang et al. | Decision-making in manufacturing often involves optimizing key process parameters using data collected from simulation experiments. Gaussian processes are widely used to surrogate the underlying system and guide optimization. Uncertainty often inherent in the decisions given by the surrogate model due to limited data and model assumptions. This paper proposes a surrogate model-based framework for estimating the uncertainty of optimal decisions and analyzing its sensitivity with respect to the objective function. The proposed approach is applied to the composite cure process simulation in manufacturing. |
| 2025-07-29 | [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](http://arxiv.org/abs/2507.21931v1) | Carel van Niekerk, Renato Vukovic et al. | Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.   RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.   By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training. |
| 2025-07-29 | [Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs](http://arxiv.org/abs/2507.21893v1) | Saeed Ghorbani | We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences. |
| 2025-07-29 | [Multi-Gap superconductivity in HgS under pressure](http://arxiv.org/abs/2507.21869v1) | Pietro Maria Forcella, Cesare Tresca et al. | Mercury chalcogenides are a class of materials that exhibit diverse structural phases under pressure, leading to a range of exotic physical properties, including topological phases and chiral phonons. In particular, the phase diagram of mercury sulfide (HgS) remains difficult to characterize, with significant uncertainty surrounding the transition pressure between phases. Based on recent experimental results, we employ Density Functional Theory and Superconducting Density Functional Theory to investigate the pressure-induced structural phase transition in HgS and its interplay with the emergence of superconductivity as the crystal transitions from the cinnabar phase (space group P3$_1$21) to the rock salt phase (space group Fm$\bar{3}$m). Remarkably, the rocksalt phase hosts a multigap superconducting state driven by distinct Fermi surface sheets, with two dominant gaps; the unusually high critical temperature of $\sim$11 K emerges naturally within this multiband scenario, highlighting the role of interband coupling beyond isotropic models. These results place HgS among the few systems where multiband superconducting gap structures emerge under pressure. |
| 2025-07-29 | [Two-neutrino $Œ≤Œ≤$ decay to excited states at next-to-leading order](http://arxiv.org/abs/2507.21868v1) | Daniel Castillo, Dorian Frycz et al. | We study two-neutrino double-beta decay ($2\nu\beta\beta$) into first-excited $0^+_2$ states of nuclei used in $\beta\beta$ decay experiments, including $^{76}$Ge, $^{82}$Se, $^{130}$Te, and $^{136}$Xe. We calculate the corresponding nuclear matrix elements (NMEs) within the nuclear shell model, using various Hamiltonians that describe well the spectroscopy of the initial and final nuclei. We evaluate the next-to-leading order (NLO) long-range NMEs recently introduced within chiral effective field theory, keeping three terms in the expansion of the energy denominator. In most cases, NLO contributions to the half-life are below 5%, but they can increase to 25% due to cancellations in the leading-order Gamow-Teller NME. A detailed analysis in terms of nuclear deformation, including triaxiality, indicates that larger deformation differences between the initial and final states generally lead to smaller NMEs, but the seniority structure of the states also plays a relevant role. The lower range of our predicted half-lives, with uncertainties dominated by the nuclear Hamiltonian used, are slightly longer than the current experimental limit in $^{76}$Ge and the very recent half-life indication in $^{82}$Se. |
| 2025-07-29 | [The Lyman-$Œ±$ Forest from LBGs: First 3D Correlation Measurement with DESI and Prospects for Cosmology](http://arxiv.org/abs/2507.21852v1) | Hiram K. Herrera-Alcantar, Eric Armengaud et al. | The Lyman-$\alpha$ (Ly$\alpha$) forest is a key tracer of large-scale structure at redshifts z > 2, traditionally studied using spectra of quasars. Here, we explore the viability Lyman Break Galaxies (LBGs) as alternative background sources for Ly$\alpha$ forest studies. We analyze 4,151 Ly$\alpha$ forest skewers extracted from LBG spectra obtained in the DESI pilot surveys in the COSMOS and XMM-LSS fields. We present the first measurement of the Ly$\alpha$ forest auto-correlation function derived exclusively from LBG spectra, probing comoving separations up to 48 $h^{-1}$Mpc at an effective redshift of $z_\mathrm{eff}$ = 2.70. The measured signal is consistent with that from DESI DR2 quasar Ly$\alpha$ forest spectra at a comparable redshift, validating LBGs as reliable background sources. We also measure the cross-correlation between the LBG Ly$\alpha$ forest and 13,362 galaxy positions, showing that this observable serves as a sensitive diagnostic for galaxy redshift uncertainties and systematic offsets. Finally, using synthetic LBG spectra and Fisher forecasts, we show that a future wide-area survey over 5000 deg$^2$, targeting 1000 LBGs per deg$^2$ at similar signal-to-noise than our dataset, could enable Ly$\alpha$ forest baryon acoustic oscillation (BAO) measurements with 0.4% precision on the isotropic BAO scale and 1.3% on the anisotropic (Alcock-Paczynski) scale. Combining BAO with a Ly$\alpha$ forest full-shape analysis improves the AP constraint to 0.6%. These results open a new path for precision cosmology at high redshift using dense LBG samples. |
| 2025-07-29 | [Probabilistic Active Goal Recognition](http://arxiv.org/abs/2507.21846v1) | Chenyuan Zhang, Cristian Rojas Cardenas et al. | In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems. |
| 2025-07-29 | [Distribution-Based Masked Medical Vision-Language Model Using Structured Reports](http://arxiv.org/abs/2507.21794v1) | Shreyank N Gowda, Ruichi Zhang et al. | Medical image-language pre-training aims to align medical images with clinically relevant text to improve model performance on various downstream tasks. However, existing models often struggle with the variability and ambiguity inherent in medical data, limiting their ability to capture nuanced clinical information and uncertainty. This work introduces an uncertainty-aware medical image-text pre-training model that enhances generalization capabilities in medical image analysis. Building on previous methods and focusing on Chest X-Rays, our approach utilizes structured text reports generated by a large language model (LLM) to augment image data with clinically relevant context. These reports begin with a definition of the disease, followed by the `appearance' section to highlight critical regions of interest, and finally `observations' and `verdicts' that ground model predictions in clinical semantics. By modeling both inter- and intra-modal uncertainty, our framework captures the inherent ambiguity in medical images and text, yielding improved representations and performance on downstream tasks. Our model demonstrates significant advances in medical image-text pre-training, obtaining state-of-the-art performance on multiple downstream tasks. |
| 2025-07-29 | [The impact of large-scale EV charging on the real-time operation of distribution systems: A comprehensive review](http://arxiv.org/abs/2507.21759v1) | Zhe Yu, Chuang Yang et al. | With the large-scale integration of electric vehicles (EVs) in the distribution grid, the unpredictable nature of EV charging introduces considerable uncertainties to the grid's real-time operations. This can exacerbate load fluctuations, compromise power quality, and pose risks to the grid's stability and security. However, due to their dual role as controllable loads and energy storage devices, EVs have the potential to mitigate these fluctuations, balance the variability of renewable energy sources, and provide ancillary services that support grid stability. By leveraging the bidirectional flow of information and energy in smart grids, the adverse effects of EV charging can be minimized and even converted into beneficial outcomes through effective real-time management strategies. This paper explores the negative impacts of EV charging on the distribution system's real-time operations and outlines methods to transform these challenges into positive contributions. Additionally, it provides an in-depth analysis of the real-time management system for EV charging, focusing on state estimation and management strategies. |
| 2025-07-28 | [Locally Adaptive Conformal Inference for Operator Models](http://arxiv.org/abs/2507.20975v1) | Trevor Harris, Yan Liu | Operator models are regression algorithms for functional data and have become a key tool for emulating large-scale dynamical systems. Recent advances in deep neural operators have dramatically improved the accuracy and scalability of operator modeling, but lack an inherent notion of predictive uncertainty. We introduce Local Spectral Conformal Inference (LSCI), a new framework for locally adaptive, distribution-free uncertainty quantification for neural operator models. LSCI uses projection-based depth scoring and localized conformal inference to generate function-valued prediction sets with statistical guarantees. We prove approximate finite-sample marginal coverage under local exchangeability, and demonstrate significant gains in adaptivity and coverage across synthetic and real-world operator learning tasks. |
| 2025-07-28 | [The Concordance of Weak Lensing and Escape Velocity Mass Estimates for Galaxy Clusters](http://arxiv.org/abs/2507.20938v1) | Alexander Rodriguez, Christopher J. Miller | In the $\Lambda$CDM paradigm, the masses of the galaxy clusters inferred using background galaxies via weak-lensing shear should agree with the masses measured using the galaxy projected radius-velocity phase-space data via the escape velocity profile. However, prior work indicates that the correlation between caustic-inferred escape masses and weak lensing masses is statistically consistent with zero. Based on recent advancements in the measurement of the escape edge and its physical interpretation, we conduct a revised comparison between these two independent mass inference techniques for 46 galaxy clusters between $0.05 \le z \le 0.3$ and over an order of magnitude in mass, $14.4 \le {\rm log}_{10} M/M_{\odot} \le 15.4$. We find excellent agreement, with a correlation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between the two mass measurements consistent with zero (0.02 $\pm$ 0.02 dex). The observed scatter between these direct mass estimates is 0.17 dex and is consistent with the reported individual mass errors, suggesting that there is no need for an additional intrinsic component. We discuss the important practical consequences of these results, focusing on the systematic uncertainties inherent to each technique, and their implications for cosmology. |
| 2025-07-28 | [Target-density formation in swarms with stochastic sensing and dynamics](http://arxiv.org/abs/2507.20911v1) | Jason Hindes, George Stantchev et al. | An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target. |
| 2025-07-28 | [DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](http://arxiv.org/abs/2507.20879v1) | Weicheng Zheng, Xiaofei Mao et al. | Vision-Language Models (VLMs) are advancing autonomous driving, yet their potential is constrained by myopic decision-making and passive perception, limiting reliability in complex environments. We introduce DriveAgent-R1 to tackle these challenges in long-horizon, high-level behavioral decision-making. DriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that adaptively switches between efficient text-based and in-depth tool-based reasoning, and an Active Perception mechanism with a vision toolkit to proactively resolve uncertainties, thereby balancing decision-making efficiency and reliability. The agent is trained using a novel, three-stage progressive reinforcement learning strategy designed to master these hybrid capabilities. Extensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art performance, outperforming even leading proprietary large multimodal models, such as Claude Sonnet 4. Ablation studies validate our approach and confirm that the agent's decisions are robustly grounded in actively perceived visual evidence, paving a path toward safer and more intelligent autonomous systems. |
| 2025-07-28 | [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](http://arxiv.org/abs/2507.20861v1) | Marco Faroni, Carlo Odesco et al. | Physics-based simulations and learning-based models are vital for complex robotics tasks like deformable object manipulation and liquid handling. However, these models often struggle with accuracy due to epistemic uncertainty or the sim-to-real gap. For instance, accurately pouring liquid from one container to another poses challenges, particularly when models are trained on limited demonstrations and may perform poorly in novel situations. This paper proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed to mitigate these inaccuracies. By incorporating estimates of model uncertainty, the proposed MCTS strategy biases the search towards actions with lower predicted uncertainty. This approach enhances the reliability of planning under uncertain conditions. Applied to a liquid pouring task, our method demonstrates improved success rates even with models trained on minimal data, outperforming traditional methods and showcasing its potential for robust decision-making in robotics. |
| 2025-07-28 | [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](http://arxiv.org/abs/2507.20850v1) | Meiting Dang, Yanping Wu et al. | Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method. |
| 2025-07-28 | [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](http://arxiv.org/abs/2507.20804v1) | Xueyao Wan, Hang Yu | Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths. |
| 2025-07-28 | [Physical Constraints on the Rhythmicity of the Biological Clock](http://arxiv.org/abs/2507.20750v1) | YeongKyu Lee, Changbong Hyeon | Circadian rhythms in living organisms are temporal orders emerging from biochemical circuits driven out of equilibrium. Here, we study how the rhythmicity of a biochemical clock is shaped using the KaiABC system. A phase diagram constructed as a function of KaiC and KaiA concentrations reveals a sharply bounded limit-cycle region, which naturally explains arrhythmia upon protein over-expression. Beyond the Hopf bifurcation, intrinsic noise enables regular oscillation via coherence resonance. Within the limit-cycle region, greater rhythmic precision incurs a higher energetic cost, following the thermodynamic uncertainty relation. The cost-minimizing period of the KaiABC clock ($\sim$21-hr) is close enough to entrain to 24-hr cycle of environment. Our study substantiates universal physical constraints on the robustness, precision, and efficiency of noisy biological clocks. |
| 2025-07-28 | [Generalized Uncertainty Principle as a Mechanism for CP Violation](http://arxiv.org/abs/2507.20727v1) | Hector Gisbert, Victor Ilisie et al. | Within quantum electrodynamics we show that the Generalized Uncertainty Principle induces higher-derivative corrections that promote the topological invariant $F_{\mu\nu}\,\widetilde F^{\mu\nu}$ to the dynamical, non-topological operator $\partial^\lambda F_{\mu\nu}\,\partial_\lambda \widetilde F^{\mu\nu}$. We explore the resulting phenomenology, focusing on the generation of electric dipole moments. Our findings open a new low-energy window for testing quantum-gravity scenarios through precision measurements of charge-parity violation. |
| 2025-07-28 | [Uncertainty-driven Embedding Convolution](http://arxiv.org/abs/2507.20718v1) | Sungjun Lim, Kangjun Noh et al. | Text embeddings are essential components in modern NLP pipelines. While numerous embedding models have been proposed, their performance varies across domains, and no single model consistently excels across all tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble weights based on embedding uncertainty, grounded in a Bayes-optimal solution under a surrogate loss. Additionally, UEC introduces an uncertainty-aware similarity function that directly incorporates uncertainty into similarity scoring. Extensive experiments on retrieval, classification, and semantic similarity benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling. |
| 2025-07-25 | [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](http://arxiv.org/abs/2507.19458v1) | Amir Fard, Arnold X. -X. Yuan | Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows. |
| 2025-07-25 | [DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment](http://arxiv.org/abs/2507.19418v1) | Yiwei Lou, Yuanpeng He et al. | Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios. |
| 2025-07-25 | [Measurement of the Inelastic Proton-Proton Cross-Section at $\sqrt{s} \geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory](http://arxiv.org/abs/2507.19326v1) | Olena Tkachenko | Measuring proton-proton interaction cross-sections at center-of-mass energies above 40 TeV remains a significant challenge in particle physics. The Pierre Auger Observatory provides a unique opportunity to study the interactions at the highest energies through the distribution of the depth of maximum shower development ($X_\mathrm{max}$) observed by its Fluorescence Detector. In previous studies, the determination of the interaction cross-section at ultrahigh energies has relied on the assumption that the tail of the $X_\mathrm{max}$ distribution is proton-dominated, which restricts the analysis to a limited energy range below the ankle and introduces related systematic uncertainties. In this contribution, we adopt a novel method for the simultaneous estimation of the proton-proton interaction cross-section and the primary cosmic-ray mass composition using data from the Pierre Auger Observatory, avoiding assumptions about one quantity to infer the other and thus improving the accuracy and robustness of our analysis. In addition, a systematic shift in the $X_\mathrm{max}$ scale is fitted to account for both experimental uncertainties and theoretical constraints on the modeling of particle interactions. The obtained results are consistent with previous analyses and provide additional constraints on hadronic interaction models. The measured proton-proton inelastic cross-section at ultra-high energies agrees well with extrapolations of accelerator data. The inferred cosmic-ray composition and the $X_\mathrm{max}$-scale shift are also compatible with previous estimates. |
| 2025-07-25 | [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](http://arxiv.org/abs/2507.19263v1) | Achille Morenville, √âric Piette | In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings. |
| 2025-07-25 | [Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation](http://arxiv.org/abs/2507.19246v1) | Robert Hahn, Sebastian Sch√∂ps | While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort. |
| 2025-07-25 | [Constraining the origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory: a three-dimensional approach](http://arxiv.org/abs/2507.19216v1) | Marta Bianciotto | Unveiling the sources of ultra-high-energy cosmic rays remains one of the main challenges of high-energy astrophysics. Measurements of anisotropies in their arrival directions are key to identifying their sources, yet magnetic deflections obscure direct associations. In this work, we reconstruct the sky regions of possible origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory by tracing their trajectories through Galactic magnetic fields using up-to-date models, while fully accounting for energy and directional uncertainties. A mixed composition at injection is assumed to model the detected charge distributions of such events. Different classes of astrophysical sources are investigated and tested for a correlation with the inferred regions of origin of the events. By incorporating constraints on the maximum propagation distances, we also allow for a three-dimensional localization of the possible source regions. Our findings provide new constraints on the sources of the highest-energy cosmic particles and offer fresh insights into the role of Galactic magnetic fields in shaping the observed ultra-high-energy cosmic-ray sky. |
| 2025-07-25 | [Emulating redshift-mixing due to blending in weak gravitational lensing](http://arxiv.org/abs/2507.19130v1) | Zekang Zhang, Daniel Gruen et al. | Galaxies whose images overlap in the focal plane of a telescope, commonly referred to as blends, are often located at different redshifts. Blending introduces a challenge to weak lensing cosmology probes, as such blends are subject to shear signals from multiple redshifts. This effect can be described by joining shear bias and redshift characterisation in the effective redshift distribution, $n_{\gamma}(z)$, which includes the response of apparent shapes of detected objects to shear of galaxies at redshift $z$. In this work, we propose a novel method to correct $n_{\gamma}(z)$ for redshift-mixed blending by emulating the shear response to neighbouring galaxies. Specifically, we design a ``half-sky-shearing'' simulation with HSC-Wide-like specifications, in which we extract the response of a detected object's measured ellipticity to shear of neighbouring galaxies among numerous galaxy pairs. We demonstrate the feasibility of accurately emulating these pairwise responses and validate the robustness of our approach under varying observing conditions and galaxy population uncertainties. We find that the effective redshift of sources at the high-redshift tail of the distribution is about 0.05 lower than expected when not modelling the effect. Given appropriately processed image simulations, our correction method can be readily incorporated into future cosmological analyses to mitigate this source of systematic error. |
| 2025-07-25 | [Boundary-layer transition in the age of data: from a comprehensive dataset to fine-grained prediction](http://arxiv.org/abs/2507.19120v1) | Wenhui Chang, Hongyuan Hu et al. | The laminar-to-turbulent transition remains a fundamental and enduring challenge in fluid mechanics. Its complexity arises from the intrinsic nonlinearity and extreme sensitivity to external disturbances. This transition is critical in a wide range of applications, including aerospace, marine engineering, geophysical flows, and energy systems. While the governing physics can be well described by the Navier-Stokes equations, practical prediction efforts often fall short due to the lack of comprehensive models for perturbation initialization and turbulence generation in numerical simulations. To address the uncertainty introduced by unforeseeable environmental perturbations, we propose a fine-grained predictive framework that accurately predicts the transition location. The framework generates an extensive dataset using nonlinear parabolized stability equations (NPSE). NPSE simulations are performed over a wide range of randomly prescribed initial conditions for the generic zero-pressure-gradient flat-plate boundary-layer flow, resulting in a large dataset that captures the nonlinear evolution of instability waves across three canonical transition pathways (Type-K, -H, and -O). From a database of 3000 simulation cases, we extract diagnostic quantities (e.g., wall pressure signals and skin-friction coefficients) from each simulation to construct a feature set that links pre-transition flow characteristics to transition onset locations. Machine learning models are systematically evaluated, with ensemble methods-particularly XGBoost-demonstrating exceptional predictive accuracy (mean relative error of approximately 0.001). Compared to methods currently available (e.g., N-factor, transitional turbulence model), this approach accounts for the physical process and achieves transition prediction without relying on any empirical parameters. |
| 2025-07-25 | [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](http://arxiv.org/abs/2507.19117v1) | Swapnil Hingmire, Ze Shi Li et al. | Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed. |
| 2025-07-25 | [Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks](http://arxiv.org/abs/2507.19111v1) | Bowen Li, Junting Chen | Dynamic low altitude networks offer significant potential for efficient and reliable data transport via unmanned aerial vehicles (UAVs) relays which usually operate with predetermined trajectories. However, it is challenging to optimize the data routing and resource allocation due to the time-varying topology and the need to control interference with terrestrial systems. Traditional schemes rely on time-expanded graphs with uniform and fine time subdivisions, making them impractical for interference-aware applications. This paper develops a dynamic space-time graph model with a cross-layer optimization framework that converts a joint routing and predictive resource allocation problem into a joint bottleneck path planning and resource allocation problem. We develop explicit deterministic bounds to handle the channel uncertainty and prove a monotonicity property in the problem structure that enables us to efficiently reach the globally optimal solution to the predictive resource allocation subproblem. Then, this approach is extended to multi-commodity transmission tasks through time-frequency allocation, and a bisection search algorithm is developed to find the optimum solution by leveraging the monotonicity of the feasible set family. Simulations verify that the single-commodity algorithm approaches global optimality with more than 30 dB performance gain over the classical graph-based methods for delay-sensitive and large data transportation. At the same time, the multi-commodity method achieves 100X improvements in dense service scenarios and enables an additional 20 dB performance gain by data segmenting. |
| 2025-07-24 | [Partial-State DADS Control for Matched Unmodeled Dynamics](http://arxiv.org/abs/2507.18609v1) | Iasson Karafyllis, Miroslav Krstic | We extend the Deadzone-Adapted Disturbance Suppression (DADS) control to time-invariant systems with dynamic uncertainties that satisfy the matching condition and for which no bounds for the disturbance and the unknown parameters are known. This problem is equivalent to partial-state adaptive feedback, where the states modeling the dynamic uncertainty are unmeasured. We show that the DADS controller can bypass small-gain conditions and achieve robust regulation for systems in spite of the fact that the strength of the interconnections has no known bound. Moreover, no gain and state drift arise, regardless of the size of the disturbances and unknown parameters. Finally, the paper provides the detailed analysis of a control system where the unmeasured state (or the dynamic uncertainty) is infinite-dimensional and described by a reaction-diffusion Partial Differential Equation, where the diffusion coefficient and the reaction term are unknown. It is shown that even in the infinite-dimensional case, a DADS controller can be designed and guarantees robust regulation of the plant state. |
| 2025-07-24 | [Searching for Gravitational Waves with Gaia and its Cross-Correlation with PTA: Absolute vs Relative Astrometry](http://arxiv.org/abs/2507.18593v1) | Massimo Vaglio, Mikel Falxa et al. | Astrometric missions like Gaia provide exceptionally precise measurements of stellar positions and proper motions. Gravitational waves traveling between the observer and distant stars can induce small, correlated shifts in these apparent positions, a phenomenon known as astrometric deflection. The precision and scale of astrometric datasets make them well-suited for searching for a stochastic gravitational wave background, whose signature appears in the two-point correlation function of the deflection field across the sky. Although Gaia achieves high accuracy in measuring angular separations in its focal plane, systematic uncertainties in the satellite's absolute orientation limit the precision of absolute position measurements. These orientation errors can be mitigated by focusing on relative angles between star pairs, which effectively cancel out common-mode orientation noise. In this work, we compute the astrometric response and the overlap reduction functions for this relative astrometry approach, correcting previous expressions presented in the literature. We use a Fisher matrix analysis to compare the sensitivity of relative astrometry to that of conventional absolute astrometry. Our analysis shows that while the relative method is theoretically sound, its sensitivity is limited for closely spaced star pairs within a single Gaia field of view. Pairs with large angular separations could provide competitive sensitivity, but are practically inaccessible due to Gaia's scanning law. Finally, we demonstrate that combining astrometric data with observations from pulsar timing arrays leads to slight improvements in sensitivity at frequencies greater than approximately 10^-7 Hz. |
| 2025-07-24 | [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](http://arxiv.org/abs/2507.18562v1) | Jiafeng Xiong, Yuting Zhao | Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. |
| 2025-07-24 | [Large deviations of ionic currents in dilute electrolytes](http://arxiv.org/abs/2507.18556v1) | Jafar Farhadi, David T. Limmer | We evaluate the exponentially rare fluctuations of the ionic current for a dilute electrolyte by means of macroscopic fluctuation theory. We consider the fluctuating hydrodynamics of a fluid electrolyte described by a stochastic Poisson-Nernst-Planck equation. We derive the Euler-Lagrange equations that dictate the optimal concentration profiles of ions conditioned on exhibiting a given current, whose form determines the likelihood of that current in the long-time limit. For a symmetric electrolyte under small applied voltages, number density fluctuations are small, and ionic current fluctuations are Gaussian with a variance determined by the Nernst-Einstein conductivity. Under large applied potentials, where number densities vary, the ionic current distribution is generically non-Gaussian. Its structure is constrained thermodynamically by Gallavotti-Cohen symmetry and the thermodynamic uncertainty principle. |
| 2025-07-24 | [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](http://arxiv.org/abs/2507.18498v1) | Zongzheng Zhang, Xuchong Qiu et al. | Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction. |
| 2025-07-24 | [A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks](http://arxiv.org/abs/2507.18492v1) | Mirhan √úrkmez, Carsten Kalles√∏e et al. | Water utilities aim to reduce the high electrical costs of Water Distribution Networks (WDNs), primarily driven by pumping. However, pump scheduling is challenging due to model uncertainties and water demand forecast errors. This paper presents a Robust Model Predictive Control (RMPC) method for optimal and reliable pump scheduling, extending a previous efficient robust control method tailored to our model. A linear model with bounded additive disturbances is used to represent tank water level evolution, with uncertainty bounds derived from WDN simulation and demand data. At each time step, a pump scheduling policy, affine in past disturbances, is optimized to satisfy system constraints over a prediction horizon. The resulting policies are then applied in a receding horizon fashion. The optimization problem is formulated to require $\mathcal{O}(N^6)$ computations per iteration with an interior-point method, which is reduced to $\mathcal{O}(N^3)$ by reformulating it into a sparse form. When evaluated on a model representing the water distribution network of Randers, a medium-sized town in Denmark, the method surpasses nominal and constraint-tightening model predictive control (MPC) approaches in terms of meeting constraints and provides comparable economic outcomes. |
| 2025-07-24 | [Relativistic Calculations of Energy Levels, Field Shift Factors, and Polarizabilities of Mercury and Copernicium](http://arxiv.org/abs/2507.18490v1) | Hongxu Liu, Jize Han et al. | Mercury (Hg) and superheavy element copernicium (Cn) are investigated using equation-of-motion relativistic coupled-cluster (EOM-RCC) and configuration interaction plus many-body perturbation theory (CI+MBPT) methods. Key atomic properties including ionization potentials (IP), excitation energies (EEs), isotope field shift factors (F), and static electric dipole polarizabilities ({\alpha}) are calculated for ground and low-lying excited states. To evaluate the theoretical accuracy, calculations for both Hg and Cn are performed, with experimental data of Hg serving as benchmarks. Furthermore, basis set dependence has been systematically evaluated in the EOM-RCC calculations, with corresponding uncertainty estimates having been provided. The calculated atomic properties could provide valuable insights into the electronic structure and chemical behavior of superheavy elements. |
| 2025-07-24 | [INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data](http://arxiv.org/abs/2507.18488v1) | Mario Figueira, Michela Cameletti et al. | Environmental processes often exhibit complex, non-linear patterns and discontinuities across space and time, posing significant challenges for traditional geostatistical modeling approaches. In this paper, we propose a hybrid spatio-temporal modeling framework that combines the interpretability and uncertainty quantification of Bayesian models -- estimated using the INLA-SPDE approach -- with the predictive power and flexibility of Random Forest (RF). Specifically, we introduce two novel algorithms, collectively named INLA-RF, which integrate a statistical spatio-temporal model with RF in an iterative two-stage framework. The first algorithm (INLA-RF1) incorporates RF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2) uses RF to directly correct selected latent field nodes. Both hybrid strategies enable uncertainty propagation between modeling stages, an aspect often overlooked in existing hybrid approaches. In addition, we propose a Kullback-Leibler divergence-based stopping criterion. We evaluate the predictive performance and uncertainty quantification capabilities of the proposed algorithms through two simulation studies. Results suggest that our hybrid approach enhances spatio-temporal prediction while maintaining interpretability and coherence in uncertainty estimates. |
| 2025-07-24 | [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](http://arxiv.org/abs/2507.18484v1) | Xiao Yang, Lingxuan Wu et al. | Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving. |
| 2025-07-24 | [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](http://arxiv.org/abs/2507.18423v1) | Mizuki Funato, Yohei Sawada | Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions. |
| 2025-07-23 | [A narrowband burst from FRB 20190520B simultaneously observed by FAST and Parkes](http://arxiv.org/abs/2507.17696v1) | Yuhao Zhu, Chenhui Niu et al. | Fast Radio Bursts (FRBs) are short-duration radio transients with mysterious origins. Since its uncertainty, there are very few FRBs that are observed by different instruments, simultaneously. This study presents a detailed analysis of a burst from FRB 20190520B observed by FAST and Parkes at the same time. The spectrum of this individual burst ended at the upper limit of the FAST frequency band and was simultaneously detected by the Parkes telescope in the 1.5-1.8 GHz range. By employing spectral energy distribution (SED) and spectral sharpness methods, we confirmed the presence of narrowband radiation in FRB 20190520B, which is crucial for understanding its radiation mechanisms. Our findings support the narrowband characteristics that most repeaters exhibit. This work also highlights the necessity of continued multiband observations to explore its periodicity and frequency-dependent properties, contributing to an in-depth understanding of FRB phenomena. |
| 2025-07-23 | [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](http://arxiv.org/abs/2507.17695v1) | Ilias Chatzistefanidis, Navid Nikaein | Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance. |
| 2025-07-23 | [The Joint Asymptotic Distribution of Entropy and Complexity](http://arxiv.org/abs/2507.17625v1) | Angelika Silbernagel, Christian Wei√ü | We derive the asymptotic distribution of ordinal-pattern frequencies under weak dependence conditions and investigate the long-run covariance matrix not only analytically for moving-average, Gaussian, and the novel generalized coin-tossing processes, but also approximately by a simulation-based approach. Then, we deduce the asymptotic distribution of the entropy-complexity pair, which emerged as a popular tool for summarizing the time-series dynamics. Here, we make the necessary distinction between a uniform and a non-uniform ordinal pattern distribution and, thus, obtain two different limit theorems. On this basis, we consider a test for serial dependence and check its finite-sample performance. Moreover, we use our asymptotic results to approximate the estimation uncertainty of entropy-complexity pairs. |
| 2025-07-23 | [Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling](http://arxiv.org/abs/2507.17526v1) | Leandro Von Krannichfeldt, Kristina Orehounig et al. | Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling. |
| 2025-07-23 | [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](http://arxiv.org/abs/2507.17477v1) | Haoran Sun, Zekun Zhang et al. | Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance. |
| 2025-07-23 | [Analysing Models for Volatility Clustering with Subordinated Processes: VGSA and Beyond](http://arxiv.org/abs/2507.17431v1) | Sourojyoti Barick, Sudip Ratan Chandra | This paper explores a comprehensive class of time-changed stochastic processes constructed by subordinating Brownian motion with Levy processes, where the subordination is further governed by stochastic arrival mechanisms such as the Cox Ingersoll Ross (CIR) and Chan Karolyi Longstaff Sanders (CKLS) processes. These models extend classical jump frameworks like the Variance Gamma (VG) and CGMY processes, allowing for more flexible modeling of market features such as jump clustering, heavy tails, and volatility persistence. We first revisit the theory of Levy subordinators and establish strong consistency results for the VG process under Gamma subordination. Building on this, we prove asymptotic normality for both the VG and VGSA (VG with stochastic arrival) processes when the arrival process follows CIR or CKLS dynamics. The analysis is then extended to the more general CGMY process under stochastic arrival, for which we derive analogous consistency and limit theorems under positivity and regularity conditions on the arrival process. A simulation study accompanies the theoretical work, confirming our results through Monte Carlo experiments, with visualizations and normality testing (via Shapiro-Wilk statistics) that show approximate Gaussian behavior even for processes driven by heavy-tailed jumps. This work provides a rigorous and unified probabilistic framework for analyzing subordinated models with stochastic time changes, with applications to financial modeling and inference under uncertainty. |
| 2025-07-23 | [Confidence Calibration in Vision-Language-Action Models](http://arxiv.org/abs/2507.17383v1) | Thomas P Zollo, Richard Zemel | Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification. |
| 2025-07-23 | [Exploring Spatial Diversity for Region-based Active Learning](http://arxiv.org/abs/2507.17367v1) | Lile Cai, Xun Xu et al. | State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves $95\%$ performance of fully supervised methods with only $5-9\%$ of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation. |
| 2025-07-23 | [On Distributionally Robust Lossy Source Coding](http://arxiv.org/abs/2507.17366v1) | Giuseppe Serra, Photios A. Stavrou et al. | In this paper, we investigate the problem of distributionally robust source coding, i.e., source coding under uncertainty in the source distribution, discussing both the coding and computational aspects of the problem. We propose two extensions of the so-called Strong Functional Representation Lemma (SFRL), considering the cases where, for a fixed conditional distribution, the marginal inducing the joint coupling belongs to either a finite set of distributions or a Kullback-Leibler divergence sphere (KL-Sphere) centered at a fixed nominal distribution. Using these extensions, we derive distributionally robust coding schemes for both the one-shot and asymptotic regimes, generalizing previous results in the literature. Focusing on the case where the source distribution belongs to a given KL-Sphere, we derive an implicit characterization of the points attaining the robust rate-distortion function (R-RDF), which we later exploit to implement a novel algorithm for computing the R-RDF. Finally, we characterize the analytical expression of the R-RDF for Bernoulli sources, providing a theoretical benchmark to evaluate the estimation performance of the proposed algorithm. |
| 2025-07-23 | [Integrating Belief Domains into Probabilistic Logic Programs](http://arxiv.org/abs/2507.17291v1) | Damiano Azzolini, Fabrizio Riguzzi et al. | Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications. |
| 2025-07-22 | [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](http://arxiv.org/abs/2507.16806v1) | Mehul Damani, Isha Puri et al. | When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models. |
| 2025-07-22 | [Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2507.16796v1) | Mian Ibad Ali Shah, Enda Barrett et al. | This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities. |
| 2025-07-22 | [Quantum teleportation of an elemental silicon nanophotonic CNOT gate](http://arxiv.org/abs/2507.16783v1) | Kai-Chi Chang, Xiang Cheng et al. | Large-scale quantum computers possess the capacity to effectively tackle practical problems that can be insurmountable for classical computers. The main challenge in building these quantum computers is to realize scalable modules for remote qubits and entanglement. By assembling small, specialized parts into a larger architecture, the modular approach mitigates complexity and uncertainty. Such a distributed architecture requires non-local quantum gate operations between remote qubits. An essential method for implementing such operations, known as quantum gate teleportation, requires only local operations, classical communication, and shared entanglement. Till today, the quantum gate teleportation using a photonic chip has remained elusive. Here we experimentally demonstrate the quantum teleportation of an on-chip controlled-NOT (CNOT) gate, assisted with the scalable silicon chip platform, high-fidelity local quantum logic gates, linear optical components, post-selected entanglement, and coincidence measurements from photonic qubits. First, we measure and characterize our teleported chip-scale CNOT gate with an average truth table fidelity of 93.1 +- 0.3%. Second, for different input polarization states, we obtain an average quantum state fidelity of 87.0 +- 2.2% with our teleported on-chip CNOT gate. Third, we use our non-local CNOT gate for remote entanglement creation of four Bell states, with an average quantum state fidelity of 86.2 +- 0.8%. Fourthly, we fully characterize our teleported on-chip CNOT gate with a quantum process fidelity 83.1 +- 2.0%, and an average non-local CNOT gate fidelity of 86.5 +- 2.2%. Our teleported photonic on-chip quantum logic gate could be extended both to multiple qubits and chip-scale modules towards fault-tolerant and large-scale distributed quantum computation. |
| 2025-07-22 | [A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling](http://arxiv.org/abs/2507.16771v1) | Michael Grosskopf, Kellin Rumsey et al. | The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case. |
| 2025-07-22 | [Planck constraints on the scale dependence of isotropic cosmic birefringence](http://arxiv.org/abs/2507.16714v1) | M. Ballardini, A. Gruppuso et al. | The rotation of the linear polarisation plane of photons during propagation, also known as cosmic birefringence, is a powerful probe of parity-violating extensions of standard electromagnetism. Using Planck legacy data, we confirm previous estimates of the isotropic birefringence angle, finding $\beta \simeq 0.30 \pm 0.05$ [deg] at 68% CL, not including the systematic error from the instrumental polarisation angle. If this is a genuine signal, it could be explained by theories of Chern--Simons-type coupled to electromagnetism, which could lead to a harmonic scale-dependent birefringence signal, if the hypothesis of an ultra-light (pseudo) scalar field does not hold. To investigate these models, we pursue two complementary approaches: first, we fit the birefringence angle estimated at different multipoles, $\beta_{\ell}$, with a power-law model and second, we perform a non-parametric Bayesian reconstruction of it. Both methods yield results consistent with a non-vanishing constant birefringence angle. The first method shows no significant dependence on the harmonic scale (up to $1.8\sigma$ CL), while the second method demonstrates that a constant model is favored by Bayesian evidence. This conclusion is robust across all four published Planck CMB solutions. Finally, we forecast that upcoming CMB observations by Simons Observatory, LiteBIRD and a wishful CMB-Stage 4 experiment could reduce current uncertainties by a factor of approximately 7. |
| 2025-07-22 | [Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains](http://arxiv.org/abs/2507.16670v1) | Amandeep Kaur, Gyan Prakash | Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty. |
| 2025-07-22 | [FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture](http://arxiv.org/abs/2507.16668v1) | Somayeh Sobati-M | Modern smart grids demand fast, intelligent, and energy-aware computing at the edge to manage real time fluctuations and ensure reliable operation. This paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration and Twin based Execution a next-generation fog cloud framework designed to enhance autonomy, resilience, and efficiency in distributed energy systems. FOGNITE combines three core components: federated learning, reinforcement learning, and digital twin validation. Each fog node trains a local CNN LSTM model on private energy consumption data, enabling predictive intelligence while preserving data privacy through federated aggregation. A reinforcement learning agent dynamically schedules tasks based on current system load and energy conditions, optimizing for performance under uncertainty.   To prevent unsafe or inefficient decisions, a hierarchical digital twin layer simulates potential actions before deployment, significantly reducing execution errors and energy waste. We evaluate FOGNITE on a real world testbed of Raspberry Pi devices, showing up to a 93.7% improvement in load balancing accuracy and a 63.2% reduction in energy waste compared to conventional architectures. By shifting smart grid control from reactive correction to proactive optimization, FOGNITE represents a step toward more intelligent, adaptive, and sustainable energy infrastructures |
| 2025-07-22 | [A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market](http://arxiv.org/abs/2507.16655v1) | Mahdi Goldani | In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis. |
| 2025-07-22 | [Bayesian Variational Inference for Mixed Data Mixture Models](http://arxiv.org/abs/2507.16545v1) | Junyang Wang, James Bennett et al. | Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. In addition, we show that the posterior means from CAVI for this model converge to the true parameter value as the sample size n tends to infinity, providing theoretical justification for our method. |
| 2025-07-22 | [A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in Nonlinear Multi-Agent Systems](http://arxiv.org/abs/2507.16520v1) | Aria Delshad, Maryam Babazadeh | This paper proposes a reinforcement learning (RL)-based backstepping control strategy to achieve fixed time consensus in nonlinear multi-agent systems with strict feedback dynamics. Agents exchange only output information with their neighbors over a directed communication graph, without requiring full state measurements or symmetric communication. Achieving fixed time consensus, where convergence occurs within a pre-specified time bound that is independent of initial conditions is faced with significant challenges due to the presence of unknown nonlinearities, inter-agent couplings, and external disturbances. This work addresses these challenges by integrating actor critic reinforcement learning with a novel fixed time adaptation mechanism. Each agent employs an actor critic architecture supported by two estimator networks designed to handle system uncertainties and unknown perturbations. The adaptation laws are developed to ensure that all agents track the leader within a fixed time regardless of their initial conditions. The consensus and tracking errors are guaranteed to converge to a small neighborhood of the origin, with the convergence radius adjustable through control parameters. Simulation results demonstrate the effectiveness of the proposed approach and highlight its advantages over state-of-the-art methods in terms of convergence speed and robustness. |
| 2025-07-21 | [Euclid preparation: Expected constraints on initial conditions](http://arxiv.org/abs/2507.15819v1) | Euclid Collaboration, F. Finelli et al. | The Euclid mission of the European Space Agency will deliver galaxy and cosmic shear surveys, which will be used to constrain initial conditions and statistics of primordial fluctuations. We present highlights for the Euclid scientific capability to test initial conditions beyond LCDM with the main probes, i.e. 3D galaxy clustering from the spectroscopic survey, the tomographic approach to 3x2pt statistics from photometric galaxy survey, and their combination. We provide Fisher forecasts from the combination of Euclid spectroscopic and photometric surveys for spatial curvature, running of the spectral index of the power spectrum of curvature perturbations, isocurvature perturbations, and primordial features. For the parameters of these models we also provide the combination of Euclid forecasts (pessimistic and optimistic) with current and future measurements of the cosmic microwave background (CMB) anisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide Fisher forecasts for how the power spectrum and bispectrum from the Euclid spectroscopic survey will constrain the local, equilateral, and orthogonal shapes of primordial non-Gaussianity. We also review how Bayesian field-level inference of primordial non-Gaussianity can constrain local primordial non-Gaussianity. We show how Euclid, with its unique combination of the main probes, will provide the tightest constraints on low redshift to date. By targeting a markedly different range in redshift and scale, Euclid's expected uncertainties are complementary to those obtained by CMB primary anisotropy, returning the tightest combined constraints on the physics of the early Universe. |
| 2025-07-21 | [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](http://arxiv.org/abs/2507.15803v1) | Danhui Chen, Ziquan Liu et al. | Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in. |
| 2025-07-21 | [Deterministic Quantum Search via Recursive Oracle Expansion](http://arxiv.org/abs/2507.15797v1) | John Burke, Ciaran McGoldrick | We introduce a novel deterministic quantum search algorithm that provides a practical alternative to conventional probabilistic search approaches. Our scheme eliminates the inherent uncertainty of quantum search without relying on arbitrary phase rotations, a key limitation of other deterministic methods. The algorithm achieves certainty by recursively expanding the base oracle so that it marks all states prefixed by the same two bits as the target, encompassing exactly one-quarter of the search space. This enables a step-by-step reduction of the superposition until the target state can be measured with certainty. The algorithm achieves deterministic success with a query complexity of $O(N^{\log_2(3)/2}) \approx O(N^{0.7925})$, falling between Grover's $O(\sqrt{N})$ scaling and the classical $O(N)$. Our approach relies exclusively on two-qubit nearest-neighbour diffusion operators, avoiding global diffusion entirely. We show that, despite the increased query complexity, this design reduces the total number of two-qubit gates required for diffusion by more than an order of magnitude for search spaces up to at least 18 qubits, with even greater advantages on hardware with limited qubit connectivity. The scheme's inherent determinism, reliance on simple nearest-neighbour, low-depth operations, and scalable recursive structure make it well-suited for hardware implementation. Additionally, we show that the algorithm naturally supports partial database search, enabling deterministic identification of selected target bits without requiring a full search, further broadening its applicability. |
| 2025-07-21 | [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](http://arxiv.org/abs/2507.15782v1) | Ruochu Yang, Yu Zhou et al. | Household robots have been a longstanding research topic, but they still lack human-like intelligence, particularly in manipulating open-set objects and navigating large environments efficiently and accurately. To push this boundary, we consider a generalized multi-object collection problem in large scene graphs, where the robot needs to pick up and place multiple objects across multiple locations in a long mission of multiple human commands. This problem is extremely challenging since it requires long-horizon planning in a vast action-state space under high uncertainties. To this end, we propose a novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a multimodal action cost similarity function, our algorithm can both reflect the history and look into the future to optimize plans, striking a good balance of quality and efficiency. Simulation experiments demonstrate that compared with latest works, our algorithm improves the overall mission performance by 30% in terms of fulfilling human commands, maximizing mission success rates, and minimizing mission costs. |
| 2025-07-21 | [Learning Climate Sensitivity from Future Observations, Fast and Slow](http://arxiv.org/abs/2507.15767v1) | Adam Michael Bauer, Cristian Proistosescu et al. | Climate sensitivity has remained stubbornly uncertain since the Charney Report was published some 45 years ago. Two factors in future climate projections could alter this dilemma: (i) an increased ratio of CO$_2$ forcing relative to aerosol cooling, owing to both continued accumulation of CO$_2$ and declining aerosol emissions, and (ii) a warming world, whereby CO$_2$-induced warming becomes more pronounced relative to climate variability. Here, we develop a novel modeling approach to explore the rates of learning about equilibrium climate sensitivity and the transient climate response (TCR) and identify the physical drivers underpinning these learning rates. Our approach has the advantage over past work by accounting for the full spectrum of parameter uncertainties and covariances, while also taking into account serially correlated internal climate variability. Moreover, we provide a physical explanation of how quickly we may hope to learn about climate sensitivity. We find that, although we are able to constrain future TCR regardless of the true underlying value, constraining ECS is more difficult, with low values of ECS being more easily ascertained than high values. This asymmetry can be explained by most of the warming this century being attributable to the fast climate mode, which is more useful for constraining TCR than it is for ECS. We further show that our inability to constrain the deep ocean response is what limits our ability to learn high values of ECS. |
| 2025-07-21 | [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](http://arxiv.org/abs/2507.15741v1) | G√°bor Lugosi, Marcos Matabuena | This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces. Leveraging a newly defined notion of homoscedasticity, we develop a conformal prediction algorithm that offers finite-sample coverage guarantees and fast convergence rates of the oracle estimator. In heteroscedastic settings, we forgo these non-asymptotic guarantees to gain statistical efficiency, proposing a local $k$--nearest--neighbor method without conformal calibration that is adaptive to the geometry of each particular nonlinear space. Both procedures work with any regression algorithm and are scalable to large data sets, allowing practitioners to plug in their preferred models and incorporate domain expertise. We prove consistency for the proposed estimators under minimal conditions. Finally, we demonstrate the practical utility of our approach in personalized--medicine applications involving random response objects such as probability distributions and graph Laplacians. |
| 2025-07-21 | [Symplectic coherence: a measure of position-momentum correlations in quantum states](http://arxiv.org/abs/2507.15738v1) | Varun Upreti, Ulysse Chabaud | The interdependence of position and momentum, as highlighted by the Heisenberg uncertainty principle, is a cornerstone of quantum physics. Yet, position-momentum correlations have received little systematic attention. Motivated by recent developments in bosonic quantum physics that underscore their relevance in quantum thermodynamics, metrology, and computing, we establish a general framework to study and quantify position-momentum correlations in quantum states. We introduce symplectic coherence, a faithful and easily computable measure defined as the Frobenius norm of the block of the covariance matrix encoding position-momentum correlations, and demonstrate that symplectic coherence is monotone under relevant operations and robust under small perturbations. Furthermore, using a recent mapping by Barthe et al. (Phys. Rev. Lett. 134, 070604) which relates the covariance matrix of a bosonic state to the density matrix of a finite-dimensional system, we show that position-momentum correlations correspond to beyond-classical correlations in a virtual finite-dimensional quantum state, with symplectic coherence mapping naturally to geometric quantum discord. Taking energy constraints into account, we determine the maximal position-momentum correlations achievable at fixed energy, revealing structural insights about the corresponding optimal states. Finally, we illustrate the operational relevance of symplectic coherence through several examples in quantum information tasks and quantum thermodynamics. In the process, we establish new technical results on matrix norms and quantum covariance matrices, and demonstrate the conceptual significance of viewing covariance matrices as density matrices of virtual quantum states. |
| 2025-07-21 | [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](http://arxiv.org/abs/2507.15727v1) | Xuchuang Wang, Bo Sun et al. | This paper introduces a novel multi-agent ski-rental problem that generalizes the classical ski-rental dilemma to a group setting where agents incur individual and shared costs. In our model, each agent can either rent at a fixed daily cost, or purchase a pass at an individual cost, with an additional third option of a discounted group pass available to all. We consider scenarios in which agents' active days differ, leading to dynamic states as agents drop out of the decision process. To address this problem from different perspectives, we define three distinct competitive ratios: overall, state-dependent, and individual rational. For each objective, we design and analyze optimal deterministic and randomized policies. Our deterministic policies employ state-aware threshold functions that adapt to the dynamic states, while our randomized policies sample and resample thresholds from tailored state-aware distributions. The analysis reveals that symmetric policies, in which all agents use the same threshold, outperform asymmetric ones. Our results provide competitive ratio upper and lower bounds and extend classical ski-rental insights to multi-agent settings, highlighting both theoretical and practical implications for group decision-making under uncertainty. |
| 2025-07-21 | [Evaluation of hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles using electrochemical permeation and thermal desorption spectroscopy](http://arxiv.org/abs/2507.15711v1) | Nicholas Winzer | Hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles was investigated using electrochemical permeation (EP) and thermal desorption spectroscopy (TDS). The trapping parameters for the test materials were evaluated by fitting the measurements with a finite element model based on the McNabb-Foster equations using least-squares optimisation. The measurements showed that hydrogen diffusion in ferrite is slowed significantly by the presence of fine (<5 nm) (Ti,Cr)C particles; coarser particles had little or no effect. The TDS measurements were consistent with hydrogen traps with a high energy barrier. The uniqueness of the hydrogen trapping parameters obtained using the fitting procedure was evaluated. It was found that the system was overdetermined; the measurements could be fitted with multiple combinations of trapping parameters. Consequently, it was not possible to determine the individual trapping parameters using this procedure. Trapping parameters were also evaluated from TDS measurements by applying Kissinger's equation. Using this procedure a trap binding energy of 0.24 eV was calculated for all materials, albeit with a high degree of uncertainty. |
| 2025-07-21 | [Ubiquity of Uncertainty in Neuron Systems](http://arxiv.org/abs/2507.15702v1) | Brandon B. Le, Bennett Lamb et al. | We demonstrate that final-state uncertainty is ubiquitous in multistable systems of coupled neuronal maps, meaning that predicting whether one such system will eventually be chaotic or nonchaotic is often nearly impossible. We propose a "chance synchronization" mechanism that governs the emergence of unpredictability in neuron systems and support it by using basin classification, uncertainty exponent, and basin entropy techniques to analyze five simple discrete-time systems, each consisting of a different neuron model. Our results illustrate that uncertainty in neuron systems is not just a product of noise or high-dimensional complexity; it is also a fundamental property of low-dimensional, deterministic models, which has profound implications for understanding brain function, modeling cognition, and interpreting unpredictability in general multistable systems. |
| 2025-07-18 | [Missing baryons recovered: a measurement of the gas fraction in galaxies and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing](http://arxiv.org/abs/2507.14136v1) | Boryana Hadzhiyska, Simone Ferraro et al. | We present new constraints on the halo masses and matter density profiles of DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs) and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing convergence map from ACT DR6. This provides an independent, lensing-based calibration of halo masses, complementary to methods relying on clustering or dynamics. We derive constraints on the mean halo mass for three DESI-selected samples, finding $\log(M_{\rm halo}/(M_\odot/h)) \approx 13.18$, 13.03 and 13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a halo model approach, we also compare the projected galaxy-matter density profiles with previously reported gas profiles inferred from measurements of the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the key uncertainties in interpreting kSZ signals -- the unknown host halo mass distribution -- by providing an independent and consistent mass calibration. The agreement between the gas and total mass profiles at large aperture suggests that sufficiently far from the group center (2--3 virial radii), we recover all the baryons, offering a resolution to the 'missing baryon' problem. We further study the cumulative gas fractions for all galaxies as well as for the most massive galaxy groups in the sample ($\log(M_{\rm halo}/(M_\odot/h)) \approx 13.5$), finding values that are physically sensible and in agreement with previous findings using kSZ and X-ray data: compared to the TNG300 simulation, the observed gas fractions are systematically lower at fixed radius by $\gtrsim$4$\sigma$, providing compelling, independent evidence for stronger baryonic feedback in the real Universe. These findings highlight the power of combining CMB lensing with galaxy surveys to probe the interplay between baryons and dark matter in group-sized halos. |
| 2025-07-18 | [On the relation between perspective-neutral, algebraic, and effective quantum reference frames](http://arxiv.org/abs/2507.14131v1) | Philipp A. Hoehn, Julian De Vuyst et al. | The framework of internal quantum reference frames (QRFs) constitutes a universal toolset for dealing with symmetries in quantum theory and has led to new revelations in quantum gravity, gauge theories and foundational physics. Multiple approaches have emerged, sometimes differing in scope and the way symmetries are implemented, raising the question as to their relation. Here, we investigate the relation between three approaches to QRFs for gauge symmetries, namely the effective semiclassical, algebraic, and perspective-neutral (PN) approaches. Rather than constructing Hilbert spaces, as the PN approach, the effective approach is based on a quantum phase space parametrized by expectation values and fluctuations, while the emphasis of the algebraic approach is on the state space of complex linear functionals on a kinematical algebra. Nevertheless, external frame information is treated as gauge in all three formalisms, manifested in constraints on states and algebra. We show that these three approaches are, in fact, equivalent for ideal QRFs, distinguished by sharp orientations, which is the previous setting of the first two approaches. Our demonstration pertains to single constraints, including relativistic ones, and encompasses QRF changes. In particular, the QRF transformations of the PN framework agree semiclassically with those of the older effective approach, by which it was inspired. As a physical application, we explore the QRF covariance of uncertainties and fluctuations, which turn out to be frame-dependent. This is particularly well-suited for the effective and algebraic approaches, for which these quantities form a natural basis. Finally, we pave the way towards extending these two approaches to non-ideal QRFs by studying the projection and gauge-fixing operations of the Page-Wootters formalism, built into the PN framework, on algebraic states. |
| 2025-07-18 | [Integrating Forecasting Models Within Steady-State Analysis and Optimization](http://arxiv.org/abs/2507.14117v1) | Aayushya Agarwal, Larry Pileggi | Extreme weather variations and the increasing unpredictability of load behavior make it difficult to determine power grid dispatches that are robust to uncertainties. While machine learning (ML) methods have improved the ability to model uncertainty caused by loads and renewables, accurately integrating these forecasts and their sensitivities into steady-state analyses and decision-making strategies remains an open challenge. Toward this goal, we present a generalized methodology that seamlessly embeds ML-based forecasting engines within physics-based power flow and grid optimization tools. By coupling physics-based grid modeling with black-box ML methods, we accurately capture the behavior and sensitivity of loads and weather events by directly integrating the inputs and outputs of trained ML forecasting models into the numerical methods of power flow and grid optimization. Without fitting surrogate load models, our approach obtains the sensitivities directly from data to accurately predict the response of forecasted devices to changes in the grid. Our approach combines the sensitivities of forecasted devices attained via backpropagation and the sensitivities of physics-defined grid devices. We demonstrate the efficacy of our method by showcasing improvements in sensitivity calculations and leveraging them to design a robust power dispatch that improves grid reliability under stochastic weather events. Our approach enables the computation of system sensitivities to exogenous factors which supports broader analyses that improve grid reliability in the presence of load variability and extreme weather conditions. |
| 2025-07-18 | [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](http://arxiv.org/abs/2507.14102v1) | Shravan Venkatraman, Pavan Kumar S et al. | Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL |
| 2025-07-18 | [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](http://arxiv.org/abs/2507.14099v1) | Markus Buchholz, Ignacio Carlucho et al. | Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges. |
| 2025-07-18 | [Global Bayesian Analysis of $\mathrm{J}/œà$ Photoproduction on Proton and Lead Targets](http://arxiv.org/abs/2507.14087v1) | Heikki M√§ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions using a color glass condensate (CGC) based calculation framework. As past calculations have shown that CGC-based models typically overpredict the $\mathrm{J}/\psi$ production in $\gamma+\mathrm{Pb}$ collisions at high center of mass energy, we address the question of whether it is possible to describe coherent and incoherent diffractive $\mathrm{J}/\psi$ data from $\gamma+p$ collisions at HERA and the LHC, and from $\gamma+\mathrm{Pb}$ collisions at the LHC simultaneously. Our results indicate that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ data is challenging, with results improving when an overall $K$-factor -- scaling $\gamma+p$ and $\gamma+\mathrm{Pb}$ cross sections to absorb model uncertainties -- is introduced. |
| 2025-07-18 | [Direct Measurement of the Accretion Disk Formed in Prompt Collapse Mergers with Future Gravitational-Wave Observatories](http://arxiv.org/abs/2507.14071v1) | Arnab Dhani, Alessandro Camilletti et al. | The production site of heavy r-process elements, such as Gold and Uranium, is uncertain. Neutron star mergers are the only astrophysical phenomenon in which we have witnessed their formation. However, the amount of heavy elements resulting from the merger remains poorly constrained, mainly due to uncertainties on the mass and angular momentum of the disk formed in the merger remnant. Matter accretion from the disk is also thought to power gamma ray-bursts. We discover from numerical relativity simulations that the accretion disk influences the ringdown gravitational-wave signal produced by binaries that promptly collapse to black-hole at merger. We propose a method to \emph{directly} measure the mass of the accretion disk left during black hole formation in binary mergers using observatories such as the Einstein Telescope or Cosmic Explorer with a relative error of 10\% for binaries at a distance of up to 30~Mpc, corresponding to an event rate of 0.001 to 0.25 events per year. |
| 2025-07-18 | [VLA-Mark: A cross modal watermark for large vision-language alignment model](http://arxiv.org/abs/2507.14067v1) | Shuliang Liu, Qi Zheng et al. | Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking |
| 2025-07-18 | [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](http://arxiv.org/abs/2507.14056v1) | Alejandro Rodriguez-Garcia, Anindya Ghosh et al. | Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks. |
| 2025-07-18 | [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](http://arxiv.org/abs/2507.14034v1) | Jochen Wulf, Jurg Meierhofer et al. | Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems. |
| 2025-07-17 | [$\texttt{raccoon}$: A Python package for removing wiggle artifacts in the JWST NIRSpec integral field spectroscopy](http://arxiv.org/abs/2507.13341v1) | Anowar J. Shajib | $\texttt{raccoon}$ is a Python package for removing resampling noise - commonly referred to as "wiggles'' - from spaxel-level spectra in datacubes obtained from the JWST Near Infrared Spectrograph's (NIRSpec) integral field spectroscopy (IFS) mode. These wiggles arise as artifacts during resampling of the 2D raw data into 3D datacubes, due to the point spread function (PSF) being undersampled. The standard JWST data reduction pipeline does not correct for this noise. The wiggle artifacts can significantly degrade the scientific usability of the data, particularly at the spaxel level, undermining the exquisite spatial resolution of NIRSpec. $\texttt{raccoon}$ provides an empirical correction by modeling and removing these artifacts, thereby restoring the fidelity of the extracted spectra. $\texttt{raccoon}$ forward-models the wiggles as a chirp function impacting one or more template spectra that are directly fit to the original data across the entire wavelength range. The best-fit wiggle model is then used to clean the data while propagating the associated uncertainties. |
| 2025-07-17 | [A Framework for Waterfall Pricing Using Simulation-Based Uncertainty Modeling](http://arxiv.org/abs/2507.13324v1) | Nicola Jean, Giacomo Le Pera et al. | We present a novel framework for pricing waterfall structures by simulating the uncertainty of the cashflow generated by the underlying assets in terms of value, time, and confidence levels. Our approach incorporates various probability distributions calibrated on the market price of the tranches at inception. The framework is fully implemented in PyTorch, leveraging its computational efficiency and automatic differentiation capabilities through Adjoint Algorithmic Differentiation (AAD). This enables efficient gradient computation for risk sensitivity analysis and optimization. The proposed methodology provides a flexible and scalable solution for pricing complex structured finance instruments under uncertainty |
| 2025-07-17 | [Testing halo models for constraining astrophysical feedback with multi-probe modeling: I. 3D Power spectra and mass fractions](http://arxiv.org/abs/2507.13317v1) | Pranjal R. S., Shivam Pandey et al. | Upcoming Stage-IV surveys will deliver measurements of distribution of matter with unprecedented precision, demanding highly accurate theoretical models for cosmological parameter inference. A major source of modeling uncertainty lies in astrophysical processes associated with galaxy formation and evolution, which remain poorly understood. Probes such as the thermal and kinematic Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio bursts offer a promising avenue for mapping the distribution and thermal properties of cosmic baryons. A unified analytical framework capable of jointly modeling these observables is essential for fully harnessing the complementary information while mitigating probe-specific systematics. In this work, we present a detailed assessment of existing analytical models, which differ in their assumptions and prescriptions for simultaneously describing the distribution of matter and baryons in the universe. Using the Magneticum hydrodynamical simulation, we test these models by jointly analyzing the 3D auto- and cross-power spectra of the matter and baryonic fields that underpin the above probes. We find that all models can reproduce the power spectra at sub-percent to few-percent accuracy, depending on the tracer combination and number of free parameters. Their ability to recover underlying halo properties, such as the evolution of gas abundance and thermodynamic profiles with halo mass, varies considerably. Our results suggest that these models require further refinement and testing for reliable interpretation of multi-wavelength datasets. |
| 2025-07-17 | [Systematic study of the validity of the eikonal model including uncertainties](http://arxiv.org/abs/2507.13291v1) | Daniel Shiu, Chlo√´ Hebborn et al. | Nuclear reactions at intermediate beam energies are often interpreted using the eikonal model. In the analysis of complex reaction probes, where few-body reaction methods are needed, the eikonal method may be used as an efficient way for describing the fragment-target reaction process. In this work, we perform a systematic study to test the validity of the eikonal approximation for nucleon-nucleus reactions. We also quantify uncertainties due to the nucleon optical potential on reaction observables. We inspect the validity of the eikonal model and its semiclassical correction by comparing it to exact solutions (obtained from solving the optical model equation with a finite differences method) for a wide range of reactions. We also study the effect of relativistic corrections, both kinematic and dynamic, by effectively incorporating the relativistic effects at intermediate energies. The uncertainties from a Bayesian global optical potential (KDUQ) are propagated to the observables of interest. Our study includes neutron and proton reactions on $^{27}$Al, $^{40}$Ca, $^{90}$Zr and $^{208}$Pb, for a wide range of energies $E_{lab}=0-400$ MeV. Our results show that for the proton absorption cross section, the eikonal model can be used down to around $60$ MeV and the semiclassical correction extends its use to $30$ MeV. However, the validity of the eikonal model for the neutron total cross section only goes down to $\approx120$ MeV, a range extended to $\approx 50$ MeV when using the semiclassical correction. We find the semi-classical correction to the eikonal model to be less effective in describing the angular distributions. The $1\sigma$ uncertainty intervals on the observables we studied is less than $5$% for most of the energies considered, but increases rapidly for higher energies, namely energies outside the range of KDUQ ($E_{lab}>200$ MeV). |
| 2025-07-17 | [Multi-wavelength study of the high Galactic latitude supernova remnant candidate \snr\ associated with the Calvera pulsar](http://arxiv.org/abs/2507.13210v1) | Emanuele Greco, Michela Rigoselli et al. | The candidate supernova remnant (SNR) G118.4+37.0 (Calvera's SNR), discovered as a faint radio ring at high Galactic latitude and coincident with extended Fermi/LAT gamma-ray emission, is likely associated to the X-ray pulsar 1RXS J141256.0+792204 (Calvera). Previous XMM-Newton data hinted at soft diffuse X-ray emission inside the ring but lacked sufficient exposure for detailed characterisation. We obtained new XMM-Newton observations, and produced count-rate images, equivalent width and median photon energy maps to identify optimal regions for spectral analysis. We complemented these observations with a reanalysis of Fermi/LAT gamma-ray data and new Telescopio Nazionale Galileo observations aimed to search for Halpha emission. The X-ray diffuse emission is well described by a model of shock-heated plasma with temperature kT \sim 0.15 keV, mildly under-solar N and o abundances and densities ne=0.1-0.7 cm-3. According to our estimates, Calvera's SNR is 10-20 kya old and lies at a distance of 4-5 kpc. A distinti "Clump" region shows hared emission equally well described by a thermal (kT\sim 1.7 keV) or a non thermal model (Gamma \sim 2.7). The brightest X-ray area is close to the gamma-ray peak and to an isolated Alpha filament. G118.4+37.0 is a middle-aged remnant which expands in a tenuous medium and encountered a denser phase, likely the relic of the wind activity of the massive progenitor star. The estimated SNR distance is consistent within the uncertainties with that estimated for Calvera, confirming that this peculiar pulsar was born in the explosion of a massive star high above the Galactic disk. Our measured ambient density, together with the patchy morphology of the gamma-ray emission and the detection of Halpha filaments indicates that a hadronic origin is compatible with the gamma-ray flux, though a mixed leptonic-hadronic cannot be excluded |
| 2025-07-17 | [Performance Portable Gradient Computations Using Source Transformation](http://arxiv.org/abs/2507.13204v1) | Kim Liegeois, Brian Kelley et al. | Derivative computation is a key component of optimization, sensitivity analysis, uncertainty quantification, and nonlinear solvers. Automatic differentiation (AD) is a powerful technique for evaluating such derivatives, and in recent years, has been integrated into programming environments such as Jax, PyTorch, and TensorFlow to support derivative computations needed for training of machine learning models, resulting in widespread use of these technologies. The C++ language has become the de facto standard for scientific computing due to numerous factors, yet language complexity has made the adoption of AD technologies for C++ difficult, hampering the incorporation of powerful differentiable programming approaches into C++ scientific simulations. This is exacerbated by the increasing emergence of architectures such as GPUs, which have limited memory capabilities and require massive thread-level concurrency. Portable scientific codes rely on domain specific programming models such as Kokkos making AD for such codes even more complex. In this paper, we will investigate source transformation-based automatic differentiation using Clad to automatically generate portable and efficient gradient computations of Kokkos-based code. We discuss the modifications of Clad required to differentiate Kokkos abstractions. We will illustrate the feasibility of our proposed strategy by comparing the wall-clock time of the generated gradient code with the wall-clock time of the input function on different cutting edge GPU architectures such as NVIDIA H100, AMD MI250x, and Intel Ponte Vecchio GPU. For these three architectures and for the considered example, evaluating up to 10 000 entries of the gradient only took up to 2.17x the wall-clock time of evaluating the input function. |
| 2025-07-17 | [Medium-induced modification of azimuthal correlations of electrons from heavy-flavor hadron decays with charged particles in Pb-Pb collisions at $\sqrt{s_{\rm{NN}} = 5.02}$ TeV](http://arxiv.org/abs/2507.13197v1) | ALICE Collaboration | The azimuthal-correlation distributions between electrons from the decays of heavy-flavor hadrons and associated charged particles in Pb-Pb collisions at $\sqrt{s_{\rm NN}} = 5.02$ TeV are reported for the 0-10% and 30-50% centrality classes. This is the first measurement to provide access to the azimuthal-correlation observables in the heavy-flavor sector in Pb-Pb collisions. The analysis is performed for trigger electrons from heavy-flavor hadron decays with transverse momentum $4 < p_{\rm T}^{\rm e} < 16$ GeV/$c$, considering associated particles within the transverse-momentum range $1 < p_{\rm T}^{\rm assoc} < 7$ GeV/$c$, and a pseudorapidity difference of $|\Delta\eta| < 1$ between the trigger electron and associated particles. The per-trigger nuclear modification factor ($I_{\rm AA}$) is calculated to compare the near- and away-side peak yields to those in pp collisions at $\sqrt{s} = 5.02$ TeV. In 0-10% central collisions, the $I_{\rm AA}$ indicates a hint of enhancement of associated-particle yields with $p_{\rm T} < 3$ GeV/$c$ on the near side, and a suppression of yields with $p_{\rm T} > 4$ GeV/$c$ on the away side. The $I_{\rm AA}$ for electron triggers from heavy-flavor hadron decays is compared with that for light-flavor and strange-particle triggers to investigate the dependence on different fragmentation processes and parton-medium dynamics, and is found to be the same within uncertainties. |
| 2025-07-17 | [$\overlineŒ£^{\pm}$ production in pp and p-Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02 TeV with ALICE](http://arxiv.org/abs/2507.13183v1) | ALICE Collaboration | The transverse momentum spectra and integrated yields of $\overline{\Sigma}^{\pm}$ have been measured in pp and p-Pb collisions at $\sqrt{s_{\mathrm{NN}}} = 5.02$ TeV with the ALICE experiment. Measurements are performed via the newly accessed decay channel $\overline{\Sigma}^{\pm} \rightarrow {\rm\overline{n}}\pi^{\pm}$. A new method of antineutron reconstruction with the PHOS electromagnetic spectrometer is developed and applied to this analysis. The $p_{\rm T}$ spectra of $\overline{\Sigma}^{\pm}$ are measured in the range $0.5 < p_{\rm T} < 3$ GeV/$c$ and compared to predictions of the PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4 models. The EPOS LHC and EPOS4 models provide the best descriptions of the measured spectra both in pp and p-Pb collisions, while models which do not account for multiparton interactions provide a considerably worse description at high $p_{\rm T}$. The total yields of $\overline{\Sigma}^{\pm}$ in both pp and p-Pb collisions are compared to predictions of the Thermal-FIST model and dynamical models PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4. All models reproduce the total yields in both colliding systems within uncertainties. The nuclear modification factors $R_{\rm pPb}$ for both $\overline{\Sigma}^{+}$ and $\overline{\Sigma}^{-}$ are evaluated and compared to those of protons, $\Lambda$ and $\Xi$ hyperons, and predictions of EPOS LHC and EPOS4 models. No deviations of $R_{\rm pPb}$ for $\overline{\Sigma}^{\pm}$ from the model predictions or measurements for other hadrons are found within uncertainties. |
| 2025-07-17 | [The Time-Energy Principle in Algebraic Geometry](http://arxiv.org/abs/2507.13134v1) | Renaud Gauthier | We consider the time-energy uncertainty principle from Quantum Mechanics and provide its Algebro-Geometric interpretation within the context of stacks. |
| 2025-07-17 | [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](http://arxiv.org/abs/2507.13092v1) | Hyo-Jeong Jang, Hye-Bin Shin et al. | Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications. |
| 2025-07-16 | [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](http://arxiv.org/abs/2507.12461v1) | Trong-Thang Pham, Anh Nguyen et al. | Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets. |
| 2025-07-16 | [Precision measurement of the ${\itŒû}_b^0$ baryon lifetime](http://arxiv.org/abs/2507.12402v1) | LHCb collaboration, R. Aaij et al. | A sample of $pp$ collision data, corresponding to an integrated luminosity of 5.4 fb$^{-1}$ and collected by the LHCb experiment during LHC Run 2, is used to measure the ratio of the lifetime of the ${\it{\Xi}}_b^0$ baryon to that of the ${\it{\Lambda}}_b^0$ baryon, $r_{\tau}\equiv\tau_{{\it{\Xi}}_b^0}/\tau_{{\it{\Lambda}}_b^0}$. The value ${r_{\tau}^{\rm Run\,2}=1.004\pm0.009\pm0.006}$ is obtained, where the first uncertainty is statistical and the second systematic. This value is averaged with the corresponding value from Run 1 to obtain ${r_{\tau} = 1.004\pm0.008\pm0.005}$. Multiplying by the known value of the ${\it{\Lambda}}_b^0$ lifetime yields ${{\tau_{{\it{\Xi}}_b^0}} = 1.475\pm0.012\pm0.008\pm0.009~{\rm ps}}$, where the last uncertainty is due to the limited knowledge of the ${\it{\Lambda}}_b^0$ lifetime. This measurement improves the precision of the current world average of the ${\it{\Xi}}_b^0$ lifetime by about a factor of two, and is in good agreement with the most recent theoretical predictions. |
| 2025-07-16 | [Surrogate modeling for uncertainty quantification in nonlinear dynamics](http://arxiv.org/abs/2507.12358v1) | S. Marelli, S. Sch√§r et al. | Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use. |
| 2025-07-16 | [The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models](http://arxiv.org/abs/2507.12335v1) | Morgan Deal, Ga√´l Buldgen et al. | The transport of chemical elements in stellar interiors is one of the greatest sources of uncertainties of solar and stellar modelling. The Sun, with its exquisite spectroscopic, helioseismic and neutrino observations, offers a prime environment to test the prescriptions used for both microscopic and macroscopic transport processes. We study in detail the impact of various formalisms for atomic diffusion on helioseismic constraints in both CLES (Scuflaire et al., 2008a) and Cesam2k2 (Morel and Lebreton 2008; Marques et al. 2013; Deal et al. 2018) models and compare both codes in detail. Moreover, due to the inability of standard models using microscopic diffusion to reproduce light element depletion in the Sun (Li, Be), another efficient process must be included to reproduce these constraints (rotation-induced: Eggenberger et al. 2022, overshooting -- or penetrative convection -- below the convective envelope: Th\'evenin et al. 2017, or ad hoc turbulence: Lebreton and Maeder 1987; Richer, Michaud, and Turcotte 2000). However, introducing such an extra mixing leads to issues with the CNO neutrino fluxes (see Buldgen et al. 2023), which seem to be systematically lower than the Borexino observations (Appel et al., 2022. Another key aspect to consider when reconciling models with neutrino fluxes is the impact of electronic screening (Mussack and D\"appen, 2011). |
| 2025-07-16 | [Uncertainty and entropies of classical channels](http://arxiv.org/abs/2507.12310v1) | Takla Nateeboon | In this thesis, I studied a mathematical development to define and quantify the uncertainty inherent in classical channels. This thesis starts with the introduction and background on how to formally think about uncertainty in the domain of classical states. The concept of probability vector majorization and its variants, relative majorization and conditional majorization, are reviewed. This thesis introduces three conceptually distinct approaches to formalize the notion of uncertainty inherent in classical channels. These three approaches define the same preordering on the domain of classical channels, leading to characterizations from many perspectives. With the solid foundation of uncertainty comparison, classical channel entropy is then defined to be an additive monotone with respect to the majorization relation. The well-known entropies in the domain of classical states are uniquely extended to the domain of channels via the optimal extensions, providing not only a solid foundation but also the quantifiers of uncertainty inherent in classical channels. |
| 2025-07-16 | [Forecasting Climate Policy Uncertainty: Evidence from the United States](http://arxiv.org/abs/2507.12276v1) | Donia Besher, Anirban Sengupta et al. | Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers strive to balance economic growth with environmental goals. High levels of CPU can slow down investments in green technologies, make regulatory planning more difficult, and increase public resistance to climate reforms, especially during times of economic stress. This study addresses the challenge of forecasting the US CPU index by building the Bayesian Structural Time Series (BSTS) model with a large set of covariates, including economic indicators, financial cycle data, and public sentiments captured through Google Trends. The key strength of the BSTS model lies in its ability to efficiently manage a large number of covariates through its dynamic feature selection mechanism based on the spike-and-slab prior. To validate the effectiveness of the selected features of the BSTS model, an impulse response analysis is performed. The results show that macro-financial shocks impact CPU in different ways over time. Numerical experiments are performed to evaluate the performance of the BSTS model with exogenous variables on the US CPU dataset over different forecasting horizons. The empirical results confirm that BSTS consistently outperforms classical and deep learning frameworks, particularly for semi-long-term and long-term forecasts. |
| 2025-07-16 | [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](http://arxiv.org/abs/2507.12262v1) | Zachary James, Joseph Guinness | Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset. |
| 2025-07-16 | [What are we talking about when we discuss the Born-Oppenheimer approximation?](http://arxiv.org/abs/2507.12223v1) | Olimpia Lombardi, Sebastian Fortin et al. | Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a comprehensive article examining the Born-Oppenheimer Approximation (BOA). Their central objective is to challenge our position on the matter-namely, that the BOA incorporates a classical assumption incompatible with the Heisenberg Uncertainty Principle. In contrast, HLT contend that the BOA involves no such classical assumption and, as a result, supports the view that chemistry can be reduced to physics. The purpose of this paper is to offer a critical analysis of the HLT article and to clarify why we consider their arguments unpersuasive. |
| 2025-07-16 | [Explainable Evidential Clustering](http://arxiv.org/abs/2507.12192v1) | Victor F. Lopes de Souza, Karima Bakhti et al. | Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of "tolerable" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time. |
| 2025-07-16 | [Learning, fast and slow: a two-fold algorithm for data-based model adaptation](http://arxiv.org/abs/2507.12187v1) | Laura Boca de Giuli, Alessio La Bella et al. | This article addresses the challenge of adapting data-based models over time. We propose a novel two-fold modelling architecture designed to correct plant-model mismatch caused by two types of uncertainty. Out-of-domain uncertainty arises when the system operates under conditions not represented in the initial training dataset, while in-domain uncertainty results from real-world variability and flaws in the model structure or training process. To handle out-of-domain uncertainty, a slow learning component, inspired by the human brain's slow thinking process, learns system dynamics under unexplored operating conditions, and it is activated only when a monitoring strategy deems it necessary. This component consists of an ensemble of models, featuring (i) a combination rule that weights individual models based on the statistical proximity between their training data and the current operating condition, and (ii) a monitoring algorithm based on statistical control charts that supervises the ensemble's reliability and triggers the offline training and integration of a new model when a new operating condition is detected. To address in-domain uncertainty, a fast learning component, inspired by the human brain's fast thinking process, continuously compensates in real time for the mismatch of the slow learning model. This component is implemented as a Gaussian process (GP) model, trained online at each iteration using recent data while discarding older samples. The proposed methodology is tested on a benchmark energy system referenced in the literature, demonstrating that the combined use of slow and fast learning components improves model accuracy compared to standard adaptation approaches. |
| 2025-07-15 | [Canonical Bayesian Linear System Identification](http://arxiv.org/abs/2507.11535v1) | Andrey Bryutkin, Matthew E. Levine et al. | Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data. |
| 2025-07-15 | [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](http://arxiv.org/abs/2507.11488v1) | Pakizar Shamoi, Nuray Toganas et al. | Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical. |
| 2025-07-15 | [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](http://arxiv.org/abs/2507.11420v1) | Mingcong Li | Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies. |
| 2025-07-15 | [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](http://arxiv.org/abs/2507.11413v1) | Christos N. Efrem, Ioannis Krikidis | We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average. |
| 2025-07-15 | [Towards NNLO QCD predictions for off-shell top-quark pair production and decays](http://arxiv.org/abs/2507.11410v1) | Luca Buonocore, Massimiliano Grazzini et al. | We consider QCD radiative corrections to $W^+W^-b {\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\Gamma_t\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\%$ level $\unicode{x2013}$ significantly smaller than the $5\%$ residual perturbative uncertainties. |
| 2025-07-15 | [Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions](http://arxiv.org/abs/2507.11394v1) | Syed Afrid Jahan, Hendrik Roch et al. | We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\sqrt{s_\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions. |
| 2025-07-15 | [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](http://arxiv.org/abs/2507.11385v1) | George D. Pasparakis, Ioannis A. Kougioumtzoglou et al. | A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics. |
| 2025-07-15 | [The miniJPAS survey quasar selection V: combined algorithm](http://arxiv.org/abs/2507.11380v1) | Ignasi P√©rez-R√†fols, L. Raul Abramo et al. | Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\sigma_{\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\sigma_{\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration. |
| 2025-07-15 | [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](http://arxiv.org/abs/2507.11357v1) | Emile van Krieken, Pasquale Minervini et al. | The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons. |
| 2025-07-15 | [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](http://arxiv.org/abs/2507.11352v1) | Yunhao Yang, Neel P. Bhatt et al. | Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics. |
| 2025-07-14 | [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](http://arxiv.org/abs/2507.10510v1) | Jiangkai Wu, Zhiyuan Ren et al. | AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. |
| 2025-07-14 | [Referential ambiguity and clarification requests: comparing human and LLM behaviour](http://arxiv.org/abs/2507.10445v1) | Chris Madge, Matthew Purver et al. | In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy. |
| 2025-07-14 | [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](http://arxiv.org/abs/2507.10425v1) | Alvaro H. C. Correia, Christos Louizos | Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift. |
| 2025-07-14 | [Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions](http://arxiv.org/abs/2507.10377v1) | T. Dumont, A. Bonhomme et al. | The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\alpha},{\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\alpha},{\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\alpha},{\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition. |
| 2025-07-14 | [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](http://arxiv.org/abs/2507.10355v1) | Bo Jiang, Xueyang Ze et al. | Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach. |
| 2025-07-14 | [Gaussian Process Methods for Very Large Astrometric Data Sets](http://arxiv.org/abs/2507.10317v1) | Timothy Hapitas, Lawrence M. Widrow et al. | We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics. |
| 2025-07-14 | [How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models](http://arxiv.org/abs/2507.10314v1) | Nina Mackensen, Nicole Reindl et al. | PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\text{RX J}0122.9\text{ -}7521$. We find $T_\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\text{RX J}0122.9\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\text{RX J}0122.9\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\text{RX J}0122.9\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\text{RX J}0122.9\text{ -}7521$. |
| 2025-07-14 | [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](http://arxiv.org/abs/2507.10291v1) | Kristine V. Ung, Connor A. Roncaioli et al. | The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\mathrm{^{1}E}$ singlet state in the $\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\mathrm{^{1}E}$ into the neutral $\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited. |
| 2025-07-14 | [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](http://arxiv.org/abs/2507.10201v1) | Gleb Shishaev, Vasily Demyanov et al. | The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure. |
| 2025-07-14 | [Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture](http://arxiv.org/abs/2507.10166v1) | Filip Surmaa, Anahita Jamshidnejad | This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems. |
| 2025-03-10 | [Controllable 3D Outdoor Scene Generation via Scene Graphs](http://arxiv.org/abs/2503.07152v1) | Yuheng Liu, Xinke Li et al. | Three-dimensional scene generation is crucial in computer vision, with applications spanning autonomous driving, gaming and the metaverse. Current methods either lack user control or rely on imprecise, non-intuitive conditions. In this work, we propose a method that uses, scene graphs, an accessible, user friendly control format to generate outdoor 3D scenes. We develop an interactive system that transforms a sparse scene graph into a dense BEV (Bird's Eye View) Embedding Map, which guides a conditional diffusion model to generate 3D scenes that match the scene graph description. During inference, users can easily create or modify scene graphs to generate large-scale outdoor scenes. We create a large-scale dataset with paired scene graphs and 3D semantic scenes to train the BEV embedding and diffusion models. Experimental results show that our approach consistently produces high-quality 3D urban scenes closely aligned with the input scene graphs. To the best of our knowledge, this is the first approach to generate 3D outdoor scenes conditioned on scene graphs. |
| 2024-09-18 | [Generation of Uncertainty-Aware Emergent Concepts in Factorized 3D Scene Graphs via Graph Neural Networks](http://arxiv.org/abs/2409.11972v2) | Jose Andres Millan-Romera, Muhammad Shaheer et al. | Enabling robots to autonomously discover emergent spatial concepts (e.g., rooms) from primitive geometric observations (e.g., planar surfaces) within 3D Scene Graphs is essential for robust indoor navigation and mapping. These graphs provide a hierarchical metric-semantic representation in which such concepts are organized. To further enhance graph-SLAM performance, Factorized 3D Scene Graphs incorporate these concepts as optimization factors that constrain relative geometry and enforce global consistency. However, both stages of this process remain largely manual: concepts are typically derived using hand-crafted, concept-specific heuristics, while factors and their covariances are likewise manually designed. This reliance on manual specification limits generalization across diverse environments and scalability to new concept classes. This paper presents, for the first time, a learning-based method to generate online spatial emergent concepts as optimizable factors within a SLAM backend, reducing the need to handcraft both concept generation and the definition of their corresponding factors and covariances. In both simulated and real indoor scenarios, our approach improves complex concept detection by 20.7% and 5.3%, trajectory estimation by 19.2%, and map reconstruction by 12.3% and 3.8%, respectively, highlighting the benefits of this integration for robust and adaptive spatial understanding. |
| 2023-12-15 | [nuScenes Knowledge Graph -- A comprehensive semantic representation of traffic scenes for trajectory prediction](http://arxiv.org/abs/2312.09676v1) | Leon Mlodzian, Zhigang Sun et al. | Trajectory prediction in traffic scenes involves accurately forecasting the behaviour of surrounding vehicles. To achieve this objective it is crucial to consider contextual information, including the driving path of vehicles, road topology, lane dividers, and traffic rules. Although studies demonstrated the potential of leveraging heterogeneous context for improving trajectory prediction, state-of-the-art deep learning approaches still rely on a limited subset of this information. This is mainly due to the limited availability of comprehensive representations. This paper presents an approach that utilizes knowledge graphs to model the diverse entities and their semantic connections within traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a knowledge graph for the nuScenes dataset, that models explicitly all scene participants and road elements, as well as their semantic and spatial relationships. To facilitate the usage of the nSKG via graph neural networks for trajectory prediction, we provide the data in a format, ready-to-use by the PyG library. All artefacts can be found here: https://github.com/boschresearch/nuScenes_Knowledge_Graph |
| 2023-11-28 | [Panoptic Video Scene Graph Generation](http://arxiv.org/abs/2311.17058v1) | Jingkang Yang, Wenxuan Peng et al. | Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG relates to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects grounded with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG to miss key details crucial for comprehensive video understanding. In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding. To advance research in this new area, we contribute the PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with a total of 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs. We also provide a variety of baseline methods and share useful design practices for future work. |
| 2023-08-10 | [Informative Scene Graph Generation via Debiasing](http://arxiv.org/abs/2308.05286v2) | Lianli Gao, Xinyu Lyu et al. | Scene graph generation aims to detect visual relationship triplets, (subject, predicate, object). Due to biases in data, current models tend to predict common predicates, e.g. "on" and "at", instead of informative ones, e.g. "standing on" and "looking at". This tendency results in the loss of precise information and overall performance. If a model only uses "stone on road" rather than "stone blocking road" to describe an image, it may be a grave misunderstanding. We argue that this phenomenon is caused by two imbalances: semantic space level imbalance and training sample level imbalance. For this problem, we propose DB-SGG, an effective framework based on debiasing but not the conventional distribution fitting. It integrates two components: Semantic Debiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD utilizes a confusion matrix and a bipartite graph to construct predicate relationships. BPL adopts a random undersampling strategy and an ambiguity removing strategy to focus on informative predicates. Benefiting from the model-agnostic process, our method can be easily applied to SGG models and outperforms Transformer by 136.3%, 119.5%, and 122.6% on mR@20 at three SGG sub-tasks on the SGG-VG dataset. Our method is further verified on another complex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph retrieval and image captioning). |
| 2023-08-09 | [Generalized Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.04802v2) | Xinyu Lyu, Lianli Gao et al. | Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. Hence, we introduce a novel research problem: Generalized Unbiased Scene Graph Generation (G-USGG), which takes into account both predicate-level and concept-level imbalance. To the end, we propose the Multi-Concept Learning (MCL) framework, which ensures a balanced learning process across rare/ uncommon/ common concepts. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. It then effectively learns concept-prototypes by applying the Concept Regularization (CR) technique. Furthermore, to achieve balanced learning over different concepts, we introduce the Balanced Prototypical Memory (BPM), which guides SGG models to generate balanced representations for concept-prototypes. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. |
| 2023-08-09 | [Bird's-Eye-View Scene Graph for Vision-Language Navigation](http://arxiv.org/abs/2308.04758v2) | Rui Liu, Xiaohan Wang et al. | Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human instructions, has shown great advances. However, current agents are built upon panoramic observations, which hinders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment under the supervision of 3D detection. During navigation, BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accurate action prediction. Our approach significantly outperforms state-of-the-art methods on REVERIE, R2R, and R4R, showing the potential of BEV perception in VLN. |
| 2020-11-20 | [Neural Scene Graphs for Dynamic Scenes](http://arxiv.org/abs/2011.10379v3) | Julian Ost, Fahim Mannan et al. | Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses. |
| 2020-02-27 | [Unbiased Scene Graph Generation from Biased Training](http://arxiv.org/abs/2002.11949v4) | Kaihua Tang, Yulei Niu et al. | Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse "human walk on / sit on / lay on beach" into "human on beach". Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., "person read book" rather than "eat") and bad long-tailed bias (e.g., "near" dominating "behind / in front of"). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect (TDE) as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods. |
| 2019-09-19 | [Triplet-Aware Scene Graph Embeddings](http://arxiv.org/abs/1909.09256v1) | Brigit Schroeder, Subarna Tripathi et al. | Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model. |

</details>

<!-- HISTORICAL_PAPERS_END -->
---



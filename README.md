## ðŸ“° Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-07-15 | [Canonical Bayesian Linear System Identification](http://arxiv.org/abs/2507.11535v1) | Andrey Bryutkin, Matthew E. Levine et al. | Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data. |
| 2025-07-15 | [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](http://arxiv.org/abs/2507.11488v1) | Pakizar Shamoi, Nuray Toganas et al. | Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical. |
| 2025-07-15 | [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](http://arxiv.org/abs/2507.11420v1) | Mingcong Li | Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies. |
| 2025-07-15 | [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](http://arxiv.org/abs/2507.11413v1) | Christos N. Efrem, Ioannis Krikidis | We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average. |
| 2025-07-15 | [Towards NNLO QCD predictions for off-shell top-quark pair production and decays](http://arxiv.org/abs/2507.11410v1) | Luca Buonocore, Massimiliano Grazzini et al. | We consider QCD radiative corrections to $W^+W^-b {\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\Gamma_t\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\%$ level $\unicode{x2013}$ significantly smaller than the $5\%$ residual perturbative uncertainties. |

<!-- LATEST_PAPERS_END -->


<!-- HISTORICAL_PAPERS_START -->

<details>
<summary>ðŸ“š View Historical Papers (15 entries)</summary>



| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-07-15 | [Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions](http://arxiv.org/abs/2507.11394v1) | Syed Afrid Jahan, Hendrik Roch et al. | We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\sqrt{s_\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions. |
| 2025-07-15 | [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](http://arxiv.org/abs/2507.11385v1) | George D. Pasparakis, Ioannis A. Kougioumtzoglou et al. | A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics. |
| 2025-07-15 | [The miniJPAS survey quasar selection V: combined algorithm](http://arxiv.org/abs/2507.11380v1) | Ignasi PÃ©rez-RÃ fols, L. Raul Abramo et al. | Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\sigma_{\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\sigma_{\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration. |
| 2025-07-15 | [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](http://arxiv.org/abs/2507.11357v1) | Emile van Krieken, Pasquale Minervini et al. | The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons. |
| 2025-07-15 | [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](http://arxiv.org/abs/2507.11352v1) | Yunhao Yang, Neel P. Bhatt et al. | Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics. |
| 2025-07-14 | [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](http://arxiv.org/abs/2507.10510v1) | Jiangkai Wu, Zhiyuan Ren et al. | AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. |
| 2025-07-14 | [Referential ambiguity and clarification requests: comparing human and LLM behaviour](http://arxiv.org/abs/2507.10445v1) | Chris Madge, Matthew Purver et al. | In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy. |
| 2025-07-14 | [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](http://arxiv.org/abs/2507.10425v1) | Alvaro H. C. Correia, Christos Louizos | Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift. |
| 2025-07-14 | [Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions](http://arxiv.org/abs/2507.10377v1) | T. Dumont, A. Bonhomme et al. | The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\alpha},{\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\alpha},{\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\alpha},{\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition. |
| 2025-07-14 | [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](http://arxiv.org/abs/2507.10355v1) | Bo Jiang, Xueyang Ze et al. | Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach. |
| 2025-07-14 | [Gaussian Process Methods for Very Large Astrometric Data Sets](http://arxiv.org/abs/2507.10317v1) | Timothy Hapitas, Lawrence M. Widrow et al. | We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics. |
| 2025-07-14 | [How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models](http://arxiv.org/abs/2507.10314v1) | Nina Mackensen, Nicole Reindl et al. | PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\text{RX J}0122.9\text{ -}7521$. We find $T_\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\text{RX J}0122.9\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\text{RX J}0122.9\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\text{RX J}0122.9\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\text{RX J}0122.9\text{ -}7521$. |
| 2025-07-14 | [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](http://arxiv.org/abs/2507.10291v1) | Kristine V. Ung, Connor A. Roncaioli et al. | The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\mathrm{^{1}E}$ singlet state in the $\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\mathrm{^{1}E}$ into the neutral $\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited. |
| 2025-07-14 | [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](http://arxiv.org/abs/2507.10201v1) | Gleb Shishaev, Vasily Demyanov et al. | The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure. |
| 2025-07-14 | [Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture](http://arxiv.org/abs/2507.10166v1) | Filip Surmaa, Anahita Jamshidnejad | This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems. |

</details>

<!-- HISTORICAL_PAPERS_END -->
---



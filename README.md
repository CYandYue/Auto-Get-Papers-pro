## ðŸ“° Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-12-17 | [When sufficiency is insufficient: the functional information bottleneck for identifying probabilistic neural representations](http://arxiv.org/abs/2512.15671v1) | Ishan Kalburge, MÃ¡tÃ© Lengyel | The neural basis of probabilistic computations remains elusive, even amidst growing evidence that humans and other animals track their uncertainty. Recent work has proposed that probabilistic representations arise naturally in task-optimized neural networks trained without explicitly probabilistic inductive biases. However, prior work has lacked clear criteria for distinguishing probabilistic representations, those that perform transformations characteristic of probabilistic computation, from heuristic neural codes that merely reformat inputs. We propose a novel information bottleneck framework, the functional information bottleneck (fIB), that crucially evaluates a neural representation based not only on its statistical sufficiency but also on its minimality, allowing us to disambiguate heuristic from probabilistic coding. To demonstrate the power of this framework, we study a variety of task-optimized neural networks that had been suggested to develop probabilistic representations in earlier work: networks trained to perform static inference tasks (such as cue combination and coordinate transformation) or dynamic state estimation tasks (Kalman filtering). In contrast to earlier claims, our minimality requirement reveals that probabilistic representations fail to emerge in these networks: they do not develop minimal codes of Bayesian posteriors in their hidden layer activities, and instead rely on heuristic input recoding. Therefore, it remains an open question under which conditions truly probabilistic representations emerge in neural networks. More generally, our work provides a stringent framework for identifying probabilistic neural codes. Thus, it lays the foundation for systematically examining whether, how, and which posteriors are represented in neural circuits during complex decision-making tasks. |
| 2025-12-17 | [A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition](http://arxiv.org/abs/2512.15650v1) | Stephen Tivenan, Indranil Sahoo et al. | Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period. |
| 2025-12-17 | [Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation](http://arxiv.org/abs/2512.15643v1) | Jairo FÃºquene-PatiÃ±o | In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study. |
| 2025-12-17 | [Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error](http://arxiv.org/abs/2512.15624v1) | Akash Yadav, Ruda Zhang | Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics. |
| 2025-12-17 | [A Decision-Theoretic Approach for Managing Misalignment](http://arxiv.org/abs/2512.15584v1) | Daniel A. Herrmann, Abinav Chari et al. | When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty. |

<!-- LATEST_PAPERS_END -->


<!-- HISTORICAL_PAPERS_START -->

<details>
<summary>ðŸ“š View Historical Papers (1075 entries)</summary>



| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-12-17 | [Extensive Observational Evidence for Massive Star Stellar Wind Variability at Low Metallicities: implications for mass-loss rate determination](http://arxiv.org/abs/2512.15539v1) | Timothy N. Parsons, Raman K. Prinja et al. | Mass-loss from massive stars is fundamental to stellar and galactic evolution and enrichment of the interstellar medium. Reliable determination of mass-loss rate is dependent upon unravelling details of massive star outflows, including optical depth structure of the stellar wind. That parameter introduces significant uncertainty due to the nearly ubiquitous presence of large-scale optically thick wind structure. We utilize suitable available ultraviolet spectra of 20 Large and Small Magellanic Cloud (LMC, SMC) OB stars to extend existing Galactic results quantifying uncertainty inherent in individual observations to lower metallicity environments. This is achieved by measuring standard deviations of mean optical depths of multiple observations of suitable wind-formed absorption profiles as a proportion of their mean optical depths. We confirm earlier findings that wind structure is prevalent at low metallicities and demonstrate that quantifying the consequent uncertainty is to some extent possible, despite the near-complete absence of time series UV spectroscopic observations in those environments. We find that the uncertainty inherent in any single observation of stellar wind optical depth at low metallicity is of similar magnitude to that already identified at Galactic metallicity (up to 45% for cooler OB stars). We further demonstrate how the effect of varying narrow absorption components in wind-formed UV spectral profiles is unlikely to be properly accounted for in existing mass-loss models. We present further evidence of a binary companion to the SMC O-type giant star AzV 75. The importance of obtaining high cadence multi-epoch, or genuine time series, UV spectroscopic observations at low metallicities is highlighted. |
| 2025-12-17 | [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](http://arxiv.org/abs/2512.15492v1) | AdriÃ¡n Detavernier, Jasper De Bock | We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability. |
| 2025-12-17 | [Energy Inference of Black-Box Quantum Computers Using Quantum Speed Limit](http://arxiv.org/abs/2512.15472v1) | Nobumasa Ishida, Yoshihiko Hasegawa | Cloud-based quantum computers do not provide users with access to hardware-level information such as the underlying Hamiltonians, which obstructs the characterization of their physical properties. We propose a method to infer the energy scales of gate Hamiltonians in such black-box quantum processors using only user-accessible data, by exploiting quantum speed limits. Specifically, we reinterpret the Margolus-Levitin and Mandelstam-Tamm bounds as estimators of the energy expectation value and variance, respectively, and relate them to the shortest time for the processor to orthogonalize a quantum state. This shortest gate time, expected to lie on the nanosecond scale, is inferred from job execution times measured in seconds by employing gate-time amplification. We apply the method to IBM's superconducting quantum processor and estimate the energy scales associated with single-, two-, and three-qubit gates. The order of estimated energy is consistent with typical drive energies in superconducting qubit systems, suggesting that current gate operations approach the quantum speed limit. Our results demonstrate that fundamental energetic properties of black-box quantum computers can be quantitatively accessed through operational time measurements, reflecting the conjugate relationship between time and energy imposed by the uncertainty principle. |
| 2025-12-17 | [Robustness Measures for Stochastic Parallel Machine Scheduling and Train Unit Shunting](http://arxiv.org/abs/2512.15471v1) | Casper Loman, Loriana Pascual et al. | In many real world scheduling problems, the processing times of tasks are subject to uncertainty. This makes it essential to design schedules that are robust and able to handle potential disruptions. Therefore, we investigate measures that give us information about the robustness of a schedule. Although many measures can be found in literature, there is no consensus on which measures are the best. We identify 14 robustness measures from the literature, as well as introduce 4 new ones. To find out which of these measures are best used for generating robust schedules, we perform an elaborate simulation study to investigate how well these robustness measures correlate with the stability of the objective function under disturbances (quality robustness) and with the stability of the schedule itself (solution robustness). We first consider the Stochastic Parallel Machine Scheduling Problem (SPMSP) with precedence constraints, which is a very general setting that is relevant for many practical situations. We then perform a second simulation study by taking the best performing measures from the first experiment, and using them for the Train Unit Shunting Problem with Service Scheduling (TUSPwSS). After establishing the correlation with quality and solution robustness, we included the measures as objective in a local search algorithm. We make a comparison between the theoretical setting of the SPMSP and the TUSPwSS, and identify a set of robustness measures that can be applied in many different settings. We show that we can achieve up to 90% decreases in delays compared to using no robustness measures. Lastly, we also identify properties that can be used to predict the effectiveness of such a robustness measure. |
| 2025-12-17 | [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](http://arxiv.org/abs/2512.15405v1) | Jianfei Ma, Wee Sun Lee | At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency. |
| 2025-12-16 | [Drell-Yan at the Electron-Ion Collider](http://arxiv.org/abs/2512.14690v1) | Henry T. Klest | The photon is arguably the most universally important particle across all fields of physics. Despite its status as a fundamental particle, at high energies the photon can be seen as a hadronic source of partons. The partonic content of the photon is very poorly constrained compared to that of the proton, with photon PDF uncertainties typically one or two orders of magnitude larger than their proton counterparts, despite the fact that its source, the $Î³\to q\bar{q}$ splitting, is perturbatively calculable. The high luminosity, excellent particle identification, and far-backward electron tagging capabilities of the Electron-Ion Collider make it an ideal environment for studying photon parton distribution functions. Similar to the $p+p$ or $Ï€+p$ systems, photoproduction at the EIC can be thought of as two parton distributions colliding. One of the most powerful processes in such collisions is production of lepton pairs, i.e. $h+p\rightarrow l^+l^-+X$, known as the Drell--Yan process. This process has the ability to access for the first time the transverse-momentum-dependent parton distributions of the photon. The transversely polarized proton beam of the EIC additionally provides a possible means of accessing the transversity distribution of the proton without relying on fragmentation functions. |
| 2025-12-16 | [Analysis and Uncertainty Quantification of Thermal Transport Measurements through Bayesian Parameter Estimation](http://arxiv.org/abs/2512.14659v1) | Jeremy Drew, Shravan Godse et al. | The thermal transport community is increasingly interested in rigorous uncertainty quantification (UQ) of their measurements. In this work, we argue that Bayesian parameter estimation (BPE) represents a powerful framework for both analysis/fitting and UQ. We provide a detailed walkthrough of the technique (including code to duplicate our results) and example analysis based on measuring the thermal conductance of a gold/sapphire interface with FDTR. Comparisons are made against traditional analysis/UQ techniques adopted by the thermal transport community. Notable advantages of BPE include the interpretability of its results, including the capacity to indicate incorrect input assumptions, as well as a way to balance overall goodness of fit against prior knowledge of feasible parameter values. In some cases, incorporating this additional information can affect not only the magnitude of error bars but the inferred values themselves. |
| 2025-12-16 | [Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty](http://arxiv.org/abs/2512.14653v1) | Yiwen Zhao, Jiatong Shi et al. | Singing voice synthesis (SVS) has seen remarkable advancements in recent years. However, compared to speech and general audio data, publicly available singing datasets remain limited. In practice, this data scarcity often leads to performance degradation in long-tail scenarios, such as imbalanced pitch distributions or rare singing styles. To mitigate these challenges, we propose uncertainty-based optimization to improve the training process of end-to-end SVS models. First, we introduce differentiable data augmentation in the adversarial training, which operates in a sample-wise manner to increase the prior uncertainty. Second, we incorporate a frame-level uncertainty prediction module that estimates the posterior uncertainty, enabling the model to allocate more learning capacity to low-confidence segments. Empirical results on the Opencpop and Ofuton-P, across Chinese and Japanese, demonstrate that our approach improves performance in various perspectives. |
| 2025-12-16 | [kSZ for everyone: the pseudo-Cl approach to stacking](http://arxiv.org/abs/2512.14625v1) | Lea Harscouet, Kevin Wolz et al. | We present a harmonic-space estimator for the cross-correlation between the kinematic Sunyaev-Zel'dovich effect and the reconstructed galaxy momentum field that offers several practical advantages over the traditional stacking approach. The estimator is easy to deploy using relatively modest computational resources and recovers all information available in the galaxy-kSZ cross-correlation. In particular, by using well-understood power spectrum estimation techniques, its statistical uncertainties, including potential correlated uncertainties with other large-scale structure observables, can be easily and accurately estimated. Moreover, standard kSZ stacking measurements can be reconstructed exactly from the estimator at a lower computational cost, employing harmonic-space, catalog-level techniques to recover all small-scale information. |
| 2025-12-16 | [Charged Higgs Pairs at the LHC: A NLO Analysis](http://arxiv.org/abs/2512.14587v1) | Mohamed Ahmed, Lisa Biermann et al. | Charged Higgs-boson pair production at hadron colliders yields information about the trilinear couplings involving charged Higgs fields in extensions of the Standard Model (SM). We consider the two-Higgs doublet model (2HDM) extension and obtain next-to-leading order QCD predictions for the charged-Higgs pair production ($H^+H^-$ production). All production modes, i.e. Drell--Yan-like production, gluon fusion and vector-boson fusion are included in the analysis. We determine uncertainties originating from the scale dependence, the parton-density functions and strong coupling $Î±_s$ at the LHC. We observe that the QCD corrections lead to a significant reduction of the relative scale dependences. These improved predictions will allow for a quantitative interpretation of experimental measurements, in case that charged Higgs states will be observed. |
| 2025-12-16 | [Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error](http://arxiv.org/abs/2512.14492v1) | Martin Slawski | The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach. |
| 2025-12-16 | [Precise Predictions for $Î¼^{\pm}e^-\rightarrowÎ¼^{\pm}e^-$ at the MUonE Experiment](http://arxiv.org/abs/2512.14431v1) | Alan Price | The proposed fixed-target experiment, MUonE, at CERN will aim to measure the hadronic contribution to the running of the QED coupling by analysing the scattering of muons on electrons. Here we present state-of-the-art predictions for the process $Î¼^{\pm}e^-\rightarrowÎ¼^{\pm}e^-$, where for the first time an all-order resummation of soft and soft-collinear logarithms has been performed. Further, we match this resummation with the complete next-to-leading and the dominant next-to-next-to-leading higher-order corrections. We find that the resummation has a dominant effect in the signal region, while the systematic matching significantly reduces the perturbative uncertainty. |
| 2025-12-16 | [Towards Real Time Control of Water Engineering with Nonlinear Hyperbolic Partial Differential Equations](http://arxiv.org/abs/2512.14387v1) | Fabio DiFonzo, Michael Holst et al. | This paper examines aspirational requirements for software addressing mixed-integer optimization problems constrained by the nonlinear Shallow Water partial differential equations (PDEs), motivated by applications such as river-flow management in hydropower cascades. Realistic deployment of such software would require the simultaneous treatment of nonlinear and potentially non-smooth PDE dynamics, limited theoretical guarantees on the existence and regularity of control-to-state mappings under varying boundary conditions, and computational performance compatible with operational decision-making. In addition, practical settings motivate consideration of uncertainty arising from forecasts of demand, inflows, and environmental conditions. At present, the theoretical foundations, numerical optimization methods, and large-scale scientific computing tools required to address these challenges in a unified and tractable manner remain the subject of ongoing research across the associated research communities. Rather than proposing a complete solution, this work uses the problem as a case study to identify and organize the mathematical, algorithmic, and computational components that would be necessary for its realization. The resulting framework highlights open challenges and intermediate research directions, and may inform both more circumscribed related problems and the design of future large-scale collaborative efforts aimed at addressing such objectives. |
| 2025-12-16 | [Measurements of the branching fractions of $Ï‡_{cJ}\to Ï†Ï†Î·, Ï†Ï†Î·^{\prime}$ and $Ï†K^+K^-Î·$](http://arxiv.org/abs/2512.14369v1) | BESIII Collaboration, M. Ablikim et al. | Using a sample of $(2712.4 \pm 14.3)\times 10^6 ~Ïˆ$(3686) events collected by the BESIII detector at the BEPCII collider, we measure the branching fractions of the decays $Ï‡_{cJ}\to Ï†Ï†Î·,~Ï†Ï†Î·^{\prime}$, and~$Ï†K^+K^-Î·$ ($J = 0, 1, 2$). The obtained branching fractions are $\mathcal{B}(Ï‡_{c0} \to Ï†Ï†Î·) = (7.40 \pm 0.23 \pm 0.55)\times10^{-4}$, $\mathcal{B}(Ï‡_{c1} \to Ï†Ï†Î·) = (3.33 \pm 0.14 \pm 0.25)\times10^{-4}$, $\mathcal{B}(Ï‡_{c2} \to Ï†Ï†Î·) = (5.46 \pm 0.17 \pm 0.40)\times10^{-4}$, $\mathcal{B}(Ï‡_{c0} \to Ï†Ï†Î·^\prime) = (2.96 \pm 0.23 \pm 0.29)\times10^{-4}$, $\mathcal{B}(Ï‡_{c1} \to Ï†Ï†Î·^\prime) = (0.69 \pm 0.10 \pm 0.08)\times10^{-4}$, $\mathcal{B}(Ï‡_{c2} \to Ï†Ï†Î·^\prime) = (0.65 \pm 0.09 \pm 0.07)\times10^{-4}$, $\mathcal{B}(Ï‡_{c0} \to Ï†K^+K^-Î·) = (1.23 \pm 0.08 \pm 0.10)\times10^{-4}$, $\mathcal{B}(Ï‡_{c1} \to Ï†K^+K^-Î·) = (1.00 \pm 0.07 \pm 0.07)\times10^{-4}$, and $\mathcal{B}(Ï‡_{c2} \to Ï†K^+K^-Î·) = (1.82 \pm 0.09 \pm 0.14)\times10^{-4}$, where $K^+K^-$ is not from the decay of a $Ï†$ meson, the first uncertainties are statistical and the second systematic. The branching fractions of $Ï‡_{cJ}\to Ï†Ï†Î·$ are measured with precision improved by factors of $1.5-1.9$, and those of $Ï‡_{cJ}\to Ï†Ï†Î·^\prime$ and $Ï†K^+K^-Î·$ are measured for the first time. |
| 2025-12-16 | [ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning](http://arxiv.org/abs/2512.14331v1) | Rishabh Dev Yadav, Avirup Das et al. | Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines. |
| 2025-12-15 | [XID+PRIMA, II: Stepping Through Hyperspectral Imaging to Deblend PRIMAger Beyond the Extragalactic Confusion Limit](http://arxiv.org/abs/2512.13682v1) | J. M. S. Donnellan, B. Pautasso et al. | The PRobe far-Infrared Mission for Astrophysics concept aims to map large areas with spectral coverage and sensitivities inaccessible to previous FIR space telescopes, covering 25-235um. We synthesise images representing a deep imaging survey, with realistic instrumental and confusion noise, reflecting the latest PRIMAger instrument specifications. We present a new Bayesian modelling approach XID+stepwise that exploits PRIMAger's hyperspectral imaging to derive self-consistent, informative flux priors by sequentially propagating constraints from short to long wavelengths. With Euclid-like prior source positions, this method recovers fluxes to within 20% to 0.2-0.7 mJy across 45-84 um, which correspond to factors of 1.3-3.4 fainter than the confusion limit. For the most confusion-dominated channels, accurate fluxes are measured to 0.9, 2.5, 7.6 and 14.8 mJy at 92, 126, 183 and 235 um, respectively, which are factors of 3-5 better than the confusion limit. Using a deeper Euclid-based prior catalogue and weak ancillary flux priors at 25 um yields further improvements, reaching up to a factor ~7 fainter than the confusion limit at 96 um. Additionally, we demonstrate that positional priors from blind source detection followed by deblending via XID+ enables PRIMAger to achieve sensitivity beyond the confusion limits using PRIMAger data alone. We show that IR-luminous galaxies at z~2 are robustly detected in a large fraction of the PRIMAger channels (>98% in 12 out of the 16 considered channels), providing dense sampling of the FIR SED even for sources several factors below the confusion limit. We explore the impact on our results for a range of systematic effects, including cirrus contamination, optical degradation, and calibration uncertainties. These findings indicate that confusion noise will not limit the key science from PRIMA extragalactic imaging surveys when employing XID+. |
| 2025-12-15 | [Towards Quantum Advantage in Chemistry](http://arxiv.org/abs/2512.13657v1) | Scott N. Genin, Ohyun Kwon et al. | Molecular simulations are widely regarded as leading candidates to demonstrate quantum advantage--defined as the point at which quantum methods surpass classical approaches in either accuracy or scale. Yet the qubit counts and error rates required to realize such an advantage remain uncertain; resource estimates for ground-state electronic structure span orders of magnitude, and no quantum-native method has been validated at a commercially relevant scale. Here we address this uncertainty by executing the iterative qubit coupled-cluster (iQCC) algorithm, designed for fault-tolerant quantum hardware, at unprecedented scale using a quantum solver on classical processors, enabling simulations of transition organo-metallic complexes requiring hundreds of logical qubits and millions of entangling gates. Using this approach, we compute the lowest triplet excited state (T$_1$) energies of Ir(III) and Pt(II) phosphorescent organometallic compounds and show that iQCC achieves the lowest mean absolute error (0.05 eV) and highest R$^2$ (0.94) relative to experiment, outperforming leading classical methods. We find these systems remain classically tractable up to $\sim$200 logical qubits, establishing the threshold at which quantum advantage in computational chemistry may emerge and clarifying resource requirements for future quantum computers. |
| 2025-12-15 | [The impact of non-Gaussianity when searching for Primordial Black Holes with LISA](http://arxiv.org/abs/2512.13648v1) | Antonio Junior Iovino, Gabriele Perna et al. | LISA can observe cosmological millihertz (mHz) gravitational wave (GW) backgrounds that may offer a decisive test for asteroid-mass primordial black hole (PBH) dark matter (DM). In standard scenarios, failing to detect a scalar-induced gravitational wave (SIGW) background would exclude the last viable window for PBH DM formed through critical collapse. We show that this conclusion becomes much weaker in the presence of astrophysical foregrounds and strongly non-Gaussian primordial density perturbations, by studying how these phenomena affect the link between SIGWs and PBHs, and reevaluate LISA's sensitivity to asteroid-mass PBHs. In addition, we analyse the interplay between PBHs and SIGWs to gain further insights into the nature of primordial non-Gaussianity. We find that uncertainties in $f_{\rm NL}$ can induce substantial uncertainties in the PBH abundance, which ultimately limits LISA's capacity to fully probe the asteroid-mass PBH DM window. |
| 2025-12-15 | [Adaptive Sampling for Hydrodynamic Stability](http://arxiv.org/abs/2512.13532v1) | Anshima Singh, David J. Silvester | An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis. |
| 2025-12-15 | [Deployable Prototype Testing and Control Allocation of the CABLESSail Concept for Solar Sail Shape Control and Momentum Management](http://arxiv.org/abs/2512.13493v1) | Soojeong Lee, Michael States et al. | This paper presents prototype testing and a control allocation algorithm for the Cable-Actuated Bio-inspired Lightweight Elastic Solar Sail (CABLESSail) concept aimed at performing momentum management of a solar sail. CABLESSail uses actuated cables routed along the structural booms of the solar sail to control the shape of the solar sail and changes the solar radiation pressure disturbance torques acting on it. Small-scale prototype tests of CABLESSail are presented in this paper, which demonstrate the effectiveness of cable actuation on deployable booms. A novel control allocation method is also presented in this paper that provides a computationally-efficient manner to determine the deformations required in each of the structural booms to impart the desired momentum management torque on the solar sail. Numerical simulation results with the proposed algorithm demonstrate robustness to uncertainty in the shape of the sail membrane, resulting in reliable generation of momentum management torques that exceed or meet the capabilities of state-of-the-art solar sail actuators. Both the prototype tests and control allocation methods presented in this paper represent key steps in raising the technology readiness level of the CABLESSail concept. |
| 2025-12-15 | [Climate change impacts on net load under technological uncertainty in European power systems](http://arxiv.org/abs/2512.13461v1) | Luna Bloin-Wibe, Erich Fischer et al. | Renewable energy sources play a major role in future net-zero energy systems. However, achieving energy system resilience remains challenging, since renewables depend on weather fluctuations, and future energy systems are subject to major design uncertainty. Existing literature mostly treats these types of uncertainty separately. Therefore, the assessment of uncertainties surrounding climate change and energy system design, and particularly their interactions, is insufficiently understood. To close this gap, we evaluate net load to assess energy system stress without relying on perfect foresight, while maintaining temporal and spatial correlations of the climate system. Net load is calculated from hourly historical and future climate model data translated to energy variables. To scope the extent of plausible energy systems, we consider eight different design scenarios inspired by the European Ten-Year Network Development Plan (TYNDP) and different levels of transmission expansion. We find that climate change impacts on net load are highly sensitive to the energy system design, implying that energy systems can be designed so that they are either hindered or helped by climate change. Furthermore, within a system scenario, climate change can change the frequency and seasonality of high net load events and their technological and meteorological composition. Wind-dominated systems with currently electrified heating levels, for instance, feature a 30% increase of high net load events under climate change, mostly in summer and fall, while fully electrified net zero systems are impacted by high net load events in winter and spring, which decrease by 50% with climate change. Our work thus calls for a wider perspective on energy-climate stress that captures the non-linear interactions of climate change and system design uncertainty, thereby overcoming the current focus on cold Dunkelflauten. |
| 2025-12-15 | [Computational discovery of ferromagnetic AT6X6 kagome compounds](http://arxiv.org/abs/2512.13431v1) | Shiya Chen, Zhen Zhang et al. | We present a systematic high-throughput density-functional-theory investigation of the structural and magnetic stability of 312 substitutional compounds in the magnetic kagome AT6X6 family. Our screening confirms the stability of many previously reported structures and predicts several additional stable candidates. Within collinear spin configurations, we find that Fe-based systems predominantly adopt antiferromagnetic ground states, whereas Mn-based analogues exhibit a more balanced distribution between ferromagnetic and antiferromagnetic order. For compounds exhibiting several nearly degenerate collinear configurations, we analyze the nature of their magnetic ground states, assess the possible emergence of non-collinear order, and discuss the limitations and uncertainties inherent to standard density-functional approaches. Our electronic-structure analysis further reveals that newly predicted ferromagnetic kagome systems display characteristic features of topological metals, with rich magnetic configurations that can be tuned by chemical substitution. Overall, these ferromagnetic kagome compounds constitute a broad and still largely unexplored materials platform for the emergence of exciting magneto-transport phenomena. |
| 2025-12-15 | [Prospects for Measuring Black Hole Masses using TDEs with the Vera C. Rubin Observatory](http://arxiv.org/abs/2512.13409v1) | K. Decker French, Brenna Mockler et al. | Tidal Disruption Events (TDEs) provide an opportunity to study supermassive black holes that are otherwise quiescent. The Vera C. Rubin Legacy Survey of Space and Time will be capable of discovering thousands of TDEs each year, allowing for a dramatic increase in the number of discovered TDEs. The optical light curves from TDEs can be used to model the physical parameters of the black hole and disrupted star, but the sampling and photometric uncertainty of the real data will couple with model degeneracies to limit our ability to recover these parameters. In this work, we aim to model the impact of the Rubin survey strategy on simulated TDE light curves to quantify the typical errors in the recovered parameters. Black hole masses $5.5< \log M_{\rm BH}/M_\odot < 8.2$ can be recovered with typical errors of 0.26 dex, with early coverage removing large outliers. Recovery of the mass of the disrupted star is difficult, limited by the degeneracy with the accretion efficiency. Only 57\% of the cases have accurate recovery of whether the events are full or partial, so we caution the use this method to assess whether TDEs are partially or fully disrupted systems. Black hole mass measurements obtained from Rubin observations of TDEs will provide powerful constraints on the black hole mass function, black hole -- galaxy co-evolution, and the population of black hole spins, though continued work to understand the origin of TDE observables and how the TDE rate varies among galaxies will be necessarily to fully utilize the upcoming rich data set from Rubin. |
| 2025-12-15 | [Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling](http://arxiv.org/abs/2512.13354v1) | Geremy LoachamÃ­n, Eleni D. Koronaki et al. | This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty. |
| 2025-12-15 | [Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases](http://arxiv.org/abs/2512.13346v1) | Yukun Lu, Bingjie Li et al. | Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent "Don't know/can't remember" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, >7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level. |
| 2025-12-12 | [Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs](http://arxiv.org/abs/2512.11791v1) | Wentao Jiang, Vamsi Varra et al. | Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment. |
| 2025-12-12 | [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](http://arxiv.org/abs/2512.11773v1) | Britton Jordan, Jordan Thompson et al. | Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements. |
| 2025-12-12 | [LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems](http://arxiv.org/abs/2512.11750v1) | Ernesto Casablanca, Oliver SchÃ¶n et al. | Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks. |
| 2025-12-12 | [Determination of $B$-meson distribution amplitudes from $B\to Ï€,K,D$ transition form factors](http://arxiv.org/abs/2512.11741v1) | Dong-Hao Li, Cai-Dian LÃ¼ et al. | Recent work on $B \to Ï€$, $K$ and $B\to D$ form factors from lattice QCD and light-cone sum rules has made it possible to constrain the inverse moment $Î»_B$ of the $B$-meson light-cone distribution amplitudes by performing a global fit of $B\to Ï€,K,D$ form factors. We have compiled the $B\to Ï€,K,D$ form factors calculated by the HPQCD, MILC, and RBC/UKQCD collaborations in the large $q^2$ region. By employing an three-parameter ansatz of the $B$-meson light-cone distribution amplitudes, we express the $B\to Ï€,K,D$ form factors at $q^2=0$ that are calculated from light-cone sum rules, in terms of the inverse moment $Î»_B$ of the leading-twist $B$-meson light-cone distribution amplitude. In the $B \to Ï€\ell Î½$ channel, we also include the available $q^2$-binned experimental data from the BaBar, Belle, and Belle~II collaborations. Using the Bourrely-Caprini-Lellouch parametrization, we perform a global fit and obtain $Î»_B=217(19)_{-17}^{+82}$~MeV and $|V_{\text{ub}}|=3.68(13)_{-1}^{+0}\times10^{-3}$. The second uncertainty is obtained by constraining $Î»_B>200$ MeV and varying the inverse logarithmic moments $\hatÏƒ_1\in[-0.7,0.7]$ and $\hatÏƒ_2\in[-6,6]$, which represents the model-dependent uncertainty from the $B$-meson light-cone distribution amplitudes. When taking into account $Î»_B$ and $\hatÏƒ_1$ as fitting parameters simultaneously, the intervals of our preditions are $Î»_B=[208, 324]$~MeV and $\hatÏƒ_1=[-0.7, 0.27]$. |
| 2025-12-12 | [The Impact of Initial Mass Dependent Convective Boundary Mixing on the Structure and Fates of Massive Stars](http://arxiv.org/abs/2512.11728v1) | Emily E. Whitehead, Raphael Hirschi et al. | While convection has been known to play a key role in stars for many decades, its implementation in one-dimensional stellar evolution codes still represents a major uncertainty today. The purpose of this work is to investigate the impact of initial mass dependent convective boundary mixing (CBM), often referred to as overshooting, on the frequency and type of nuclear burning shell interactions that occur in low metallicity massive stars and the subsequent effect on their fates. Two grids of models were calculated using the Modules for Experiments in Stellar Astrophysics (MESA) code and a 22-isotope nuclear network, each with a different strength of CBM applied. One grid uses the typical CBM value for diffusive overshooting used in literature whereas the other grid uses CBM values guided by the results of 3D convection simulations. Interactions between the carbon, neon and oxygen shells (C-Ne-O) are common throughout both grids. The higher CBM grid also exhibits more frequent H-He and He-C interactions at lower initial masses than in the lower CBM grid. Several models also undergo multiple interaction events during evolution. While future work will be needed to fully assess the impact of the new CBM and the interactions it leads to, one expects interesting effects like unusual nucleosynthesis including more common or enhanced i- and gamma-process nucleosynthesis. Furthermore, SN precursors and a significant change to the pre-SN structure are also expected, with many models not having the commonly expected onion-ring like structure and having a different explosion probability. |
| 2025-12-12 | [Machine-learned global glacier ice volumes](http://arxiv.org/abs/2512.11685v1) | N. Maffezzoli, E. Rignot et al. | We present a global dataset of glacier ice thickness modeled with IceBoost v2.0, a machine learning model trained on 7 million ice thickness measurements and informed by physical and geometrical predictors. We model the distributed ice thickness for every glacier in the two latest Randolph Glacier Inventory releases (v6.0 and v7.0), totaling 215,547 and 274,531 glacier outlines, respectively, plus 955 ice masses contiguous with the Greenland Ice Sheet. We find a global glacier volume of $(149 \pm 38)\times 10^3$ km$^3$, consistent with the previous ensemble estimate of $(147 \pm 28)\times 10^3$ km$^3$. The corresponding sea-level equivalent, $323 \pm 91$ mm, is likewise consistent with the earlier value of $315 \pm 63$ mm. Compared to measurements, IceBoost error is 20-45% lower than the other solutions in the high Arctic, highlighting the value of machine-learning approaches. Confidence in our solution is highest at higher latitudes. Over mountainous terrain, small glaciers, and under-represented lower-latitude regions, confidence is lower. IceBoost v2.0 demonstrates strong generalization at ice sheet margins. On the Geikie Plateau (East Greenland), we find nearly twice as much ice as previously reported, highlighting the method's potential to infer bed topography in parts of the ice sheets. The quality of the solutions depends on the accuracy of the training data, the Digital Elevation Model, ice velocity fields, and glacier geometries, including nunataks. Using the Jensen Gap, we probe the model's curvature with respect to input errors and find it is strongly concave over low-slope, thick-ice regions, implying a potential downward bias in predicted thickness under input uncertainty. The released dataset can be used to model future glacier evolution and sea-level rise, inform the design of glaciological surveys and field campaigns, as well as guide policies on freshwater management. |
| 2025-12-12 | [Influence of Exchange-Correlation Functionals and Neural Network Architectures on Li$^+$-Ion Conductivity in Solid-State Electrolyte from Molecular Dynamics Simulations with Machine-Learning Force Fields](http://arxiv.org/abs/2512.11650v1) | Zicun Li, Huanjing Gong et al. | With the rapid advancement of machine learning techniques for materials simulations, machine-learned force fields (MLFFs) have become a powerful tool that complements first-principles calculations by enabling high-accuracy molecular dynamics simulations over extended timescales. Typically, MLFFs are trained on data generated from density functional theory (DFT) using a specific exchange-correlation (XC) functional, with the goal of reproducing DFT-level properties. However, the uncertainties in MLFF-based simulations--arising from variations in both MLFF model architectures and the choice of XC functionals--remain insufficiently understood. In this work, we construct MLFF models of different architectures trained on DFT data from both semilocal and hybrid functionals to describe Li$^+$ diffusion in the solid-state electrolyte Li$_6$PS$_5$Cl. We systematically investigate how different XC functionals influence the Li$^+$ diffusion coefficient. To reduce statistical uncertainty, the mean squared displacements are averaged over 300 independent molecular dynamics (MD) trajectories of 70 ps each, yielding statistical variations below $1\%$. This enables a clear assessment of the respective influences of the functional and the MLFF model. Due to its tendency to underestimate band gaps and migration barriers, the semilocal functional predicts consistently higher Li$^+$ diffusion coefficients, compared to the hybrid functional. Furthermore, comparisons among various neural network methods reveal that the differences in predicted diffusion coefficients arising from different network architectures are of the same order of magnitude as those caused by different functionals, indicating that the choice of the network model itself substantially influences the MLFF predictions. This observation calls from an urgent need for standardized protocols to minimize model-dependent biases in MLFF-based MD. |
| 2025-12-12 | [Polarization Entanglement in Atomic Biphotons via OAM-to-Spin Mapping](http://arxiv.org/abs/2512.11625v1) | Chang-Wei Lin, Yi-Ting Ma et al. | We demonstrate polarization-entangled biphotons in a cold-atom double-$Î›$ system, overcoming atomic selection rules that suppress polarization correlations and favor orbital angular momentum (OAM) entanglement. Using spatial light modulators, we coherently map a selected two-dimensional OAM subspace onto the polarization basis and thereby open an otherwise inaccessible polarization channel. Quantum-state tomography confirms that the mapping preserves the biphoton coherence. The four polarization Bell states are generated with fidelities of $92\text{-}94\%$ with few-percent statistical uncertainties, and an average Clauser-Horne-Shimony-Holt parameter of $S=2.44$ verifies the survival of nonlocal correlations. To the best of our knowledge, this work presents the first demonstration of OAM-to-polarization entanglement transfer in a cold-atom spontaneous four-wave mixing platform and establishes a practical interface for integrating atomic OAM resources with polarization-based quantum communication networks. |
| 2025-12-12 | [AI Benchmark Democratization and Carpentry](http://arxiv.org/abs/2512.11588v1) | Gregor von Laszewski, Wesley Brewer et al. | Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.   Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.   Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry. |
| 2025-12-12 | [The magnitude of the dark ages 21-cm signal in the context of existing early and late time constraints on $Î›$CDM](http://arxiv.org/abs/2512.11568v1) | H. T. J. Bevins | The dark ages 21-cm signal is a promising probe of the currently unobserved infant universe between the formation of the Cosmic Microwave Background around $z \approx 1100$ and the first galaxies around $z\approx 30$. A detection of the signal will help researchers understanding the nature of dark matter and dark energy, the expansion of the universe and any extensions to the concordance $Î›$CDM model that could explain the reported Cosmic Dawn 21-cm signal from EDGES and the Hubble tension. In this letter we take existing constraints on the $Î›$CDM cosmological model from two early time probes, Planck and WMAP, and two late time probes, DES galaxy lensing and clustering and Baryon Acoustic Oscillations, and propagate these through to constraints on the magnitude of the dark ages 21-cm signal. We constrain the magnitude and central frequency of the signal while methodically accounting for uncertainties in the cosmological parameters. We find that within the context of our modelling assumptions and the $Î›$CDM paradigm, the depth of the dark ages 21-cm signal is known to better than 1 mK and the central frequency to within 0.05 MHz. |
| 2025-12-11 | [Detection prospects for heavy WIMP dark matter near supermassive black holes, particularly in M31](http://arxiv.org/abs/2512.10923v1) | Andrei E. Egorov | This work analyzes the detection prospects for weakly interacting massive particles (WIMPs) in dark matter (DM) density spikes around nearby supermassive black holes (SMBHs) by observations in very high energy gamma-ray band. Such spikes are unique targets, which provide a possibility to discover the basic thermal s-wave annihilating WIMP with any mass up to the theoretical unitarity limit ~ 100 TeV. All relevant SMBHs were checked, and only MW* and M31* were identified as worthwhile objects. Cherenkov Telescope Array (CTA) sensitivity to heavy WIMPs in M31* was estimated. It was obtained that CTA will be able to probe a major part of TeV-scale WIMP parameter space in case of optimistic spike density configuration in M31*. In certain scenarios, M31* may yield even stronger constraints than MW*. Relevant systematic uncertainties were explored. |
| 2025-12-11 | [Shedding Light on Large Space-Based Telescopes: Modeling Stray Light due to Primary Mirror Damage from Micrometeoroid Impacts](http://arxiv.org/abs/2512.10915v1) | Megan T. Gialluca, Jonathan W. Arenberg et al. | A large space-based telescope aimed at detecting and characterizing the atmospheres of Earth-like planets orbiting Sun-like stars will require unprecedented contrast and stability. However, damage to the primary mirror due to micrometeoroid impacts will provide a stochastic, time-dependent source of stray light in the coronagraph's field of view that could significantly lengthen exposure times and reduce the expected science yield. To better quantify the impact of stray light and inform the Habitable Worlds Observatory mission design process, we present estimates of stray light in different micrometeoroid damage scenarios for a broad range of targets, and use that to find the expected decrease in science yield (i.e., the expected number of detected exoEarth candidates). We find that stray light due to micrometeoroid damage may significantly reduce yield, by 30% -- 60% in some cases, but significant uncertainties remain due to the unknown maximum expected impactor energy, and the relationship between impact energy and expected crater size. Micrometeoroid damage therefore needs further exploration, as it has the potential to reduce scientific yield, and in turn drive the development of mitigation strategies, selection of telescope designs, and selection of observing priorities in the future. |
| 2025-12-11 | [Spectral Theory of the Weighted Fourier Transform with respect to a Function in $\mathbb{R}^n$: Uncertainty Principle and Diffusion-Wave Applications](http://arxiv.org/abs/2512.10880v1) | Gustavo Dorrego, Luciano Luque | In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case in \cite{Dorrego}, to the $n$-dimensional Euclidean space $\mathbb{R}^{n}$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Î”_{Ï†,Ï‰})^{s}$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation, demonstrating that the fundamental solution can be expressed in terms of the Fox H-function, intrinsically related to the generalized $Ï‰$-Mellin transform introduced in \cite{Dorrego}. In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case, to the n-dimensional Euclidean space $\mathbb{R}^n$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Î”_{Ï†,Ï‰})^s$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation involving the weighted Hilfer derivative. We demonstrate that the fundamental solution can be explicitly expressed in terms of the Fox H-function, revealing an intrinsic connection with the generalized Mellin transform. |
| 2025-12-11 | [Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling](http://arxiv.org/abs/2512.10873v1) | Qitian Lu, Himanshu Sharma et al. | Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks. |
| 2025-12-11 | [Bayesian Symbolic Regression via Posterior Sampling](http://arxiv.org/abs/2512.10849v1) | Geoffrey F. Bomarito, Patrick E. Leser | Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications. |
| 2025-12-11 | [Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS](http://arxiv.org/abs/2512.10819v1) | Mohammed Aboelela | Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger. |
| 2025-12-11 | [AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators](http://arxiv.org/abs/2512.10773v1) | Samaksh Ujjawal, Shivansh Pratap Singh et al. | Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests. |
| 2025-12-11 | [Symbol-Level Precoding for Integrated Sensing and Covert Communication](http://arxiv.org/abs/2512.10752v1) | Yufei Wang, Qiang Li et al. | Integrated sensing and communication (ISAC) systems have emerged as a promising solution to improve spectrum efficiency and enable functional convergence. However, ensuring secure information transmission while maintaining high-quality sensing performance remains a significant challenge. In this paper, we investigate an integrated sensing and covert communication (ISCC) system, in which a base station (BS) simultaneously serves multiple downlink users and senses malicious targets that may act as both potential eavesdroppers (Eves) and wardens. We propose a novel symbol-level precoding (SLP)-based waveform design for ISCC that achieves covert communication intrinsically, without requiring additional transmission resources such as artificial noise. The proposed design integrates symbol shaping to enhance reliability for legitimate users and noise shaping to obscure transmission activities from the targets. For imperfect channel state information (CSI), the framework incorporates bounded uncertainty models for user channels and target angles, yielding a more robust design. The resulting ISCC waveform optimization problem is non-convex; to address this, we develop a low-complexity proximal distance algorithm (PDA) with closed-form updates under both PSK and QAM modulations. Simulation results demonstrate that the proposed method achieves superior covertness and sensing-communication performance with negligible degradation compared to traditional beamforming and conventional SLP approaches without noise-shaping mechanisms. |
| 2025-12-11 | [Efficient pulsar distance measurement with multiple nanohertz gravitational-wave sources](http://arxiv.org/abs/2512.10729v1) | Si-Ren Xiao, Ji-Yu Song et al. | In recent years, pulsar timing arrays (PTAs) have reported evidence for a nanohertz gravitational-wave (GW) background. As radio telescope sensitivity improves, PTAs are also expected to detect continuous gravitational waves from individual supermassive black hole binaries. Nanohertz GWs generate both Earth and pulsar terms in the timing data, and the time delay between the two terms encodes the pulsar distance. Precise pulsar distance measurements are critical to fully exploiting pulsar-term information, which can improve the measurement precision of GW sources' sky position parameters and thus enhance the GW sky-localization capability. In this work, we propose a new pulsar distance estimation method by using pulsar-term phase information from GWs. We construct two-dimensional distance posteriors for pulsar pairs based on the simulated GW signals and combine them to constrain individual pulsar distances. Compared with the existing one-dimensional method, our approach reduces the impact of source-parameter uncertainties on pulsar distance measurements. Considering four GW sources and a PTA of 20 pulsars with a white-noise level of 20 ns, we find that a significant fraction of pulsars at distances $\lesssim 1.4$ kpc can achieve sub-parsec distance precision over a 15-year observation. |
| 2025-12-11 | [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](http://arxiv.org/abs/2512.10720v1) | Lingjing Kong, Shaoan Xie et al. | Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems. |
| 2025-12-10 | [A Precise $Î±_s$ Determination from the R-improved QCD Static Energy](http://arxiv.org/abs/2512.09888v1) | Jose M. Mena-Valle | The strong coupling $Î±_s$ is extracted with high precision through fits to lattice-QCD data for the static energy. Our theoretical framework is based on R-improving the three-loop fixed-order prediction for the static energy: we remove the $u=1/2$ renormalon and resum the associated large infrared logarithms. Combined with radius-dependent renormalization scales (the so-called profile functions), this procedure extends the range of validity of perturbation theory to distances as large as $\sim 0.5\,$fm. In addition, we resum large ultrasoft logarithms to N$^3$LL accuracy using renormalization-group evolution. Since the standard four-loop R-evolution treats N$^4$LL and higher-order contributions asymmetrically, we also incorporate this potential source of bias in our analysis. Our estimate of the perturbative uncertainty is obtained through a random scan over the parameters controlling the profile functions and the implementation of R-evolution. We analyze how the extracted value of $Î±_s$ depends on the shortest and longest distances included in the fit, on the details of the R-evolution procedure, on the fitting strategy itself, and on the accuracy of ultrasoft resummation. From our final analysis, and after evolution to the $Z$ pole, we obtain $Î±^{(n_f=5)}_s(m_Z)=0.1170\pm 0.0009$, a result fully compatible with the world average and with a comparable uncertainty. |
| 2025-12-10 | [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](http://arxiv.org/abs/2512.09850v1) | Simone Cuonzo, Nina Deliu | We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.   We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees. |
| 2025-12-10 | [Kinematics of Distant Milky Way Halo RR Lyrae Stars out to 160 kpc](http://arxiv.org/abs/2512.09795v1) | Yuting Feng, Puragra Guhathakurta et al. | We present a kinematical study of the outer halo (r_GC approximately 60 to 160 kpc) of the Milky Way based on spectroscopy of 55 RR Lyrae stars obtained with the ESI instrument on the Keck II telescope. Our spectroscopic targets were selected from three photometric surveys: NGVS, DES, and Pan-STARRS1. We derive center-of-mass radial velocities with uncertainties of 6 to 35 km s^-1. The halo velocity dispersion measured from our sample is 70 plus/minus 7 km s^-1. The velocity field shows a possible dipole-like structure, with redshifted northern and blueshifted southern hemispheres. Fitting a Milky Way - Large Magellanic Cloud dipole perturbation model yields a weak or marginal dipole signal with amplitude -30 (+16, -20) km s^-1 and apex direction (l, b) = (-38.2 (+42.4, -31.5), -41.3 (+27.9, -23.8)) deg, along with a bulk compression velocity of -16 plus/minus 11 km s^-1. Although limited by sky coverage and sample size, our results are consistent with the presence of LMC-induced disequilibrium in the distant halo beyond 100 kpc. In addition to the 55 RR Lyrae stars, our spectroscopy reveals that 10 additional photometrically selected RR Lyrae candidates are actually quasar or blazar contaminants, highlighting the need for caution regarding such contaminants in sparsely sampled photometric surveys. Our study demonstrates that single-epoch spectroscopy of RR Lyrae stars is a viable method for probing the kinematics of the outer halo, and future surveys such as Rubin LSST and DESI-II have the potential to significantly advance this effort. |
| 2025-12-10 | [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](http://arxiv.org/abs/2512.09775v1) | Vladimir Balditsyn, Philippe Lalanda et al. | The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts. |
| 2025-12-10 | [First measurement of the absolute branching fractions of $Î£^+$ nonleptonic decays and test of the $Î”I = 1/2$ rule % $Î£^+ \to p Ï€^0$ and $Î£^+ \to n Ï€^+$](http://arxiv.org/abs/2512.09628v1) | BESIII Collaboration, M. Ablikim et al. | Based on $(10087 \pm 44) \times 10^6$ $J/Ïˆ$ events collected by the BESIII detector at the center-of-mass energy $\sqrt{s} = 3.097$ GeV, the first absolute measurement of the branching fractions for the decays $Î£^+ \to p Ï€^0$ and $Î£^+ \to n Ï€^+$ is performed. The branching fractions are determined to be $B_{Î£^+ \to p Ï€^0} = (49.79 \pm 0.06 \pm 0.22)\%$ and $B_{Î£^+ \to n Ï€^+} = (49.87 \pm 0.05 \pm 0.29)\%$, where the first uncertainties are statistical and the second systematic. These results show significant deviations from the PDG values for both decays, with differences of 4.4$Ïƒ$ for $Î£^+ \to p Ï€^0$ and 3.4$Ïƒ$ for $Î£^+ \to n Ï€^+$. Furthermore, the $Î”I = 1/2$ rule is tested in nonleptonic $Î£^\pm$ decays. The observed results deviate from zero by more than $5Ïƒ$, indicating the presence of the $Î”I = 3/2$ transition amplitude in the $Î£$ hyperon decays. |
| 2025-12-10 | [Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates](http://arxiv.org/abs/2512.09586v1) | Prashant Kumar Choudhary, Nouhaila Innan et al. | Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery. |
| 2025-12-10 | [Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis](http://arxiv.org/abs/2512.09576v1) | David Seu, Nicolas Longepe et al. | Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets. |
| 2025-12-10 | [Can Intense Quantum Light Beat Classical Uncertainty Relations?](http://arxiv.org/abs/2512.09558v1) | Felipe Reibnitz Willemann, Mauro Antezza et al. | Uncertainty relations are fundamental to quantum mechanics, encoding limits on the simultaneous measurement of conjugate observables. Violations of joint uncertainty bounds can certify entanglement -- a resource critical for quantum information protocols and increasingly relevant in strong-field physics. Here, we investigate the pairwise time-delay and frequency-bandwidth uncertainties for arbitrary multimode quantum states of light, deriving a general lower bound for their joint product. We find that the nonclassical correction scales inversely with the average photon number, a behavior rooted in the so-called ``monogamy of entanglement''. These results clarify the intensity scaling of quantum advantages in nonclassical light states and highlight the interplay between entanglement and photon statistics. |
| 2025-12-10 | [Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search](http://arxiv.org/abs/2512.09538v1) | Ekaterina Fadeeva, Maiya Goloburda et al. | Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance. |
| 2025-12-10 | [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](http://arxiv.org/abs/2512.09517v1) | Nabil Anan Orka, Ehtashamul Haque et al. | This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (Îµ = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis. |
| 2025-12-09 | [Update of the nonlocal sub-leading ${O}_1$ - ${O}_7$ contribution to $\bar B \to X_s Î³$ at LO](http://arxiv.org/abs/2512.08902v1) | Michael Benzke, Maria Vittoria Garzelli et al. | In all previous calculations of the non-local sub-leading contribution to the inclusive penguin decay $\bar B \to X_s Î³$ due to the interference of the electroweak operators ${O}_1^c$ - ${O}_{7Î³}$ the local Voloshin term was subtracted. In view of the ongoing analysis at order $Î±_s$, we present a calculation of the complete non-local contribution which takes into account the high correlation between the uncertainties of the local Voloshin and the non-local term of the previous analyses. |
| 2025-12-09 | [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](http://arxiv.org/abs/2512.08879v1) | Mohammad Abu-Shaira, Ajita Rattani et al. | Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression. |
| 2025-12-09 | [Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse](http://arxiv.org/abs/2512.08864v1) | Steve Barrett, Malcolm Murray et al. | Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone. |
| 2025-12-09 | [Prediction Intervals for Individual Treatment Effects in a Multiple Decision Point Framework using Conformal Inference](http://arxiv.org/abs/2512.08828v1) | Swaraj Bose, Walter Dempsey | Accurately quantifying uncertainty of individual treatment effects (ITEs) across multiple decision points is crucial for personalized decision-making in fields such as healthcare, finance, education, and online marketplaces. Previous work has focused on predicting non-causal longitudinal estimands or constructing prediction bands for ITEs using cross-sectional data based on exchangeability assumptions. We propose a novel method for constructing prediction intervals using conformal inference techniques for time-varying ITEs with weaker assumptions than prior literature. We guarantee a lower bound for coverage, which is dependent on the degree of non-exchangeability in the data. Although our method is broadly applicable across decision-making contexts, we support our theoretical claims with simulations emulating micro-randomized trials (MRTs) -- a sequential experimental design for mobile health (mHealth) studies. We demonstrate the practical utility of our method by applying it to a real-world MRT - the Intern Health Study (IHS). |
| 2025-12-09 | [Stationary Point Constrained Inference via Diffeomorphisms](http://arxiv.org/abs/2512.08735v1) | Michael Price, Debdeep Pati et al. | Stationary points or derivative zero crossings of a regression function correspond to points where a trend reverses, making their estimation scientifically important. Existing approaches to uncertainty quantification for stationary points cannot deliver valid joint inference when multiple extrema are present, an essential capability in applications where the relative locations of peaks and troughs carry scientific significance. We develop a principled framework for functions with multiple regions of monotonicity by constraining the number of stationary points. We represent each function in the diffeomorphic formulation as the composition of a simple template and a smooth bijective transformation, and show that this parameterization enables coherent joint inference on the extrema. This construction guarantees a prespecified number of stationary points and provides a direct, interpretable parameterization of their locations. We derive non-asymptotic confidence bounds and establish approximate normality for the maximum likelihood estimators, with parallel results in the Bayesian setting. Simulations and an application to brain signal estimation demonstrate the method's accuracy and interpretability. |
| 2025-12-09 | [$J/Ïˆ$-Meson Nucleon Scattering Length from Threshold Photoproduction on Light Nuclei](http://arxiv.org/abs/2512.08701v1) | Igor I. Strakovsky, William J. Briscoe et al. | The quality of recent SRC/CT Collaboration $J/Ïˆ$ photoproduction data off a $^4$He target from Hall~D at Jefferson Laboratory, combined with the feasibility of measuring the reaction close to the free-nucleon energy threshold, opens the door to using incoherent $J/Ïˆ$ photoproduction to access a variety of interesting physics aspects. An example is an estimate of the $J/Ïˆ~p$ scattering length $|Î±_{J/Ïˆ~p}|$ on the bound proton obtained using the Vector Meson Dominance model. This value can be compared with that of the free proton from the GlueX Collaboration. One may then project what would be expected from the SRC/CT Collaboration Experiment E12--25--002, which was recently approved by the JLab PAC. Using a plane-wave theoretical model to generate quasi-data, we find the experiment could achieve a result of $|Î±_{J/Ïˆ~p}| = 3.08\pm 0.45~\mathrm{mfm}$, an uncertainty competitive with that of the free-proton measurement. A comparison between the two would allow an evaluation of the effects of medium modification in the case of light nuclei. |
| 2025-12-09 | [Atomic and molecular systems for radiation thermometry](http://arxiv.org/abs/2512.08668v1) | Stephen P. Eckel, Eric B. Norrgard et al. | Atoms and simple molecules are excellent candidates for new standards and sensors because they are both all identical and their properties are determined by the immutable laws of quantum physics. Here, we introduce the concept of building a standard and sensor of radiative temperature using atoms and molecules. Such standards are based on precise measurement of the rate at which blackbody radiation (BBR) either excites or stimulates emission for a given atomic transition. We summarize the recent results of two experiments while detailing the rate equation models required for their interpretation. The cold atom thermometer (CAT) uses a gas of laser cooled $^{85}$Rb Rydberg atoms to probe the BBR spectrum near 130~GHz. This primary, {\it i.e.}, not traceable to a measurement of like kind, temperature measurement currently has a total uncertainty of approximately 1~\%, with clear paths toward improvement. The compact blackbody radiation atomic sensor (CoBRAS) uses a vapour of $^{85}$Rb and monitors fluorescence from states that are either populated by BBR or populated by spontaneous emission to measure the blackbody spectrum near 24.5~THz. The CoBRAS has an excellent relative precision of $u(T)\approx 0.13$~K, with a clear path toward implementing a primary |
| 2025-12-09 | [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](http://arxiv.org/abs/2512.08656v1) | Lauritz Rismark Fosso, Herman BiÃ¸rn Amundsen et al. | Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions. |
| 2025-12-09 | [Measurement of the differential $t$-channel production cross-section of single top quarks and top antiquarks at $\sqrt{s} = $13 TeV with the ATLAS detector](http://arxiv.org/abs/2512.08573v1) | Maren Stratmann | Differential production cross-sections of single top quarks and top antiquarks are measured in proton-proton collisions at a centre-of-mass energy $\sqrt{s} = $13 TeV. The full Run-2 dataset collected by the ATLAS detector at the LHC in the years 2015-2018 is used. The differential cross-sections are measured as a function of the transverse momentum and absolute rapidity of the top (anti)quark. The measurement results are compared to predictions obtained from fixed order calculations, different matrix-element event generators and different parton distribution function sets. The results agree with the theoretical predictions within the measurement uncertainties. An effective field theory interpretation of the measurement sets constraints on the contribution of the four-fermion operator $O_{Qq}^{3,1}$. |
| 2025-12-09 | [Astrometric Reconnaissance of Exoplanetary Systems (ARES). I. Methodology validation with HST point-source images of Proxima Centauri](http://arxiv.org/abs/2512.08533v1) | M. Libralato, L. Bedin et al. | We present the first results of the Astrometric Reconnaissance of Exoplanetary Systems (ARES) project, aimed at validating and characterizing candidate exoplanets around the nearest systems using multi-epoch Hubble Space Telescope (HST) data. In this first paper, we focus on Proxima Centauri, leveraging archival and recent HST observations in point-source imaging mode. We refine the geometric-distortion calibration of the HST detector used, and develop a robust methodology to derive high-precision astrometric parameters by combining HST measurements with the Gaia DR3 catalog. We determine Proxima's position, proper motion, and parallax with uncertainties at the $\sim$0.4-mas, 50-$Î¼$as yr$^{-1}$, and 0.2-mas level, respectively, achieving consistent results with what measured by Gaia within $\sim$1$Ïƒ$. We further investigate the presence of the candidate exoplanet Proxima c by analyzing the proper-motion anomaly derived from combining long-term HST-based and short-term Gaia astrometry. Under the assumption of a circular, face-on orbit, we obtain an estimated mass of $m_c = 3.4^{+5.2}_{-3.4}$ $M_\odot$, broadly consistent with radial-velocity constraints but limited by our current uncertainties. These results establish the foundation for the next phase of ARES, which will exploit HST spatial-scanning observations to achieve astrometric precisions of a few tens of $Î¼$as and enable a direct search for astrometric signatures of low-mass companions. |
| 2025-12-08 | [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](http://arxiv.org/abs/2512.07795v1) | Nearchos Potamitis, Lars Klein et al. | Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench . |
| 2025-12-08 | [Distribution-informed Online Conformal Prediction](http://arxiv.org/abs/2512.07770v1) | Dongjian Hu, Junxi Wu et al. | Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines. |
| 2025-12-08 | [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](http://arxiv.org/abs/2512.07756v1) | Mayank Anand, Ujair Alam et al. | Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM. |
| 2025-12-08 | [Symmetric Vaccine Efficacy](http://arxiv.org/abs/2512.07739v1) | Lucy D'Agostino McGowan, Sarah C. Lotspeich et al. | Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package. |
| 2025-12-08 | [Time-asymptotic behavior of the Boltzmann equation with random inputs in whole space and its stochastic Galerkin approximation](http://arxiv.org/abs/2512.07735v1) | Shi Jin, Qi Shao et al. | We consider the Boltzmann equation with random uncertainties arising from the initial data and collision kernel in the {\it whole space}, along with their stochastic Galerkin (SG) approximations. By employing Green's function method, we show that, the higher-order derivatives of the solution with respect to the random variable exhibit polynomial decay over time. These results are then applied to analyze the SG method for the SG system and to demonstrate the polynomial decay of the numerical error over time. |
| 2025-12-08 | [Decomposition Sampling for Efficient Region Annotations in Active Learning](http://arxiv.org/abs/2512.07606v1) | Jingna Qiu, Frauke Wilm et al. | Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git. |
| 2025-12-08 | [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](http://arxiv.org/abs/2512.07564v1) | Kassoum Sanogo, Renzo Ardiccioni | Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems. |
| 2025-12-08 | [Data-Driven Robust Safety Verification for Markov Decision Processes](http://arxiv.org/abs/2512.07550v1) | Abhijit Mazumdar, Manuela L. Bujorianu et al. | In this paper, we propose a data-driven robust safety verification framework for stochastic dynamical systems modeled as Markov decision processes with time-varying and uncertain transition probabilities. Rather than assuming access to the exact nominal transition kernel, we consider the realistic setting where only samples from multiple system executions are available. These samples may correspond to different transition models inside an ambiguity set around the nominal transition kernel. Using these observations, we construct a unified ambiguity set that captures both inherent run-to-run variability in the transition dynamics and finite-sample statistical uncertainty. This ambiguity set is formalized through a Wasserstein-distance ball around a nominal empirical distribution and naturally induces an interval Markov decision process representation of the underlying system. Within this representation, we introduce a robust safety function that characterizes reach-avoid type probabilistic safety under all transition kernels consistent with the interval Markov decision process. We further derive high-confidence safety guarantees for the true, unknown time-varying system. A numerical example illustrates the applicability and effectiveness of the proposed approach. |
| 2025-12-08 | [XMCQDPT2-Fidelity Transfer-Learning Potentials and a Wavepacket Oscillation Model with Power-Law Decay for Ultrafast Photodynamics](http://arxiv.org/abs/2512.07537v1) | Ivan V. Dudakov, Pavel M. Radzikovitsky et al. | A central pursuit in theoretical chemistry is the accurate simulation of photochemical reactions, which are governed by nonadiabatic transitions through conical intersections. Machine learning has emerged as a transformative tool for constructing the necessary potential energy surfaces, but applying it to excited states faces a fundamental barrier: the cost of generating high-level quantum chemistry data. We overcome this challenge by developing machine-learning interatomic potentials (MLIPs) that achieve multi-state multi-reference perturbation theory accuracy through various techniques, such as transfer, multi-state, and $Î”$-learning. Applied to the methaniminium cation, our highest-fidelity transfer-learning model uncovers its complete photodissociation landscape following S$_2$ photoexcitation. The comprehensive XMCQDPT2/SA(3)-CASSCF(12,12) electronic structure description captures all competing decay channels, including S$_1$ branching into photoisomerization and direct H$_2$-loss pathways. Our results show that the population dynamics generally depends on the MLIP model, correlating with its performance. At the same time, the introduction of MLIP-uncertainty corrections based on the predictions of an ensemble of models brings different approaches into agreement, validating this metric as essential for reliable dynamics. To interpret the population dynamics, we introduce a wavepacket oscillation model - a mechanistically transparent, power-law kinetics framework that extracts state-specific lifetimes directly from first-principles simulations. The model quantitatively reproduces the ultrafast decay, creating a direct link between quantum transition probabilities and classical rate constants. The kinetic fits yield channel-specific lifetimes, supporting the recently discovered photochemical pathway mediated by a novel $ÏƒÏ€^*/S_0$ conical intersection. |
| 2025-12-08 | [Precision Higgs Probe of Type-II Seesaw](http://arxiv.org/abs/2512.07532v1) | Saiyad Ashanujjaman, P. S. Bhupal Dev et al. | Despite direct searches at the LHC excluding triplet-like Higgs bosons up to several hundred GeV over much of the type-II seesaw model parameter space, parts of it -- most notably those featuring ``cascade decays'' of the charged Higgs bosons into their neutral partners and off-shell $W$ bosons -- still remain unconstrained. Meanwhile, measurements of the diphoton signal strength of the Standard Model (SM) Higgs boson -- potentially modified by loop contributions from triplet-like Higgs states -- are in good agreement with the SM expectation, with combined experimental uncertainties currently at approximately 8\%. Given the trend in previous measurements, it is expected that future precision Higgs measurements at the HL-LHC and a future lepton collider such as CEPC, FCC-ee, or Muon Collider will be consistent the standard diphoton signal strength, albeit with significantly reduced uncertainties, down to about 0.7\%. Presuming this and considering all relevant constraints, we explore whether such increasingly precise diphoton measurements can indirectly probe the parameter space that currently evades direct searches. We find that sub-percent-level determinations of the diphoton rate will decisively probe a substantial fraction of this otherwise elusive region. |
| 2025-12-05 | [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](http://arxiv.org/abs/2512.05954v1) | Shima Imani, Seungwhan Moon et al. | We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems |
| 2025-12-05 | [Constraining r-process nucleosynthesis via enhanced accuracy neutron-capture experiments](http://arxiv.org/abs/2512.05944v1) | C. Domingo-Pardo, C. Lederer-Woods et al. | The isotopic abundances of r-process elements in the solar system are traditionally derived as residuals from the subtraction of s-process contributions from total solar abundances. However, the uncertainties in s-process nucleosynthesis -particularly those arising from Maxwellian Averaged Cross Sections (MACS)- propagate directly into the r-process residuals, affecting their reliability. Building upon the seminal work of Goriely (1999), who introduced a multi-event s-process model to quantify these uncertainties, we revisit the problem using a simplified yet effective approach. By assuming that the relative uncertainty in s-process isotopic abundances scales linearly with the MACS uncertainties from data libraries (KADoNiS), we identify a subset of isotopes for which the r-process residuals remain significantly uncertain. Using updated solar abundances (Lodders 2025) and s-process contributions from Bisterzo et al. (2014), we present a short list of isotopes that are prime candidates for improved (n,g) measurements at CERN n_TOF in the near future. Our analysis provides a practical framework for prioritizing future experimental efforts that will profit from upgrades and enhancements of the n_TOF facility. It also highlights the need to revisit key neutron-capture cross sections to refine our understanding of the r-process isotopic abundance pattern, commonly used as a benchmark in stellar models of explosive nucleosynthesis. |
| 2025-12-05 | [Adsorption energies are necessary but not sufficient to identify good catalysts](http://arxiv.org/abs/2512.05938v1) | Shahana Chatterjee, Alexander Davis et al. | As a core technology for green chemical synthesis and electrochemical energy storage, electrocatalysis is central to decarbonization strategies aimed at combating climate change. In this context, computational and machine learning driven catalyst discovery has emerged as a major research focus. These approaches frequently use the thermodynamic overpotential, calculated from adsorption free energies of reaction intermediates, as a key parameter in their analysis. In this paper, we explore the large-scale applicability of such overpotential estimates for identifying good catalyst candidates by using datasets from the Open Catalyst Project (OC20 and OC22). We start by quantifying the uncertainty in predicting adsorption energies using \textit{ab initio} methods and find that $\sim$0.3-0.5 eV is a conservative estimate for a single adsorption energy prediction. We then compute the overpotential of all materials in the OC20 and OC22 datasets for the hydrogen and oxygen evolution reactions. We find that while the overpotential allows the identification of known good catalysts such as platinum and iridium oxides, the uncertainty is large enough to misclassify a broad fraction of the datasets as ``good'', which limits its value as a screening criterion. These results question the reliance on overpotential estimation as a primary evaluation metric to sort through catalyst candidates and calls for a shift in focus in the computational catalysis and machine learning communities towards other metrics such as synthesizability, stability, lifetime or affordability. |
| 2025-12-05 | [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](http://arxiv.org/abs/2512.05930v1) | Shima Imani, Seungwhan Moon et al. | Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities. |
| 2025-12-05 | [World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty](http://arxiv.org/abs/2512.05927v1) | Zhiting Mei, Tenny Yin et al. | Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection. |
| 2025-12-05 | [The Bayesian Way: Uncertainty, Learning, and Statistical Reasoning](http://arxiv.org/abs/2512.05883v1) | Juan Sosa, Carlos A. MartÃ­nez et al. | This paper offers a comprehensive introduction to Bayesian inference, combining historical context, theoretical foundations, and core analytical examples. Beginning with Bayes' theorem and the philosophical distinctions between Bayesian and frequentist approaches, we develop the inferential framework for estimation, interval construction, hypothesis testing, and prediction. Through canonical models, we illustrate how prior information and observed data are formally integrated to yield posterior distributions. We also explore key concepts including loss functions, credible intervals, Bayes factors, identifiability, and asymptotic behavior. While emphasizing analytical tractability in classical settings, we outline modern extensions that rely on simulation-based methods and discuss challenges related to prior specification and model evaluation. Though focused on foundational ideas, this paper sets the stage for applying Bayesian methods in contemporary domains such as hierarchical modeling, nonparametrics, and structured applications in time series, spatial data, networks, and political science. The goal is to provide a rigorous yet accessible entry point for students and researchers seeking to adopt a Bayesian perspective in statistical practice. |
| 2025-12-05 | [Safe Output Regulation of Coupled Hyperbolic PDE-ODE Systems](http://arxiv.org/abs/2512.05822v1) | Ji Wang, Miroslav Krstic | This paper presents a safe output regulation control strategy for a class of systems modeled by a coupled $2\times 2$ hyperbolic PDE-ODE structure, subject to fully distributed disturbances throughout the system. A state-feedback controller is developed by the {nonovershooting backstepping} method to simultaneously achieve exponential output regulation and enforce safety constraints on the system output, which is the state furthest from the control input. To handle unmeasured PDE states and external disturbances, a state observer and a disturbance estimator are designed. Explicit bounds on the estimation errors are derived and used to construct a robust safe regulator that accounts for the uncertainties. The proposed control scheme guarantees that: 1) If the system output is initially within the safe region, it remains there; otherwise, it will be rescued to the safety within a prescribed time; 2) The output tracking error converges to zero exponentially; 3) The observer accurately estimates both the distributed states and external disturbances, with estimation errors converging to zero exponentially; 4) All signals in the closed-loop system remain bounded. The effectiveness of the proposed method is demonstrated through a UAV delivery scenario with a cable-suspended payload, where the payload is regulated to track a desired reference while avoiding collisions with barriers. |
| 2025-12-05 | [UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection](http://arxiv.org/abs/2512.05814v1) | Fubao Zhu, Zhanyuan Jia et al. | Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy. |
| 2025-12-05 | [Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling](http://arxiv.org/abs/2512.05809v1) | Saurav Jha, M. Jehanzeb Mirza et al. | Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney. |
| 2025-12-05 | [SRG/eROSITA-SDSS view on the relation between X-ray and UV emission for quasars](http://arxiv.org/abs/2512.05807v1) | S. A. Prokhorenko, S. Yu. Sazonov et al. | Motivated by the idea of using quasars as standardizable candles for cosmology, we examine the relation between X-ray (at 2 keV, $L_{\rm 2keV}$) and ultraviolet (at 2500 Angstrom, $L_{\rm 2500}$) monochromatic luminosities of quasars using a sample of 2414 X-ray sources from the SRG/eROSITA all-sky survey cross-matched with the Sloan Digital Sky Survey data release 16 quasar catalogue (SDSS DR16Q), at redshifts between 0.5 and 2.5. These objects are bright both in X-rays and in the optical, so that the sample is characterized by nearly 100% statistical completeness. We have developed a new method for determining the $L_{\rm 2keV}-L_{\rm 2500}$ relation, which consistently takes into account (i) X-ray and UV flux limited object selection, (ii) X-ray and UV variability of quasars, and (iii) the decreasing space density of quasars with increasing luminosity. Assuming a linear relation between $l_{\rm X}\equiv\log(L_{\rm 2keV}/[{\rm erg\,s^{-1}\,Hz^{-1}}])$ and $l_{\rm UV}\equiv\log(L_{\rm 2500}/[{\rm erg\,s^{-1}\,Hz^{-1}}])$, we find the slope, $Î³=0.69\pm0.02$ (hereafter all uncertainties are quoted at the 68% confidence level), and normalization, $l_{\rm X}=26.45\pm0.02$ at $l_{\rm UV}=30.5$, of the $L_{\rm 2keV}$ ($L_{\rm 2500}$) dependence. These values are not substantially different from the results of previous studies. A key novel aspect of our work is allowance for intrinsic scatter (which adds to the dispersion induced by quasar variability and flux measurement uncertainties) of the $L_{\rm 2keV}-L_{\rm 2500}$ relation in both variables, i.e. in X-ray and UV luminosity. The intrinsic X-ray scatter ($Ïƒ^2_{\rm intX}=0.066\pm0.005$) strongly dominates over the UV one ($Ïƒ^2_{\rm intUV}=0.001^{+0.003}_{-0.001}$). Further studies should seek to explain this behaviour in terms of accretion onto supermassive black holes and orientation of quasars with respect to the observer. |
| 2025-12-04 | [Measurement of the branching fractions and longitudinal polarisations of $B^0_{(s)} \to K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}$ decays](http://arxiv.org/abs/2512.05102v1) | LHCb collaboration, R. Aaij et al. | A time- and flavour-integrated amplitude analysis of $B^0$ and $B^0_s$ decays to the $(K^+Ï€^-)(K^-Ï€^+)$ final state in the $K^*(892)^0 \kern 0.18em \overline{\kern -0.18em K}{}^{*}(892)^0$ region is presented, using $pp$ collision data recorded with the LHCb detector in 2011--2018, corresponding to an integrated luminosity of $9\,\text{fb}^{-1}$. The branching fractions of the $B^0$ and $B^0_s$ decays are measured relative to the $B^0 \to D^-Ï€^+$ and $B^0_s \to D^-_s Ï€^+$ modes, respectively. The corresponding longitudinal polarisation fractions are found to be $f_L^{d} = 0.600 \pm 0.022 \pm 0.017$ and $f_L^{s} = 0.159 \pm 0.010 \pm 0.007$, where the uncertainties are statistical and systematic, respectively. The theory-motivated $L_{K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}}$ observable is found to be $L_{K^{*0} \kern 0.18em \overline{\kern -0.18em K}{}^{*0}} = 4.92 \pm 0.55 \pm 0.47 \pm 0.02 \pm 0.10$, where the uncertainties are statistical, systematic, due to uncertainty of external mass and lifetime measurements, and due to knowledge of the fragmentation fraction ratio, respectively. This confirms the previously reported tension between experimental determinations and theoretical predictions of longitudinal polarisation in $B \to VV$ decays. |
| 2025-12-04 | [PSR J0952-0607: Tightening a Record-High Neutron Star Mass](http://arxiv.org/abs/2512.05099v1) | Roger W. Romani, Maya Beleznay et al. | We report on new orbit-minimum photometry and revised radial-velocity fitting that provide an improved measurement of the mass of the neutron star (NS) in pulsar PSR~J0952$-$0607 at $M_NS = 2.35\pm 0.11 M_\odot$. With its fast spin and unusually low magnetic field, this NS has evidently experienced unusual evolution, likely connected with its high mass, which is now $2.5Ïƒ$ above that of the heaviest pulsar with a white dwarf companion, as measured by Shapiro delay techniques. By tightening the mass measurement, we also raise the maximum (commonly called Tolman-Oppenheimer-Volkoff) NS mass to $M_{\rm TOV} > 2.27\,M_\odot$$(2.12\,M_\odot)$ at $1Ïƒ$$(3Ïƒ)$ confidence, which improves bounds on the dense-matter equation of state. While the statistical error decreases and systematic issues should be modest, uncertainties remain; we comment briefly on these factors and prospects for further improvement. |
| 2025-12-04 | [Model-Free Assessment of Simulator Fidelity via Quantile Curves](http://arxiv.org/abs/2512.05024v1) | Garud Iyengar, Yu-Shiou Willy Lin et al. | Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs. |
| 2025-12-04 | [Internal superfluid response and torque evolution in the giant glitch of PSR J1718-3718](http://arxiv.org/abs/2512.04972v1) | Peng Liu, Zhonghao Tu et al. | We investigate the post-glitch rotational evolution of pulsars by analyzing the 2007 giant glitch of PSR J1718$-$3718 using a vortex creep model that incorporates both inward and outward nonlinear vortex motion, along with a time-varying external torque. A comprehensive fitting framework is developed, constrained by prior knowledge of moment of inertia participation from previous glitch studies. We apply a Markov Chain Monte Carlo approach to quantify uncertainties and parameter correlations. The model reproduces the observed timing data and yields physically consistent values for moment of inertia fractions and creep timescales. Our results indicate that inward creep and a long-term change in external torque dominate the observed increase in spin-down rate, pointing to structural changes within the star-likely triggered by a crustquake that initiated both vortex motion and a change in the moment of inertia. We estimate that the glitch involved approximately $2.4 \times 10^{12}$ inward-moving vortices and $\sim 142$ crustal plates with a typical size of $\sim 0.03$ km. This study demonstrates that detailed post-glitch modeling of sparse timing data can simultaneously constrain internal superfluid dynamics and external torque evolution, providing a quantitative framework to probe the structural properties of neutron star interiors. |
| 2025-12-04 | [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](http://arxiv.org/abs/2512.04938v1) | Raquel Norel, Michele Merler et al. | Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally. |
| 2025-12-04 | [First observation and measurement of the ${}^{198}\text{Hg}$ bosonic transition in an optical lattice clock](http://arxiv.org/abs/2512.04920v1) | Clara Zyskind, Thomas LauprÃªtre et al. | We report the first observation of the magnetic-field-induced (5d10 6s2)1S0-(5d10 6s6p)3P0 transition in a bosonic isotope of mercury, 198Hg, realized in an optical lattice clock. We characterize this new isotope, determining key features such as the quadratic Zeeman shift, the probe light shift, and the magic frequency. We also report a first comparison between the 198Hg optical lattice clock and 87Sr. In this comparison, the 198Hg clock has a relative frequency stability of 6x10-16/sqrt(tau/s) and a total relative systematic uncertainty of 6.9x10-16. This comparison yields the first direct determination of the 198Hg/87Sr optical frequency ratio: 198Hg/87Sr = 2.629 315 734 684 118 1, with the same relative uncertainty. |
| 2025-12-04 | [Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty](http://arxiv.org/abs/2512.04918v1) | Kailiang Liu, Ying Chen et al. | Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling. |
| 2025-12-04 | [Bayesian stepwise estimation of qubit rotations](http://arxiv.org/abs/2512.04898v1) | Mylenne Manrique, Marco Barbieri et al. | This work investigates Bayesian stepwise estimation (Se) for measuring the two parameters of a unitary qubit rotation. While asymptotic analysis predicts a precision advantage for SE over joint estimation (JE) in regimes where the quantum Fisher information matrix is near-singular ("sloppy" models), we demonstrate that this advantage is mitigated within a practical Bayesian framework with limited resources. We experimentally implement a SE protocol using polarisation qubits, achieving uncertainties close to the classical Van Trees bounds. However, comparing the total error to the ultimate quantum Van Trees bound for JE reveals that averaging over prior distributions erases the asymptotic SE advantage. Nevertheless, the stepwise strategy retains a significant practical benefit as it operates effectively with simple, fixed measurements, whereas saturating the JE bound typically requires complex, parameter-dependent operations. |
| 2025-12-04 | [Searching for new physics with $^{136}$Xe double beta decay spectrum in PandaX-4T](http://arxiv.org/abs/2512.04849v1) | PandaX Collaboration, Zhe Yuan et al. | The continuous spectrum of double beta decay ($Î²Î²$) provides a sensitive probe to test the predictions of the Standard Model and to search for signatures of new physics beyond it. We present a comprehensive analysis of the $^{136}$Xe $Î²Î²$ spectrum utilizing $37.8 \pm 0.6$ kg$\cdot$yr of $^{136}$Xe exposure from the PandaX-4T experiment. The analysis yields the most precise measurement to date of the $^{136}$Xe two-neutrino double beta decay ($2Î½Î²Î²$) half-life, $(2.14 \pm 0.05) \times 10^{21}$ years, the uncertainty of which is reduced by a factor of two compared to our previous result. We measure the parameter $Î¾_{31}^{2Î½}$, defined as the ratio between the subleading and leading components of the $^{136}$Xe $2Î½Î²Î²$ nuclear matrix element, to be $0.59^{+0.41}_{-0.38}$, which is consistent with theoretical predictions. We also search for Majoron-emitting modes of $^{136}$Xe $Î²Î²$, establishing the most stringent limit for the spectral index $n=7$. |
| 2025-12-04 | [aim-resolve: Automatic Identification and Modeling for Bayesian Radio Interferometric Imaging](http://arxiv.org/abs/2512.04840v1) | Richard Fuchs, Jakob KnollmÃ¼ller et al. | Modern radio interferometers deliver large volumes of data containing high-sensitivity sky maps over wide fields-of-view. These large area observations can contain various and superposed structures such as point sources, extended objects, and large-scale diffuse emission. To fully realize the potential of these observations, it is crucial to build appropriate sky emission models which separate and reconstruct the underlying astrophysical components. We introduce aim-resolve, an automatic and iterative method that combines the Bayesian imaging algorithm resolve with deep learning and clustering algorithms in order to jointly solve the reconstruction and source extraction problem. The method identifies and models different astrophysical components in radio observations while providing uncertainty quantification of the results. By using different model descriptions for point sources, extended objects, and diffuse background emission, the method efficiently separates the individual components and improves the overall reconstruction. We demonstrate the effectiveness of this method on synthetic image data containing multiple different sources. We further show the application of aim-resolve to an L-band (856 - 1712 MHz) MeerKAT observation of the radio galaxy ESO 137-006 and other radio galaxies in that environment. We observe a reasonable object identification for both applications, yielding a clean separation of the individual components and precise reconstructions of point sources and extended objects along with detailed uncertainty quantification. In particular, the method enables the creation of catalogs containing source positions and brightnesses and the corresponding uncertainties. The full decoupling of sky emission model and instrument response makes the method applicable to a wide variety of instruments or wavelength bands. |
| 2025-12-03 | [The effect of baryons on the positions and velocities of satellite galaxies in the MTNG simulation](http://arxiv.org/abs/2512.04080v1) | Sergio Contreras, Raul E. Angulo et al. | Mock galaxy catalogues are often constructed from dark-matter-only simulations based on the galaxy-halo connection. Although modern mocks can reproduce galaxy clustering to some extent, the absence of baryons affects the spatial and kinematic distributions of galaxies in ways that remain insufficiently quantified. We compare the positions and velocities of satellite galaxies in the MTNG hydrodynamic simulation with those in its dark-matter-only counterpart, assessing how baryonic effects influence galaxy clustering and contrasting them with the impact of galaxy selection, i.e. the dependence of clustering on sample definition. Using merger trees from both runs, we track satellite subhaloes until they become centrals, allowing us to match systems even when their z=0 positions differ. We then compute positional and velocity offsets as functions of halo mass and distance from the halo centre, and use these to construct a subhalo catalogue from the dark-matter-only simulation that reproduces the galaxy distribution in the hydrodynamic run. Satellites in the hydrodynamic simulation lie 3-4% closer to halo centres than in the dark-matter-only case, with an offset that is nearly constant with halo mass and increases toward smaller radii. Satellite velocities are also systematically higher in the dark-matter-only run. At scales of 0.1 Mpc/h, these spatial and kinematic differences produce 10-20% variations in clustering amplitude -corresponding to 1-3$Ïƒ$ assuming DESI-like errors- though the impact decreases at larger scales. These baryonic effects are relevant for cosmological and lensing analyses and should be accounted for when building high-fidelity mocks. However, they remain smaller than the differences introduced by galaxy selection, which thus represents the dominant source of uncertainty when constructing mocks based on observable quantities. |
| 2025-12-03 | [Learning Steerable Clarification Policies with Collaborative Self-play](http://arxiv.org/abs/2512.04068v1) | Jonathan Berant, Maximillian Chen et al. | To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time. |
| 2025-12-03 | [On topological and algebraic structures of categorical random variables](http://arxiv.org/abs/2512.04020v1) | Inocencio Ortiz, Santiago GÃ³mez-Guerrero et al. | Based on entropy and symmetrical uncertainty (SU), we define a metric for categorical random variables and show that this metric can be promoted into an appropriate quotient space of categorical random variables. Moreover, we also show that there is a natural commutative monoid structure in the same quotient space, which is compatible with the topology induced by the metric, in the sense that the monoid operation is continuous. |
| 2025-12-03 | [Physics-Embedded Gaussian Process for Traffic State Estimation](http://arxiv.org/abs/2512.04004v1) | Yanlin Chen, Kehua Chen et al. | Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE. |
| 2025-12-03 | [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](http://arxiv.org/abs/2512.03992v1) | Zexin Lin, Hawen Wan et al. | Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments. |
| 2025-12-03 | [Applied Neural Network-Based Active Control for Vortex-Induced Vibrations Suppression in a Two-Degree-of-Freedom Cylinder](http://arxiv.org/abs/2512.03990v1) | Soha Ilbeigi, Ashkan Bagherzadeh et al. | Vortex-Induced Vibrations (VIVs) of cylindrical structures present significant challenges in various engineering applications, including marine risers, tall buildings, and renewable energy systems. Hence, it is vital to control Vortex-Induced Vibrations of cylindrical structures. For this purpose, in this study a novel approach is introduced to VIV control, based on a model-based active control strategy integrated with a Neural Network (NN) in the presence of uncertainty modeling. The proposed method utilizes a closed-loop control system, where feedback from the system's dynamic state is used to generate adaptive control commands, enabling the system to respond to changing flow conditions and nonlinearities. Then, the controllability analysis is conducted to assess the efficiency of the control strategy in mitigating VIV. Two control approaches are implemented: simple learning and composite learning. Both strategies significantly enhance vibration suppression, achieving up to 99% reduction in vibrations despite uncertainties in the system. The results demonstrate the potential of the proposed method to enhance the efficiency, stability, and lifespan of structures subject to VIV. |
| 2025-12-03 | [Performance and efficiency of a transformer-based quark/gluon jet tagger in the ATLAS experiment](http://arxiv.org/abs/2512.03949v1) | ATLAS Collaboration | A deep-learning approach based on the transformer architecture is developed to distinguish between jets originating from quarks and gluons. The algorithm operates on jets with transverse momentum $p_{\text{T}} > 20$ and pseudorapidity $|Î·| < 4.5$ and takes as input several properties derived from the jet constituents, using information from the ATLAS detector's tracker and calorimeter. The algorithm's performance is evaluated by analyzing dijet data events from proton-proton collisions at $\sqrt{s} = 13$ and $13.6$ TeV during Run 2 and Run 3 of the Large Hadron Collider. Two methods are used to obtain distributions from quark- or gluon-initiated jets in data: a matrix method fully based on Monte Carlo simulation and a new approach named `jet topics' which has less dependence on the modelling of the physics process under study. The quark and gluon identification efficiencies measured in data for the 50% quark-identification-efficiency working point vary from the simulated ones for quark-initiated (gluon-initiated) jets by factors of 0.88-1.30 (0.61-1.05) with uncertainties of 10%-70% (10%-95%). The uncertainties estimated with the jet topics method are smaller than those estimated with the matrix method, with up to 20% less systematic uncertainty in some phase-space regions. The advances in jet identification reported here provide a robust tool for precision Standard Model measurements and searches for new physics at the LHC. |
| 2025-12-03 | [Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response](http://arxiv.org/abs/2512.03936v1) | Aron Distelzweig, Yiwei Wang et al. | Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks. |
| 2025-12-03 | [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](http://arxiv.org/abs/2512.03928v1) | Michele Alessi, Alessio Ansuini et al. | We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors. |
| 2025-12-03 | [Towards Quantum Stochastic Optimization for Energy Systems under Uncertainty: Joint Chance Constraints with Quantum Annealing](http://arxiv.org/abs/2512.03925v1) | David Ribes, Tatiana Gonzalez Grandon | Uncertainty is fundamental in modern power systems, where renewable generation and fluctuating demand make stochastic optimization indispensable. The chance constrained unit commitment problem (UCP) captures this uncertainty but rapidly becomes computationally challenging as the number of scenarios grows. Quantum computing has been proposed as a potential route to overcome such scaling barriers. In this work, we evaluate the applicability of quantum annealing platforms to the chance constrained UCP. Focusing on a scenario approximation, we reformulated the problem as a mixed integer linear program and solved it using DWave hybrid quantum classical solver alongside Gurobi. The hybrid solver proved competitive under strict runtime limits for large scenario sets (15,000 in our experiments), while Gurobi remained superior on smaller cases. QUBO reformulations were also tested, but current annealers cannot accommodate stochastic UCPs due to hardware limits, and deterministic cases suffered from embedding overhead. Our study delineates where chance constrained UCPs can already be addressed with hybrid quantum classical methods, and where current quantum annealers remain fundamentally limited. |
| 2025-12-02 | [Radiative decays of the $Î£_c$, $Îž'_c$ and $Î©_c$ charmed baryons](http://arxiv.org/abs/2512.03008v1) | A. DÃ¡vila-Rivera, H. GarcÃ­a-Tecocoatzi et al. | In this work, we study the radiative decays of the $Î£_c$, $Îž'_c$ and $Î©_c$ charmed baryons, which belong to the flavor sextet ($\bf {6}_{\rm F}$), within the constituent quark model formalism. The electromagnetic transitions are calculated from the ground and $P$-wave states to ground states, as well as from the second shell states to both the ground and $P$-wave final states. These decays play a crucial role in confirming the existence of certain resonances when strong decays are not allowed. %A relevant case is the $Î©_c^{0}$ spin excitation with $\bf J^P ={\frac{3}{2}}^{+}$ denoted as $Î©_c^{*0}$. This state cannot decay strongly, but has a nonvanishing predicted electromagnetic decay width in the $Î©_c^{0} Î³$ channel. The $Î©_c^{*0}$ was precisely observed in this channel. Moreover, electromagnetic decay widths are particularly useful for identifying resonances when states have the same mass and total decay width. A relevant case is the $Î©_c (3327)$ state, whose branching ratios between the strong decay channels are comparable; thus, the radiative decay widths may help assign this state. We also make the assignment of the recently discovered $Îž'_c(2923)^{+}$ baryon, which is consistent with being the isospin partner of the $Îž'_c(2923)^{0}$. This study presents, for the first time, the calculation of electromagnetic decays for $D_Ï$-wave states, $Ï-Î»$ mixed states, and $Ï$-mode radially excited states in the charm sector. Throughout our calculations, we account for uncertainties arising from both experimental and model-dependent errors. |
| 2025-12-02 | [MIRI spectrophotometry of GN-z11: Detection and nature of an optical red continuum component](http://arxiv.org/abs/2512.02997v1) | A. Crespo GÃ³mez, L. Colina et al. | We present new MIRI F560W, F770W and F1000W imaging of GN-z11, extending the previous rest-frame coverage from 0.38 to 0.86$Î¼$m. We report significant detections (14$Ïƒ$) in the F560W and F770W images, and a marginal detection (3.2$Ïƒ$) in F1000W. Here, we analyse its SED combining new MIRI imaging data with archival NIRSpec/Prism and MRS spectroscopy, and NIRCam imaging. The continuum emission shows a flat energy distribution, in f$_Î½$, up to 0.5$Î¼$m, compatible with the presence of a mixed stellar population of young (4$\pm$1 Myr) and mature (63$\pm$23 Myr) stars that also account for the [O III], H$Î²$ and H$Î±$ emission lines. The continuum at rest-frame 0.66$Î¼$m shows a 36$\pm$3% flux excess above the predicted flux for a mixed stellar population, pointing to the presence of an additional source contributing at these wavelengths. This excess increases to 91$\pm$28% at rest-frame 0.86$Î¼$m, although with a large uncertainty due to the marginal detection in the F1000W filter. We consider that hot dust emission in the dusty torus around a type 2 AGN could be responsible for the observed excess. Alternatively, this excess could be due to hot dust emission or to a photoluminiscence dust process (Extended Red Emission, ERE) under the extreme UV radiation field, as observed in some local metal-poor galaxies and in young compact starbursts. The presence of a type 1 AGN is not supported by the observed SED since high-z QSOs contribute at wavelengths above rest-frame 1$Î¼$m, and an additional ad-hoc red source would be required to explain the observed flux excess at 0.66 and 0.86$Î¼$m. Additional deep MIRI imaging covering the rest-frame near-IR are needed to confirm the flux detection at 10$Î¼$m with higher significance, and to discriminate between the different hot dust emission in the extreme starburst and AGN scenarios with MIRI imaging at longer wavelengths. |
| 2025-12-02 | [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2512.02982v1) | Xiang Xu, Ao Liang et al. | Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation. |
| 2025-12-02 | [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](http://arxiv.org/abs/2512.02981v1) | Zhongyu Yang, Yingfang Yuan et al. | Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness. |
| 2025-12-02 | [Hypothesis Testing for Generalized Thurstone Models](http://arxiv.org/abs/2512.02912v1) | Anuran Makur, Japneet Singh | In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $Î˜((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets. |
| 2025-12-02 | [Statistical-Symbolic Verification of Perception-Based Autonomous Systems using State-Dependent Conformal Prediction](http://arxiv.org/abs/2512.02893v1) | Yuang Geng, Thomas Waite et al. | Reachability analysis has been a prominent way to provide safety guarantees for neurally controlled autonomous systems, but its direct application to neural perception components is infeasible due to imperfect or intractable perception models. Typically, this issue has been bypassed by complementing reachability with statistical analysis of perception error, say with conformal prediction (CP). However, existing CP methods for time-series data often provide conservative bounds. The corresponding error accumulation over time has made it challenging to combine statistical bounds with symbolic reachability in a way that is provable, scalable, and minimally conservative. To reduce conservatism and improve scalability, our key insight is that perception error varies significantly with the system's dynamical state. This article proposes state-dependent conformal prediction, which exploits that dependency in constructing tight high-confidence bounds on perception error. Based on this idea, we provide an approach to partition the state space, using a genetic algorithm, so as to optimize the tightness of conformal bounds. Finally, since using these bounds in reachability analysis leads to additional uncertainty and branching in the resulting hybrid system, we propose a branch-merging reachability algorithm that trades off uncertainty for scalability so as to enable scalable and tight verification. The evaluation of our verification methodology on two complementary case studies demonstrates reduced conservatism compared to the state of the art. |
| 2025-12-02 | [Discovery and rectification of an error in high resistance traceability at NPL: a case study in how metrology works](http://arxiv.org/abs/2512.02887v1) | Stephen Giblin | We broach a seldom-discussed topic in precision metrology; how subtle errors in calibration processes are discovered and remedied. We examine a case study at the National Physical Laboratory (NPL), UK, involving the calibration of DC standard resistors of value 100 MOhm and 1 GOhm. Results from the period 2001 to 2015 were in error by approximately 0.7 parts per million (ppm), with quoted uncertainties (k=2) of 0.4 ppm and 1.6 ppm respectively. Inter-comparisons did not detect the error, mainly because the uncertainty due to the transportation drift of the comparison standards was too large to resolve it. Likewise, research into single-electron current standards did not detect the error because at this resistance value it was on the borderline of statistical significance. The key event was a comparison between PTB (Germany) and NPL (UK) of a new small-current measuring instrument, the ultrastable low-noise current amplifier (ULCA). At that time, the transport stability of the ULCA was not well established. Nevertheless, calibrations of the ULCA at NPL using a 100 MOhm resistor were sufficiently discrepant with the PTB calibrations to motivate a thorough investigation into the NPL traceability chain, which uncovered the error. This instructive episode illustrates a positive interplay between calibration and research activities and shows that cutting-edge calibration uncertainties must be supported by a vigorous research programme. It is also important for NMIs to maintain a comfortable buffer (at least a factor of 10) between their claimed uncertainty and the uncertainty that their customers require, so that small errors can be resolved without significant impact on measurement stakeholders. |
| 2025-12-02 | [Insights into Extragalactic Background Light constraints with MAGIC archival data](http://arxiv.org/abs/2512.02880v1) | R. Grau, A. Moralejo | The Extragalactic Background Light (EBL) is the accumulated light emitted throughout the history of the universe, spanning the UV, optical, and IR spectral ranges. Stars and dust in galaxies are expected to be the main source of the EBL. However, recent direct measurements performed beyond Pluto's orbit (less affected by foregrounds than those performed from the Earth) hint at an EBL level in the optical band larger than the one expected from the integrated contribution of known galaxy populations. One approach that could solve this controversy uses Very High Energy (VHE) photons coming from sources at cosmological distances. These photons can interact with the EBL producing electron-positron pairs, a process that leaves an imprint on the observed VHE spectrum. This technique, however, requires assumptions on the intrinsic spectrum of the source, which can compromise the robustness of EBL constraints. In this contribution, we used Monte Carlo simulations and archival data of the MAGIC telescopes to study the impact that the assumptions adopted in the literature have on the estimate of the EBL density, and how using more generic ones would modify the results. Our results show how the EBL density constraints obtained highly depend on the intrinsic spectral shape assumed for the source. We have studied two different methods to reduce the assumptions on the intrinsic spectral shape to get more robust results. This will be especially important for upcoming with new VHE facilities, where systematic uncertainties are expected to play a more significant role compared to statistical ones. |
| 2025-12-02 | [The future of AI in critical mineral exploration](http://arxiv.org/abs/2512.02879v1) | Jef Caers | The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage |
| 2025-12-02 | [Correcting for sampling variability in maximum likelihood-based one-sample log-rank tests](http://arxiv.org/abs/2512.02878v1) | Moritz Fabian Danzer, Rene Schmidt | Single-arm studies in the early development phases of new treatments are not uncommon in the context of rare diseases or in paediatrics. If an assessment of efficacy is to be made at the end of such a study, the observed endpoints can be compared with reference values that can be derived from historical data. For a time-to-event endpoint, a statistical comparison with a reference curve can be made using the one-sample log-rank test. In order to ensure the interpretability of the results of this test, the role of the reference curve is crucial. This quantity is often estimated from a historical control group using a parametric procedure. Hence, it should be noted that it is subject to estimation uncertainty. However, this aspect is not taken into account in the one-sample log-rank test statistic. We analyse this estimation uncertainty for the common situation that the reference curve is estimated parametrically using the maximum likelihood method, and indicate how the variance estimation of the one-sample log-rank test can be adapted in order to take this variability into account. The resulting test procedures are illustrated using a data example and analysed in more detail using simulations. |
| 2025-12-01 | [Fault-tolerant mutual-visibility: complexity and solutions for grid-like networks](http://arxiv.org/abs/2512.01978v1) | Serafino Cicerone, Gabriele Di Stefano et al. | Networks are often modeled using graphs, and within this setting we introduce the notion of $k$-fault-tolerant mutual visibility. Informally, a set of vertices $X \subseteq V(G)$ in a graph $G$ is a $k$-fault-tolerant mutual-visibility set ($k$-ftmv set) if any two vertices in $X$ are connected by a bundle of $k+1$ shortest paths such that: ($i$) each shortest path contains no other vertex of $X$, and ($ii$) these paths are internally disjoint. The cardinality of a largest $k$-ftmv set is denoted by $\mathrm{f}Î¼^{k}(G)$. The classical notion of mutual visibility corresponds to the case $k = 0$.   This generalized concept is motivated by applications in communication networks, where agents located at vertices must communicate both efficiently (i.e., via shortest paths) and confidentially (i.e., without messages passing through the location of any other agent). The original notion of mutual visibility may fail in unreliable networks, where vertices or links can become unavailable.   Several properties of $k$-ftmv sets are established, including a natural relationship between $\mathrm{f}Î¼^{k}(G)$ and $Ï‰(G)$, as well as a characterization of graphs for which $\mathrm{f}Î¼^{k}(G)$ is large. It is shown that computing $\mathrm{f}Î¼^{k}(G)$ is NP-hard for any positive integer $k$, whether $k$ is fixed or not. Exact formulae for $\mathrm{f}Î¼^{k}(G)$ are derived for several specific graph topologies, including grid-like networks such as cylinders and tori, and for diameter-two networks defined by Hamming graphs and by the direct product of complete graphs. |
| 2025-12-01 | [AI-Driven Optimization under Uncertainty for Mineral Processing Operations](http://arxiv.org/abs/2512.01977v1) | William Xu, Amir Eskanlou et al. | The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware. |
| 2025-12-01 | [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](http://arxiv.org/abs/2512.01975v1) | Xu Zhang, Jin Yuan et al. | Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input. |
| 2025-12-01 | [Bounded treewidth, multiple context-free grammars, and downward closures](http://arxiv.org/abs/2512.01973v1) | C. Aiswarya, Pascal Baumann et al. | The reachability problem in multi-pushdown automata (MPDA) has many applications in static analysis of recursive programs. An example is safety verification of multi-threaded recursive programs with shared memory. Since these problems are undecidable, the literature contains many decidable (and efficient) underapproximations of MPDA.   A uniform framework that captures many of these underapproximations is that of bounded treewidth (tw): To each execution of the MPDA, we associate a graph; then we consider the subset of all graphs that have a wt at most $k$, for some constant $k$. In fact, bounding tw is a generic approach to obtain classes of systems with decidable reachability, even beyond MPDA underapproximations. The resulting systems are also called MSO-definable bounded-tw systems.   While bounded tw is a powerful tool for reachability and similar types of analysis, the word languages (i.e. action sequences corresponding to executions) of these systems remain far from understood.   For the slight restriction of bounded special tw, or "bounded-stw" (which is equivalent to bounded tw on MPDA, and even includes all bounded-tw systems studied in the literature), this work reveals a connection with multiple context-free languages (MCFL), a concept from computational linguistics. We show that the word languages of MSO-definable bounded-stw systems are exactly the MCFL.   We exploit this connection to provide an optimal algorithm for computing downward closures (dcl) for MSO-definable bounded-stw systems. Computing dcl is a notoriously difficult task that has many applications in the verification of complex systems: As an example application, we show that in programs with dynamic spawning of MSO-definable bounded-stw processes, safety verification has the same complexity as in the case of processes with sequential recursive processes. |
| 2025-12-01 | [Towards Precision Gluon Densities at Small $x$: From Resummation to Collider Observables](http://arxiv.org/abs/2512.01961v1) | Francesco Giovanni Celiberto, Marco Bonvini | Accurate gluon densities at small $x$ are essential for reducing theoretical uncertainties in collider predictions, yet remain one of the least constrained ingredients in global analyses. We report recent advances in the resummation of small-$x$ logarithms in the gluon sector, focusing on collinear distributions and their interplay with transverse-momentum-dependent formulations. Particular attention is paid to the impact of gluon-proton spin correlations and to the emergence of unintegrated gluon densities derived from resummed dynamics. These developments aim to fill the current precision gap in the small-$x$ region and enable robust applications to LHC and future-collider observables. |
| 2025-12-01 | [Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model](http://arxiv.org/abs/2512.01924v1) | Kentaro Fujii, Shingo Murata | Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions. |
| 2025-12-01 | [Uncertainty quantification in load profiles with rising EV and PV adoption: the case of residential, industrial, and office buildings](http://arxiv.org/abs/2512.01914v1) | Aiko Fias, Md Umar Hashmi et al. | The integration of photovoltaic (PV) generation and electric vehicle (EV) charging intro- duces significant uncertainty in electricity consumption patterns, particularly at the distribution level. This paper presents a comparative study for selecting metrics for uncertainty quantification (UQ) for net load profiles of residential, industrial, and office buildings under increased DER penetration. A variety of statistical metrics is evaluated for their usefulness in quantifying un- certainty, including, but not limited to, standard deviation, entropy, ramps, and distance metrics. The proposed metrics are classified into baseline-free, with baseline and error-based. These UQ metrics are evaluated for increased penetration of EV and PV. The results highlight suitable metrics to quantify uncertainty per consumer type and demonstrate how net load uncertainty is affected by EV and PV adoption. Additionally, it is observed that joint consideration of EV and PV can reduce overall uncertainty due to compensatory effects of EV charging and PV generation due to temporal alignment during the day. Uncertainty reduction is observed across all datasets and is most pronounced for the office building dataset. |
| 2025-12-01 | [NeuroHJR: Hamilton-Jacobi Reachability-based Obstacle Avoidance in Complex Environments with Physics-Informed Neural Networks](http://arxiv.org/abs/2512.01897v1) | Granthik Halder, Rudrashis Majumder et al. | Autonomous ground vehicles (AGVs) must navigate safely in cluttered environments while accounting for complex dynamics and environmental uncertainty. Hamilton-Jacobi Reachability (HJR) offers formal safety guarantees through the computation of forward and backward reachable sets, but its application is hindered by poor scalability in environments with numerous obstacles. In this paper, we present a novel framework called NeuroHJR that leverages Physics-Informed Neural Networks (PINNs) to approximate the HJR solution for real-time obstacle avoidance. By embedding system dynamics and safety constraints directly into the neural network loss function, our method bypasses the need for grid-based discretization and enables efficient estimation of reachable sets in continuous state spaces. We demonstrate the effectiveness of our approach through simulation results in densely cluttered scenarios, showing that it achieves safety performance comparable to that of classical HJR solvers while significantly reducing the computational cost. This work provides a new step toward real-time, scalable deployment of reachability-based obstacle avoidance in robotics. |
| 2025-12-01 | [Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets](http://arxiv.org/abs/2512.01888v1) | Adrienne M. Propp, Mauro Perego et al. | Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics. |
| 2025-12-01 | [First detections of methanol maser lines from a rare transition family](http://arxiv.org/abs/2512.01875v1) | Bradley R. Johnson, Simon P. Ellingsen et al. | We report the first observations in a rare family of class II methanol maser transitions in both CH$_3$OH and $^{13}$CH$_3$OH toward three southern high-mass star formation regions, along with the first maser detected in the $^{13}$CH$_3$OH line. The $8_2 \rightarrow 9_1 A^{-}$ methanol transition was observed in both CH$_3$OH and $^{13}$CH$_3$OH (at 28.9 GHz and 41.9 GHz respectively) toward three sources; G358.93-0.03, NGC6334I and G345.01+1.79, all of which are star formation regions with recent maser flaring events. We report the first maser detection of the 41.9 GHz line in $^{13}$CH$_3$OH toward G358.93-0.03 and the first confirmed maser detection of the 28.9 GHz line in CH$_3$OH toward NGC6334I. Additionally we report a maser detection of the 28.9 GHz line in CH$_3$OH toward G358.93-0.03, meaning that with our detection of the 41.9 GHz line, this is the first isotopic detection of these lines toward G358.93-0.03. The newly detected maser transitions are associated with the primary millimetre continuum sources (MM1) in both G358.93-0.03 and NGC6334I, within the varying positional uncertainties. |
| 2025-11-28 | [Epistemic and Aleatoric Uncertainty Quantification in Weather and Climate Models](http://arxiv.org/abs/2511.23448v1) | Laura A. Mansfield, Hannah M. Christensen | Representing and quantifying uncertainty in physical parameterisations is a central challenge in weather and climate modelling, and approaches are often developed separately for different timescales. Here, we introduce a unified framework for analysing uncertainty in parameterisations across weather and climate regimes. Using the Lorenz 1996 system as a testbed for simplified chaotic dynamics, we quantify uncertainties in a subgrid-scale parameterisation using a Bayesian Neural Network (BNN). This allows us to disentangle aleatoric uncertainty, arising from internal variability in the training data, and epistemic uncertainties, arising from poorly constrained parameters during training. At runtime, we sample uncertainties in line with stochastic approaches in weather models and perturbed-parameter methods in climate models. On weather timescales, aleatoric uncertainty dominates, underscoring the value of stochastic parameterisations. On longer, climate timescales and under changing forcings, accounting for both types of uncertainty is necessary for well-calibrated ensembles, with epistemic uncertainty widening the range of explored climate states, and aleatoric uncertainty promoting transitions between them. Constraining parameter uncertainty with short simulations reduces epistemic uncertainty and improves long-term model behaviour under perturbed forcings. This framework links concepts from machine learning with traditional uncertainty quantification in Earth system modelling, offering a pathway toward seamless treatment of uncertainty in weather and climate prediction. |
| 2025-11-28 | [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](http://arxiv.org/abs/2511.23440v1) | Bernhard Klein, Falk Selker et al. | Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems. |
| 2025-11-28 | [Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets](http://arxiv.org/abs/2511.23433v1) | Maria Alejandra Valdez Cabrera, Amy D Willis et al. | Connected acyclic graphs (trees) are data objects that hierarchically organize categories. Collections of trees arise in a diverse variety of fields, including evolutionary biology, public health, machine learning, social sciences and anatomy. Summarizing a collection of trees by a single representative is challenging, in part due to the dimension of both the sample and parameter space. We frame consensus tree estimation as a structured feature-selection problem, where leaves and edges are the features. We introduce a partial order on leaf-labeled trees, use it to define true and false discoveries for a candidate summary tree, and develop an estimation algorithm that controls the false discovery rate at a nominal level for a broad class of non-parametric generative models. Furthermore, using the partial order structure, we assess the stability of each feature in a selected tree. Importantly, our method accommodates unequal leaf sets and non-binary trees, allowing the estimator to reflect uncertainty by collapsing poorly supported structure instead of forcing full resolution. We apply the method to study the archaeal origin of eukaryotic cells and to quantify uncertainty in deep branching orders. While consensus tree construction has historically been viewed as an estimation task, reframing it as feature selection over a partially ordered set allows us to obtain the first estimator with finite-sample and model-free guarantees. More generally, our approach provides a foundation for integrating tools from multiple testing into tree estimation. |
| 2025-11-28 | [From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products](http://arxiv.org/abs/2511.23407v1) | Jan BaumgÃ¤rtner, Malte Hansjosten et al. | To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts. |
| 2025-11-28 | [Bounded-Error Quantum Simulation via Hamiltonian and Lindbladian Learning](http://arxiv.org/abs/2511.23392v1) | Tristan Kraft, Manoj K. Joshi et al. | Analog Quantum Simulators offer a route to exploring strongly correlated many-body dynamics beyond classical computation, but their predictive power remains limited by the absence of quantitative error estimation. Establishing rigorous uncertainty bounds is essential for elevating such devices from qualitative demonstrations to quantitative scientific tools. Here we introduce a general framework for bounded-error quantum simulation, which provides predictions for many-body observables with experimentally quantifiable uncertainties. The approach combines Hamiltonian and Lindbladian Learning--a statistically rigorous inference of the coherent and dissipative generators governing the dynamics--with the propagation of their uncertainties into the simulated observables, yielding confidence bounds directly derived from experimental data. We demonstrate this framework on trapped-ion quantum simulators implementing long-range Ising interactions with up to 51 ions, and validate it where classical comparison is possible. We analyze error bounds on two levels. First, we learn an open-system model from experimental data collected in an initial time window of quench dynamics, simulate the corresponding master equation, and quantitatively verify consistency between theoretical predictions and measured dynamics at long times. Second, we establish error bounds directly from experimental measurements alone, without relying on classical simulation--crucial for entering regimes of quantum advantage. The learned models reproduce the experimental evolution within the predicted bounds, demonstrating quantitative reliability and internal consistency. Bounded-error quantum simulation provides a scalable foundation for trusted analog quantum computation, bridging the gap between experimental platforms and predictive many-body physics. The techniques presented here directly extend to digital quantum simulation. |
| 2025-11-28 | [The Kinematic Properties of TÅ»O Candidate HV 11417 with Gaia DR3](http://arxiv.org/abs/2511.23368v1) | Anna J. G. O'Grady | HV 11417 is a candidate Thorne-Å»ytkow Object, a red supergiant with a neutron star core, located within the Small Magellanic Cloud (SMC). Previous studies have questioned, using Gaia DR2 data, whether HV 11417 was truly located at the distance of the SMC or was instead a foreground star. However, the proper motion measurement uncertainties for HV 11417 in DR2 were high. In this work, we use Gaia DR3 data to show that HV 11417 is very likely to be a true member of the SMC. We further analyze the kinematics of HV 11417 relative to its local environment, and compare it to populations of massive and evolved stars in the SMC. We find HV 11417 has a local transverse velocity of $52\pm15$ km/s, and thus qualifies as a runaway star (v$_\mathrm{loc}\geq$ 30 km/s). This runaway classification does not conclusively prove its nature as a TÅ»O, particularly given results from recent TÅ»O models, but does indicate that HV 11417 experienced a kinematic disruption in its evolution. |
| 2025-11-28 | [Data-driven Reachability Verification with Probabilistic Guarantees under Koopman Spectral Uncertainty](http://arxiv.org/abs/2511.23322v1) | Jianqiang Ding, Shankar A. Deka | Providing rigorous reachability guarantees for unknown complex systems is a crucial and challenging task. In this paper, we present a novel data-driven framework that addresses this challenge by leveraging Koopman operator theory. Instead of operating in the state space, the proposed method encodes model uncertainty from finite data directly into Koopman spectral representation with quantifiable error bounds. Leveraging this spectral information, we systematically determine time intervals within which trajectories from the initial set are guaranteed, with a prescribed probability, to reach the target set. This enables the rigorous reachability verification without explicit computation of reachable sets, thereby offering a significant advantage in scalability and applicability. We finally validate the effectiveness of the proposed framework through case studies on representative dynamical systems. |
| 2025-11-28 | [Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering](http://arxiv.org/abs/2511.23304v1) | Zijian Fu, Changsheng Lv et al. | In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released. |
| 2025-11-28 | [Intrinsic $k_T$ and soft gluons in Monte Carlo event generators](http://arxiv.org/abs/2511.23291v1) | Louis Moureaux, Aleksandra Lelek et al. | Experimental measurements of the transverse momentum of Drell-Yan lepton pairs are sensitive to non-perturbative physics associated with the intrinsic parton transverse momentum $k_T$. We discuss recent determinations of intrinsic $k_T$ in the context of transverse momentum dependent (TMD) parton branching calculations and collinear parton-shower Monte Carlo generators. We illustrate the influence of the soft-gluon resolution scale and the non-perturbative Sudakov region on the intrinsic $k_T$ extraction. We emphasize the relevance of the correct treatment of correlated uncertainties between different transverse momentum bins in TMD fits and present an application to the determination of the intrinsic $k_T$ in the forward rapidity region. |
| 2025-11-28 | [Quark mixing from muon collider neutrinos](http://arxiv.org/abs/2511.23288v1) | David Marzocca, Francesco Montagno et al. | A high energy muon collider naturally produces a collimated beam of neutrinos for a fixed-target experiment at a dedicated far-forward facility. The high intensity and energy of the beam makes it ideally suited for astonishingly precise measurements of neutrino scattering on nucleons in the deeply inelastic regime, enabling the determination of the Cabibbo--Kobayashi--Maskawa~(CKM) quark mixing matrix. We assess the floor to the attainable sensitivity set by irreducible sources of uncertainties from the imperfect knowledge of the parton distribution (PDF) and fragmentation functions, showing that a strong improvement is possible well above current standards. As a by-product, our analysis also outlines extraordinary perspectives for a combined determination of the PDF. The results demonstrate the potential of a parasitic neutrino experiment at the muon collider, motivating detailed future studies. |
| 2025-11-26 | [Uncertainty Quantification for Visual Object Pose Estimation](http://arxiv.org/abs/2511.21666v1) | Lorenzo Shaikewitz, Charis Georgiou et al. | Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets. |
| 2025-11-26 | [Updated bounds on the (1,2) neutrino oscillation parameters after first JUNO results](http://arxiv.org/abs/2511.21650v1) | Francesco Capozzi, Eligio Lisi et al. | Within the standard $3Î½$ framework, we discuss updated bounds on the leading oscillation parameters related to the $(Î½_1,\,Î½_2)$ states, namely, the squared mass difference $Î´m^2=m^2_2-m^2_1$ and the mixing parameter $\sin^2Î¸_{12}$. A previous global analysis of 2024 oscillation data estimated $Î´m^2$ and $\sin^2Î¸_{12}$ with fractional $1Ïƒ$ errors of about $2.3\%$ and $4.5\%$, respectively. First we update the analysis by applying the latest SNO+ constraints, that slightly shift the $(Î´m^2,\,\sin^2Î¸_{12})$ best fits. Then we apply the constraints placed by the first JUNO results, that significantly reduce the uncertainties of both parameters. Our updated global bounds (as of 2025) can be summarized as: $Î´m^2/10^{-5}{\rm eV}^2 = 7.48\pm 0.10$ and $\sin^2Î¸_{12}=0.3085\pm0.0073$ (with correlation $Ï=-0.20$), corresponding to $1Ïƒ$ uncertainties as small as $1.3\%$ and $2.4\%$, respectively. We also comment on minor physical and statistical effects that, in the future, may contribute to lift the current mass-ordering degeneracy of $(Î´m^2,\,Î¸_{12})$ estimates. |
| 2025-11-26 | [Active Learning for GCN-based Action Recognition](http://arxiv.org/abs/2511.21625v1) | Hichem Sahbi | Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work. |
| 2025-11-26 | [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](http://arxiv.org/abs/2511.21607v1) | Zarin Tahia Hossain, Mostafa Milani | Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines. |
| 2025-11-26 | [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](http://arxiv.org/abs/2511.21570v1) | Maria Perez-Ortiz | In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century. |
| 2025-11-26 | [Enhanced antineutrino emission from $Î²$ decay in core-collapse supernovae with self-consistent weak decay rates](http://arxiv.org/abs/2511.21567v1) | T. Dasher, A. RavliÄ‡ et al. | Nuclear weak-interaction rates are known to exert a prominent effect in the late-stages of stellar collapse. Despite their importance, most studies to date on core-collapse supernovae (CCSNe) have focused primarily on the effects of electron captures, generally neglecting $Î²$ decay contributions. In this Letter, we present the first CCSNe simulation incorporating global $Î²$ decay rates from a microscopic theory. These are enabled by a large-scale evaluation of both electron capture and $Î²$ decay rates, obtained self-consistently utilizing the relativistic energy density functional theory and finite-temperature quasiparticle random-phase approximation. We find a significant enhancement of antineutrino emissivity by more than 4 orders of magnitude due to the inclusion of $Î²$ decay rates, as well as 3 orders of magnitude for antineutrino luminosity. It is expected that these new rates could help us constrain the model uncertainties related to weak-interaction processes, improving the prediction of antineutrino signal during the final stages of stellar death and potentially influencing the late-stage evolution of massive stars. |
| 2025-11-26 | [A decoupled alignment kernel for peptide membrane permeability predictions](http://arxiv.org/abs/2511.21566v1) | Ali Amirahmadi, GÃ¶kÃ§e Geylan et al. | Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics. |
| 2025-11-26 | [Cosmological Implications of the Extended Uncertainty Principle: Energy Conditions, Stability, and Late Time Acceleration](http://arxiv.org/abs/2511.21546v1) | Maryam Roushan, Narges Rashidi | We study the cosmological consequences of the Extended Uncertainty Principle (EUP) by deriving modified Friedmann equations through thermodynamic arguments. The evolution of the effective equation of state induced by EUP corrections is analyzed and characterized using the Chevallier-Polarski-Linder (CPL) parametrization. We then examine the fulfillment of classical energy conditions, including the null, weak, strong, and dominant conditions. The dynamical and thermodynamic stability of the model is investigated, showing that the EUP cosmology admits a late-time de Sitter attractor. Finally, we evaluate the effective speed of sound associated with the model and discuss implications for perturbative stability. Our findings indicate that EUP-induced corrections can produce a consistent late-time acceleration without requiring a cosmological constant. |
| 2025-11-26 | [Testing modified gravity with 3x2pt analyses in galaxy mocks](http://arxiv.org/abs/2511.21468v1) | Marc Alemany-Gotor, Cristian Viglione et al. | Stage-IV surveys will enable unprecedented tests of gravity on cosmological scales. However, assuming General Relativity in the analysis of large-scale structure could introduce systematic biases if gravity deviates from GR at these scales. Modified gravity theories, such as the Hu-Sawicki formulation of $f(R)$ gravity, offer an alternative explanation for cosmic acceleration without invoking a cosmological constant, while remaining consistent with Solar System tests through screening mechanisms. In this work, we quantify the cosmological parameter biases that arise when using a combination of galaxy clustering and weak-lensing data-vectors, the so-called 3x2pt analysis, from an $f(R)$ galaxy mock under the incorrect assumption of GR, using for the first time high-fidelity full-sky galaxy mock catalogues. We employ a pair of twin simulations: one with GR and one with Hu--Sawicki $f(R)$ gravity with $|f_{R0}| = 10^{-5}$. The mocks are built using an HOD method to populate the dark matter haloes with galaxies, calibrated against SDSS observations at low redshift. Using conservative scale cuts to minimise modelling uncertainties, we perform 3x2pt analyses and infer cosmological parameters through nested sampling, validating our pipeline with the GR mock. Our results show that when analysing the $f(R)$ galaxy mock assuming GR, the recovered cosmological parameters are very significantly biased, even when considering conservative scale cuts: the Figure of Bias reaches $\sim12Ïƒ$ for both $\{Î©_{\rm m}, Ïƒ_8\}$ and $S_8$. These biases persist even when marginalising over the galaxy bias and baryonic feedback, demonstrating that nuisance parameters cannot absorb the effects of modified gravity. We conclude that incorrectly assuming GR in a universe governed by $f(R)$ gravity leads to severe and detectable biases in cosmological inference for Stage-IV surveys. |
| 2025-11-26 | [The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods](http://arxiv.org/abs/2511.21363v1) | Kevin Iselborn, David Dembinsky et al. | The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes. |
| 2025-11-25 | [Carrier transport and electrical bandgaps in epitaxial CrN layers](http://arxiv.org/abs/2511.20625v1) | Duc V. Dinh, Jens Herfort et al. | The transport properties and electrical bandgap of nominally undoped ~75-nm-thick CrN layers simultaneously grown on AlN(0001) and AlN(11\bar{2}2) templates using plasma-assisted molecular beam epitaxy are investigated. The layers grown on AlN(0001) and AlN(11\bar{2}2) exhibit (111) and (113) surface orientations, respectively. All layers exhibit antiferromagnetism with a NÃ©el temperature of ~280 K, observed by temperature-dependent magnetic and electrical measurements. Hall-effect measurements demonstrate n-type semiconducting behavior across a wide temperature range from 4 to 920 K. At low temperatures (4 - 260 K), the data show parallel conduction channels from a metallic impurity band and the conduction band. The carrier mobility exhibits a temperature dependence consistent with a nondegenerate semiconductor, governed by ionized-impurity scattering below 400 K and phonon scattering above 400 K. An analysis of the temperature-dependent carrier density between 300 and 920 K yields two activation energies associated with intrinsic conduction: 0.15 eV (with an uncertainty of -0.02/+0.10 eV), which we attribute to the fundamental bandgap, and 0.50 eV (with an uncertainty of -0.05/+0.15 eV) representing a higher energy transition. |
| 2025-11-25 | [Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning](http://arxiv.org/abs/2511.20613v1) | Panayiotis Danassis, Naman Goel | The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios. |
| 2025-11-25 | [Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition](http://arxiv.org/abs/2511.20612v1) | Yujin Kim, Sarah Dean | Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD |
| 2025-11-25 | [Quantum Key Distribution: Bridging Theoretical Security Proofs, Practical Attacks, and Error Correction for Quantum-Augmented Networks](http://arxiv.org/abs/2511.20602v1) | Nitin Jha, Abhishek Parakh et al. | Quantum Key Distribution (QKD) is revolutionizing cryptography by promising information-theoretic security through the immutable laws of quantum mechanics. Yet, the challenge of transforming these idealized security models into practical, resilient systems remains a pressing issue, especially as quantum computing evolves. In this review, we critically dissect and synthesize the latest advancements in QKD protocols and their security vulnerabilities, with a strong emphasis on rigorous security proofs. We actively categorize contemporary QKD schemes into three key classes: uncertainty principle-based protocols (e.g., BB84), hybrid architectures that enable secure direct communication (eg, three-stage protocol), and continuous-variable frameworks. We further include two modern classes of QKD protocols, namely Twin-field QKD and Device-Independent QKD, both of which were developed to have practical implementations over the last decade. Moreover, we highlight important experimental breakthroughs and innovative mitigation strategies, including the deployment of advanced Quantum Error Correction Codes (QECCs), that significantly enhance channel fidelity and system robustness. By mapping the current landscape, from sophisticated quantum attacks to state-of-the-art error correction methods, this review fills an important gap in the literature. To bring everything together, the relevance of this review concerning quantum augmented networks (QuANets) is also presented. This allows the readers to gain a comprehensive understanding of the security promises of quantum key distribution from theoretical proofs to experimental validations. |
| 2025-11-25 | [Variational bagging: a robust approach for Bayesian uncertainty quantification](http://arxiv.org/abs/2511.20594v1) | Shitao Fan, Ilsang Ohn et al. | Variational Bayes methods are popular due to their computational efficiency and adaptability to diverse applications. In specifying the variational family, mean-field classes are commonly used, which enables efficient algorithms such as coordinate ascent variational inference (CAVI) but fails to capture parameter dependence and typically underestimates uncertainty. In this work, we introduce a variational bagging approach that integrates a bagging procedure with variational Bayes, resulting in a bagged variational posterior for improved inference. We establish strong theoretical guarantees, including posterior contraction rates for general models and a Bernstein-von Mises (BVM) type theorem that ensures valid uncertainty quantification. Notably, our results show that even when using a mean-field variational family, our approach can recover off-diagonal elements of the limiting covariance structure and provide proper uncertainty quantification. In addition, variational bagging is robust to model misspecification, with covariance structures matching those of the target covariance. We illustrate our variational bagging method in numerical studies through applications to parametric models, finite mixture models, deep neural networks, and variational autoencoders (VAEs). |
| 2025-11-25 | [PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic](http://arxiv.org/abs/2511.20586v1) | Koffi Ismael Ouattara, Ioannis Krontiris et al. | Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \emph{Trust Nodes} and \emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle. |
| 2025-11-25 | [An improved time delay from VLA and ATCA monitoring of the gravitational lens system PKS 1830-211](http://arxiv.org/abs/2511.20582v1) | A. D. Biggs | We have measured a time delay of 25.3 +/- 2.0 d (1-sigma confidence) in the Einstein ring gravitational lens system PKS 1830-211 from an analysis of archival VLA and ATCA monitoring data observed between 1997 and 2004. A small portion of the ATCA data was previously used to determine a time delay and our result is consistent with the previous value, but with an uncertainty that is smaller by more than a factor of two. The long time-baseline of the monitoring reveals that the flux density ratio is smoothly varying on a time-scale of years, an effect which we attribute to millilensing by massive objects (>>1 M_sun) in the lensing galaxy. Image A is unpolarized in the VLA monitoring, but VLBI observations show that this is partly due to beam dilution by an unpolarized counter-jet that is only present in that image. Based on the identification of this feature as a counter-jet, we conclude that its unexpected prominence in image A is a consequence of lensing and that more detailed modelling is required in order to reconcile the VLBI morphology of each image. |
| 2025-11-25 | [NNLO QCD predictions for $WÎ³Î³$ production at the LHC](http://arxiv.org/abs/2511.20581v1) | Paolo Garbarino, Massimiliano Grazzini et al. | Triboson production processes play a crucial role in probing the electroweak sector of the Standard Model, as they involve quartic gauge-boson couplings already at the tree level. With these measurements entering the precision era at the Large Hadron Collider (LHC), accurate theoretical predictions become indispensable. We present the computation of the next-to-next-to-leading-order (NNLO) QCD radiative corrections to the production of a $W$ boson in association with two photons ($WÎ³Î³$) at the LHC. The calculation is exact, except for the finite part of the two-loop contribution, which is included in the leading-colour approximation. Predictions for the fiducial cross section and selected kinematic distributions are provided at a centre-of-mass energy of $\sqrt{s}=13$ TeV, under standard experimental selection cuts. In line with observations for other multiboson processes involving direct photons, we find sizable NNLO corrections that enhance the next-to-leading-order predictions by about $23\%$, with residual perturbative uncertainties that can be roughly estimated to be at the $5\%$ level. |
| 2025-11-25 | [Active learning with physics-informed neural networks for optimal sensor placement in deep tunneling through transversely isotropic elastic rocks](http://arxiv.org/abs/2511.20574v1) | Alec Tristani, ChloÃ© Arson | This paper presents a deep learning strategy to simultaneously solve Partial Differential Equations (PDEs) and back-calculate their parameters in the context of deep tunnel excavation. A Physics-Informed Neural Network (PINN) model is trained with synthetic data that emulates in situ displacement measurements in the host rock and at the cavity wall, obtained from extensometers and convergence monitoring. As acquiring field observations can be costly, a sequential training approach based on active learning is implemented to determine the most informative locations for new sensors. In particular, Monte Carlo dropout is used to quantify epistemic uncertainty and query measurements in regions where the model is least confident. This approach reduces the amount of required field data and optimizes sensor placement. The PINN is tested to reconstruct the displacement field around a deep tunnel of circular section excavated in transversely isotropic elastic rock and to determine rock constitutive and stress-field parameters. Results demonstrate excellent performance on small, scattered, and noisy datasets, achieving high precision for the Young's moduli, shear modulus, horizontal-to-vertical far-field stress ratio, and the orientation of the bedding planes. The proposed framework shall ultimately support decision-making for optimal subsurface monitoring and for adaptive tunnel design and control. |
| 2025-11-25 | [Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics](http://arxiv.org/abs/2511.20570v1) | Tasha Kim, Oiwi Parker Jones | Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action. |
| 2025-11-24 | [Gamma-ray Time Delay and Magnification Ratio in the Gravitationally-Lensed Blazar PKS 1830-211](http://arxiv.org/abs/2511.19419v1) | S. Buson, M. De Toma et al. | We present the characterization of macrolensing properties of the gravitationally lensed system PKS 1830-211, utilizing data from the Fermi Large Area Telescope. While at gamma-rays we can not spatially resolve the lensed images, a macrolensing-induced time pattern is expected in the blazar's lightcurve, resulting from the delay between variable gamma-ray components originating from its two brightest lensed images. Compared to our previous study, here we employ high-quality lightcurves coupled with prolonged outburst activity, and improved time-series techniques. Analyzing six independent data segments, we identified a delay of 20.26 +/- 0.62 days (statistical and stochastic uncertainty), with a chance detection probability at the 2.5 x $10^{-5}$ level (post-trial). We also present a novel approach to the magnification ratio estimate based on a comparison with simulated data. Our work suggests that the gamma-ray flux ratio between the two main lens components is $Î¼_Î³ \lesssim$ 1.8. We do not observe convincing evidence of microlensing effects, as previously claimed. The measured gamma-ray time delay is in 2-sigma tension with radio-based estimates, suggesting either distinct emission sites, underestimated radio uncertainties, or gamma-ray production in a region opaque to radio. Our study highlights the potential of well sampled lightcurves and advanced time-series techniques to distinguish true lensing-induced delays from stochastic variability. When combined with improved radio constraints and refined lens models, PKS 1830-211 and similar sources constitute promising systems for time-delay cosmography, offering new insights into both jet structure and cosmological parameters. |
| 2025-11-24 | [Puzzling Isotonic Odd-Even Staggering of Charge Radii in Deformed Rare Earth Nuclei](http://arxiv.org/abs/2511.19395v1) | Endre Takacs, Hunter Staiger et al. | The nuclear charge radius is a fundamental observable that encodes key aspects of nuclear structure, deformation, and pairing. Isotonic (constant neutron number) systematics in the deformed rare-earth region have long suggested that odd-$Z$ nuclei are more compact than their even-$Z$ neighbors - except for Lu, whose recommended radius appeared anomalously large relative to Yb and Hf. We report a high-precision determination of the natural-abundance-averaged Lu-Yb charge-radius difference using extreme-ultraviolet spectroscopy of highly charged Na-like and Mg-like ions, supported by high-accuracy relativistic atomic-structure calculations - a recently introduced method with the unique ability to measure inter-element charge radius differences. Combined with muonic-atom and optical isotope-shift data, our result resolves the longstanding Lu inversion anomaly and reestablishes a pronounced odd-even staggering along the $N=94$ isotonic chain. The magnitude of this staggering is unexpectedly large, far exceeding that observed in semi-magic nuclei and in deformed isotopic sequences. State-of-the-art nuclear density functional theory calculations, including quantified uncertainties, fail to reproduce this enhancement, possibly indicating missing structural effects in current models. Our work demonstrates the power of highly charged ions for precise, element-crossing charge-radius measurements and provides stringent new constraints for future theoretical and experimental studies of nuclear-size systematics. |
| 2025-11-24 | [Guesswork in the gap: the impact of uncertainty in the compact binary population on source classification](http://arxiv.org/abs/2511.19393v1) | Utkarsh Mali, Reed Essick | The nature of the compact objects within the supposed "lower mass gap" remains uncertain. Observations of GW190814 and GW230529 highlight the challenges gravitational waves face in distinguishing neutron stars from black holes. Interpreting these systems is especially difficult because classifications depend simultaneously on measurement noise, compact binary population models, and equation of state (EOS) constraints on the maximum neutron star mass. We analyze 66 confident events from GWTC-3 to quantify how the probability of a component being a neutron star, P(NS), varies across the population. The effects are substantial, the dominant drivers of classification are the pairing preferences of neutron stars with other compact objects, and the neutron star spin distributions. The data reveals that P(NS) varies between 1% - 67% for GW230529's primary and between 51% - 100% for GW190425's primary. By contrast, P(NS) for GW190814's secondary varies by <10%, demonstrating robustness from its high signal-to-noise ratio and small mass ratio. Analysis using EOS information tends to affect P(NS) through the inferred maximum neutron star mass rather than the maximum spin. As it stands, P(NS) remains sensitive to numerous population parameters, limiting its reliability and potentially leading to ambiguous classifications of future GW events. |
| 2025-11-24 | [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](http://arxiv.org/abs/2511.19390v1) | Rudy Morel, Francesco Pio Ramunno et al. | Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability. |
| 2025-11-24 | [Normative active inference: A numerical proof of principle for a computational and economic legal analytic approach to AI governance](http://arxiv.org/abs/2511.19334v1) | Axel Constant, Mahault Albarracin et al. | This paper presents a computational account of how legal norms can influence the behavior of artificial intelligence (AI) agents, grounded in the active inference framework (AIF) that is informed by principles of economic legal analysis (ELA). The ensuing model aims to capture the complexity of human decision-making under legal constraints, offering a candidate mechanism for agent governance in AI systems, that is, the (auto)regulation of AI agents themselves rather than human actors in the AI industry. We propose that lawful and norm-sensitive AI behavior can be achieved through regulation by design, where agents are endowed with intentional control systems, or behavioral safety valves, that guide real-time decisions in accordance with normative expectations. To illustrate this, we simulate an autonomous driving scenario in which an AI agent must decide when to yield the right of way by balancing competing legal and pragmatic imperatives. The model formalizes how AIF can implement context-dependent preferences to resolve such conflicts, linking this mechanism to the conception of law as a scaffold for rational decision-making under uncertainty. We conclude by discussing how context-dependent preferences could function as safety mechanisms for autonomous agents, enhancing lawful alignment and risk mitigation in AI governance. |
| 2025-11-24 | [Revisiting model-independent constraints on spatial curvature and cosmic ladders calibration: updated and forecast analyses](http://arxiv.org/abs/2511.19332v1) | Arianna Favale, AdriÃ  GÃ³mez-Valent et al. | Model-independent approaches have gained increasing attention as powerful tools to investigate persistent tensions between cosmological observations and the predictions of $Î›$CDM. Notably, recent DESY5 Type Ia Supernovae (SNIa) and DESI Baryon Acoustic Oscillation (BAO) data challenge the validity of the cosmological constant, and they remain in tension with SH0ES local distance ladder measurements under standard pre-recombination physics. Building on our previous work, MNRAS 523 (2023) 3, 3406-3422, we present a follow-up analysis of the model-independent calibration of the local and inverse distance ladders using cosmic chronometers (CCH) data and Gaussian Processes. We jointly constrain the SNIa absolute magnitude, $M$, the comoving sound horizon at the baryon-drag epoch, $r_d$, and the spatial curvature parameter, $Î©_k$, using CCH with DESY5 and DESI DR1/DR2. We find this data combination compatible with a flat universe at $\sim1.7Ïƒ$, with $Î©_k=-0.143\pm0.085$, showing weaker compatibility than with Pantheon+, while the ladder calibrators read $M=-19.324_{-0.095}^{+0.092}$ and $r_d=(144.00^{+5.38}_{-4.88}$) Mpc. Although current uncertainties limit the precision of our constraints and prevent us from arbitrating the Hubble tension, it is nevertheless instructive to explore the constraining power of our methodology with future SNIa, CCH, and BAO from surveys such as LSST, Euclid, and DESI. We present the first forecast analysis for the triad $(M,Î©_k,r_d)$, finding that, in an optimistic scenario, upcoming data will improve agnostic constraints on $M$ by $\sim$54% and on $r_d$ by $\sim$66%, enabling a $\sim2$% determination of $H_0$. Precision on $Î©_k$ will increase by $\sim50$%. Our analysis outlines which improvements in future data - whether in quality, quantity, or redshift coverage - are likely to most effectively tighten these constraints.[abridged] |
| 2025-11-24 | [IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection](http://arxiv.org/abs/2511.19301v1) | Johannes Meier, Florian GÃ¼nther et al. | Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.   To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset. |
| 2025-11-24 | [Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection](http://arxiv.org/abs/2511.19274v1) | Mingyang Chen, Jiawei Du et al. | Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences. |
| 2025-11-24 | [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](http://arxiv.org/abs/2511.19263v1) | Lucas Li, Jean-Baptiste Puel et al. | Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction. |
| 2025-11-24 | [Integrating Complex Covariate Transformations in Generalized Additive Models](http://arxiv.org/abs/2511.19234v1) | Claudia Collarin, Matteo Fasiolo et al. | Transformations of covariates are widely used in applied statistics to improve interpretability and to satisfy assumptions required for valid inference. More broadly, feature engineering encompasses a wider set of practices aimed at enhancing predictive performance, and is typically performed as part of a data pre-processing step. In contrast, this paper integrates a substantial component of the feature engineering process directly into the modelling stage. This is achieved by introducing a novel general framework for embedding interpretable covariate transformations within multi-parameter Generalised Additive Models (GAMs). Our framework accommodates any sufficiently differentiable scalar-valued transformation of potentially high-dimensional and complex covariates. These transformations are treated as integral model components, with their parameters estimated jointly with regression coefficients via maximum a posteriori (MAP) methods, and joint uncertainty quantified via approximate Bayesian techniques. Smoothing parameters are selected in an empirical Bayes framework using a Laplace approximation to the marginal likelihood, supported by efficient computation based on implicit differentiation methods. We demonstrate the flexibility and practical value of the proposed methodology through applications to forecasting electricity net-demand in Great Britain and to modelling house prices in London. The proposed methods are implemented by the gamFactory R package, available at https://github.com/mfasiolo/gamFactory. |
| 2025-11-21 | [Scaling Conditional Autoencoders for Portfolio Optimization via Uncertainty-Aware Factor Selection](http://arxiv.org/abs/2511.17462v1) | Ryan Engel, Yu Chen et al. | Conditional Autoencoders (CAEs) offer a flexible, interpretable approach for estimating latent asset-pricing factors from firm characteristics. However, existing studies usually limit the latent factor dimension to around K=5 due to concerns that larger K can degrade performance. To overcome this challenge, we propose a scalable framework that couples a high-dimensional CAE with an uncertainty-aware factor selection procedure. We employ three models for quantile prediction: zero-shot Chronos, a pretrained time-series foundation model (ZS-Chronos), gradient-boosted quantile regression trees using XGBoost and RAPIDS (Q-Boost), and an I.I.D bootstrap-based sample mean model (IID-BS). For each model, we rank factors by forecast uncertainty and retain the top-k most predictable factors for portfolio construction, where k denotes the selected subset of factors. This pruning strategy delivers substantial gains in risk-adjusted performance across all forecasting models. Furthermore, due to each model's uncorrelated predictions, a performance-weighted ensemble consistently outperforms individual models with higher Sharpe, Sortino, and Omega ratios. |
| 2025-11-21 | [A Framework for Adaptive Stabilisation of Nonlinear Stochastic Systems](http://arxiv.org/abs/2511.17436v1) | Seth Siriya, Jingge Zhu et al. | We consider the adaptive control problem for discrete-time, nonlinear stochastic systems with linearly parameterised uncertainty. Assuming access to a parameterised family of controllers that can stabilise the system in a bounded set within an informative region of the state space when the parameter is well-chosen, we propose a certainty equivalence learning-based adaptive control strategy, and subsequently derive stability bounds on the closed-loop system that hold for some probabilities. We then show that if the entire state space is informative, and the family of controllers is globally stabilising with appropriately chosen parameters, high probability stability guarantees can be derived. |
| 2025-11-21 | [Delegation and Lobbying](http://arxiv.org/abs/2511.17391v1) | Thomas Groll, Sharyn O'Halloran | This chapter examines the link between delegation and lobbying, two themes central to political economy. Delegation models explore how legislatures manage uncertainty and control bureaucratic agents, while lobbying models analyze how organized interests influence policy through contributions, information, and advocacy. We review the growing body of research that integrates these literatures, showing how the prospect of lobbying affects legislative incentives to delegate and how the structure of delegated authority shapes lobbying strategies. We highlight common-agency frameworks that capture the recursive relationship between delegation and lobbying and empirical studies documenting how venue choice, information provision, and interest group mobilization mediate delegation outcomes. We also review applications to agency oversight and fiscal policy. Finally, we present a model of regulatory rule-making that embeds lobbying directly into the delegation decision, offering predictions for both theory and empirical analysis. |
| 2025-11-21 | [Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions](http://arxiv.org/abs/2511.17380v1) | Zheng Wang, Yi Zhang et al. | Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts. |
| 2025-11-21 | [POPxf: An Exchange Format for Polynomial Observable Predictions](http://arxiv.org/abs/2511.17348v1) | Ilaria Brivio, Ken Mimasu et al. | We introduce the Polynomial Observable Prediction Exchange Format, POPxf, a structured, machine-readable data format for the publication and exchange of semi-analytical theoretical predictions in high energy physics. The format is designed to encode observables that can be expressed in terms of polynomials in model parameters, with particular emphasis on Effective Field Theory applications. All relevant assumptions and metadata are recorded explicitly, and the treatment of uncertainties and correlations is flexible enough to capture parameter-dependent effects. The format aims to improve reproducibility, facilitate global fits and reinterpretations, and streamline the use of theoretical predictions across the particle physics community. |
| 2025-11-21 | [Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks](http://arxiv.org/abs/2511.17345v1) | Hichem Sahbi | Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work. |
| 2025-11-21 | [Measurements of differential charged-current cross sections on argon for electron neutrinos with final-state protons in MicroBooNE](http://arxiv.org/abs/2511.17342v1) | MicroBooNE collaboration, P. Abratenko et al. | This work presents single-differential electron-neutrino charged-current cross sections on argon measured using the MicroBooNE detector at the Fermi National Accelerator Laboratory. The analysis uses data recorded when the Neutrinos at the Main Injector beam was operating in both neutrino and antineutrino modes, with exposures of $2 \times 10^{20}$ and $5 \times 10^{20}$ protons on target, respectively. A selection algorithm targeting electron-neutrino charged-current interactions with at least one proton, one electron, and no pions in the final topology is used to measure differential cross sections as a function of outgoing electron energy, total visible energy, and opening angle between the electron and the most energetic proton. The interaction rate as a function of proton multiplicity is also reported. The total cross section is measured as [4.1 $\pm$ 0.4 (stat.) $\pm$ 1.2 (syst.)]$\,$$\times \,10^{-39} \mathrm{cm}^{2}/\,\mathrm{nucleon}$. The unfolded cross-section measurements are compared to predictions from neutrino event generators commonly employed in the field. Good agreement is seen across all variables within uncertainties. |
| 2025-11-21 | [On the baryon budget in the X-ray-emitting circumgalactic medium of Milky Way-mass galaxies](http://arxiv.org/abs/2511.17313v1) | Yi Zhang, Soumya Shreeram et al. | Recent observations with SRG/eROSITA have revealed the average X-ray surface brightness profile of the X-ray-emitting circumgalactic medium (CGM) around Milky Way (MW)-mass galaxies, offering valuable insights into the baryon mass in these systems. However, the estimation of the baryon mass depends critically on several assumptions regarding the gas density profile, temperature, metallicity, and the underlying halo mass distribution. Here, we assess how these assumptions affect the inferred baryon mass of the X-ray-emitting CGM in MW-mass galaxies, based on the stacked eROSITA signal. We find that variations in temperature profiles and uncertainties in the halo mass introduce the dominant sources of uncertainty, resulting in X-ray-emitting baryon mass estimates that vary by nearly a factor of four ($0.8-3.5\times10^{11} M_\odot$). Assumptions about metallicity contribute an additional uncertainty of approximately $50\%$. We emphasize that accurate X-ray spectral constraints on gas temperature and metallicity, along with careful modeling of halo mass uncertainty, are essential for accurately estimating the baryon mass for MW-mass galaxies. Future X-ray microcalorimeter missions will be crucial for determining the hot CGM properties and closing the baryon census at the MW-mass scale. |
| 2025-11-21 | [MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning](http://arxiv.org/abs/2511.17299v1) | TomÃ¡Å¡ Musil, MatÄ›j PetrlÃ­k et al. | Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research. |
| 2025-11-21 | [Far-infrared to centimeter emission of very nearby galaxies with archival data](http://arxiv.org/abs/2511.17268v1) | L. Correia, C. Bot et al. | Compared to the well-studied infrared and radio domains, galaxy emission in the millimeter (mm) - centimeter (cm) range has been less observed. In this domain, galaxy emission consists of thermal dust, free-free and synchrotron emissions with a possible additional contribution from anomalous microwave emission (AME) peaking near 1 cm.The aim of this study is to accurately characterize the integrated spectral energy distribution (SED) of galaxies in the mm-cm range. We used COBE-DIRBE, IRAS, Planck, and WMAP all-sky surveys, brought to the same resolution of $1^\circ$, to cover 18 photometric bands from 97$Î¼$m to 1.3 cm. Given the low angular resolution and mixing with foreground and background emission that hampers the detection of the galaxy, our sample consists of 6 of the brightest, nearby galaxies: LMC, SMC, M31, M33, NGC 253 and NGC 4945. We subtract Milky Way dust emission, distant unresolved galaxies, and foreground point sources in the fields. We fit each integrated SED with a model of thermal dust, free-free, synchrotron, AME and Cosmic Microwave Background (CMB) temperature fluctuations. The integrated SEDs of our sample of galaxies are well fitted by the model within the uncertainties, although degeneracies between the different components contributing to the mm-cm emission complicate the estimation of their individual contributions. We do not clearly detect AME in any of our target galaxies, and AME emissivity upper limits are weak compared to Galactic standards, suggesting that the signal of AME might be diluted at the scale of a whole galaxy. We infer positive CMB fluctuations in the background of 5 out of our 6 galaxies. This effect might be related to the degeneracy between the dust emissivity index and CMB fluctuations in the background, or linked to the specific spatial distribution of CMB fluctuations coupled with the low resolution and small number statistics. |
| 2025-11-20 | [A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability](http://arxiv.org/abs/2511.16631v1) | Haihao Shi, Zhenyang Huang et al. | We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $Ï„(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $Ï„(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $Ï„(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae. |
| 2025-11-20 | [Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring](http://arxiv.org/abs/2511.16628v1) | Tammam Bakeer, Max Herbers et al. | In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.   The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics. |
| 2025-11-20 | [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](http://arxiv.org/abs/2511.16625v1) | Elias Hossain, Md Mehedi Hasan Nipu et al. | We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems. |
| 2025-11-20 | [KrkNLO matching and phenomenology for vector boson processes](http://arxiv.org/abs/2511.16605v1) | Pratixan Sarmah, Andrzej SiÃ³dmok et al. | The combination of NLO matrix elements with parton showers is indispensable for LHC physics. Differences between matching methods introduce matching uncertainties, corresponding to formally higher-order terms. We recently presented the process-independent generalisation of the KrkNLO method for NLO matching, which employs a modified PDF factorisation scheme to achieve NLO accuracy. With this factorisation scheme, the method can be used for colour-singlet final-states, and was previously implemented in the Herwig Monte Carlo Event Generator and applied to the diphoton-production process.   Here we present the extension of the implementation of the KrkNLO method within Herwig to support the full class of applicable processes, using an external matrix-element library. We re-validate the implementation, and use it to study the NLO matching uncertainty for four vector-boson production processes at the LHC: $W$, $ZÎ³$, $WW$ and $ZZ$. We demonstrate that the KrkNLO method effectively eliminates the negative-weight problem in NLO event generation, across the four processes studied. We provide detailed comparisons between KrkNLO and variants of the MC@NLO method with different shower starting-scale choices, across processes and throughout phase-space, including double-differential observables. For each process, we compare the predictions to LHC data from ATLAS. |
| 2025-11-20 | [Bayesian polarization calibration and imaging in very long baseline interferometry](http://arxiv.org/abs/2511.16556v1) | Jong-Seo Kim, Jakob Roth et al. | Extracting polarimetric information from very long baseline interferometry (VLBI) data is demanding but vital for understanding the synchrotron radiation process and the magnetic fields of celestial objects, such as active galactic nuclei (AGNs). However, conventional CLEAN-based calibration and imaging methods provide suboptimal resolution without uncertainty estimation of calibration solutions, while requiring manual steering from an experienced user. We present a Bayesian polarization calibration and imaging method using Bayesian imaging software resolve for VLBI data sets, that explores the posterior distribution of antenna-based gains, polarization leakages, and polarimetric images jointly from pre-calibrated data. We demonstrate our calibration and imaging method with observations of the quasar 3C273 with the VLBA at 15 GHz and the blazar OJ287 with the GMVA+ALMA at 86 GHz. Compared to the CLEAN method, our approach provides physically realistic images that satisfy positivity of flux and polarization constraints and can reconstruct complex source structures composed of various spatial scales. Our method systematically accounts for calibration uncertainties in the final images and provides uncertainties of Stokes images and calibration solutions. The automated Bayesian approach for calibration and imaging will be able to obtain high-fidelity polarimetric images using high-quality data from next-generation radio arrays. The pipeline developed for this work is publicly available. |
| 2025-11-20 | [YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras](http://arxiv.org/abs/2511.16521v1) | Fan Yang, Sosuke Yamao et al. | Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications. |
| 2025-11-20 | [General-purpose Data-driven Wall Model for Low-speed Flows Part I: Baseline Model](http://arxiv.org/abs/2511.16511v1) | Yuenong Ling, Imran Hayat et al. | We present a general-purpose wall model for large-eddy simulation. The model builds on the building-block flow principle, leveraging essential physics from simple flows to train a generalizable model applicable across complex geometries and flow conditions. The model addresses key limitations of traditional equilibrium wall models (EQWM) and improves upon shortcomings of earlier building-block-based approaches. The model comprises four components: (i) a baseline wall model, (ii) an error model, (iii) a classifier, and (iv) a confidence score. The baseline model predicts the wall-shear stress, while the error model estimates epistemic errors and aleatoric errors, both used for uncertainty quantification. In Part I of this work, we present the baseline model, while the remaining three components are introduced in Part II. The baseline model is designed to capture a broad range of flow phenomena, including turbulence over curved walls and zero, adverse, and favorable mean pressure gradients, as well as flow separation and laminar flow. The problem is formulated as a regression task to predict wall shear stress using a neural network. Model inputs are localized in space and dimensionless, with their selection guided by information-theoretic criteria. Training data include, among other cases, a newly generated direct numerical simulation dataset of turbulent boundary layers under favorable and adverse PG conditions. Validation is carried out through both a priori and a posteriori tests. The a priori evaluation spans 140 diverse high-fidelity numerical datasets and experiments (67 training cases included), covering turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries, among others. We demonstrate that the baseline wall model outperforms the EQWM in 90% of test scenarios, while maintaining errors below 20% for 98% of the cases. |
| 2025-11-20 | [Quantum corrections in general relativity explored through a GUP-inspired maximal acceleration analysis](http://arxiv.org/abs/2511.16502v1) | Christian Corda, Carlo Cafaro et al. | A maximun acceleration analysis by Pati dating back to 1992 is here improved by replacing the traditional Heisenberg Uncertainty Principle (HUP) with the Generalized Uncertainty Principle (GUP), which predicts the existence of a minimum length in Nature. This new approach allows one to find a numerical value for the maximum acceleration existing in Nature for a physical particle that turns out to be a_{max}\simeq4\frac{c^{2}}{l_{P}}, that is, a function of two fundamental physical quantities such as the speed of light c and the Planck length l_{p}. An application of this result to black hole (BH) physics allows one to estimate a new quantum limit to general relativity. It is indeed shown that, for every real Schwarzschild BH, the maximum gravitational acceleration occurs, without becoming infinite, when the Schwarzschild radial coordinate reaches the gravitational radius. This means that quantum corrections to general relativity become necessary not at the Planck scale, as the majority of researchers in the field think, but at the Schwarzschild scale, in agreement with recent interesting results in the literature. In other words, the quantum nature of physics, which in this case manifests itself through the GUP, appears to prohibit the existence of real singularities, in this current case forbiddiing the gravitational acceleration of a Schwarzschild BH from becoming infinite. |
| 2025-11-20 | [Integrating Deep Learning and Spatial Statistics in Marine Ecosystem Monitoring](http://arxiv.org/abs/2511.16447v1) | Gian Mario Sangiovanni, Gianluca Mastrantonio et al. | In ecology, photogrammetry is a crucial method for efficiently collecting non-destructive samples of natural environments. When estimating the spatial distribution of animals, detecting objects in large-scale images becomes crucial. Object detection models enable large-scale analysis but introduce uncertainty because detection probability depends on various factors. To address detection bias, we model the distribution of a species of benthic animals (holothurians) in an area of the Italian Tyrrhenian coast near Giglio Island using a Thinned Log-Gaussian Cox Process (LGCP). We assume that a "true" intensity function accurately describes the distribution, while the observed process, resulting from independent thinning, is represented by a degraded intensity. The detection function controls the thinning mechanism, influenced by the object's location and other detection-related features. We use manual identification of holothurians as our benchmark. We compare automatic detection with this benchmark, an unthinned LGCP, and the thinned model to highlight the improvements gained from the proposed approach.Our method allows researchers to use photogrammetry, automatically identify objects of interest, and correct biases and approximations caused by the observation process. |
| 2025-11-20 | [Black-Box Uncertainty Estimation for Deep Learning Models in Atomistic Simulations](http://arxiv.org/abs/2511.16439v1) | Idan Fonea, Amir Peles et al. | We analyze an ensemble-based approach for uncertainty quantification (UQ) in atomistic neural networks. This method generates an epistemic uncertainty signal without requiring changes to the underlying multi-headed regression neural network architecture, making it suitable for sealed or black-box models. We apply this method to molecular systems, specifically sodium (Na) and aluminum (Al), under various temperature conditions. By scaling the uncertainty signal, we account for heteroscedasticity in the data. We demonstrate the robustness of the scaled UQ signal for detecting out-of-distribution (OOD) behavior in several scenarios. This UQ signal also correlates with model convergence during training, providing an additional tool for optimizing the training process. |
| 2025-11-19 | [Resolving Ratio Redundancy in Chemical Freeze-out Studies with Principal Component Analysis and Bayesian Calibration](http://arxiv.org/abs/2511.15707v1) | Nachiketa Sarkar | We introduce a Principal Component Analysis (PCA)--Bayesian framework for extracting chemical freeze-out conditions in relativistic heavy-ion collisions that resolves long-standing ambiguities in hadron-ratio--based analyses. By constructing all possible hadron-yield ratios from a chosen set of species and transforming them into an orthogonal PCA basis, the method removes linear redundancies and eliminates the information loss and systematic uncertainties associated with ratio selection. Energy-wise Bayesian calibration of the Hadron Resonance Gas (HRG) model is then performed directly in this decorrelated space, with a Gaussian Process emulator enabling fast and accurate model evaluations. A detailed Sobol sensitivity analysis, together with the PCA loading structure, identifies the most informative ratio combinations and reveals a transition from chemical-potential--dominated to temperature-controlled freeze-out with increasing $\sqrt{s_{NN}}$. The calibrated model reproduces all measured ratios, and the extracted freeze-out parameters are consistent with previous HRG determinations. |
| 2025-11-19 | [Terrestrial Matter Effects on Reactor Antineutrino Oscillations: Constant vs. Fluctuated Density Profiles](http://arxiv.org/abs/2511.15702v1) | Yu-Feng Li, Andong Wang et al. | The JUNO Collaboration has recently released its first reactor antineutrino oscillation result, achieving unprecedented precision in the measurement of $Î”m^2_{21}$ and $\sin^2Î¸_{12}$. We emphasize that the accurate determination and modeling of the terrestrial matter density profile are fundamental for extracting the oscillation parameters and probing the neutrino mass ordering. This paper presents a realistic piecewise-constant model for the shallow crustal density profile along the baselines from Taishan and Yangjiang to the experimental hall, based on geological and petrophysical information. The uncertainty in the density profiles arises from variations in the density and length of each segment, both of which are conservatively estimated to be 10\%. A careful comparison of constant and fluctuated density profiles is provided and the implications for the precision measurement of oscillation parameters are discussed. Finally, we also discuss the possibility of tomography of the shallow Earth's crust in future reactor neutrino experiments. |
| 2025-11-19 | [Further Reduction of the PDF Uncertainty in the High-Mass Drell-Yan Spectrum Utilizing Neutral and Charged Current Inputs](http://arxiv.org/abs/2511.15683v1) | Yao Fu, Raymond Brock et al. | Uncertainties in the parametrization of Parton Distribution Functions are a serious limiting systematic uncertainty in Large Hadron Collider searches for Beyond the Standard Model physics. This is especially true for measurements at high scales induced by quark and anti-quark collisions, where Drell-Yan continuum backgrounds are dominant. In Phys. Rev. D99, 054004 (2019) we presented a unique strategy for improving uncertainties using neutral current Drell-Yan backgrounds and here we update that strategy and include charged current Drell-Yan final states in the program and demonstrate significant improvements. Through a judicious selection of measurable kinematical quantities can reduce the assigned systematic PDF uncertainties by significant factors in limit-setting or discovery for neutral and charged, high mass Intermediate Vector Bosons. This approach will be take advantage of the huge statistical precision of future High Luminosity, Large Hadron Collider Standard Model datasets and could also improve uncertainties in the high statistics results from LHC Run 3. |
| 2025-11-19 | [Branching fraction measurement of the $\mathitÎ› \to p Î¼^- \overlineÎ½_Î¼$ decay](http://arxiv.org/abs/2511.15681v1) | LHCb collaboration, R. Aaij et al. | A measurement of the branching fraction for the decay $\mathitÎ› \to p Î¼^- \overlineÎ½_Î¼$ is presented using $\textit{pp}$ collision data collected by the LHCb experiment at a centre-of-mass energy of 13 TeV. The analysis is based on data recorded between 2016 and 2018, corresponding to an integrated luminosity of $5.4 \ \text{fb}^{-1}$. The result is obtained using $\mathitÎ› \to p Ï€^-$ decays as a normalisation channel. The measured branching fraction is $B(\mathitÎ› \to p Î¼^- \overlineÎ½_Î¼)= (1.462 \pm 0.016 \pm 0.100 \pm 0.011 ) \times 10^{-4}$, where the uncertainties are statistical, systematic, and due to the limited knowledge of the normalisation mode branching fraction, respectively. This result improves the precision of the branching fraction measurement by a factor of two compared to the previous best measurement and sets a more stringent bound on lepton flavour universality in $s \to u$ quark transitions. It is consistent with previous measurements, and the extracted lepton flavour universality test observable, $R^{Î¼e} = \frac{Î“(\mathitÎ› \to p Î¼^- \overlineÎ½_Î¼)}{Î“(\mathitÎ› \to p e^- \overlineÎ½_e)} = 0.175 \pm 0.012$, agrees with the Standard Model prediction. |
| 2025-11-19 | [From Qubits to Couplings: A Hybrid Quantum Machine Learning Framework for LHC Physics](http://arxiv.org/abs/2511.15672v1) | Marwan Ait Haddou, Mohamed Belfkir et al. | In this paper, we propose a new Hybrid Quantum Machine Learning (HyQML) framework to improve the sensitivity of double Higgs boson searches in the $HH \to b\bar{b}Î³Î³$ final state at $\sqrt{s}$ = 13.6 TeV. The proposed model combines parameterized quantum circuits with a classical neural network meta-model, enabling event-level features to be embedded in a quantum feature space while maintaining the optimization stability of classical learning. The hybrid model outperforms both a state-of-the-art XGBoost model and a purely quantum implementation by a factor of two, achieving an expected 95% CL upper limit on the non-resonant double Higgs boson production cross-section of $1.9\timesÏƒ_{\text{SM}}$ and $2.1\timesÏƒ_{\text{SM}}$ under background normalization uncertainties of 10% and 50%, respectively. In addition, expected constraints on the Higgs boson self-coupling $Îº_Î»$ and quartic vector-boson-Higgs coupling $Îº_{2V}$ are found to be improved compared to the classical and purely quantum models. |
| 2025-11-19 | [FantÃ´mas: An analysis of parton distributions in a pion with BÃ©zier parametrizations](http://arxiv.org/abs/2511.15657v1) | Lucas Kotz, Aurore Courtoy et al. | We systematically explore the parametrization dependence of the Parton Distribution Functions (PDFs) to better quantify the true uncertainty from global QCD analyses. To achieve this, we employ a novel technique that automates the generation of polynomial parametrizations for PDFs using BÃ©zier curves. This technique is implemented in a C++ module, named FantÃ´mas, which is now included within the xFitter program. As an example, we examine the charged pion PDF. Our analysis reveals that the sea and gluon distributions in the pion are strongly correlated, and PDF solutions featuring a vanishing gluon and a large quark sea are still experimentally allowed. |
| 2025-11-19 | [INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases](http://arxiv.org/abs/2511.15656v1) | Edward Vendrow, Julia Chae et al. | Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis. |
| 2025-11-19 | [Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning](http://arxiv.org/abs/2511.15633v1) | Tao Hu, Lan Li et al. | Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation. |
| 2025-11-19 | [When to Think and When to Look: Uncertainty-Guided Lookback](http://arxiv.org/abs/2511.15613v1) | Jing Bi, Filippos Bellos et al. | Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets. |
| 2025-11-19 | [Assessing (H)EFT theory errors by pitting EoM against Field Redefinitions](http://arxiv.org/abs/2511.15609v1) | Rodrigo Alonso, Christoph Englert et al. | Truncations of effective field theory expansions are technically necessary but inherently intertwined with the redundancies of general field redefinitions. This can be viewed as a juxtaposition of power-counting and theoretical uncertainties, which seek to estimate neglected higher-dimensional interactions through approaches based on community consensus. One can then understand the invariance of physics under field redefinitions as a data-informed validation of different power-counting schemes, or as a means of assigning theoretical errors in comparison with algebraic, equation of motion-based replacements. Such an approach generalises widely accepted procedures for estimating theoretical uncertainties within the SM to non-renormalisable interactions. We perform a case study for a representative example in Higgs Effective Field theory, focusing on universal Higgs properties tensioned against process-dependent sensitivity expectations. |
| 2025-11-18 | [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](http://arxiv.org/abs/2511.14755v1) | Albert Lin, Alessandro Pinto et al. | As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. |
| 2025-11-18 | [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](http://arxiv.org/abs/2511.14751v1) | Yutian Chen, Yuheng Qiu et al. | We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. |
| 2025-11-18 | [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](http://arxiv.org/abs/2511.14749v1) | Alexander Vedernikov, Puneet Kumar et al. | Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06. |
| 2025-11-18 | [Systematic Study of the Self-Renormalized Nucleon Gluon PDF in Large-Momentum Effective Theory](http://arxiv.org/abs/2511.14708v1) | Alex NieMiera, William Good et al. | We present a systematic study of the nucleon gluon parton distribution function (PDF) using the self-renormalized large-momentum effective theory (LaMET) approach in lattice QCD. This work extends previous gluon-PDF extractions by performing a detailed analysis of key systematic effects, including gauge-link smearing, lattice spacing, pion mass, and nucleon boost momentum. The self-renormalization framework mitigates ultraviolet divergences associated with Wilson-line self-energy and renormalon contributions by combining lattice matrix elements with perturbative short-distance information, thereby preserving the correct infrared structure. Calculations are performed on $N_f=2+1+1$ HISQ ensembles generated by the MILC Collaboration at three lattice spacings and two pion masses, with boosted nucleon states reaching momenta up to 2.2~GeV. We determine renormalization factors from zero-momentum matrix elements and apply hybrid renormalization to suppress discretization artifacts. After extrapolating large-separation behavior and performing Fourier transforms, we reconstruct quasi-PDFs and match them to lightcone PDFs using next-to-leading order Wilson coefficients. Our results demonstrate that smearing and lattice-spacing effects are under control, and pion-mass and lattice-spacing dependence is mild relative to the current $O(10^6)$ statistics; however, momentum dependence remains a significant source of uncertainty. Future work including even larger boost momenta will be essential to reduce systematics in lattice determinations of the gluon PDF and to advance toward precision QCD phenomenology at the LHC and the future Electron-Ion Collider. |
| 2025-11-18 | [Compensating random transition-detection blackouts in Markov networks](http://arxiv.org/abs/2511.14679v1) | Alexander M. Maier, Benjamin HÃ¤sler et al. | In Markov networks, measurement blackouts with unknown frequency compromise observations such that thermodynamic quantities can no longer be inferred reliably. In particular, the observed currents neither discern equilibrium from non-equilibrium nor can they be used in extant estimators of entropy production. Our strategy to eliminate these effects is based on formally attributing the blackouts to a second channel connecting states. The unknown frequency of blackouts and the true underlying transition rates can be determined from the short-time limit of observed waiting-time distributions. A post-modification of observed trajectory data yields a virtual effective dynamics from which the lower bound on entropy production based on thermodynamic uncertainty relations can be recovered fully. Moreover, the post-processed data can be used in waiting-time based estimators. Crucially, our strategy does neither require the blackouts to occur homogeneously nor symmetrically under time-reversal. |
| 2025-11-18 | [High-resolution weak lensing mass mapping from DES-Y3 data using diffusion-based prior](http://arxiv.org/abs/2511.14667v1) | Supranta S. Boruah, Michael Jacob et al. | High-resolution mapping of cosmic mass distribution is essential for a variety of astrophysical applications including understanding cosmic structure formation, and galaxy formation and evolution. However dark matter is not directly observed and therefore we need advanced methods for solving inverse problems to reconstruct the underlying cosmic matter distribution. Here, we train a generative diffusion model and use it in the Diffusion Posterior Sampling (DPS) framework to reconstruct mass maps from Dark Energy Survey-Year 3 (DES-Y3) weak gravitational lensing data at high (1 arcminute) resolution. We show that the standard DPS results are biased, but they can be easily corrected by scaling the log-likelihood score during the diffusion process, yielding unbiased results with proper uncertainty quantification. The resulting mass maps reveal cosmic structures with enhanced detail, opening the door for improved astrophysical studies using the obtained mass maps. |
| 2025-11-18 | [Automated Prediction of Thermodynamic Properties via Bayesian Free-Energy Reconstruction from Molecular Dynamics](http://arxiv.org/abs/2511.14655v1) | Ekaterina Spirande, Timofei Miryashkin et al. | Accurate free-energy calculations are essential for predicting thermodynamic properties and phase stability, but existing methods are limited: phonon-based approaches neglect anharmonicity and liquids, while molecular dynamics (MD) is computationally demanding, neglects low-temperature quantum effects, and often requires manual planning and post-processing of simulations. We present a unified workflow that reconstructs the Helmholtz free-energy surface from MD data using Gaussian Process Regression (GPR), augmented with zero-point energy corrections from harmonic/quasi-harmonic theory. The framework propagates statistical uncertainties, mitigates finite-size effects, and employs active learning to optimize sampling in the volume-temperature space. It applies seamlessly to both crystalline and liquid phases. We demonstrate the methodology by computing heat capacities, thermal expansion, isothermal and adiabatic bulk moduli, and melting properties for nine elemental FCC and BCC metals using 20 classical and machine-learned interatomic potentials, with all predictions accompanied by quantified confidence intervals. Automated, general, and uncertainty-aware, the workflow advances high-throughput thermodynamics and provides a systematic benchmark for interatomic potentials. |
| 2025-11-18 | [Forecasting Synchrotron Spectral Parameters with QUIJOTE-MFI2 in combination with Planck and WMAP](http://arxiv.org/abs/2511.14572v1) | Ana Almeida, JosÃ© Alberto RubiÃ±o-MartÃ­n et al. | We present a parametric component separation forecast for the QUIJOTE-MFI2 instrument (10-20 GHz), assessing its impact on constraining polarised synchrotron emission at $1^\circ$ FWHM and $N_{\rm side}=64$. Using simulated sky maps based on power-law and curved synchrotron spectra, we show that adding QUIJOTE-MFI2 to existing WMAP+$Planck$+MFI data yields statistically unbiased parameter estimates with substantial uncertainty reductions: improvement factors reach $\sim$10 for the synchrotron spectral index ($Î²_s$), $\sim$5 for the curvature parameter ($C_s$), and $\sim$43 for polarisation amplitudes in bright regions. Deep QUIJOTE cosmological fields enable $Î²_s$ constraints even in intrinsically low SNR regions where WMAP+$Planck$ alone remain prior-dominated. Current combined sensitivities are insufficient to detect a synchrotron curvature of $C_s=-0.052$ on a pixel-by-pixel basis, but a $2Ïƒ$ detection is achievable for $|C_s|\gtrsim 0.14$ in the brightest regions of the Galactic plane. In those deep cosmological fields, combining QUIJOTE-MFI2 with WMAP and $Planck$ reduces the median synchrotron residual at 100 GHz by a factor 2.2 (to 0.022 $Î¼$K$_{\rm CMB}$) and the total residual by a factor 1.8 (to 0.030 $Î¼$K$_{\rm CMB}$). These results demonstrate that QUIJOTE-MFI2 will provide critical low-frequency information for modelling Galactic synchrotron emission, offering valuable complementary constraints for future CMB surveys such as LiteBIRD and the Simons Observatory. |
| 2025-11-18 | [Uncertainty analysis of URANS simulations coupled with an anisotropic pressure fluctuation model](http://arxiv.org/abs/2511.14564v1) | Ali Eidi, Richard P. Dwight | Accurate prediction of pressure and velocity fluctuations in turbulent flows is essential for understanding flow-induced vibration and structural fatigue. This study investigates the role of turbulence model parameter uncertainty in such predictions using a combination of global sensitivity analysis, surrogate modeling, and Bayesian inference. The methodology is applied to two fluid-only flow cases: turbulent channel flow and turbulent annular flow. In the channel flow case, calibrated parameter distributions lead to improved agreement with reference data. In the annular case, limited parameter identifiability is observed, though predictions remain consistent with high-fidelity trends. The results demonstrate both the potential and limitations of model calibration strategies in wall-bounded turbulent flows. |
| 2025-11-18 | [A Bayesian INLA-SPDE Approach to Spatio-Temporal Point-Grid Fusion with Change-of-Support and Misaligned Covariates](http://arxiv.org/abs/2511.14535v1) | Weiyue Zheng, Andrew Elliott et al. | We propose a spatio-temporal data-fusion framework for point data and gridded data with variables observed on different spatial supports. A latent Gaussian field with a MatÃ©rn-SPDE prior provides a continuous space representation, while source-specific observation operators map observations to both point measurements and gridded averages, addressing change-of-support and covariate misalignment. Additionally incorporating temporal dependence enables prediction at unknown locations and time points. Inference and prediction are performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equations approach, which delivers fast computation with uncertainty quantification. Our contributions are: a hierarchical model that jointly fuses multiple data sources of the same variable under different spatial and temporal resolutions and measurement errors, and a practical implementation that incorporates misaligned covariates via the same data fusion framework allowing differing covariate supports. We demonstrate the utility of this framework via simulations calibrated to realistic sensor densities and spatial coverage. Using the simulation framework, we explore the stability and performance of the approach with respect to the number of time points and data/covariate availability, demonstrating gains over single-source models through point and gridded data fusion. We apply our framework to soil moisture mapping in the Elliot Water catchment (Angus, Scotland). We fuse in-situ sensor data with aligned and misaligned covariates, satellite data and elevation data to produce daily high resolution maps with uncertainty. |
| 2025-11-17 | [Stellar Flares in the TESS Light Curves of Planet-hosting M dwarfs](http://arxiv.org/abs/2511.13716v1) | Benjamin K. Capistrant, Jason Dittmann | M dwarfs are magnetically active stars that frequently produce flares, which have implications for both stellar evolution and exoplanet studies. Flare occurrence rates and activity levels of M dwarfs correlate with stellar characteristics such as age, mass, and rotation period. We search TESS observations of a known active population of M dwarfs as well as a volume-limited sample of M dwarfs within 15 parsecs. We detect flares in the light curves of these stars, including 276 of 538 M dwarfs within 15 pc, and calculate cumulative flare frequency distributions (FFDs) for each star. Based on flaring behavior, we categorize stars into relatively higher and lower activity groups and fit power laws to their FFDs to compare the power law exponent ($Î±$) across activity levels. We find $Î±=1.99 \pm 0.07$ for the combined FFD of the lower activity M dwarfs, compared to averages of $Î±= 1.94 \pm 0.58$ for highly active stars with 10-100 detected flares, and $Î±= 2.03 \pm 0.43$ for those with > 100 detected flares, suggesting little evolution in the power law distribution of flares as M dwarfs transition from high to low activity states. The uncertainties for the active star groups reflect the standard deviation of $Î±$ values across individual stars within each subset. Because stellar flares and associated stellar activity complicate exoplanet observations, we also examine the subset of M dwarfs with JWST transmission spectroscopy follow-up observations in Cycles 1-3. The flares we detect for these targets are consistent with the broader 15 pc sample, providing context for interpreting planetary atmosphere retrievals from JWST spectra. |
| 2025-11-17 | [The Scatter of the Many Outweighs the Scatter of the Few: Systematic Error Asymmetry in Steeply-Falling Mass Functions for High-Redshift JWST Galaxies](http://arxiv.org/abs/2511.13708v1) | Jay R. Krishnan, Kevork N. Abazajian | The discovery of massive, high redshift galaxies with JWST has been argued to challenge $Î›$CDM: such systems would require extremely rare halos and baryon-to-stellar-mass conversion efficiencies unphysically approaching--or exceeding--100%. If confirmed at galaxy formation forbidden efficiencies, these galaxies could signal new physics beyond standard cosmological structure formation. We develop a galaxy model framework that ties the linear power spectrum to the inferred efficiencies of galaxy growth in order to test the structure formation models. In addition, we incorporate multiple sources of error, including (i) observational sample variance, (ii) asymmetric scatter induced by the steepness of the high-mass halo tail, and (iii) systematic uncertainties in stellar mass estimates. We find that the inferred efficiency of star formation is dominated by systematic uncertainties on the spectral energy distribution inferred stellar mass of the JWST detected galaxies. The systematic uncertainty augments the asymmetry in scatter that largely brings the inferred efficiencies to be in line with that expected from early galaxy formation models. Our framework can be used to test $Î›$CDM as errors are reduced and further detections are made. |
| 2025-11-17 | [Scientific Data Compression and Super-Resolution Sampling](http://arxiv.org/abs/2511.13675v1) | Minh Vu, Andrey Lokhov | Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity. |
| 2025-11-17 | [Sense and Sensitivity - I. Uncertainty analysis of the gas-phase chemistry in AGB outflows](http://arxiv.org/abs/2511.13638v1) | M. Van de Sande, M. Gueguen et al. | Chemical reaction networks are central to all chemical models. Each rate coefficient has an associated uncertainty, which is generally not taken into account when calculating the chemistry. We performed the first uncertainty analysis of a chemical model of C-rich and O-rich AGB outflows using the Rate22 reaction network. Quantifying the error on the model predictions enables us to determine the need for adding complexity to the model. Using a Monte Carlo sampling method, we quantified the impact of the uncertainties on the chemical kinetic data on the predicted fractional abundances and column densities. The errors are caused by a complex interplay of reactions forming and destroying each species. Parent species show an error on their envelope sizes, which is not caused by the uncertainty on their photodissociation rate, but rather the chemistry reforming the parent after its photodissociation. Using photodissociation models to estimate the envelope size might be an oversimplification. The error on the CO envelope impacts retrieved mass-loss rates by up to a factor of two. For daughter species, the error on the peak fractional abundance ranges from a factor of a few to three orders of magnitude, and is on average about 10\% of its value. This error is positively correlated with the error on the column density. The standard model suffices for many species, e.g., the radial distribution of cyanopolyynes and hydrocarbon radicals around IRC +10216. However, including spherical asymmetries, dust-gas chemistry, and photochemistry induced by a close-by stellar companion are still necessary to explain certain observations. |
| 2025-11-17 | [Variable-temperature attenuator calibration method for on-wafer microwave noise characterization of low-noise amplifiers](http://arxiv.org/abs/2511.13619v1) | Anthony J. Ardizzi, Jiayin Zhang et al. | Low-noise cryogenic microwave amplifiers are widely used in applications such as radio astronomy and quantum computing. On-wafer noise characterization of cryogenic low-noise transistors is desirable because it facilitates more rapid characterization of devices prior to packaging, but obtaining accurate noise measurements is difficult due to the uncertainty arising from the input loss and temperature gradients prior to the device-under-test (DUT). Here, we report a calibration method that enables the simultaneous determination of the backend noise temperature and effective-noise-ratio at the input plane of the DUT. The method is based on measuring the S-parameters and noise power of a series of attenuators at two or more distinct physical temperatures. We validate our method by measuring the noise temperature of InP HEMTs in 4-8 GHz. The calibration method can be generalized to measure the microwave noise temperature of any two-port device so long as a series of attenuators can be measured at two or more distinct physical temperatures. |
| 2025-11-17 | [A Gentle Introduction to Conformal Time Series Forecasting](http://arxiv.org/abs/2511.13608v1) | M. Stocker, W. MaÅ‚gorzewicz et al. | Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions. |
| 2025-11-17 | [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](http://arxiv.org/abs/2511.13586v1) | Yinuo Xu, Yan Cui et al. | Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.   To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.   To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction. |
| 2025-11-17 | [Coclique level structure for stochastic chemical reaction networks](http://arxiv.org/abs/2511.13569v1) | Simone Bruno, Yi Fu et al. | Continuous time Markov chains are commonly used as models for the stochastic behavior of chemical reaction networks. More precisely, these Stochastic Chemical Reaction Networks (SCRNs) are frequently used to gain a mechanistic understanding of how chemical reaction rate parameters impact the stochastic behavior of these systems. One property of interest is mean first passage times (MFPTs) between states. However, deriving explicit formulas for MFPTs can be highly complex. In order to address this problem, we first introduce the concept of coclique level structure and develop theorems to determine whether certain SCRNs have this feature by studying associated graphs. Additionally, we develop an algorithm to identify, under specific assumptions, all possible coclique level structures associated with a given SCRN. Finally, we demonstrate how the presence of such a structure in a SCRN allows us to derive closed form formulas for both upper and lower bounds for the MFPTs. Our methods can be applied to SCRNs taking values in a generic finite state space and can also be applied to models with non-mass-action kinetics. We illustrate our results with examples from the biological areas of epigenetics, neurobiology and ecology. |
| 2025-11-17 | [In-memory phononic learning toward cognitive mechanical intelligence](http://arxiv.org/abs/2511.13543v1) | Yuning Zhang, K. W. Wang | Modern autonomous systems are driving the critical need for next-generation adaptive materials and structures with embodied intelligence, i.e., the embodiment of memory, perception, learning, and decision-making within the mechanical domain. A fundamental challenge is the seamless and efficient integration of memory with information processing in a physically interpretable way that enables cognitive learning and decision-making under uncertainty. Prevailing paradigms, from intricate logic cascades to black-box morphological computing or physical neural networks, are seriously limited by trade-offs among efficiency, scalability, interpretability, transparency, and reliance on additional electronics. Here, we introduce in-memory phononic learning, a paradigm-shifting framework that unifies nonvolatile mechanical memory with wave-based perception within a phononic metastructure. Our system encodes spatial information into stable structural states as mechanical memory that directly programs its elastic wave-propagation landscape. This memory/wave-dynamics coupling enables effective sensory perception, decomposing complex patterns into informative geometric features through frequency-selective wave localization. Learning is created by optimizing input waveforms to selectively probe these features for memory-pattern classification, with decisions inferred directly from the output wave energy, thereby completing the entire information loop mechanically through an efficient and physically transparent mechanism without hidden architectures or electronics. This work transcends the paradigm of 'materials that compute' to cognitive matter capable of interpreting dynamic environments, paving the way for future intelligent structural-material systems with low power consumption, more direct interaction with surroundings, and enhanced cybersecurity and resilience in harsh conditions. |
| 2025-11-17 | [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](http://arxiv.org/abs/2511.13533v1) | Jeffrey Wen, Rizwan Ahmad et al. | In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data. |
| 2025-11-17 | [Simultaneous variances of Pauli strings, weighted independence numbers, and a new kind of perfection of graphs](http://arxiv.org/abs/2511.13531v1) | Zhen-Peng Xu, Jie Wang et al. | A set of Pauli stings is well characterized by the graph that encodes its commutatitivity structure, i.e., by its frustration graph. This graph provides a natural interface between graph theory and quantum information, which we explore in this work. We investigate all aspects of this interface for a special class of graphs that bears tight connections between the groundstate structures of a spin systems and topological structure of a graph. We call this class $\hbar$-perfect, as it extends the class of perfect and $h$-perfect graphs.   Having an $\hbar$-perfect graph opens up several applications: we find efficient schemes for entanglement detection, a connection to the complexity of shadow tomography, tight uncertainty relations and a construction for computing good lower on bounds ground state energies. Conversely this also induces quantum algorithms for computing the independence number. Albeit those algorithms do not immediately promise an advantage in runtime, we show that an approximate Hamilton encoding of the independence number can be achieved with an amount of qubits that typically scales logarithmically in the number of vertices. We also we also determine the behavior of $\hbar$-perfectness under basic graph operations and evaluate their prevalence among all graphs. |
| 2025-11-17 | [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](http://arxiv.org/abs/2511.13514v1) | Pragatheeswaran Vipulananthan, Kamal Premaratne et al. | Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{Ã¶}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process. |
| 2025-11-17 | [The Liquid Buffer: Multi-Year Storage for Defossilization and Energy Security under Climate Uncertainty](http://arxiv.org/abs/2511.13513v1) | Leonard GÃ¶ke, Jan Wohland et al. | The climate-driven uncertainty of renewable generation and electricity demand challenges energy security in net-zero energy systems. By introducing a scalable stochastic model that implicitly accounts for 51'840 climate years, this paper identifies multi-year storage of liquid hydrocarbons as a key option for managing climate uncertainty and ensuring energy security. In Europe, multi-year storage reduces system costs by 4.1%, fossil imports by 86%, and curtailment by 60%. The benefit of multi-year storage is that a renewable surplus in one year is not curtailed but converted to synthetic oil, with hydrogen as an intermediate product, and stored to balance a future deficit. We find that the required energy capacity for liquid hydrocarbons is 525 TWh, a quarter of the European Union's current oil and gas reserves, complemented by 116 TWh for hydrogen storage. Security of supply remains high and unserved energy only amounts to 0.0035 per thousand, well below the common target of 0.02 per thousand. |
| 2025-11-17 | [The bulk metal content of WASP-80 b from joint interior-atmosphere retrievals: Breaking degeneracies and exploring biases with panchromatic spectra](http://arxiv.org/abs/2511.13483v1) | Lorena AcuÃ±a-Aguirre, Laura Kreidberg et al. | WASP-80 b is an unusually low-density exoplanet in tension with the metal-rich composition expected for a planet of its mass. We aim to derive precise constraints on WASP-80 b's bulk metal mass fraction, atmospheric composition, and thermal structure. We conducted a suite of retrievals using three approaches: traditional interior-only, atmosphere-only, and joint interior-atmosphere retrievals. We coupled the open-source models GASTLI and petitRADTRANS, which describe planetary structure and thermal evolution, and atmospheric chemistry and clouds, respectively. Our retrievals combine mass and age with panchromatic spectra from JWST and HST in both transmission (0.5-4 $Î¼$m) and emission (1-12 $Î¼$m) as observational constraints. We identify two fiducial scenarios. In the first, WASP-80 b has an internal temperature consistent with its age in the absence of external heating sources, and its atmosphere is in chemical equilibrium, with an atmospheric metallicity M/H = 2.75$^{+0.88}_{-0.56}$x solar, a bulk metal mass fraction $Z_{planet}=0.12\pm0.02$, and a core mass $M_{core}=3.49^{+3.49}_{-1.59} \ M_{\oplus}$. In the second scenario, WASP-80 b may be inflated by an additional heat source - possibly induced by magnetic fields - with an atmospheric metallicity M/H = 10.00$^{+8.20}_{-4.75}$x solar, $Z_{planet}=0.28\pm0.11$, and $M_{core}=31.8^{+21.3}_{-17.5} \ M_{\oplus}$. The super-solar M/H and sub-solar C/O ratios in both scenarios suggest late pebble or planetesimal accretion, while additional heating is required to reconcile the data with the more massive core predicted by the core accretion paradigm. In general, joint retrievals are inherently affected by a degeneracy between atmospheric chemistry and internal structure. Together with flexible cloud treatment and an unweighted likelihood, this leads to larger uncertainties in bulk and atmospheric compositions than previously claimed. |
| 2025-11-17 | [Probing parameters estimation with Gaussian non-commutative measurements](http://arxiv.org/abs/2511.13451v1) | Alice P. G. Hall, Carlos H. S. Vieira et al. | Gaussian quantum states and channels are pivotal across many branches of quantum science and their applications, including the processing and storage of quantum information, the investigation of thermodynamics in the quantum regime, and quantum computation. The great advantage is that Gaussian states are experimentally accessible via their first and second statistical moments. In this work, we investigate parameter estimation for Gaussian states, in which the probe-state preparation stage involves two noncommutative Gaussian measurements on the position and momentum observables, introducing tunable parameters. The influence of these noncommutative Gaussian measurements is investigated through the quantum Fisher information (QFI). We showed that the QFI for characterizing Gaussian channels can be increased by adjusting the uncertainty parameters in the preparation of the probe state. Furthermore, if the probe is initially in a thermal state, probe-state preparation may generate quantum coherence in its energy basis. We showed that not only does the amount of coherence affect the improvement of the QFI, but also the rate of change of the coherence with respect to the parameter to be estimated. The proposed probe-state protocol is applied to two paradigmatic single-mode Gaussian channels, the attenuator and amplification channels, which are building blocks of Gaussian quantum information. Our results contribute to the use of coherence in quantum metrology and are experimentally feasible in quantum-optical devices. |
| 2025-11-17 | [The Intrinsic Angular - Momentum of Particles and the Resolution of the Spin-Statistics Theorem](http://arxiv.org/abs/2511.13360v1) | Enrico Santamato, Francesco De Martini | The traditional Standard Quantum Mechanics (SQM) theory is unable to solve the Spin-s problem, i.e., to justify the utterly important "Pauli Exclusion Principle". A complete and straightforward solution of the Spin-Statistics problem is presented based on the "Weyl Integrable Quantum Mechanics" (WIQM) theory. This theory provides a Weyl-gauge invariant formulation of the Standard Quantum Mechanics and reproduces successfully, with no restrictions, the full set of the quantum mechanical processes, including the formulation of Dirac's or SchrÃ¶dinger's equation, of Heisenberg's uncertainty relations, and of the nonlocal EPR correlations. etc. When the Weyl Integrable Quantum Mechanics is applied to a system made of many identical particles with spin, an additional constant property of all elementary particles enters naturally into play: the "intrinsic helicity", or the "intrinsic angular - momentum". This additional particle property, not considered by Standard Quantum Mechanics, determines the correct Spin-Statistics Connection (SSC) observed in Nature. All this leads to the consideration of a novel, most complete (in the EPR sense) quantum mechanical theory. |
| 2025-11-17 | [Basis Immunity: Isotropy as a Regularizer for Uncertainty](http://arxiv.org/abs/2511.13334v1) | Florent Segonne | Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.   In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.   Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.   This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts. |
| 2025-11-17 | [Sub-Solar Mass Intermediate Mass Ratio Inspirals: Waveform Systematics and Detection Prospects with Gravitational Waves](http://arxiv.org/abs/2511.13324v1) | Devesh Giri, Bhooshan Gadre | We investigate the detectability and waveform systematics of sub-solar mass intermediate mass-ratio inspirals (SSM-IMRIs), characterized by mass ratios $q \sim 10^2-10^4$. Using the black hole perturbation theory surrogate model \textsc{BHPTNRSur1dq1e4} as a reference, we assess the performance of the \textsc{IMRPhenomX} phenomenological family in the high-mass-ratio regime. We find that the inclusion of higher-order gravitational wave modes is critical; their exclusion may degrade the signal-to-noise ratio by factors of $\sim3-5$ relative to quadrupole-only templates. With optimal mode inclusion, SSM-IMRIs are observable out to luminosity distances of $\sim575$ Mpc ($z\sim0.12$) with Advanced LIGO and $\sim10.5$ Gpc ($z\sim1.4$) with the Einstein Telescope. However, we identify substantial systematic uncertainties in current phenomenological approximants. Matches between \textsc{IMRPhenomX} and the reference surrogate model \textsc{BHPTNRSur1dq1e4} degrade to values as low as 0.2 for edge-on inclinations, and fitting factors consistently fall below 0.9, indicating a significant loss of effectualness in template-bank searches. Bayesian parameter estimation reveals that these modeling discrepancies induce systematic biases that exceed statistical errors by multiple standard deviations, underscoring the necessity for waveform models calibrated to perturbation theory in the intermediate mass-ratio regime for robust detection and inference. |
| 2025-11-14 | [Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems](http://arxiv.org/abs/2511.11567v1) | Allen Emmanuel Binny, Anushri Dixit | Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. |
| 2025-11-14 | [Drone Swarm Energy Management](http://arxiv.org/abs/2511.11557v1) | Michael Z. Zgurovsky, Pavlo O. Kasyanov et al. | This note presents an analytical framework for decision-making in drone swarm systems operating under uncertainty, based on the integration of Partially Observable Markov Decision Processes (POMDP) with Deep Deterministic Policy Gradient (DDPG) reinforcement learning. The proposed approach enables adaptive control and cooperative behavior of unmanned aerial vehicles (UAVs) within a cognitive AI platform, where each agent learns optimal energy management and navigation policies from dynamic environmental states. We extend the standard DDPG architecture with a belief-state representation derived from Bayesian filtering, allowing for robust decision-making in partially observable environments. In this paper, for the Gaussian case, we numerically compare the performance of policies derived from DDPG to optimal policies for discretized versions of the original continuous problem. Simulation results demonstrate that the POMDP-DDPG-based swarm control model significantly improves mission success rates and energy efficiency compared to baseline methods. The developed framework supports distributed learning and decision coordination across multiple agents, providing a foundation for scalable cognitive swarm autonomy. The outcomes of this research contribute to the advancement of energy-aware control algorithms for intelligent multi-agent systems and can be applied in security, environmental monitoring, and infrastructure inspection scenarios. |
| 2025-11-14 | [Testing the cosmological Euler equation: viscosity, equivalence principle, and gravity beyond general relativity](http://arxiv.org/abs/2511.11554v1) | Ziyang Zheng, Malte Schneider et al. | We investigate how the cosmological Euler equation can be tested in the presence of viscous dark matter, violations of the equivalence principle (EP), and modifications of gravity, while relying on minimal theoretical assumptions. Extending the previous analysis, we generalize the observable $E_P$, which quantifies EP violation, to $\tilde{E}_P$, discuss the degeneracy between bulk and shear viscosities and EP-violating effects, and explicitly show that the EP can still be tested in the small-viscosity limit. In addition, we identify a model-independent observable, $C_{\rm vis,0}$, which characterizes the present-day dark matter viscosity and can be measured from relativistic galaxy number counts by cross-correlating two galaxy populations. We perform forecasts for three forthcoming Stage-IV surveys: DESI, Euclid, and SKA Phase 2 (SKA2), and find that $C_{\rm vis,0}$ can be tightly constrained, at the level of $\mathcal{O}(10^{-6})$ or better in all cases. Among these surveys, SKA2 provides the tightest constraint, with a $1Ïƒ$ uncertainty of $1.08 \times 10^{-7}$ on $C_{\rm vis,0}$. |
| 2025-11-14 | [Looking at infrared background radiation anisotropies with Spitzer II. Small scale anisotropies and their implications for new and upcoming space surveys](http://arxiv.org/abs/2511.11501v1) | Aidan J. Kaminsky, Alexander Kashlinsky et al. | Spitzer-based cosmic infrared background (CIB) fluctuations at arcminute-to-degree scales indicate the presence of new populations, whereas sub-arcminute power arises from known $z\lesssim 6$ galaxies. We reconstruct the evolution of the near-IR CIB anisotropies on sub-arcminute scales by known galaxy populations. This method is based on, and significantly advanced over, the empirical reconstruction by \cite{Helgason2012} which is combined with the halo model connecting galaxies to their host dark matter (DM) halos. The modeled CIB fluctuations from known galaxies produce the majority of the observed small-scale signal down to statistical uncertainties of $< 10\%$ and we constrain the evolution of the halo mass regime hosting such galaxies. Thus the large-scale CIB fluctuations from new populations are produced by sources with negligible small-scale power. This appears to conflict with the presented Intra-halo light (IHL) models, but is accounted for if the new sources are at high $z$. Our analysis spanning several Spitzer datasets allows us to narrow the estimated contributions of remaining known galaxies to the CIB anisotropies to be probed potentially from surveys by new and upcoming space missions such as Euclid, SPHEREx, and Roman. Of these, the Roman surveys have the best prospects for measuring the source-subtracted CIB and probing the nature of the underlying new populations at $Î»<2\ Î¼$m, followed by Euclid's surveys, while for SPHEREx the source-subtracted CIB signal from them appears significantly overwhelmed by the CIB from remaining known galaxies. |
| 2025-11-14 | [Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images](http://arxiv.org/abs/2511.11486v1) | Roman Kinakh, Gonzalo R. RÃ­os-MuÃ±oz et al. | Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows. |
| 2025-11-14 | [Risk-Aware Deep Reinforcement Learning for Dynamic Portfolio Optimization](http://arxiv.org/abs/2511.11481v1) | Emmanuel Lwele, Sabuni Emmanuel et al. | This paper presents a deep reinforcement learning (DRL) framework for dynamic portfolio optimization under market uncertainty and risk. The proposed model integrates a Sharpe ratio-based reward function with direct risk control mechanisms, including maximum drawdown and volatility constraints. Proximal Policy Optimization (PPO) is employed to learn adaptive asset allocation strategies over historical financial time series. Model performance is benchmarked against mean-variance and equal-weight portfolio strategies using backtesting on high-performing equities. Results indicate that the DRL agent stabilizes volatility successfully but suffers from degraded risk-adjusted returns due to over-conservative policy convergence, highlighting the challenge of balancing exploration, return maximization, and risk mitigation. The study underscores the need for improved reward shaping and hybrid risk-aware strategies to enhance the practical deployment of DRL-based portfolio allocation models. |
| 2025-11-14 | [Higher-order QCD corrections to top-quark pair production in association with a jet](http://arxiv.org/abs/2511.11431v1) | Simon Badger, Matteo Bechetti et al. | The production of a top-quark pair, the heaviest known elementary particle, in association with a light jet is a key process for studying the properties of the Standard Model of Particle Physics. Due to its significance as a signal process with considerable sensitivity to the top-quark mass and as a background process for new physics searches, it is crucial to predict differential cross sections with high precision. In this article, we present, for the first time, predictions for various kinematical observables at next-to-next-to-leading order in Quantum Chromodynamics. The perturbative behavior is analyzed, and uncertainties arising from missing higher-order contributions are substantially reduced. The necessary two-loop amplitudes have been evaluated in the leading-color approximation, and we provide estimates for the impact of the missing contributions. |
| 2025-11-14 | [Risk averse deterministic Kalman filters for uncertain dynamical systems](http://arxiv.org/abs/2511.11350v1) | Karl Kunisch, Jesper SchrÃ¶der | Taking a deterministic viewpoint this work investigates extensions of the Kalman-Bucy filter for state reconstruction to systems containing parametric uncertainty in the state operator. The emphasis lies on risk averse designs reducing the probability of large reconstruction errors. In a theoretical analysis error bounds in terms of the variance of the uncertainties are derived. The article concludes with a numerical implementation of two examples allowing for a comparison of risk neutral and risk averse estimators. |
| 2025-11-14 | [DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding](http://arxiv.org/abs/2511.11313v1) | Tanveer Hannan, Dimitrios Mallios et al. | Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material. |
| 2025-11-14 | [Simulating an Autonomous System in CARLA using ROS 2](http://arxiv.org/abs/2511.11310v1) | Joseph Abdo, Aditya Shibu et al. | Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360Â° light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS). |
| 2025-11-13 | [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](http://arxiv.org/abs/2511.10619v1) | Avrim Blum, Marten Garicano et al. | The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Î©(k)$ and $Î©(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied. |
| 2025-11-13 | [Optimizing the flight path for a scouting Uncrewed Aerial Vehicle](http://arxiv.org/abs/2511.10598v1) | Raghav Adhikari, Sachet Khatiwada et al. | Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty. |
| 2025-11-13 | [Carbox: an end-to-end differentiable astrochemical simulation framework](http://arxiv.org/abs/2511.10558v1) | Gijs VermariÃ«n, Tommaso Grassi et al. | Since the first observations of interstellar molecules, astrochemical simulations have been employed to model and understand its formation and destruction path- ways. With the advent of high-resolution telescopes such as JWST and ALMA, the number of detected molecules has increased significantly, thereby creating a need for increasingly complex chemical reaction networks. To model such complex systems, we have developed Carbox, a new astrochemical simulation code that leverages the modern high-performance transformation framework Jax. With Jax enabling computational efficiency and differentiability, Carbox can easily utilize GPU acceleration, be used to study sensitivity and uncertainty, and interface with advances in Scientific Machine Learning. All of these features are crucial for modeling the molecules observed by current and next-generation telescopes. |
| 2025-11-13 | [VELOCE III. Reconstructing Radial Velocity Curves of Classical Cepheids](http://arxiv.org/abs/2511.10534v1) | Giordano Viviani, Richard I. Anderson | We present a novel framework for accurately reconstructing radial velocity (RV) curves of classical Cepheids (Cepheids) from sparsely sampled time-series data suitable for application in large spectroscopic surveys. The framework provides a set of priors for the principal components of RV curves established based on high-precision measurements from the VELOCE project; template RV curves of Cepheids can be readily extracted from our results. We demonstrate the ability of our framework to estimate unbiased pulsation average velocities, $v_Î³$, to within $20-30$m/s, and peak-to-peak amplitudes, $P2P$, to within $\sim 2\%$. Subsampling the initial data set, we show that $v_Î³$ and $P2P$ can be determined to within $\sim 0.35$ km/s and $\sim 6-7\%$, respectively, from as few as three observations. We fitted existing time-series RV data of Cepheids in the LMC and SMC using this framework and obtained typical RMSE of $0.5-2.0$ km/s. The typical total uncertainty on $v_Î³$ achieved for the SMC Cepheids is $\sim 0.85$ km/s, providing sensitivity to spectroscopic binaries (SB). We identified 8 SB1 systems; two and one of which are new detections in the LMC and SMC, respectively. This yields a single-lined SB fraction of $\sim 25\%$ and $29\%$ in the two galaxies, similar to the Milky Way's SB fraction of $29\%$ established as part of VELOCE. Despite their relatively small number, LMC Cepheids reproduce the known line-of-sight component of the LMC's large-scale rotation, which differs in the extremes by more than $80$km/s. The kinematics of the SMC are more complex and not sufficiently sampled by the available Cepheids. Our framework is designed to yield accurate $v_Î³$ and $P2P$ of Cepheids observed by large spectroscopic surveys, such as 4MOST, SDSS-V, and others, and will unlock new insights into the kinematics and multiplicity of evolved intermediate-mass stellar populations. |
| 2025-11-13 | [Evaluation of Grid-based Uncertainty Propagation for Collaborative Self-Calibration in Indoor Positioning Systems](http://arxiv.org/abs/2511.10526v1) | Andrea Jung, Paul Schwarzbach et al. | Radio-based localization systems conventionally require stationary reference points (e.g. anchors) with precisely surveyed positions, making deployment time-consuming and costly. This paper presents an empirical evaluation of collaborative self-calibration for Ultra-Wideband (UWB) networks, extending a discrete Bayesian approach based on grid-based uncertainty propagation. The enhanced algorithm reduces measurement availability requirements while maintaining positioning accuracy through probabilistic state estimation. We validate the approach using real-world data from controlled indoor UWB network experiments with 12 nodes in a static environment. Experimental evaluation demonstrates 0.28~m mean ranging error under line-of-sight conditions and 1.11~m overall ranging error across mixed propagation scenarios, achieving sub-meter positioning accuracy. Results demonstrate the algorithm's robustness to measurement noise and partial connectivity scenarios typical in industrial deployments. The findings contribute to automated UWB network initialization for indoor positioning applications, reducing infrastructure dependency compared to manual anchor calibration procedures. |
| 2025-11-13 | [Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling](http://arxiv.org/abs/2511.10501v1) | Georgios Chalkiadakis, Charilaos Akasiadis et al. | This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability. |
| 2025-11-13 | [Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance](http://arxiv.org/abs/2511.10475v1) | Ã‡aÄŸrÄ± Eser, Zeynep Sonat BaltacÄ± et al. | Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM. |
| 2025-11-13 | [Riccati-ZORO: An efficient algorithm for heuristic online optimization of internal feedback laws in robust and stochastic model predictive control](http://arxiv.org/abs/2511.10473v1) | Florian Messerer, Yunfan Gao et al. | We present Riccati-ZORO, an algorithm for tube-based optimal control problems (OCP). Tube OCPs predict a tube of trajectories in order to capture predictive uncertainty. The tube induces a constraint tightening via additional backoff terms. This backoff can significantly affect the performance, and thus implicitly defines a cost of uncertainty. Optimizing the feedback law used to predict the tube can significantly reduce the backoffs, but its online computation is challenging.   Riccati-ZORO jointly optimizes the nominal trajectory and uncertainty tube based on a heuristic uncertainty cost design. The algorithm alternates between two subproblems: (i) a nominal OCP with fixed backoffs, (ii) an unconstrained tube OCP, which optimizes the feedback gains for a fixed nominal trajectory. For the tube optimization, we propose a cost function informed by the proximity of the nominal trajectory to constraints, prioritizing reduction of the corresponding backoffs. These ideas are developed in detail for ellipsoidal tubes under linear state feedback. In this case, the decomposition into the two subproblems yields a substantial reduction of the computational complexity with respect to the state dimension from $\mathcal{O}(n_x^6)$ to $\mathcal{O}(n_x^3)$, i.e., the complexity of a nominal OCP.   We investigate the algorithm in numerical experiments, and provide two open-source implementations: a prototyping version in CasADi and a high-performance implementation integrated into the acados OCP solver. |
| 2025-11-13 | [Correlations of ALMA CO(2-1) with JWST mid-infrared fluxes down to scale of $\lesssim$100 parsec in nearby star-forming galaxies from PHANGS](http://arxiv.org/abs/2511.10464v1) | Tao Jing, Cheng Li | We investigate the correlations of CO (2-1) emission (${I_{\rm CO}}$) with PAH (${I_{\rm F770W, PAH}}$ and ${I_{\rm F1130W}}$) and dust (${I_{\rm F2100W}}$) emission down to scales of $\lesssim$ 100 pc, by applying \raddest, a novel regression technique recently developed by T. Jing & C. Li (2025) that effectively handles uncertainties and outliers in datasets, to 19 nearby star-forming galaxies in the PHANGS sample. We find that for the majority of the data points in all galaxies, the scaling of ${I_{\rm CO}}$ with ${I_{\rm F770W, PAH}}$, ${I_{\rm F1130W}}$, and ${I_{\rm F2100W}}$ can be well described by log-linear relations, though with substantial dependence on ionization conditions (i.e., HII-like, composite-like, and AGN-like). Under given ionization conditions, significant galaxy-to-galaxy variations are identified, and are primarily attributed to variations of intercept $b$, which exhibits clear bimodality. This bimodality is related to the overall host galaxy star formation strength. The differences in slope $k$ and intrinsic scatter $Ïƒ$ across different MIR bands (${I_{\rm F770W, PAH}}$, ${I_{\rm F1130W}}$, and ${I_{\rm F2100W}}$) are minor compared to their galaxy-to-galaxy variations. All parameters ($k$, $b$, and $Ïƒ$) depend on the spatial scale of measurement, suggesting that the coupling among CO, PAH, and dust is regulated by different mechanisms at varying scales. We identify non-log-linear behaviors in the brightest regions, where deviations are primarily characterized by flattening of slope. No significant (3$Ïƒ$) correlations are found between global properties and the best-fit parameters. We discuss the comparison to previous studies and plausible physics behind the statistical results obtained in this work. |
| 2025-11-13 | [A scalable and accurate framework for self-calibrating null depth retrieval using neural posterior estimation](http://arxiv.org/abs/2511.10455v1) | Baoyi Zeng, Marc-Antoine Martinod et al. | Accurate null depth retrieval is critical in nulling interferometry. However, achieving accurate null depth calibration is challenging due to various noise sources, instrumental imperfections, and the complexity of real observational environments. These challenges necessitate advanced calibration techniques that can efficiently handle such uncertainties while maintaining a high accuracy. This paper aims to incorporate machine-learning techniques with a Bayesian inference to improve the accuracy and efficiency of null depth retrieval in nulling interferometry. Specifically, it explores the use of neural posterior estimation (NPE) to develop models that overcome the computational limitations of conventional methods, such as numerical self-calibration (NSC), providing a more robust solution for accurate null depth calibration. An NPE-based model was developed, with a simulator that incorporates real data to better represent specific conditions. The model was tested on both synthetic and observational data from the LBTI nuller for evaluation. The NPE model successfully demonstrated improved efficiency, achieving results comparable to current methods in use. It achieved a null depth retrieval accuracy down to a few $10^{-4}$ on real observational data, matching the performance of conventional approaches while offering significant computational advantages, reducing the data retrieval time to one-quarter of the time required by self-calibration methods. The NPE model presents a practical and scalable solution for null depth calibration in nulling interferometry, offering substantial improvements in efficiency over existing methods with a better precision and application to other interferometric techniques. |
| 2025-11-10 | [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](http://arxiv.org/abs/2511.07403v1) | Hunar Batra, Haoqin Tu et al. | Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning. |
| 2025-11-10 | [The position of SN 1987A](http://arxiv.org/abs/2511.07394v1) | C. Tegkelidis, J. Larsson et al. | The accurate positional measurement of Supernova (SN) 1987A is important for determining the kick velocity of its compact object and the velocities of the ejecta and various shock components. In this work, we perform absolute astrometry to determine the position of SN 1987A. We used multi-epoch Hubble Space Telescope imaging to model the early ejecta and the equatorial ring (ER). We combined our measurements and obtained the celestial coordinates in the International Celestial Reference System (ICRS) by registering the observations onto Gaia Data Release 3. The final average position of the different measurements is ${\alpha = 5^{\mathrm{h}}~ 35^{\rm{m}}~ 27^{\rm{s}}.9884(30)}$, ${\delta = -69^{\circ}~ 16'~ 11''.1134(136)}$ (ICRS J2016). The early ejecta position is located 14 mas south and 16 mas east of the ER center, with the offset being significant at 96% confidence. The offset may be due to instrument and/or filter-dependent systematics and registration uncertainties, though an intrinsic explosion offset relative to the ER remains possible. Image registration with proper motion corrections yields similar astrometry and a source proper motion of ${\mu_{\rm east} (\equiv \rm{PM_{\alpha }*}) = 1.60 \pm 0.15 ~\rm{mas ~ yr^{-1}}}$ and ${\mu_{\rm{north}} (\equiv \rm{PM_{\delta}}) = 0.44 \pm 0.09~\rm{mas ~ yr^{-1}}}$, in agreement with the typical local motion of the Large Magellanic Cloud. The absolute positional uncertainty of 21 mas adds a systematic uncertainty to the sky-plane kick velocity of ${123}~(t/40~\rm{yr})^{-1}~\rm{km~s}^{-1}$, where $t$ is the time since the explosion. Comparing the location of the compact source observed with JWST to our updated position implies a sky-plane kick of ${399\pm148~\mathrm{km~s^{-1}}}$ and a 3D kick of ${472\pm126~\mathrm{km~s^{-1}}}$, which is consistent with previous estimates. |
| 2025-11-10 | [Policy Learning for Perturbance-wise Linear Quadratic Control Problem](http://arxiv.org/abs/2511.07388v1) | Haoran Zhang, Wenhao Zhang et al. | We study finite horizon linear quadratic control with additive noise in a perturbancewise framework that unifies the classical model, a constraint embedded affine policy class, and a distributionally robust formulation with a Wasserstein ambiguity set. Based on an augmented affine representation, we model feasibility as an affine perturbation and unknown noise as distributional perturbation from samples, thereby addressing constrained implementation and model uncertainty in a single scheme. First, we construct an implementable policy gradient method that accommodates nonzero noise means estimated from data. Second, we analyze its convergence under constant stepsizes chosen as simple polynomials of problem parameters, ensuring global decrease of the value function. Finally, numerical studies: mean variance portfolio allocation and dynamic benchmark tracking on real data, validating stable convergence and illuminating sensitivity tradeoffs across horizon length, trading cost intensity, state penalty scale, and estimation window. |
| 2025-11-10 | [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](http://arxiv.org/abs/2511.07377v1) | June Moh Goo, Zichao Zeng et al. | LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems. |
| 2025-11-10 | [Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning](http://arxiv.org/abs/2511.07368v1) | Dake Bu, Wei Huang et al. | Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RLVR and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of exploration and entropy stability in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.   To reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to chain-of-thought reweighting. We show that several phenomena recently observed in empirical studies arise naturally in this setting: (1) RLVR induces a squeezing effect, reducing reasoning entropy and forgetting some correct paths; (2) population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and (3) certain rare, high-uncertainty reasoning paths by the base model are responsible for solving hard problem instances.   Together, these explain why exploration -- even when confined to the base model's reasoning scope -- remains essential: it preserves access to rare but crucial reasoning traces needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further show that exploration strategies such as rejecting easy instances and KL regularization help preserve rare reasoning traces. Empirical simulations corroborate our theoretical results. |
| 2025-11-10 | [Offset-Free Robust Nonlinear Control Using Data-Driven Model: A Nonlinear Multi-Model Computationally Efficient Approach](http://arxiv.org/abs/2511.07255v1) | Carine Menezes Rebello, Erbet Almeida Costa et al. | Robust model predictive control (MPC) aims to preserve performance under model-plant mismatch, yet robust formulations for nonlinear MPC (NMPC) with data-driven surrogates remain limited. This work proposes an offset-free robust NMPC scheme based on symbolic regression (SR). Using a compact NARX structure, we identify interpretable surrogate models that explicitly represent epistemic (structural) uncertainty at the operating-zone level, and we enforce robustness by embedding these models as hard constraints. gest margins. The single-zone variant (RNMPC$_{SZ}$) was investigated. Synthetic data were generated within one operating zone to identify SR models that are embedded as constraints, while the nominal predictor remains fixed, and the multi-zone variant (RNMPC$_{MZ}$), zone-specific SR models from multiple operating zones are jointly enforced in the constraint set; at each set-point change, the nominal predictor is re-scheduled to the SR model of the newly active zone. In both cases, robustness is induced by the intersection of the admissible sets defined by the enforced SR models, without modifying the nominal cost or introducing ancillary tube dynamics. The approach was validated on a simulated pilot-scale electric submersible pump (ESP) system with pronounced nonlinearities and dynamically varying safety envelopes (downthrust and upthrust). RNMPC$_{SZ}$ and RNMPC$_{MZ}$ maintained disturbance tracking and rejection, and by intersecting models in the constraints, they increased margins and eliminated violations (especially near the upthrust), with a slight increase in settling time. Including up to four models per zone did not increase the time per iteration, maintaining real-time viability; RNMPC$_{MZ}$ presented the lar |
| 2025-11-10 | [Interpolation Problem for Multidimensional Stationary Processes with Missing Observations](http://arxiv.org/abs/2511.07240v1) | Oleksandr Masyutka, Mikhail Moklyachuk et al. | The problem of the mean-square optimal linear estimation of linear functionals which depend on the unknown values of a multidimensional continuous time stationary stochastic process is considered. Estimates are based on observations of the process with an additive stationary stochastic noise process at points which do not belong to some finite intervals of a real line. The problem is investigated in the case of spectral certainty, where the spectral densities of the processes are exactly known. Formulas for calculating the mean-square errors and spectral characteristics of the optimal linear estimates of functionals are proposed under the condition of spectral certainty. The minimax (robust) method of estimation is applied in the case spectral uncertainty, where spectral densities of the processes are not known exactly while some sets of admissible spectral densities of the processes are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible spectral densities |
| 2025-11-10 | [Extrapolation Problem for Multidimensional Stationary Sequences with Missing Observations](http://arxiv.org/abs/2511.07228v1) | Oleksandr Masyutka, Mikhail Moklyachuk et al. | This paper focuses on the problem of the mean square optimal estimation of linear functionals which depend on the unknown values of a multidimensional stationary stochastic sequence.   Estimates are based on observations of the sequence with an additive stationary noise sequence.   The aim of the paper is to develop methods of finding the optimal estimates of the functionals in the case of missing observations.   The problem is investigated in the case of spectral certainty where the spectral densities of the sequences are exactly known.   Formulas for calculating the mean-square errors and the spectral characteristics of the optimal linear estimates of functionals are derived under the condition of spectral certainty.   The minimax (robust) method of estimation is applied in the case of spectral uncertainty, where spectral densities of the sequences are not known exactly while sets of admissible spectral densities are given. Formulas that determine the least favorable spectral densities and the minimax spectral characteristics of the optimal estimates of functionals are proposed for some special sets of admissible densities. |
| 2025-11-10 | [Prospects for geoneutrino detection with JUNO](http://arxiv.org/abs/2511.07227v1) | Thomas Adam, Shakeel Ahmad et al. | Geoneutrinos, which are antineutrinos emitted during the decay of long-lived radioactive elements inside Earth, serve as a unique tool for studying the composition and heat budget of our planet. The Jiangmen Underground Neutrino Observatory (JUNO) experiment in China, which has recently completed construction, is expected to collect a sample comparable in size to the entire existing world geoneutrino dataset in less than a year. This paper presents an updated estimation of sensitivity to geoneutrinos of JUNO using the best knowledge available to date about the experimental site, the surrounding nuclear reactors, the detector response uncertainties, and the constraints expected from the TAO satellite detector. To facilitate comparison with present and future geological models, our results cover a wide range of predicted signal strengths. Despite the significant background from reactor antineutrinos, the experiment will measure the total geoneutrino flux with a precision comparable to that of existing experiments within its first few years, ultimately achieving a world-leading precision of about 8% over ten years. The large statistics of JUNO will also allow separation of the Uranium-238 and Thorium-232 contributions with unprecedented precision, providing crucial constraints on models of formation and composition of Earth. Observation of the mantle signal above the lithospheric flux will be possible but challenging. For models with the highest predicted mantle concentrations of heat-producing elements, a 3-sigma detection over six years requires knowledge of the lithospheric flux to within 15%. Together with complementary measurements from other locations, the geoneutrino results of JUNO will offer cutting-edge, high-precision insights into the interior of Earth, of fundamental importance to both the geoscience and neutrino physics communities. |
| 2025-11-10 | [Resilient by Design - Active Inference for Distributed Continuum Intelligence](http://arxiv.org/abs/2511.07202v1) | Praveen Kumar Donta, Alfreds Lapkovskis et al. | Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free-energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework. |
| 2025-11-07 | [The $ZÎ±^2$ correction to superallowed beta decays in effective field theory and implications for $|V_{ud}|$](http://arxiv.org/abs/2511.05446v1) | Zehua Cao, Richard J. Hill et al. | Superallowed ($0^+\rightarrow0^+$) beta decays currently provide the most precise extraction of quark mixing in the Standard Model. Their interpretation as a measurement of $|V_{ud}|$ relies on a reliable first-principles computation of QED radiative corrections expressed as a series in $Z\alpha$ and $\alpha$. In this work, we provide the first model-independent result for two-loop, $O(Z\alpha^2)$, long-distance radiative corrections where the nuclei are treated as heavy point-like particles.   We use renormalization group analysis to obtain new results at $O(Z\alpha^3)$ for the coefficient of double-logarithms in the ratio of the maximal beta energy to the inverse nuclear size, $\Em/R^{-1}$. We use the Kinoshita-Lee-Nauenberg theorem to obtain new results at $O(Z^2\alpha^3)$ for the coefficient of logarithms in the ratio of maximal beta energy to the electron mass, $\log(2\Em/\me)$. We identify a structure-dependent, and therefore short-distance, contribution to the traditional $Z\alpha^2$ correction that should be revisited.. We provide the first comprehensive update to the long-distance corrections in almost forty years and comment on the impact of our findings for extractions of $|V_{ud}|$. We find that shifts in the long-distance corrections are $2.5\times$ larger than past estimates of their uncertainty, $1.5\times$ larger than the statistical uncertainty from the combined fit of superallowed decays, and about $1/2$ the size of estimated systematic error, which stems dominantly from nuclear structure effects. |
| 2025-11-07 | [Adversarially Robust Multitask Adaptive Control](http://arxiv.org/abs/2511.05444v1) | Kasra Fallah, Leonardo F. Toso et al. | We study adversarially robust multitask adaptive linear quadratic control; a setting where multiple systems collaboratively learn control policies under model uncertainty and adversarial corruption. We propose a clustered multitask approach that integrates clustering and system identification with resilient aggregation to mitigate corrupted model updates. Our analysis characterizes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior affect the expected regret of certainty-equivalent (CE) control across LQR tasks. We establish non-asymptotic bounds demonstrating that the regret decreases inversely with the number of honest systems per cluster and that this reduction is preserved under a bounded fraction of adversarial systems within each cluster. |
| 2025-11-07 | [Neutrino-Nucleus Scattering Cross Sections at Medium Energies](http://arxiv.org/abs/2511.05413v1) | Vishvas Pandey | The weak interactions of neutrinos with other Standard Model particles are well described within the Standard Model of particle physics. However, modern accelerator-based neutrino experiments employ nuclei as targets, where neutrinos interact with bound nucleons, turning a seemingly simple electroweak process into a complex many-body problem in nuclear physics. At the time of writing this Encyclopedia of Particle Physics chapter, neutrino-nucleus interactions remain one of the leading sources of systematic uncertainty in accelerator-based neutrino oscillation measurements.   This chapter provides a pedagogical overview of neutrino interactions with nuclei in the medium-energy regime, spanning a few hundred MeV to several GeV. It introduces the fundamental electroweak formalism, outlines the dominant interaction mechanisms - including quasielastic scattering, resonance production, and deep inelastic scattering - and discusses how nuclear effects such as Fermi motion, nucleon-nucleon correlations, meson-exchange currents, and final-state interactions modify observable cross sections. The chapter also presents a brief survey of the foundational and most widely used theoretical models for neutrino-nucleus cross sections, together with an overview of current and upcoming accelerator-based neutrino oscillation experiments that are shaping the field.   Rather than targeting experts, this chapter serves as a primer for advanced undergraduates, graduate students, and early-career researchers entering the field. It provides a concise foundation for understanding neutrino-nucleus scattering, its relevance to oscillation experiments, and its broader connections to both particle and nuclear physics. |
| 2025-11-07 | [Bayesian learning for accurate and robust biomolecular force fields](http://arxiv.org/abs/2511.05398v1) | Vojtech Kostal, Brennon L. Shanks et al. | Molecular dynamics is a valuable tool to probe biological processes at the atomistic level - a resolution often elusive to experiments. However, the credibility of molecular models is limited by the accuracy of the underlying force field, which is often parametrized relying on ad hoc assumptions. To address this gap, we present a Bayesian framework for learning physically grounded parameters directly from ab initio molecular dynamics data. By representing both model parameters and data probabilistically, the framework yields interpretable, statistically rigorous models in which uncertainty and transferability emerge naturally from the learning process. This approach provides a transparent, data-driven foundation for developing predictive molecular models and enhances confidence in computational descriptions of biophysical systems. We demonstrate the method using 18 biologically relevant molecular fragments that capture key motifs in proteins, nucleic acids, and lipids, and, as a proof of concept, apply it to calcium binding to troponin - a central event in cardiac regulation. |
| 2025-11-07 | [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](http://arxiv.org/abs/2511.05397v1) | Samarth Chopra, Alex McMoil et al. | While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: https://everydayvla.github.io/ |
| 2025-11-07 | [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](http://arxiv.org/abs/2511.05396v1) | Yiting He, Zhishuai Liu et al. | Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments. |
| 2025-11-07 | [Quantum-Uncertainty-Governed Spin Dynamics in s-d Coupled Systems](http://arxiv.org/abs/2511.05388v1) | Jie Zheng, Jiyong Kang et al. | We investigate quantum fluctuation effects arising from the Heisenberg uncertainty principle governing angular momentum operators in the full dynamical evolution of disentanglement-entanglement-disentanglement between itinerant electrons and localized magnetic moments under the s-d exchange interaction. Beyond the conventional deterministic spin-transfer torque, we identify an intrinsic channel for the transfer of spin quantum fluctuations. By extending the Landau-Lifshitz-Gilbert equation to include both quantum and thermal stochastic fields, we reveal a temperature regime where quantum fluctuations dominate spin dynamics. Furthermore, voltage-controlled magnetic anisotropy can exponentially amplify these quantum fluctuation signals, enabling their binary detection via tunneling magnetoresistance in magnetic tunnel junctions. These results establish a microscopic framework for quantum fluctuation-driven spin dynamics and provide a fundamental route toward spin-based quantum true random number generation. |
| 2025-11-07 | [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](http://arxiv.org/abs/2511.05330v1) | Jan-Hendrik Ewering, Robin E. Herrmann et al. | Embedding non-restrictive prior knowledge, such as energy conservation laws, in learning-based approaches is a key motive to construct physically consistent models from limited data, relevant for, e.g., model-based control. Recent work incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to obtain uncertainty-quantifying models that adhere to the underlying physical principles. However, these works rely on velocity or momentum data, which is rarely available in practice. In this paper, we consider dynamics learning with non-conservative Hamiltonian GPs, and address the more realistic problem setting of learning from input-output data. We provide a fully Bayesian scheme for estimating probability densities of unknown hidden states, of GP hyperparameters, as well as of structural hyperparameters, such as damping coefficients. Considering the computational complexity of GPs, we take advantage of a reduced-rank GP approximation and leverage its properties for computationally efficient prediction and training. The proposed method is evaluated in a nonlinear simulation case study and compared to a state-of-the-art approach that relies on momentum measurements. |
| 2025-11-07 | [Economic uncertainty and exchange rates linkage revisited: modelling tail dependence with high frequency data](http://arxiv.org/abs/2511.05315v1) | Nourhaine Nefzi, Abir Abid | The aim of this paper is to dig deeper into understanding the exchange rates and uncertainty dependence. Using the novel Baker et al. (2020)'s daily Twitter Uncertainty Index and BRICS exchange rates, we investigate their extreme tail dependence within an original time-varying copula framework. Our analysis makes several noteworthy results. Evidence for Indian, Russian and South African currencies indicates an elliptical copulas' dominance implying neither asymmetric features nor extreme movements in their dependence structure with the global economic uncertainty. Importantly, Brazilian and Chinese currencies tail dependence is upward trending suggesting a safe-haven role in times of high global economic uncertainty including the recent COVID-19 pandemic. In such circumstances, these markets offer opportunities to significant gains through portfolio diversification. |
| 2025-11-07 | [Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage](http://arxiv.org/abs/2511.05266v1) | Gabriel SerrÃ£o Seabra, Nikolaj T. MÃ¼cke et al. | Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment. |
| 2025-11-06 | [On the Exoplanet Yield of Gaia Astrometry](http://arxiv.org/abs/2511.04673v1) | Caleb Lammers, Joshua N. Winn | We re-examine the expected yield of Gaia astrometric planet detections using updated models for giant-planet occurrence, the local stellar population, and Gaia's demonstrated astrometric precision. Our analysis combines a semi-analytic model that clarifies key scaling relations with more realistic Monte Carlo simulations. We predict $7{,}500 \pm 2{,}100$ planet discoveries in the 5-year dataset (DR4) and $120{,}000 \pm 22{,}000$ over the full 10-year mission (DR5), with the dominant error arising from uncertainties in giant-planet occurrence. We evaluate the sensitivity of these forecasts to the detection threshold and the desired precision for measurements of planet masses and orbital parameters. Roughly $1{,}900 \pm 540$ planets in DR4 and $38{,}000 \pm 7{,}300$ planets in DR5 should have masses and orbital periods determined to better than $20$%. Most detections will be super-Jupiters ($3$ - $13 M_{\rm J}$) on $2$ - $5$AU orbits around GKM-type stars ($0.4$ - $1.3 M_\odot$) within $500$ pc. Unresolved binary stars will lead to spurious planet detections, but we estimate that genuine planets will outnumber them by a factor of $5$ or more. An exception is planets around M-dwarfs with $a < 1$AU, for which the false-positive rate is expected to be about $50$%. To support community preparation for upcoming data releases, we provide mock catalogs of Gaia exoplanets and planet-impostor binaries. |
| 2025-11-06 | [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](http://arxiv.org/abs/2511.04659v1) | Huaguan Chen, Wei Han et al. | Extreme precipitation nowcasting demands high spatiotemporal fidelity and extended lead times, yet existing approaches remain limited. Numerical Weather Prediction (NWP) and its deep-learning emulations are too slow and coarse for rapidly evolving convection, while extrapolation and purely data-driven models suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce a gray-box, fully three-dimensional nowcasting framework that directly processes volumetric radar reflectivity and couples physically constrained neural operators with datadriven learning. The model learns vertically varying 3D advection fields under a conservative advection operator, parameterizes spatially varying diffusion, and introduces a Brownian-motion--inspired stochastic term to represent unresolved motions. A residual branch captures small-scale convective initiation and microphysical variability, while a diffusion-based stochastic module estimates uncertainty. The framework achieves more accurate forecasts up to three-hour lead time across precipitation regimes and ranked first in 57\% of cases in a blind evaluation by 160 meteorologists. By restoring full 3D dynamics with physical consistency, it offers a scalable and robust pathway for skillful and reliable nowcasting of extreme precipitation. |
| 2025-11-06 | [Where to Experiment? Site Selection Under Distribution Shift via Optimal Transport and Wasserstein DRO](http://arxiv.org/abs/2511.04658v1) | Adam Bouyamourn | How should researchers select experimental sites when the deployment population differs from observed data? I formulate the problem of experimental site selection as an optimal transport problem, developing methods to minimize downstream estimation error by choosing sites that minimize the Wasserstein distance between population and sample covariate distributions. I develop new theoretical upper bounds on PATE and CATE estimation errors, and show that these different objectives lead to different site selection strategies. I extend this approach by using Wasserstein Distributionally Robust Optimization to develop a site selection procedure robust to adversarial perturbations of covariate information: a specific model of distribution shift. I also propose a novel data-driven procedure for selecting the uncertainty radius the Wasserstein DRO problem, which allows the user to benchmark robustness levels against observed variation in their data. Simulation evidence, and a reanalysis of a randomized microcredit experiment in Morocco (Cr\'epon et al.), show that these methods outperform random and stratified sampling of sites when covariates have prognostic R-squared > .5, and alternative optimization methods i) for moderate-to-large size problem instances ii) when covariates are moderately informative about treatment effects, and iii) under induced distribution shift. |
| 2025-11-06 | [Photodetection of Squeezed Light: a Whittaker-Shannon Analysis](http://arxiv.org/abs/2511.04657v1) | Jasper Kranias, Christian Drago et al. | The Whittaker-Shannon decomposition provides a temporally localized description of squeezed light, making it applicable in the CW limit and leading to a definition of squeezing strength based on the number of photon pairs at a time. We show examples of its usefulness by calculating quadrature variance in a homodyne detection scheme, coincidence detection probabilities in the continuous-wave limit, and analyzing the Hong-Ou-Mandel effect for strongly squeezed light. Quadrature uncertainty falls farther below the shot noise limit when squeezing is strong, but effects due to correlations between photon pairs are most significant with weak squeezing. Our analysis extends previous results to more general scenarios, and we leverage the Whittaker-Shannon formalism to interpret them based on the temporal properties of photon pairs. |
| 2025-11-06 | [Nonparametric Safety Stock Dimensioning: A Data-Driven Approach for Supply Chains of Hardware OEMs](http://arxiv.org/abs/2511.04616v1) | Elvis Agbenyega, Cody Quick | Resilient supply chains are critical, especially for Original Equipment Manufacturers (OEMs) that power today's digital economy. Safety Stock dimensioning-the computation of the appropriate safety stock quantity-is one of several mechanisms to ensure supply chain resiliency, as it protects the supply chain against demand and supply uncertainties. Unfortunately, the major approaches to dimensioning safety stock heavily assume that demand is normally distributed and ignore future demand variability, limiting their applicability in manufacturing contexts where demand is non-normal, intermittent, and highly skewed. In this paper, we propose a data-driven approach that relaxes the assumption of normality, enabling the demand distribution of each inventory item to be analytically determined using Kernel Density Estimation. Also, we extended the analysis from historical demand variability to forecasted demand variability. We evaluated the proposed approach against a normal distribution model in a near-world inventory replenishment simulation. Afterwards, we used a linear optimization model to determine the optimal safety stock configuration. The results from the simulation and linear optimization models showed that the data-driven approach outperformed traditional approaches. In particular, the data-driven approach achieved the desired service levels at lower safety stock levels than the conventional approaches. |
| 2025-11-06 | [Analyzing the topological structure of composite dynamical systems](http://arxiv.org/abs/2511.04603v1) | Michael Robinson, Michael L. Szulczewski et al. | This chapter explores dynamical structural equation models (DSEMs) and their nonlinear generalizations into sheaves of dynamical systems. It demonstrates these two disciplines on part of the food web in the Bering Sea. The translation from DSEMs to sheaves passes through a formal construction borrowed from electronics called a netlist that specifies how data route through a system. A sheaf can be considered a formal hypothesis about how variables interact, that then specifies how observations can be tested for consistency, how missing data can be inferred, and how uncertainty about the observations can be quantified. Sheaf modeling provides a coherent mathematical framework for studying the interaction of various dynamical subsystems that together determine a larger system. |
| 2025-11-06 | [Machine Learning for Electron-Scale Turbulence Modeling in W7-X](http://arxiv.org/abs/2511.04567v1) | Ionut-Gabriel Farcas, Don Lawrence Carl Agapito Fernando et al. | Constructing reduced models for turbulent transport is essential for accelerating profile predictions and enabling many-query tasks such as uncertainty quantification, parameter scans, and design optimization. This paper presents machine-learning-driven reduced models for Electron Temperature Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model predicts the ETG heat flux as a function of three plasma parameters: the normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of normalized electron temperature and density radial gradients ($\eta_e$), and the electron-to-ion temperature ratio ($\tau$). We first construct models across seven radial locations using regression and an active machine-learning-based procedure. This process initializes models using low-cardinality sparse-grid training data and then iteratively refines their training sets by selecting the most informative points from a pre-existing simulation database. We evaluate the prediction capabilities of our models using out-of-sample datasets with over $393$ points per location, and $95\%$ prediction intervals are estimated via bootstrapping to assess prediction uncertainty. We then investigate the construction of generalized reduced models, including a generic, position-independent model, and assess their heat flux prediction capabilities at three additional locations. Our models demonstrate robust performance and predictive accuracy comparable to the original reference simulations, even when applied beyond the training domain. |
| 2025-11-06 | [Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI](http://arxiv.org/abs/2511.04564v1) | Yoh-ichi Mototake, Makoto Sasaki | Physics-informed machine learning (PIML) integrates partial differential equations (PDEs) into machine learning models to solve inverse problems, such as estimating coefficient functions (e.g., the Hamiltonian function) that characterize physical systems. This framework enables data-driven understanding and prediction of complex physical phenomena. While coefficient functions in PIML are typically estimated on the basis of predictive performance, physics as a discipline does not rely solely on prediction accuracy to evaluate models. For example, Kepler's heliocentric model was favored owing to small discrepancies in planetary motion, despite its similar predictive accuracy to the geocentric model. This highlights the inherent uncertainties in data-driven model inference and the scientific importance of selecting physically meaningful solutions. In this paper, we propose a framework to quantify and analyze such uncertainties in the estimation of coefficient functions in PIML. We apply our framework to reduced model of magnetohydrodynamics and our framework shows that there are uncertainties, and unique identification is possible with geometric constraints. Finally, we confirm that we can estimate the reduced model uniquely by incorporating these constraints. |
| 2025-11-06 | [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](http://arxiv.org/abs/2511.04556v1) | Zihang Ding, Kun Zhang | Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource. |
| 2025-11-06 | [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](http://arxiv.org/abs/2511.04534v1) | Jonas E. Katona, Emily K. de Jong et al. | Reduced-order models (ROMs) can efficiently simulate high-dimensional physical systems, but lack robust uncertainty quantification methods. Existing approaches are frequently architecture- or training-specific, which limits flexibility and generalization. We introduce a post hoc, model-agnostic framework for predictive uncertainty quantification in latent space ROMs that requires no modification to the underlying architecture or training procedure. Using conformal prediction, our approach estimates statistical prediction intervals for multiple components of the ROM pipeline: latent dynamics, reconstruction, and end-to-end predictions. We demonstrate the method on a latent space dynamical model for cloud microphysics, where it accurately predicts the evolution of droplet-size distributions and quantifies uncertainty across the ROM pipeline. |
| 2025-11-05 | [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](http://arxiv.org/abs/2511.03724v1) | Richard Dewey, Janos Botyanszki et al. | AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players. |
| 2025-11-05 | [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](http://arxiv.org/abs/2511.03695v1) | Lipeng Zu, Hansong Zhou et al. | Offline reinforcement learning (RL) enables training from fixed data without online interaction, but policies learned offline often struggle when deployed in dynamic environments due to distributional shift and unreliable value estimates on unseen state-action pairs. We introduce Behavior-Adaptive Q-Learning (BAQ), a framework designed to enable a smooth and reliable transition from offline to online RL. The key idea is to leverage an implicit behavioral model derived from offline data to provide a behavior-consistency signal during online fine-tuning. BAQ incorporates a dual-objective loss that (i) aligns the online policy toward the offline behavior when uncertainty is high, and (ii) gradually relaxes this constraint as more confident online experience is accumulated. This adaptive mechanism reduces error propagation from out-of-distribution estimates, stabilizes early online updates, and accelerates adaptation to new scenarios. Across standard benchmarks, BAQ consistently outperforms prior offline-to-online RL approaches, achieving faster recovery, improved robustness, and higher overall performance. Our results demonstrate that implicit behavior adaptation is a principled and practical solution for reliable real-world policy deployment. |
| 2025-11-05 | [Final state sensitivity and fractal basin boundaries from coupled Chialvo neurons](http://arxiv.org/abs/2511.03671v1) | Bennett Lamb, Brandon B. Le | We investigate and quantify the basin geometry and extreme final state uncertainty of two identical electrically asymmetrically coupled Chialvo neurons. The system's diverse behaviors are presented, along with the mathematical reasoning behind its chaotic and nonchaotic dynamics as determined by the structure of the coupled equations. The system is found to be multistable with two qualitatively different attractors. Although each neuron is individually nonchaotic, the chaotic basin takes up the vast majority of the coupled system's state space, but the nonchaotic basin stretches to infinity due to chance synchronization. The boundary between the basins is found to be fractal, leading to extreme final state sensitivity. This uncertainty and its potential effect on the synchronization of biological neurons may have significant implications for understanding human behavior and neurological disease. |
| 2025-11-05 | [ALMA and JWST Imaging of $z\ >\ 6$ Quasars: No Spatial Position Offset Observed Between Quasars and Their Host Galaxies](http://arxiv.org/abs/2511.03659v1) | Aurora Wilde, Marcel Neeleman et al. | We present a study determining the spatial offset between the position of the supermassive black hole (as traced through their broad line regions) and the host galaxy in six $z > 6$ quasars. We determined the host galaxy's position from $\lesssim$$0.1^{\prime\prime}$ ($\lesssim$ 600 pc) resolution Atacama Large Millimeter/sub-millimeter Array (ALMA) [CII] 158 $\mu m$ and corresponding dust continuum imaging. We determined the quasar's position from $\lesssim$ 400 pc resolution James Webb Space Telescope Near-Infrared Camera (JWST NIRCam) imaging. We estimated the observational uncertainties on the quasar's position using astrometric data from the Global Astrometric Interferometer for Astrophysics (GAIA) of field stars within the NIRCam images. We find that all six quasars are found within the central $\sim 400$ pc of their host galaxy dust continuum and [CII] emission. Apparent offsets seen in rest-frame optical JWST observations are not detected in our ALMA data, suggesting they likely result from dust obscuration rather than a true physical separation between the SMBH and its host galaxy. Kinematic modeling of these data further reveals that none of the galaxies show evidence for recent merger activity, and most of the galaxies can be accurately modeled using a simple disk model. The lack of an offset supports theoretical models that predict that positional offset within these galaxies are either short-lived or intrinsically rare. |
| 2025-11-05 | [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](http://arxiv.org/abs/2511.03652v1) | Azizollah Taheri, Derya Aksaray | This paper addresses a motion planning problem to achieve spatio-temporal-logical tasks, expressed by syntactically co-safe linear temporal logic specifications (scLTL\next), in uncertain environments. Here, the uncertainty is modeled as some probabilistic knowledge on the semantic labels of the environment. For example, the task is "first go to region 1, then go to region 2"; however, the exact locations of regions 1 and 2 are not known a priori, instead a probabilistic belief is available. We propose a novel automata-theoretic approach, where a special product automaton is constructed to capture the uncertainty related to semantic labels, and a reward function is designed for each edge of this product automaton. The proposed algorithm utilizes value iteration for online replanning. We show some theoretical results and present some simulations/experiments to demonstrate the efficacy of the proposed approach. |
| 2025-11-05 | [Geometrically robust least squares through manifold optimization](http://arxiv.org/abs/2511.03644v1) | Jeremy Coulson, Alberto Padoan et al. | This paper presents a methodology for solving a geometrically robust least squares problem, which arises in various applications where the model is subject to geometric constraints. The problem is formulated as a minimax optimization problem on a product manifold, where one variable is constrained to a ball describing uncertainty. To handle the constraint, an exact penalty method is applied. A first-order gradient descent ascent algorithm is proposed to solve the problem, and its convergence properties are illustrated by an example. The proposed method offers a robust approach to solving a wide range of problems arising in signal processing and data-driven control. |
| 2025-11-05 | [Quantifying Weighted Morphological Content of Large-Scale Structures via Simulation-Based Inference](http://arxiv.org/abs/2511.03636v1) | M. H. Jalali Kanafi, S. M. S. Movahed | In this work, we perform a simulation-based forecasting analysis to compare the constraining power of two higher-order summary statistics of the large-scale structure (LSS), the Minkowski Functionals (MFs) and the Conditional Moments of Derivative (CMD), with a particular focus on their sensitivity to nonlinear and anisotropic features in redshift-space. Our analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations at redshift $z=0.5$, employing a likelihood-free inference framework implemented via neural posterior estimation. At the fiducial cosmology of the Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs components, improving the constraint precision by ${\sim}(44\%,\,52\%)$, ${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$, respectively. A joint configuration combining the MFs and CMD further enhances the precision by approximately ${\sim}27\%$ compared to the standard MFs alone, highlighting the complementary anisotropy-sensitive information captured by the CMD in contrast to the scalar morphological content encapsulated by the MFs. We further extend the forecasting analysis to a continuous range of cosmological parameter values and multiple smoothing scales. Our results show that, although the absolute forecast uncertainty for each component of summary statistics depends on the underlying parameter values and the adopted smoothing scale, the relative constraining power among the summary statistics remains nearly constant throughout. |
| 2025-11-05 | [LiveTradeBench: Seeking Real-World Alpha with Large Language Models](http://arxiv.org/abs/2511.03628v1) | Haofei Yu, Fenghai Li et al. | Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty. |
| 2025-11-05 | [Tensor-Efficient High-Dimensional Q-learning](http://arxiv.org/abs/2511.03595v1) | Junyi Wu, Dan Li | High-dimensional reinforcement learning faces challenges with complex calculations and low sample efficiency in large state-action spaces. Q-learning algorithms struggle particularly with the curse of dimensionality, where the number of state-action pairs grows exponentially with problem size. While neural network-based approaches like Deep Q-Networks have shown success, recent tensor-based methods using low-rank decomposition offer more parameter-efficient alternatives. Building upon existing tensor-based methods, we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor decomposition via improved block coordinate descent on discretized state-action spaces, incorporating novel exploration and regularization mechanisms. The key innovation is an exploration strategy that combines approximation error with visit count-based upper confidence bound to prioritize actions with high uncertainty, avoiding wasteful random exploration. Additionally, we incorporate a frequency-based penalty term in the objective function to encourage exploration of less-visited state-action pairs and reduce overfitting to frequently visited regions. Empirical results on classic control tasks demonstrate that TEQL outperforms conventional matrix-based methods and deep RL approaches in both sample efficiency and total rewards, making it suitable for resource-constrained applications, such as space and healthcare where sampling costs are high. |
| 2025-11-05 | [Approaches to the Inverse Fourier Transformation with Limited and Discrete Data](http://arxiv.org/abs/2511.03593v1) | Yu-Fei Ling, Min-Huan Chu et al. | We investigate several approaches to address the inverse problem that arises in the limited inverse Fourier transform (L-IDFT) of quasi-distributions. The methods explored include Tikhonov regularization, the Backus-Gilbert method, the Bayesian approach with Gaussian Random Walk (GRW) prior, and the feedforward artificial neural networks (ANNs). We evaluate the performance of these methods using both mock data generated from toy models and real lattice data from quasi distribution, and further compare them with the physics-driven $\lambda$-extrapolation approach. Our results demonstrate that the L-IDFT constitutes a moderately tractable inverse problem Except for the Backus-Gilbert method, all the other approaches are capable of correctly reconstructing the quasi-distributions in momentum space. In particular, the Bayesian approach with GRW and the feedforward ANNs yield more stable and accurate reconstructions. Based on these investigations, we conclude that, for a given L-IDFT problem, selecting an appropriate reconstruction method according to the input data and carefully assessing the potential systematic uncertainties are essential for obtaining reliable results. |
| 2025-11-04 | [Intercomparison of a High-Resolution Regional Climate Model Ensemble for Catchment-Scale Water Cycle Processes under Human Influence](http://arxiv.org/abs/2511.02799v1) | J. L. Roque, F. Da Silva Lopes et al. | Understanding regional hydroclimatic variability and its drivers is essential for anticipating the impacts of climate change on water resources and sustainability. Yet, considerable uncertainty remains in the simulation of the coupled land atmosphere water and energy cycles, largely due to structural model limitations, simplified process representations, and insufficient spatial resolution. Within the framework of the Collaborative Research Center 1502 DETECT, this study presents a coordinated intercomparison of regional climate model simulations designed for water cycle process analysis over Europe. We analyze the performance of simulations using the ICON and TSMP1 model systems and covering the period from 1990 to 2020, comparing against reference datasets (E-OBS, GPCC, and GLEAM). We focus on 2 m air temperature, precipitation and evapotranspiration over four representative basins, the Ebro, Po, Rhine, and Tisa, within the EURO CORDEX domain.   Our analysis reveals systematic cold biases across all basins and seasons, with ICON generally outperforming TSMP1. Precipitation biases exhibit substantial spread, particularly in summer, reflecting the persistent challenge of accurately simulating precipitation. ICON tends to underestimate evapotranspiration, while TSMP1 performs better some seasons. Sensitivity experiments further indicate that the inclusion of irrigation improves simulation performance in the Po basin, which is intensively irrigated, and that higher-resolution sea surface temperature forcing data improves the overall precipitation representation. This baseline evaluation provides a first assessment of the DETECT multimodel ensemble and highlights key structural differences influencing model skill across hydroclimatic regimes. |
| 2025-11-04 | [Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](http://arxiv.org/abs/2511.02748v1) | Farhad Rezazadeh, Hatim Chergui et al. | We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative "what-if" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening. |
| 2025-11-04 | [Bayesian full waveform inversion with learned prior using deep convolutional autoencoder](http://arxiv.org/abs/2511.02737v1) | Shuhua Hu, Mrinal K Sen et al. | Full waveform inversion (FWI) can be expressed in a Bayesian framework, where the associated uncertainties are captured by the posterior probability distribution (PPD). In practice, solving Bayesian FWI with sampling-based methods such as Markov chain Monte Carlo (MCMC) is computationally demanding because of the extremely high dimensionality of the model space. To alleviate this difficulty, we develop a deep convolutional autoencoder (CAE) that serves as a learned prior for the inversion. The CAE compresses detailed subsurface velocity models into a low-dimensional latent representation, achieving more effective and geologically consistent model reduction than conventional dimension reduction approaches. The inversion procedure employs an adaptive gradient-based MCMC algorithm enhanced by automatic differentiation-based FWI to compute gradients efficiently in the latent space. In addition, we implement a transfer learning strategy through online fine-tuning during inversion, enabling the framework to adapt to velocity structures not represented in the original training set. Numerical experiments with synthetic data show that the method can reconstruct velocity models and assess uncertainty with improved efficiency compared to traditional MCMC methods. |
| 2025-11-04 | [Measuring the expansion history of the Universe with DESI Cosmic Chronometers](http://arxiv.org/abs/2511.02730v1) | S. I. Loubser | Studying large samples of massive, passively evolving galaxies (called cosmic chronometers, CC) provides us with the unique ability to measure the Universe's expansion history without assuming a cosmological model. The Dark Energy Spectroscopic Instrument (DESI) DR1 is currently the largest, publicly available, homogeneous set of galaxies with reliable spectroscopic redshifts, and covers a wide range in redshift. We extracted all massive galaxies (stellar mass $\log M_{\star}/M_{\odot} > 10.75$, and velocity dispersion $\sigma > 280$ km s$^{-1}$), with no emission in [OII] $\lambda$ 3727 $\r{A}$, with reliable redshifts as well as reliable D4000$_{\rm n}$ measurements from DR1. From this sample of 360 000 massive, passive galaxies, we used D4000$_{\rm n}$ and the method of cosmic chronometers to get three new direct, independent measurements of $H(z)=$ 88.48 $\pm\ 0.57(\rm stat) \pm 12.32(\rm syst)$, $H(z)=$ 119.45 $\pm\ 6.39(\rm stat) \pm 16.64(\rm syst)$, and $H(z)= 108.28 \pm 10.07(\rm stat) \pm 15.08(\rm syst)$ $\rm km\ s^{-1}\ Mpc^{-1}$ at $z=0.46$, $z=0.67$, and $z=0.83$, respectively. This sample, which covers $0.3 < z < 1.0$, is the largest CC sample to date, and we reach statistical uncertainties of 0.65$\%$, 5.35$\%$, and 9.30$\%$ on our three measurements. Our measurements show no significant tension with the $\textit{Planck}$ $\Lambda$CDM cosmology. In our analysis, we also illustrate that even amongst samples of massive, passive galaxies, the effect of downsizing can clearly be seen. |
| 2025-11-04 | [Mass ratio estimates for overcontact binaries using the derivatives of light curves. II. Systems with deep eclipses](http://arxiv.org/abs/2511.02629v1) | Shinjirou Kouzuma | This is the second paper that proposes a simple method for estimating mass ratios using the derivatives of light curves for overcontact binaries. In the first paper (Kouzuma 2023, ApJ, 958, 84) , we presented a method to estimate the mass ratios for systems exhibiting a double-peak feature in the second derivatives of their light curves around eclipses. This second paper focuses on overcontact systems that are not addressed in the first paper, that is, systems lacking a double peak in the second derivative. A sample of synthetic light curves for overcontact binaries consists of 89670, covering a parameter space typical of overcontact systems. On the basis of a recent study that proposed a new classification scheme using light-curve derivatives up to the fourth order, the sample light curves were classified. We found that time intervals between two local extrema in the derivatives are associated with the mass ratio in systems that exhibit a high degree of eclipse obscuration. Using regression analysis for the identified associations, we derived empirical formulae to estimate the mass ratio and its associated uncertainty. The application of our proposed method to real overcontact binary data demonstrated its effectiveness in providing reliable estimates for both values. |
| 2025-11-04 | [Model Parameter Reconstruction of Electroweak Phase Transition with TianQin and LISA: Insights from the Dimension-Six Model](http://arxiv.org/abs/2511.02612v1) | Aidi Yang, Chikako Idegawa et al. | We investigate the capability of TianQin and LISA to reconstruct the model parameters in the Lagrangian of new physics scenarios that can generate a strong first-order electroweak phase transition. Taking the dimension-six Higgs operator extension of the Standard Model as a representative scenario for a broad class of new physics models, we establish the mapping between the model parameter $\Lambda$ and the observable spectral features of the stochastic gravitational wave background. We begin by generating simulated data incorporating Time Delay Interferometry channel noise, astrophysical foregrounds, and signals from the dimensional-six model. The data are then compressed and optimized, followed by geometric parameter inference using both Fisher matrix analysis and Bayesian nested sampling with PolyChord, which efficiently handles high-dimensional, multimodal posterior distributions. Finally, machine learning techniques are employed to achieve precise reconstruction of the model parameter $\Lambda$. For benchmark points producing strong signals, parameter reconstruction with both TianQin and LISA yields relative uncertainties of approximately $20$--$30\%$ in the signal amplitude and sub-percent precision in the model parameter $\Lambda$. TianQin's sensitivity is limited to stronger signals within its optimal frequency band, whereas LISA can reconstruct parameters across a broader range of signal strengths. Our results demonstrate that reconstruction precision depends on signal strength, astrophysical foregrounds, and instrumental noise characteristics. |
| 2025-11-04 | [Trustworthy Quantum Machine Learning: A Roadmap for Reliability, Robustness, and Security in the NISQ Era](http://arxiv.org/abs/2511.02602v1) | Ferhat Ozgur Catak, Jungwon Seo et al. | Quantum machine learning (QML) is a promising paradigm for tackling computational problems that challenge classical AI. Yet, the inherent probabilistic behavior of quantum mechanics, device noise in NISQ hardware, and hybrid quantum-classical execution pipelines introduce new risks that prevent reliable deployment of QML in real-world, safety-critical settings. This research offers a broad roadmap for Trustworthy Quantum Machine Learning (TQML), integrating three foundational pillars of reliability: (i) uncertainty quantification for calibrated and risk-aware decision making, (ii) adversarial robustness against classical and quantum-native threat models, and (iii) privacy preservation in distributed and delegated quantum learning scenarios. We formalize quantum-specific trust metrics grounded in quantum information theory, including a variance-based decomposition of predictive uncertainty, trace-distance-bounded robustness, and differential privacy for hybrid learning channels. To demonstrate feasibility on current NISQ devices, we validate a unified trust assessment pipeline on parameterized quantum classifiers, uncovering correlations between uncertainty and prediction risk, an asymmetry in attack vulnerability between classical and quantum state perturbations, and privacy-utility trade-offs driven by shot noise and quantum channel noise. This roadmap seeks to define trustworthiness as a first-class design objective for quantum AI. |
| 2025-11-04 | [Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis](http://arxiv.org/abs/2511.02509v1) | R. Alberich, N. A. Cruz et al. | High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures. |
| 2025-11-04 | [Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification](http://arxiv.org/abs/2511.02469v1) | Kaito Takano, Masanori Hirano et al. | Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts. |
| 2025-11-04 | [Decentralized Voltage Control of AC Microgrids with Constant Power Loads using Control Barrier Functions](http://arxiv.org/abs/2511.02438v1) | Grigoris Michos, George C. Konstantopoulos | This paper proposes a novel nonlinear decentralized voltage controller for constrained regulation of meshed AC Microgrid networks with high penetration of constant power loads. Perceiving the load demand as an unknown disturbance, the network model is reformulated in a cascaded structure composed of a nominal, i.e. uncertainty-free, and an error subsystem. The latter captures the distance between the true and the nominal state trajectories, for which we prove boundedness via a suitable control barrier function. Under sufficient conditions, we prove asymptotic stability of the cascaded dynamics with respect to an equilibrium set and also provide an estimate of the region of attraction. In addition, it is rigorously shown that the proposed nonlinear control law also enforces constrained regulation around a rated voltage value, without the need of saturation devices. The operation of the closed-loop system is illustrated in a simulation scenario, demonstrating bounded operation and convergence to a neighbourhood of the desired reference vector. |
| 2025-10-31 | [WallGo investigates: Theoretical uncertainties in the bubble wall velocity](http://arxiv.org/abs/2510.27691v1) | Jorinde van de Vis, Philipp Schicho et al. | We examine theoretical uncertainties in state-of-the-art calculations of the bubble wall velocity during first-order cosmological phase transitions. By utilising the software WallGo for two extensions of the Standard Model, we find several $\mathcal{O}(1)$ uncertainties arising from the number of particles taken out of equilibrium, the logarithmically and power enhanced collision integrals, the treatment of thermal masses, the nucleation temperature, the $\tanh$ ansatz, and the perturbative order of the effective potential. However, we show that the linearisation of the Boltzmann equations is generally a good approximation with much smaller associated errors. We further clarify the limitations of the quasiparticle approximation in regions with negative mass squared. This study provides a detailed uncertainty budget and highlights where future efforts should be directed to improve the reliability of wall velocity and hence gravitational wave predictions. |
| 2025-10-31 | [Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in Creative Work](http://arxiv.org/abs/2510.27681v1) | Sean Kelley, David De Cremer et al. | As AI becomes more deeply embedded in knowledge work, building assistants that support human creativity and expertise becomes more important. Yet achieving synergy in human-AI collaboration is not easy. Providing AI with detailed information about a user's demographics, psychological attributes, divergent thinking, and domain expertise may improve performance by scaffolding more effective multi-turn interactions. We implemented a personalized LLM-based assistant, informed by users' psychometric profiles and an AI-guided interview about their work style, to help users complete a marketing task for a fictional startup. We randomized 331 participants to work with AI that was either generic (n = 116), partially personalized (n = 114), or fully personalized (n=101). Participants working with personalized AI produce marketing campaigns of significantly higher quality and creativity, beyond what AI alone could have produced. Compared to generic AI, personalized AI leads to higher self-reported levels of assistance and feedback, while also increasing participant trust and confidence. Causal mediation analysis shows that personalization improves performance indirectly by enhancing collective memory, attention, and reasoning in the human-AI interaction. These findings provide a theory-driven framework in which personalization functions as external scaffolding that builds common ground and shared partner models, reducing uncertainty and enhancing joint cognition. This informs the design of future AI assistants that maximize synergy and support human creative potential while limiting negative homogenization. |
| 2025-10-31 | [Deep learning denoising unlocks quantitative insights in operando materials microscopy](http://arxiv.org/abs/2510.27667v1) | Samuel Degnan-Morgenstern, Alexander E. Cohen et al. | Operando microscopy provides direct insight into the dynamic chemical and physical processes that govern functional materials, yet measurement noise limits the effective resolution and undermines quantitative analysis. Here, we present a general framework for integrating unsupervised deep learning-based denoising into quantitative microscopy workflows across modalities and length scales. Using simulated data, we demonstrate that deep denoising preserves physical fidelity, introduces minimal bias, and reduces uncertainty in model learning with partial differential equation (PDE)-constrained optimization. Applied to experiments, denoising reveals nanoscale chemical and structural heterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron phosphate (LFP), enables automated particle segmentation and phase classification in optical microscopy of graphite electrodes, and reduces noise-induced variability by nearly 80% in neutron radiography to resolve heterogeneous lithium transport. Collectively, these results establish deep denoising as a powerful, modality-agnostic enhancement that advances quantitative operando imaging and extends the reach of previously noise-limited techniques. |
| 2025-10-31 | [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](http://arxiv.org/abs/2510.27598v1) | Yunze Wu, Dayuan Fu et al. | AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark. |
| 2025-10-31 | [InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research](http://arxiv.org/abs/2510.27598v2) | Yunze Wu, Dayuan Fu et al. | AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark. |
| 2025-10-31 | [Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs](http://arxiv.org/abs/2510.27558v1) | Sushil Samuel Dinesh, Shinkyu Park | This paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models. |
| 2025-10-31 | [EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities](http://arxiv.org/abs/2510.27545v1) | Travis Davies, Yiqi Huang et al. | Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts. |
| 2025-10-31 | [Risk-constrained stochastic scheduling of multi-market energy storage systems](http://arxiv.org/abs/2510.27528v1) | Gabriel D. PatrÃ³n, Di Zhang et al. | Energy storage can promote the integration of renewables by operating with charge and discharge policies that balance an intermittent power supply. This study investigates the scheduling of energy storage assets under energy price uncertainty, with a focus on electricity markets. A two-stage stochastic risk-constrained approach is employed, whereby electricity price trajectories or specific power markets are observed, allowing for recourse in the schedule. Conditional value-at-risk is used to quantify tail risk in the optimization problems; this allows for the explicit specification of a probabilistic risk limit. The proposed approach is tested in an integrated hydrogen system (IHS) and a battery energy storage system (BESS). In the joint design and operation context for the IHS, the risk constraint results in larger installed unit capacities, increasing capital cost but enabling more energy inventory to buffer price uncertainty. As shown in both case studies, there is an operational trade-off between risk and expected reward; this is reflected in higher expected costs (or lower expected profits) with increasing levels of risk aversion. Despite the decrease in expected reward, both systems exhibit substantial benefits of increasing risk aversion. This work provides a general method to address uncertainties in energy storage scheduling, allowing operators to input their level of risk tolerance on asset decisions. |
| 2025-10-31 | [Euclid: Systematic uncertainties from the halo mass conversion on galaxy cluster number count data analyses](http://arxiv.org/abs/2510.27505v1) | T. Gayoux, P. -S. Corasaniti et al. | The large catalogues of galaxy clusters expected from the Euclid survey will enable cosmological analyses of cluster number counts that require accurate cosmological model predictions. One possibility is to use parametric fits calibrated against $N$-body simulations, that capture the cosmological parameter dependence of the halo mass function. Several studies have shown that this can be obtained through a calibration against haloes with spherical masses defined at the virial overdensity. In contrast, if different mass definitions are used for the HMF and the scaling relation, a mapping between them is required. Here, we investigate the impact of such a mapping on the cosmological parameter constraints inferred from galaxy cluster number counts. Using synthetic data from $N$-body simulations, we show that the standard approach, which relies on assuming a concentration-mass relation, can introduce significant systematic bias. In particular, depending on the mass definition and the relation assumed, this can lead to biased constraints at more than 2$\sigma$ level. In contrast, we find that in all the cases we have considered, the mass conversion based on the halo sparsity statistics result in a systematic bias smaller than the statistical error. |
| 2025-10-31 | [Evaluation of Reference Equations of State for Density Prediction in Regasified LNG Mixtures Using High-Precision Experimental Data](http://arxiv.org/abs/2510.27502v1) | Daniel Lozano-MartÃ­n, Dirk Tuma et al. | This study evaluates the performance of three reference equations of state (EoS), AGA8-DC92, GERG-2008, and SGERG-88, in predicting the density of regasified liquefied natural gas (RLNG) mixtures. A synthetic nine-component RLNG mixture was gravimetrically prepared. High-precision density measurements were obtained using a single-sinker magnetic suspension densimeter over a temperature range of (250 to 350) K and pressures up to 20 MPa. The experimental data were compared with EoS predictions to evaluate their accuracy. AGA8-DC92 and GERG-2008 showed excellent agreement with the experimental data, with deviations within their stated uncertainty. In contrast, SGERG-88 exhibited significantly larger deviations for this RLNG mixture, particularly at low temperatures of (250 to 260) K, where discrepancies reached up to 3 %. Even at 300 K, deviations larger than 0.4 % were observed at high pressures, within the model's uncertainty, but notably higher than those of the other two EoSs. The analysis was extended to three conventional 11-component natural gas mixtures (labeled G420 NG, G431 NG, and G432 NG), previously studied by our group using the same methodology. While SGERG-88 showed reduced accuracy for the RLNG mixture, it performed reasonably well for these three mixtures, despite two of them have a very similar composition to the RLNG. This discrepancy is attributed to the lower CO2 and N2 content typical in RLNG mixtures, demonstrating the sensitivity of EoS performance to minor differences in composition. These findings highlight the importance of selecting appropriate EoS models for accurate density prediction in RLNG applications. |
| 2025-10-31 | [Differential Set Selection via Confidence-Guided Entropy Minimization](http://arxiv.org/abs/2510.27479v1) | MarÃ­a del Carmen Romero, Mariana del Fresno et al. | This paper addresses the challenge of identifying a minimal subset of discrete, independent variables that best predicts a binary class. We propose an efficient iterative method that sequentially selects variables based on which one provides the most statistically significant reduction in conditional entropy, using confidence bounds to account for finite-sample uncertainty. Tests on simulated data demonstrate the method's ability to correctly identify influential variables while minimizing spurious selections, even with small sample sizes, offering a computationally tractable solution to this NP-complete problem. |
| 2025-10-30 | [Boosting the cosmic 21-cm signal with exotic Lyman-$Î±$ from dark matter](http://arxiv.org/abs/2510.26791v1) | Dominic Agius, Tracy Robyn Slatyer | The 21-cm signal from the epoch of cosmic dawn ($z \sim 10-30$) offers a powerful probe of new physics. One standard mechanism for constraining decaying dark matter from 21-cm observations relies on heating of the intergalactic medium by the decay products, an effect whose observability is entangled with the uncertain Lyman-$\alpha$ fluxes and X-ray heating from the first stars. In this Letter, we explore a novel mechanism, where the Lyman-$\alpha$ photons produced from dark matter decay initiate early Wouthuysen-Field coupling of the spin temperature to the gas temperature, thereby boosting the 21-cm signal. This mechanism provides constraints on dark matter that are less dependent on uncertainties associated with star formation than constraints on exotic heating. We study this effect for decaying dark matter with masses $m_{\chi}\sim20.4-27.2$ eV, where diphoton decay efficiently produces Lyman-series photons. We present forecasts for the Hydrogen Epoch of Reionization Array and the Square Kilometre Array, showing their potential to probe an unconstrained parameter space for light decaying DM, including axion-like particles. |
| 2025-10-30 | [Moments of parton distributions functions of the pion from lattice QCD using gradient flow](http://arxiv.org/abs/2510.26738v1) | Anthony Francis, Patrick Fritzsch et al. | We present a nonperturbative determination of the pion valence parton distribution function (PDF) moment ratios $\left\langle x^{n-1} \right\rangle / \left\langle x \right\rangle$ up to $n=6$, using the gradient flow in lattice QCD. As a testing ground, we employ SU($3$) isosymmetric gauge configurations generated by the OpenLat initiative with a pseudoscalar mass of $m_\pi \simeq 411~\text{MeV}$. Our analysis uses four lattice spacings and a nonperturbatively improved action, enabling full control over the continuum extrapolation, and the limit of vanishing flow time, $t\to0$. The flowed ratios exhibit O($a^2$) scaling across the ensembles, and the continuum-extrapolated results, matched to the $\overline {\text{MS}}$ scheme at $\mu = 2$ GeV using next-to-next-to-leading order matching coefficients, show only mild residual flow-time dependence. The resulting ratios, computed with a relatively small number of configurations, are consistent with phenomenological expectations for the pion's valence distribution, with statistical uncertainties that are competitive with modern global fits. These findings demonstrate that the gradient flow provides an efficient and systematically improvable method to access partonic quantities from first principles. Future extensions of this work will target lighter pion masses toward the physical point, and applications to nucleon structure such as the proton PDFs and the gluon and sea-quark distributions. |
| 2025-10-30 | [Time-Optimal Model Predictive Control for Linear Systems with Multiplicative Uncertainties](http://arxiv.org/abs/2510.26712v1) | Renato Quartullo, Andrea Garulli et al. | This paper presents a time-optimal Model Predictive Control (MPC) scheme for linear discrete-time systems subject to multiplicative uncertainties represented by interval matrices. To render the uncertainty propagation computationally tractable, the set-valued error system dynamics are approximated using a matrix-zonotope-based bounding operator. Recursive feasibility and finite-time convergence are ensured through an adaptive terminal constraint mechanism. A key advantage of the proposed approach is that all the necessary bounding sets can be computed offline, substantially reducing the online computational burden. The effectiveness of the method is illustrated via a numerical case study on an orbital rendezvous maneuver between two satellites. |
| 2025-10-30 | [The ODYSSEUS Survey. Spatial correlation of magnetospheric inclinations points to parsec-scale star-cloud connection](http://arxiv.org/abs/2510.26687v1) | Caeley V. Pittman, Catherine C. Espaillat et al. | The properties of stars and planets are shaped by the initial conditions of their natal clouds. However, the spatial scales over which the initial conditions can exert a significant influence are not well constrained. We report the first evidence for parsec-scale spatial correlations of stellar magnetospheric inclinations ($i_{\rm mag}$), observed in the Lupus low-mass star forming region. Applying consensus clustering with a hierarchical density-based clustering algorithm, we demonstrate that the detected spatial dependencies are stable against perturbations by measurement uncertainties. The $i_{\rm mag}$ correlation scales are on the order of ~3 pc, which aligns with the typical scales of the Lupus molecular cloud filaments. Our results reveal a connection between large-scale forces -- in the form of expanding shells from the Upper Scorpius and Upper-Centaurus-Lupus regions -- and sub-au scale system configurations. We find that Lupus has a non-uniform $i_{\rm mag}$ distribution and suggest that this results from the preferential elongation of protostellar cores along filamentary axes. Non-uniformity would have significant implications for exoplanet occurrence rate calculations, so future work should explore the longevity of these biases driven by the star-cloud connection. |
| 2025-10-30 | [Refined additive uncertainty principle](http://arxiv.org/abs/2510.26664v1) | Ivan Bortnovskyi, June Duvivier et al. | Signal recovery from incomplete or partial frequency information is a fundamental problem in harmonic analysis and applied mathematics, with wide-ranging applications in communications, imaging, and data science. Historically, the classical uncertainty principles, such as those by Donoho and Stark, have provided essential bounds relating the sparsity of a signal and its Fourier transform, ensuring unique recovery under certain support size constraints.   Recent advances have incorporated additive combinatorial notions, notably additive energy, to refine these uncertainty principles and capture deeper structural properties of signal supports. Building upon this line of work, we present a strengthened additive energy uncertainty principle for functions $f:\mathbb{Z}_N^d\to\mathbb{C}$, introducing explicit correction terms that measure how far the supports are from highly structured extremal sets like subgroup cosets.   We have two main results. Our first theorem introduces a correction term which strictly improves the additive energy uncertainty principle from Aldahleh et al., provided that the classical uncertainty principle is not satisfied with equality. Our second theorem uses the improvement to obtain a better recovery condition. These theorems deliver strictly improved bounds over prior results whenever the product of the support sizes differs from the ambient dimension, offering a more nuanced understanding of the interplay between additive structure and Fourier sparsity. Importantly, we leverage these improvements to establish sharper sufficient conditions for unique and exact recovery of signals from partially observed frequencies, explicitly quantifying the role of additive energy in recoverability. |
| 2025-10-30 | [Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets](http://arxiv.org/abs/2510.26602v1) | Tanmay Mishra, Dakota Hamilton et al. | The increasing integration of renewable energy sources and distributed energy resources (DER) into modern power systems introduces significant uncertainty, posing challenges for maintaining grid flexibility and reliability. Hybrid energy systems (HES), composed of controllable generators, flexible loads, and battery storage, offer a decentralized solution to enhance flexibility compared to single centralized resources. This paper presents a two-level framework to enable HES participation in frequency regulation markets. The upper level performs a chance-constrained optimization to choose capacity bids based on historical regulation signals. At the lower level, a real-time control strategy disaggregates the regulation power among the constituent resources. This real-time control strategy is then benchmarked against an offline optimal dispatch to evaluate flexibility performance. Additionally, the framework evaluates the profitability of overbidding strategies and identifies thresholds beyond which performance degradation may lead to market penalties or disqualification. The proposed framework also compare the impact of imbalance of power capacities on performance and battery state of charge (SoC) through asymmetric HES configurations. |
| 2025-10-30 | [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](http://arxiv.org/abs/2510.26601v1) | Anirban Ray, Vera Galinova et al. | Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions. |
| 2025-10-30 | [FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles](http://arxiv.org/abs/2510.26588v1) | Gang Li, Chunlei Zhai et al. | Visual navigation algorithms for quadrotors often exhibit a large variation in performance when transferred across different vehicle platforms and scene geometries, which increases the cost and risk of field deployment. To support systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity, configurable benchmarking framework that measures how platform kinodynamics and scenario structure jointly affect navigation robustness. FLYINGTRUST models vehicle capability with two compact, physically interpretable indicators: maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The benchmark pairs a diverse scenario library with a heterogeneous set of real and virtual platforms and prescribes a standardized evaluation protocol together with a composite scoring method that balances scenario importance, platform importance and performance stability. We use FLYINGTRUST to compare representative optimization-based and learning-based navigation approaches under identical conditions, performing repeated trials per platform-scenario combination and reporting uncertainty-aware metrics. The results reveal systematic patterns: navigation success depends predictably on platform capability and scene geometry, and different algorithms exhibit distinct preferences and failure modes across the evaluated conditions. These observations highlight the practical necessity of incorporating both platform capability and scenario structure into algorithm design, evaluation, and selection, and they motivate future work on methods that remain robust across diverse platforms and scenarios. |
| 2025-10-30 | [A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains](http://arxiv.org/abs/2510.26541v1) | Aidan Furlong, Robert Salko et al. | The use of ML in engineering has grown steadily to support a wide array of applications. Among these methods, deep neural networks have been widely adopted due to their performance and accessibility, but they require large, high-quality datasets. Experimental data are often sparse, noisy, or insufficient to build resilient data-driven models. Transfer learning, which leverages relevant data-abundant source domains to assist learning in data-scarce target domains, has shown efficacy. Parameter transfer, where pretrained weights are reused, is common but degrades under large domain shifts. Domain-adversarial neural networks (DANNs) help address this issue by learning domain-invariant representations, thereby improving transfer under greater domain shifts in a semi-supervised setting. However, DANNs can be unstable during training and lack a native means for uncertainty quantification. This study introduces a fully-supervised three-stage framework, the staged Bayesian domain-adversarial neural network (staged B-DANN), that combines parameter transfer and shared latent space adaptation. In Stage 1, a deterministic feature extractor is trained on the source domain. This feature extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a Bayesian neural network is built on the adapted feature extractor for fine-tuning on the target domain to handle conditional shifts and yield calibrated uncertainty estimates. This staged B-DANN approach was first validated on a synthetic benchmark, where it was shown to significantly outperform standard transfer techniques. It was then applied to the task of predicting critical heat flux in rectangular channels, leveraging data from tube experiments as the source domain. The results of this study show that the staged B-DANN method can improve predictive accuracy and generalization, potentially assisting other domains in nuclear engineering. |
| 2025-10-30 | [An extraction of the Collins-Soper kernel from a joint analysis of experimental and lattice data](http://arxiv.org/abs/2510.26489v1) | Artur Avkhadiev, Valerio Bertone et al. | We present a first joint extraction of the Collins-Soper kernel (CSK) combining experimental and lattice QCD data in the context of an analysis of transverse-momentum-dependent distributions (TMDs). Based on a neural-network parametrization, we perform a Bayesian reweighting of an existing fits of TMDs using lattice data, as well as a joint TMD fit to lattice and experimental data. We consistently find that the inclusion of lattice information shifts the central value of the CSK by approximately 10% and reduces its uncertainty by 40-50%, highlighting the potential of lattice inputs to improve TMD extractions. |
| 2025-10-29 | [An OPF-based Control Framework for Hybrid AC-MTDC Power Systems under Uncertainty](http://arxiv.org/abs/2510.25671v1) | Hongjin Du, Rahul Rane et al. | The increasing integration of renewable energy, particularly offshore wind, introduces significant uncertainty into hybrid AC-HVDC systems due to forecast errors and power fluctuations. Conventional control strategies typically rely on fixed setpoints and neglect frequency deviations, which can compromise system stability under rapid renewable variations. To address this challenge, this paper presents a forecast-integrated, optimal power flow (OPF)-based adaptive control framework. Wind speed forecasts generated using a Random Forest model are incorporated into a time-coupled OPF to determine baseline converter setpoints in anticipation of wind fluctuations, which are further adjusted in real time based on actual operating conditions. An adaptive droop control scheme is developed that jointly considers DC voltage and AC frequency deviations. The effectiveness of the proposed control framework is validated through hardware-in-the-loop (HIL) simulations, demonstrating its capability to ensure stable and robust operation of hybrid AC-HVDC systems under high penetration of renewable energy. |
| 2025-10-29 | [Observing Orbital Decay in the Ultracompact Hot Subdwarf Binary System ZTFJ213056.71+442046.5](http://arxiv.org/abs/2510.25653v1) | Paul Teckenburg, Thomas Kupfer et al. | Ultracompact Galactic binary systems (UCBs) emit low-frequency gravitational waves (GWs). The emission of GWs is causing these systems to lose angular momentum, which is detectable by observing an orbital period decay. ZTFJ213056.71+442046.5 (ZTFJ2130) is an UCB with a period of 39.3401(1) minutes consisting of a Roche lobe-filling hot subdwarf and a white dwarf companion. We attempt to measure the orbital decay rate $\dot{P}$ caused by GW emission of ZTFJ2130 and predict the expected GW signal for LISA. High-speed photometry was conducted using the FLI Kepler KL4040FI CMOS camera, mounted to the 1.2-meter Oskar L\"uhning telescope at the Hamburg Observatory as well as the Hamamatsu ORCA-Quest 2 qCMOS camera at the 1.23-meter telescope at CAHA in Spain. ZTFJ2130 was observed on six nights between August 2024 and September 2025. The obtained lightcurves combined with previous high-cadence observations were used to conduct an O-C timing analysis. Additionally, we employed the LISA data analysis tool ldasoft to model the expected GW data. We measure a period change of $(-1.97\pm0.05)\times10^{-12}\,\mathrm{ss^{-1}}$. Assuming only GW emission, this result was used to calculate a chirp mass of $(0.408\pm0.006)\,\mathrm{M_{\odot}}$. From ldasoft we predict that LISA will be able to measure the chirp mass with an uncertainty of 5%. We measure $\dot{P}$ with an uncertainty of only 2% and show that modern (q)CMOS detectors are well suited to provide precise timing measurements, enabling the measurement of the orbital decay of UCBs with high precision with modest size telescopes. The derived orbital decay is fully consistent with predictions from spectral and lightcurve modeling. We show that future observations with LISA can potentially provide a deviation from only gravitational wave effects, e.g. due to accretion, if the effect is sufficiently large. |
| 2025-10-29 | [Extra dip in ultrahigh energy neutrino spectrum from generalized uncertainty principle](http://arxiv.org/abs/2510.25635v1) | J. Barranco, Emiliano DurÃ¡n | We revisited the scenario of a resonant enhancement in the oscillation probability due to an interaction potential between neutrinos and dark matter with the novelty of the inclusion of the generalized uncertainty principle. It is shown that a new resonant conversion appears at higher energies. This effect could be tested with future neutrino data as new dips in the ultrahigh energy neutrino flux. |
| 2025-10-29 | [Predictability of Storms in an Idealized Climate Revealed by Machine Learning](http://arxiv.org/abs/2510.25633v1) | Wuqiushi Yao, Or Hadas et al. | The midlatitude climate and weather are shaped by storms, yet the factors governing their predictability remain insufficiently understood. Here, we use a Convolutional Neural Network (CNN) to predict and quantify uncertainty in the intensity growth and trajectory of over 200,000 storms simulated in a 200-year aquaplanet GCM. This idealized framework provides a controlled climate background for isolating factors that govern predictability. Results show that storm intensity is less predictable than trajectory. Strong baroclinicity accelerates storm intensification and reduces its predictability, consistent with theory. Crucially, enhanced jet meanders further degrade forecast skill, revealing a synoptic source of uncertainty. Using sensitivity maps from explainable AI, we find that the error growth rate is nearly doubled by the more meandering structure. These findings highlight the potential of machine learning for advancing understanding of predictability and its governing mechanisms. |
| 2025-10-29 | [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](http://arxiv.org/abs/2510.25599v1) | Christopher BÃ¼lte, Yusuf Sale et al. | Regression tasks, notably in safety-critical domains, require proper uncertainty quantification, yet the literature remains largely classification-focused. In this light, we introduce a family of measures for total, aleatoric, and epistemic uncertainty based on proper scoring rules, with a particular emphasis on kernel scores. The framework unifies several well-known measures and provides a principled recipe for designing new ones whose behavior, such as tail sensitivity, robustness, and out-of-distribution responsiveness, is governed by the choice of kernel. We prove explicit correspondences between kernel-score characteristics and downstream behavior, yielding concrete design guidelines for task-specific measures. Extensive experiments demonstrate that these measures are effective in downstream tasks and reveal clear trade-offs among instantiations, including robustness and out-of-distribution detection performance. |
| 2025-10-29 | [General model for estimating range variances of terrestrial laser scanners based on (un-)scaled intensity values](http://arxiv.org/abs/2510.25587v1) | Omar AbdelGafar, Selin Palaz et al. | Recent advancements in technology have established terrestrial laser scanners (TLS) as a powerful instrument in geodetic deformation analysis. As TLS becomes increasingly integrated into this field, it is essential to develop a comprehensive stochastic model that accurately captures the measurement uncertainties. A key component of this model is the construction of a complete and valid variance-covariance matrix (VCM) for TLS polar measurements, which requires the estimation of variances for range, vertical, and horizontal angles, as well as their correlations. While angular variances can be obtained from manufacturer specifications, the range variance varies with different intensity measurements. As a primary contribution, this study presents an effective methodology for measuring and estimating TLS range variances using both raw and scaled intensity values. A two-dimensional scanning approach is applied to both controlled targets and arbitrary objects using TLS instruments that provide raw intensity values (e.g., Z+F~Imager~5016A) and those that output scaled intensities (e.g., Leica~ScanStation~P50). The methodology is further evaluated using field observations on a water dam surface. Overall, this work introduces a comprehensive workflow for modeling range uncertainties in high-end TLS systems. |
| 2025-10-29 | [Robust variable selection for spatial point processes observed with noise](http://arxiv.org/abs/2510.25550v1) | Dominik Sturm, Ivo F. Sbalzarini | We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process. |
| 2025-10-29 | [A Unified Photometric Redshift Calibration for Weak Lensing Surveys using the Dark Energy Spectroscopic Instrument](http://arxiv.org/abs/2510.25419v1) | Johannes U. Lange, Diana Blanco et al. | The effective redshift distribution $n(z)$ of galaxies is a critical component in the study of weak gravitational lensing. Here, we introduce a new method for determining $n(z)$ for weak lensing surveys based on high-quality redshifts and neural network-based importance weights. Additionally, we present the first unified photometric redshift calibration of the three leading stage-III weak lensing surveys, the Dark Energy Survey (DES), the Hyper Suprime-Cam (HSC) survey and the Kilo-Degree Survey (KiDS), with state-of-the-art spectroscopic data from the Dark Energy Spectroscopic Instrument (DESI). We verify our method using a new, data-driven approach and obtain $n(z)$ constraints with statistical uncertainties of order $\sigma_{\bar z} \sim 0.01$ and smaller. Our analysis is largely independent of previous photometric redshift calibrations and, thus, provides an important cross-check in light of recent cosmological tensions. Overall, we find excellent agreement with previously published results on the DES Y3 and HSC Y1 data sets while there are some differences on the mean redshift with respect to the previously published KiDS-1000 results. We attribute the latter to mismatches in photometric noise properties in the COSMOS field compared to the wider KiDS SOM-gold catalog. At the same time, the new $n(z)$ estimates for KiDS do not significantly change estimates of cosmic structure growth from cosmic shear. Finally, we discuss how our method can be applied to future weak lensing calibrations with DESI data. |
| 2025-10-29 | [Model-Free Robust Beamforming in Satellite Downlink using Reinforcement Learning](http://arxiv.org/abs/2510.25393v1) | Alea SchrÃ¶der, Steffen Gracla et al. | Satellite-based communications are expected to be a substantial future market in 6G networks. As satellite constellations grow denser and transmission resources remain limited, frequency reuse plays an increasingly important role in managing inter-user interference. In the multi-user downlink, precoding enables the reuse of frequencies across spatially separated users, greatly improving spectral efficiency. The analytical calculation of suitable precodings for perfect channel information is well studied, however, their performance can quickly deteriorate when faced with, e.g., outdated channel state information or, as is particularly relevant for satellite channels, when position estimates are erroneous. Deriving robust precoders under imperfect channel state information is not only analytically intractable in general but often requires substantial relaxations of the optimization problem or heuristic constraints to obtain feasible solutions. Instead, in this paper we flexibly derive robust precoding algorithms from given data using reinforcement learning. We describe how we adapt the applied Soft Actor-Critic learning algorithm to the problem of downlink satellite beamforming and show numerically that the resulting precoding algorithm adjusts to all investigated scenarios. The considered scenarios cover both single satellite and cooperative multi-satellite beamforming, using either global or local channel state information, and two error models that represent increasing levels of uncertainty. We show that the learned algorithms match or markedly outperform two analytical baselines in sum rate performance, adapting to the required level of robustness. We also analyze the mechanisms that the learned algorithms leverage to achieve robustness. The implementation is publicly available for use and reproduction of the results. |
| 2025-10-29 | [Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods](http://arxiv.org/abs/2510.25386v1) | Kumar Manas, Mert Keser et al. | This survey provides an analysis of current methodologies integrating legal and logical specifications into the perception, prediction, and planning modules of automated driving systems. We systematically explore techniques ranging from logic-based frameworks to computational legal reasoning approaches, emphasizing their capability to ensure regulatory compliance and interpretability in dynamic and uncertain driving environments. A central finding is that significant challenges arise at the intersection of perceptual reliability, legal compliance, and decision-making justifiability. To systematically analyze these challenges, we introduce a taxonomy categorizing existing approaches by their theoretical foundations, architectural implementations, and validation strategies. We particularly focus on methods that address perceptual uncertainty and incorporate explicit legal norms, facilitating decisions that are both technically robust and legally defensible. The review covers neural-symbolic integration methods for perception, logic-driven rule representation, and norm-aware prediction strategies, all contributing toward transparent and accountable autonomous vehicle operation. We highlight critical open questions and practical trade-offs that must be addressed, offering multidisciplinary insights from engineering, logic, and law to guide future developments in legally compliant autonomous driving systems. |
| 2025-10-28 | [ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking](http://arxiv.org/abs/2510.24698v1) | Baixuan Li, Dingchu Zhang et al. | Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption. |
| 2025-10-28 | [Towards constraining cosmological parameters with SPT-3G observations of 25% of the sky](http://arxiv.org/abs/2510.24669v1) | A. Vitrier, K. Fichman et al. | The South Pole Telescope (SPT), using its third-generation camera, SPT-3G, is conducting observations of the cosmic microwave background (CMB) in temperature and polarization across approximately 10 000 deg$^2$ of the sky at 95, 150, and 220 GHz. This comprehensive dataset should yield stringent constraints on cosmological parameters. In this work, we explore its potential to address the Hubble tension by forecasting constraints from temperature, polarization, and CMB lensing on Early Dark Energy (EDE) and the variation in electron mass in spatially flat and curved universes. For this purpose, we investigate first whether analyzing the distinct SPT-3G observation fields independently, as opposed to as a single, unified region, results in a loss of information relevant to cosmological parameter estimation. We develop a realistic temperature and polarization likelihood pipeline capable of analyzing these fields in these two ways, and subsequently forecast constraints on cosmological parameters. Our findings indicate that any loss of constraining power from analyzing the fields separately is primarily concentrated at low multipoles ($\ell$ < 50) and the overall impact on the relative uncertainty on standard $\Lambda$CDM parameters is minimal (< 3%). Our forecasts suggest that SPT-3G data should improve by more than a factor of 200 and 3000 the Figure of Merit (FoM) of the EDE and the varying electron mass models, respectively, when combined with Planck data. The likelihood pipeline developed and used in this work is made publicly available online. |
| 2025-10-28 | [Fast Bayesian Multilevel Quasi-Monte Carlo](http://arxiv.org/abs/2510.24604v1) | Aleksei G. Sorokin, Pieterjan Robbe et al. | Existing multilevel quasi-Monte Carlo (MLQMC) methods often rely on multiple independent randomizations of a low-discrepancy (LD) sequence to estimate statistical errors on each level. While this approach is standard, it can be less efficient than simply increasing the number of points from a single LD sequence. However, a single LD sequence does not permit statistical error estimates in the current framework. We propose to recast the MLQMC problem in a Bayesian cubature framework, which uses a single LD sequence and quantifies numerical error through the posterior variance of a Gaussian process (GP) model. When paired with certain LD sequences, GP regression and hyperparameter optimization can be carried out at only $\mathcal{O}(n \log n)$ cost, where $n$ is the number of samples. Building on the adaptive sample allocation used in traditional MLQMC, where the number of samples is doubled on the level with the greatest expected benefit, we introduce a new Bayesian utility function that balances the computational cost of doubling against the anticipated reduction in posterior uncertainty. We also propose a new digitally-shift-invariant (DSI) kernel of adaptive smoothness, which combines multiple higher-order DSI kernels through a weighted sum of smoothness parameters, for use with fast digital net GPs. A series of numerical experiments illustrate the performance of our fast Bayesian MLQMC method and error estimates for both single-level problems and multilevel problems with a fixed number of levels. The Bayesian error estimates obtained using digital nets are found to be reliable, although, in some cases, mildly conservative. |
| 2025-10-28 | [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](http://arxiv.org/abs/2510.24595v1) | Azadeh Pourkabirian, Kai Li et al. | Hybrid precoding is an indispensable technique to harness the full potential of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In this paper, we propose a new hybrid precoding approach that combines digital and analog precoding to optimize data transmission over multiple antennas. This approach steers signals in specific directions, leading to maximizing sum-rate and suppressing side-lobe interference. When dealing with complex signals, changes in phase are naturally associated with changes in angle, and these variations are inherently correlated. The correlation between the angle and phase is essential for accurately determining the channel characteristics. An important aspect of this approach is that we model the angle and phase as correlated variables following a bivariate Gaussian distribution, and for the first time, we define a joint angle and phase entropy to measure the uncertainty of angle and phase variations in wireless channels. This entropy is crucial to adapt the proposed precoding method with variations. Simulation result validate the accuracy of our analytical findings, demonstrating 18.31% increase in sum-rate and an 11.47% improvement in robustness compared to other state-of-the-art methods. |
| 2025-10-28 | [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](http://arxiv.org/abs/2510.24577v1) | He Yang, Fei Ren et al. | We are very delighted to see the fast development of physics-informed extreme learning machine (PIELM) in recent years for higher computation efficiency and accuracy in physics-informed machine learning. As a summary or review on PIELM is currently not available, we would like to take this opportunity to show our perspective and experience for this promising research direction. We can see many efforts are made to solve PDEs with sharp gradients, nonlinearities, high-frequency behavior, hard constraints, uncertainty, multiphysics coupling. Despite the success, many urgent challenges remain to be tackled, which also provides us opportunities to develop more robust, interpretable, and generalizable PIELM frameworks with applications in science and engineering. |
| 2025-10-28 | [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](http://arxiv.org/abs/2510.24561v1) | Qingyue Zhang, Chang Chu et al. | With the widespread adoption of LLMs, LoRA has become a dominant method for PEFT, and its initialization methods have attracted increasing attention. However, existing methods have notable limitations: many methods do not incorporate target-domain data, while gradient-based methods exploit data only at a shallow level by relying on one-step gradient decomposition, which remains unsatisfactory due to the weak empirical performance of the one-step fine-tuning model that serves as their basis, as well as the fact that these methods either lack a rigorous theoretical foundation or depend heavily on restrictive isotropic assumptions. In this paper, we establish a theoretical framework for data-aware LoRA initialization based on asymptotic analysis. Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher-gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information. By solving this problem, we obtain an optimal initialization strategy for LoRA. Building on this theoretical framework, we develop an efficient algorithm, LoRA-DA, which estimates the terms in the optimization problem from a small set of target domain samples and obtains the optimal LoRA initialization. Empirical results across multiple benchmarks demonstrate that LoRA-DA consistently improves final accuracy over existing initialization methods. Additional studies show faster, more stable convergence, robustness across ranks, and only a small initialization overhead for LoRA-DA. The source code will be released upon publication. |
| 2025-10-28 | [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](http://arxiv.org/abs/2510.24554v1) | Vignesh Kottayam Viswanathan, Yifan Bai et al. | In this work, we present a hierarchical framework designed to support robotic inspection under environment uncertainty. By leveraging a known environment model, existing methods plan and safely track inspection routes to visit points of interest. However, discrepancies between the model and actual site conditions, caused by either natural or human activities, can alter the surface morphology or introduce path obstructions. To address this challenge, the proposed framework divides the inspection task into: (a) generating the initial global view-plan for region of interests based on a historical map and (b) local view replanning to adapt to the current morphology of the inspection scene. The proposed hierarchy preserves global coverage objectives while enabling reactive adaptation to the local surface morphology. This enables the local autonomy to remain robust against environment uncertainty and complete the inspection tasks. We validate the approach through deployments in real-world subterranean mines using quadrupedal robot. |
| 2025-10-28 | [CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?](http://arxiv.org/abs/2510.24505v1) | Qing Zong, Jiayu Liu et al. | Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability. |
| 2025-10-28 | [Sample-efficient and Scalable Exploration in Continuous-Time RL](http://arxiv.org/abs/2510.24482v1) | Klemens Iten, Lenart Treven et al. | Reinforcement learning algorithms are typically designed for discrete-time dynamics, even though the underlying real-world control systems are often continuous in time. In this paper, we study the problem of continuous-time reinforcement learning, where the unknown system dynamics are represented using nonlinear ordinary differential equations (ODEs). We leverage probabilistic models, such as Gaussian processes and Bayesian neural networks, to learn an uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily maximizes a weighted sum of the extrinsic reward and model epistemic uncertainty. This yields a scalable and sample-efficient approach to continuous-time model-based RL. We show that COMBRL achieves sublinear regret in the reward-driven setting, and in the unsupervised RL setting (i.e., without extrinsic rewards), we provide a sample complexity bound. In our experiments, we evaluate COMBRL in both standard and unsupervised RL settings and demonstrate that it scales better, is more sample-efficient than prior methods, and outperforms baselines across several deep RL tasks. |
| 2025-10-28 | [Collaborating Unmanned Aerial Vehicle and Ground Sensors for Urban Signalized Network Traffic Monitoring](http://arxiv.org/abs/2510.24460v1) | Jiarong Yao, Chaopeng Tan et al. | Reliable estimation of network-wide traffic states is essential for urban traffic management. Unmanned Aerial Vehicles (UAVs), with their airborne full-sample continuous trajectory observation, bring new opportunities for traffic state estimation. In this study, we will explore the optimal UAV deployment problem in road networks in conjunction with ground sensors, including connected vehicle (CV) and loop detectors, to achieve more reliable estimation of vehicle path reconstruction as well as movement-based arrival rates and queue lengths. Oriented towards reliable estimation of traffic states, we propose an index, feasible domain size, as the uncertainty measurement, and transform the optimal UAV deployment problem into minimizing the observation uncertainty of network-wide traffic states. Given the large-scale and nonlinear nature of the problem, an improved quantum genetic algorithm (IQGA) that integrates two customized operators is proposed to enhance neighbor searching and solution refinement, thereby improving the observability of UAV pairs. Evaluation was conducted on an empirical network with 18 intersections. Results demonstrated that a UAV fleet size of 7 is sufficient for traffic monitoring, with more than 60\% of network-wide observation uncertainty reduced. Through horizontal comparison with three baselines, the optimal UAV location scheme obtained by the proposed method can reach an improvement of up to 7.23\% and 5.02\% in the estimation accuracy of arrival rate and queue length, respectively. The proposed IQGA is also shown to be faster in solution convergence than the classic QGA by about 9.22\% with better exploration ability in optimum searching. |
| 2025-10-27 | [Lightweight Robust Direct Preference Optimization](http://arxiv.org/abs/2510.23590v1) | Cheol Woo Kim, Shresth Verma et al. | Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants. |
| 2025-10-27 | [Cosmic magnification on multi-catalogue Herschel submillimetre galaxies](http://arxiv.org/abs/2510.23582v1) | R. Fernandez-Fernandez, M. M. Cueli et al. | {Submillimetre galaxies (SMGs) are excellent background sources for magnification-bias studies, but the limited sky coverage in the submillimetre (sub-mm) band constrains their statistical power. Beyond H-ATLAS, Herschel produced additional sub-mm catalogues, though not optimised for spatial statistical lensing analyses.} {Our goal is to refine cosmological constraints from SMG magnification bias by exploiting the full sub-mm sky surveyed by Herschel.} {We expanded the SMG sample by incorporating other Herschel catalogues overlapping SDSS spectroscopic lenses. Random catalogues were generated via kernel density estimation to compute cross-correlations, and Markov Chain Monte Carlo methods were applied to infer astrophysical and cosmological parameters for each catalogue and for the combined dataset.} {We report the first detection of magnification bias in SMGs beyond H-ATLAS, reinforcing the robustness of this observable. Individual Herschel catalogues yield reasonable central values for $\Omega_m$ and $\sigma_8$, although with large uncertainties. The combined analysis, dominated by the more powerful H-ATLAS sample, gives results consistent with $\Lambda$CDM: $\Omega_m = 0.30^{+0.05}_{-0.07}$, $\sigma_8 = 0.80 (+/- 0.07)$, and $h < 0.80$, in better agreement with \textit{Planck} 2018 than previous non-tomographic studies.} {SMGs are promising tracers for magnification bias, but the narrow sub-mm coverage remains a major limitation. Wider surveys optimised for lensing would enable cross-correlations on larger scales, yielding tighter cosmological constraints.} |
| 2025-10-27 | [UrbanVLA: A Vision-Language-Action Model for Urban Micromobility](http://arxiv.org/abs/2510.23576v1) | Anqi Li, Zhiyong Wang et al. | Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties. |
| 2025-10-27 | [COMAP Pathfinder -- Season 2 results IV. A stack on eBOSS/DESI quasars](http://arxiv.org/abs/2510.23568v1) | D. A. Dunne, K. A. Cleary et al. | We present a stack of data from the second season of the CO Mapping Array Project (COMAP) Pathfinder on the positions of quasars from eBOSS and DESI. COMAP is a Line Intensity Mapping (LIM) experiment targeting dense molecular gas via CO(1--0) emission at $z\sim3$. COMAP's Season 2 represents a $3\times$ increase in map-level sensitivity over the previous Early Science data release. We do not detect any CO emission in the stack, instead finding an upper limit of $10.0\times 10^{10}\ \mathrm{K\ km\ s^{-1}\ pc^2}$ at 95\% confidence within an $\sim 18\ \mathrm{cMpc}$ box. We compare this upper limit to models of the CO emission stacked on quasars and find a tentative ($\sim 3 \sigma$) tension between the limit and the brightest stack models after accounting for a suite of additional sources of experimental attenuation and uncertainty, including quasar velocity uncertainty, pipeline signal loss, cosmic variance, and interloper emission in the LIM data. The COMAP-eBOSS/DESI stack is primarily a measurement of the CO luminosity in the quasars' wider environment and is therefore potentially subject to environmental effects such as feedback. With our current simple models of the galaxy-halo connection, we are thus unable to confidently rule out any models of cosmic CO with the stack alone. Conversely, the stack's sensitivity to these large-scale environmental effects has the potential to make it a powerful tool for galaxy formation science, once we are able to constrain the average CO luminosity via the auto power spectrum (a key goal of COMAP). |
| 2025-10-27 | [Dark Energy Survey Year 6 Results: Redshift Calibration of the Weak Lensing Source Galaxies](http://arxiv.org/abs/2510.23566v1) | B. Yin, A. Amon et al. | Determining the distribution of redshifts for galaxies in wide-field photometric surveys is essential for robust cosmological studies of weak gravitational lensing. We present the methodology, calibrated redshift distributions, and uncertainties of the final Dark Energy Survey Year 6 (Y6) weak lensing galaxy data, divided into four redshift bins centered at $\langle z \rangle = [0.414, 0.538, 0.846, 1.157]$. We combine independent information from two methods on the full shape of redshift distributions: optical and near-infrared photometry within an improved Self-Organizing Map $p(z)$ (SOMPZ) framework, and cross-correlations with spectroscopic galaxy clustering measurements (WZ), which we demonstrate to be consistent both in terms of the redshift calibration itself and in terms of resulting cosmological constraints within 0.1$\sigma$. We describe the process used to produce an ensemble of redshift distributions that account for several known sources of uncertainty. Among these, imperfection in the calibration sample due to the lack of faint, representative spectra is the dominant factor. The final uncertainty on mean redshift in each bin is $\sigma_{\langle z\rangle} = [0.012, 0.008,0.009, 0.024]$. We ensure the robustness of the redshift distributions by leveraging new image simulations and a cross-check with galaxy shape information via the shear ratio (SR) method. |
| 2025-10-27 | [Dark Energy Survey Year 6 Results: Clustering-redshifts and importance sampling of Self-Organised-Maps $n(z)$ realizations for $3\times2$pt samples](http://arxiv.org/abs/2510.23565v1) | W. d'Assignies, G. M. Bernstein et al. | This work is part of a series establishing the redshift framework for the $3\times2$pt analysis of the Dark Energy Survey Year 6 (DES Y6). For DES Y6, photometric redshift distributions are estimated using self-organizing maps (SOMs), calibrated with spectroscopic and many-band photometric data. To overcome limitations from color-redshift degeneracies and incomplete spectroscopic coverage, we enhance this approach by incorporating clustering-based redshift constraints (clustering-z, or WZ) from angular cross-correlations with BOSS and eBOSS galaxies, and eBOSS quasar samples. We define a WZ likelihood and apply importance sampling to a large ensemble of SOM-derived $n(z)$ realizations, selecting those consistent with the clustering measurements to produce a posterior sample for each lens and source bin. The analysis uses angular scales of 1.5-5 Mpc to optimize signal-to-noise while mitigating modeling uncertainties, and marginalizes over redshift-dependent galaxy bias and other systematics informed by the N-body simulation Cardinal. While a sparser spectroscopic reference sample limits WZ constraining power at $z>1.1$, particularly for source bins, we demonstrate that combining SOMPZ with WZ improves redshift accuracy and enhances the overall cosmological constraining power of DES Y6. We estimate an improvement in $S_8$ of approximately 10\% for cosmic shear and $3\times2$pt analysis, primarily due to the WZ calibration of the source samples. |
| 2025-10-27 | [Towards Stochastic (N-1)-Secure Redispatch](http://arxiv.org/abs/2510.23551v1) | Oleksii Molodchyk, Hendrik DrÃ¶gehorn et al. | The intermittent nature of renewable power availability is one of the major sources of uncertainty in power systems. While markets can guarantee that the demand is covered by the available generation, transmission system operators have to often intervene via economic redispatch to ensure that the physical constraints of the network are satisfied. To account for uncertainty, the underlying optimal power flow (OPF) routines have to be modified. Recently, polynomial chaos expansion (PCE) has been suggested in the literature as a tool for stochastic OPF problems. However, the usage of PCE-based methods in security-constrained OPF for (N-1)-secure operations has not yet been explored. In this paper, we propose a procedure that iteratively solves a PCE-overloaded stochastic OPF problem by including line outage constraints until an (N-1)-secure solution is achieved. We demonstrate the efficacy of our method by comparing it with a Monte-Carlo simulation on a 118-bus example system. |
| 2025-10-27 | [Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with Application to the Richards Equation](http://arxiv.org/abs/2510.23550v1) | Yumo Yang, Anass Ben Bouazza et al. | The estimation of unknown parameters in nonlinear partial differential equations (PDEs) offers valuable insights across a wide range of scientific domains. In this work, we focus on estimating plant root parameters in the Richards equation, which is essential for understanding the soil-plant system in agricultural studies. Since conventional methods are computationally intensive and often yield unstable estimates, we develop a new Gaussian process collocation method for efficient Bayesian inference. Unlike existing Gaussian process-based approaches, our method constructs an approximate posterior distribution using samples drawn from a Gaussian process model fitted to the observed data, which does not require any structural assumption about the underlying PDE. Further, we propose to use an importance sampling procedure to correct for the discrepancy between the approximate and true posterior distributions. As an alternative, we also devise a prior-guided Bayesian optimization algorithm leveraging the approximate posterior. Simulation studies demonstrate that our method yields robust estimates under various settings. Finally, we apply our method on a real agricultural data set and estimate the plant root parameters with uncertainty quantification. |
| 2025-10-27 | [An Error-Based Safety Buffer for Safe Adaptive Control (Extended Version)](http://arxiv.org/abs/2510.23491v1) | Peter A. Fisher, Johannes Autenrieb et al. | We consider the problem of adaptive control of a class of feedback linearizable plants with matched parametric uncertainties whose states are accessible, subject to state constraints, which often arise due to safety considerations. In this paper, we combine adaptation and control barrier functions into a real-time control architecture that guarantees stability, ensures control performance, and remains safe even with the parametric uncertainties. Two problems are considered, differing in the nature of the parametric uncertainties. In both cases, the control barrier function is assumed to have an arbitrary relative degree. In addition to guaranteeing stability, it is proved that both the control objective and safety objective are met with near-zero conservatism. No excitation conditions are imposed on the command signal. Simulation results demonstrate the non-conservatism of all of the theoretical developments. |
| 2025-10-27 | [Human-AI Collaborative Uncertainty Quantification](http://arxiv.org/abs/2510.23476v1) | Sima Noorani, Shayan Kiyani et al. | AI predictive systems are increasingly embedded in decision making pipelines, shaping high stakes choices once made solely by humans. Yet robust decisions under uncertainty still rely on capabilities that current AI lacks: domain knowledge not captured by data, long horizon context, and reasoning grounded in the physical world. This gap has motivated growing efforts to design collaborative frameworks that combine the complementary strengths of humans and AI. This work advances this vision by identifying the fundamental principles of Human AI collaboration within uncertainty quantification, a key component of reliable decision making. We introduce Human AI Collaborative Uncertainty Quantification, a framework that formalizes how an AI model can refine a human expert's proposed prediction set with two goals: avoiding counterfactual harm, ensuring the AI does not degrade correct human judgments, and complementarity, enabling recovery of correct outcomes the human missed. At the population level, we show that the optimal collaborative prediction set follows an intuitive two threshold structure over a single score function, extending a classical result in conformal prediction. Building on this insight, we develop practical offline and online calibration algorithms with provable distribution free finite sample guarantees. The online method adapts to distribution shifts, including human behavior evolving through interaction with AI, a phenomenon we call Human to AI Adaptation. Experiments across image classification, regression, and text based medical decision making show that collaborative prediction sets consistently outperform either agent alone, achieving higher coverage and smaller set sizes across various conditions. |
| 2025-10-24 | [From nuclear star clusters to Little Red Dots: black hole growth, mergers, and tidal disruptions](http://arxiv.org/abs/2510.21709v1) | Konstantinos Kritos, Joseph Silk | Little Red Dots, discovered by the James Webb Space Telescope, are hypothesized to be active galactic nuclei containing a supermassive black hole, possibly surrounded by a dense stellar cluster, large amounts of gas, and likely by a population of stellar-mass black holes. We develop a simple nuclear star cluster model to evolve the rapid mass growth of black hole seeds into the supermassive regime. The combined processes of tidal disruption events, black hole captures, and gas accretion are accounted for self-consistently in our model. Given the observed number density of Little Red Dots, and under reasonable assumptions, we predict at least a few tens of tidal disruption events and at least a few black hole captures at $z=4$-$6$, with a tidal disruption event rate an order of magnitude larger than the black hole capture rate. We also estimate the uncertainties in these estimates. Finally, we comment on the low x-ray luminosity of Little Red Dots. |
| 2025-10-24 | [Connecting Chemical Enrichment with Resolved Star Formation Histories](http://arxiv.org/abs/2510.21707v1) | Christopher T. Garling, Alex M. Garcia et al. | We present a new framework for modeling the chemical enrichment histories of galaxies by integrating the chemical evolution with resolved star formation histories (SFHs) derived from color-magnitude diagrams. This novel approach links the time evolution of the metallicity of the star-forming ISM to the cumulative stellar mass formed in the galaxy, enabling a physically motivated, self-consistent description of chemical evolution. We apply this methodology to four isolated, gas-rich Local Group dwarf galaxies -- WLM, Aquarius, Leo A, and Leo P -- using deep HST and JWST imaging. For WLM, Aquarius, and Leo A, we independently validate our metallicity evolution results using ages and metallicities of individual red giant stars with spectroscopic measurements, finding good agreement. We quantify systematic uncertainties by repeating our analysis with multiple stellar evolution and bolometric correction libraries. We then compare the observed chemical enrichment histories to predictions from the TNG50 and FIREbox cosmological hydrodynamic simulations and the Galacticus semi-analytic model. We find that the enrichment history of WLM is best reproduced by the FIREbox simulation, while TNG50 and Galacticus predict higher metallicities at early times. Our results suggest that differences in stellar feedback and metal recycling prescriptions drive significant variation in the predicted chemical enrichment of dwarf galaxies, particularly at early times. This work demonstrates the power of combining resolved SFHs with physically motivated chemical evolution models to constrain galaxy formation physics and highlights the need for further observational and theoretical studies of metal retention and recycling in low-mass dwarf galaxies. |
| 2025-10-24 | [On Uncertainty Calibration for Equivariant Functions](http://arxiv.org/abs/2510.21691v1) | Edward Berman, Jacob Ginesin et al. | Data-sparse settings such as robotic manipulation, molecular physics, and galaxy morphology classification are some of the hardest domains for deep learning. For these problems, equivariant networks can help improve modeling across undersampled parts of the input space, and uncertainty estimation can guard against overconfidence. However, until now, the relationships between equivariance and model confidence, and more generally equivariance and model calibration, has yet to be studied. Since traditional classification and regression error terms show up in the definitions of calibration error, it is natural to suspect that previous work can be used to help understand the relationship between equivariance and calibration error. In this work, we present a theory relating equivariance to uncertainty estimation. By proving lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under various equivariance conditions, we elucidate the generalization limits of equivariant models and illustrate how symmetry mismatch can result in miscalibration in both classification and regression. We complement our theoretical framework with numerical experiments that clarify the relationship between equivariance and uncertainty using a variety of real and simulated datasets, and we comment on trends with symmetry mismatch, group size, and aleatoric and epistemic uncertainties. |
| 2025-10-24 | [Comparing the data reduction pipelines of FRIPON, DFN, WMPL, and AMOS: Geminids Case Study](http://arxiv.org/abs/2510.21690v1) | P. M. Shober, J. Vaubaillon et al. | Methods. We processed a dataset of 584 Geminid fireballs observed by FRIPON between 2016 and 2023. The single-station astrometric data is converted into the Global Fireball Exchange (GFE) standard format for uniform processing. We assess variations in trajectory, velocity, radiant, and orbital element calculations across the pipelines and compare them to previously published Geminid measurements.   Results. The radiant and velocity solutions provided by the four data reduction pipelines are all within the range of previously published values. However, there are some nuances. Particularly, the radiants estimated by WMPL, DFN, and AMOS are nearly identical. Whereas FRIPON reports a systematic shift in right ascension (-0.3 degrees), caused by improper handling of the precession. Additionally, the FRIPON data reduction pipeline also tends to overestimate the initial velocity (+0.3 km s-1) due to the deceleration model used as the velocity solver. The FRIPON velocity method relies on having a well-constrained deceleration profile; however, for the Geminids, many are low-deceleration events, leading to an overestimation of the initial velocity. On the other end of the spectrum, the DFN tends to predict lower velocities, particularly for poorly observed events. However, this velocity shift vanishes for the DFN when we only consider Geminids with at least three observations or more. The primary difference identified in the analysis concerns the velocity uncertainties. Despite all four pipelines achieving similar residuals between their trajectories and observations, their velocity uncertainties vary systematically, with WMPL outputting the smallest values, followed by AMOS, FRIPON, and DFN. |
| 2025-10-24 | [Energy storage in a continuous-variable quantum battery with nonlinear coupling](http://arxiv.org/abs/2510.21672v1) | C. A. Downing, M. S. Ukhtary | In the quantum world, the process of energy storage can be enhanced thanks to various nonclassical phenomena. This inspiring fact suggests quantum batteries as plausible sources of power for future quantum devices, at least in principle. However, thermodynamically not all of the energy stored in a quantum battery is useful for doing work. By considering a class of models based upon quantum continuous variables, here we show how the maximum extractable energy from a bosonic quantum battery can be intimately related to Heisenberg's uncertainty principle. We found that realizing minimum uncertainty essentially guarantees that all of the energy stored in a Gaussian quantum battery can be withdrawn and used to do work. For a standard system where the charger and battery are coupled linearly, this criterion is satisfied rather trivially. However, our theoretical results demonstrate that - for a quantum battery with nonlinear coupling - a state of minimum uncertainty can also be achieved nontrivially via the generation of quantum squeezing. We characterize the charging performance of our proposed continuous variable quantum batteries in detail, and we hope that our theory may be useful in the design of a new generation of efficient quantum batteries harnessing bosonic excitations, such as those built with photonic architectures. |
| 2025-10-24 | [FlowSynth: Instrument Generation Through Distributional Flow Matching and Test-Time Search](http://arxiv.org/abs/2510.21667v1) | Qihui Yang, Randal Leistikow et al. | Virtual instrument generation requires maintaining consistent timbre across different pitches and velocities, a challenge that existing note-level models struggle to address. We present FlowSynth, which combines distributional flow matching (DFM) with test-time optimization for high-quality instrument synthesis. Unlike standard flow matching that learns deterministic mappings, DFM parameterizes the velocity field as a Gaussian distribution and optimizes via negative log-likelihood, enabling the model to express uncertainty in its predictions. This probabilistic formulation allows principled test-time search: we sample multiple trajectories weighted by model confidence and select outputs that maximize timbre consistency. FlowSynth outperforms the current state-of-the-art TokenSynth baseline in both single-note quality and cross-note consistency. Our approach demonstrates that modeling predictive uncertainty in flow matching, combined with music-specific consistency objectives, provides an effective path to professional-quality virtual instruments suitable for real-time performance. |
| 2025-10-24 | [Modest-Align: Data-Efficient Alignment for Vision-Language Models](http://arxiv.org/abs/2510.21606v1) | Jiaxiang Liu, Yuan Wang et al. | Cross-modal alignment aims to map heterogeneous modalities into a shared latent space, as exemplified by models like CLIP, which benefit from large-scale image-text pretraining for strong recognition capabilities. However, when operating in resource-constrained settings with limited or low-quality data, these models often suffer from overconfidence and degraded performance due to the prevalence of ambiguous or weakly correlated image-text pairs. Current contrastive learning approaches, which rely on single positive pairs, further exacerbate this issue by reinforcing overconfidence on uncertain samples. To address these challenges, we propose Modest-Align, a lightweight alignment framework designed for robustness and efficiency. Our approach leverages two complementary strategies -- Random Perturbation, which introduces controlled noise to simulate uncertainty, and Embedding Smoothing, which calibrates similarity distributions in the embedding space. These mechanisms collectively reduce overconfidence and improve performance on noisy or weakly aligned samples. Extensive experiments across multiple benchmark datasets demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval tasks, achieving competitive results with over 100x less training data and 600x less GPU time than CLIP. Our method offers a practical and scalable solution for cross-modal alignment in real-world, low-resource scenarios. |
| 2025-10-24 | [An unsupervised tour through the hidden pathways of deep neural networks](http://arxiv.org/abs/2510.21582v1) | Diego Doimo | The goal of this thesis is to improve our understanding of the internal mechanisms by which deep artificial neural networks create meaningful representations and are able to generalize. We focus on the challenge of characterizing the semantic content of the hidden representations with unsupervised learning tools, partially developed by us and described in this thesis, which allow harnessing the low-dimensional structure of the data. Chapter 2. introduces Gride, a method that allows estimating the intrinsic dimension of the data as an explicit function of the scale without performing any decimation of the data set. Our approach is based on rigorous distributional results that enable the quantification of uncertainty of the estimates. Moreover, our method is simple and computationally efficient since it relies only on the distances among nearest data points. In Chapter 3, we study the evolution of the probability density across the hidden layers in some state-of-the-art deep neural networks. We find that the initial layers generate a unimodal probability density getting rid of any structure irrelevant to classification. In subsequent layers, density peaks arise in a hierarchical fashion that mirrors the semantic hierarchy of the concepts. This process leaves a footprint in the probability density of the output layer, where the topography of the peaks allows reconstructing the semantic relationships of the categories. In Chapter 4, we study the problem of generalization in deep neural networks: adding parameters to a network that interpolates its training data will typically improve its generalization performance, at odds with the classical bias-variance trade-off. We show that wide neural networks learn redundant representations instead of overfitting to spurious correlation and that redundant neurons appear only if the network is regularized and the training error is zero. |
| 2025-10-24 | [Surrogate-based quantification of policy uncertainty in generative flow networks](http://arxiv.org/abs/2510.21523v1) | RamÃ³n Nartallo-Kaluarachchi, Robert Manson-Sawko et al. | Generative flow networks are able to sample, via sequential construction, high-reward, complex objects according to a reward function. However, such reward functions are often estimated approximately from noisy data, leading to epistemic uncertainty in the learnt policy. We present an approach to quantify this uncertainty by constructing a surrogate model composed of a polynomial chaos expansion, fit on a small ensemble of trained flow networks. This model learns the relationship between reward functions, parametrised in a low-dimensional space, and the probability distributions over actions at each step along a trajectory of the flow network. The surrogate model can then be used for inexpensive Monte Carlo sampling to estimate the uncertainty in the policy given uncertain rewards. We illustrate the performance of our approach on a discrete and continuous grid-world, symbolic regression, and a Bayesian structure learning task. |
| 2025-10-24 | [EXKALIBUR: Towards a Kaonic Atoms Periodic Table to test Fundamental Interactions](http://arxiv.org/abs/2510.21519v1) | Simone Manti, Leonardo Abbene et al. | Kaonic atoms, formed when a negatively charged kaon replaces an electron, provide a unique laboratory to test fundamental interactions at low energies. EXKALIBUR (EXtensive Kaonic Atoms research: from LIthium and Beryllium to URanium) is a program to perform systematic, high-precision X-ray spectroscopy of selected kaonic atoms across the periodic table at the DA$\Phi$NE accelerator at the National Laboratory of Frascati (INFN-LNF). Here, we outline its detector-driven strategy: Silicon Drift Detectors for 10-40 keV transitions in light targets (Li, Be, B, O), CdZnTe detectors for 40-300 keV lines in intermediate-$Z$ systems (Mg, Al, Si, S), and a High-Purity Germanium detector for high-$Z$ atoms (Se, Zr, Ta, Mo, W, Pb), complemented by VOXES, a high-resolution crystal spectrometer for sub-eV studies. EXKALIBUR plans to (i) reduce the charged-kaon mass uncertainty below 10 keV, (ii) produce a database of nuclear shifts and widths to constrain multi-nucleon K$^{-}$-nucleus interaction models, and (iii) provide precision data for testing bound-state QED in strong fields. We summarize the planned measurements and expected sensitivities within DA$\Phi$NE luminosities. |
| 2025-10-23 | [Simulation-calibrated Bayesian inference for progenitor properties of the microquasar SS 433](http://arxiv.org/abs/2510.20811v1) | Nathan Steinle, Matthew Mould et al. | SS\,433 is one of the most extreme Galactic X-ray binaries, exhibiting semi-relativistic jets and super-critical accretion, and harboring a compact object, likely a black hole. Despite decades of observation and modeling, the precise nature of its progenitor binary remains uncertain. To estimate the zero-age main sequence (ZAMS) properties of binaries that evolve into SS\,433-like systems, we apply simulation-based calibration to Bayesian inference and convolve a multivariate Gaussian likelihood constructed from six measured binary parameters of SS\,433 with the isolated binary evolution model \textsc{COSMIC}. Employing the dynamic nested sampler of \texttt{dynesty}, we perform posterior inference over a ten-dimensional progenitor parameter space defined by the masses, orbital parameters, mass transfer possibilities, and natal kick velocity. We find that SS\,433-like systems arise from specific regions of binary evolution parameter space depending on key assumptions, such as the mass transfer rate and uncertainty taken from observations. Our simulation-based calibration framework, implemented with a suite of machine learning algorithms and scored by a heuristic reliability metric, allows us to iteratively build posterior distributions of the progenitors of SS\,433-like systems. This analysis reveals 90\% confidence intervals for the ZAMS primary mass $(8, 11)$ M$_\odot$, secondary mass $(32, 40)$ M$_\odot $, orbital period $(136, 2259)$ days, eccentricity $(0.26, 0.6)$, common envelope evolution efficiency $(0.44, 0.76)$, accreted fraction in stable mass transfer $(0.22, 0.6)$, and black hole natal kick velocity magnitude $(5, 68)$ km/s. These results demonstrate the feasibility of direct probabilistic inference of X-ray binary progenitors to offer new insights into the evolution of high-accretion-rate systems such as SS\,433. |
| 2025-10-23 | [Quantum black holes: inside and outside](http://arxiv.org/abs/2510.20799v1) | Wei-Chen Lin, Dong-han Yeom et al. | For a unitary description of an evaporating black hole, one usually chooses the time slices that cover only outside of the event horizon, which is mostly problem-free because the event horizon is not encountered. However, is there any justification for avoiding time slices that cover inside the event horizon? To answer the question, we investigate the Wheeler-DeWitt equation, where the time slices can cover both inside and outside the event horizon. We find that one can reasonably construct a wave packet that covers outside, but the wave function must be annihilated near the event horizon. This observation strongly suggests that we cannot choose a coherent state for a spacelike hypersurface that crosses the event horizon. To explain the unitary time evolution, we must keep the slices as coherent states; hence, they must always be outside the event horizon. In contrast, inside the horizon, we cannot have a single coherent state of a classical spacetime. Hence, the interior must be a superposition of several coherent states, which implies that there exists a horizon-scale uncertainty and a black hole should be viewed as a highly quantum macroscopic object. We provide a synthetic approach to understanding the information loss paradox from this perspective. |
| 2025-10-23 | [Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks](http://arxiv.org/abs/2510.20795v1) | Juan Alejandro Pinto Castro, HÃ©ctor J. HortÃºa et al. | Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology. |
| 2025-10-23 | [Reinforcement Learning and Consumption-Savings Behavior](http://arxiv.org/abs/2510.20748v1) | Brandon Kaplowitz | This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a "scarring" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict. |
| 2025-10-23 | [Bayesian Prediction under Moment Conditioning](http://arxiv.org/abs/2510.20742v1) | Nicholas G. Polson, Daniel Zantedeschi | Prediction is a central task of statistics and machine learning, yet many inferential settings provide only partial information, typically in the form of moment constraints or estimating equations. We develop a finite, fully Bayesian framework for propagating such partial information through predictive distributions. Building on de Finetti's representation theorem, we construct a curvature-adaptive version of exchangeable updating that operates directly under finite constraints, yielding an explicit discrete-Gaussian mixture that quantifies predictive uncertainty. The resulting finite-sample bounds depend on the smallest eigenvalue of the information-geometric Hessian, which measures the curvature and identification strength of the constraint manifold. This approach unifies empirical likelihood, Bayesian empirical likelihood, and generalized method-of-moments estimation within a common predictive geometry. On the operational side, it provides computable curvature-sensitive uncertainty bounds for constrained prediction; on the theoretical side, it recovers de Finetti's coherence, Doob's martingale convergence and local asymptotic normality as limiting cases of the same finite mechanism. Our framework thus offers a constructive bridge between partial information and full Bayesian prediction. |
| 2025-10-23 | [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](http://arxiv.org/abs/2510.20725v1) | Jasmine Bayrooti, Sattar Vakili et al. | Thompson sampling (TS) is a powerful and widely used strategy for sequential decision-making, with applications ranging from Bayesian optimization to reinforcement learning (RL). Despite its success, the theoretical foundations of TS remain limited, particularly in settings with complex temporal structure such as RL. We address this gap by establishing no-regret guarantees for TS using models with Gaussian marginal distributions. Specifically, we consider TS in episodic RL with joint Gaussian process (GP) priors over rewards and transitions. We prove a regret bound of $\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$, where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis addresses several challenges, including the non-Gaussian nature of value functions and the recursive structure of Bellman updates, and extends classical tools such as the elliptical potential lemma to multi-output settings. This work advances the understanding of TS in RL and highlights how structural assumptions and model uncertainty shape its performance in finite-horizon Markov Decision Processes. |
| 2025-10-23 | [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](http://arxiv.org/abs/2510.20666v1) | Mariona Jaramillo-Civill, Luis GonzÃ¡lez-GudiÃ±o et al. | Global Navigation Satellite System (GNSS) signals are vulnerable to jamming, particularly in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed the received signal strength (RSS) field due to limited spatial context. We propose a hybrid Bayesian mixture-of-experts framework that fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling. The PL expert ensures physical consistency, while the CNN leverages building-height maps to capture urban propagation effects. Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field. Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points, while uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive. |
| 2025-10-23 | [Blue supergiants and the zero point of the Tully-Fisher relation: a path to a new independent test of the Hubble constant](http://arxiv.org/abs/2510.20652v1) | Rolf-Peter Kudritzki, Fabio Bresolin et al. | Blue supergiant distances of nearby galaxies obtained with the flux-weighted gravity-luminosity relationship are used for a measurement of the zero points of Tully-Fisher relationships at different photometric passbands. The Cousins I-band and the infrared WISE bands W1 and W2 are investigated. The results are compared with previous work using Cepheid and Tip-of-the-Red-Giant-Branch distances. No significant differences were encountered. This supports the large values of the Hubble constant greater than 73km/s/Mpc found with the Tully-Fisher distance ladder work over the last decade. Applying blue supergiant distances on the I-band Tully-Fisher relation observations yields a Hubble constant H0 = 76.2+/-6.2 km/s/Mpc. The large uncertainty is caused by the still relatively small blue supergiant galaxies sample size but will be reduced in future work. |
| 2025-10-23 | [Strong Lensing Model and Dust Extinction Maps of the Host Galaxy of Type Ia Supernova H0pe](http://arxiv.org/abs/2510.20561v1) | A. Galan, S. Schuldt et al. | Strong gravitational lensing by massive galaxy clusters offers particularly rare opportunities to observe multiple images of distant ($z\gtrsim2$) Type Ia supernovae (SNe) and resolve the properties of their host galaxies. A recent outstanding example is the Type Ia SN "H0pe" ($z=1.78$), discovered in James Webb Space Telescope (JWST) NIRCam images when it was still triply imaged by the galaxy cluster PLCK G165.7+67.0 (G165, $z=0.35$). In this work we build a new strong lensing model of G165, first by using only the position of multiple images of background galaxies. We then increase significantly the number of constraints around the position of SN H0pe by modeling the extended surface brightness of the SN host galaxy. The average uncertainty on mass model parameters is reduced by more than an order of magnitude. We also study the spatial distribution of dust in the arc to estimate the dust extinction at the position of SN H0pe. We find good statistical agreement of the extinction estimate at $\lesssim1\sigma$ with three fully independent methods based on spectral energy distribution fitting. Moreover, our extended-image lens model of G165 allows us to map the dust distribution of the host galaxy from the image plane to the source plane. Supernova H0pe exploded in a region with a relatively high extinction of $A_V \approx 0.9\ {\rm mag}$ at around $\sim 1\ {\rm kpc}$ from its host center. This work shows that extended image modeling in lensing clusters simultaneously reduces the uncertainty on lens model parameters and enables spatially resolved analyses of lensed transients host galaxies. Such modeling advances are expected to play an important role in future cosmological analyses using strongly lensed SNe. |
| 2025-10-23 | [Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy](http://arxiv.org/abs/2510.20551v1) | Jacob Ayers, Richard Hahnloser et al. | Conditional differential entropy provides an intuitive measure for relatively ranking time-series complexity by quantifying uncertainty in future observations given past context. However, its direct computation for high-dimensional processes from unknown distributions is often intractable. This paper builds on the information theoretic prediction error bounds established by Fang et al. \cite{fang2019generic}, which demonstrate that the conditional differential entropy \textbf{$h(X_k \mid X_{k-1},...,X_{k-m})$} is upper bounded by a function of the determinant of the covariance matrix of next-step prediction errors for any next step prediction model. We add to this theoretical framework by further increasing this bound by leveraging Hadamard's inequality and the positive semi-definite property of covariance matrices.   To see if these bounds can be used to rank the complexity of time series, we conducted two synthetic experiments: (1) controlled linear autoregressive processes with additive Gaussian noise, where we compare ordinary least squares prediction error entropy proxies to the true entropies of various additive noises, and (2) a complexity ranking task of bio-inspired synthetic audio data with unknown entropy, where neural network prediction errors are used to recover the known complexity ordering.   This framework provides a computationally tractable method for time-series complexity ranking using prediction errors from next-step prediction models, that maintains a theoretical foundation in information theory. |
| 2025-10-22 | [How Accurate Are DFT Forces? Unexpectedly Large Uncertainties in Molecular Datasets](http://arxiv.org/abs/2510.19774v1) | Domantas Kuryla, Fabian Berger et al. | Training of general-purpose machine learning interatomic potentials (MLIPs) relies on large datasets with properties usually computed with density functional theory (DFT). A pre-requisite for accurate MLIPs is that the DFT data are well converged to minimize numerical errors. A possible symptom of errors in DFT force components is nonzero net force. Here, we consider net forces in datasets including SPICE, Transition1x, ANI-1x, ANI-1xbb, AIMNet2, QCML, and OMol25. Several of these datasets suffer from significant nonzero DFT net forces. We also quantify individual force component errors by comparison to recomputed forces using more reliable DFT settings at the same level of theory, and we find significant discrepancies in force components averaging from 1.7 meV/{\AA} in the SPICE dataset to 33.2 meV/{\AA} in the ANI-1x dataset. These findings underscore the importance of well converged DFT data as increasingly accurate MLIP architectures become available. |
| 2025-10-22 | [Beta-decay Half Lives beyond $^{54}$Ca: A Systematic Survey of Decay Properties approaching the Neutron Dripline](http://arxiv.org/abs/2510.19757v1) | W. -J. Ong, Z. Y. Xu et al. | In an experiment performed at the Facility for Rare Isotope Beams (FRIB) using the FRIB Decay Station initiator (FDSi), 15 new half lives of isotopes near $^{54}$Ca were measured. A new method of extracting lifetimes from experimental data, taking into account the unknown $\beta$-delayed neutron emission branches of very neutron-rich nuclei, was developed to enable systematic uncertainty analysis. The experiment observed a dramatic change in the half-life systematics for the isotopes with neutron number N =34. Beyond N =34, the decline of nuclear lifetime is much slower, leading to longer than anticipated lifetimes for near-dripline nuclei. State-of-the-art shell-model calculations can explain the experimental results for Z$>$19 nuclei, revealing the imprint of shell effects and the need for modification of single-particle neutron states. The results from a newly developed QRPA model with potential for making global predictions were also tested against the experimental results and good agreement was found. |
| 2025-10-22 | [CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees](http://arxiv.org/abs/2510.19754v1) | Aman Bilkhoo, Milad Kazemi et al. | Counterfactual explanations (CFXs) provide human-understandable justifications for model predictions, enabling actionable recourse and enhancing interpretability. To be reliable, CFXs must avoid regions of high predictive uncertainty, where explanations may be misleading or inapplicable. However, existing methods often neglect uncertainty or lack principled mechanisms for incorporating it with formal guarantees. We propose CONFEX, a novel method for generating uncertainty-aware counterfactual explanations using Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX explanations are designed to provide local coverage guarantees, addressing the issue that CFX generation violates exchangeability. To do so, we develop a novel localised CP procedure that enjoys an efficient MILP encoding by leveraging an offline tree-based partitioning of the input space. This way, CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty and optimality. We evaluate CONFEX against state-of-the-art methods across diverse benchmarks and metrics, demonstrating that our uncertainty-aware approach yields robust and plausible explanations. |
| 2025-10-22 | [BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models](http://arxiv.org/abs/2510.19749v1) | Catherine Villeneuve, Benjamin Akera et al. | Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts. |
| 2025-10-22 | [Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+Î´}$](http://arxiv.org/abs/2510.19734v1) | Bhavya Agrawalla, Krishnakumar Balasubramanian et al. | Stochastic Gradient Descent (SGD) has become a cornerstone method in modern data science. However, deploying SGD in high-stakes applications necessitates rigorous quantification of its inherent uncertainty. In this work, we establish \emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online least-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in a \emph{growing-dimensional regime}. Existing approaches to high-dimensional inference for projection parameters, such as~\cite{chang2023inference}, rely on inverting empirical covariance matrices and require at least $t \gtrsim d^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees, rendering them computationally expensive and restrictive in the allowable dimensional scaling. In contrast, we show that a CLT holds for SGD iterates when the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta > 0$, significantly extending the dimensional regime permitted by prior works while improving computational efficiency. The proposed online SGD-based procedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$ memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime of covariance-inversion methods. To render the theory practically applicable, we further develop an \emph{online variance estimator} for the asymptotic variance appearing in the CLT and establish \emph{high-probability deviation bounds} for this estimator. Collectively, these results yield the first fully online and data-driven framework for constructing confidence intervals for SGD iterates in the near-optimal scaling regime $t \gtrsim d^{1+\delta}$. |
| 2025-10-22 | [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](http://arxiv.org/abs/2510.19670v1) | Hasan Akgul, Mari Eplik et al. | We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments. |
| 2025-10-22 | [DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference](http://arxiv.org/abs/2510.19669v1) | Xiang Liu, Xuming Hu et al. | Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their efficiency, enabling them to reach high performance without overthinking. First, we analyze the entropy of token probabilities in reasoning traces. Across three models, we observe a consistent U-shaped entropy pattern: high entropy on easy problems despite high accuracy, low entropy on problems with medium difficulty, and high entropy on hard problems reflecting uncertainty. Specifically, we notice 22--25\% entropy reduction from easy to medium difficulty regions, suggesting an {overthinking} phenomenon on easy instances. Building on these insights, we introduce \textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard inference strategies per question based on their difficulty and reasoning trace entropy. Each inference strategy consists of a fixed prompt, temperature and maximum token length. In contrast to existing efficiency optimization methods, our approach does not fine-tune base LLM but a small probe that classifies LLM's final hidden state, allowing inexpensive adaptation. We comprehensively evaluate our method on five models and eight benchmarks. Our method achieves comparable or improved accuracy while reducing token usage by up to 22.4\%, establishing a practical path toward compute-efficient reasoning. |
| 2025-10-22 | [Engineering the shapes of quark-gluon plasma droplets by comparing anisotropic flow in small symmetric and asymmetric collision systems](http://arxiv.org/abs/2510.19645v1) | STAR Collaboration | The observation of collective flow phenomena in small collision systems challenges our understanding of quark-gluon plasma (QGP) formation and evolution. This complexity lies in the initial geometries, which are influenced by both nucleon configuration and subnucleonic fluctuations, introducing uncertainties in interpreting flow patterns. We disentangle these contributions through comparative measurements of elliptic ($v_2$) and triangular ($v_3$) flow in asymmetric $d$+Au and symmetric $^{16}$O+$^{16}$O collisions at $\sqrt{s_{NN}}=200$ GeV, which produce medium of comparable sizes but with vastly different initial geometries. The larger $v_2$ in $d$+Au reflects its dominant elliptic geometry, while the similar $v_3$ in both systems is better explained by considering subnucleonic fluctuations. These contrasting flow patterns are quantitatively described by a state-of-the-art hydrodynamic model tuned to large-system Au+Au data, indicating efficient transformation of initial geometries to final-state anisotropies. These results provide evidence for droplet formation in small systems with transport properties that are similar to those observed in large collision systems, consistent with QGP-like behavior. |
| 2025-10-22 | [A Climate-Aware Deep Learning Framework for Generalizable Epidemic Forecasting](http://arxiv.org/abs/2510.19611v1) | Jinpyo Hong, Rachel E. Baker | Precise outbreak forecasting of infectious diseases is essential for effective public health responses and epidemic control. The increased availability of machine learning (ML) methods for time-series forecasting presents an enticing avenue to enhance outbreak forecasting. Though the COVID-19 outbreak demonstrated the value of applying ML models to predict epidemic profiles, using ML models to forecast endemic diseases remains underexplored. In this work, we present ForecastNet-XCL (an ensemble model based on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to addresses this gap by creating accurate multi-week RSV forecasts up to 100 weeks in advance based on climate and temporal data, without access to real-time surveillance on RSV. The framework combines high-resolution feature learning with long-range temporal dependency capturing mechanisms, bolstered by an autoregressive module trained on climate-controlled lagged relations. Stochastic inference returns probabilistic intervals to inform decision-making. Evaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed statistical baselines, individual neural nets, and conventional ensemble methods in both within- and cross-state scenarios, sustaining accuracy over extended forecast horizons. Training on climatologically diverse datasets enhanced generalization furthermore, particularly in locations having irregular or biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and uncertainty-aware design make it a deployable early-warning tool amid escalating climate pressures and constrained surveillance resources. |
| 2025-10-22 | [CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization](http://arxiv.org/abs/2510.19597v1) | Zhou Lei, Pan Gang et al. | Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment. |
| 2025-10-21 | [NNLO QCD$\otimes$QED corrections to unpolarized and polarized SIDIS](http://arxiv.org/abs/2510.18872v1) | Saurav Goyal, Roman N. Lee et al. | We present the first computation of next-to-next-to-leading order (NNLO) pure QED and mixed QCD$\otimes$QED corrections to unpolarized and polarized semi-inclusive deep-inelastic scattering (SIDIS). Building on our previous NNLO QCD results, these corrections are crucial for improving the theoretical precision. The coefficient functions are derived within the QCD factorization framework using dimensional regularization, with consistent renormalization and mass factorization. A detailed phenomenological analysis shows that the NNLO QED and QCD$\otimes$QED terms enhance perturbative stability and reduce scale uncertainties. These results are essential for high-precision SIDIS predictions at future facilities such as the Electron-Ion Collider. |
| 2025-10-21 | [Reexamining Evidence of a Pair-Instability Mass Gap in the Binary Black Hole Population](http://arxiv.org/abs/2510.18867v1) | Anarya Ray, Vicky Kalogera | The fourth gravitational wave transient catalog~(GWTC-4) has enabled empirical probes of the theorized pair-instability gap in the higher end of the binary black hole~(BBH) mass-spectrum. In this letter, using flexibly parametrized models, we show that at present there is no evidence of a sharp drop-off in the spectrum of black hole masses near $~40-50M_{\odot}$. We simultaneously characterize the transition in the distribution of BBH mass-ratios, effective aligned and effective precessing spins using our flexible models. From the transitions in our inferred spin and mass-ratio distributions, we find that the high-mass broad-spin sub-population has a significant fraction~($52^{+18}_{-23}\%$) of systems with mass ratios in the range $0.6-1$. This suggests that alternatives to the hypothesis of 2G+1G hierarchical systems dominating BBH formation above $\sim 40-50 M_{\odot}$ are more consistent with the GWTC-4 detection sample. By comparing with the predictions of star cluster simulations, we further show that contributions from (2G+2G) systems are not abundant enough to alleviate this discrepancy. We also demonstrate the effects of strong model assumptions on this inference, which can lead to biased astrophysical interpretation from restricted priors. We note that our results do not exclude that a high-mass gap may be identified as our sample size increases. We constrain the lower bound on the location of a possible PISN cutoff still allowed within measurement uncertainties to be $(57^{+17}_{-10}M_{\odot})$ and discuss its implications on the S factor of $^{12}\mathrm{C}(\alpha, \gamma)^{16}O$ at 300 kev. |
| 2025-10-21 | [Degeneracy-Aware Pulsar Parameter Estimation from Light Curves via Deep Learning and Test-Time Optimization](http://arxiv.org/abs/2510.18807v1) | Abu Bucker Siddik, Diane Oyen et al. | Probing properties of neutron stars from photometric observations of these objects helps us answer crucial questions at the forefront of multi-messenger astronomy, such as, what is behavior of highest density matter in extreme environments and what is the procedure of generation and evolution of magnetic fields in these astrophysical environments? However, uncertainties and degeneracies-where different parameter sets produce similar light curves-make this task challenging. We propose a deep learning framework for inferring pulsar parameters from observed light curves. Traditional deep learning models are not designed to produce multiple degenerate solutions for a given input. To address this, we introduce a custom loss function that incorporates a light curve emulator as a forward model, along with a dissimilarity loss that encourages the model to capture diverse, degenerate parameter sets for a given light curve. We further introduce a test-time optimization scheme that refines predicted parameters by minimizing the discrepancy between the observed light curve and those reconstructed by the forward model from predicted parameters during inference. The model is trained using a suite of state-of-the-art simulated pulsar light curves. Finally, we demonstrate that the parameter sets predicted by our approach reproduce light curves that are consistent with the true observation. |
| 2025-10-21 | [Gravitational-wave and electromagnetic detections in the context of the CosmoDC2 LSST synthetic catalog](http://arxiv.org/abs/2510.18727v1) | Ranier Menote, Valerio Marra et al. | We release CosmoDC2_BCO, a synthetic catalog of gravitational-wave events and electromagnetic counterparts associated with galaxies from CosmoDC2. The catalog provides intrinsic and extrinsic source parameters, signal-to-noise ratios, parameter uncertainties, sky localization areas, and kilonova apparent magnitudes in LSST filters. Our results show that third-generation detector networks substantially increase detection rates and improve parameter estimation. Second-generation detectors, when combined with third-generation ones, significantly enhance sky localization and distance precision, particularly for BNS mergers. Assuming a simplified Target of Opportunity strategy, we estimate that an LSST-like survey, partnered with the CE+ET+LVK network at 70% duty cycle, could detect about 5000 kilonovae with GW counterparts over a 10-year period on a 16000 deg^2 footprint, predominantly from low-mass BNS mergers that produce long-lived supermassive neutron star remnants. While this is a substantial number, it represents only a small fraction of the total neutron star mergers expected to be observed by third-generation networks. These projections rely on several simplifying assumptions-including the adopted merger rate, the kilonova luminosity distribution, and the configuration and scheduling of future surveys-which introduce notable uncertainties. Therefore, the estimated detection numbers should be interpreted with appropriate caution. |
| 2025-10-21 | [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](http://arxiv.org/abs/2510.18713v1) | Joongkyu Lee, Seouh-won Yi et al. | We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size. |
| 2025-10-21 | [Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations](http://arxiv.org/abs/2510.18697v1) | Phuoc Nguyen, Francesco Verdoja et al. | A fundamental aspect for building intelligent autonomous robots that can assist humans in their daily lives is the construction of rich environmental representations. While advances in semantic scene representations have enriched robotic scene understanding, current approaches lack a connection between spatial features and dynamic events; e.g., connecting the blue mug to the event washing a mug. In this work, we introduce the event-grounding graph (EGG), a framework grounding event interactions to spatial features of a scene. This representation allows robots to perceive, reason, and respond to complex spatio-temporal queries. Experiments using real robotic data demonstrate EGG's capability to retrieve relevant information and respond accurately to human inquiries concerning the environment and events within. Furthermore, the EGG framework's source code and evaluation dataset are released as open-source at: https://github.com/aalto-intelligent-robotics/EGG. |
| 2025-10-21 | [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](http://arxiv.org/abs/2510.18648v1) | Miro Miranda, Marcela Charfuelan et al. | Water is essential for agricultural productivity. Assessing water shortages and reduced yield potential is a critical factor in decision-making for ensuring agricultural productivity and food security. Crop simulation models, which align with physical processes, offer intrinsic explainability but often perform poorly. Conversely, machine learning models for crop yield modeling are powerful and scalable, yet they commonly operate as black boxes and lack adherence to the physical principles of crop growth. This study bridges this gap by coupling the advantages of both worlds. We postulate that the crop yield is inherently defined by the water availability. Therefore, we formulate crop yield as a function of temporal water scarcity and predict both the crop drought stress and the sensitivity to water scarcity at fine-scale resolution. Sequentially modeling the crop yield response to water enables accurate yield prediction. To enforce physical consistency, a novel physics-informed loss function is proposed. We leverage multispectral satellite imagery, meteorological data, and fine-scale yield data. Further, to account for the uncertainty within the model, we build upon a deep ensemble approach. Our method surpasses state-of-the-art models like LSTM and Transformers in crop yield prediction with a coefficient of determination ($R^2$-score) of up to 0.82 while offering high explainability. This method offers decision support for industry, policymakers, and farmers in building a more resilient agriculture in times of changing climate conditions. |
| 2025-10-21 | [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](http://arxiv.org/abs/2510.18631v1) | Carlo Proietti, Antonio Yuste-Ginel | Modelling qualitative uncertainty in formal argumentation is essential both for practical applications and theoretical understanding. Yet, most of the existing works focus on \textit{abstract} models for arguing with uncertainty. Following a recent trend in the literature, we tackle the open question of studying plausible instantiations of these abstract models. To do so, we ground the uncertainty of arguments in their components, structured within rules and premises. Our main technical contributions are: i) the introduction of a notion of expressivity that can handle abstract and structured formalisms, and ii) the presentation of both negative and positive expressivity results, comparing the expressivity of abstract and structured models of argumentation with uncertainty. These results affect incomplete abstract argumentation frameworks, and their extension with dependencies, on the abstract side, and ASPIC+, on the structured side. |
| 2025-10-21 | [Deep Q-Learning Assisted Bandwidth Reservation for Multi-Operator Time-Sensitive Vehicular Networking](http://arxiv.org/abs/2510.18553v1) | Abdullah Al-Khatib, Albert Gergus et al. | Very few available individual bandwidth reservation schemes provide efficient and cost-effective bandwidth reservation that is required for safety-critical and time-sensitive vehicular networked applications. These schemes allow vehicles to make reservation requests for the required resources. Accordingly, a Mobile Network Operator (MNO) can allocate and guarantee bandwidth resources based on these requests. However, due to uncertainty in future reservation time and bandwidth costs, the design of an optimized reservation strategy is challenging. In this article, we propose a novel multi-objective bandwidth reservation update approach with an optimal strategy based on Double Deep Q-Network (DDQN). The key design objectives are to minimize the reservation cost with multiple MNOs and to ensure reliable resource provisioning in uncertain situations by solving scenarios such as underbooked and overbooked reservations along the driving path. The enhancements and advantages of our proposed strategy have been demonstrated through extensive experimental results when compared to other methods like greedy update or other deep reinforcement learning approaches. Our strategy demonstrates a 40% reduction in bandwidth costs across all investigated scenarios and simultaneously resolves uncertain situations in a cost-effective manner. |
| 2025-10-21 | [Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data](http://arxiv.org/abs/2510.18548v1) | Ying Yao, Daniel J. Graham | Accurate annual average daily traffic (AADT) data are vital for transport planning and infrastructure management. However, automatic traffic detectors across national road networks often provide incomplete coverage, leading to underrepresentation of minor roads. While recent machine learning advances have improved AADT estimation at unmeasured locations, most models produce only point predictions and overlook estimation uncertainty. This study addresses that gap by introducing an interval prediction approach that explicitly quantifies predictive uncertainty. We integrate a Quantile Random Forest model with Principal Component Analysis to generate AADT prediction intervals, providing plausible traffic ranges bounded by estimated minima and maxima. Using data from over 2,000 minor roads in England and Wales, and evaluated with specialized interval metrics, the proposed method achieves an interval coverage probability of 88.22%, a normalized average width of 0.23, and a Winkler Score of 7,468.47. By combining machine learning with spatial and high-dimensional analysis, this framework enhances both the accuracy and interpretability of AADT estimation, supporting more robust and informed transport planning. |
| 2025-10-20 | [Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks](http://arxiv.org/abs/2510.17798v1) | Samuel Talkington, Cameron Khanpour et al. | This paper presents probabilistic bounds for the spectrum of the admittance matrix and classical linear power flow models under uncertain network parameters; for example, probabilistic line contingencies. Our proposed approach imports tools from probability theory, such as concentration inequalities for random matrices with independent entries. It yields error bounds for common approximations of the AC power flow equations under parameter uncertainty, including the DC and LinDistFlow approximations. |
| 2025-10-20 | [QUIJOTE scientific results XIX. New constraints on the synchrotron spectral index using a semi-blind component separation method](http://arxiv.org/abs/2510.17761v1) | Debabrata Adak, J. A. RubiÃ±o-MartÃ­n et al. | We introduce a novel approach to estimate the spectral index, $\beta_s$, of polarised synchrotron emission, combining the moment expansion of CMB foregrounds and the constrained-ILC method. We reconstruct the maps of the first two synchrotron moments, combining multi-frequency data, and apply the `T-T plot' technique between two moment maps to estimate the synchrotron spectral index. This approach offers a new technique for mapping the foreground spectral parameters, complementing the model-based parametric component separation methods. Applying this technique, we derive a new constraint on the spectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey 11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and Ka bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar Spur regions, we obtain an inverse-variance-weighted mean synchrotron index of $\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter, consistent with previous results based on parametric methods using the same dataset. We find that the inverse-variance-weighted mean spectral index, including both statistical and systematic uncertainties, is $\beta_s^{\rm plane} = -3.05 \pm 0.01$ in the Galactic plane and $\beta_s^{\rm high\text{-}lat} = -3.13 \pm 0.02$ at high latitudes, indicating a moderate steepening of the spectral index from low to high Galactic latitudes. Our analysis indicates that, within the current upper limit on the AME polarisation fraction, our results are not subject to any appreciable bias. Furthermore, we infer the spectral index over the entire QUIJOTE survey region, partitioning the sky into 21 patches. This technique can be further extended to constrain the synchrotron spectral curvature by reconstructing higher-order moments when better-quality data become available. |
| 2025-10-20 | [Enabling Fine-Grained Operating Points for Black-Box LLMs](http://arxiv.org/abs/2510.17727v1) | Ege Beyazit, KL Navaneet et al. | Black-box Large Language Models (LLMs) provide practical and accessible alternatives to other machine learning methods, as they require minimal labeled data and machine learning expertise to develop solutions for various decision making problems. However, for applications that need operating with constraints on specific metrics (e.g., precision $\geq$ 95%), decision making with black-box LLMs remains unfavorable, due to their low numerical output cardinalities. This results in limited control over their operating points, preventing fine-grained adjustment of their decision making behavior. In this paper, we study using black-box LLMs as classifiers, focusing on efficiently improving their operational granularity without performance loss. Specifically, we first investigate the reasons behind their low-cardinality numerical outputs and show that they are biased towards generating rounded but informative verbalized probabilities. Then, we experiment with standard prompt engineering, uncertainty estimation and confidence elicitation techniques, and observe that they do not effectively improve operational granularity without sacrificing performance or increasing inference cost. Finally, we propose efficient approaches to significantly increase the number and diversity of available operating points. Our proposed approaches provide finer-grained operating points and achieve comparable to or better performance than the benchmark methods across 11 datasets and 3 LLMs. |
| 2025-10-20 | [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](http://arxiv.org/abs/2510.17690v1) | Xihong Su | This dissertation makes three main contributions. First, We identify a new connection between policy gradient and dynamic programming in MMDPs and propose the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov policy that maximizes the discounted return averaged over the uncertain models. CADP adjusts model weights iteratively to guarantee monotone policy improvements to a local maximum. Second, We establish sufficient and necessary conditions for the exponential ERM Bellman operator to be a contraction and prove the existence of stationary deterministic optimal policies for ERM-TRC and EVaR-TRC. We also propose exponential value iteration, policy iteration, and linear programming algorithms for computing optimal stationary policies for ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the optimal risk-averse value functions. The proposed Q-learning algorithms compute the optimal stationary policy for ERM-TRC and EVaR-TRC. |
| 2025-10-20 | [LILO: Bayesian Optimization with Interactive Natural Language Feedback](http://arxiv.org/abs/2510.17671v1) | Katarzyna Kobalczyk, Zhiyuan Jerry Lin et al. | For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space. Unlike preferential BO, which only accepts restricted feedback formats and requires customized models for each domain-specific problem, our approach leverages LLMs to turn varied types of textual feedback into consistent utility signals and to easily include flexible user priors without manual kernel design. At the same time, our method maintains the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes. |
| 2025-10-20 | [Space-Time Rate-Splitting Multiple Access for Multibeam LEO Satellite Networks](http://arxiv.org/abs/2510.17625v1) | Jaehyup Seong, Byungju Lee et al. | This paper proposes a novel space-time rate-splitting multiple access (ST-RSMA) framework for multibeam low Earth orbit (LEO) satellite communications (SATCOM) systems, where space-time coding is integrated into the common stream transmission. This design enables full diversity gain in the common stream transmission for all users, regardless of the uncertainty of the channel state information (CSI) and network load conditions, thereby overcoming the performance limitations of conventional RSMA that employs a single beamforming vector for all users. To further enhance performance, we develop a weighted minimum mean square error (WMMSE)-based algorithm tailored to ST-RSMA that jointly optimizes the power allocation for the common stream and the power/beamforming vectors for private streams, aiming to maximize the minimum user rate. Numerical results show that ST-RSMA significantly outperforms conventional RSMA and other multiple access techniques, offering a robust and scalable solution for LEO SATCOM. |
| 2025-10-20 | [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](http://arxiv.org/abs/2510.17614v1) | Praphul Singh, Corey Barrett et al. | Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection. |
| 2025-10-20 | [Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials](http://arxiv.org/abs/2510.17550v1) | Andrew Hall, Duncan Wilson et al. | The main objective of dose finding trials is to find an optimal dose amongst a candidate set for further research. The trial design in oncology proceeds in stages with a decision as to how to treat the next group of patients made at every stage until a final sample size is reached or the trial stopped early.   This work applies a Bayesian decision-theoretic approach to the problem, proposing a new utility function based on both efficacy and toxicity and grounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed framework seeks to better capture real clinical judgements by allowing attitudes to risk to vary when the judgements are of gains or losses, which are defined with respect to an intermediate outcome known as a reference point. We call this method Reference Dependent Decision Theoretic dose finding (R2DT).   A simulation study demonstrates that the framework can perform well and produce good operating characteristics. The simulation results demonstrate that R2DT is better at detecting the optimal dose in scenarios where candidate doses are around minimum acceptable efficacy and maximum acceptable toxicity thresholds.   The proposed framework shows that a flexible utility function, which better captures clinician beliefs, can lead to trials with good operating characteristics, including a high probability of finding the optimal dose. Our work demonstrates proof-of-concept for this framework, which should be evaluated in a broader range of settings. |
| 2025-10-20 | [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](http://arxiv.org/abs/2510.17548v1) | Nisrine Rair, Alban Goupil et al. | Language models are often evaluated with scalar metrics like accuracy, but such measures fail to capture how models internally represent ambiguity, especially when human annotators disagree. We propose a topological perspective to analyze how fine-tuned models encode ambiguity and more generally instances.   Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from topological data analysis, reveals that fine-tuning restructures embedding space into modular, non-convex regions aligned with model predictions, even for highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$ prediction purity, yet alignment with ground-truth labels drops in ambiguous data, surfacing a hidden tension between structural confidence and label uncertainty.   Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry directly uncovering decision regions, boundary collapses, and overconfident clusters. Our findings position Mapper as a powerful diagnostic tool for understanding how models resolve ambiguity. Beyond visualization, it also enables topological metrics that may inform proactive modeling strategies in subjective NLP tasks. |
| 2025-10-20 | [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](http://arxiv.org/abs/2510.17496v1) | Giacomo Camposampiero, Michael Hersche et al. | We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes. |
| 2025-10-17 | [A Unifying Convexification Framework for Chance-Constrained Programs via Bilinear Extended Formulations over a Simplex](http://arxiv.org/abs/2510.15861v1) | Danial Davarnia, Hamed Rahimian | Chance-constrained programming is a widely used framework for decision-making under uncertainty, yet its mixed-integer reformulations involve nonconvex mixing sets with a knapsack constraint, leading to weak relaxations and computational challenges. Most existing approaches for strengthening the relaxations of these sets rely primarily on extensions of a specific class of valid inequalities, limiting both convex hull coverage and the discovery of fundamentally new structures. In this paper, we develop a novel convexification framework that reformulates chance-constrained sets as bilinear sets over a simplex in a lifted space and employs a step-by-step aggregation procedure to derive facet-defining inequalities in the original space of variables. Our approach generalizes and unifies established families of valid inequalities in the literature while introducing new ones that capture substantially larger portions of the convex hull. Main contributions include: (i) the development of a new aggregation-based convexification technique for bilinear sets over a simplex in a lower-dimensional space; (ii) the introduction of a novel bilinear reformulation of mixing sets with a knapsack constraint -- arising from single-row relaxations of chance constraints -- over a simplex, which enables the systematic derivation of strong inequalities in the original variable space; and (iii) the characterization of facet-defining inequalities within a unified framework that contains both existing and new families. Preliminary computational experiments demonstrate that our inequalities describe over 90\% of the facet-defining inequalities of the convex hull of benchmark instances, significantly strengthening existing relaxations and advancing the polyhedral understanding of chance-constrained programs. |
| 2025-10-17 | [Quantum Monte Carlo Calculations of Light Nuclei with Fully Propagated Theoretical Uncertainties](http://arxiv.org/abs/2510.15860v1) | Ryan Curry, Kai Hebeler et al. | We report on the first quantum Monte Carlo calculations of helium isotopes with fully propagated theoretical uncertainties from the interaction to the many-body observables. To achieve this, we build emulators for solutions to the Faddeev equations for the binding energy and Gamow-Teller matrix element of $^3\text{H}$, as well as for auxiliary-field diffusion Monte Carlo calculations of the $^4\text{He}$ charge radius, employing local two- and three-body interactions up to next-to-next-to-leading order in chiral effective field theory. We use these emulators to determine the posterior distributions for all low-energy couplings that appear in the interaction up to this order using Bayesian inference while accounting for theoretical uncertainties. We then build emulators for auxiliary-field diffusion Monte Carlo for helium isotopes and propagate the full posterior distributions to these systems. Our approach serves as a framework for $\textit{ab initio}$ studies of atomic nuclei with consistently treated and correlated theoretical uncertainties. |
| 2025-10-17 | [Bio-inspired Microgrid Management based on Brain's Sensorimotor Gating](http://arxiv.org/abs/2510.15847v1) | Panos C. Papageorgiou, Anastasios E. Giannopoulos et al. | Microgrids are emerging as key enablers of resilient, sustainable, and intelligent power systems, but they continue to face challenges in dynamic disturbance handling, protection coordination, and uncertainty. Recent efforts have explored Brain Emotional Learning (BEL) controllers as bio-inspired solutions for microgrid control. Building on this growing trajectory, this article introduces a new paradigm for Neuro-Microgrids, inspired by the brain's sensorimotor gating mechanisms, specifically the Prepulse Inhibition (PPI) and Prepulse Facilitation (PPF). Sensorimotor gating offers a biological model for selectively suppressing or amplifying responses depending on contextual relevance. By mapping these principles onto the hierarchical control architecture of microgrids, we propose a Sensorimotor Gating-Inspired Neuro-Microgrid (SG-NMG) framework. In this architecture, PPI-like control decisions correspond to protective damping in primary and secondary management of microgrids, whereas PPF-like decisions correspond to adaptive amplification of corrective control actions. The framework is presented through analytical workflow design, neuro-circuitry analogies, and integration with machine learning methods. Finally, open challenges and research directions are outlined, including the mathematical modeling of gating, digital twin validation, and cross-disciplinary collaboration between neuroscience and industrial power systems. The resulting paradigm highlights sensorimotor gating as a promising framework for designing self-protective, adaptive, and resilient microgrids. |
| 2025-10-17 | [From Localization to Discovery: Bayesian Ranking of Electromagnetic Counterparts to Gravitational-Wave Events](http://arxiv.org/abs/2510.15836v1) | Kendall Ackley | The robust association of electromagnetic candidates discovered during follow-up of gravitational-wave alerts is challenging, not only due to the large sky areas and broad distance uncertainties, but also due to the tens to hundreds of unrelated optical transients that are observed per event. We present a Bayesian ranking method to identify electromagnetic counterparts to GW events using only location information. The framework combines three-dimensional gravitational wave skymaps with host-galaxy information, a morphology-aware host association, empirical offset priors, and peculiar velocity corrections. We apply the method to GW170817 where it ranks AT2017gfo as the top candidate and correctly selects NGC\,4993 as the host. The approach is directly applicable to transient candidates with only location information and enables more efficient follow-up with prioritized candidates and leads to more reliable counterpart identification in current and future observing runs. |
| 2025-10-17 | [The kinetic Sunyaev Zeldovich effect as a benchmark for AGN feedback models in hydrodynamical simulations: insights from DESI + ACT](http://arxiv.org/abs/2510.15822v1) | Leah Bigwood, Masaya Yamamoto et al. | Baryonic feedback remains one of the largest uncertainties in cosmological hydrodynamical simulations, with different prescriptions producing divergent predictions for the fraction of gas expelled from halos, the radial extent of the gas expulsion and the impact on large scale matter clustering. We present the first systematic study of the kinetic Sunyaev-Zel'dovich (kSZ) effect across a wide range of simulations (FLAMINGO, ANTILLES, BAHAMAS, SIMBA, FABLE and their variants), and compare them directly to DESI Year 1 + ACT kSZ measurements. We ensure a like-for-like comparison with observations by developing a robust methodology that accounts for the halo mass selection using galaxy-galaxy lensing, cosmic variance, miscentering and satellites, establishing the kSZ effect as a new benchmark for the simulations. We find that fiducial feedback models are disfavoured by >3 sigma, while simulations with more powerful AGN feedback within the FLAMINGO and BAHAMAS suites, as well as SIMBA, reproduce the observed kSZ signal within <2 sigma. We use the ANTILLES simulation suite to demonstrate that the amplitude of the kSZ effect is a strong predictor of matter power spectrum suppression, competitive with baryon fraction metrics. These results establish the kSZ as a critical probe for evaluating feedback physics and for advancing the fidelity of cosmological simulations. |
| 2025-10-17 | [Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction](http://arxiv.org/abs/2510.15780v1) | Alireza Moradi, Mathieu Tanneau et al. | Accurate forecasting is critical for reliable power grid operations, particularly as the share of renewable generation, such as wind and solar, continues to grow. Given the inherent uncertainty and variability in renewable generation, probabilistic forecasts have become essential for informed operational decisions. However, such forecasts frequently suffer from calibration issues, potentially degrading decision-making performance. Building on recent advances in Conformal Predictions, this paper introduces a tailored calibration framework that constructs context-aware calibration sets using a novel weighting scheme. The proposed framework improves the quality of probabilistic forecasts at the site and fleet levels, as demonstrated by numerical experiments on large-scale datasets covering several systems in the United States. The results demonstrate that the proposed approach achieves higher forecast reliability and robustness for renewable energy applications compared to existing baselines. |
| 2025-10-17 | [Multiphysics inversion with variable complexity of receiver-function, surface-wave dispersion and magnetotelluric data reduces uncertainty for lithosphere structure](http://arxiv.org/abs/2510.15779v1) | P. Shahsavari, J. Dettmer et al. | We present a probabilistic multiphysics inversion based on Bayesian inference with trans-dimensional models. We jointly consider magnetotelluric, receiver function, and Rayleigh-wave dispersion data to infer one-dimensional lithospheric structure in the vicinity of Athabasca, Canada. The location is on the North American Craton with a cover of sediments from the Western Canada Sedimentary Basin. The trans-dimensional model uses layer nodes that include parameters that are activated or deactivated based on data information. Furthermore, the number of nodes is based on data information. Hence, the parameterization uncertainty is included in the uncertainty estimates. Furthermore, the layer nodes permit trans-dimensional decoupling such that some discontinuities may be represented by only some of the parameters. In probabilistic multiphysics inversion, it is important that the various data types are weighed objectively. Here, the weights are the data covariance matrices of the various data types. We apply empirical estimation of data covariance matrices and employ hierarchical scaling parameters to reduce the dependence on some assumptions required by the empirical approach. Hence, we account for noise variances and covariances, which is crucial for successful probabilistic multiphysics inversion. The parameter estimates and data covariance matrices are obtained with the reversible-jump Markov chain Monte Carlo algorithm with parallel tempering to enhance the efficiency. Since covariance matrix estimation changes data weights, the estimation process is carried out while samples are not recorded for inference. The results at the Athabasca site fit the data and produce plausible data covariance matrices for the data weights. |
| 2025-10-17 | [Integrating Conductor Health into Dynamic Line Rating and Unit Commitment under Uncertainty](http://arxiv.org/abs/2510.15740v1) | Geon Roh, Jip Kim | Dynamic line rating (DLR) enables greater utilization of existing transmission lines by leveraging real-time weather data. However, the elevated temperature operation (ETO) of conductors under DLR is often overlooked, despite its long-term impact on conductor health. This paper addresses this issue by 1) quantifying depreciation costs associated with ETO and 2) proposing a Conductor Health-Aware Unit Commitment (CHA-UC) that internalizes these costs in operational decisions. The CHA-UC incorporates a robust linear approximation of conductor temperature and integration of expected depreciation costs due to hourly ETO into the objective function. Case studies on the Texas 123-bus backbone test system using NOAA weather data demonstrate that the proposed CHA-UC model reduces the total cost by 0.8% and renewable curtailment by 84%compared to static line rating (SLR), while conventional DLR operation without risk consideration resulted in higher costs due to excessive ETO. Further analysis of the commitment decisions and the line temperature statistics confirms that the CHA-UC achieves safer line flows by shifting generator commitments. Finally, we examine the emergent correlation between wind generation and DLR forecast errors, and show that CHA-UC adaptively manages this effect by relaxing flows for risk-hedging conditions while tightening flows for risk-amplifying ones. |
| 2025-10-17 | [Robust Insurance Pricing and Liquidity Management](http://arxiv.org/abs/2510.15709v1) | Shunzhi Pang | With the rise of emerging risks, model uncertainty poses a fundamental challenge in the insurance industry, making robust pricing a first-order question. This paper investigates how insurers' robustness preferences shape competitive equilibrium in a dynamic insurance market. Insurers optimize their underwriting and liquidity management strategies to maximize shareholder value, leading to equilibrium outcomes that can be analytically derived and numerically solved. Compared to a benchmark without model uncertainty, robust insurance pricing results in significantly higher premiums and equity valuations. Notably, our model yields three novel insights: (1) The minimum, maximum, and admissible range of aggregate capacity all expand, indicating that insurers' liquidity management becomes more conservative. (2) The expected length of the underwriting cycle increases substantially, far exceeding the range commonly reported in earlier empirical studies. (3) While the capacity process remains ergodic in the long run, the stationary density becomes more concentrated in low-capacity states, implying that liquidity-constrained insurers require longer to recover. Together, these findings provide a potential explanation for recent skepticism regarding the empirical evidence of underwriting cycles, suggesting that such cycles may indeed exist but are considerably longer than previously assumed. |
| 2025-10-17 | [Two-Stage Data-Driven Contextual Robust Optimization: An End-to-End Learning Approach for Online Energy Applications](http://arxiv.org/abs/2510.15696v1) | Carlos Gamboa, Alexandre Street et al. | Traditional end-to-end contextual robust optimization models are trained for specific contextual data, requiring complete retraining whenever new contextual information arrives. This limitation hampers their use in online decision-making problems such as energy scheduling, where multiperiod optimization must be solved every few minutes. In this paper, we propose a novel Data-Driven Contextual Uncertainty Set, which gives rise to a new End-to-End Data-Driven Contextual Robust Optimization model. For right-hand-side uncertainty, we introduce a reformulation scheme that enables the development of a variant of the Column-and-Constraint Generation (CCG) algorithm. This new CCG method explicitly considers the contextual vector within the cut expressions, allowing previously generated cuts to remain valid for new contexts, thereby significantly accelerating convergence in online applications. Numerical experiments on energy and reserve scheduling problems, based on classical test cases and large-scale networks (with more than 10,000 nodes), demonstrate that the proposed method reduces computation times and operational costs while capturing context-dependent risk structures. The proposed framework (model and method), therefore, offers a unified, scalable, and prescriptive approach to robust decision-making under uncertainty, effectively bridging data-driven learning and optimization. |
| 2025-10-16 | [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](http://arxiv.org/abs/2510.14959v1) | Lizhi Yang, Blake Werner et al. | Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter. |
| 2025-10-16 | [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](http://arxiv.org/abs/2510.14926v1) | Khalak Mahadeviya, Saulo V. Moreira et al. | We investigate current fluctuations in open quantum systems beyond the weak-coupling and Markovian regimes, focusing on a coherently driven qubit strongly coupled to a structured bosonic environment. By combining full counting statistics with the reaction coordinate mapping, we develop a framework that enables the calculation of steady-state current fluctuations and their temporal correlations in the strong-coupling regime. Our analysis reveals that, unlike in weak coupling, both the average current and its fluctuations exhibit nonmonotonic dependence on the system-environment interaction strength. Notably, we identify a regime where current noise is suppressed below the classical thermodynamic uncertainty bound, coinciding with enhanced anticorrelations in quantum jump trajectories and faster system relaxation. We further show that these features are linked to nonclassical properties of the reaction coordinate mode, such as non-Gaussianity and quantum coherence. Our results provide new insights and design principles for controlling current fluctuations in quantum devices operating beyond the standard weak-coupling paradigm. |
| 2025-10-16 | [Astrophysical uncertainties challenge 21-cm forecasts: A primordial black hole case study](http://arxiv.org/abs/2510.14877v1) | Dominic Agius, Rouven Essig et al. | The 21-cm signal is a powerful probe of the early Universe's thermal history and could provide a unique avenue for constraining exotic physics. Previous studies have forecasted stringent constraints on energy injections from exotic sources that heat, excite, and ionize the background gas and thereby modify the 21-cm signal. In this work, we quantify the substantial impact that astrophysical uncertainties have on the projected sensitivity to exotic energy injection. In particular, there are significant uncertainties in the minimum star-forming dark matter halo mass, the Lyman-$\alpha$ emission, and the X-ray emission, whose values characterize the fiducial astrophysical model when projecting bounds. As a case study, we investigate the energy injection of accreting primordial black holes of mass $\sim 1~M_\odot-10^3~M_\odot$, also taking into account uncertainties in the accretion model. We show that, depending on the chosen fiducial model and accretion uncertainties, the sensitivity of future 21-cm data could constrain the abundance of primordial black holes to be either slightly stronger, or significantly weaker, than current limits from the Cosmic Microwave Background. |
| 2025-10-16 | [A new photometric ephemeris for the 2M1510 AB double brown dwarf eclipsing binary system](http://arxiv.org/abs/2510.14801v1) | Seb T. Millward, Vedad Kunovac | Eclipsing brown dwarfs are important calibrators of sub-stellar evolution models used to infer the characteristics of directly imaged brown dwarfs and giant exoplanets. Only two double brown dwarf eclipsing binary systems are known, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a poorly constrained orbital period. Here we analyse TESS full-frame image (FFI) photometry of this faint ($T=15.9$) binary and detect a significant (${>}10\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with previous data. We refine the orbital period to $20.897782 \pm 0.000036$ d, reducing its present-day uncertainty from 18 h to 8 min. Our work is crucial for scheduling follow-up observations of this system for detailed study with other photometric facilities. We also find that a recent orbital solution from Doppler data is inconsistent with existing photometry. A timing offset in the Doppler data may have produced a spurious signal mimicking retrograde apsidal precession, from which the claimed circumbinary planet 2M1510 ABb was inferred. From our best attempt at correcting the data we were unable to reconcile the radial velocity data with the photometry, suggesting that the radial velocity uncertainties are underestimated, and that the circumbinary planet 2M1510 ABb may be a false positive. |
| 2025-10-16 | [Measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays with the LHCb Upgrade I detector](http://arxiv.org/abs/2510.14732v1) | LHCb collaboration, R. Aaij et al. | A measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays is reported, based on a data sample of proton-proton collisions collected with the LHCb Upgrade I detector in 2024 at a centre-of-mass energy of $13.6\,$TeV, corresponding to an integrated luminosity of $6.2\,\mathrm{fb}^{-1}$. The $D^0 \to K^0_{\rm S} \pi^+ \pi^-$ decay is used as calibration channel to cancel residual detection and production asymmetries. The time-integrated $C\!P$ asymmetry for the $D^0 \to K^0_{\rm S} K^0_{\rm S}$ mode is measured to be $$ {\cal A}^{C\!P} (D^0 \to K^0_{\rm S} K^0_{\rm S}) = (1.86 \pm 1.04\pm 0.41)\%, $$ where the first uncertainty is statistical, and the second is systematic. This is the most precise determination of this quantity to date. |
| 2025-10-16 | [Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock](http://arxiv.org/abs/2510.14715v1) | Guobin Mou | Recent long-term radio monitoring of tidal disruption events (TDEs) suggests that radio afterglows are common. Most studies argue that these afterglows may arise from forward shocks (FS) produced by the interaction between the TDE outflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical models do not model the evolution of relativistic electrons in space, which introduces uncertainties. Here we conducted hydrodynamic simulations to study the hydrodynamic evolution of relativistic electrons, and calculated the synchrotron spectra via radiative transfer. We focus on the FS scenario with non-relativistic outflows, and various parameters of the outflow and CNM are explored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a Galactic center - like CNM can produce mJy-level radio afterglows at a distance of 100 Mpc. The self-absorption frequency exhibits a slow decline at early times and a rapid decrease at late times. We derived the temporal evolution of the high-frequency radio flux, revealing its characteristic rise and decline pattern. We also find that: (1) the radio spectra for narrow outflows are clearly anisotropic along different sight lines; (2) the FS parameters inferred from radio spectra using conventional analytical formulas deviate significantly from those in simulations, in which the inferred shock radii are half of those from simulations, and the inferred energies are an order of magnitude lower. |
| 2025-10-16 | [Fast and Scalable Score-Based Kernel Calibration Tests](http://arxiv.org/abs/2510.14711v1) | Pierre Glaser, David Widmann et al. | We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a non-parametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a conditional goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the properties of our test on various synthetic settings. |
| 2025-10-16 | [Practical, Utilitarian Algorithm Configuration](http://arxiv.org/abs/2510.14683v1) | Devon Graham, Kevin Leyton-Brown | Utilitarian algorithm configuration identifies a parameter setting for a given algorithm that maximizes a user's utility. Utility functions offer a theoretically well-grounded approach to optimizing decision-making under uncertainty and are flexible enough to capture a user's preferences over algorithm runtimes (e.g., they can describe a sharp cutoff after which a solution is no longer required, a per-hour cost for compute, or diminishing returns from algorithms that take longer to run). COUP is a recently-introduced utilitarian algorithm configuration procedure which was designed mainly to offer strong theoretical guarantees about the quality of the configuration it returns, with less attention paid to its practical performance. This paper closes that gap, bringing theoretically-grounded, utilitarian algorithm configuration to the point where it is competitive with widely used, heuristic configuration procedures that offer no performance guarantees. We present a series of improvements to COUP that improve its empirical performance without degrading its theoretical guarantees and demonstrate their benefit experimentally. Using a case study, we also illustrate ways of exploring the robustness of a given solution to the algorithm selection problem to variations in the utility function. |
| 2025-10-16 | [Precision of an autonomous demon exploiting nonthermal resources and information](http://arxiv.org/abs/2510.14578v1) | Juliette Monsel, Matteo Acciai et al. | Quantum-dot systems serve as nanoscale heat engines exploiting thermal fluctuations to perform a useful task. Here, we investigate a multi-terminal triple-dot system, operating as a refrigerator that extracts heat from a cold electronic contact. In contrast to standard heat engines, this system exploits a nonthermal resource. This has the intriguing consequence that cooling can occur without extracting energy from the resource on average -- a seemingly demonic action -- while, however, requiring the resource to fluctuate. Using full counting statistics and stochastic trajectories, we analyze the performance of the device in terms of the cooling-power precision, employing performance quantifiers motivated by the thermodynamic and kinetic uncertainty relations. We focus on two regimes with large output power, which are based on two operational principles: exploiting information on one hand and the nonthermal properties of the resource on the other. We show that these regimes significantly differ in precision. In particular, the regime exploiting the nonthermal properties of the resource can have cooling-power fluctuations that are suppressed with respect to the input fluctuations by an order of magnitude. We also substantiate the interpretation of the two different working principles by analyzing cross-correlations between input and output heat currents and information flow. |
| 2025-10-16 | [Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory](http://arxiv.org/abs/2510.14555v1) | Amal Sakr, Andrea Araldo et al. | The introduction of new services, such as Mobile Edge Computing (MEC), requires a massive investment that cannot be assumed by a single stakeholder, for instance the Infrastructure Provider (InP). Service Providers (SPs) however also have an interest in the deployment of such services. We hence propose a co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form the so-called grand coalition composed of all the stakeholders with the aim of sharing costs and revenues and maximizing their payoffs. The challenge comes from the fact that future revenues are uncertain. We devise in this case a novel stochastic coalitional game formulation which builds upon robust game theory and derive a lower bound on the probability of the stability of the grand coalition, wherein no player can be better off outside of it. In the presence of some correlated fluctuations of revenues however, stability can be too conservative. In this case, we make use also of profitability, in which payoffs of players are non-negative, as a necessary condition for co-investment. The proposed framework is showcased for MEC deployment, where computational resources need to be deployed in nodes at the edge of a telecommunication network. Numerical results show high lower bound on the probability of stability when the SPs' revenues are of similar magnitude and the investment period is sufficiently long, even with high levels of uncertainty. In the case where revenues are highly variable however, the lower bound on stability can be trivially low whereas co-investment is still profitable. |
| 2025-10-15 | [$\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF](http://arxiv.org/abs/2510.13805v1) | David Gebauer, Anik Halder et al. | We present $\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework for analysing a higher-order weak lensing statistic, the integrated 3-point correlation function (i3PCF). Our approach forward-models the cosmic shear field using the $\texttt{CosmoGridV1}$ suite of N-body simulations, including a comprehensive set of systematic effects such as intrinsic alignment, baryonic feedback, photometric redshift uncertainty, shear calibration bias, and shape noise. Using this, we have produced a set of DES Y3-like synthetic measurements for 2-point shear correlation functions $\xi_{\pm}$ (2PCFs) and i3PCFs $\zeta_{\pm}$ across 6 cosmological and 11 systematic parameters. Having validated these measurements against theoretical predictions and thoroughly examined for potential systematic biases, we have found that the impact of source galaxy clustering and reduced shear on the i3PCF is negligible for Stage-III surveys. Furthermore, we have tested the Gaussianity assumption for the likelihood of our data vector and found that while the sampling distribution of the 2PCF can be well approximated by a Gaussian function, the likelihood of the combined 2PCF + i3PCF data vector including filter sizes of $90'$ and larger can deviate from this assumption. Our SBI pipeline employs masked autoregressive flows to perform neural likelihood estimation and is validated to give statistically accurate posterior estimates. On mock data, we find that including the i3PCF yields a substantial $63.8\%$ median improvement in the figure of merit for $\Omega_m - \sigma_8 - w_0$. These findings are consistent with previous works on the i3PCF and demonstrate that our SBI framework can achieve the accuracy and realism needed to analyse the i3PCF in wide-area weak lensing surveys. |
| 2025-10-15 | [Splitting Isotope Shift in the $1s2p\,^3\!P_{0,1,2}$ Fine-Structure Triplet in $^{12,13,14}$C$^{4+}$: Experiment and Theory](http://arxiv.org/abs/2510.13779v1) | Patrick MÃ¼ller, Kristian KÃ¶nig et al. | We report measurements and theoretical calculations of the fine-structure splittings in all three $1s2s\,^3\!S_1\rightarrow\,1s2p\,^3\!P_{0,1,2}$ transitions in the heliumlike systems of the isotopes $^{12,13,14}$C. The metastable triplet state was efficiently populated in an electron beam ion source and the C$^{4+}$ ions were electrostatically accelerated to 50\,keV to perform collinear laser spectroscopy. From the determined transition frequencies, the splitting isotope shift (SIS), i.e., the difference in fine-structure splittings between different isotopes of the same element, was extracted. In the SIS, theoretical uncertainties due to higher-order quantum electrodynamic corrections are strongly suppressed since they are independent of both nuclear mass and the fine-structure quantum number $J$ in lowest order. Comparison with theory provides an important test of experimental accuracy, particularly in the $^{13}$C$^{4+}$ case, for which the nuclear spin leads to hyperfine-induced fine-structure mixing. At the same time, the even-even isotopes $^{12,14}$C$^{4+}$ without nuclear spin can be used to confirm theory. Theoretical values of the SIS are given for all the heliumlike ions with $2\le Z\le 10$. |
| 2025-10-15 | [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](http://arxiv.org/abs/2510.13750v1) | Zhiqi Huang, Vivek Datla et al. | We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment. |
| 2025-10-15 | [VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions](http://arxiv.org/abs/2510.13705v1) | Fan Chang, Yijia Fang | In this paper, we uncover a new uncertainty principle that governs the complexity of Boolean functions. This principle manifests as a fundamental trade-off between two central measures of complexity: a combinatorial complexity of its supported set, captured by its Vapnik-Chervonenkis dimension ($\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial degree over various fields. We establish two primary inequalities that formalize this trade-off:$\mathrm{VC}(f)+\deg(f)\ge n,$ and $\mathrm{VC}(f)+\deg_{\mathbb{F}_2}(f)\ge n$. In particular, these results recover the classical uncertainty principle on the discrete hypercube, as well as the Sziklai--Weiner's bound in the case of $\mathbb{F}_2$. |
| 2025-10-15 | [Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping](http://arxiv.org/abs/2510.13672v1) | MarcÃ­lio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo | Dengue remains one of Brazil's major epidemiological challenges, marked by strong intra-urban inequalities and the influence of climatic and socio-environmental factors. This study analyzed confirmed dengue cases in Recife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model implemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal component. Covariates included population density, household size, income, drainage channels, lagged precipitation, and mean temperature. Population density and household size had positive effects on dengue risk, while income and channel presence were protective. Lagged precipitation increased risk, and higher temperatures showed an inverse association, suggesting thermal thresholds for vector activity. The model achieved good fit (DIC=65817; WAIC=64506) and stable convergence, with moderate residual spatial autocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019. Spatio-temporal estimates revealed persistent high-risk clusters in northern and western Recife, overlapping with areas of higher density and social vulnerability. Beyond reproducing historical patterns, the Bayesian model supports probabilistic forecasting and early warning systems. Compared with classical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty and spatial-temporal dependence, offering credible interval inference for decision-making in urban health management. |
| 2025-10-15 | [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](http://arxiv.org/abs/2510.13643v1) | Akib Mohammed Khan, Bartosz Krawczyk | Foundation models such as DINOv2 have shown strong performance in few-shot anomaly detection, yet two key questions remain unexamined: (i) how susceptible are these detectors to adversarial perturbations; and (ii) how well do their anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a training-free deep nearest-neighbor detector over DINOv2 features, we present one of the first systematic studies of adversarial attacks and uncertainty estimation in this setting. To enable white-box gradient attacks while preserving test-time behavior, we attach a lightweight linear head to frozen DINOv2 features only for crafting perturbations. Using this heuristic, we evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible perturbations can flip nearest-neighbor relations in feature space to induce confident misclassification. Complementing robustness, we probe reliability and find that raw anomaly scores are poorly calibrated, revealing a gap between confidence and correctness that limits safety-critical use. As a simple, strong baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly scores for uncertainty estimation. The resulting calibrated posteriors yield significantly higher predictive entropy on adversarially perturbed inputs than on clean ones, enabling a practical flagging mechanism for attack detection while reducing calibration error (ECE). Our findings surface concrete vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an evaluation protocol and baseline for robust, uncertainty-aware anomaly detection. We argue that adversarial robustness and principled uncertainty quantification are not optional add-ons but essential capabilities if anomaly detection systems are to be trustworthy and ready for real-world deployment. |
| 2025-10-15 | [Radiative decays of the $Î©(2012)$ as a hadronic molecule](http://arxiv.org/abs/2510.13623v1) | Qing-Hua Shen, Jun-Xu Lu et al. | We present a theoretical investigation of the radiative decay process $\Omega(2012) \to \gamma \Omega$, where the $\Omega(2012)$ resonance with spin-parity $J^P=\frac{3}{2}^-$, is treated as a dynamically generated state from $\bar{K}\Xi(1530)$ and $\eta \Omega$ in $s$-wave and $\bar{K}\Xi$ in $d$-wave. The radiative decay width of the $\Omega(2012)$ is calculated using a triangular loop mechanism, where the $\Omega(2012)$ couples to the $\bar{K} \Xi(1530)$ channel. Subsequently, the final state interactions between $\Xi(1530)$ and $\bar{K}$ transition to a photon and $\Omega$ through the exchange of a $\Xi$ baryon. Our calculations yield a radiative decay width of $13.2 ^{+4.5}_{-3.9}$ KeV, with uncertainties arising from the model parameters. This result provides valuable insights into the nature of the $\Omega(2012)$ resonance and its decay dynamics. It is expected that the calculations presented here could be verified by future experiments, which would open a new door for studying the still elusive nature of the $\Omega(2012)$. |
| 2025-10-15 | [Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity](http://arxiv.org/abs/2510.13609v1) | Ahmad Awad, Erik ScharwÃ¤chter | Soil organic carbon (SOC) sequestration projects require unbiased, precise and cost-effective Monitoring, Reporting, and Verification (MRV) systems that balance sampling costs against uncertainty deductions imposed by regulatory frameworks. Design-based estimators guarantee unbiasedness but cannot exploit auxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can improve precision but require independent validation for each project. Model-assisted estimation offers a robust compromise, combining model predictions with probability sampling to retain design-based guarantees while improving precision. We evaluate the scientific integrity and efficiency of the simple regression estimator (SRE), a well-known model-assisted estimator, via an extensive simulation study. Our simulations span diverse SOC stock variances, sample sizes, and model performances. We assess three core properties: empirical bias, empirical confidence interval coverage, and precision gain relative to the design-based Horvitz-Thompson estimator (HTE). Results show negligible bias and valid coverage probabilities for n > 40, regardless of SOC stock variance. Below this threshold, variance approximations and normality assumptions yield unreliable uncertainty estimates. With correlated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains over HTE. With uncorrelated variables, no gains are observed, but performance converges to HTE for n >= 40. Model-assisted estimation can enhance project economics without compromising scientific rigor. Regulators should permit such estimators while mandating minimum sample size thresholds. Project proponents should routinely employ such estimators when correlated ancillary variables exist. The industry should prioritize the retrieval of high-quality, project-specific covariates to maximize precision gains and thereby the project economics. |
| 2025-10-15 | [Unsupervised Constitutive Model Discovery from Sparse and Noisy Data](http://arxiv.org/abs/2510.13559v1) | Vahab Knauf Narouie, Jorge-Humberto Urrea-Quintero et al. | Recently, unsupervised constitutive model discovery has gained attention through frameworks based on the Virtual Fields Method (VFM), most prominently the EUCLID approach. However, the performance of VFM-based approaches, including EUCLID, is affected by measurement noise and data sparsity, which are unavoidable in practice. The statistical finite element method (statFEM) offers a complementary perspective by providing a Bayesian framework for assimilating noisy and sparse measurements to reconstruct the full-field displacement response, together with quantified uncertainty. While statFEM recovers displacement fields under uncertainty, it does not strictly enforce consistency with constitutive relations or aim to yield interpretable constitutive models. In this work, we couple statFEM with unsupervised constitutive model discovery in the EUCLID framework, yielding statFEM--EUCLID. The framework is demonstrated for isotropic hyperelastic materials. The results show that this integration reduces sensitivity to noise and data sparsity, while ensuring that the reconstructed fields remain consistent with both equilibrium and constitutive laws. |
| 2025-10-15 | [Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage](http://arxiv.org/abs/2510.13514v1) | Andreas C. Makrides, Adam Suski et al. | The transition to a fully decarbonised electricity system depends on integrating new technologies that ensure reliability alongside sustainability. However, missing risk markets hinder investment in reliability-enhancing technologies by exposing investors to revenue uncertainty. This study provides the first quantitative assessment of how missing risk markets affect investment decisions in power systems that depend on long-duration energy storage (LDES) for reliability. We develop a two-stage stochastic equilibrium model with risk-averse market participants, which independently sizes power and energy capacity. We apply the method to a case study of a deeply decarbonised power system in Great Britain. The results show that incomplete risk markets reduce social welfare, harm reliability, and discourage investment in LDES and other technologies with volatile revenue streams. Revenue volatility leads to substantial risk premiums and higher financing costs for LDES, creating a barrier to its large-scale deployment. These findings demonstrate the importance of policy mechanisms that hedge revenue risk to lower the cost of capital and accelerate investment in reliability-enhancing, zero-carbon technologies |
| 2025-10-14 | [What If : Understanding Motion Through Sparse Interactions](http://arxiv.org/abs/2510.12777v1) | Stefan Andreas Baumann, Nick Stracke et al. | Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer. |
| 2025-10-14 | [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](http://arxiv.org/abs/2510.12768v1) | Fengzhi Guo, Chih-Chuan Hsu et al. | Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints. |
| 2025-10-14 | [(Non-Parametric) Bootstrap Robust Optimization for Portfolios and Trading Strategies](http://arxiv.org/abs/2510.12725v1) | Daniel Cunha Oliveira, Grover Guzman et al. | Robust optimization provides a principled framework for decision-making under uncertainty, with broad applications in finance, engineering, and operations research. In portfolio optimization, uncertainty in expected returns and covariances demands methods that mitigate estimation error, parameter instability, and model misspecification. Traditional approaches, including parametric, bootstrap-based, and Bayesian methods, enhance stability by relying on confidence intervals or probabilistic priors but often impose restrictive assumptions. This study introduces a non-parametric bootstrap framework for robust optimization in financial decision-making. By resampling empirical data, the framework constructs flexible, data-driven confidence intervals without assuming specific distributional forms, thus capturing uncertainty in statistical estimates, model parameters, and utility functions. Treating utility as a random variable enables percentile-based optimization, naturally suited for risk-sensitive and worst-case decision-making. The approach aligns with recent advances in robust optimization, reinforcement learning, and risk-aware control, offering a unified perspective on robustness and generalization. Empirically, the framework mitigates overfitting and selection bias in trading strategy optimization and improves generalization in portfolio allocation. Results across portfolio and time-series momentum experiments demonstrate that the proposed method delivers smoother, more stable out-of-sample performance, offering a practical, distribution-free alternative to traditional robust optimization methods. |
| 2025-10-14 | [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](http://arxiv.org/abs/2510.12699v1) | Sunny Yu, Ahmad Jabbar et al. | Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs. |
| 2025-10-14 | [EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels](http://arxiv.org/abs/2510.12687v1) | Kunyu Peng, Di Wen et al. | Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM. |
| 2025-10-14 | [Measurement of the tau anomalous magnetic moment using Ultra-peripheral collisions with the ALICE detector in Run 3 Pb-Pb data](http://arxiv.org/abs/2510.12661v1) | Roman LaviÄka, Paul Alois BÃ¼hler | The anomalous magnetic moment of the tau lepton ($a_{\tau}$) is a sensitive probe for the search for deviations from the Standard Model predictions and thus for new physics. This study investigates the feasibility of measuring $a_{\tau}$ using ultra-peripheral collisions (UPCs) at the LHC, where photon-photon interactions ($\gamma\gamma \to \tau^+ \tau^-$) produce tau lepton pairs. We focus on events recorded by the ALICE detector during Run 3 Pb-Pb data-taking. Events are selected in the decay channel where one tau decays into an electron and neutrinos, and the other decays into a charged pion, or three charged pions, and neutrinos. These samples are enhanced with decays into muons, which are inseparable in the ALICE detector. The clean environment of UPCs minimizes hadronic background, while the advanced particle identification capabilities of the ALICE Time Projection Chamber (TPC) and Time-of-Flight (TOF) systems allow for efficient separation of electrons, pions, and background particles. In this talk, prospects for measuring this process by ALICE in Run 3, which benefits from high statistics and improved systematics uncertainties, will be discussed. Results will provide tighter constraints on $a_{\tau}$, contributing to the broader effort to test the Standard Model's robustness and explore physics beyond it. |
| 2025-10-14 | [Moment-based Posterior Sampling for Multi-reference Alignment](http://arxiv.org/abs/2510.12651v1) | Axel Janson, Joakim AndÃ©n | We propose a Bayesian approach to the problem of multi-reference alignment -- the recovery of signals from noisy, randomly shifted observations. While existing frequentist methods accurately recover the signal at arbitrarily low signal-to-noise ratios, they require a large number of samples to do so. In contrast, our proposed method leverages diffusion models as data-driven plug-and-play priors, conditioning these on the sample power spectrum (a shift-invariant statistic) enabling both accurate posterior sampling and uncertainty quantification. The use of an appropriate prior significantly reduces the required number of samples, as illustrated in simulation experiments with comparisons to state-of-the-art methods such as expectation--maximization and bispectrum inversion. These findings establish our approach as a promising framework for other orbit recovery problems, such as cryogenic electron microscopy (cryo-EM). |
| 2025-10-14 | [Designing Tools with Control Confidence](http://arxiv.org/abs/2510.12630v1) | Ajith Anil Meera, Abian Torres et al. | Prehistoric humans invented stone tools for specialized tasks by not just maximizing the tool's immediate goal-completion accuracy, but also increasing their confidence in the tool for later use under similar settings. This factor contributed to the increased robustness of the tool, i.e., the least performance deviations under environmental uncertainties. However, the current autonomous tool design frameworks solely rely on performance optimization, without considering the agent's confidence in tool use for repeated use. Here, we take a step towards filling this gap by i) defining an optimization framework for task-conditioned autonomous hand tool design for robots, where ii) we introduce a neuro-inspired control confidence term into the optimization routine that helps the agent to design tools with higher robustness. Through rigorous simulations using a robotic arm, we show that tools designed with control confidence as the objective function are more robust to environmental uncertainties during tool use than a pure accuracy-driven objective. We further show that adding control confidence to the objective function for tool design provides a balance between the robustness and goal accuracy of the designed tools under control perturbations. Finally, we show that our CMAES-based evolutionary optimization strategy for autonomous tool design outperforms other state-of-the-art optimizers by designing the optimal tool within the fewest iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence. |
| 2025-10-14 | [Learning-To-Measure: In-context Active Feature Acquisition](http://arxiv.org/abs/2510.12624v1) | Yuta Kobayashi, Zilin Jing et al. | Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness. |
| 2025-10-14 | [Enhancing Robust Multi-Market Participation of Renewable-Based VPPs through Flexible Resources](http://arxiv.org/abs/2510.12589v1) | Hadi Nemati, Ãlvaro Ortega et al. | In the transition toward a sustainable power system, renewable-based Virtual Power Plants (RVPPs) have emerged as a promising solution to the challenges of integrating renewable energy sources into electricity markets. Their viability, however, depends on effective market participation strategies and the ability to manage uncertainties while leveraging flexible resources. This paper analyzes the impact of different flexible resources - such as concentrated solar power plants, hydro plants, biomass plants, and flexible demand - on the participation of RVPPs in energy and reserve markets. Multiple sources of uncertainty in generation, consumption, and electricity prices are addressed using a two-stage robust optimization approach. The contribution of different technologies to RVPP profitability is evaluated through a marginal contribution method, ensuring fair allocation of profits among them according to their actual role in energy and reserve provision across markets. Simulations for an RVPP in southern Spain demonstrate how strategic decisions and the availability of flexible resources influence viability, market participation, and unit scheduling. |
| 2025-10-13 | [BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe](http://arxiv.org/abs/2510.11719v1) | M. Grayling, S. Thorp et al. | We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia supernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged, gravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for magnifications and time-delays across multiple images while marginalising over an achromatic, Gaussian process-based treatment of microlensing, to allow for time-dependent deviations from a typical SN Ia SED caused by gravitational lensing by stars in the lensing system. BayeSN-TD is able to robustly infer time delays and produce well-calibrated uncertainties, even when applied to simulations based on a different SED model and incorporating chromatic microlensing, strongly validating its suitability for time-delay cosmography. We then apply BayeSN-TD to publicly available photometry of the glSN Ia SN H0pe, inferring time delays between images BA and BC of $\Delta T_{BA}=121.9^{+9.5}_{-7.5}$ days and $\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days along with absolute magnifications $\beta$ for each image, $\beta_A = 2.38^{+0.72}_{-0.54}$, $\beta_B=5.27^{+1.25}_{-1.02}$ and $\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and magnifications with existing lens models of this system, we infer $H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous analysis of this system; incorporating additional constraints based on spectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While this is not yet precise enough to draw a meaningful conclusion with regard to the `Hubble tension', upcoming analysis of SN H0pe with more accurate photometry enabled by template images, and other glSNe, will provide stronger constraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses. |
| 2025-10-13 | [Bayesian Topological Convolutional Neural Nets](http://arxiv.org/abs/2510.11704v1) | Sarah Harkins Dayton, Hayden Everett et al. | Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification. |
| 2025-10-13 | [Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation](http://arxiv.org/abs/2510.11689v1) | Maggie Wang, Stephen Tian et al. | Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ . |
| 2025-10-13 | [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](http://arxiv.org/abs/2510.11654v1) | Daniel Berhane Araya, Duoduo Liao | Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches. |
| 2025-10-13 | [StatTestCalculator: A New General Tool for Statistical Analysis in High Energy Physics](http://arxiv.org/abs/2510.11637v1) | Emil Abasov, Lev Dudko et al. | We present StatTestCalculator (STC), a new open-source statistical analysis tool designed for analysis high energy physics experiments. STC provides both asymptotic calculations and Monte Carlo simulations for computing the exact statistical significance of a discovery or for setting upper limits on signal model parameters. We review the underlying statistical formalism, including profile likelihood ratio test statistics for discovery and exclusion hypotheses, and the asymptotic distributions that allow quick significance estimates. We explain the relevant formulas for the likelihood functions, test statistic distributions, and significance metrics (both with and without incorporating systematic uncertainties). The implementation and capabilities of STC are described, and we validate its performance against the widely-used CMS Combine tool. We find excellent agreement in both the expected discovery significances and upper limit calculations. STC is a flexible framework that can accommodate systematic uncertainties and user-defined statistical models, making it suitable for a broad range of analyses. |
| 2025-10-13 | [The Fractional Two-Sided Quaternionic Dunkl Transform and Heisenberg-Type Inequalities](http://arxiv.org/abs/2510.11597v1) | Mohamed Essenhajy | This report investigates the main definitions and fundamental properties of the fractional two-sided quaternionic Dunkl transform in two dimensions. We present key results concerning its structure and emphasize its connections to classical harmonic analysis. Special attention is given to inversion, boundedness, spectral behavior, and explicit formulas for structured functions such as radial or harmonic functions. Within this framework, we establish a generalized form of the classical Heisenberg-type uncertainty principle. Building on this foundation, we further extend the result by proving a higher-order Heisenberg-type inequality valid for arbitrary moments $p \geq 1$, with sharp constants characterized through generalized Hermite functions. Finally, by analyzing the interplay between the two-sided fractional quaternionic Dunkl transform and the two-sided fractional quaternionic Fourier transform, we derive a corresponding uncertainty principle for the latter. |
| 2025-10-13 | [Reproducibility: The New Frontier in AI Governance](http://arxiv.org/abs/2510.11595v1) | Israel Mason-Williams, Gabryel Mason-Williams | AI policymakers are responsible for delivering effective governance mechanisms that can provide safe, aligned and trustworthy AI development. However, the information environment offered to policymakers is characterised by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and creating deep uncertainty and divides on which risks should be prioritised from a governance perspective. We posit that the current publication speeds in AI combined with the lack of strong scientific standards, via weak reproducibility protocols, effectively erodes the power of policymakers to enact meaningful policy and governance protocols. Our paper outlines how AI research could adopt stricter reproducibility guidelines to assist governance endeavours and improve consensus on the AI risk landscape. We evaluate the forthcoming reproducibility crisis within AI research through the lens of crises in other scientific domains; providing a commentary on how adopting preregistration, increased statistical power and negative result publication reproducibility protocols can enable effective AI governance. While we maintain that AI governance must be reactive due to AI's significant societal implications we argue that policymakers and governments must consider reproducibility protocols as a core tool in the governance arsenal and demand higher standards for AI research. Code to replicate data and figures: https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance |
| 2025-10-13 | [Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization](http://arxiv.org/abs/2510.11539v1) | Denglin Cheng, Jiarong Kang et al. | Accurate state estimation is critical for legged and aerial robots operating in dynamic, uncertain environments. A key challenge lies in specifying process and measurement noise covariances, which are typically unknown or manually tuned. In this work, we introduce a bi-level optimization framework that jointly calibrates covariance matrices and kinematic parameters in an estimator-in-the-loop manner. The upper level treats noise covariances and model parameters as optimization variables, while the lower level executes a full-information estimator. Differentiating through the estimator allows direct optimization of trajectory-level objectives, resulting in accurate and consistent state estimates. We validate our approach on quadrupedal and humanoid robots, demonstrating significantly improved estimation accuracy and uncertainty calibration compared to hand-tuned baselines. Our method unifies state estimation, sensor, and kinematics calibration into a principled, data-driven framework applicable across diverse robotic platforms. |
| 2025-10-13 | [Characterizing planetary systems with SPIRou: questions about the magnetic cycle of 55 Cnc A and two new planets around B](http://arxiv.org/abs/2510.11523v1) | C. Moutou, P. Petit et al. | One of the first exoplanet hosts discovered thirty years ago, the star 55 Cnc has been constantly observed ever since. It is now known to host at least five planets with orbital periods ranging from 17 hours to 15 years. It is also one of the most extreme metal rich stars in the neighbourhood and it has a low-mass secondary star. In this article, we present data obtained at the Canada-France-Hawai'i Telescope with the SPIRou spectropolarimeter on both components of the 55 Cnc stellar system. We revisit the long-period radial-velocity signals of 55 Cnc A, with a focus on the role of the magnetic cycle, and propose the existence of a sixth planet candidate, whose period falls close to that of the magnetic cycle, or half of it. The other massive outer planet has a revised period of 13.15 years and a minimum mass of 3.8 MJup. Although some uncertainty remains on these outer planets, the characterization of the four inner planets is very robust through the combination of many different data sets, and all signals are consistent in the nIR and optical domains. In addition, the magnetic topology of the solar-type primary component of the system is observed by SPIRou at the minimum of its activity cycle, characterized by an amplitude ten times smaller than observed during its maximum in 2017. For the low-mass component 55 Cnc B, we report the discovery of two exoplanets in the system, with a period of 6.799+-0.0014 and 33.75+-0.04 days and a minimum mass of 3.5+-0.8 and 5.3+-1.4 MEarth, respectively. The secondary magnetic field is very weak and the current data set does not allow its precise characterization, setting an upper limit of 10 G. The system 55 Cnc stands out as the sixth binary system with planetary systems around both components, and the first one with non equal-mass stellar components. |
| 2025-10-13 | [Uncertainty Quantification for Retrieval-Augmented Reasoning](http://arxiv.org/abs/2510.11483v1) | Heydar Soudani, Hamed Zamani et al. | Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods. |
| 2025-10-10 | [Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference](http://arxiv.org/abs/2510.09574v1) | Daria de tinguy, Tim Verbelen et al. | Autonomous navigation in unfamiliar environments requires robots to simultaneously explore, localise, and plan under uncertainty, without relying on predefined maps or extensive training. We present a biologically inspired, Active Inference-based framework, Active Inference MAPping and Planning (AIMAPP). This model unifies mapping, localisation, and decision-making within a single generative model. Inspired by hippocampal navigation, it uses topological reasoning, place-cell encoding, and episodic memory to guide behaviour. The agent builds and updates a sparse topological map online, learns state transitions dynamically, and plans actions by minimising Expected Free Energy. This allows it to balance goal-directed and exploratory behaviours. We implemented a ROS-compatible navigation system that is sensor and robot-agnostic, capable of integrating with diverse hardware configurations. It operates in a fully self-supervised manner, is resilient to drift, and supports both exploration and goal-directed navigation without any pre-training. We demonstrate robust performance in large-scale real and simulated environments against state-of-the-art planning models, highlighting the system's adaptability to ambiguous observations, environmental changes, and sensor noise. The model offers a biologically inspired, modular solution to scalable, self-supervised navigation in unstructured settings. AIMAPP is available at https://github.com/decide-ugent/AIMAPP. |
| 2025-10-10 | [Yukawa interactions in Quantum Gravity](http://arxiv.org/abs/2510.09572v1) | Gustavo P. de Brito, Manuel Reichert et al. | We present the first complete next-to-leading-order analysis of a Yukawa system within the framework of asymptotically safe quantum gravity. Our results are obtained through a systematic resummation of higher-order operators, revealing two distinct resummation mechanisms -- one of which has not been explored previously. In addition, we introduce a novel approach to estimate systematic uncertainties by simulating the impact of neglected higher-order contributions.   We demonstrate that quantum gravity fluctuations anti-screen Yukawa interactions, thereby resolving previously inconclusive leading-order results. This anti-screening mechanism enables the generation of finite interactions from an asymptotically free Yukawa fixed point. Consequently, our findings provide strong evidence that non-vanishing Yukawa couplings are compatible with asymptotically safe quantum gravity, which is a necessary requirement for the Standard Model to emerge from an asymptotically safe ultraviolet completion. |
| 2025-10-10 | [Low-redshift constraints on structure growth from CMB lensing tomography](http://arxiv.org/abs/2510.09563v1) | Andrea Rubiola, Matteo Zennaro et al. | We present constraints on the amplitude of matter fluctuations from the clustering of galaxies and their cross-correlation with the gravitational lensing convergence of the cosmic microwave background (CMB), focusing on low redshifts ($z\lesssim0.3$), where potential deviations from a perfect cosmological constant dominating the growth of structure could be more prominent. Specifically, we make use of data from the 2MASS photometric survey (\tmpz) and the \wisc galaxy survey, in combination with CMB lensing data from \planck. Using a hybrid effective field theory (HEFT) approach to model galaxy bias we obtain constraints on the combination $S_8=\sigma_8\sqrt{\Omega_m/0.3}$, where $\sigma_8$ is the amplitude of matter fluctuations, and $\Omega_m$ is the non-relativistic matter fraction. Using a prior on $\Omega_m$ based on the baryon acoustic oscillation measurements of DESI, we find $S_8=0.79\pm0.06$, in reasonable agreement with CMB constraints. We also find that, in the absence of this prior, the data favours a value of $\Omega_m=0.245\pm0.024$, that is 2.8$\sigma$ lower than \planck. This result is driven by the broadband shape of the galaxy auto-correlation, and may be affected by theoretical uncertainties in the HEFT power spectrum templates. We further reconstruct the low-redshift growth history, finding it to be compatible with the \planck predictions, as well as existing constraints from lensing tomography. Finally, we study our constraints on the HEFT bias parameters of the galaxy samples studied, finding them to be in reasonable agreement with coevolution predictions. |
| 2025-10-10 | [A LENS on DUNE-PRISM: Characterizing a Neutrino Beam with Off-Axis Measurements](http://arxiv.org/abs/2510.09546v1) | Julia Gehrlein, Joachim Kopp et al. | Upcoming precision long-baseline neutrino oscillation experiments will be severely limited by the large systematic uncertainties associated with neutrino flux predictions and neutrino--nucleus cross sections. A promising remedy is the PRISM (Precision Reaction Independent Spectrum Measurement) technique, whereby the near detector measures the neutrino spectrum at different angles with respect to the beam axis. These measurements are then linearly combined into a prediction of the oscillated neutrino flux at the far detector. This prediction is data-driven, but still dependent on some theoretical knowledge about the neutrino flux. In this paper, we study to what extent off-axis measurements themselves can be used to directly constrain neutrino flux models. In particular, we use them to extract separately the fluxes and spectra of different meson species in the beam. We call this measurement LENS (Lateral Extraction of Neutrino Spectra). Second, we demonstrate how the thus improved flux model helps to further constrain the far detector flux prediction, thereby ultimately improving oscillation measurements. |
| 2025-10-10 | [Hierarchical Progressive Survey (HiPS) format: moving from visualisation to scientific analysis](http://arxiv.org/abs/2510.09533v1) | Fabrizio Giordano, Yago Ascasibar et al. | Context. In the current era of multi-wavelength and multi-messenger astronomy, international organisations are actively working on the definition of new standards for the publication of astronomical data, and substantial effort is devoted to make them available through public archives. Aims. We present a set of tools that allow user-friendly access and basic scientific analysis of observations in Hierarchical Progressive Survey (HiPS) format, and we use them to gauge the quality of representative skymaps at ultraviolet, optical, and infrared wavelengths. Methods. We apply a fully-automatic procedure to derive aperture photometry in 10 different bands for the 323 nearby galaxies in the Herschel Reference Sample (HRS), and compare its results with the rigorous analyses involving specialised knowledge and human intervention carried out by the HRS team. Results. Our experiment shows that 9 of the 10 skymaps considered preserve the original quality of the data, and the photometric fluxes returned by our pipeline are consistent with the HRS measurements within a few per cent. In the case of Herschel PACS maps at 100 {\mu}m, we uncovered a systematic error that we ascribe to an inconsistent combination of data products with different spatial resolution. For the remaining skymaps, the estimated statistical uncertainties provide a realistic indication of the differences with respect to the HRS catalogue. Conclusions. In principle, the currently available HiPS skymaps in Flexible Image Transport System (FITS) format allow to carry out broadband photometric analyses with an accuracy of the order of a few percent, but some level human intervention is still required. In addition to assessing data quality, we also propose a series of recommendations to realise the full potential of the HiPS format for the scientific analysis of large astronomical data sets. |
| 2025-10-10 | [Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study](http://arxiv.org/abs/2510.09498v1) | Rogier P. Krijnen, Akshay Joshi et al. | Fully capturing this behavior in traditional homogenized tissue testing requires the excitation of multiple deformation modes, i.e. combined triaxial shear tests and biaxial stretch tests. Inherently, such multimodal experimental protocols necessitate multiple tissue samples and extensive sample manipulations. Intrinsic inter-sample variability and manipulation-induced tissue damage might have an adverse effect on the inversely identified tissue behavior. In this work, we aim to overcome this gap by focusing our attention to the use of heterogeneous deformation profiles in a parameter estimation problem. More specifically, we adapt EUCLID, an unsupervised method for the automated discovery of constitutive models, towards the purpose of parameter identification for highly nonlinear, orthotropic constitutive models using a Bayesian inference approach and three-dimensional continuum elements. We showcase its strength to quantitatively infer, with varying noise levels, the material model parameters of synthetic myocardial tissue slabs from a single heterogeneous biaxial stretch test. This method shows good agreement with the ground-truth simulations and with corresponding credibility intervals. Our work highlights the potential for characterizing highly nonlinear and orthotropic material models from a single biaxial stretch test with uncertainty quantification. |
| 2025-10-10 | [FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents](http://arxiv.org/abs/2510.09483v1) | Lars Ohnemus, Nils Hantke et al. | Dynamic Scene Graphs (DSGs) provide a structured representation of hierarchical, interconnected environments, but current approaches struggle to capture stochastic dynamics, partial observability, and multi-agent activity. These aspects are critical for embodied AI, where agents must act under uncertainty and delayed perception. We introduce FOGMACHINE , an open-source framework that fuses DSGs with discrete-event simulation to model object dynamics, agent observations, and interactions at scale. This setup enables the study of uncertainty propagation, planning under limited perception, and emergent multi-agent behavior. Experiments in urban scenarios illustrate realistic temporal and spatial patterns while revealing the challenges of belief estimation under sparse observations. By combining structured representations with efficient simulation, FOGMACHINE establishes an effective tool for benchmarking, model training, and advancing embodied AI in complex, uncertain environments. |
| 2025-10-10 | [Failure Prediction at Runtime for Generative Robot Policies](http://arxiv.org/abs/2510.09459v1) | Ralf RÃ¶mer, Adrian Kobras et al. | Imitation learning (IL) with generative models, such as diffusion and flow matching, has enabled robots to perform complex, long-horizon tasks. However, distribution shifts from unseen environments or compounding action errors can still cause unpredictable and unsafe behavior, leading to task failure. Early failure prediction during runtime is therefore essential for deploying robots in human-centered and safety-critical environments. We propose FIPER, a general framework for Failure Prediction at Runtime for generative IL policies that does not require failure data. FIPER identifies two key indicators of impending failure: (i) out-of-distribution (OOD) observations detected via random network distillation in the policy's embedding space, and (ii) high uncertainty in generated actions measured by a novel action-chunk entropy score. Both failure prediction scores are calibrated using a small set of successful rollouts via conformal prediction. A failure alarm is triggered when both indicators, aggregated over short time windows, exceed their thresholds. We evaluate FIPER across five simulation and real-world environments involving diverse failure modes. Our results demonstrate that FIPER better distinguishes actual failures from benign OOD situations and predicts failures more accurately and earlier than existing methods. We thus consider this work an important step towards more interpretable and safer generative robot policies. Code, data and videos are available at https://tum-lsy.github.io/fiper_website. |
| 2025-10-10 | [Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior](http://arxiv.org/abs/2510.09401v1) | Matthew R. Williams, F. Hunter McGuire et al. | Parameter estimation and inference from complex survey samples typically focuses on global model parameters whose estimators have asymptotic properties, such as from fixed effects regression models. We present a motivating example of Bayesian inference for a multi-level or mixed effects model in which both the local parameters (e.g. group level random effects) and the global parameters may need to be adjusted for the complex sampling design. We evaluate the limitations of the survey-weighted pseudo-posterior and an existing automated post-processing method to incorporate the complex survey sample design for a wide variety of Bayesian models. We propose modifications to the automated process and demonstrate their improvements for multi-level models via a simulation study and a motivating example from the National Survey on Drug Use and Health. Reproduction examples are available from the authors and the updated R package is available via github:https://github.com/RyanHornby/csSampling |
| 2025-10-10 | [Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation](http://arxiv.org/abs/2510.09390v1) | Mert Ä°nan, Anthony Sicilia et al. | Establishing shared goals is a fundamental step in human-AI communication. However, ambiguities can lead to outputs that seem correct but fail to reflect the speaker's intent. In this paper, we explore this issue with a focus on the data visualization domain, where ambiguities in natural language impact the generation of code that visualizes data. The availability of multiple views on the contextual (e.g., the intended plot and the code rendering the plot) allows for a unique and comprehensive analysis of diverse ambiguity types. We develop a taxonomy of types of ambiguity that arise in this task and propose metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, we demonstrate that our ambiguity metrics better correlate with human annotations than uncertainty baselines. Our work also explores how multi-turn dialogue can reduce ambiguity, therefore, improve code accuracy by better matching user goals. We evaluate three pragmatic models to inform our dialogue strategies: Gricean Cooperativity, Discourse Representation Theory, and Questions under Discussion. A simulated user study reveals how pragmatic dialogues reduce ambiguity and enhance code accuracy, highlighting the value of multi-turn exchanges in code generation. |
| 2025-10-09 | [Scalable Offline Metrics for Autonomous Driving](http://arxiv.org/abs/2510.08571v1) | Animikh Aich, Adwait Kulkarni et al. | Real-World evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e., by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed. |
| 2025-10-09 | [Cleaning Galactic foregrounds with spatially varying spectral dependence from CMB observations with \texttt{fgbuster}](http://arxiv.org/abs/2510.08534v1) | Arianna Rizzieri, ClÃ©ment Leloup et al. | In the context of maximum-likelihood parametric component separation for next-generation full-sky CMB polarization experiments, we study the impact of fitting different spectral parameters of Galactic foregrounds in distinct subsets of pixels on the sky, with the goal of optimizing the search for primordial B modes. Using both simulations and analytical arguments, we highlight how the post-component separation uncertainty and systematic foreground residuals in the cleaned CMB power spectrum depend on spatial variations in the spectral parameters. We show that allowing spectral parameters to vary across subsets of the sky pixels is essential to achieve competitive S/N on the reconstructed CMB after component separation while keeping residual foreground bias under control. Although several strategies exist to define pixel subsets for the spectral parameters, each with its advantages and limitations, we show using current foreground simulations in the context of next-generation space-borne missions that there are satisfactory configurations in which both statistical and systematic residuals become negligible. The exact magnitude of these residuals, however, depends on the mission's specific characteristics, especially its frequency coverage and sensitivity. We also show that the post-component separation statistical uncertainty is only weakly dependent on the properties of the foregrounds and propose a semi-analytical framework to estimate it. On the contrary, the systematic foreground residuals highly depend on both the properties of the foregrounds and the chosen spatial resolution of the spectral parameters. |
| 2025-10-09 | [Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression](http://arxiv.org/abs/2510.08512v1) | Nikolaos Stathoulopoulos, Christoforos Kanellakis et al. | Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans. |
| 2025-10-09 | [Constraining the new contributions to electron $g-2$ in a radiative neutrino mass model](http://arxiv.org/abs/2510.08504v1) | Bayu Dirgantara, J. Julio | We examine electron and muon anomalous magnetic dipole moments within a radiative neutrino mass model featuring TeV-scale scalar leptoquarks $S(3,1,-1/3)$ and $R(3,2,1/6)$. We utilize textures with decoupling electron and muon sectors, so that both electron and muon anomalous magnetic dipole moments could receive internal chiral enhancements from different heavy up-type quarks while in the same time evading the stringent $\mu\to e\gamma$ constraint. A successful fit to neutrino oscillation data requires the simultaneous presence of one- and two-loop neutrino mass contributions. This severely constrains the parameter space of the model, which results in a negligible new physics correction to the muon $g-2$. The electron $g-2$ discrepancy implied by the rubidium experiment, on the other hand, can be resolved within $2\sigma$ uncertainty provided that neutrino mass ordering is inverted. Lepton-flavor-violating tau decay rates, such as $\tau\to e\gamma$ and $\tau\to 3e$, are predicted to be within the sensitivities of next-generation experiments. |
| 2025-10-09 | [Diffusion-Based Probabilistic Modeling for Hourly Streamflow Prediction and Assimilation](http://arxiv.org/abs/2510.08488v1) | Wencong Yang, Haoyu Ji et al. | Hourly predictions are critical for issuing flood warnings as the flood peaks on the hourly scale can be distinctly higher than the corresponding daily ones. Currently a popular hourly data-driven prediction scheme is multi-time-scale long short-term memory (MTS-LSTM), yet such models face challenges in probabilistic forecasts or integrating observations when available. Diffusion artificial intelligence (AI) models represent a promising method to predict high-resolution information, e.g., hourly streamflow. Here we develop a denoising diffusion probabilistic model (h-Diffusion) for hourly streamflow prediction that conditions on either observed or simulated daily discharge from hydrologic models to generate hourly hydrographs. The model is benchmarked on the CAMELS hourly dataset against record-holding MTS-LSTM and multi-frequency LSTM (MF-LSTM) baselines. Results show that h-Diffusion outperforms baselines in terms of general performance and extreme metrics. Furthermore, the h-Diffusion model can utilize the inpainting technique and recent observations to accomplish data assimilation that largely improves flood forecasting performance. These advances can greatly reduce flood forecasting uncertainty and provide a unified probabilistic framework for downscaling, prediction, and data assimilation at the hourly scale, representing risks where daily models cannot. |
| 2025-10-09 | [Stochastic Volatility-in-mean VARs with Time-Varying Skewness](http://arxiv.org/abs/2510.08415v1) | Leonardo N. Ferreira, Haroon Mumtaz et al. | This paper introduces a Bayesian vector autoregression (BVAR) with stochastic volatility-in-mean and time-varying skewness. Unlike previous approaches, the proposed model allows both volatility and skewness to directly affect macroeconomic variables. We provide a Gibbs sampling algorithm for posterior inference and apply the model to quarterly data for the US and the UK. Empirical results show that skewness shocks have economically significant effects on output, inflation and spreads, often exceeding the impact of volatility shocks. In a pseudo-real-time forecasting exercise, the proposed model outperforms existing alternatives in many cases. Moreover, the model produces sharper measures of tail risk, revealing that standard stochastic volatility models tend to overstate uncertainty. These findings highlight the importance of incorporating time-varying skewness for capturing macro-financial risks and improving forecast performance. |
| 2025-10-09 | [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](http://arxiv.org/abs/2510.08389v1) | Rui Wang, Zeming Wei et al. | Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness. |
| 2025-10-09 | [The Two-Sided Clifford Dunkl Transform and Miyachi's Theorem](http://arxiv.org/abs/2510.08347v1) | Mohamed Essenhajy, Said Fahlaoui | Recent advances have extended the Dunkl transform to the setting of Clifford algebras. In particular, the two-sided quaternionic Dunkl transform has been introduced as a Dunkl analogue of the two-dimensional quaternionic Fourier transform. In this paper, we develop the two-sided Clifford Dunkl transform, defined using two square roots of -1 in Cl_{p,q}. We establish its fundamental properties, including the inversion and Plancherel formulas, and provide two explicit expressions for the associated translation operator. Moreover, we prove an analogue of Miyachi's theorem for this transform, thereby extending a classical result in harmonic analysis to the Clifford-Dunkl framework. |
| 2025-10-09 | [Design of chemical recycling processes for PUR foam under uncertainty](http://arxiv.org/abs/2510.08301v1) | Patrick Lotz, Luca Bosetti et al. | Optimization problems in chemical process design involve a significant number of discrete and continuous decisions. When taking into account uncertainties, the search space is very difficult to explore, even for experienced engineers. Moreover, it should be taken into account that while some decisions are fixed at the design stage, other parameters can be adapted to the realization of the uncertainty during the operation of the plant. This leads to a two-stage optimization problem which is difficult to solve. To address this challenge, we propose to combine commercial process simulation software with an evolutionary strategy. This approach is applied to designing a downstream process to isolate valuable products from pyrolysis oil produced by the catalytic pyrolysis of rigid polyurethane foam. The suggested algorithm consistently performed better than a manually designed robust process. Additionally, the analysis of different scenarios provided insight into promising changes in the overall layout of the recycling process. |
| 2025-10-09 | [Adversarial Thermodynamics](http://arxiv.org/abs/2510.08298v1) | Maite Arcos, Philippe Faist et al. | In thermodynamics, an agent's ability to extract work is fundamentally constrained by their environment. Traditional frameworks struggle to capture how strategic decision-making under uncertainty -- particularly an agent's tolerance for risk -- determines the trade-off between extractable work and probability of success in finite-scale experiments. Here, we develop a framework for non-equilibrium thermodynamics based on adversarial resource theories, in which work extraction is modelled as an adversarial game for an agent extracting work. Within this perspective, we recast the Szilard engine as a game isomorphic to Kelly gambling, an information-theoretic model of optimal betting under uncertainty -- but with a thermodynamic utility function. Extending the framework to finite-size regimes, we apply a risk-reward trade-off to find an interpretation of the Renyi-divergences, in terms of extractable work for a given failure probability. By incorporating risk sensitivity via utility functions, we show that the guaranteed amount of work a rational agent would accept instead of undertaking a risky protocol is given by a R\'enyi divergence. This provides a unified picture of thermodynamics and gambling, and highlights how generalized free energies emerge from an adversarial setup. |
| 2025-10-08 | [Probing evolution of Long GRB properties through their cosmic formation history aided by Machine Learning predicted redshifts](http://arxiv.org/abs/2510.07306v1) | Dhruv S. Bal, Aditya Narendra et al. | Gamma-ray Bursts (GRBs) are valuable probes of cosmic star formation reaching back into the epoch of reionization, and a large dataset with known redshifts ($z$) is an important ingredient for these studies. Usually, $z$ is measured using spectroscopy or photometry, but $\sim80\%$ of GRBs lack such data. Prompt and afterglow correlations can provide estimates in these cases, though they suffer from systematic uncertainties due to assumed cosmologies and due to detector threshold limits. We use a sample with $z$ estimated via machine learning models, based on prompt and afterglow parameters, without relying on cosmological assumptions. We then use an augmented sample of GRBs with measured and predicted redshifts, forming a larger dataset. We find that the predicted redshifts are a crucial step forward in understanding the evolution of GRB properties. We test three cases: no evolution, an evolution of the beaming factor, and an evolution of all terms captured by an evolution factor $(1+z)^\delta$. We find that these cases can explain the density rate in the redshift range between 1-2, but neither of the cases can explain the derived rate densities at smaller and higher redshifts, which may point towards an evolution term different than a simple power law. Another possibility is that this mismatch is due to the non-homogeneity of the sample, e.g., a non-collapsar origin of some long GRB within the sample. |
| 2025-10-08 | [On the Convergence of Moral Self-Correction in Large Language Models](http://arxiv.org/abs/2510.07290v1) | Guangliang Liu, Haitao Mao et al. | Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance. |
| 2025-10-08 | [Muonium HFS Uncertainty Revisited](http://arxiv.org/abs/2510.07281v1) | Michael I. Eides | Uncertainty of the quantum electrodynamics theoretical prediction for the hyperfine splitting in the ground state of muonium is considered. It is compared with the respective discussion in the two most recent CODATA adjustments of the fundamental physical constants. |
| 2025-10-08 | [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](http://arxiv.org/abs/2510.07210v1) | Donald Pfaffmann, Matthias Klusch et al. | We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proximal policy optimization and approximated online POMDP planning with heuristic confidence-based vertical pruning to reduce its execution time without compromising safety of driving. Our experimental performance analysis on the CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed that HyPlan may navigate safer than selected relevant baselines and perform significantly faster than considered alternative online POMDP planners. |
| 2025-10-08 | [A Broader View of Thompson Sampling](http://arxiv.org/abs/2510.07208v1) | Yanlin Qu, Hongseok Namkoong et al. | Thompson Sampling is one of the most widely used and studied bandit algorithms, known for its simple structure, low regret performance, and solid theoretical guarantees. Yet, in stark contrast to most other families of bandit algorithms, the exact mechanism through which posterior sampling (as introduced by Thompson) is able to "properly" balance exploration and exploitation, remains a mystery. In this paper we show that the core insight to address this question stems from recasting Thompson Sampling as an online optimization algorithm. To distill this, a key conceptual tool is introduced, which we refer to as "faithful" stationarization of the regret formulation. Essentially, the finite horizon dynamic optimization problem is converted into a stationary counterpart which "closely resembles" the original objective (in contrast, the classical infinite horizon discounted formulation, that leads to the Gittins index, alters the problem and objective in too significant a manner). The newly crafted time invariant objective can be studied using Bellman's principle which leads to a time invariant optimal policy. When viewed through this lens, Thompson Sampling admits a simple online optimization form that mimics the structure of the Bellman-optimal policy, and where greediness is regularized by a measure of residual uncertainty based on point-biserial correlation. This answers the question of how Thompson Sampling balances exploration-exploitation, and moreover, provides a principled framework to study and further improve Thompson's original idea. |
| 2025-10-08 | [Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks](http://arxiv.org/abs/2510.07199v1) | Shirin Shoushtari, Edward P. Chandler et al. | Poisson denoising plays a central role in photon-limited imaging applications such as microscopy, astronomy, and medical imaging. It is common to train deep learning models for denoising using the mean-squared error (MSE) loss, which corresponds to computing the posterior mean $\mathbb{E}[x \mid y]$. When the noise is Gaussian, Tweedie's formula enables approximation of the posterior distribution through its higher-order moments. However, this connection no longer holds for Poisson denoising: while $ \mathbb{E}[x \mid y] $ still minimizes MSE, it fails to capture posterior uncertainty. We propose a new strategy for Poisson denoising based on training a log-network. Instead of predicting the posterior mean $ \mathbb{E}[x \mid y] $, the log-network is trained to learn $\mathbb{E}[\log x \mid y]$, leveraging the logarithm as a convenient parameterization for the Poisson distribution. We provide a theoretical proof that the proposed log-network enables recovery of higher-order posterior moments and thus supports posterior approximation. Experiments on simulated data show that our method matches the denoising performance of standard MMSE models while providing access to the posterior. |
| 2025-10-08 | [Bayesian Portfolio Optimization by Predictive Synthesis](http://arxiv.org/abs/2510.07180v1) | Masahiro Kato, Kentaro Baba et al. | Portfolio optimization is a critical task in investment. Most existing portfolio optimization methods require information on the distribution of returns of the assets that make up the portfolio. However, such distribution information is usually unknown to investors. Various methods have been proposed to estimate distribution information, but their accuracy greatly depends on the uncertainty of the financial markets. Due to this uncertainty, a model that could well predict the distribution information at one point in time may perform less accurately compared to another model at a different time. To solve this problem, we investigate a method for portfolio optimization based on Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for meta-learning. We assume that investors have access to multiple asset return prediction models. By using BPS with dynamic linear models to combine these predictions, we can obtain a Bayesian predictive posterior about the mean rewards of assets that accommodate the uncertainty of the financial markets. In this study, we examine how to construct mean-variance portfolios and quantile-based portfolios based on the predicted distribution information. |
| 2025-10-08 | [Optimal network pricing with oblivious users: a new model and algorithm](http://arxiv.org/abs/2510.07157v1) | Yixuan Li, Andersen Ang et al. | Traffic modeling is important in modern society. In this work we propose a new model on the optimal network pricing (Onp) with the assumption of oblivious users, in which the users remain oblivious to real-time traffic conditions and others' behavior. Inspired by works on transportation research and network pricing for selfish traffic, we mathematically derive and prove a new formulation of Onp with decision-dependent modeling that relax certain existing modeling constraints in the literature. Then, we express the Onp formulation as a constrained nonconvex stochastic quadratic program with uncertainty, and we propose an efficient algorithm to solve the problem, utilizing graph theory, sparse linear algebra and stochastic approximation. Lastly, we showcase the effectiveness of the proposed algorithm and the usefulness of the new Onp formulation. The proposed algorithm achieves a 5x speedup by exploiting the sparsity structure of the model. |
| 2025-10-08 | [CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized Cluster Strong Lens Modeling](http://arxiv.org/abs/2510.07131v1) | Yushan Xie, Huanyuan Shan et al. | Strongly lensed supernovae (glSNe) provide a powerful, independent method to measure the Hubble constant, $H_{0}$, through time delays between their multiple images. The accuracy of this measurement depends critically on both the precision of time delay estimation and the robustness of lens modeling. In many current cluster-scale modeling algorithms, all multiple images used for modeling are simplified as point sources to reduce computational costs. In the first paper of the CURLING program, we demonstrated that such a point-like approximation can introduce significant uncertainties and biases in both magnification reconstruction and cosmological inference. In this study, we explore how such simplifications affect $H_0$ measurements from glSNe. We simulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at $z=0.336$, assuming time delays are measured from LSST-like light curves. The lens model is constructed using JWST-like imaging data, utilizing both Lenstool and a pixelated method developed in CURLING. Under a fiducial cosmology with $H_0=70\rm \ km \ s^{-1}\ Mpc^{-1}$, the Lenstool model yields $H_0=69.91^{+6.27}_{-5.50}\rm \ km\ s^{-1}\ Mpc^{-1}$, whereas the pixelated framework improves the precision by over an order of magnitude, $H_0=70.39^{+0.82}_{-0.60}\rm \ km \ s^{-1}\ Mpc^{-1}$. Our results indicate that in the next-generation observations (e.g., JWST), uncertainties from lens modeling dominate the error budget for $H_0$ inference, emphasizing the importance of incorporating the extended surface brightness of multiple images to fully leverage the potential of glSNe for cosmology. |
| 2025-10-08 | [Mitigating Increase-Decrease Gaming with Alternative Connection Agreements: A Defender-Attacker-Defender Game](http://arxiv.org/abs/2510.07102v1) | Bart van der Holst, Thomas Swarts et al. | Redispatch markets are widely used by system operators to manage network congestion. A well-known drawback, however, is that Flexibility Service Providers (FSPs) may strategically adjust their baselines in anticipation of redispatch actions, thereby aggravating congestion and raising system costs. To address this increase-decrease gaming, Distribution System Operators (DSOs) could use Alternative Connection Agreements (ACAs) to conditionally limit the available connection capacity of market participants in the day-ahead stage. In this paper, we present a novel Defender-Attacker-Defender game to investigate the potential of this approach in distribution networks under load and price uncertainty. We solve the resulting trilevel optimization model using a custom branch-and-bound algorithm, and we demonstrate that it efficiently solves the problem without exploring many nodes in the branch-and-bound search tree for most simulated scenarios. The case study demonstrates that applying ACAs can substantially lower redispatch costs (e.g. by 25%) for the DSO with only a limited impact on FSP profits. The effectiveness of the approach critically depends on how often the DSO can invoke ACAs and on the extent to which the DSO can anticipate strategic bidding behavior of the FSP. |
| 2025-10-07 | [Studying the gravitational-wave population without looking that FAR out](http://arxiv.org/abs/2510.06220v1) | Noah E. Wolfe, Matthew Mould et al. | From catalogs of gravitational-wave transients, the population-level properties of their sources and the formation channels of merging compact binaries can be constrained. However, astrophysical conclusions can be biased by misspecification or misestimation of the population likelihood. Despite detection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio (SNR), the current catalog is likely contaminated by noise transients. Further, computing the population likelihood becomes less accurate as the catalog grows. Current methods to address these challenges often scale poorly with the number of events and potentially become infeasible for future catalogs. Here, we evaluate a simple remedy: increasing the significance threshold for including events in population analyses. To determine the efficacy of this approach, we analyze simulated catalogs of up to 1600 gravitational-wave signals from black-hole mergers using full Bayesian parameter estimation with current detector sensitivities. We show that the growth in statistical uncertainty about the black-hole population, as we analyze fewer events but with higher SNR, depends on the source parameters of interest. When the SNR threshold is raised from 11 to 15 -- reducing our catalog size by two--thirds -- we find that statistical uncertainties on the mass distribution only grow by a few 10% and constraints on the spin distribution are essentially unchanged; meanwhile, uncertainties on the high-redshift cosmic merger rate more than double. Simultaneously, numerical uncertainty in the estimate of the population likelihood more than halves, allowing us to ensure unbiased inference without additional computational expense. Our results demonstrate that focusing on higher-significance events is an effective way to facilitate robust astrophysical inference with growing gravitational-wave catalogs. |
| 2025-10-07 | [Conformalized Gaussian processes for online uncertainty quantification over graphs](http://arxiv.org/abs/2510.06181v1) | Jinwen Xu, Qin Lu et al. | Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP. |
| 2025-10-07 | [Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques](http://arxiv.org/abs/2510.06136v1) | Jieyun Wang, Anna L. Smith | Latent space models assume that network ties are more likely between nodes that are closer together in an underlying latent space. Euclidean space is a popular choice for the underlying geometry, but hyperbolic geometry can mimic more realistic patterns of ties in complex networks. To identify the underlying geometry, past research has applied non-Euclidean extensions of multidimensional scaling (MDS) to the observed geodesic distances: the shortest path lengths between nodes. The difference in stress, a standard goodness-of-fit metric for MDS, across the geometries is then used to select a latent geometry with superior model fit (lower stress). The effectiveness of this method is assessed through simulations of latent space networks in Euclidean and hyperbolic geometries. To better account for uncertainty, we extend permutation-based hypothesis tests for MDS to the latent network setting. However, these tests do not incorporate any network structure. We propose a parametric bootstrap distribution of networks, conditioned on observed geodesic distances and the Gaussian Latent Position Model (GLPM). Our method extends the Davidson-MacKinnon J-test to latent space network models with differing latent geometries. We pay particular attention to large and sparse networks, and both the permutation test and the bootstrapping methods show an improvement in detecting the underlying geometry. |
| 2025-10-07 | [Multiprobe constraints on early and late time dark energy](http://arxiv.org/abs/2510.06114v1) | Alexander Reeves, Simone Ferraro et al. | We perform a multiprobe analysis combining cosmic microwave background (CMB) data from Planck and the Atacama Cosmology Telescope (ACT), ACT CMB lensing, and large-scale structure (LSS) measurements from the Dark Energy Spectroscopic Instrument (DESI), including DESI Legacy Imaging Survey (LS) galaxies and baryon acoustic oscillations (BAOs). We present the first $5\times2$pt analysis of ACT DR6 lensing, DESI LS, and Planck ISW. Within $\Lambda$CDM, this yields $S_8 = \sigma_8(\Omega_m/0.3)^{0.5} = 0.819 \pm 0.016$, in good agreement with primary CMB inferences and provides a sound-horizon-free Hubble constant constraint of $H_0 = 70.0 \pm 4.4$ km s$^{-1}$ Mpc$^{-1}$. Then, combining with CMB primary and BAO, we reconfirm a CMB-BAO discrepancy in the $\Omega_m$-$\frac{D_v}{r_d}$ plane, which is heightened when combining BAO with the $5\times2$pt data vector. We explore two dark-energy extensions that may reconcile this: an early-time modification, early dark energy (EDE), and late-time dynamical dark energy (DDE) parameterized by $w_0w_a$. For CMB primary+BAO+$5\times2$pt, we find a $3.3\sigma$ preference for DDE over $\Lambda$CDM, while EDE is modestly favoured at $2.3\sigma$. The models address different shortcomings of $\Lambda$CDM: DDE relaxes the neutrino mass bound ($M_\nu<0.17$eV vs. $<0.050$eV under $\Lambda$CDM), making it compatible with neutrino oscillation measurements, while EDE raises the Hubble constant to $H_0=70.5\pm1.2\,\mathrm{km\,s^{-1}\,Mpc^{-1}}$, easing the discrepancy with SH0ES. However, neither model resolves both issues simultaneously. Our analysis indicates that both DDE and EDE remain viable extensions of $\Lambda$CDM within current uncertainties and demonstrates the capacity of combined probes to place increasingly stringent constraints on cosmological parameters. |
| 2025-10-07 | [Mass loading of outflows from evolving Young Massive Clusters](http://arxiv.org/abs/2510.06100v1) | C. J. K. Larkin, C. Hawcroft et al. | Feedback from Young Massive Clusters (YMCs) is an important driver of galaxy evolution. In the first few Myr, mechanical feedback is dominated by collective effects of the massive stellar winds in the YMC. The mass-loss rates and terminal wind velocities of these stars change by orders of magnitude over pre-SN timescales as the massive stars evolve, and mass-loss rates of Cool Supergiant (CSG) stars in particular are uncertain by a factor $\sim~20$ or more. In this work we perform a first study of the time evolution of average cluster wind velocity $\bar{V}_{\mathrm{cl}}$ as a function of stellar metallicity $Z$, assuming single star evolution. We also check the validity of assuming Wolf-Rayet stars dominate the feedback effects of a YMC, as often done when interpreting X-ray and $\gamma$-ray observations, and test how sensitive $\bar{V}_{\mathrm{cl}}$ is to current uncertainties in mass-loss rates. We use pySTARBURST99 to calculate integrated properties of YMCs for $Z$ in the range of $0.0004-0.02$, representing a range of environments from IZw18 to the Galactic Centre. We find that $\bar{V}_{\mathrm{cl}}$ drops off rapidly for sub-LMC $Z$, and we recommend a value of $500-1000\,~\textrm{km~s}^{-1}$ be used in this regime. We show accounting only for WR stars can overestimate $\bar{V}_{\mathrm{cl}}$ by $500-2000\,~\textrm{km~s}^{-1}$ at $Z \geq Z_\text{LMC}$. We also find that different RSG mass-loss assumptions can change the inferred $\bar{V}_{\mathrm{cl}}$ by $\sim1000\,~\textrm{km~s}^{-1}$, highlighting the need for improved observational constraints for RSGs in YMCs. |
| 2025-10-07 | [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](http://arxiv.org/abs/2510.06096v1) | Matthieu Bou, Nyal Patel et al. | The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI. |
| 2025-10-07 | [Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data](http://arxiv.org/abs/2510.06080v1) | Nga Nguyen, Olivier Bonnefon et al. | Biological control strategies against mosquito-borne diseases--such as the sterile insect technique (SIT), RIDL, and Wolbachia-based releases--require reliable estimates of dispersal and survival of released males. We propose a mechanistic--statistical framework for mark--release--recapture (MRR) data linking an individual-based 2D diffusion model with its reaction--diffusion limit. Inference is based on solving the macroscopic system and embedding it in a Poisson observation model for daily trap counts, with uncertainty quantified via a parametric bootstrap. We validate identifiability using simulated data and apply the model to an urban MRR campaign in El Cano (Havana, Cuba) involving four weekly releases of sterile Aedes aegypti males. The best-supported model suggests a mean life expectancy of about five days and a typical displacement of about 180 m. Unlike empirical fits of survival or dispersal, our mechanistic approach jointly estimates movement, mortality, and capture, yielding biologically interpretable parameters and a principled framework for designing and evaluating SIT-based interventions. |
| 2025-10-07 | [Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition](http://arxiv.org/abs/2510.06033v1) | Jim Dai, Manxi Wu et al. | Stochastic processing networks (SPNs) have broad applications in healthcare, transportation, and communication networks. The control of SPN is to dynamically assign servers in batches under uncertainty to optimize long-run performance. This problem is challenging as the policy dimension grows exponentially with the number of servers, making standard reinforcement learning and policy optimization methods intractable at scale. We propose an atomic action decomposition framework that addresses this scalability challenge by breaking joint assignments into sequential single-server assignments. This yields policies with constant dimension, independent of the number of servers. We study two classes of atomic policies, the step-dependent and step-independent atomic policies, and prove that both achieve the same optimal long-run average reward as the original joint policies. These results establish that computing the optimal SPN control can be made scalable without loss of optimality using the atomic framework. Our results offer theoretical justification for the strong empirical success of the atomic framework in large-scale applications reported in previous articles. |
| 2025-10-07 | [Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers](http://arxiv.org/abs/2510.06025v1) | Kevin Raina, Tanya Schmah | Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST and CIFAR-10 In-Distributions, with 5000 training samples or less, show that the Bayesian methods outperform corresponding deterministic methods. |
| 2025-10-07 | [Uncertainty in Machine Learning](http://arxiv.org/abs/2510.06007v1) | Hans Weytjens, Wouter Verbeke | This book chapter introduces the principles and practical applications of uncertainty quantification in machine learning. It explains how to identify and distinguish between different types of uncertainty and presents methods for quantifying uncertainty in predictive models, including linear regression, random forests, and neural networks. The chapter also covers conformal prediction as a framework for generating predictions with predefined confidence intervals. Finally, it explores how uncertainty estimation can be leveraged to improve business decision-making, enhance model reliability, and support risk-aware strategies. |
| 2025-10-06 | [Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A Particle-Tracking Study](http://arxiv.org/abs/2510.05084v1) | Ethan Kahn | Electrospray thrusters are emerging as a leading propulsion technology for CubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power requirements. However, the divergent ion plumes can impinge on spacecraft surfaces, particularly body-mounted solar arrays, causing contamination and thrust efficiency losses. This study presents a validated particle-tracking simulation to quantify the effects of thruster placement on thrust efficiency and surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs a cosine power distribution ($k=1.8$) with half-angle $46^\circ$, validated against experimental data with errors below 7%. Results show that thrust efficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted configurations to 100% for side-mounted configurations with deployable arrays. CubeSat size significantly affects impingement: 3U platforms experience 46.4% contamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable solar arrays reduce contamination by 77% compared to body-mounted arrays, while side-mounted thrusters eliminate impingement entirely at the cost of only 1.6% efficiency loss. Corner-mounted configurations at $30^\circ$ cant provide intermediate performance with 88.9% efficiency and 11.1% contamination. These quantitative design guidelines enable mission planners to optimize thruster integration based on power budget and propellant mass constraints, with statistical uncertainty below 0.15% across all configurations. |
| 2025-10-06 | [The role of entropy production and thermodynamic uncertainty relations in the thermalization of open quantum systems](http://arxiv.org/abs/2510.05072v1) | Ãlvaro Tejero | The asymmetry between heating and cooling in open quantum systems is a hallmark of nonequilibrium dynamics, yet its thermodynamic origin has remained unclear. Here, we investigate the thermalization of a quantum system weakly coupled to a thermal bath, focusing on the entropy production rate and the quantum thermokinetic uncertainty relation (TKUR). We derive an analytical expression for the entropy production rate, showing that heating begins with a higher entropy production, which drives faster thermalization than cooling. The quantum TKUR links this asymmetry to heat current fluctuations, demonstrating that larger entropy production suppresses fluctuations, making heating more stable than cooling. Our results reveal the thermodynamic basis of asymmetric thermalization and highlight uncertainty relations as key to nonequilibrium quantum dynamics. |
| 2025-10-06 | [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](http://arxiv.org/abs/2510.05054v1) | Peter Van Katwyk, Karianne J. Bergen | Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework. |
| 2025-10-06 | [Inferring the spins of merging black holes in the presence of data-quality issues](http://arxiv.org/abs/2510.05029v1) | Rhiannon Udall, Sophie Bini et al. | Gravitational waves from black hole binary mergers carry information about the component spins, but inference is sensitive to analysis assumptions, which may be broken by terrestrial noise transients known as glitches. Using a variety of simulated glitches and gravitational wave signals, we study the conditions under which glitches can bias spin measurements. We confirm the theoretical expectation that inference and subtraction of glitches invariably leaves behind residual power due to statistical uncertainty, no matter the strength (signal-to-noise ratio; SNR) of the original glitch. Next we show that low-SNR glitches - including those below the threshold for flagging data-quality issues - can still significantly bias spin inference. Such biases occur for a range of glitch morphologies, even in cases where glitches and signals are not precisely aligned in phase. Furthermore, we find that residuals of glitch subtraction can result in biases as well. Our results suggest that joint inference of the glitch and gravitational wave parameters, with appropriate models and priors, is required to address these uncertainties inherent in glitch mitigation via subtraction. |
| 2025-10-06 | [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](http://arxiv.org/abs/2510.05027v1) | Ethan Davis | We introduce a framework for applying metaheuristic algorithms, such as ant colony optimization (ACO), to combinatorial optimization problems (COPs) like the traveling salesman problem (TSP). The framework consists of three sequential stages: broad exploration of the parameter space, exploitation of top-performing parameters, and uncertainty quantification (UQ) to assess the reliability of results. As a case study, we apply ACO to the TSPLIB berlin52 dataset, which has a known optimal tour length of 7542. Using our framework, we calculate that the probability of ACO finding the global optimum is approximately 1/40 in a single run and improves to 1/5 when aggregated over ten runs. |
| 2025-10-06 | [Risk-Adjusted Policy Learning and the Social Cost of Uncertainty: Theory and Evidence from CAP evaluation](http://arxiv.org/abs/2510.05007v1) | Giovanni Cerulli, Francesco Caracciolo | This paper develops a risk-adjusted alternative to standard optimal policy learning (OPL) for observational data by importing Roy's (1952) safety-first principle into the treatment assignment problem. We formalize a welfare functional that maximizes the probability that outcomes exceed a socially required threshold and show that the associated pointwise optimal rule ranks treatments by the ratio of conditional means to conditional standard deviations. We implement the framework using microdata from the Italian Farm Accountancy Data Network to evaluate the allocation of subsidies under the EU Common Agricultural Policy. Empirically, risk-adjusted optimal policies systematically dominate the realized allocation across specifications, while risk aversion lowers overall welfare relative to the risk-neutral benchmark, making transparent the social cost of insurance against uncertainty. The results illustrate how safety-first OPL provides an implementable, interpretable tool for risk-sensitive policy design, quantifying the efficiency-insurance trade-off that policymakers face when outcomes are volatile. |
| 2025-10-06 | [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](http://arxiv.org/abs/2510.05006v1) | Koen Vellenga, H. Joe Steinhauer et al. | Deep neural networks (DNNs) are increasingly applied to safety-critical tasks in resource-constrained environments, such as video-based driver action and intention recognition. While last layer probabilistic deep learning (LL-PDL) methods can detect out-of-distribution (OOD) instances, their performance varies. As an alternative to last layer approaches, we propose extending pre-trained DNNs with transformation layers to produce multiple latent representations to estimate the uncertainty. We evaluate our latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) approaches against eight PDL methods across four video-based driver action and intention recognition datasets, comparing classification performance, calibration, and uncertainty-based OOD detection. We also contribute 28,000 frame-level action labels and 1,194 video-level intention labels for the NuScenes dataset. Our results show that LUR and RLUR achieve comparable in-distribution classification performance to other LL-PDL approaches. For uncertainty-based OOD detection, LUR matches top-performing PDL methods while being more efficient to train and easier to tune than approaches that require Markov-Chain Monte Carlo sampling or repulsive training procedures. |
| 2025-10-06 | [Doubly Discordant SH0ES NGC4258 Cepheid Relations (HVI), and Impactful Extinction Laws](http://arxiv.org/abs/2510.04998v1) | Daniel Majaess | S$H_0$ES 2016-2022 $HVI$ data for classical Cepheids in the keystone galaxy NGC4258 yield doubly discordant Wesenheit Leavitt functions:~$\Delta W_{0,H-VI} = -0.13\pm0^{m}.02$ ($-0^{m}.17$ unweighted) and that is paired with a previously noted $\Delta W_{0,I-VI}\simeq-0^{m}.3$, which in concert with complimentary evidence suggest the 2016 S$H_0$ES NGC4258-anchored $H_0 \pm \sigma_{H_0}$ warrants scrutiny (i.e., $\sigma_{H_0}/{H_0}\gtrsim 6$\%). Cepheid distance uncertainties are further exacerbated by extinction law ambiguities endemic to such Leavitt relations (e.g., NGC4258), particularly for comparatively obscured variables (e.g., $\Delta d \gtrsim 4$\%, reddened Cepheid subsamples in the Milky Way, M31, NGC2442, NGC4424, NGC5643, NGC7250). Lastly, during the analysis it was identified that the 2022 S$H_0$ES database relays incorrect SMC Cepheid photometry. |
| 2025-10-06 | [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](http://arxiv.org/abs/2510.04996v1) | Wei Xiong, Chenlu Ye et al. | Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada. |
| 2025-10-06 | [Line emission search from DM annihilation in the Galactic Center with LST-1](http://arxiv.org/abs/2510.04977v1) | Abhishek Abhishek, Shotaro Abe et al. | Dark Matter remains a great mystery in modern physics. Among various candidates, the weakly interacting massive particles (WIMPs) scenario stands out and is under extensive study. The detection of the hypothetical gamma-ray emission from WIMP annihilation could act as a direct probe of electroweak-scale interactions, complementing DM collider searches and other direct DM detection techniques. At very high energies (VHE), WIMP self-annihilation is expected to produce gamma rays together with other Standard Model particles. The galactic center (GC), due to its relative proximity to the Earth and its high expected DM density, is a prime target for monoenergetic line searches. IACTs have placed strong constraints on the DM properties at the GC, with the MAGIC providing the most stringent limits from 20 TeV to 100 TeV, exploiting large zenith angle (LZA) observations. However, the limited field of view (FoV) of the MAGIC telescopes (< 3.5{\deg} ) prevented a detailed study of the extended region around the GC in which an enhanced DM density is expected. The LST-1 of the CTAO, located at the Roque de Los Muchachos Observatory (La Palma, Spain), close to the MAGIC site, has been observing the GC since 2021. With its wide FoV of 4.5{\deg}, LST-1 could contribute significantly to the WIMPs search at the GC. The observations are performed at LZA (ZA > 58{\deg}), which, while required due to the source's low altitude, also optimizes the detection of gamma rays up to 100 TeV and beyond. We present a study of the systematic uncertainties in WIMP line emission searches with LST-1. Our work examines the instrument response functions for LZA observations, background rejection in monoscopic mode, and includes updated results from simulations, highlighting new methods for spectral line searches. |
| 2025-10-03 | [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](http://arxiv.org/abs/2510.03181v1) | Ha Manh Bui, Felix Parker et al. | We study the Non-Stationary Reinforcement Learning (RL) under distribution shifts in both finite-horizon episodic and infinite-horizon discounted Markov Decision Processes (MDPs). In the finite-horizon case, the transition functions may suddenly change at a particular episode. In the infinite-horizon setting, such changes can occur at an arbitrary time step during the agent's interaction with the environment. While the Q-learning Upper Confidence Bound algorithm (QUCB) can discover a proper policy during learning, due to the distribution shifts, this policy can exploit sub-optimal rewards after the shift happens. To address this issue, we propose Density-QUCB (DQUCB), a shift-aware Q-learning~UCB algorithm, which uses a transition density function to detect distribution shifts, then leverages its likelihood to enhance the uncertainty estimation quality of Q-learning~UCB, resulting in a balance between exploration and exploitation. Theoretically, we prove that our oracle DQUCB achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the computational efficiency of model-free RL and outperforms QUCB baselines by having a lower regret across RL tasks, as well as a real-world COVID-19 patient hospital allocation task using a Deep-Q-learning architecture. |
| 2025-10-03 | [Calibrated Uncertainty Sampling for Active Learning](http://arxiv.org/abs/2510.03162v1) | Ha Manh Bui, Iliana Maifeld-Carucci et al. | We study the problem of actively learning a classifier with a low calibration error. One of the most popular Acquisition Functions (AFs) in pool-based Active Learning (AL) is querying by the model's uncertainty. However, we recognize that an uncalibrated uncertainty model on the unlabeled pool may significantly affect the AF effectiveness, leading to sub-optimal generalization and high calibration error on unseen data. Deep Neural Networks (DNNs) make it even worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty. Specifically, we utilize a kernel calibration error estimator under the covariate shift and formally show that AL with this AF eventually leads to a bounded calibration error on the unlabeled pool and unseen test data. Empirically, our proposed method surpasses other AF baselines by having a lower calibration and generalization error across pool-based AL settings. |
| 2025-10-03 | [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](http://arxiv.org/abs/2510.03155v1) | Stevens Johnson, Varun Puram et al. | Accurate modeling of neuronal action potential (AP) onset timing is crucial for understanding neural coding of danger signals. Traditional leaky integrate-and-fire (LIF) models, while widely used, exhibit high relative error in predicting AP onset latency, especially under strong or rapidly changing stimuli. Inspired by recent experimental findings and quantum theory, we present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats AP onset as a probabilistic event, represented by a Gaussian wave packet in time. This approach captures the biological variability and uncertainty inherent in neuronal firing. We systematically compare the relative error of AP onset predictions between the classical LIF and QI-LIF models using synthetic data from hippocampal and sensory neurons subjected to varying stimulus amplitudes. Our results demonstrate that the QI-LIF model significantly reduces prediction error, particularly for high-intensity stimuli, aligning closely with observed biological responses. This work highlights the potential of quantum-inspired computational frameworks in advancing the accuracy of neural modeling and has implications for quantum engineering approaches to brain-inspired computing. |
| 2025-10-03 | [Statistical framework for nuclear parameter uncertainties in nucleosynthesis modeling of r- and i-process](http://arxiv.org/abs/2510.03138v1) | S. Martinet, G. Goriely et al. | Propagating nuclear uncertainties to nucleosynthesis simulations is key to understand the impact of theoretical uncertainties on the predictions, especially for processes far from the stability region, where nuclear properties are scarcely known. While systematic (model) uncertainties have been thoroughly studied, the statistical (parameter) ones have been more rarely explored, as constraining them is more challenging. We present here a methodology to determine coherently parameter uncertainties by anchoring the theoretical uncertainties to the experimentally known nuclear properties through the use of the Backward Forward Monte Carlo method. We use this methodology for two nucleosynthesis processes: the intermediate neutron capture process (i-process) and the rapid neutron capture process (r-process). We determine coherently for the i-process the uncertainties from the (n,$\gamma$) rates while we explore the impact of nuclear mass uncertainties for the r-process. The effect of parameter uncertainties on the final nucleosynthesis is in the same order as model uncertainties, suggesting the crucial need for more experimental constraints on key nuclei of interest. We show how key nuclear properties, such as relevant (n,$\gamma$) rates impacting the i-process tracers, could enhance tremendously the prediction of stellar evolution models by experimentally constraining them. |
| 2025-10-03 | [A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control](http://arxiv.org/abs/2510.03100v1) | Tianhua Gao | Quadrotor stability under complex dynamic disturbances and model uncertainties poses significant challenges. One of them remains the underfitting problem in high-dimensional features, which limits the identification capability of current learning-based methods. To address this, we introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from which we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for geometric control. Specifically, the high-dimensional mapping for identification is axially ``sliced" into multiple low-dimensional submappings (``slices"). In this way, the complex high-dimensional problem is decomposed into a set of simple low-dimensional tasks addressed by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation without any pre-training or persistent excitation (PE) condition. To enhance the interpretability of the proposed approach, we prove that the full-state closed-loop system exhibits arbitrarily close to exponential stability despite multi-dimensional time-varying disturbances and model uncertainties. This result is novel as it demonstrates exponential convergence without requiring pre-training for unknown disturbances and specific knowledge of the model. |
| 2025-10-03 | [Polarization dependence of spin-electric transitions in molecular exchange qubits](http://arxiv.org/abs/2510.03099v1) | Filippo Troiani, Athanassios K. Boudalis | Quasi-optical experiments are emerging as a powerful technique to probe magnetic transitions in molecular spin systems. However, the simultaneous presence of the electric- and magnetic-dipole induced transitions poses the challenge of discriminating between these two contributions. Besides, the identification of the spin-electric transitions can hardly rely on the peak intensity, because of the current uncertainties on the value of the spin-electric coupling in most molecular compounds. Here, we compute the polarizations required for electric- and magnetic-dipole induced transitions through spin-Hamiltonian models of molecular spin triangles. We show that the polarization allows a clear discrimination between the two kinds of transitions. In addition, it allows one to identify the physical origin of the zero-field splitting in the ground multiplet, a debated issue with significant implications on the coherence properties of the spin qubit implemented in molecular spin triangles. |
| 2025-10-03 | [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](http://arxiv.org/abs/2510.03046v1) | Soohaeng Yoo Willow, Tae Hyeon Park et al. | Machine learning potentials (MLPs) have become essential for large-scale atomistic simulations, enabling ab initio-level accuracy with computational efficiency. However, current MLPs struggle with uncertainty quantification, limiting their reliability for active learning, calibration, and out-of-distribution (OOD) detection. We address these challenges by developing Bayesian E(3) equivariant MLPs with iterative restratification of many-body message passing. Our approach introduces the joint energy-force negative log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models uncertainty in both energies and interatomic forces, yielding superior accuracy compared to conventional NLL losses. We systematically benchmark multiple Bayesian approaches, including deep ensembles with mean-variance estimation, stochastic weight averaging Gaussian, improved variational online Newton, and laplace approximation by evaluating their performance on uncertainty prediction, OOD detection, calibration, and active learning tasks. We further demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by quantifying energy and force uncertainties. Using Bayesian active learning by disagreement (BALD), our framework outperforms random sampling and energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs achieve competitive accuracy with state-of-the-art models while enabling uncertainty-guided active learning, OOD detection, and energy/forces calibration. This work establishes Bayesian equivariant neural networks as a powerful framework for developing uncertainty-aware MLPs for atomistic simulations at scale. |
| 2025-10-03 | [Distributional Inverse Reinforcement Learning](http://arxiv.org/abs/2510.03013v1) | Feiyang Wu, Ye Zhao et al. | We propose a distributional framework for offline Inverse Reinforcement Learning (IRL) that jointly models uncertainty over reward functions and full distributions of returns. Unlike conventional IRL approaches that recover a deterministic reward estimate or match only expected returns, our method captures richer structure in expert behavior, particularly in learning the reward distribution, by minimizing first-order stochastic dominance (FSD) violations and thus integrating distortion risk measures (DRMs) into policy learning, enabling the recovery of both reward distributions and distribution-aware policies. This formulation is well-suited for behavior analysis and risk-aware imitation learning. Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks demonstrate that our method recovers expressive reward representations and achieves state-of-the-art imitation performance. |
| 2025-10-03 | [Real-Time Peer-to-Peer Energy Trading for Multi-Microgrids: Improved Double Auction Mechanism and Prediction-Free Online Trading Approach](http://arxiv.org/abs/2510.02985v1) | Kaidi Huang, Lin Cheng et al. | Peer-to-peer energy trading offers a promising solution for enhancing renewable energy utilization and economic benefits within interconnected microgrids. However, existing real-time P2P markets face two key challenges: high computational complexity in trading mechanisms, and suboptimal participant decision-making under diverse uncertainties. Existing prediction-based decision-making methods rely heavily on accurate forecasts, which are typically unavailable for microgrids, while prediction-free methods suffer from myopic behaviors. To address these challenges, this paper proposes an improved double auction mechanism combined with an adaptive step-size search algorithm to reduce computational burden, and a data-driven dual-reference online optimization (DDOO) framework to enhance participant decision-making. The improved mechanism simplifies bidding procedures, significantly reducing computational burden and ensuring rapid convergence to the market equilibrium. Additionally, the prediction-free DDOO framework mitigates myopic decision-making by introducing two informative reference signals. Case studies on a 20-microgrid system demonstrate the effectiveness and scalability of the proposed mechanism and approach. The improved mechanism significantly decreases the computational time while increasing local energy self-sufficiency periods from 0.01% to 29.86%, reducing reverse power flow periods from 24.51% to 3.96%, and lowering average operating costs by 19.20%. Compared with conventional approaches such as Lyapunov optimization and model predictive control, the DDOO framework achieves a 10%-13% reduction in operating costs with an optimality gap of only 5.76%. |
| 2025-10-03 | [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](http://arxiv.org/abs/2510.02976v1) | Alvaro Paz, Pauli Mustalahti et al. | This paper presents a framework for real-time optimal controlling of a heavy-duty skid-steered mobile platform for trajectory tracking. The importance of accurate real-time performance of the controller lies in safety considerations of situations where the dynamic system under control is affected by uncertainties and disturbances, and the controller should compensate for such phenomena in order to provide stable performance. A multiple-shooting nonlinear model-predictive control framework is proposed in this paper. This framework benefits from suitable algorithm along with readings from various sensors for genuine real-time performance with extremely high accuracy. The controller is then tested for tracking different trajectories where it demonstrates highly desirable performance in terms of both speed and accuracy. This controller shows remarkable improvement when compared to existing nonlinear model-predictive controllers in the literature that were implemented on skid-steered mobile platforms. |
| 2025-10-02 | [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](http://arxiv.org/abs/2510.02279v1) | Mykyta Ielanskyi, Kajetan Schweighofer et al. | Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings. |
| 2025-10-02 | [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](http://arxiv.org/abs/2510.02216v1) | Zeqi Ye, Minshuo Chen | Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance. |
| 2025-10-02 | [Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale](http://arxiv.org/abs/2510.02189v1) | Boris Kriuk | Arctic warming threatens over 100 billion in permafrost-dependent infrastructure across Northern territories, yet existing risk assessment frameworks lack spatiotemporal validation, uncertainty quantification, and operational decision-support capabilities. We present a hybrid physics-machine learning framework integrating 2.9 million observations from 171,605 locations (2005-2021) combining permafrost fraction data with climate reanalysis. Our stacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic Net) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal cross-validation preventing data leakage. To address machine learning limitations in extrapolative climate scenarios, we develop a hybrid approach combining learned climate-permafrost relationships (60%) with physical permafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over 10 years), we project mean permafrost fraction decline of -20.3 pp (median: -20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point loss. Infrastructure risk classification identifies 15% high-risk zones (25% medium-risk) with spatially explicit uncertainty maps. Our framework represents the largest validated permafrost ML dataset globally, provides the first operational hybrid physics-ML forecasting system for Arctic infrastructure, and delivers open-source tools enabling probabilistic permafrost projections for engineering design codes and climate adaptation planning. The methodology is generalizable to other permafrost regions and demonstrates how hybrid approaches can overcome pure data-driven limitations in climate change applications. |
| 2025-10-02 | [On the uncertainty in predicting the stochastic gravitational wave background from compact binary coalescences](http://arxiv.org/abs/2510.02163v1) | Michael Ebersold, Tania Regimbau | The stochastic gravitational-wave background from compact binary coalescences is expected to be the first detectable stochastic signal via cross-correlation searches with terrestrial detectors. It encodes the cumulative merger history of stellar-mass binaries across cosmic time, offering a unique probe of the high-redshift Universe. However, predicting the background spectrum is challenging due to numerous modeling choices, each with distinct uncertainties. In this work, we present a comprehensive forecast of the astrophysical gravitational-wave background from binary black holes, binary neutron stars, and neutron star-black hole systems. We systematically assess the impact of uncertainties in population properties, waveform features, and the modeling of the merger rate evolution. By combining all uncertainties, we derive credible bands for the background spectrum, spanning approximately an order of magnitude in the fractional energy density. These results provide thorough predictions to facilitate the interpretation of current upper limits and future detections. |
| 2025-10-02 | [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](http://arxiv.org/abs/2510.02109v1) | Jong Bum Won, Wesley De Neve et al. | Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast. |
| 2025-10-02 | [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](http://arxiv.org/abs/2510.02088v1) | Alexander Schmitz, Andreas Petersen et al. | The analysis of the size of nanoparticles is an essential task in plasma technology and dusty plasmas. Light scattering techniques, based on Mie theory, can be used as a non-invasive and in-situ diagnostic tool for this purpose. However, the standard back-calculation methods require expertise from the user. To address this, we introduce a neural network that performs the same task. We discuss how we set up and trained the network to analyze the size of plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up to several hundred nanometers, depending on the used wavelength. The diagnostic approach is kinetic, which means that the particles need to change in size due to growth or etching. An uncertainty analysis as well as a test with experimental data are presented. Our neural network achieves results that agree with those of prior fitting algorithms while offering higher methodical stability. The model also holds a major advantage in terms of computing speed and automation. |
| 2025-10-02 | [Event-triggered control and communication for single-master multi-slave teleoperation systems with Try-Once-Discard protocol](http://arxiv.org/abs/2510.02072v1) | Yuling Li, Chenxi Li et al. | Single-master multi-slave (SMMS) teleoperation systems can perform multiple tasks remotely in a shorter time, cover large-scale areas, and adapt more easily to single-point failures, thereby effectively encompassing a broader range of applications. As the number of slave manipulators sharing a communication network increases, the limitation of communication bandwidth becomes critical. To alleviate bandwidth usage, the Try-Once-Discard (TOD) scheduling protocol and event-triggered mechanisms are often employed separately. In this paper, we combine both strategies to optimize network bandwidth and energy consumption for SMMS teleoperation systems. Specifically, we propose event-triggered control and communication schemes for a class of SMMS teleoperation systems using the TOD scheduling protocol. Considering dynamic uncertainties, the unavailability of relative velocities, and time-varying delays, we develop adaptive controllers with virtual observers based on event-triggered schemes to achieve master-slave synchronization. Stability criteria for the SMMS teleoperation systems under these event-triggered control and communication schemes are established, demonstrating that Zeno behavior is excluded. Finally, experiments are conducted to validate the effectiveness of the proposed algorithms. |
| 2025-10-02 | [Strong-lensing rates of massive black hole binaries in LISA](http://arxiv.org/abs/2510.02061v1) | Juan GutiÃ©rrez, Macarena Lagos | Similarly to electromagnetic (EM) signals, gravitational lensing by intervening galaxies can also affect gravitational waves (GWs). In this paper, we estimate the strong-lensing rate of massive black hole mergers observed with LISA. Given the uncertainties in the source populations as well as in the population of galaxies at high redshift, we consider: six different source population models, including light and heavy seeds, as well as three lens population models, including redshift-independent and redshift-dependent evolution properties. Among all the scenarios explored, the expected number of strong lensed events detected in a 4-year observation time in LISA ranges between 0.13-231 with most of them having two (one) images detectable in the heavy (light) seed scenarios. The event numbers obtained correspond to 0.2%-0.9% of all detected unlensed events. Out of all the detectable strong-lensed events, up to 61% (in the light-seed scenario) and 1% (in the heavy-seed scenario) of them are above the detectability threshold solely due to strong lensing effects and would otherwise be undetectable. For detectable pairs of strong-lensed events by galaxy lenses, we also find between 72%-81% of them to have time delays from 1 week to 1 year. |
| 2025-10-02 | [Chemical transport by weakly nonlinear internal gravity waves in stars](http://arxiv.org/abs/2510.02031v1) | Yifeng Mao, Daniel Lecoanet | While it is well-known that internal gravity waves (IGWs) transport chemicals in the radiative zones of stars, there remains substantial uncertainty on the amount of, and physical mechanism behind, this transport. Most previous studies have relied on heuristic theories, or numerical simulations that may be hard to extrapolate to stellar parameters. In this work, we present the first rigorous asymptotic calculation of (passive) chemical transport by IGWs, in the limit of small wave amplitude. We find that the net transport by a coherent packet of waves scales like wave amplitude to the fourth power, and verify these analytic calculations with numerical simulations. Because the transport is equally likely to be positive as negative, the transport by a random superposition of waves is expected to scale as wave amplitude to the eighth power. These results show that closer comparisons between theoretical arguments and numerical calculations are essential for interpreting numerical simulations of chemical transport by IGWs, and making accurate predictions of this process for stellar evolution modeling. |
| 2025-10-02 | [A Large Sample of JWST/NIRSpec Brown Dwarfs: New Distant Discoveries](http://arxiv.org/abs/2510.02026v1) | Zhijun Tu, Shu Wang et al. | Brown dwarfs are essential probes of stellar and planetary formation, yet their low luminosities pose challenges for detection at large Galactic distances. The James Webb Space Telescope (JWST), with its unprecedented near-infrared sensitivity, enables the discovery and characterization of distant substellar objects, including those in the Milky Way's thick disk and halo. We conducted a systematic search using over 40,000 publicly available JWST/NIRSpec PRISM/CLEAR spectra and identified 68 brown dwarfs through spectral template matching and visual inspection. Among them, 12 are newly identified candidates, including 8 T dwarfs and 4 M/L dwarfs, most at distances exceeding 1 kpc. Remarkably, two sources -- JWST J001418.22-302223.2 and JWST J033240.07-274907.8 -- are found at distances greater than 5 kpc, making them the most distant brown dwarfs within the Milky Way. Spectral fits were performed using a nested sampling Monte Carlo algorithm with three model grids: Sonora Elf Owl, LOWZ, and SAND. The analysis reveals that cloud-free models are unable to reproduce L/T transition spectra, whereas the SAND model provides a more accurate representation of cloud effects in metal-poor environments. With the newly identified distant brown dwarfs, we also investigated the vertical metallicity gradient of brown dwarfs. Overall, the metallicities do not show an evident trend with Galactic height $|Z|$, due to the limited sample size and the uncertainties in metallicity measurements. |
| 2025-09-30 | [Uncertainty Quantification for Regression using Proper Scoring Rules](http://arxiv.org/abs/2509.26610v1) | Alexander Fishkov, Kajetan Schweighofer et al. | Quantifying uncertainty of machine learning model predictions is essential for reliable decision-making, especially in safety-critical applications. Recently, uncertainty quantification (UQ) theory has advanced significantly, building on a firm basis of learning with proper scoring rules. However, these advances were focused on classification, while extending these ideas to regression remains challenging. In this work, we introduce a unified UQ framework for regression based on proper scoring rules, such as CRPS, logarithmic, squared error, and quadratic scores. We derive closed-form expressions for the resulting uncertainty measures under practical parametric assumptions and show how to estimate them using ensembles of models. In particular, the derived uncertainty measures naturally decompose into aleatoric and epistemic components. The framework recovers popular regression UQ measures based on predictive variance and differential entropy. Our broad evaluation on synthetic and real-world regression datasets provides guidance for selecting reliable UQ measures. |
| 2025-09-30 | [Beyond Suboptimality: Resource-Rationality and Task Demands Shape the Complexity of Perceptual Representations](http://arxiv.org/abs/2509.26606v1) | Andrew Jun Lee, Daniel Turek et al. | Early theories of perception as probabilistic inference propose that uncertainty about the interpretation of sensory input is represented as a probability distribution over many interpretations -- a relatively complex representation. However, critics argue that persistent demonstrations of suboptimal perceptual decision-making indicate limits in representational complexity. We contend that suboptimality arises not from genuine limits, but participants' resource-rational adaptations to task demands. For example, when tasks are solvable with minimal attention to stimuli, participants may neglect information needed for complex representations, relying instead on simpler ones that engender suboptimality. Across three experiments, we progressively reduced the efficacy of resource-rational strategies on a carefully controlled decision task. Model fits favored simple representations when resource-rational strategies were effective, and favored complex representations when ineffective, suggesting that perceptual representations can be simple or complex depending on task demands. We conclude that resource-rationality is an epistemic constraint for experimental design and essential to a complete theory of perception. |
| 2025-09-30 | [Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning](http://arxiv.org/abs/2509.26605v1) | MaÃ«l Macuglia, Paul Friedrich et al. | Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents. |
| 2025-09-30 | [The JWST EXCELS Survey: A spectroscopic investigation of the ionizing properties of star-forming galaxies at 1<z<8](http://arxiv.org/abs/2509.26591v1) | R. Begley, R. J. McLure et al. | Charting the Epoch of Reionization demands robust assessments of what drives the production of ionizing photons in high-redshift star-forming galaxies (SFGs), and requires better predictive capabilities from current observations. Using a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution spectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical analysis of their ionizing photon production efficiencies ($\xi_\rm{ion}$). We consider $\xi_\rm{ion}$, measured with Balmer line measurements, in relation to a number of key galaxy properties including; nebular emission line strengths ($W_\lambda(\rm{H\alpha})$ and $W_\lambda$( [OIII])), UV luminosity ($M_\rm{UV}$) and UV slope ($\beta_\rm{UV}$), as well as dust attenuation ($E(B-V)_\rm{neb}$) and redshift. Implementing a Bayesian linear regression methodology, we fit $\xi_\rm{ion}$ against the principal observables while fully marginalising over all measurement uncertainties, mitigating against the impact of outliers and determining the intrinsic scatter. Significant relations between $\xi_\rm{ion}$ and $ W_\lambda(\rm{H\alpha})$, $W_\lambda$([OIII]) and $\beta_\rm{UV}$ are recovered. Moreover, the weak trends with $M_\rm{UV}$ and redshift can be fully explained by the remaining property dependencies. Expanding our analysis to multivariate regression, we determine that $W_\lambda(\rm{H\alpha})$ or $W_\lambda$([OIII]), along with $\beta_\rm{UV}$ and $E(B-V)_\rm{neb}$, are the most important observables for accurately predicting $\xi_\rm{ion,0}$. The latter identifies the most common outliers as SFGs with relatively high $E(B-V)_\rm{neb}\gtrsim0.5$, possibly indicative of obscured star-formation or strong differential attenuation. Combining these properties enable $\xi_\rm{ion,0}$ to be inferred with an accuracy of $\sim0.15\,$dex, with a population intrinsic scatter of $\sigma_\rm{int}\sim0.035\,$dex. |
| 2025-09-30 | [Signal-Aware Workload Shifting Algorithms with Uncertainty-Quantified Predictors](http://arxiv.org/abs/2509.26511v1) | Ezra Johnson, Adam Lechowicz et al. | A wide range of sustainability and grid-integration strategies depend on workload shifting, which aligns the timing of energy consumption with external signals such as grid curtailment events, carbon intensity, or time-of-use electricity prices. The main challenge lies in the online nature of the problem: operators must make real-time decisions (e.g., whether to consume energy now) without knowledge of the future. While forecasts of signal values are typically available, prior work on learning-augmented online algorithms has relied almost exclusively on simple point forecasts. In parallel, the forecasting research has made significant progress in uncertainty quantification (UQ), which provides richer and more fine-grained predictive information. In this paper, we study how online workload shifting can leverage UQ predictors to improve decision-making. We introduce $\texttt{UQ-Advice}$, a learning-augmented algorithm that systematically integrates UQ forecasts through a $\textit{decision uncertainty score}$ that measures how forecast uncertainty affects optimal future decisions. By introducing $\textit{UQ-robustness}$, a new metric that characterizes how performance degrades with forecast uncertainty, we establish theoretical performance guarantees for $\texttt{UQ-Advice}$. Finally, using trace-driven experiments on carbon intensity and electricity price data, we demonstrate that $\texttt{UQ-Advice}$ consistently outperforms robust baselines and existing learning-augmented methods that ignore uncertainty. |
| 2025-09-30 | [Nondestructive characterization of laser-cooled atoms using machine learning](http://arxiv.org/abs/2509.26479v1) | G. De Sousa, M. Doris et al. | We develop machine learning techniques for estimating physical properties of laser-cooled potassium-39 atoms in a magneto-optical trap using only the scattered light -- i.e., fluorescence -- that is intrinsic to the cooling process. In-situ snap-shot images of fluorescing atomic ensembles directly reveal the spatial structure of these millimeter-scale objects but contain no obvious information regarding internal properties such as the temperature. We first assembled and labeled a balanced dataset sampling $8\times10^3$ different experimental parameters that includes examples with: large and dense atomic ensembles, a complete absence of atoms, and everything in between. We describe a range of models trained to predict atom number and temperature solely from fluorescence images. These run the gamut from a poorly performing linear regression model based only on integrated fluorescence to deep neural networks that give number and temperature with fractional uncertainties of $0.1$ and $0.2$ respectively. |
| 2025-09-30 | [Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification](http://arxiv.org/abs/2509.26457v1) | Artur Barros, Carlos Caetano et al. | Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Î² spectrum](http://arxiv.org/abs/2509.26390v1) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete \beta decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full \beta decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of \beta particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision \beta decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {\sigma}) enhancement in \beta emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for \beta decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Î² spectrum](http://arxiv.org/abs/2509.26390v2) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete $\beta$ decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full $\beta$ decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of $\beta$ particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision $\beta$ decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {$\sigma$}) enhancement in $\beta$ emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for $\beta$ decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition](http://arxiv.org/abs/2509.26385v1) | Zejin Gao, Ksheera Sagar et al. | We consider the problem of fully Bayesian posterior estimation and uncertainty quantification in undirected Gaussian graphical models via Markov chain Monte Carlo (MCMC) under recently-developed element-wise graphical priors, such as the graphical horseshoe. Unlike the conjugate Wishart family, these priors are non-conjugate; but have the advantage that they naturally allow one to encode a prior belief of sparsity in the off-diagonal elements of the precision matrix, without imposing a structure on the entire matrix. Unfortunately, for a graph with $p$ nodes and with $n$ samples, the state-of-the-art MCMC approaches for the element-wise priors achieve a per iteration complexity of $O(p^4),$ which is prohibitive when $p\gg n$. In this regime, we develop a suitably reparameterized MCMC with per iteration complexity of $O(p^3)$, providing a one-order of magnitude improvement, and consequently bringing the computational cost at par with the conjugate Wishart family, which is also $O(p^3)$ due to a use of the classical Bartlett decomposition, but this decomposition does not apply outside the Wishart family. Importantly, the proposed benefit is obtained solely due to our reparameterization in an MCMC scheme targeting the true posterior, that reverses the recently developed telescoping block decomposition of Bhadra et al. (2024), in a suitable sense. There is no variational or any other approximate Bayesian computation scheme considered in this paper that compromises targeting the true posterior. Simulations and the analysis of a breast cancer data set confirm both the correctness and better algorithmic scaling of the proposed reverse telescoping sampler. |
| 2025-09-30 | [Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography](http://arxiv.org/abs/2509.26382v1) | Shijie Lin, Bin Hu et al. | Time-delay cosmography, by monitoring the multiply imaged gravitational lenses in the time domain, offers a promising and independent method for measuring cosmological distances. However, in addition to the main deflector that produces the multiple images, the large-scale structure along the line-of-sight (LoS) will also deflect the traveling light rays, known as weak lensing (WL). Due to resolution limitations, accurately measuring WL on arcsecond scales is highly challenging. In this work, we evaluate the LoS effects on both lensing images and time-delay measurements using a more straightforward, high-resolution N-body simulation that provides a more realistic matter distribution compared to the traditional, computationally cheaper halo rendering method. We employ the multi-plane ray tracing technique, which is traditionally utilized to compute WL effects at the arcminute scale, extending its application to the strong lensing regime at the arcsecond scale. We focus on the quadruple-image system and present the following findings: 1. In addition to a constant external convergence, large-scale structures within a region approximately 2 arcminutes in angular size act as external perturbers, inducing inhomogeneous fluctuations on the arcsecond scale; 2. These fluctuations cannot be fully accounted for by external shear alone, necessitating the inclusion of external flexion; 3. While incorporating flexion provides a reasonably good fit to the lensing image, the time-delay distance still exhibits a $6.2$\textperthousand~bias and a $2.5\%$ uncertainty. This underscores the limitations of the single-plane approximation, as time-delay errors accumulate along the LoS. |
| 2025-09-29 | [Safe Planning in Unknown Environments using Conformalized Semantic Maps](http://arxiv.org/abs/2509.25124v1) | David Smith Sundarsingh, Yifei Li et al. | This paper addresses semantic planning problems in unknown environments under perceptual uncertainty. The environment contains multiple unknown semantically labeled regions or objects, and the robot must reach desired locations while maintaining class-dependent distances from them. We aim to compute robot paths that complete such semantic reach-avoid tasks with user-defined probability despite uncertain perception. Existing planning algorithms either ignore perceptual uncertainty - thus lacking correctness guarantees - or assume known sensor models and noise characteristics. In contrast, we present the first planner for semantic reach-avoid tasks that achieves user-specified mission completion rates without requiring any knowledge of sensor models or noise. This is enabled by quantifying uncertainty in semantic maps - constructed on-the-fly from perceptual measurements - using conformal prediction in a model- and distribution-free manner. We validate our approach and the theoretical mission completion rates through extensive experiments, showing that it consistently outperforms baselines in mission success rates. |
| 2025-09-29 | [New insights from the flavor dependence of quark transverse momentum distributions in the pion](http://arxiv.org/abs/2509.25098v1) | Lorenzo Rossi, Alessandro Bacchetta et al. | We update our previous extraction of transverse momentum distributions of unpolarized quarks in the pion by implementing a more comprehensive description of theoretical uncertainties and, for the first time, by exploring possible differences among quark flavors. We extract such distributions from all available data for unpolarized pion-nucleus Drell-Yan processes, where the cross section is differential in the transverse momentum of the final lepton pair. The cross section involves transverse momentum distributions in the nucleon, that we consistently take from our previous studies. |
| 2025-09-29 | [Curriculum Imitation Learning of Distributed Multi-Robot Policies](http://arxiv.org/abs/2509.25097v1) | JesÃºs Roche, Eduardo SebastiÃ¡n et al. | Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements. |
| 2025-09-29 | [Finite-Size Security Bounds in Semi-Quantum Key Distribution: Spectral, Operator-Theoretic, and Entropic Perspectives](http://arxiv.org/abs/2509.25078v1) | Zahidur Rezwan Ratul | We study Semi-Quantum Key Distribution (SQKD) with a focus on finite-size security bounds, developed through three complementary perspectives. (i) Spectral disturbance: wrong-basis L\"uders updates produce closed-form spectra and purity loss, which serve as basis-independent indicators of disturbance. (ii) Operator-theoretic reduction: in Z/Z-sifted rounds, intercept-resend attacks can be represented as an effective depolarizing channel, characterized by a fidelity-QBER relation. (iii) Entropic trade-offs: Maassen-Uffink and memory-assisted uncertainty relations certify security through X tests and reflection rounds, even when the sifted QBER is low. The exposition provides step-by-step derivations supported by physically interpretable figures, and the framework concludes with finite-size estimates based on concentration inequalities that are suited for practical parameter estimation. |
| 2025-09-29 | [Confidence-Guided Error Correction for Disordered Speech Recognition](http://arxiv.org/abs/2509.25048v1) | Abner Hernandez, TomÃ¡s Arias Vergara et al. | We investigate the use of large language models (LLMs) as post-processing modules for automatic speech recognition (ASR), focusing on their ability to perform error correction for disordered speech. In particular, we propose confidence-informed prompting, where word-level uncertainty estimates are embedded directly into LLM training to improve robustness and generalization across speakers and datasets. This approach directs the model to uncertain ASR regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare our approach to both transcript-only fine-tuning and post hoc confidence-based filtering. Evaluations show that our method achieves a 10% relative WER reduction compared to naive LLM correction on the Speech Accessibility Project spontaneous speech and a 47% reduction on TORGO, demonstrating the effectiveness of confidence-aware fine-tuning for impaired speech. |
| 2025-09-29 | [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](http://arxiv.org/abs/2509.25034v1) | Heming Fu, Guojun Xiong et al. | As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management. |
| 2025-09-29 | [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](http://arxiv.org/abs/2509.25031v1) | Sophia V. Kuhn, Rafael Bischof et al. | Aging infrastructure portfolios pose a critical resource allocation challenge: deciding which structures require intervention and which can safely remain in service. Structural assessments must balance the trade-off between cheaper, conservative analysis methods and accurate but costly simulations that do not scale portfolio-wide. We propose Bayesian neural network (BNN) surrogates for rapid structural pre-assessment of worldwide common bridge types, such as reinforced concrete frame bridges. Trained on a large-scale database of non-linear finite element analyses generated via a parametric pipeline and developed based on the Swiss Federal Railway's bridge portfolio, the models accurately and efficiently estimate high-fidelity structural analysis results by predicting code compliance factors with calibrated epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware triage: flagging likely critical structures and providing guidance where refined analysis is pertinent. We demonstrate the framework's effectiveness in a real-world case study of a railway underpass, showing its potential to significantly reduce costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios. |
| 2025-09-29 | [The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools](http://arxiv.org/abs/2509.25028v1) | Eric Browne | Randomness plays a pivotal yet paradoxical role in computational music creativity: it can spark novelty, but unchecked chance risks incoherence. This paper presents a thematic review of contemporary AI music systems, examining how designers incorporate randomness and uncertainty into creative practice. I draw on the concept of structured uncertainty to analyse how stochastic processes are constrained within musical and interactive frameworks. Through a comparative analysis of six systems - Musika (Pasini and Schl\"uter, 2022), MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) - we identify recurring design patterns that support musical coherence, user control, and co-creativity. To my knowledge, this is the first thematic review examining randomness in AI music through structured uncertainty, offering practical insights for designers and artists aiming to support expressive, collaborative, or improvisational interactions. |
| 2025-09-29 | [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](http://arxiv.org/abs/2509.25017v1) | Spyros Kondylatos, Gustau Camps-Valls et al. | Wildfires are among the most severe natural hazards, posing a significant threat to both humans and natural ecosystems. The growing risk of wildfires increases the demand for forecasting models that are not only accurate but also reliable. Deep Learning (DL) has shown promise in predicting wildfire danger; however, its adoption is hindered by concerns over the reliability of its predictions, some of which stem from the lack of uncertainty quantification. To address this challenge, we present an uncertainty-aware DL framework that jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance short-term wildfire danger forecasting. In the next-day forecasting, our best-performing model improves the F1 Score by 2.3% and reduces the Expected Calibration Error by 2.1% compared to a deterministic baseline, enhancing both predictive skill and calibration. Our experiments confirm the reliability of the uncertainty estimates and illustrate their practical utility for decision support, including the identification of uncertainty thresholds for rejecting low-confidence predictions and the generation of well-calibrated wildfire danger maps with accompanying uncertainty layers. Extending the forecast horizon up to ten days, we observe that aleatoric uncertainty increases with time, showing greater variability in environmental conditions, while epistemic uncertainty remains stable. Finally, we show that although the two uncertainty types may be redundant in low-uncertainty cases, they provide complementary insights under more challenging conditions, underscoring the value of their joint modeling for robust wildfire danger prediction. In summary, our approach significantly improves the accuracy and reliability of wildfire danger forecasting, advancing the development of trustworthy wildfire DL systems. |
| 2025-09-29 | [Addressing Methodological Uncertainty in MCDM with a Systematic Pipeline Approach to Data Transformation Sensitivity Analysis](http://arxiv.org/abs/2509.24996v1) | Juan B. Cabral, Alvaro Roy Schachner | Multicriteria decision-making methods exhibit critical dependence on the choice of normalization techniques, where different selections can alter 20-40% of the final rankings. Current practice is characterized by the ad-hoc selection of methods without systematic robustness evaluation. We present a framework that addresses this methodological uncertainty through automated exploration of the scaling transformation space. The implementation leverages the existing Scikit-Criteria infrastructure to automatically generate all possible methodological combinations and provide robust comparative analysis. |
| 2025-09-26 | [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](http://arxiv.org/abs/2509.22633v1) | Gen Li, Yuling Yan | Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters. |
| 2025-09-26 | [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](http://arxiv.org/abs/2509.22612v1) | Jonne SÃ¤levÃ¤, Duygu Ataman et al. | In this paper, we introduce a set of resampling-based methods for quantifying uncertainty and statistical precision of evaluation metrics in multilingual and/or multitask NLP benchmarks. We show how experimental variation in performance scores arises from both model- and data-related sources, and that accounting for both of them is necessary to avoid substantially underestimating the overall variability over hypothetical replications. Using multilingual question answering, machine translation, and named entity recognition as example tasks, we also demonstrate how resampling methods are useful for computing sampling distributions for various quantities used in leaderboards such as the average/median, pairwise differences between models, and rankings. |
| 2025-09-26 | [Likelihood-free inference for gravitational-wave data analysis and public alerts](http://arxiv.org/abs/2509.22561v1) | Ethan Marx, Deep Chatterjee et al. | Rapid and reliable detection and dissemination of source parameter estimation data products from gravitational-wave events, especially sky localization, is critical for maximizing the potential of multi-messenger astronomy. Machine learning based detection and parameter estimation algorithms are emerging as production ready alternatives to traditional approaches. Here, we report validation studies of AMPLFI, a likelihood-free inference solution to low-latency parameter estimation of binary black holes. We use simulated signals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run (O3) to compare sky localization performance with BAYESTAR, the algorithm currently in production for rapid sky localization of candidates from matched-filter pipelines. We demonstrate sky localization performance, measured by searched area and volume, to be equivalent with BAYESTAR. We show accurate reconstruction of source parameters with uncertainties for use distributing low-latency coarse-grained chirp mass information. In addition, we analyze several candidate events reported by the LVK in the third gravitational-wave transient catalog (GWTC-3) and show consistency with the LVK's analysis. Altogether, we demonstrate AMPLFI's ability to produce data products for low-latency public alerts. |
| 2025-09-26 | [Event Generator Tuning as a Robustness Test](http://arxiv.org/abs/2509.22526v1) | Jean Wolfs, Chris M. Marshall | Neutrino oscillation experiments use Monte Carlo event generators to predict neutrino-nucleus interactions. Cross section uncertainties are typically implemented by varying the parameters of the model(s) used in the generator. We study the performance of two commonly-used model configurations of the GENIE generator (G18_10a_02_11a and AR23_0i_00_000) and their uncertainties by tuning parameters to cross section data, and then comparing the resulting tuned prediction to a suite of other measurements from T2K, MicroBooNE, and MINERvA. This reveals whether the model can simultaneously describe several datasets, as well as whether the uncertainties are adequately robust. We find that G18 and especially AR23 are reasonable in predicting lower-energy measurements from T2K and MicroBooNE, but unable to describe MINERvA data, and discuss the implications for short-baseline oscillation searches. We attempt to replicate a tuning procedure developed by MicroBooNE using several different measurements, and find substantially different results depending on which measurement is used, and that the MicroBooNE tune does not agree with other measurements. We conclude that the SBN experiment should not tune its generator to external data. |
| 2025-09-26 | [A high-stability optical clock based on a continuously ground-state cooled Al$^+$ ion without compromising its accuracy](http://arxiv.org/abs/2509.22525v1) | Fabian Dawel, Lennart Pelzer et al. | Single ion optical clocks have shown systematic frequency uncertainties below $10^{-18}$, but typically require more than one week of averaging to achieve a corresponding statistical uncertainty. This time can be reduced with longer probe times, but comes at the cost of a higher time-dilation shift due to motional heating of the ions in the trap. We show that sympathetic ground-state cooling using electromagnetically-induced transparency (EIT) of an \Al clock ion via a co-trapped \Ca ion during clock interrogation suppresses the heating of the ions. \Al can be kept close to the motional ground state, independent from the chosen interrogation time, at a relative time dilation shift of $(-1.69\pm0.20)\times10^{-18}$. The \Ca cooling light introduces an additional light shift on the \Al clock transition of $(-9.27\pm 1.03)\times10^{-18}$. We project that the uncertainty of this light shift can be further reduced by nearly an order of magnitude. This sympathetic cooling enables seconds of interrogation time with $10^{-19}$ motional and cooling laser-induced uncertainties for \Al and can be employed in other ion clocks as well. |
| 2025-09-26 | [Metric response of relative entropy: a universal indicator of quantum criticality](http://arxiv.org/abs/2509.22515v1) | Pritam Sarkar, Diptiman Sen et al. | The information-geometric origin of fidelity susceptibility and its utility as a universal probe of quantum criticality in many-body settings have been widely discussed. Here we explore the metric response of quantum relative entropy (QRE), by tracing out all but $n$ adjacent sites from the ground state of spin chains of finite length $N$, as a parameter of the corresponding Hamiltonian is varied. The diagonal component of this metric defines a susceptibility of the QRE that diverges at quantum critical points (QCPs) in the thermodynamic limit. We study two spin-$1/2$ models as examples, namely the integrable transverse field Ising model (TFIM) and a non-integrable Ising chain with three-spin interactions. We demonstrate distinct scaling behaviors for the peak of the QRE susceptibility as a function of $N$: namely a square logarithmic divergence in TFIM and a power-law divergence in the non-integrable chain. This susceptibility encodes uncertainty of entanglement Hamiltonian gradients and is also directly connected to other information measures such as Petz-R\'enyi entropies. We further show that this susceptibility diverges even at finite $N$ if the subsystem size, $n$, exceeds a certain value when the Hamiltonian is tuned to its classical limits due to the rank of the RDMs being finite; unlike the divergence associated with the QCPs which require $N \rightarrow \infty$. |
| 2025-09-26 | [A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random](http://arxiv.org/abs/2509.22499v1) | Yunshu Zhang, Chan Park et al. | Instrumental variable (IV) methods offer a valuable approach to account for outcome data missing not-at-random. A valid missing data instrument is a measured factor which (i) predicts the nonresponse process and (ii) is independent of the outcome in the underlying population. For point identification, all existing IV methods for missing data including the celebrated Heckman selection model, a priori restrict the extent of selection bias on the outcome scale, therefore potentially understating uncertainty due to missing data. In this work, we introduce an IV framework which allows the degree of selection bias on the outcome scale to remain completely unrestricted. The new approach instead relies for identification on (iii) a key multiplicative selection model, which posits that the instrument and any hidden common correlate of selection and the outcome, do not interact on the multiplicative scale. Interestingly, we establish that any regular statistical functional of the missing outcome is nonparametrically identified under (i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald ratio estimand in causal inference. For estimation and inference, we characterize the influence function for any functional defined on a nonparametric model for the observed data, which we leverage to develop semiparametric multiply robust IV estimators. Several extensions of the methods are also considered, including the important practical setting of polytomous and continuous instruments. Simulation studies illustrate the favorable finite sample performance of proposed methods, which we further showcase in an HIV study nested within a household health survey study we conducted in Mochudi, Botswana, in which interviewer characteristics are used as instruments to correct for selection bias due to dependent nonresponse in the HIV component of the survey study. |
| 2025-09-26 | [On an optimization framework for damage localization in structures](http://arxiv.org/abs/2509.22492v1) | Owais Saleem, Tim Suchan et al. | Efficient structural damage localization remains a challenge in structural health monitoring (SHM), particularly when the problem is coupled with uncertainty of conditions and complexity of structures. Traditional methods simply based on experimental data processing are often not sufficiently reliable, while complex models often struggle with computational inefficiency given the tremendous amount of model parameters. This paper focuses on closing the gap between data-driven SHM and physics-based model updating by offering a solution for real-world infrastructure. We first concentrate on fusing multi-source damage-sensitive features (DSF) based on experimental modal data into spatially mapped belief masses to pre-screen candidate damage locations. The resulting candidate damage locations are integrated into an inverse Finite Element method (FEM) model calibration process. We propose an optimization framework to identify the most probable damage scenario with single and multi-damage cases. We present the corresponding numerical results in this paper, which open the door to extend the application of the framework to a complex real bridge structure. |
| 2025-09-26 | [BÃ©zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](http://arxiv.org/abs/2509.22476v1) | Chen Li, Meilong Xu et al. | Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, B\'ezier Meets Diffusion, for cross-domain image generation. First, we introduce a B\'ezier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance. |
| 2025-09-26 | [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](http://arxiv.org/abs/2509.22469v1) | Ben Rossano, Jaein Lim et al. | This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation. |
| 2025-09-25 | [Efficient Digital Methods to Quantify Sensor Output Uncertainty](http://arxiv.org/abs/2509.21311v1) | Orestis Kaparounakis, Phillip Stanley-Marbell | Accurate characterization of sensor output uncertainty is important for reliable data interpretation in many applications. Here, we investigate the impact of transducer-level measurement uncertainty on overall sensor measurement accuracy due to limited-precision information about sensor components. We explain our method using thermopile-based sensors as an example class of sensors. We show how sensor calibration and conversion equations, which are an essential part of all sensing systems, propagate uncertainties resulting from the quantization of calibration parameters, to the final, compensated sensor output. The experimental results show that the epistemic uncertainty of calibration-related quantities leads to absolute error in the sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for one commonly-used thermopile sensor. In one instance of using the epistemic uncertainty information in edge detection, we show reduction of false-positives edges to zero for the conventional Canny operator, while maintaining accuracy. We show these ideas are practical and possible on actual embedded sensor systems by prototyping them on two commercially-available uncertainty tracking hardware platforms, one with average power dissipation 16.7 mW and 42.9x speedup compared to the equal-confidence Monte Carlo computation (the status quo), and the other with average power dissipation 147.15 mW and 94.4x speedup, paving the way for use in real time. |
| 2025-09-25 | [Emission line tracers of galactic outflows driven by stellar feedback in simulations of isolated disk galaxies](http://arxiv.org/abs/2509.21295v1) | Elliot L. Howatson, Alexander J. Richings et al. | Hydrodynamic simulations can connect outflow observables to the physical conditions of outflowing gas. Here, we use simulations of isolated disk galaxies ranging from dwarf mass ($M_{200} = 10^{10}\mathrm{M}_{\odot}$) to Milky Way mass ($M_{200} = 10^{12}\mathrm{M}_{\odot}$), based on the FIRE-2 subgrid models to investigate multiphase galactic outflows. We use the CHIMES non-equilibrium chemistry module to create synthetic spectra of common outflow tracers ([CII]$_{158\rm{\mu m}}$, $\mathrm{CO}_{J(1-0)}$, H$\alpha$ and $[\mathrm{OIII}]_{5007\text{A}}$). Using our synthetic spectra we measure the mass outflow rate, kinetic power and momentum flux using observational techniques. In [CII]$_{158\rm{\mu m}}$ we measure outflow rates of $10^{-4}$ to $1$ $\mathrm{M_{\odot}yr^{-1}}$ across an SFR range of $10^{-3}$ to $1$ $\text{M}_{\odot}\text{yr}^{-1}$, which is in reasonable agreement with observations. The significant discrepancy is in $\mathrm{CO}_{J(1-0)}$, with the simulations lying $\approx1$ dex below the observational sample. We test observational assumptions used to derive outflow properties from synthetic spectra. We find the greatest uncertainty lies in measurements of electron density, as estimates using the SII doublet can overestimate the actual electron density by up to 2 dex, which changes mass outflow rates by up to 4 dex. We also find that molecular outflows are especially sensitive to the conversion factor between CO luminosity and H2 mass, with outflow rates changing by up to 4 dex in our least massive galaxy. Comparing the outflow properties derived from the synthetic spectra to those derived directly from the simulation, we find that [CII]$_{158\rm{\mu m}}$ probes outflows at greater distances from the disk, whilst we find that molecular gas does not survive at large distances within outflows within our modestly star-forming disk galaxies simulated in this work. |
| 2025-09-25 | [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](http://arxiv.org/abs/2509.21210v1) | Ali Kafili Gavgani, Amin Talaeizadeh et al. | Conventional multi-rotors are under-actuated systems, hindering them from independently controlling attitude from position. In this study, we present several distinct configurations that incorporate additional control inputs for manipulating the angles of the propeller axes. This addresses the mentioned limitations, making the systems "omniorientational". We comprehensively derived detailed dynamic models for all introduced configurations and validated by a methodology using Simscape Multibody simulations. Two controllers are designed: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the "Power Consumption Factor" defined in this study. Moreover, the controllers effectively managed harsh disturbances and uncertainties. Simulations compare and analyze the proposed configurations and controllers, majorly considering their power consumption. Furthermore, we conduct a qualitative comparison to evaluate the impact of different types of uncertainties on the control system, highlighting areas for potential model or hardware improvements. The analysis in this study provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design. This research aligns with the project SAC-1, one of the objectives of Sharif AgRoLab. |
| 2025-09-25 | [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](http://arxiv.org/abs/2509.21201v1) | Yang Fu, Peng Qin et al. | The vision of 6G networks aims to enable edge inference by leveraging ubiquitously deployed artificial intelligence (AI) models, facilitating intelligent environmental perception for a wide range of applications. A critical operation in edge inference is for an edge node (EN) to aggregate multi-view sensory features extracted by distributed agents, thereby boosting perception accuracy. Over-the-air computing (AirComp) emerges as a promising technique for rapid feature aggregation by exploiting the waveform superposition property of analog-modulated signals, which is, however, incompatible with existing digital communication systems. Meanwhile, hybrid reconfigurable intelligent surface (RIS), a novel RIS architecture capable of simultaneous signal amplification and reflection, exhibits potential for enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital AirComp (HRD-AirComp) scheme, which employs vector quantization to map high-dimensional features into discrete codewords that are digitally modulated into symbols for wireless transmission. By judiciously adjusting the AirComp transceivers and hybrid RIS reflection to control signal superposition across agents, the EN can estimate the aggregated features from the received signals. To endow HRD-AirComp with a task-oriented design principle, we derive a surrogate function for inference accuracy that characterizes the impact of feature quantization and over-the-air aggregation. Based on this surrogate, we formulate an optimization problem targeting inference accuracy maximization, and develop an efficient algorithm to jointly optimize the quantization bit allocation, agent transmission coefficients, EN receiving beamforming, and hybrid RIS reflection beamforming. Experimental results demonstrate that the proposed HRD-AirComp outperforms baselines in terms of both inference accuracy and uncertainty. |
| 2025-09-25 | [The depletion of the asteroid belt and the impact history of the Earth](http://arxiv.org/abs/2509.21194v1) | Julio A. Fernandez | We have evaluated the rate at which the asteroid belt is losing material, and how it splits between macroscopic bodies and meteoritic dust. The mass loss process is due to the injection of asteroid fragments into unstable dynamical regions, associated to mean-motion resonances with Jupiter, Saturn and Mars or secular resonances, from where they are scattered either to the region of the terrestrial planets or to the vicinity of Jupiter's orbit. Asteroid fragments that do not escape from the belt are ground down by mutual collisions to meteoritic dust. Under the assumption that 25\% of the zodiacal dust mass is of asteroidal origin, we find that the asteroid belt is currently losing a fraction of about $\mu_o \simeq 8.8 \times 10^{-5}$ Ma$^{-1}$ of its collisionally-active mass (without the primordial objects Ceres, Vesta and Pallas), about 20\% as macroscopic bodies, and 80\% as dust particles that feed the zodiacal dust cloud. Extrapolation of the current mass loss rate to the past suggests only a moderate increase of the asteroid belt mass and the mass loss rate around 3.0 - 3.5 Ga ago (by about 50\% and a factor of two respectively). Yet, should the computed $\mu_o$ be somewhat underestimated owing to the different uncertainties associated to its computation, the extrapolation to the past would lead to quite different results. For instance, a moderate increase in the computed $\mu_o$, say by a factor of three, would lead to an exponential increase of the asteroid mass and mass loss rate about 3.5 Ga ago. A greater asteroid mass loss rate in the past should be correlated with a more intense impact rate of the Earth, Moon and the other terrestrial planets, which is indeed what suggests the geologic record (Hartmann 2007). |
| 2025-09-25 | [$Î›_{c}(2910)$ and $Î›_{c}(2940)$ productions in $p \bar{p}$ annihilation and $K^{-}p$ scattering processes](http://arxiv.org/abs/2509.21176v1) | Quan-Yun Guo, Dian-Yong Chen | In this work, we investigate the productions of $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ in the $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes by utilizing an effective Lagrangian approach, where both $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ are considered as $D^{\ast}N$ molecular states with $J^{P}=1/2^{-}$ and $3/2^{-}$, respectively. The cross sections and $D^0 p$ invariant mass distributions at exemplified center-of-mass energy for the involved processes are estimated. At $\sqrt{s}=10$ $\mathrm{GeV}$, our estimations indicate that the cross sections for $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes are $(2.1^{+5.6}_{-1.6})$ nb, $(2.2^{+5.9}_{-1.7})$ nb, and $(0.6^{+1.6}_{-0.4})$ nb, respectively, where the uncertainties are resulted from the variation of model parameter. Furthermore, Our estimations of the $D^{0}p$ invariant mass spectra reveal that the peak structure around 2.9 GeV primarily originates from $\Lambda_c(2910)$ across all three processes, however the contributions from $\Lambda_c(2940)$ makes the structure asymmetric. It is expected that the total cross sections and the $D^0 p$ invariant mass distributions estimated in the present work can be tested by future experiments at $\mathrm{\bar{P}ANDA}$ and J-PARC. |
| 2025-09-25 | [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](http://arxiv.org/abs/2509.21161v1) | Giuseppe Serra, Florian Buettner | Continual Learning (CL) is recently gaining increasing attention for its ability to enable a single model to learn incrementally from a sequence of new classes. In this scenario, it is important to keep consistent predictive performance across all the classes and prevent the so-called Catastrophic Forgetting (CF). However, in safety-critical applications, predictive performance alone is insufficient. Predictive models should also be able to reliably communicate their uncertainty in a calibrated manner - that is, with confidence scores aligned to the true frequencies of target events. Existing approaches in CL address calibration primarily from a data-centric perspective, relying on a single temperature shared across all tasks. Such solutions overlook task-specific differences, leading to large fluctuations in calibration error across tasks. For this reason, we argue that a more principled approach should adapt the temperature according to the distance to the current task. However, the unavailability of the task information at test time/during deployment poses a major challenge to achieve the intended objective. For this, we propose Distance-Aware Temperature Scaling (DATS), which combines prototype-based distance estimation with distance-aware calibration to infer task proximity and assign adaptive temperatures without prior task information. Through extensive empirical evaluation on both standard benchmarks and real-world, imbalanced datasets taken from the biomedical domain, our approach demonstrates to be stable, reliable and consistent in reducing calibration error across tasks compared to state-of-the-art approaches. |
| 2025-09-25 | [Normalizing Flows are Capable Visuomotor Policy Learning Models](http://arxiv.org/abs/2509.21073v1) | Simon Kristoffersson Lind, Jialong Li et al. | The field of general purpose robotics has recently embraced powerful probabilistic models, such as diffusion models, to model and learn complex behaviors. However, these models often come with significant trade-offs, namely high computational costs for inference and a fundamental inability to quantify output uncertainty. We argue that a model's trustworthiness, a critical factor for reliable, general-purpose robotics, is inherently linked to its ability to provide confidence measures.   In this work, we introduce Normalizing Flows Policy, a novel visuomotor policy learning model based on Normalizing Flows. We show that Normalizing Flows are a natural and powerful alternative to diffusion models, providing both a statistically sound measure of confidence and a highly efficient inference process. Through comprehensive experiments across four distinct simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves performance comparable to, and often surpassing, Diffusion Policy, and it does so not only with improved sample efficiency but also with up to 30 times faster inference. Additionally, our ablation study validates several key architectural and training techniques that enable Normalizing Flows to perform well in this domain. |
| 2025-09-25 | [A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data](http://arxiv.org/abs/2509.21041v1) | Maeve Upton, Eamonn Organ et al. | Accurate estimation of solar irradiance is essential for reliable modelling of solar photovoltaic (PV) power production. In Ireland's highly variable maritime climate, where ground-based measurement stations are sparsely distributed, selecting an appropriate solar irradiance dataset presents a significant challenge. This study introduces a novel Bayesian spatio-temporal modelling framework for predicting solar irradiance at hourly and sub-hourly (10-minute) resolutions across Ireland. Cross-validation demonstrates that our model is statistically robust across all temporal resolutions with hourly showing highest prediction precision whereas 10-minute resolution encounters higher errors but better uncertainty quantification. In separate evaluations, we compare our model against alternative data sources, including reanalysis datasets and nearest-station interpolation, and find that it consistently provides superior site-specific accuracy. At the hourly scale, our model outperforms ERA5 in agreement with ground-based observations. At the sub-hourly scale, 10-minute resolution estimates provide solar PV power outputs consistent with residential and industrial solar PV installations in Ireland. Beyond surpassing existing datasets, our model delivers full uncertainty quantification, scalability and the capacity for real-time implementation, offering a powerful tool for solar energy prediction and the estimation of losses due to overload clipping from inverter undersizing. |
| 2025-09-25 | [Study on Locomotive Epidemic Dynamics in a Stochastic Spatio-Temporal Simulation Model on a Multiplex Network](http://arxiv.org/abs/2509.21017v1) | H. M. Shadman Tabib, Jaber Ahmed Deedar et al. | This study presents an integrated approach to understanding epidemic dynamics through a stochastic spatio-temporal simulation model on a multiplex network, blending physical and informational layers. The physical layer maps the geographic movement of individuals, while the information layer tracks the spread of knowledge and health behavior via social interactions. We explore the interplay between physical mobility, information flow, and epidemic outcomes by simulating disease spread within this dual-structured network. Our model employs stochastic elements to mirror human behavior, mobility, and information dissemination uncertainties. Through simulations, we assess the impact of network structure, mobility patterns, and information spread speed on epidemic dynamics. The findings highlight the crucial role of effective communication in curbing disease transmission, even in highly mobile societies. Additionally, our agent-based simulation allows for real-time scenario analysis through a user interface, offering insights into leveraging physical and informational networks for epidemic control. This research sheds light on designing strategic interventions in complex social systems to manage disease outbreaks. |
| 2025-09-24 | [Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing](http://arxiv.org/abs/2509.20349v1) | Ramona Rubini, Siavash Khodakarami et al. | Accurate time-series forecasting for complex physical systems is the backbone of modern industrial monitoring and control. While deep learning models excel at capturing complex dynamics, currently, their deployment is limited due to physical inconsistency and robustness, hence constraining their reliability in regulated environments. We introduce process-informed forecasting (PIF) models for temperature in pharmaceutical lyophilization. We investigate a wide range of models, from classical ones such as Autoregressive Integrated Moving Average Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning architectures, including Kolmogorov-Arnold Networks (KANs). We compare three different loss function formulations that integrate a process-informed trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a Residual-Based Attention (RBA) mechanism. We evaluate all models not only for accuracy and physical consistency but also for robustness to sensor noise. Furthermore, we test the practical generalizability of the best model in a transfer learning scenario on a new process. Our results show that PIF models outperform their data-driven counterparts in terms of accuracy, physical plausibility and noise resilience. This work provides a roadmap for developing reliable and generalizable forecasting solutions for critical applications in the pharmaceutical manufacturing landscape. |
| 2025-09-24 | [Quantum speed limits based on Jensen-Shannon and Jeffreys divergences for general physical processes](http://arxiv.org/abs/2509.20347v1) | Jucelino Ferreira de Sousa, Diego Paiva Pires | We discuss quantum speed limits (QSLs) for finite-dimensional quantum systems undergoing a general physical process. These QSLs were obtained using two families of entropic measures, namely the square root of the Jensen-Shannon divergence, which in turn defines a faithful distance of quantum states, and the square root of the quantum Jeffreys divergence. The results apply to both closed and open quantum systems, and are evaluated in terms of the Schatten speed of the evolved state, as well as cost functions that depend on the smallest and largest eigenvalues of both initial and instantaneous states of the quantum system. To illustrate our findings, we focus on the unitary and nonunitary dynamics of mixed single-qubit states. In the first case, we obtain speed limits $\textit{\`{a} la}$ Mandelstam-Tamm that are inversely proportional to the variance of the Hamiltonian driving the evolution. In the second case, we set the nonunitary dynamics to be described by the noisy operations: depolarizing channel, phase damping channel, and generalized amplitude damping channel. We provide analytical results for the two entropic measures, present numerical simulations to support our results on the speed limits, comment on the tightness of the bounds, and provide a comparison with previous QSLs. Our results may find applications in the study of quantum thermodynamics, entropic uncertainty relations, and also complexity of many-body systems. |
| 2025-09-24 | [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](http://arxiv.org/abs/2509.20293v1) | Benjamin Feuer, Chiung-Yi Tseng et al. | LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md |
| 2025-09-24 | [4D Driving Scene Generation With Stereo Forcing](http://arxiv.org/abs/2509.20251v1) | Hao Lu, Zhuang Ma et al. | Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}. |
| 2025-09-24 | [$S_8$ from Tully-Fisher, fundamental plane, and supernova distances agree with Planck](http://arxiv.org/abs/2509.20235v1) | Richard Stiskalek | Peculiar velocity measurements constrain the parameter combination $f\sigma_8$, the product of the linear growth rate $f$ and the fluctuation amplitude $\sigma_8$. Under the approximation that $f$ is a monotonic function of $\Omega_{\rm m}$, this can be related to $S_8 \equiv \sigma_8 \sqrt{\Omega_{\rm m}/0.3}$, enabling direct comparison with weak lensing and cosmic microwave background results. We exploit this by using three classes of direct-distance tracers -- the Tully-Fisher relation, the fundamental plane, and Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical forward model jointly calibrates each distance indicator and a linear theory reconstruction of the local Universe. This is the first consistent Bayesian analysis to combine all three major classes of distance indicators within a common framework, enabling cross-checks of systematics across diverse galaxy populations. All three tracers yield consistent values of $S_8$ that are also in agreement with Planck. Our joint constraint is $S_8 = 0.819 \pm 0.030$, with the uncertainty dominated by the 2M++ galaxy field. These results demonstrate that peculiar velocity surveys provide a robust, consistent measurement of $S_8$, and support concordance with the cosmic microwave background. |
| 2025-09-24 | [InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection](http://arxiv.org/abs/2509.20140v1) | Zongyi Li, Junchuan Zhao et al. | Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence/Arousal/Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis. |
| 2025-09-24 | [Causal Understanding by LLMs: The Role of Uncertainty](http://arxiv.org/abs/2509.20088v1) | Oscar Lithgow-Serrano, Vani Kanjirangat et al. | Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining. |
| 2025-09-24 | [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](http://arxiv.org/abs/2509.20077v1) | Xun Li, Rodrigo Santa Cruz et al. | To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution. We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments. |
| 2025-09-24 | [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](http://arxiv.org/abs/2509.20065v1) | Maggie Mi, Aline Villavicencio et al. | Language models often struggle with idiomatic, figurative, or context-sensitive inputs, not because they produce flawed outputs, but because they misinterpret the input from the outset. We propose an input-only method for anticipating such failures using token-level likelihood features inspired by surprisal and the Uniform Information Density hypothesis. These features capture localized uncertainty in input comprehension and outperform standard baselines across five linguistically challenging datasets. We show that span-localized features improve error detection for larger models, while smaller models benefit from global patterns. Our method requires no access to outputs or hidden activations, offering a lightweight and generalizable approach to pre-generation error prediction. |
| 2025-09-24 | [Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors](http://arxiv.org/abs/2509.20059v1) | Koki Kanzaki, Koya Sato | This paper proposes a high-accuracy radio map construction method tailored for environments where location information is affected by bursty errors. Radio maps are an effective tool for visualizing wireless environments. Although extensive research has been conducted on accurate radio map construction, most existing approaches assume noise-free location information during sensing. In practice, however, positioning errors ranging from a few to several tens of meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring such errors during inference can lead to significant degradation in radio map accuracy. This study highlights that these errors often tend to be biased when using mobile devices as sensors. We introduce a novel framework that models these errors together with spatial correlation in radio propagation by embedding them as tunable parameters in the marginal log-likelihood function. This enables ex-post calibration of location uncertainty during radio map construction. Numerical results based on practical human mobility data demonstrate that the proposed method can limit RMSE degradation to approximately 0.25-0.29 dB, compared with Gaussian process regression using noise-free location data, whereas baseline methods suffer performance losses exceeding 1 dB. |
| 2025-09-23 | [Recalibration of the H$Î±$ surface brightness-radius relation for planetary nebulae using Gaia DR3: new distances and the Milky Way oxygen radial gradient](http://arxiv.org/abs/2509.19239v1) | Oscar Cavichia, Hektor Monteiro et al. | The spatial distribution of chemical elements in the Galactic disk provides key constraints on models of galaxy evolution. However, studies using planetary nebulae (PNe) as tracers have been historically limited by large uncertainties in their distances. To overcome the long-standing distance uncertainties, we recalibrated the H$\alpha$ surface brightness-radius relation (Frew et al. 2016) with Gaia DR3 parallaxes, deriving statistical distances for 1,200 PNe and Bayesian distances for 419 objects with reliable parallaxes. Adopting Bayesian values preferentially, we determined the O/H radial gradient for 230 disk PNe. We tested three models: a single linear gradient, a segmented fit with one break, and a segmented fit with two breaks. Although model selection is statistically inconclusive, segmented fits indicate a change in slope near the solar radius ($R \sim 8$ kpc), with a flatter or slightly positive gradient inward and a steeper negative gradient outward. This feature may reflect changes in star formation efficiency driven by the Galactic bar or the corotation resonance of the spiral arms. Comparison with other tracers - Cepheids, red giants, and open clusters - shows qualitative consistency. The two-dimensional O/H distribution in the Galactic plane supports the adopted distances and reveals modest azimuthal asymmetry, with enhanced abundances near the bar at positive longitudes, and a bimodal abundance structure between the inner and outer solar regions. Our results provide new constraints on the chemical evolution of the Milky Way, the impact of non-axisymmetric structures, and the possible existence of distinct radial abundance regimes across the Galactic disk. |
| 2025-09-23 | [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](http://arxiv.org/abs/2509.19233v1) | Milad Leyli-abadi, Antoine Marot et al. | In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page. |
| 2025-09-23 | [A decentralized future for the open-science databases](http://arxiv.org/abs/2509.19206v1) | Gaurav Sharma, Viorel Munteanu et al. | Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come. |
| 2025-09-23 | [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](http://arxiv.org/abs/2509.19185v1) | Mohammed Mehedi Hasan, Hao Li et al. | Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents. |
| 2025-09-23 | [Bayesian Neural Networks versus deep ensembles for uncertainty quantification in machine learning interatomic potentials](http://arxiv.org/abs/2509.19180v1) | Riccardo Farris, Emanuele Telari et al. | Neural-network-based machine learning interatomic potentials have emerged as powerful tools for predicting atomic energies and forces, enabling accurate and efficient simulations in atomistic modeling. A key limitation of traditional deep learning approaches, however, is their inability to provide reliable estimates of predictive uncertainty. Such uncertainty quantification is critical for assessing model reliability, especially in materials science, where often the model is applied on out-of-distribution data. Different strategies have been proposed to address this challenge, with deep ensembles and Bayesian neural networks being among the most widely used. In this work, we introduce an implementation of Bayesian neural networks with variational inference in the aenet-PyTorch framework. To evaluate their applicability to machine learning interatomic potentials, we systematically compare the performance of variational BNNs and deep ensembles on a dataset of 7,815 TiO$_{2}$ structures. The models are trained on both the full dataset and a subset to assess how variations in data representation influence predictive accuracy and uncertainty estimation. This analysis provides insights into the strengths and limitations of each approach, offering practical guidance for the development of uncertainty-aware machine learning interatomic potentials. |
| 2025-09-23 | [Intrinsic Heisenberg Lower Bounds on Schwarzschild and Weyl-Class Spacelike Slices](http://arxiv.org/abs/2509.19099v1) | Thomas SchÃ¼rmann | We establish a coordinate-invariant Heisenberg-type lower bound for quantum states strictly localized in geodesic balls of radius $r_g$ on horizon-regular spacelike slices of static, spherically symmetric, asymptotically flat (AF) black-holes. Via a variance-eigenvalue equivalence the momentum uncertainty reduces to the first Dirichlet eigenvalue of the Laplace-Beltrami operator, yielding a slice-uniform Hardy baseline $\sigma_p r_g \ge \hbar/2$ under mild convexity assumptions on the balls; the bound is never attained and admits a positive gap both on compact interior regions and uniformly far out. For the Schwarzschild Painlev\'e-Gullstrand (PG) slice, whose induced 3-geometry is Euclidean, one recovers the exact Euclidean scale $\sigma_p r_g \ge \pi\hbar$, which is optimal among all admissible slices. The entire construction extends across the black-hole horizon, and it transfers to the static axisymmetric Weyl class, where the Hardy floor, strict gap, and AF $\pi$-scale persist (a global PG-like optimum need not exist). |
| 2025-09-23 | [Hybrid Adaptive Robust Stochastic Optimization Model for the Design of a Photovoltaic Battery Energy Storage System](http://arxiv.org/abs/2509.19054v1) | Alba Lun Mora Pous, Fernando Garcia-MuÃ±oz et al. | Future energy projections and their inherent uncertainty play a key role in the design of photovoltaic-battery energy storage systems (PV-BESS) for household use. In this study, both stochastic and robust optimization techniques are simultaneously integrated into a Hybrid Adaptive Robust-Stochastic Optimization (HARSO) model. Uncertainty in future PV generation is addressed using a stochastic approach, while uncertainty in power demand is handled through robust optimization. To solve the tri-level structure emerging from the hybrid approach, a Column-and-Constraint Generation (CCG) algorithm is implemented. The model also accounts for battery degradation by considering multiple commercially available battery chemistries, enabling a more realistic evaluation of long-term system costs and performance. To demonstrate its applicability, the model is applied to a case study involving the optimal design of a PV-BESS system for a household in Spain. The empirical analysis includes both first-life (FL) and second-life (SL) batteries with different chemistries, providing a comprehensive evaluation of design alternatives under uncertainty. Results indicate that the optimal solution is highly dependent on the level of robustness considered, leading to a shift in design strategy. Under less conservative settings, robustness is achieved by increasing battery capacity, while higher levels of conservatism favor expanding PV capacity to meet demand. |
| 2025-09-23 | [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](http://arxiv.org/abs/2509.18998v1) | Christina Schenk, Jacobo Ayensa JimÃ©nez et al. | Computational models provide crucial insights into complex biological processes such as cancer evolution, but their mechanistic nature often makes them nonlinear and parameter-rich, complicating calibration. We systematically evaluate parameter probability distributions in cell migration models using Bayesian calibration across four complementary strategies: parametric and surrogate models, each with and without explicit model discrepancy. This approach enables joint analysis of parameter uncertainty, predictive performance, and interpretability. Applied to a real data experiment of glioblastoma progression in microfluidic devices, surrogate models achieve higher computational efficiency and predictive accuracy, whereas parametric models yield more reliable parameter estimates due to their mechanistic grounding. Incorporating model discrepancy exposes structural limitations, clarifying where model refinement is necessary. Together, these comparisons offer practical guidance for calibrating and improving computational models of complex biological systems. |
| 2025-09-23 | [Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints](http://arxiv.org/abs/2509.18988v1) | Ziliang Lyu, Miroslav Krstic et al. | This paper considers the problem of adaptively overriding unsafe actions of a nominal controller in the presence of high-relative-degree nonovershooting constraints and parametric uncertainties. To prevent the design from being coupled with high-order derivatives of the parameter estimation error, we adopt a modular design approach in which the controller and the parameter identifier are designed separately. The controller module ensures that any safety violations caused by parametric uncertainties remain bounded, provided that the parameter estimation error and its first-order derivative are either bounded or square-integrable. The identifier module, in turn, guarantees that these requirements on the parameter estimation error are satisfied. Both theoretical analysis and simulation results demonstrate that the closed-loop safety violation is bounded by a tunable function of the initial estimation error. Moreover, as time increases, the parameter estimate converges to the true value, and the amount of safety violation decreases accordingly. |
| 2025-09-23 | [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](http://arxiv.org/abs/2509.18954v1) | Minoo Dolatabadi, Fardin Ayar et al. | LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map, which may not always be available, or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty. In this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness. |
| 2025-09-22 | [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](http://arxiv.org/abs/2509.18090v1) | Jiahe Li, Jiawei Zhang et al. | Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR. |
| 2025-09-22 | [Robust, Online, and Adaptive Decentralized Gaussian Processes](http://arxiv.org/abs/2509.18011v1) | Fernando Llorente, Daniel Waxman et al. | Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling. |
| 2025-09-22 | [Cosmic inventory of the background fields of relativistic particles in the Universe](http://arxiv.org/abs/2509.17954v1) | Jonathan Biteau | The extragalactic background is composed of the emission from all astrophysical sources, both resolved and unresolved, in addition to any diffuse components. In the last decade, there has been significant progress in our understanding of the cosmic history of extragalactic emissions associated with stellar evolution and accretion onto supermassive black holes, largely enabled by the extensive body of multi-wavelength data. The brightness of the extragalactic sky is now measured in photons, neutrinos, and cosmic rays, using observatories on the ground, in the sea, and in the ice, satellites in Earth orbit, and probes at the edge of the solar system. This wealth of disparate data is essential to unraveling the mysteries of the source populations that contribute to the extragalactic background.   In this contribution, we present an open database containing the most comprehensive collection of measurements of the extragalactic background spectrum to date. The combination of multi-messenger measurements over 27 frequency decades allows us to estimate the energy density of most extragalactic background components with an uncertainty of less than 30%. We explore the consistency of this cosmic inventory of the observed fields of relativistic particles populating the Universe with the cosmic history of star formation and accretion around supermassive black holes. Models incorporating these cosmic histories, as well as the redshift-dependent luminosity functions of extragalactic sources, currently match the electromagnetic component of the extragalactic background spectrum over 14 frequency decades, from the near UV to sub-TeV gamma rays. The knowledge gained from synthetic population models in the electromagnetic bands may become a crucial tool for understanding the origin of the most energetic extragalactic messengers, neutrinos and ultrahigh-energy cosmic rays. |
| 2025-09-22 | [Deep Hierarchical Learning with Nested Subspace Networks](http://arxiv.org/abs/2509.17874v1) | Paulius Rauba, Mihaela van der Schaar | Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models. |
| 2025-09-22 | [Modeling Scintillation Photon Transport and Reconstruction Algorithms for the Time-of-Flight Detector in the T2K Neutrino Experiment](http://arxiv.org/abs/2509.17860v1) | C. Alt, A. Blanchet et al. | The T2K ND280 upgrade aims to reduce the systematic uncertainty of the CP-violating phase, $\delta_{CP}$, to reject non-CP violation hypothesis at $3\sigma$ confidence level. A crucial component of the ND280 upgrade, alongside the Super Fine Grained Detector (SuperFGD) and two High-Angle Time Projection Chambers (TPCs), is the Time-of-Flight (ToF) detector, which significantly enhances background rejection and particle identification capabilities. The ToF detector features six modules in a cube configuration, each with 20 plastic scintillator bars measuring $\text{220}\times\text{12}\times\text{1}\,\text{cm}^3$ and is equipped with Silicon Photomultiplier (SiPM) arrays at both ends to capture scintillation light. This letter outlines the modelling of the detector response and the signal reconstruction process. |
| 2025-09-22 | [How Realistic are Idealized Copper Surfaces? A Machine Learning Study of Rough Copper-Water Interfaces](http://arxiv.org/abs/2509.17833v1) | Linus C. Erhard, Johannes SchÃ¶rghuber et al. | Copper is a highly promising catalyst for the electrochemical CO$_2$ reduction reaction (CO2RR) since it is the only pure metal that can form highly added-value products such as ethylene and ethanol. Since the CO2RR takes place in aqueous solution, the detailed atomic structure of the water-copper interface is essential for unraveling the key reaction mechanisms. In this study, we investigate copper-water interfaces exhibiting nanometer-scale roughnesses. We introduce two molecular dynamics protocols to create rough copper surfaces, which are subsequently brought into contact with water. From these interfaces, we sample additional training configurations from machine-learning-interatomic-potential-driven molecular dynamics simulations containing hundreds of thousands of atoms. An active learning workflow is developed to identify regions with high spatially resolved uncertainty and convert them into DFT-feasible cells through a modified amorphous matrix embedding approach. Finally, we analyze the local environments at the interface using unsupervised machine-learning techniques. Unique environments emerge on the rough copper surfaces absent from model systems, including stacking-fault-induced configurations and undercoordinated corner atoms. Notably, corner atoms consistently feature chemisorbed water molecules in our simulations, indicating their potential importance in catalytic processes. |
| 2025-09-22 | [On Fast Attitude Filtering Based on Matrix Fisher Distribution with Stability Guarantee](http://arxiv.org/abs/2509.17827v1) | Shijie Wang, Haichao Gui et al. | This paper addresses two interrelated problems of the nonlinear filtering mechanism and fast attitude filtering with the matrix Fisher distribution (MFD) on the special orthogonal group. By analyzing the distribution evolution along Bayes' rule, we reveal two essential properties that enhance the performance of Bayesian attitude filters with MFDs, particularly in challenging conditions, from a theoretical viewpoint.   Benefiting from the new understanding of the filtering mechanism associated with MFDs, two closed-form filters with MFDs is then proposed. These filters avoid the burdensome computations in previous MFD-based filters by introducing linearized error systems with right-invariant errors but retaining the two advantageous properties. Moreover, we leverage the two properties and closed-form filtering iteration to prove the almost-global exponential stability of the proposed filter with right-invariant error for the single-axis rotation, which, to our knowledge, is not achieved by existing directional statistics-based filters. Numerical simulations demonstrate that the proposed filters are significantly more accurate than the classic invariant Kalman filter. Besides, they are also as accurate as recent MFD-based Bayesian filters in challenging circumstances with large initial error and measurement uncertainty but consumes far less computation time (about 1/5 to 1/100 of previous MFD-based attitude filters). |
| 2025-09-22 | [RoboSeek: You Need to Interact with Your Objects](http://arxiv.org/abs/2509.17783v1) | Yibo Peng, Jiahao Yang et al. | Optimizing and refining action execution through   exploration and interaction is a promising way for robotic   manipulation. However, practical approaches to interaction driven robotic learning are still underexplored, particularly for   long-horizon tasks where sequential decision-making, physical   constraints, and perceptual uncertainties pose significant chal lenges. Motivated by embodied cognition theory, we propose   RoboSeek, a framework for embodied action execution that   leverages interactive experience to accomplish manipulation   tasks. RoboSeek optimizes prior knowledge from high-level   perception models through closed-loop training in simulation   and achieves robust real-world execution via a real2sim2real   transfer pipeline. Specifically, we first replicate real-world   environments in simulation using 3D reconstruction to provide   visually and physically consistent environments., then we train   policies in simulation using reinforcement learning and the   cross-entropy method leveraging visual priors. The learned   policies are subsequently deployed on real robotic platforms   for execution. RoboSeek is hardware-agnostic and is evaluated   on multiple robotic platforms across eight long-horizon ma nipulation tasks involving sequential interactions, tool use, and   object handling. Our approach achieves an average success rate   of 79%, significantly outperforming baselines whose success   rates remain below 50%, highlighting its generalization and   robustness across tasks and platforms. Experimental results   validate the effectiveness of our training framework in complex,   dynamic real-world settings and demonstrate the stability of the   proposed real2sim2real transfer mechanism, paving the way for   more generalizable embodied robotic learning. Project Page:   https://russderrick.github.io/Roboseek/ |
| 2025-09-22 | [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](http://arxiv.org/abs/2509.17726v1) | Javier Bisbal, Patrick Winter et al. | Accurate anatomical labeling of intracranial arteries is essential for cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming and subject to interoperator variability. We present a deep learning-based framework for automated artery labeling from 3D Time-of-Flight Magnetic Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating uncertainty quantification to enhance interpretability and reliability. We evaluated three convolutional neural network architectures: (1) a UNet with residual encoder blocks, reflecting commonly used baselines in vascular labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and spatial attention mechanisms for enhanced curvilinear structure recognition; and (3) nnUNet, a self-configuring framework that automates preprocessing, training, and architectural adaptation based on dataset characteristics. Among these, nnUNet achieved the highest labeling performance (average Dice score: 0.922; average surface distance: 0.387 mm), with improved robustness in anatomically complex vessels. To assess predictive confidence, we implemented test-time augmentation (TTA) and introduced a novel coordinate-guided strategy to reduce interpolation errors during augmented inference. The resulting uncertainty maps reliably indicated regions of anatomical ambiguity, pathological variation, or manual labeling inconsistency. We further validated clinical utility by comparing flow velocities derived from automated and manual labels in co-registered 4D Flow MRI datasets, observing close agreement with no statistically significant differences. Our framework offers a scalable, accurate, and uncertainty-aware solution for automated cerebrovascular labeling, supporting downstream hemodynamic analysis and facilitating clinical integration. |
| 2025-09-22 | [Enhanced fill probability estimates in institutional algorithmic bond trading using statistical learning algorithms with quantum computers](http://arxiv.org/abs/2509.17715v1) | Axel Ciceri, Austin Cottrell et al. | The estimation of fill probabilities for trade orders represents a key ingredient in the optimization of algorithmic trading strategies. It is bound by the complex dynamics of financial markets with inherent uncertainties, and the limitations of models aiming to learn from multivariate financial time series that often exhibit stochastic properties with hidden temporal patterns. In this paper, we focus on algorithmic responses to trade inquiries in the corporate bond market and investigate fill probability estimation errors of common machine learning models when given real production-scale intraday trade event data, transformed by a quantum algorithm running on IBM Heron processors, as well as on noiseless quantum simulators for comparison. We introduce a framework to embed these quantum-generated data transforms as a decoupled offline component that can be selectively queried by models in low-latency institutional trade optimization settings. A trade execution backtesting method is employed to evaluate the fill prediction performance of these models in relation to their input data. We observe a relative gain of up to ~ 34% in out-of-sample test scores for those models with access to quantum hardware-transformed data over those using the original trading data or transforms by noiseless quantum simulation. These empirical results suggest that the inherent noise in current quantum hardware contributes to this effect and motivates further studies. Our work demonstrates the emerging potential of quantum computing as a complementary explorative tool in quantitative finance and encourages applied industry research towards practical applications in trading. |
| 2025-09-19 | [CAPOS: The bulge Cluster APOGEE Survey VI. Characterizing multiple stellar populations and chemical abundances in the bulge globular cluster NGC 6569](http://arxiv.org/abs/2509.16168v1) | NicolÃ¡s Barrera, Sandro Villanova et al. | Context. The CAPOS project aims to obtain accurate mean abundances and radial velocities for many elements, and it explores the multiple population (MP) phenomenon in Galactic bulge globular clusters (BGCs). NGC 6569 is one of the CAPOS targets. Aims. This study provides a detailed high-resolution spectroscopic analysis of NGC 6569 to derive precise abundances for elements of different nucleosynthetic origins and to unveil its MPs by focusing on key spectral features. Methods. We analyzed APOGEE-2 near-infrared spectra of 11 giant members with the BACCHUS code, deriving abundances for 12 elements (C, N, O, Mg, Si, Ca, Ti, Fe, Ni, Al, Ce, Nd). Isochrone fitting with Gaia+2MASS photometry was used to estimate atmospheric parameters, cluster distance, and extinction. Results. We obtained [Fe/H] = -0.91 $\pm$ 0.06, consistent with APOGEE pipeline values; the scatter lies within uncertainties. The cluster shows [$\alpha$/Fe] = 0.36 $\pm$ 0.06 dex, similar to other GCs. Al appears homogeneous, while N is strongly enriched ([N/Fe] = 0.68 $\pm$ 0.34) with a spread of 0.90 dex, yielding two populations anticorrelated in C. The n-capture elements Ce and Nd are overabundant compared to the Sun but consistent with GCs of similar metallicity, with $\langle$[Ce/Nd]$\rangle$ = -0.17 $\pm$ 0.12. We also measure RV = -49.75 $\pm$ 3.68 km s$^{-1}$, consistent with previous works, while d$_\odot$ = 12.4 $\pm$ 1.45 kpc and E(B-V) = 0.68 are both higher than literature values. Conclusions. MPs in NGC 6569 are confirmed through a clear C-N anticorrelation. The cluster shows [$\alpha$/Fe] enhancement from Type II SNe and no Mg-Al-Si anticorrelation, suggesting rapid and homogeneous formation. The $\langle$[Ce/Nd]$\rangle$ ratio points to contributions from r-process events such as neutron star mergers, while overall Ce and Nd abundances are reported here for the first time in this cluster. |
| 2025-09-19 | [Lensed stars in galaxy-galaxy strong lensing -- a JWST prediction for the Cosmic Horseshoe](http://arxiv.org/abs/2509.16154v1) | Sung Kei Li, Luke Weisenbach et al. | We explore for the first time the possibility of detecting lensed star transients in galaxy-galaxy strong lensing systems upon repeated, deep imaging using the {\it James-Webb Space Telescope} ({\it JWST}). Our calculation predicts that the extremely high recent star formation rate of $\sim 100\,M_{\odot}\textrm{yr}^{-1}$ over the last 50 Myr (not accounting for image multiplicity) in the ``Cosmic Horseshoe'' lensed system ($z = 2.381$) generates many young, bright stars, of which their large abundance is expected to lead to a detection rate of $\sim 60$ transients per pointing in {\it JWST} observations with a $5\sigma$ limiting magnitude of $\sim 29\,m_{AB}$. With the high expected detection rate and little room for uncertainty for the lens model compared with cluster lenses, our result suggests that the Cosmic Horseshoe could be an excellent tool to test the nature of Dark Matter based on the spatial distribution of transients, and can be used to constrain axion mass if Dark Matter is constituted of ultra-light axions. We also argue that the large distance modulus of $\sim46.5\,$mag at $z \approx 2.4$ can act as a filter to screen out less massive stars as transients and allow one to better constrain the high-mass end of the stellar initial mass function based on the transient detection rate. Follow-up {\it JWST} observations of the Cosmic Horseshoe with would allow one to better probe the nature of Dark Matter and the star formation properties, such as the initial mass function at the cosmic noon, via lensed star transients. |
| 2025-09-19 | [First evidence of $CP$ violation in beauty baryon to charmonium decays](http://arxiv.org/abs/2509.16103v1) | LHCb collaboration, R. Aaij et al. | A study of the difference in the $CP$ asymmetries between ${\Lambda}^0_b \rightarrow J / \psi p \pi^-$ and ${\Lambda}^0_b \rightarrow J / \psi p K^-$ decays, $\Delta {\cal A}_{CP}$, is performed using proton-proton collision data collected by the LHCb experiment in the years 2015--2018, corresponding to an integrated luminosity of $6 {\rm fb}^{-1}$. This quantity is measured to be $ \Delta {\cal A}_{CP}=(4.03\pm 1.18\pm 0.23)\%$, where the first uncertainty is statistical and the second is systematic. When combined with the previous LHCb result, a value of $\Delta {\cal A}_{CP} = (4.31 \pm 1.06 \pm 0.28)\%$ is obtained, corresponding to a significance of $3.9\sigma$ against the $CP$ symmetry hypothesis. Studies of triple-product asymmetries, which provide an additional probe of $CP$ violation, show no significant deviation from $CP$ symmetry. |
| 2025-09-19 | [Compose by Focus: Scene Graph-based Atomic Skills](http://arxiv.org/abs/2509.16053v1) | Han Qi, Changhe Chen et al. | A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine "focused" scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks. |
| 2025-09-19 | [Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation](http://arxiv.org/abs/2509.16007v1) | James E. Warner, Geoffrey F. Bomarito et al. | Multifidelity uncertainty propagation combines the efficiency of low-fidelity models with the accuracy of a high-fidelity model to construct statistical estimators of quantities of interest. It is well known that the effectiveness of such methods depends crucially on the relative correlations and computational costs of the available computational models. However, the question of how to automatically tune low-fidelity models to maximize performance remains an open area of research. This work investigates automated model tuning, which optimizes model hyperparameters to minimize estimator variance within a target computational budget. Focusing on multifidelity trajectory simulation estimators, the cost-versus-precision tradeoff enabled by this approach is demonstrated in a practical, online setting where upfront tuning costs cannot be amortized. Using a real-world entry, descent, and landing example, it is shown that automated model tuning largely outperforms hand-tuned models even when the overall computational budget is relatively low. Furthermore, for scenarios where the computational budget is large, model tuning solutions can approach the best-case multifidelity estimator performance where optimal model hyperparameters are known a priori. Recommendations for applying model tuning in practice are provided and avenues for enabling adoption of such approaches for budget-constrained problems are highlighted. |
| 2025-09-19 | [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](http://arxiv.org/abs/2509.15987v1) | AurÃ©lien Cecille, Stefan Duffner et al. | Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines. |
| 2025-09-19 | [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](http://arxiv.org/abs/2509.15981v1) | Yujie Zhu, Charles A. Hepburn et al. | In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD. |
| 2025-09-19 | [Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems](http://arxiv.org/abs/2509.15961v1) | Nicole Aretz, Thomas Lynn et al. | We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition. |
| 2025-09-19 | [Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration](http://arxiv.org/abs/2509.15959v1) | Zhuoyue Zhang, Haitong Xu | Autonomous navigation in maritime domains is accelerating alongside advances in artificial intelligence, sensing, and connectivity. Opaque decision-making and poorly calibrated human-automation interaction remain key barriers to safe adoption. This article synthesizes 100 studies on automation transparency for Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA), human factors, interface design, and regulation. We (i) map the Guidance-Navigation-Control stack to shore-based operational modes -- remote supervision (RSM) and remote control (RCM) -- and identify where human unsafe control actions (Human-UCAs) concentrate in handover and emergency loops; (ii) summarize evidence that transparency features (decision rationales, alternatives, confidence/uncertainty, and rule-compliance indicators) improve understanding and support trust calibration, though reliability and predictability often dominate trust; (iii) distill design strategies for transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI presentation (textual/graphical overlays, color coding, conversational and immersive UIs), and engineer-facing processes (resilient interaction design, validation, and standardization). We integrate methods for Human-UCA identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and operator workload monitoring, and outline regulatory and rule-based implications including COLREGs formalization and route exchange. We conclude with an adaptive transparency framework that couples operator state estimation with explainable decision support to reduce cognitive overload and improve takeover timeliness. The review highlights actionable figure-of-merit displays (e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs (rule traceability, confidence), and training pipelines (HIL/MIL, simulation) as near-term levers for safer MASS operations. |
| 2025-09-19 | [Quantum Metric Spaces: Replacing Fuzzy Metrics with the Hilbert Space Structure of Quantum States](http://arxiv.org/abs/2509.15945v1) | Nicola Fabiano | Fuzzy metric spaces, grounded in t-norms and membership functions, have been widely proposed to model uncertainty in machine learning, decision systems, and artificial intelligence. Yet these frameworks treat uncertainty as an external layer of imprecision imposed upon classical, point-like entities - a conceptual mismatch for domains where indeterminacy is intrinsic, such as quantum systems or cognitive representations. We argue that fuzzy metrics are unnecessary for modeling such uncertainty: instead, the well-established structure of complex Hilbert spaces - the foundational language of quantum mechanics for over a century - provides a natural, rigorous, and non-contradictory metric space where the ``points'' are quantum states themselves. The distance between states is given by the Hilbert norm, which directly encodes state distinguishability via the Born rule. This framework inherently captures the non-classical nature of uncertainty without requiring fuzzy logic, t-norms, or membership degrees. We demonstrate its power by modeling AI concepts as Gaussian wavefunctions and classifying ambiguous inputs via quantum overlap integrals. Unlike fuzzy methods, our approach naturally handles interference, distributional shape, and concept compositionality through the geometry of state vectors. We conclude that fuzzy metric spaces, while historically useful, are obsolete for representing intrinsic uncertainty - superseded by the more robust, predictive, and ontologically coherent framework of quantum state geometry. |
| 2025-09-18 | [Explaining deep learning for ECG using time-localized clusters](http://arxiv.org/abs/2509.15198v1) | AhcÃ¨ne Boubekki, Konstantinos Patlatzoglou et al. | Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns. |
| 2025-09-18 | [Parallel Simulation of Contact and Actuation for Soft Growing Robots](http://arxiv.org/abs/2509.15180v1) | Yitian Gao, Lucas Chen et al. | Soft growing robots, commonly referred to as vine robots, have demonstrated remarkable ability to interact safely and robustly with unstructured and dynamic environments. It is therefore natural to exploit contact with the environment for planning and design optimization tasks. Previous research has focused on planning under contact for passively deforming robots with pre-formed bends. However, adding active steering to these soft growing robots is necessary for successful navigation in more complex environments. To this end, we develop a unified modeling framework that integrates vine robot growth, bending, actuation, and obstacle contact. We extend the beam moment model to include the effects of actuation on kinematics under growth and then use these models to develop a fast parallel simulation framework. We validate our model and simulator with real robot experiments. To showcase the capabilities of our framework, we apply our model in a design optimization task to find designs for vine robots navigating through cluttered environments, identifying designs that minimize the number of required actuators by exploiting environmental contacts. We show the robustness of the designs to environmental and manufacturing uncertainties. Finally, we fabricate an optimized design and successfully deploy it in an obstacle-rich environment. |
| 2025-09-18 | [To CLEAN or not to CLEAN: Data Processing in the ngVLA era](http://arxiv.org/abs/2509.15176v1) | Hendrik MÃ¼ller | Radio interferometric imaging has long relied on the CLEAN algorithm, valued for its speed, robustness, and integration with calibration pipelines. However, next-generation facilities such as the ngVLA, SKA, and ALMAs Wideband Sensitivity Upgrade will produce data volumes and dynamic ranges that exceed the scalability of traditional methods. CLEAN remains dominant due to its simplicity and accumulated expertise, yet its assumption of modeling the sky as point sources limits its ability to recover extended emission and hampers automation. We review CLEANs limitations and survey alternatives, including multiscale extensions, compressive sensing, Regularized Maximum Likelihood, Bayesian inference, and AI-driven approaches. Forward-modeling methods enable higher fidelity, flexible priors, and uncertainty quantification, albeit at greater computational cost. Hybrid approaches such as Autocorr-CLEAN, CG-CLEAN, and PolyCLEAN retain CLEANs workflow while incorporating modern optimization. We argue hybrids are best suited for the near term, while Bayesian and AI-based frameworks represent the long-term future of interferometric imaging. |
| 2025-09-18 | [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](http://arxiv.org/abs/2509.15147v1) | Viktor Kovalchuk, Nikita Kotelevskii et al. | Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training. |
| 2025-09-18 | [A model-independent measurement of the CKM angle $Î³$ in the decays $B^\pm\to[K^+K^-Ï€^+Ï€^-]_D h^\pm$ and $B^\pm\to[Ï€^+Ï€^-Ï€^+Ï€^-]_D h^\pm$ ($h = K, Ï€$)](http://arxiv.org/abs/2509.15139v1) | LHCb collaboration, R. Aaij et al. | A model-independent determination of the CKM angle $\gamma$ is presented, using the $B^\pm\to[K^+K^-\pi^+\pi^-]_D h^\pm$ and $B^\pm\to[\pi^+\pi^-\pi^+\pi^-]_D h^\pm$ decays, with $h=K,\pi$. This measurement is the first phase-space-binned study of these decay modes, and uses a sample of proton-proton collision data collected by the LHCb experiment, corresponding to an integrated luminosity of $9$fb$^{-1}$. The phase-space bins are optimised for sensitivity to $\gamma$, and in each bin external inputs from the BESIII experiment are used to constrain the charm strong-phase parameters. The result of this binned analysis is $\gamma = (53.9_{-8.9}^{+9.5})^\circ$, where the uncertainty includes both statistical and systematic contributions. Furthermore, when combining with existing phase-space-integrated measurements of the same decay modes, a value of $\gamma = (52.6_{-6.4}^{+8.5})^\circ$ is obtained, which is one of the most precise determinations of $\gamma$ to date. |
| 2025-09-18 | [Sequential sample size calculations and learning curves safeguard the robust development of a clinical prediction model for individuals](http://arxiv.org/abs/2509.15134v1) | Amardeep Legha, Joie Ensor et al. | When prospectively developing a new clinical prediction model (CPM), fixed sample size calculations are typically conducted before data collection based on sensible assumptions. But if the assumptions are inaccurate the actual sample size required to develop a reliable model may be very different. To safeguard against this, adaptive sample size approaches have been proposed, based on sequential evaluation of a models predictive performance. Aim: illustrate and extend sequential sample size calculations for CPM development by (i) proposing stopping rules based on minimising uncertainty (instability) and misclassification of individual-level predictions, and (ii) showcasing how it safeguards against inaccurate fixed sample size calculations. Using the sequential approach repeats the pre-defined model development strategy every time a chosen number (e.g., 100) of participants are recruited and adequately followed up. At each stage, CPM performance is evaluated using bootstrapping, leading to prediction and classification stability statistics and plots, alongside optimism-adjusted measures of calibration and discrimination. Our approach is illustrated for development of acute kidney injury using logistic regression CPMs. The fixed sample size calculation, based on perceived sensible assumptions suggests recruiting 342 patients to minimise overfitting; however, the sequential approach reveals that a much larger sample size of 1100 is required to minimise overfitting (targeting population-level stability). If the stopping rule criteria also target small uncertainty and misclassification probability of individual predictions, the sequential approach suggests an even larger sample size (n=1800). Our sequential sample size approach allows users to dynamically monitor individual-level prediction and classification instability and safeguard against using inaccurate assumptions. |
| 2025-09-18 | [LOFAR 58 MHz Legacy Survey of the 3CRR Catalog](http://arxiv.org/abs/2509.15115v1) | J. M. Boxelaar, F. De Gasperin et al. | The Low Frequency Array (LOFAR) is uniquely able to perform deep, 15" resolutions imaging at frequencies below 100 MHz. Observations in this regime, using the Low Band Antenna (LBA) system, are significantly affected by instrumental and ionospheric distortions. Recent developments in calibration techniques have enabled routine production of high-fidelity images at these challenging frequencies. The aim of this paper is to obtain images of the radio sources included in the Third Cambridge catalog, second revised version (3CRR), at an observing frequency of 58 MHz, with an angular resolution of 15"and sensitivity to both compact and diffuse radio emission. This work also aims to produce accurate flux measurements for all sources. This dataset is designed to serve as a reference for low-frequency radio galaxy studies and future spectral aging analyses. We deliver 58. MHz radio images for the complete 3CRR sample including flux density measurements. We determined that the LBA has an accurate flux density scale with an average flux uncertainty of 10%. This is an important confirmation for any future works using the LOFAR LBA system. With these results we characterize the bright radio galaxy population with new high-resolution low-frequency images. We also provide high-resolution models of these sources which will be useful for calibrating future surveys. This legacy survey significantly expands the available high-resolution data at low frequencies and is the first fully imaged high-resolution sample at ultra low frequencies (< 100 MHz). It lays the foundation for future studies of radio galaxy physics, low-energy cosmic-ray populations, and the interplay between radio jets and their environments. |
| 2025-09-18 | [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](http://arxiv.org/abs/2509.15107v1) | Amy Rafferty, Rishi Ramaesh et al. | Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks. |
| 2025-09-18 | [Precise measurement of the $t\bar{t}$ production cross-section and lepton differential distributions in $eÎ¼$ dilepton events from $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector](http://arxiv.org/abs/2509.15066v1) | ATLAS Collaboration | The inclusive top quark pair ($t\bar{t}$) cross-section $\sigma_{t\bar{t}}$ has been measured in $\sqrt{s}=13$ TeV proton-proton collisions, using 140 fb$^{-1}$ of data collected by the ATLAS experiment at the Large Hadron Collider. Using events with an opposite-charge $e\mu$ pair and $b$-tagged jets, the cross-section is measured to be: $\sigma_{t\bar{t}} = 829.3 \pm 1.3\,\mathrm{(stat)}\ \pm 8.0\,\mathrm{(syst)}\ \pm 7.3\,\mathrm{(lumi)}\ \pm 1.9\,\mathrm{(beam)}\,\mathrm{pb},$ where the uncertainties reflect the limited size of the data sample, experimental and theoretical systematic effects, the integrated luminosity, and the proton beam energy, giving a total uncertainty of 1.3%. The result is used to determine the top quark pole mass via the dependence of the predicted cross-section on $m_t^\mathrm{pole}$, giving $m_t^\mathrm{pole}=172.8^{+1.5}_{-1.7}$ GeV. The same event sample is used to measure absolute and normalised differential cross-sections for the $t\bar{t}\rightarrow e\mu\nu\bar{\nu}b\bar{b}$ process as a function of single-lepton and dilepton kinematic variables. Complementary measurements of $e\mu b\bar{b}$ production, treating both $t\bar{t}$ and $Wt$ events as signal, are also provided. Both sets of differential cross-sections are compared to the predictions of various Monte Carlo event generators, demonstrating that the state-of-the-art generators Powheg MiNNLO and Powheg $bb4l$ describe the data better than Powheg hvq. |
| 2025-09-18 | [Constraining Cosmology with Double-Source-Plane Strong Gravitational Lenses From the AGEL Survey](http://arxiv.org/abs/2509.15012v1) | Duncan J. Bowden, Nandini Sahu et al. | Double-source-plane strong gravitational lenses (DSPLs), with two sources at different redshifts, are independent cosmological probes of the dark energy equation of state parameter $w$ and the matter density parameter $\Omega_{\rm m}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer cosmological constraints from this system for flat $\Lambda$CDM and flat $w$CDM cosmologies. From the joint posterior of $w$ and $\Omega_{\rm m}$ in the flat $w$CDM cosmology, we extract the following median values and 1$\sigma$ uncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\Omega_{\rm m} = 0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with two previously analyzed DSPLs, we present the joint constraint on these parameters from a sample of three, the largest galaxy-scale DSPL sample used for cosmological measurement to date. The combined precision of $w$ from three DSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave background (CMB) measurements improves the precision of $w$ from CMB-only constraints by 39%, demonstrating the complementarity of DSPLs with the CMB. Despite their promising constraining power, DSPLs are limited by sample size, with only a handful discovered so far. Although ongoing and near-future wide-area sky surveys will increase the number of known DSPLs by up to two orders of magnitude, these systems will still require dedicated high-resolution imaging and spectroscopic follow-ups like those presented in this paper. Our ASTRO 3D Galaxy Evolution with Lenses (AGEL) collaboration is undertaking such follow-up campaigns for several newly discovered DSPLs and will provide cosmological measurements from larger samples of DSPLs in the future. |
| 2025-09-17 | [Large deviations for probability graphons](http://arxiv.org/abs/2509.14204v1) | Pierfrancesco Dionigi, Giulio Zucal | We establish a large deviation principle (LDP) for probability graphons, which are symmetric functions from the unit square into the space of probability measures. This notion extends classical graphons and provides a flexible framework for studying the limit behavior of large dense weighted graphs. In particular, our result generalizes the seminal work of Chatterjee and Varadhan (2011), who derived an LDP for Erd\H{o}s-R\'enyi random graphs via graphon theory. We move beyond their binary (Bernoulli) setting to encompass arbitrary edge-weight distributions. Specifically, we analyze the distribution on probability graphons induced by random weighted graphs in which edges are sampled independently from a common reference probability measure supported on a compact Polish space. We prove that this distribution satisfies an LDP with a good rate function, expressed as an extension of the Kullback-Leibler divergence between probability graphons and the reference measure. This theorem can also be viewed as a Sanov-type result in the graphon setting. Our work provides a rigorous foundation for analyzing rare events in weighted networks and supports statistical inference in structured random graph models under distributional edge uncertainty. |
| 2025-09-17 | [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](http://arxiv.org/abs/2509.14167v1) | Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes et al. | Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains. |
| 2025-09-17 | [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](http://arxiv.org/abs/2509.14151v1) | Rongyu Zhang, Jiaming Liu et al. | Vision-centric Bird's Eye View (BEV) perception holds considerable promise for autonomous driving. Recent studies have prioritized efficiency or accuracy enhancements, yet the issue of domain shift has been overlooked, leading to substantial performance degradation upon transfer. We identify major domain gaps in real-world cross-domain scenarios and initiate the first effort to address the Domain Adaptation (DA) challenge in multi-view 3D object detection for BEV perception. Given the complexity of BEV perception approaches with their multiple components, domain shift accumulation across multi-geometric spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain adaptation. In this paper, we introduce an innovative geometric-aware teacher-student framework, BEVUDA++, to diminish this issue, comprising a Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model. Specifically, RDT effectively blends target LiDAR with dependable depth predictions to generate depth-aware information based on uncertainty estimation, enhancing the extraction of Voxel and BEV features that are essential for understanding the target domain. To collaboratively reduce the domain shift, GCS maps features from multiple spaces into a unified geometric embedding space, thereby narrowing the gap in data distribution between the two domains. Additionally, we introduce a novel Uncertainty-guided Exponential Moving Average (UEMA) to further reduce error accumulation due to domain shifts informed by previously obtained uncertainty guidance. To demonstrate the superiority of our proposed method, we execute comprehensive experiments in four cross-domain scenarios, securing state-of-the-art performance in BEV 3D object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night adaptation. |
| 2025-09-17 | [Safe Sliding Mode Control in Position for Double Integrator Systems](http://arxiv.org/abs/2509.14121v1) | Marco A. Gomez, Christopher D. Cruz-Ancona | We address the problem of robust safety control design for double integrator systems. We show that, when the constraints are defined only on position states, it is possible to construct a safe sliding domain from the dynamic of a simple integrator that is already safe. On this domain, the closed-loop trajectories remain robust and safe against uncertainties and disturbances. Furthermore, we design a controller gain that guarantees convergence to the safe sliding domain while avoiding the given unsafe set. The concept is initially developed for first-order sliding mode and is subsequently generalized to an adaptive framework, ensuring that trajectories remain confined to a predefined vicinity of the sliding domain, outside the unsafe region. |
| 2025-09-17 | [Online Bayesian Risk-Averse Reinforcement Learning](http://arxiv.org/abs/2509.14077v1) | Yuhao Wang, Enlu Zhou | In this paper, we study the Bayesian risk-averse formulation in reinforcement learning (RL). To address the epistemic uncertainty due to a lack of data, we adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the parameter uncertainty of the unknown underlying model. We derive the asymptotic normality that characterizes the difference between the Bayesian risk value function and the original value function under the true unknown distribution. The results indicate that the Bayesian risk-averse approach tends to pessimistically underestimate the original value function. This discrepancy increases with stronger risk aversion and decreases as more data become available. We then utilize this adaptive property in the setting of online RL as well as online contextual multi-arm bandits (CMAB), a special case of online RL. We provide two procedures using posterior sampling for both the general RL problem and the CMAB problem. We establish a sub-linear regret bound, with the regret defined as the conventional regret for both the RL and CMAB settings. Additionally, we establish a sub-linear regret bound for the CMAB setting with the regret defined as the Bayesian risk regret. Finally, we conduct numerical experiments to demonstrate the effectiveness of the proposed algorithm in addressing epistemic uncertainty and verifying the theoretical properties. |
| 2025-09-17 | [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](http://arxiv.org/abs/2509.14054v1) | Weihao Yan, Christoph Brune et al. | Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications. |
| 2025-09-17 | [Network representations reveal structured uncertainty in music](http://arxiv.org/abs/2509.14053v1) | Lluc Bono RossellÃ³, Robert Jankowski et al. | Music, as a structured yet perceptually rich experience, can be modeled as a network to uncover how humans encode and process auditory information. While network-based representations of music are increasingly common, the impact of feature selection on structural properties and cognitive alignment remains underexplored. In this study, we evaluated eight network models, each constructed from symbolic representations of piano compositions using distinct combinations of pitch, octave, duration, and interval, designed to be representative of existing approaches in the literature. By comparing these models through topological metrics, entropy analysis, and divergence with respect to inferred cognitive representations, we assessed both their structural and perceptual efficiency. Our findings reveal that simpler, feature-specific models better match human perception, whereas complex, multidimensional representations introduce cognitive inefficiencies. These results support the view that humans rely on modular, parallel cognitive networks--an architecture consistent with theories of predictive processing and free energy minimization. Moreover, we find that musical networks are structurally organized to guide attention toward transitions that are both uncertain and inferable. The resulting structure concentrates uncertainty in a few frequently visited nodes, creating local entropy gradients that alternate between stable and unpredictable regions, thereby enabling the expressive dynamics of tension and release that define the musical experience. These findings show that network structures make the organization of uncertainty in music observable, offering new insight into how patterned flows of expectation shape perception, and open new directions for studying how musical structures evolve across genres, cultures, and historical periods through the lens of network science. |
| 2025-09-17 | [Day-Ahead Transmission Grid Topology Optimization Considering Renewable Energy Sources' Uncertainty](http://arxiv.org/abs/2509.13994v1) | Giacomo Bastianel, Dirk Van Hertem et al. | The increasing renewable penetration introduces significant uncertainty in power system operations. At the same time, the existing transmission grid is often already congested, and urgently needed reinforcements are frequently delayed due to several constraints. To address these challenges, adjusting the grid topology based on congestion patterns is considered a non-costly remedy to guarantee efficient power transmission. Based on this idea, this paper proposes a grid topology optimization model combining optimal transmission switching and busbar splitting for AC and hybrid AC/DC grids. The methodology incorporates RES forecast uncertainty through a scenario-based stochastic optimization approach, using real offshore wind data and K-means clustering to generate representative forecast error scenarios. The proposed model includes several formulations to be compared with a plain optimal power flow (OPF) model: hourly optimizing the topology, one topology for 24 hours, or a limited number of switching actions over a day. The grid topology optimization model is formulated as a Mixed-Integer Quadratic Convex Problem, optimized based on the day-ahead (D-1) RES forecast and validated for AC-feasibility via an AC-OPF formulation. Based on the generation setpoints of the feasibility check, a redispatch simulation based on the measured (D) RES realization is then computed. The methodology is tested on an AC 30-bus test case and a hybrid AC/DC 50-bus test case, for a 24-hours (30-bus) and a 14-days (both test cases) time series. The results highlight the economic benefits brought by grid topology optimization for congested test cases with high penetration of RES. In addition, the results demonstrate that accounting for RES uncertainty with at least 6 to 8 scenarios leads to lower or comparable total costs to deterministic day-ahead forecasts, even when limiting the frequency of topological actions. |
| 2025-09-17 | [Improved systematic evaluation of a strontium optical clock with uncertainty below $1\times 10^{-18}$](http://arxiv.org/abs/2509.13991v1) | Zhi-Peng Jia, Jie Li et al. | We report a systematic uncertainty of $9.2\times 10^{-19}$ for the USTC Sr1 optical lattice clock, achieving accuracy at the level required for the roadmap of the redefinition of the SI second. A finite-element model with {\it in situ}-validated, spatially-resolved chamber emissivity reduced blackbody radiation shift uncertainty to $6.3\times 10^{-19}$. Concurrently, an externally mounted lattice cavity combined with a larger beam waist suppressed density shifts. Enhanced lattice depth modulation consolidated lattice light shift uncertainty to $6.3\times 10^{-19}$ by enabling simultaneous determination of key polarizabilities and magic wavelength. Magnetic shifts were resolved below $10^{-18}$ via precision characterization of the second-order Zeeman coefficient. Supported by a crystalline-coated ultra-low-expansion cavity-stabilized laser and refined temperature control suppressing BBR fluctuations, the clock also achieves a frequency stability better than $1\times10^{-18}$ at 30,000-s averaging time. These developments collectively establish a new benchmark in USTC Sr1 clock performance and pave the way for high-accuracy applications in metrology and fundamental physics. |
| 2025-09-17 | [Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game](http://arxiv.org/abs/2509.13985v1) | Yixun Wen, Yulong Gao et al. | Generalized Nash equilibrium problem (GNEP) is fundamental for practical applications where multiple self-interested agents work together to make optimal decisions. In this work, we study GNEP with shared distributionally robust chance constraints (DRCCs) for incorporating inevitable uncertainties. The DRCCs are defined over the Wasserstein ball, which can be explicitly characterized even with limited sample data. To determine the equilibrium of the GNEP, we propose an exact approach to transform the original computationally intractable problem into a deterministic formulation using the Nikaido-Isoda function. Specifically, we show that when all agents' objectives are quadratic in their respective variables, the equilibrium can be obtained by solving a typical mixed-integer nonlinear programming (MINLP) problem, where the integer and continuous variables are decoupled in both the objective function and the constraints. This structure significantly improves computational tractability, as demonstrated through a case study on the charging station pricing problem. |
| 2025-09-16 | [Axion-photon conversion in transient compact stars: Systematics, constraints, and opportunities](http://arxiv.org/abs/2509.13322v1) | Damiano F. G. Fiorillo, Ãngel Gil Muyor et al. | We study magnetic conversion of ultra-relativistic axion-like particles (ALPs) into photons in compact-star environments, focusing on the hot, transient conditions of core-collapse supernova (SN) remnants and neutron-star mergers (NSMs). We address previously overlooked uncertainties, particularly the suppression caused by ejected matter near the stellar surface, a region crucial to the conversion process. We derive analytical expressions for the transition rate; they reveal the influence of key parameters and their uncertainties. We update constraints using historical gamma-ray data from SN~1987A and find $g_{a\gamma}<5\times10^{-12}~{\rm GeV}^{-1}$ for $m_a\lesssim10^{-9}$ meV. We also forecast sensitivities for a future Galactic SN and for NSMs, assuming observations with Fermi-LAT or similar gamma-ray instruments. We distinguish ALPs -- defined as coupling only to photons and produced via Primakoff scattering -- from axions, which also couple to nucleons and emerge through nuclear bremsstrahlung. We omit pionic axion production due to its large uncertainties and inconsistencies, though it could contribute comparably to bremsstrahlung under optimistic assumptions. For the compact sources, we adopt time-averaged one-zone models, guided by numerical simulations, to enable clear and reproducible parametric studies. |
| 2025-09-16 | [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](http://arxiv.org/abs/2509.13313v1) | Xixi Wu, Kuan Li et al. | Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents. |
| 2025-09-16 | [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](http://arxiv.org/abs/2509.13305v1) | Kuan Li, Zhongwang Zhang et al. | Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap. |
| 2025-09-16 | [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](http://arxiv.org/abs/2509.13262v1) | Zhizhong Zhao, Ke Chen | Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet existing methods are either computationally intensive, such as Bayesian or ensemble methods, or provide only partial, task-specific estimates, such as single-forward-pass techniques. In this paper, we propose a post-hoc single-forward-pass framework that jointly captures aleatoric and epistemic uncertainty without modifying or retraining pretrained models. Our method applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs) on each side. We prove that, under ideal conditions, the total MAR equals the harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency Discrepancy Score} (SDS) for fine-grained epistemic estimation across regression and classification. For regression, side-specific quantile regression yields prediction intervals with improved empirical coverage, which are further calibrated via SDS. For classification, when calibration data are available, we apply SPA-based calibration identities to adjust the softmax outputs and then compute predictive entropy on these calibrated probabilities. Extensive experiments on diverse regression and classification benchmarks demonstrate that our framework matches or exceeds several state-of-the-art UQ methods while incurring minimal overhead.   Our source code is available at https://github.com/zzz0527/SPC-UQ. |
| 2025-09-16 | [SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty](http://arxiv.org/abs/2509.13189v1) | Zhao Feng, Bicheng Yan et al. | We present a direct inverse modeling method named SURGIN, a SURrogate-guided Generative INversion framework tailed for subsurface multiphase flow data assimilation. Unlike existing inversion methods that require adaptation for each new observational configuration, SURGIN features a zero-shot conditional generation capability, enabling real-time assimilation of unseen monitoring data without task-specific retraining. Specifically, SURGIN synergistically integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a score-based generative model (SGM), framing the conditional generation as a surrogate prediction-guidance process in a Bayesian perspective. Instead of directly learning the conditional generation of geological parameters, an unconditional SGM is first pretrained in a self-supervised manner to capture the geological prior, after which posterior sampling is performed by leveraging a differentiable U-FNO surrogate to enable efficient forward evaluations conditioned on unseen observations. Extensive numerical experiments demonstrate SURGIN's capability to decently infer heterogeneous geological fields and predict spatiotemporal flow dynamics with quantified uncertainty across diverse measurement settings. By unifying generative learning with surrogate-guided Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and uncertainty quantification in parametric functional spaces. |
| 2025-09-16 | [Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments](http://arxiv.org/abs/2509.13176v1) | Qiushi Bu, Wen Su et al. | We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility. |
| 2025-09-16 | [Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression](http://arxiv.org/abs/2509.13150v1) | Shima Mohammadi, Mohsen Jenadeleh et al. | Nowadays, image compression solutions are increasingly designed to operate within high-fidelity quality ranges, where preserving even the most subtle details of the original image is essential. In this context, the ability to detect and quantify subtle compression artifacts becomes critically important, as even slight degradations can impact perceptual quality in professional or quality sensitive applications, such as digital archiving, professional editing and web delivery. However, the performance of current objective image quality assessment metrics in this range has not been thoroughly investigated. In particular, it is not well understood how reliably these metrics estimate distortions at or below the threshold of Just Noticeable Difference (JND). This study directly addresses this issue by proposing evaluation methodologies for assessing the performance of objective quality metrics and performing a comprehensive evaluation using the JPEG AIC-3 dataset which is designed for high-fidelity image compression. Beyond conventional criteria, the study introduces Z-RMSE to incorporate subjective score uncertainty and applies novel statistical tests to assess significant differences between metrics. The analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity subsets, examines the impact of cropping in subjective tests, and a public dataset with benchmarks and evaluation tools is released to support further research. |
| 2025-09-16 | [An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios](http://arxiv.org/abs/2509.13132v1) | Zhihao Zhang, Chengyang Peng et al. | Autonomous driving in dense, dynamic environments requires decision-making systems that can exploit both spatial structure and long-horizon temporal dependencies while remaining robust to uncertainty. This work presents a novel framework that integrates multi-channel bird's-eye-view occupancy grids with transformer-based sequence modeling for tactical driving in complex roundabout scenarios. To address the imbalance between frequent low-risk states and rare safety-critical decisions, we propose the Uncertainty-Weighted Decision Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate per-token predictive entropy, which is then used as a weight in the student model's loss function. This mechanism amplifies learning from uncertain, high-impact states while maintaining stability across common low-risk transitions. Experiments in a roundabout simulator, across varying traffic densities, show that UWDT consistently outperforms other baselines in terms of reward, collision rate, and behavioral stability. The results demonstrate that uncertainty-aware, spatial-temporal transformers can deliver safer and more efficient decision-making for autonomous driving in complex traffic environments. |
| 2025-09-16 | [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](http://arxiv.org/abs/2509.13084v1) | Yunyao Lu, Yihang Wu et al. | Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments. |
| 2025-09-16 | [Physics potential of the IceCube Upgrade for atmospheric neutrino oscillations](http://arxiv.org/abs/2509.13066v1) | IceCube Collaboration | The IceCube Upgrade is an extension of the existing IceCube Neutrino Observatory and will be deployed in the 2025-2026 austral summer. It will significantly improve the sensitivity of the detector to atmospheric neutrino oscillations. The existing 86-string IceCube array contains a dense in-fill known as DeepCore which is optimized to measure neutrinos with energies down to a few GeV. The IceCube Upgrade will consist of seven new densely-instrumented strings placed within the DeepCore volume to further enhance the performance in the GeV energy range. The additional strings will feature new optical modules, each containing multiple PMTs, in contrast to the existing modules that each contain a single PMT. This will more than triple the number of PMT channels with respect to the current IceCube configuration, allowing for improved detection efficiency and reconstruction performance at GeV energies. We describe necessary updates to simulation, event selection, and reconstruction to accommodate the higher data rates observed by the upgraded detector and the addition of multi-PMT modules. We determine the expected sensitivity of the IceCube Upgrade to the atmospheric neutrino oscillation parameters sin$^2\theta_{23}$ and $\Delta m^2_{32}$, the appearance of tau neutrinos and the neutrino mass ordering. The IceCube Upgrade will provide neutrino oscillation measurements that are of similar precision to those from accelerator experiments, while providing complementarity by probing higher energies and longer baselines, and with different sources of systematic uncertainties. |
| 2025-09-15 | [Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks](http://arxiv.org/abs/2509.12199v1) | Asif Iqbal, Subhabrata Majumdar et al. | Precise determination of galaxy cluster masses is crucial for establishing reliable mass-observable scaling relations in cluster cosmology. We employ graph neural networks (GNNs) to estimate galaxy cluster masses from radially sampled profiles of the intra-cluster medium (ICM) inferred from X-ray observations. GNNs naturally handle inputs of variable length and resolution by representing each ICM profile as a graph, enabling accurate and flexible modeling across diverse observational conditions. We trained and tested GNN model using state-of-the-art hydrodynamical simulations of galaxy clusters from The Three Hundred Project. The mass estimates using our method exhibit no systematic bias compared to the true cluster masses in the simulations. Additionally, we achieve a scatter in recovered mass versus true mass of about 6\%, which is a factor of six smaller than obtained from a standard hydrostatic equilibrium approach. Our algorithm is robust to both data quality and cluster morphology and it is capable of incorporating model uncertainties alongside observational uncertainties. Finally, we apply our technique to XMM-Newton observed galaxy cluster samples and compare the GNN derived mass estimates with those obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our results provide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZ derived masses, with higher mass clusters exhibiting a greater degree of deviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$, albeit with significant dispersion due to its mass dependence. This work takes a significant step towards establishing unbiased observable mass scaling relations by integrating X-ray, SZ and optical datasets using deep learning techniques, thereby enhancing the role of galaxy clusters in precision cosmology. |
| 2025-09-15 | [Approaches to Analysis and Design of AI-Based Autonomous Vehicles](http://arxiv.org/abs/2509.12169v1) | Tao Yan, Zheyu Zhang et al. | Artificial intelligence (AI) models are becoming key components in an autonomous vehicle (AV), especially in handling complicated perception tasks. However, closing the loop through AI-based feedback may pose significant risks on reliability of autonomous driving due to very limited understanding about the mechanism of AI-driven perception processes. To overcome it, this paper aims to develop tools for modeling, analysis, and synthesis for a class of AI-based AV; in particular, their closed-loop properties, e.g., stability, robustness, and performance, are rigorously studied in the statistical sense. First, we provide a novel modeling means for the AI-driven perception processes by looking at their error characteristics. Specifically, three fundamental AI-induced perception uncertainties are recognized and modeled by Markov chains, Gaussian processes, and bounded disturbances, respectively. By means of that, the closed-loop stochastic stability (SS) is established in the sense of mean square, and then, an SS control synthesis method is presented within the framework of linear matrix inequalities (LMIs). Besides the SS properties, the robustness and performance of AI-based AVs are discussed in terms of a stochastic guaranteed cost, and criteria are given to test the robustness level of an AV when in the presence of AI-induced uncertainties. Furthermore, the stochastic optimal guaranteed cost control is investigated, and an efficient design procedure is developed innovatively based on LMI techniques and convex optimization. Finally, to illustrate the effectiveness, the developed results are applied to an example of car following control, along with extensive simulation. |
| 2025-09-15 | [Superresolving Non-linear PDE Dynamics with Reduced-Order Autodifferentiable Ensemble Kalman Filtering For Turbulence Modeling and Flow Regulation](http://arxiv.org/abs/2509.12149v1) | Mrigank Dhingra, Omer San | Accurately reconstructing and forecasting high-resolution (HR) states from computationally cheap low-resolution (LR) observations is central to estimation-and-control of spatio-temporal PDE systems. We develop a unified superresolution pipeline based on the reduced-order autodifferentiable Ensemble Kalman filter (ROAD-EnKF). The method learns a low-dimensional latent dynamics model and a nonlinear decoder from latent variables to HR fields; the learned pair is embedded in an EnKF, enabling simultaneous state estimation and control-oriented forecasting with quantified uncertainty. We evaluate on three benchmarks: 1-D viscous Burgers equation (shock formation), Kuramoto-Sivashinsky (KS) equation (chaotic dynamics), and 2-D Navier-Stokes-Kraichnan turbulence (NSKT) (vortex decaying dynamics at Re 16,000). LR data are obtained by factors of 4-8 downsampling per spatial dimension and are corrupted with noise. On Burgers and KS, the latent models remain stable far beyond the observation window, accurately predicting shock propagation and chaotic attractor statistics up to 150 steps. On 2-D NSKT, the approach preserves the kinetic-energy spectrum and enstrophy budget of the HR data, indicating suitability for control scenarios that depend on fine-scale flow features. These results position ROAD-EnKF as a principled and efficient framework for physics-constrained superresolution, bridging LR sensing and HR actuation across diverse PDE regimes. |
| 2025-09-15 | [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](http://arxiv.org/abs/2509.12094v1) | Tianqi Zhao, Russa Biswas et al. | Graph machine learning models often achieve similar overall performance yet behave differently at the node level, failing on different subsets of nodes with varying reliability. Standard evaluation metrics such as accuracy obscure these fine grained differences, making it difficult to diagnose when and where models fail. We introduce NodePro, a node profiling framework that enables fine-grained diagnosis of model behavior by assigning interpretable profile scores to individual nodes. These scores combine data-centric signals, such as feature dissimilarity, label uncertainty, and structural ambiguity, with model-centric measures of prediction confidence and consistency during training. By aligning model behavior with these profiles, NodePro reveals systematic differences between models, even when aggregate metrics are indistinguishable. We show that node profiles generalize to unseen nodes, supporting prediction reliability without ground-truth labels. Finally, we demonstrate the utility of NodePro in identifying semantically inconsistent or corrupted nodes in a structured knowledge graph, illustrating its effectiveness in real-world settings. |
| 2025-09-15 | [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](http://arxiv.org/abs/2509.12043v1) | Mayur Patil, Qadeer Ahmed et al. | Traffic flow forecasting is essential for managing congestion, improving safety, and optimizing various transportation systems. However, it remains a prevailing challenge due to the stochastic nature of urban traffic and environmental factors. Better predictions require models capable of accommodating the traffic variability influenced by multiple dynamic and complex interdependent factors. In this work, we propose a Graph Neural Network (GNN) framework to address the stochasticity by leveraging adaptive adjacency matrices using log-normal distributions and Coefficient of Variation (CV) values to reflect real-world travel time variability. Additionally, weather factors such as temperature, wind speed, and precipitation adjust edge weights and enable GNN to capture evolving spatio-temporal dependencies across traffic stations. This enhancement over the static adjacency matrix allows the model to adapt effectively to traffic stochasticity and changing environmental conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP) framework to provide reliable uncertainty quantification, achieving target coverage while maintaining acceptable prediction intervals. Experimental results demonstrate that the proposed model, in comparison with baseline methods, showed better prediction accuracy and uncertainty bounds. We, then, validate this method by constructing traffic scenarios in SUMO and applying Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under Test (VUT) to reflect real-world variability. The simulated mean travel time of the VUT falls within the intervals defined by INRIX historical data, verifying the model's robustness. |
| 2025-09-15 | [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](http://arxiv.org/abs/2509.12034v1) | Emmanuel Adjei Domfeh, Christopher L. Dancy | In high-stakes disaster scenarios, timely and informed decision-making is critical yet often challenged by uncertainty, dynamic environments, and limited resources. This paper presents a systematic review of Human-AI collaboration patterns that support decision-making across all disaster management phases. Drawing from 51 peer-reviewed studies, we identify four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Within these, we analyze sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. Our review highlights how AI systems may enhance situational awareness, improves response efficiency, and support complex decision-making, while also surfacing critical limitations in scalability, interpretability, and system interoperability. We conclude by outlining key challenges and future research directions, emphasizing the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and equitable recovery outcomes. |
| 2025-09-15 | [Learning from Uncertain Similarity and Unlabeled Data](http://arxiv.org/abs/2509.11984v1) | Meng Wei, Zhongnian Li et al. | Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches. |
| 2025-09-15 | [Probabilistic modelling of atmosphere-surface coupling with a copula Bayesian network](http://arxiv.org/abs/2509.11975v1) | Laura Mack, Marvin KÃ¤hnert et al. | Land-atmosphere coupling is an important process for correctly modelling near-surface temperature profiles, but it involves various uncertainties due to subgrid-scale processes, such as turbulent fluxes or unresolved surface heterogeneities, suggesting a probabilistic modelling approach. We develop a copula Bayesian network (CBN) to interpolate temperature profiles, acting as alternative to T2m-diagnostics used in numerical weather prediction (NWP) systems. The new CBN results in (1) a reduction of the warm bias inherent to NWP predictions of wintertime stable boundary layers allowing cold temperature extremes to be better represented, and (2) consideration of uncertainty associated with subgrid-scale spatial variability. The use of CBNs combines the advantages of uncertainty propagation inherent to Bayesian networks with the ability to model complex dependence structures between random variables through copulas. By combining insights from copula modelling and information entropy, criteria for the applicability of CBNs in the further development of parameterizations in NWP models are derived. |
| 2025-09-15 | [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](http://arxiv.org/abs/2509.11966v1) | Sangjoon Park, Yeonjong Shin et al. | Poroelasticity -- coupled fluid flow and elastic deformation in porous media -- often involves spatially variable permeability, especially in subsurface systems. In such cases, simulations with random permeability fields are widely used for probabilistic analysis, uncertainty quantification, and inverse problems. These simulations require repeated forward solves that are often prohibitively expensive, motivating the development of efficient surrogate models. However, efficient surrogate modeling techniques for poroelasticity with random permeability fields remain scarce. In this study, we propose a surrogate modeling framework based on the deep operator network (DeepONet), a neural architecture designed to learn mappings between infinite-dimensional function spaces. The proposed surrogate model approximates the solution operator that maps random permeability fields to transient poroelastic responses. To enhance predictive accuracy and stability, we integrate three strategies: nondimensionalization of the governing equations, input dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step training procedure that decouples the optimization of branch and trunk networks. The methodology is evaluated on two benchmark problems in poroelasticity: soil consolidation and ground subsidence induced by groundwater extraction. In both cases, the DeepONet achieves substantial speedup in inference while maintaining high predictive accuracy across a wide range of permeability statistics. These results highlight the potential of the proposed approach as a scalable and efficient surrogate modeling technique for poroelastic systems with random permeability fields. |
| 2025-09-15 | [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](http://arxiv.org/abs/2509.11964v1) | Junyoung Kim, Minsik Jeon et al. | Semantic mapping aims to construct a 3D semantic representation of the environment, providing essential knowledge for robots operating in complex outdoor settings. While Bayesian Kernel Inference (BKI) addresses discontinuities of map inference from sparse sensor data, existing semantic mapping methods suffer from various sources of uncertainties in challenging outdoor environments. To address these issues, we propose an uncertainty-aware semantic mapping framework that handles multiple sources of uncertainties, which significantly degrade mapping performance. Our method estimates uncertainties in semantic predictions using Evidential Deep Learning and incorporates them into BKI for robust semantic inference. It further aggregates noisy observations into coherent Gaussian representations to mitigate the impact of unreliable points, while employing geometry-aligned kernels that adapt to complex scene structures. These Gaussian primitives effectively fuse local geometric and semantic information, enabling robust, uncertainty-aware mapping in complex outdoor scenarios. Comprehensive evaluation across diverse off-road and urban outdoor environments demonstrates consistent improvements in mapping quality, uncertainty calibration, representational flexibility, and robustness, while maintaining real-time efficiency. |
| 2025-09-12 | [The CHARA Array Polarization Model and Prospects for Spectropolarimetry](http://arxiv.org/abs/2509.10451v1) | Linling Shuai, John D. Monnier et al. | Polarimetric data provide key insights into infrared emission mechanisms in the inner disks of YSOs and the details of dust formation around AGB stars. While polarization measurements are well-established in radio interferometry, they remain challenging at visible and near-infrared due to the significant time-variable birefringence introduced by the complex optical beamtrain. In this study, we characterize instrumental polarization effects within the optical path of the CHARA Array, focusing on the H-band MIRC-X and K-band MYSTIC beam combiners. Using Jones matrix formalism, we developed a comprehensive model describing diattenuation and retardance across the array. By applying this model to an unpolarized calibrator, we derived the instrumental parameters for both MIRC-X and MYSTIC. Our results show differential diattenuation consistent with >= 97% reflectivity per aluminum-coated surface at 45 deg incidence. The differential retardance exhibits small wavelength-dependent variations, in some cases larger than we expected. Notably, telescope W2 exhibits a significantly larger phase shift in the Coude path, attributable to a fixed aluminum mirror (M4) used in place of deformable mirrors present on the other telescopes during the observing run. We also identify misalignments in the LiNbO_3 birefringent compensator plates on S1 (MIRC-X) and W2 (MYSTIC). After correcting for night-to-night offsets, we achieve calibration accuracies of $\pm$ 3.4% in visibility ratio and $\pm$ 1.4 deg in differential phase for MIRC-X, and $\pm$ 5.9% and $\pm$ 2.4 deg, respectively, for MYSTIC. Given that the differential intrinsic polarization of spatially resolved sources, such as AGB stars and YSOs, typically greater than these instrumental uncertainties, our results demonstrate that CHARA is now capable of achieving high-accuracy measurements of intrinsic polarization in astrophysical targets. |
| 2025-09-12 | [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](http://arxiv.org/abs/2509.10419v1) | Francesco Vitale, Tommaso Zoppi et al. | Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability. |
| 2025-09-12 | [A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives](http://arxiv.org/abs/2509.10393v1) | ClÃ©mentine Chazal, Heishiro Kanagawa et al. | Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called 'gradient discrepancy', and in particular a 'kernel gradient discrepancy' (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel charasterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained. To illustrate this point several novel algorithms are proposed, including a natural generalisation of Stein variational gradient descent, with applications to mean-field neural networks and prediction-centric uncertainty quantification presented. On the theoretical side, our principal contribution is to establish sufficient conditions for desirable properties of KGD, such as continuity and convergence control. |
| 2025-09-12 | [Vendi Information Gain for Active Learning and its Application to Ecology](http://arxiv.org/abs/2509.10390v1) | Quan Nguyen, Adji Bousso Dieng | While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG achieves impressive predictive accuracy close to full supervision using less than 10% of the labels. It consistently outperforms standard baselines across metrics and batch sizes, collecting more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments. |
| 2025-09-12 | [Theory uncertainties of the irreducible background to VBF Higgs production](http://arxiv.org/abs/2509.10368v1) | Xuan Chen, Silvia Ferrario Ravasio et al. | Higgs boson production through gluon fusion in association with two jets is an irreducible background to Higgs boson production through vector boson fusion, one of the most important channels for analyzing and understanding the Higgs boson properties at the Large Hadron Collider. Despite a range of available simulation tools, precise predictions for the corresponding final states are notoriously hard to achieve. Using state-of-the-art fixed-order calculations as the baseline for a comparison, we perform a detailed study of similarities and differences in existing event generators. We provide consistent setups for the simulations that can be used to obtain identical parametric precision in various programs used by experiments. We find that NLO calculations for the two-jet final state are essential to achieve reliable predictions. |
| 2025-09-12 | [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](http://arxiv.org/abs/2509.10348v1) | Yehudit Aperstein, Amit Tzahar et al. | Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings. |
| 2025-09-12 | [OpenCSP: A Deep Learning Framework for Crystal Structure Prediction from Ambient to High Pressure](http://arxiv.org/abs/2509.10293v1) | Yinan Wang, Xiaoyang Wang et al. | High-pressure crystal structure prediction (CSP) underpins advances in condensed matter physics, planetary science, and materials discovery. Yet, most large atomistic models are trained on near-ambient, equilibrium data, leading to degraded stress accuracy at tens to hundreds of gigapascals and sparse coverage of pressure-stabilized stoichiometries and dense coordination motifs. Here, we introduce OpenCSP, a machine learning framework for CSP tasks spanning ambient to high-pressure conditions. This framework comprises an open-source pressure-resolved dataset alongside a suite of publicly available atomistic models that are jointly optimized for accuracy in energy, force, and stress predictions. The dataset is constructed via randomized high-pressure sampling and iteratively refined through an uncertainty-guided concurrent learning strategy, which enriches underrepresented compression regimes while suppressing redundant DFT labeling. Despite employing a training corpus one to two orders of magnitude smaller than those of leading large models, OpenCSP achieves comparable or superior performance in high-pressure enthalpy ranking and stability prediction. Across benchmark CSP tasks spanning a wide pressure window, our models match or surpass MACE-MPA-0, MatterSim v1 5M, and GRACE-2L-OAM, with the largest gains observed at elevated pressures. These results demonstrate that targeted, pressure-aware data acquisition coupled with scalable architectures enables data-efficient, high-fidelity CSP, paving the way for autonomous materials discovery under ambient and extreme conditions. |
| 2025-09-12 | [Astroclimes -- measuring the abundance of CO$_2$ and CH$_4$ in the Earth's atmosphere using astronomical observations](http://arxiv.org/abs/2509.10258v1) | Marcelo Aron Fetzner Keniger, David Armstrong et al. | Monitoring the abundance of greenhouse gases (GHGs) such as carbon dioxide (CO$_2$) and methane (CH$_4$) is necessary to quantify their impact on global warming and climate change. Although a number of satellites and ground-based networks measure the total column volume mixing ratio (VMR) of these gases, they rely on sunlight, and column measurements at night are comparatively scarce. We present a new algorithm, Astroclimes, that hopes to complement and extend nighttime CO$_2$ and CH4 column measurements. Astroclimes can measure the abundance of GHGs on Earth by generating a model telluric transmission spectra and fitting it to the spectra of telluric standard stars in the near-infrared taken by ground-based telescopes. A Markov Chain Monte Carlo (MCMC) analysis on an extensive dataset from the CARMENES spectrograph showed that Astroclimes was able to recover the long term trend known to be present in the molecular abundances of both CO$_2$ and CH$_4$, but not their seasonal cycles. Using the Copernicus Atmosphere Monitoring Service (CAMS) global greenhouse gas reanalysis model (EGG4) as a benchmark, we identified an overall vertical shift in our data and quantified the long term scatter in our retrievals. The scatter on a 1 hour timescale, however, is much lower, and is on par with the uncertainties on individual measurements. Although currently the precision of the method is not in line with state of the art techniques using dedicated instrumentation, it shows promise for further development. |
| 2025-09-12 | [Learning Constraint Surrogate Model for Two-stage Stochastic Unit Commitment](http://arxiv.org/abs/2509.10246v1) | Amir Bahador Javadi, Amin Kargarian et al. | The increasing penetration of renewable energy sources introduces significant uncertainty in power system operations, making traditional deterministic unit commitment approaches computationally expensive. This paper presents a machine learning surrogate modeling approach designed to reformulate the feasible design space of the two-stage stochastic unit commitment (TSUC) problem, reducing its computational complexity. The proposed method uses a support vector machine (SVM) to construct a surrogate model based on the governing equations of the learner. This model replaces the original 2|L| * |S| transmission line flow constraints, where |S| is the number of uncertainty scenarios and |L| is the number of transmission lines with |S| much less than |L|, with a significantly reduced set of 1 * |S| linear inequality constraints. The approach is theoretically grounded in the polyhedral structure of the feasible region under the DC power flow approximation, enabling the transformation of 2|L| line flow limit constraints into a single linear constraint. The surrogate model is trained using data generated from computationally efficient DC optimal power flow simulations. Simulation results on the IEEE 57-bus and 118-bus systems demonstrate SVM halfspace constraint accuracy of 99.72% and 99.88%, respectively, with TSUC computational time reductions of 46% and 31% and negligible generation cost increases (0.63% and 0.88% on average for IEEE 57- and 118-bus systems, respectively). This shows the effectiveness of the proposed approach for practical power system operations under renewable energy uncertainty. |
| 2025-09-12 | [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](http://arxiv.org/abs/2509.10227v1) | Ãngel LadrÃ³n, Miguel SÃ¡nchez-DomÃ­nguez et al. | Fatigue life prediction is essential in both the design and operational phases of any aircraft, and in this sense safety in the aerospace industry requires early detection of fatigue cracks to prevent in-flight failures. Robust and precise fatigue life predictors are thus essential to ensure safety. Traditional engineering methods, while reliable, are time consuming and involve complex workflows, including steps such as conducting several Finite Element Method (FEM) simulations, deriving the expected loading spectrum, and applying cycle counting techniques like peak-valley or rainflow counting. These steps often require collaboration between multiple teams and tools, added to the computational time and effort required to achieve fatigue life predictions. Machine learning (ML) offers a promising complement to traditional fatigue life estimation methods, enabling faster iterations and generalization, providing quick estimates that guide decisions alongside conventional simulations.   In this paper, we present a ML-based pipeline that aims to estimate the fatigue life of different aircraft wing locations given the flight parameters of the different missions that the aircraft will be operating throughout its operational life. We validate the pipeline in a realistic use case of fatigue life estimation, yielding accurate predictions alongside a thorough statistical validation and uncertainty quantification. Our pipeline constitutes a complement to traditional methodologies by reducing the amount of costly simulations and, thereby, lowering the required computational and human resources. |
| 2025-09-11 | [Cosmology inference with perturbative forward modeling at the field level: a comparison with joint power spectrum and bispectrum analyses](http://arxiv.org/abs/2509.09673v1) | Kazuyuki Akitsu, Marko SimonoviÄ‡ et al. | We extend field-level inference to jointly constrain the cosmological parameters $\{A,\omega_{\rm cdm},H_0\}$, in both real and redshift space. Our analyses are based on mock data generated using a perturbative forward model, with noise drawn from a Gaussian distribution with a constant power spectrum. This idealized setting, where the field-level likelihood is exactly Gaussian, allows us to precisely quantify the information content in the nonlinear field on large scales. We find that field-level inference accurately recovers all cosmological parameters in both real and redshift space, with uncertainties consistent with perturbation theory expectations. We show that these error bars are comparable to those obtained from a joint power spectrum and bispectrum analysis using the same perturbative model. Finally, we perform several tests using the Gaussian field-level likelihood to fit the mock data where the true noise model is non-Gaussian, and find significant biases in the inferred cosmological parameters. These results highlight that the success of field-level inference critically depends on using the correct likelihood, which may be the primary challenge for applying this method to smaller scales even in the perturbative regime. |
| 2025-09-11 | [Measuring Epistemic Humility in Multimodal Large Language Models](http://arxiv.org/abs/2509.09658v1) | Bingkui Tong, Jiaer Xia et al. | Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench. |
| 2025-09-11 | [Reconstructing the origin of black hole mergers using sparse astrophysical models](http://arxiv.org/abs/2509.09647v1) | V. Gayathri, Giuliano Iorio et al. | The astrophysical origin of binary black hole mergers discovered by LIGO and Virgo remains uncertain. Efforts to reconstruct the processes that lead to mergers typically rely on either astrophysical models with fixed parameters, or continuous analytical models that can be fit to observations. Given the complexity of astrophysical formation mechanisms, these methods typically cannot fully take into account model uncertainties, nor can they fully capture the underlying processes. Here, we present a merger population analysis that can take a discrete set of simulated model distributions as its input to interpret observations. The analysis can take into account multiple formation scenarios as fractional contributors to the total set of observations, and can naturally account for model uncertainties. We apply this technique to investigate the origin of black hole mergers observed by LIGO Virgo. Specifically, we consider a model of AGN assisted black hole merger distributions, exploring a range of AGN parameters along with several {{SEVN}} population synthesis models that vary in common envelope efficiency parameter ($\alpha$) and metallicity ($Z$). We estimate the posterior distributions for AGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs. The inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN sub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN sub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$. |
| 2025-09-11 | [Constraints on Ultra-heavy DM from TeV-PeV gamma-ray diffuse measurements](http://arxiv.org/abs/2509.09609v1) | Manuel Rocamora, Pedro De La Torre Luque et al. | Recent experiments have measured the Galactic $\gamma$-ray diffuse emission up to PeV energies, opening a window to study acceleration of Galactic cosmic rays and their propagation up to the cosmic-ray knee. Furthermore, these observations provide a powerful tool to set strong constraints into very-heavy dark matter particles, with masses in the TeV-PeV range. In this paper, we explore the potential of the newest observations of diffuse emissions at the Galactic plane from HAWC and LHAASO to probe this kind of dark matter over a wide mass range. Here, we model secondary emissions (inverse-Compton) from the electrons and positrons produced in the annihilation/decay of dark matter, on top of their prompt $\gamma$-ray emission, including the effects of absorption of high-energy photons via pair production. Furthermore, we show that including the astrophysical backgrounds (namely diffuse emission from cosmic-ray collisions or emission from unresolved sources) can significantly improve these limits. We find that the new measurements provided, specially by LHAASO with the combination of the WCDA and KM2A detectors, allow us to set strong constraints in decaying dark matter, being competitive and even improving the strongest constraints at the moment. We also highlight that these regions lead to constraints that are less affected by uncertainties from the dark matter distribution and discuss how CTA north and SWGO will be able to improve limits in this mass range. |
| 2025-09-11 | [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](http://arxiv.org/abs/2509.09599v1) | Ira J. S. Shokar, Rich R. Kerswell et al. | We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events. |
| 2025-09-11 | [ObjectReact: Learning Object-Relative Control for Visual Navigation](http://arxiv.org/abs/2509.09594v1) | Sourav Garg, Dustin Craggs et al. | Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/ |
| 2025-09-11 | [Unified Framework for Hybrid Aleatory and Epistemic Uncertainty Propagation via Decoupled Multi-Probability Density Evolution Method](http://arxiv.org/abs/2509.09535v1) | Yi Luo, Meng-Ze Lyu et al. | This paper presents a unified framework for uncertainty propagation in dynamical systems involving hybrid aleatory and epistemic uncertainties. The framework accommodates precise probabilistic, imprecise probabilistic, and non-probabilistic representations, including the distribution-free probability-box (p-box). A central aspect of the framework involves transforming the original uncertainty inputs into an augmented random space, yielding the primary challenge of determining the conditional probability density function (PDF) of the response quantity of interest given epistemic uncertainty parameters. The recently proposed decoupled multi-probability density evolution method (decoupled M-PDEM) is employed to numerically solve the conditional PDF for complex dynamical systems. Several numerical examples illustrate the applicability, efficiency, and accuracy of the proposed framework. These include a linear single-degree-of-freedom (SDOF) system subject to Gaussian white noise with its natural frequency modeled as a p-box, a 10-DOF hysteretic structure subject to imprecise seismic loads, and a crash box model with mixed random and interval system parameters. |
| 2025-09-11 | [Mapping of discrete range modulated proton radiograph to water-equivalent path length using machine learning](http://arxiv.org/abs/2509.09514v1) | Atiq Ur Rahman, Chun-Chieh Wang et al. | Objective. Proton beams enable localized dose delivery. Accurate range estimation is essential, but planning still relies on X-ray CT, which introduces uncertainty in stopping power and range. Proton CT measures water equivalent thickness directly but suffers resolution loss from multiple Coulomb scattering. We develop a data driven method that reconstructs water equivalent path length (WEPL) maps from energy resolved proton radiographs, bypassing intermediate reconstructions. Approach. We present a machine learning pipeline for WEPL from high dimensional radiographs. Data were generated with the TOPAS Monte Carlo toolkit, modeling a clinical nozzle and a patient CT. Proton energies spanned 70-230 MeV across 72 projection angles. Principal component analysis reduced input dimensionality while preserving signal. A conditional GAN with gradient penalty was trained for WEPL prediction using a composite loss (adversarial, MSE, SSIM, perceptual) to balance sharpness, accuracy, and stability. Main results. The model reached a mean relative WEPL deviation of 2.5 percent, an SSIM of 0.97, and a proton radiography gamma index passing rate of 97.1 percent (2 percent delta WEPL, 3 mm distance-to-agreement) on a simulated head phantom. Results indicate high spatial fidelity and strong structural agreement. Significance. WEPL can be mapped directly from proton radiographs with deep learning while avoiding intermediate steps. The method mitigates limits of analytic techniques and may improve treatment planning. Future work will tune the number of PCA components, include detector response, explore low dose settings, and extend multi angle data toward full proton CT reconstruction; it is compatible with clinical workflows. |
| 2025-09-11 | [Spin Constraints on 4U 1630-47 via combined Continuum Fitting and Reflection methods: a comparative study using Frequentist and Bayesian statistics](http://arxiv.org/abs/2509.09481v1) | Debtroy Das, Honghui Liu et al. | We present a comprehensive Bayesian spectral analysis of the black hole X-ray binary 4U 1630-47 during its 2022 outburst, using simultaneous \textit{NICER} and \textit{NuSTAR} observations. Using the traditional frequentist approach, we build our model combining reflection spectroscopy with continuum fitting techniques and analyse the data. In the Bayesian framework, we jointly constrain the black hole's spin, mass, inclination, and distance within a unified framework. Employing nested sampling, we capture parameter degeneracies and rigorously propagate both statistical and systematic uncertainties. Our results yield robust and precise spin measurements from both approaches. Our Bayesian analysis fetches spin $a_*= 0.93_{-0.04}^{+0.05}$, mass $M_{\rm BH} = 9.0_{-2.0}^{+2.0} \, M_\odot$, distance $d_{\rm BH} = 10.5_{-1.2}^{+1.3}$~kpc, and inclination angle $i=53.8_{-1.3}^{+1.3}$~deg. It also demonstrates the power of Bayesian inference in fetching valuable insights into the complex physics of black hole accretion and enabling high-confidence measurements of fundamental parameters. |
| 2025-09-11 | [Dark Vector Boson Bremsstrahlung: New Form Factors for a Broader Class of Models](http://arxiv.org/abs/2509.09437v1) | Felix Kling, Peter Reimitz et al. | We explore the sensitivity of collider experiments to a broad class of GeV-scale dark vector models of new physics via production in proton and neutron bremsstrahlung and initial state radiation. This is achieved using a new physically motivated model for timelike vector form factors with generic charges for both protons and neutrons, which is fit to a variety of timelike and spacelike data with quantified uncertainties. The production model for both proton and neutron bremsstrahlung is applied to re-cast and extend the reach of existing FASER data to GeV-mass dark photons, $U(1)_B$, $U(1)_{B-L}$, and photophobic vectors, as well as forecasts for millicharged particles at FORMOSA. |
| 2025-09-10 | [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](http://arxiv.org/abs/2509.08777v1) | Eric Slyman, Mehrab Tanjim et al. | Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation. |
| 2025-09-10 | [FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model](http://arxiv.org/abs/2509.08742v1) | Yanlong Wang, Jian Xu et al. | Financial time series forecasting is both highly significant and challenging. Previous approaches typically standardized time series data before feeding it into forecasting models, but this encoding process inherently leads to a loss of important information. Moreover, past time series models generally require fixed numbers of variables or lookback window lengths, which further limits the scalability of time series forecasting. Besides, the interpretability and the uncertainty in forecasting remain areas requiring further research, as these factors directly impact the reliability and practical value of predictions. To address these issues, we first construct a diverse financial image-text dataset (FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization (UARPO) method to enable the model not only output predictions but also analyze the uncertainty of those predictions. We then proposed FinZero, a multimodal pre-trained model finetuned by UARPO to perform reasoning, prediction, and analytical understanding on the FVLDB financial time series. Extensive experiments validate that FinZero exhibits strong adaptability and scalability. After fine-tuning with UARPO, FinZero achieves an approximate 13.48\% improvement in prediction accuracy over GPT-4o in the high-confidence group, demonstrating the effectiveness of reinforcement learning fine-tuning in multimodal large model, including in financial time series forecasting tasks. |
| 2025-09-10 | [Quantifying model prediction sensitivity to model-form uncertainty](http://arxiv.org/abs/2509.08708v1) | Teresa Portone, Rebekah D. White et al. | Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration). |
| 2025-09-10 | [Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of X-ray instruments used in the large, multi-mission observational campaign](http://arxiv.org/abs/2509.08649v1) | XRISM collaboration | Accurate X-ray spectroscopic measurements are fundamental for deriving basic physical parameters of the most abundant baryon components in the Universe. The plethora of X-ray observatories currently operational enables a panchromatic view of the high-energy emission of celestial sources. However, uncertainties in the energy-dependent calibration of the instrument transfer functions (e.g. the effective area, energy redistribution, or gain) can limit - and historically, did limit - the accuracy of X-ray spectroscopic measurements.   We revised the status of the cross-calibration among the scientific payload on board four operation missions: Chandra, NuSTAR, XMM-Newton, and the recently launched XRISM. XRISM carries the micro-calorimeter Resolve, which yields the best energy resolution at energies above 2 keV. For this purpose, we used the data from a 10-day-long observational campaign targeting the nearby active galactic nucleus NGC 3783, carried out in July 2024.   We present a novel model-independent method for assessing the cross-calibration status that is based on a multi-node spline of the spectra with the highest-resolving power (XRISM/Resolve in our campaign). We also estimated the impact of the intrinsic variability of NGC 3783 on the cross-calibration status due to the different time coverages of participating observatories and performed an empirical reassessment of the Resolve throughput at low energies.   Based on this analysis, we derived a set of energy-dependent correction factors of the observed responses, enabling a statistically robust analysis of the whole spectral dataset. They will be employed in subsequent papers describing the astrophysical results of the campaign. |
| 2025-09-10 | [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](http://arxiv.org/abs/2509.08638v1) | Rebecca Martin, Jay Patrikar et al. | Specialized machine learning models, regardless of architecture and training, are susceptible to failures in deployment. With their increasing use in high risk situations, the ability to audit these models by determining their operational design domain (ODD) is crucial in ensuring safety and compliance. However, given the high-dimensional input spaces, this process often requires significant human resources and domain expertise. To alleviate this, we introduce \coolname, an LLM-Agent centric framework for automated generation of semantically relevant test cases to search for failure modes in specialized black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit a uncertainty-aware failure distribution model on a learned text-embedding manifold by projecting the high-dimension input space to low-dimension text-embedding latent space. The LLM-Agent is tasked with iteratively building the failure landscape by leveraging tools for generating test-cases to probe the model-under-test (MUT) and recording the response. The agent also guides the search using tools to probe uncertainty estimate on the low dimensional manifold. We demonstrate this process in a simple case using models trained with missing digits on the MNIST dataset and in the real world setting of vision-based intruder detection for aerial vehicles. |
| 2025-09-10 | [Isotope shift spectroscopy in mercury vapors: a valid alternative to ytterbium for new physics search](http://arxiv.org/abs/2509.08622v1) | Stefania Gravina, Antonio Castrillo et al. | Precision isotope shift metrology in the deep-UV region has been performed for all bosonic isotopes of mercury with a zero nuclear spin, by using the technique of frequency-comb referenced, wavelength-modulated, saturated absorption spectroscopy. The absolute center frequencies of the 6s$^2$ $^1$S$_0$ $\rightarrow$ 6s6p $^3$P$_1$ intercombination line have been measured with precision in the range of 2.5 - 5.9 10$^{-12}$, in temperature-stabilized mercury vapor samples with natural abundances. Frequency shifts in four isotope pairs have been determined with unprecedented accuracy, the global uncertainty being improved by a factor greater than 20 with respect to the best experimental data of the past literature. Our data set, when combined with previous measurements on the 6s6p $^3$P$_2$$\rightarrow$6s7s $^3$S$_1$ transition at 546 nm, allows us to build a King plot that reveals a nonlinearity with a statistical significance of 4.6$\sigma$. |
| 2025-09-10 | [No-Knowledge Alarms for Misaligned LLMs-as-Judges](http://arxiv.org/abs/2509.08593v1) | AndrÃ©s Corrada-Emmanuel | If we use LLMs as judges to evaluate the complex decisions of other LLMs, who or what monitors the judges? Infinite monitoring chains are inevitable whenever we do not know the ground truth of the decisions by experts and we do not want to trust them. One way to ameliorate our evaluation uncertainty is to exploit the use of logical consistency between disagreeing experts. By observing how LLM judges agree and disagree while grading other LLMs, we can compute the only possible evaluations of their grading ability. For example, if two LLM judges disagree on which tasks a third one completed correctly, they cannot both be 100\% correct in their judgments. This logic can be formalized as a Linear Programming problem in the space of integer response counts for any finite test. We use it here to develop no-knowledge alarms for misaligned LLM judges. The alarms can detect, with no false positives, that at least one member or more of an ensemble of judges are violating a user specified grading ability requirement. |
| 2025-09-10 | [Interpretability as Alignment: Making Internal Understanding a Design Principle](http://arxiv.org/abs/2509.08592v1) | Aadit Sengupta, Pratinav Seth et al. | Large neural models are increasingly deployed in high-stakes settings, raising concerns about whether their behavior reliably aligns with human values. Interpretability provides a route to internal transparency by revealing the computations that drive outputs. We argue that interpretability especially mechanistic approaches should be treated as a design principle for alignment, not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer intuitive but correlational explanations, while mechanistic techniques like circuit tracing or activation patching yield causal insight into internal failures, including deceptive or misaligned reasoning that behavioral methods like RLHF, red teaming, or Constitutional AI may overlook. Despite these advantages, interpretability faces challenges of scalability, epistemic uncertainty, and mismatches between learned representations and human concepts. Our position is that progress on safe and trustworthy AI will depend on making interpretability a first-class objective of AI research and development, ensuring that systems are not only effective but also auditable, transparent, and aligned with human intent. |
| 2025-09-10 | [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](http://arxiv.org/abs/2509.08578v1) | Hong Liu | Timely and robust influenza incidence forecasting is critical for public health decision-making. To address this, we present MAESTRO, a Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves robustness by adaptively fusing multi-modal inputs-including surveillance, web search trends, and meteorological data-and leveraging a comprehensive spectro-temporal architecture. The model first decomposes time series into seasonal and trend components. These are then processed through a hybrid feature enhancement pipeline combining Transformer-based encoders, a Mamba state-space model for long-range dependencies, multi-scale temporal convolutions, and a frequency-domain analysis module. A cross-channel attention mechanism further integrates information across the different data modalities. Finally, a temporal projection head performs sequence-to-sequence forecasting, with an optional estimator to quantify prediction uncertainty. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO shows strong competitive performance, demonstrating a superior model fit and relative accuracy, achieving a state-of-the-art R-square of 0.956. Extensive ablations confirm the significant contributions of both multi-modal fusion and the spectro-temporal components. Our modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens.Our publicly available pipeline presents a powerful, unified framework, demonstrating the critical synergy of advanced spectro-temporal modeling and multi-modal data fusion for robust epidemiological forecasting. |
| 2025-09-10 | [Accelerating first-principles molecular-dynamics thermal conductivity calculations for complex systems](http://arxiv.org/abs/2509.08573v1) | Sandro Wieser, YuJie Cen et al. | Atomistic simulations of heat transport in complex materials are costly and hard to converge. This has led to the development of several noise reduction techniques applicable to equilibrium molecular-dynamics simulations. We analyze the performance of those strategies, taking InAs nanowires as our benchmark due to the diverse structures and complex phonon spectra of these quasi-1D systems. We demonstrate how, for low-thermal-conductivity systems, cepstral analysis can reduce computational demands while still delivering accurate results that do not require discarding arbitrary parts of the dataset. However, issues with this approach are revealed when treating high-thermal-conductivity systems, where the thermal conductivity is significantly underestimated. We discuss alternative methods to be used in that situation, relying on uncertainty propagation from independent simulations. We show that the contributions of the covariance matrix have to be included for a quantitative assessment of the error. The combination of these strategies with machine-learning interatomic potentials (MLIPs) provides an accelerated, robust workflow applicable to a diverse set of systems, as our examples using a highly transferable MACE potential illustrate. |
| 2025-09-09 | [Advanced Weights for IXPE Polarization Analysis](http://arxiv.org/abs/2509.07981v1) | Jack T. Dinsmore, Roger W. Romani | As the Imaging X-ray Polarimetry Explorer (IXPE) measures increasingly faint sources, the need for precise polarimetry extraction becomes paramount. In addition to previously described neural-net (NN) weights, we introduce here point-spread function weights and particle background weights, which can be critical for faint sources. In some cases these can be augmented by time/phase and energy weights. We provide a publicly available analysis tool to incorporate these new weights, validate our method on simulated data, and test it on archival IXPE observations. Together these weights decrease the area of the polarization uncertainty contour by a factor of two and will be essential for background-limited IXPE observations. |
| 2025-09-09 | [Early warning for lensed gravitational wave counterparts from time delays of their host galaxies observed in the optical](http://arxiv.org/abs/2509.07967v1) | Sourabh Magare, Anupreeta More et al. | Gravitational lensing of gravitational waves (GWs) can be leveraged to provide early-warning times of $\mathcal{O}({\rm hours})$ to $\mathcal{O}({\rm days})$ before the merger of Binary Neutron Stars (BNSs) and Neutron Star Black Holes (NSBHs). This in turn could enable electromagnetic (EM) telescopes to capture emissions surrounding the time of the merger. In this work, we assess the practicability of lensing-driven early-warning by analysing optical images of the lensed host galaxy to predict the arrival time of subsequent BNS/NSBH signals following the observation of the first signal. We produce mock lenses with image quality and resolution similar to images taken with the Hubble Space Telescope (HST) and the ground-based Hyper Suprime-Cam (HSC) on the Subaru telescope. We compare the time delay uncertainties between these two cases for typical lensed image configurations and multiplicity. These include doubles and quads, and among quads: the fold, cusp, cross image configurations. We find that time delay uncertainties for doubles are comparable for both HST and HSC mocks. On the other hand, quads tend to provide accurate time-delay predictions (typical relative error $\sim0.1$) with HST. Analysis of a real lens led to a difference in time-delay estimates of $\mathcal{O}(\rm days)$ between the predictions derived from HST and HSC data. Our work therefore strongly advocates the need for high-resolution EM observations of lensed host galaxies to feasibly enable lensing-driven early-warning. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: Redshift Calibration of the MagLim++ Lens Sample](http://arxiv.org/abs/2509.07964v1) | G. Giannini, A. Alarcon et al. | In this work, we derive and calibrate the redshift distribution of the MagLim++ lens galaxy sample used in the Dark Energy Survey Year 6 (DES Y6) 3x2pt cosmology analysis. The 3x2pt analysis combines galaxy clustering from the lens galaxy sample and weak gravitational lensing. The redshift distributions are inferred using the SOMPZ method - a Self-Organizing Map framework that combines deep-field multi-band photometry, wide-field data, and a synthetic source injection (Balrog) catalog. Key improvements over the DES Year 3 (Y3) calibration include a noise-weighted SOM metric, an expanded Balrog catalogue, and an improved scheme for propagating systematic uncertainties, which allows us to generate O($10^8$) redshift realizations that collectively span the dominant sources of uncertainty. These realizations are then combined with independent clustering-redshift measurements via importance sampling. The resulting calibration achieves typical uncertainties on the mean redshift of 1-2%, corresponding to a 20-30% average reduction relative to DES Y3. We compress the $n(z)$ uncertainties into a small number of orthogonal modes for use in cosmological inference. Marginalizing over these modes leads to only a minor degradation in cosmological constraints. This analysis establishes the MagLim++ sample as a robust lens sample for precision cosmology with DES Y6 and provides a scalable framework for future surveys. |
| 2025-09-09 | [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](http://arxiv.org/abs/2509.07961v1) | Valen Tagliabue, Leonard Dung | We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration. |
| 2025-09-09 | [Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation](http://arxiv.org/abs/2509.07957v1) | Shunlei Li, Longsen Gao et al. | Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: improved mitigation of spatially varying observational systematics with masking](http://arxiv.org/abs/2509.07943v1) | M. RodrÃ­guez-Monroy, N. Weaverdyck et al. | As photometric surveys reach unprecedented statistical precision, systematic uncertainties increasingly dominate large-scale structure probes relying on galaxy number density. Defining the final survey footprint is critical, as it excludes regions affected by artefacts or suboptimal observing conditions. For galaxy clustering, spatially varying observational systematics, such as seeing, are a leading source of bias. Template maps of contaminants are used to derive spatially dependent corrections, but extreme values may fall outside the applicability range of mitigation methods, compromising correction reliability. The complexity and accuracy of systematics modelling depend on footprint conservativeness, with aggressive masking enabling simpler, robust mitigation. We present a unified approach to define the DES Year 6 joint footprint, integrating observational systematics templates and artefact indicators that degrade mitigation performance. This removes extreme values from an initial seed footprint, leading to the final joint footprint. By evaluating the DES Year 6 lens sample MagLim++ plus plus on this footprint, we enhance the Iterative Systematics Decontamination (ISD) method, detecting non-linear systematic contamination and improving correction accuracy. While the mask's impact on clustering is less significant than systematics decontamination, it remains non-negligible, comparable to statistical uncertainties in certain w(theta) scales and redshift bins. Supporting coherent analyses of galaxy clustering and cosmic shear, the final footprint spans 4031.04 deg2, setting the basis for DES Year 6 1x2pt, 2x2pt, and 3x2pt analyses. This work highlights how targeted masking strategies optimise the balance between statistical power and systematic control in Stage-III and -IV surveys. |
| 2025-09-09 | [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](http://arxiv.org/abs/2509.07925v1) | Tuo Wang, Adithya Kulkarni et al. | Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ. |
| 2025-09-09 | [Forecasting dementia incidence](http://arxiv.org/abs/2509.07874v1) | JÃ©rÃ´me R. Simons, Yuntao Chen et al. | This paper estimates the stochastic process of how dementia incidence evolves over time. We proceed in two steps: first, we estimate a time trend for dementia using a multi-state Cox model. The multi-state model addresses problems of both interval censoring arising from infrequent measurement and also measurement error in dementia. Second, we feed the estimated mean and variance of the time trend into a Kalman filter to infer the population level dementia process. Using data from the English Longitudinal Study of Aging (ELSA), we find that dementia incidence is no longer declining in England. Furthermore, our forecast is that future incidence remains constant, although there is considerable uncertainty in this forecast. Our two-step estimation procedure has significant computational advantages by combining a multi-state model with a time series method. To account for the short sample that is available for dementia, we derive expressions for the Kalman filter's convergence speed, size, and power to detect changes and conclude our estimator performs well even in short samples. |
| 2025-09-09 | [Are Humans as Brittle as Large Language Models?](http://arxiv.org/abs/2509.07869v1) | Jiahui Li, Sean Papay et al. | The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs. |
| 2025-09-09 | [Jet cone size dependence of single inclusive jet suppression due to jet quenching in Pb+Pb collisions at $\sqrt{s_{\rm NN}}=5.02$ TeV](http://arxiv.org/abs/2509.07842v1) | Qing-Fei Han, Man Xie et al. | Jet suppression in high-energy heavy-ion collisions results from jet energy loss and transverse-momentum broadening during jet propagation through the quark-gluon plasma (QGP). The jet cone size ($R$) dependence of this suppression offers crucial insights into the energy loss mechanisms and QGP transport properties. In our study, we implement a comprehensive approach within the perturbative QCD parton model that incorporates both elastic and inelastic energy loss mechanisms. For elastic processes the contribution from recoiling thermal partons reduces the net in-cone energy loss for a given jet radius. For inelastic processes, we account for the angular distribution of radiated gluons, the thermalization of soft gluons, and transverse-momentum broadening. Using this framework, we calculate the jet nuclear modification factors ($R_{AA}$) and their double ratios $R_{AA}(R=0.2-1.0)/R_{AA}(R=0.2)$, and systematically compare with ALICE, ATLAS and CMS data in 0-10\% and 30-50\% Pb+Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02~TeV. Numerical results show that $R_{AA}$ increases with the cone size $R$ because the in-cone energy loss decreases at larger radii. Specifically, as the radius $R$ grows, the probability for elastically scattered partons to escape the jet cone and the likelihood for radiated gluons to fall outside the cone both decrease, resulting in a net reduction of energy loss. The $R_{AA}$ double ratios are approximately unity for small radii ($R=0.4$ relative to $R=0.2$) and at high $p_{\rm T}\gtrsim200$ GeV$/c$, in agreement with the data within uncertainties. |
| 2025-09-08 | [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](http://arxiv.org/abs/2509.06938v1) | Praneet Suresh, Jack Stanley et al. | As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk. |
| 2025-09-08 | [Black-hole mass estimation through accretion disk spectral fitting for high-redshift blazars](http://arxiv.org/abs/2509.06933v1) | G. Kyriopoulos, M. Petropoulou et al. | High-redshift ($z>2$) blazars, with relativistic jets aligned toward us, probe the most powerful end of the active galactic nuclei (AGN) population. We aim at determining the black hole masses and mass accretion rates of high-$z$ blazars in a common framework that utilizes a Markov Chain Monte Carlo (MCMC) fitting method and the Shakura-Sunayev multi-temperature accretion disk model, accounting also for attenuation due to neutral hydrogen gas in the intergalactic medium (IGM). We compiled a sample of 23 high-redshift blazars from the literature with publicly available infrared-to-ultraviolet photometric data. We performed a Bayesian fit to the spectral energy distribution (SED) of the accretion disk, accounting for upper limits, and determined the black hole masses and mass accretion rates with their uncertainties. We also examined the impact of optical-ultraviolet attenuation due to gas in the IGM. We find that neglecting IGM attenuation in SED fits leads to systematically larger black-hole mass estimates and correspondingly lower Eddington ratios, with the bias becoming more severe at higher redshift. Our MCMC fits yield median black-hole masses in the range $\sim (10^{8}-10^{10})\,M_{\odot}$ and a broad distribution of median Eddington ratios ($\lambda_{\rm Edd}\sim 0.04 - 1$). Comparison with previous literature shows no clear method-dependent systematic offsets, although individual mass estimates can differ by up to a factor of a few. We also demonstrate that assumptions about black-hole spin introduce a systematic degeneracy. This work is to our knowledge the first systematic study to model the accretion-disk emission of a large sample of high-$z$ blazars within a single, consistent statistical framework. Our results emphasize the importance of accounting for IGM attenuation and of using uniform fitting methods when comparing disk-based black hole estimates across samples. |
| 2025-09-08 | [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](http://arxiv.org/abs/2509.06902v1) | Aivin V. Solatorio | Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty. |
| 2025-09-08 | [Stochastic modelling of cosmic-ray sources for Galactic diffuse emissions](http://arxiv.org/abs/2509.06857v1) | Anton Stall, Philipp Mertsch | Galactic diffuse emissions in gamma rays and neutrinos arise from interactions of cosmic rays with the interstellar medium and probe the cosmic-ray intensity away from the Solar system. Model predictions for those are influenced by the properties of cosmic-ray sources, and understanding the impact of cosmic-ray sources on Galactic diffuse emissions is key for interpreting measurements by LHAASO, Tibet AS-gamma, IceCube, and the upcoming SWGO. We consider supernova remnants as prototypical cosmic-ray sources and study the impact of their discreteness on the Galactic diffuse emissions in different source injection and near-source transport models in a stochastic Monte Carlo study. Three lessons exemplify the results of our simulations: First, the distributions of Galactic diffuse emission intensities can be described by a mixture model of stable laws and Gaussian distributions. Second, the maximal deviations caused by discrete sources across the sky depend on energy, reaching typically tens of percent in burst-like and energy-dependent escape scenarios but order unity or larger in a time-dependent diffusion scenario. Third, the additional model uncertainty from source stochasticity is subdominant in burst-like and energy-dependent escape scenarios, but becomes sizeable above some tens of TeV in the time-dependent diffusion scenario, where it can help reconcile model predictions with LHAASO measurements. With increased spatial resolution, especially at energies beyond tens of TeV, measurements of Galactic diffuse emissions can be expected to constrain source models and locate cosmic ray sources. |
| 2025-09-08 | [Adversarial Obstacle Placement with Spatial Point Processes for Optimal Path Disruption](http://arxiv.org/abs/2509.06837v1) | Li Zhou, Elvan Ceyhan et al. | We investigate the Optimal Obstacle Placement (OOP) problem under uncertainty, framed as the dual of the Optimal Traversal Path problem in the Stochastic Obstacle Scene paradigm. We consider both continuous domains, discretized for analysis, and already discrete spatial grids that form weighted geospatial networks using 8-adjacency lattices. Our unified framework integrates OOP with stochastic geometry, modeling obstacle placement via Strauss (regular) and Mat\'ern (clustered) processes, and evaluates traversal using the Reset Disambiguation algorithm. Through extensive Monte Carlo experiments, we show that traversal cost increases by up to 40% under strongly regular placements, while clustered configurations can decrease traversal costs by as much as 25% by leaving navigable corridors compared to uniform random layouts. In mixed (with both true and false obstacles) scenarios, increasing the proportion of true obstacles from 30% to 70% nearly doubles the traversal cost. These findings are further supported by statistical analysis and stochastic ordering, providing rigorous insights into how spatial patterns and obstacle compositions influence navigation under uncertainty. |
| 2025-09-08 | [Uncertainty Principle from Operator Asymmetry](http://arxiv.org/abs/2509.06760v1) | Xingze Qiu | The uncertainty principle is fundamentally rooted in the algebraic asymmetry between observables. We introduce a new class of uncertainty relations grounded in the resource theory of asymmetry, where incompatibility is quantified by an observable's intrinsic, state-independent capacity to break the symmetry associated with another. This ``operator asymmetry,'' formalized as the asymmetry norm, leads to a variance-based uncertainty relation for pure states that can be tighter than the standard Robertson bound, especially in the near-compatible regime. Most significantly, this framework resolves a long-standing open problem in quantum information theory: the formulation of a universally valid, product-form uncertainty relation for the Wigner-Yanase skew information. We demonstrate the practical power of our framework by deriving tighter quantum speed limits for the dynamics of nearly conserved quantities, which are crucial for understanding non-equilibrium phenomena such as prethermalization and many-body localization. This work provides both a new conceptual lens for understanding quantum uncertainty and a powerful toolkit for its application. |
| 2025-09-08 | [Nested Optimal Transport Distances](http://arxiv.org/abs/2509.06702v1) | Ruben Bontorno, Songyan Hou | Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches. |
| 2025-09-08 | [Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties](http://arxiv.org/abs/2509.06697v1) | Tanujit Chakraborty, Donia Besher et al. | Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \texttt{narfima} \textbf{R} package provides an implementation of our approach. |
| 2025-09-08 | [Port-Hamiltonian Neural Networks: From Theory to Simulation of Interconnected Stochastic Systems](http://arxiv.org/abs/2509.06674v1) | Luca Di Persio, Matthias Ehrhardt et al. | This work introduces a new framework integrating port-Hamiltonian systems (PHS) and neural network architectures. This framework bridges the gap between deterministic and stochastic modeling of complex dynamical systems. We introduce new mathematical formulations and computational methods that expand the geometric structure of PHS to account for uncertainty, environmental noise, and random perturbations. Building on these advances, we introduce stochastic port-Hamiltonian neural networks (pHNNs), which facilitate the accurate learning and prediction of non-autonomous and interconnected stochastic systems. Our proposed framework generalizes passivity concepts to the stochastic regime, ensuring stability while maintaining the system's energy-consistent structure. Extensive simulations, including those involving damped mass-spring systems, Duffing oscillators, and robotic control tasks, demonstrate the capability of pHNNs to capture complex dynamics with high fidelity, even under noise and uncertainty. This unified approach establishes a foundation for the robust, data-driven modeling and control of nonlinear stochastic systems. |
| 2025-09-08 | [The complementary of CTAO, direct detection and collider searches for dark matter in Effective Field Theories and Simplified models](http://arxiv.org/abs/2509.06628v1) | Igor Reis, Andre Scaffidi et al. | This paper explores the sensitivity of the Cherenkov Telescope Array Observatory to dark matter annihilation in the Galactic Center, within the frameworks of Effective Field Theory and Simplified Models. We present sensitivity forecasts, utilizing an up-to-date instrument configuration and incorporating the latest models for Galactic Diffuse Emission. A key aspect of our work is the inclusion of updated dark matter density profiles, J-factors, and velocity dispersion distributions derived from the FIRE-2 cosmological hydrodynamical simulations, which significantly impact the expected indirect detection signals. Furthermore, we update the constraints from direct detection experiments (Xenon1T and LZ) taking into account the astrophysical uncertainties informed by the FIRE-2 simulations, and also investigate limits coming from collider searches (ATLAS and CMS). Our analysis reveals improved constraints on the effective suppression scale ($M_*$) in the Effective Field Theory framework and on the mediator mass ($M_{med}$) in Simplified Models compared to previous studies, highlighting the complementarity of the Cherenkov Telescope Array Observatory with direct and collider searches in probing a wide range of dark matter scenarios. We discuss the implications of these results for various dark matter interaction types, including scalar, pseudoscalar, vector, and axial-vector mediators, and emphasize the importance of considering realistic astrophysical inputs in interpreting dark matter search results across different experimental fronts. |
| 2025-09-05 | [Testing Magnetic Field Configurations in Spider Pulsar PSR J1723-2837 with IXPE](http://arxiv.org/abs/2509.05240v1) | Michela Negro, Haocheng Zhang et al. | We present the first X-ray polarimetry observations of a redback millisecond pulsar binary, \src, with the Imaging X-ray Polarimetry Explorer (IXPE). Redbacks are compact binaries in which a rotation-powered millisecond pulsar interacts with a non-degenerate companion via an intrabinary shock, forming ideal laboratories for probing pulsar winds and relativistic shock physics, where ordered magnetic fields and particle acceleration shape the observed radiation. We conduct a spectro-polarimetric analysis combining IXPE data with archival Chandra, XMM-Newton, NuSTAR, and Swift observations. We explore two limiting magnetic field configurations, parallel and perpendicular to the bulk flow, and simulate their expected polarization signatures using the {\tt 3DPol} radiative transport code. To account for the rapid rotation of the polarization angle predicted by these models, we implement a phase-dependent Stokes alignment procedure that preserves the polarization degree while correcting for phase-rotating PA. We also devise a new maximum-likelihood fitting strategy to determine the phase-dependence of the polarization angle by minimizing the polarization degree uncertainty. This technique shows a hint the binary may be rotating clockwise relative to the celestial north pole. We find no significant detection of polarization in the IXPE data, with PD<51% at 99% confidence level. Our results excludes the high-polarization degree scenario predicted by the perpendicular field model during the brightest orbital phase bin. Simulations show that doubling the current exposure would make the parallel configuration detectable. The new PA rotation technique is also applicable to IXPE data of many sources whose intrinsic PA variation is apriori not known but is strictly periodic. |
| 2025-09-05 | [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](http://arxiv.org/abs/2509.05201v1) | Nariman Niknejad, Gokul S. Sankar et al. | This paper presents a robust model predictive control (MPC) framework that explicitly addresses the non-Gaussian noise inherent in deep learning-based perception modules used for state estimation. Recognizing that accurate uncertainty quantification of the perception module is essential for safe feedback control, our approach departs from the conventional assumption of zero-mean noise quantification of the perception error. Instead, it employs set-based state estimation with constrained zonotopes to capture biased, heavy-tailed uncertainties while maintaining bounded estimation errors. To improve computational efficiency, the robust MPC is reformulated as a linear program (LP), using a Minkowski-Lyapunov-based cost function with an added slack variable to prevent degenerate solutions. Closed-loop stability is ensured through Minkowski-Lyapunov inequalities and contractive zonotopic invariant sets. The largest stabilizing terminal set and its corresponding feedback gain are then derived via an ellipsoidal approximation of the zonotopes. The proposed framework is validated through both simulations and hardware experiments on an omnidirectional mobile robot along with a camera and a convolutional neural network-based perception module implemented within a ROS2 framework. The results demonstrate that the perception-aware MPC provides stable and accurate control performance under heavy-tailed noise conditions, significantly outperforming traditional Gaussian-noise-based designs in terms of both state estimation error bounding and overall control performance. |
| 2025-09-05 | [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](http://arxiv.org/abs/2509.05186v1) | Benjamin J. Zhang, Siting Liu et al. | In-context operator networks (ICON) are a class of operator learning methods based on the novel architectures of foundation models. Trained on a diverse set of datasets of initial and boundary conditions paired with corresponding solutions to ordinary and partial differential equations (ODEs and PDEs), ICON learns to map example condition-solution pairs of a given differential equation to an approximation of its solution operator. Here, we present a probabilistic framework that reveals ICON as implicitly performing Bayesian inference, where it computes the mean of the posterior predictive distribution over solution operators conditioned on the provided context, i.e., example condition-solution pairs. The formalism of random differential equations provides the probabilistic framework for describing the tasks ICON accomplishes while also providing a basis for understanding other multi-operator learning methods. This probabilistic perspective provides a basis for extending ICON to \emph{generative} settings, where one can sample from the posterior predictive distribution of solution operators. The generative formulation of ICON (GenICON) captures the underlying uncertainty in the solution operator, which enables principled uncertainty quantification in the solution predictions in operator learning. |
| 2025-09-05 | [Orlicz spaces and the uncertainty principle](http://arxiv.org/abs/2509.05185v1) | A. Iosevich, I. Li et al. | Let $f$ be a finite signal. The classical uncertainty principle tells us that the product of the support of $f$ and the support of $\hat{f}$, the Fourier transform of $f$, must satisfy $|supp(f)|\cdot|supp(\hat{f})|\geq |G|$. Recently, Iosevich and Mayeli improved the uncertainty principle for signals with Fourier supported on generic sets. This was done by employing the Fourier restriction theory in $L^p$ spaces. In this paper, we extended the $(p,q)$-restriction setting to Orlicz spaces. Then we apply uncertainty principles to the problem of exact recovery, which again extends and recovers the result that Iosevich and Mayeli obtained in Lebesgue spaces. |
| 2025-09-05 | [Deep-Field Analytical Calibration](http://arxiv.org/abs/2509.05152v1) | Andy Park, Xiangchong Li et al. | The next generation of imaging surveys, including the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), Euclid, and the Nancy Grace Roman Space Telescope, will place unprecedented constraints on cosmology using weak gravitational lensing. To fully exploit their statistical power, shear measurement methods must achieve sub-percent accuracy while mitigating systematic biases from noise, the point-spread function (PSF), blending, and shear-dependent detection. The analytical calibration framework (\texttt{AnaCal}) has demonstrated such accuracy but requires adding noise to images, reducing their effective depth. We introduce Deep-Field Analytical Calibration (\textsc{deep-field~}\texttt{AnaCal}), an extension of \texttt{AnaCal} that leverages deep-field images to compute shear responses while preserving wide-field statistical power. We validate the method on isolated and blended galaxy simulations with LSST-like seeing and noise, showing it meets the stringent requirement of multiplicative bias $|m| < 3\times10^{-3}$ at 99.7% confidence. Relative to standard \texttt{AnaCal} on wide-field images, this method improves effective galaxy number density from $17$ to $30$ arcmin$^{-2}$ for simulated 10-year LSST data. Assuming deep fields with $10\times$ the exposure of wide fields, we find the pixel noise variance in shear estimation is reduced by $30%$ and the overall shear uncertainty by $\sim 25%$. Finally, we assess sample variance impacts using the LSST Deep Drilling Fields strategy, finding an equivalent calibration uncertainty of $\lesssim 0.3%$. These results establish \textsc{deep-field~}\texttt{AnaCal} as a promising approach for shear calibration in upcoming weak lensing surveys. |
| 2025-09-05 | [An emulator-based forecasting on astrophysics and cosmology with 21 cm and density cross-correlations during EoR](http://arxiv.org/abs/2509.05096v1) | Barun Maity | The 21 cm signal arising from fluctuations in the neutral hydrogen field, and its cross-correlation with other tracers of cosmic density, are promising probes of the high-redshift Universe. In this study, we assess the potential of the 21 cm power spectrum, along with its cross power spectrum with dark matter density and associated bias, to constrain both astrophysics during the reionization era and the underlying cosmology. Our methodology involves emulating these estimators using an Artificial Neural Network (ANN), enabling efficient exploration of the parameter space. Utilizing a photon-conserving semi-numerical reionization model, we construct emulators at a fixed redshift ($z = 7.0$) for $k$-modes relevant to upcoming telescopes such as SKA-Low. We generate $\sim7000$ training samples by varying both cosmological and astrophysical parameters along with initial conditions, achieving high accuracy when compared to true simulation outputs. While forecasting, the model involves five free parameters: three cosmological ($\Omega_m$, $h$, $\sigma_8$) and two astrophysical (ionizing efficiency, $\zeta$, and minimum halo mass, $M_{\mathrm{min}}$). Using a fiducial model at the mid-reionization stage, we create a mock dataset and perform forecasting with the trained emulators. Assuming a 5% observational uncertainty combined with emulator error, we find that the 21 cm and 21 cm-density cross power spectra can constrain the Hubble parameter ($h$) to better than 6% at a confidence interval of 95%, with tight constraints on the global neutral fraction ($Q_{\mathrm{HI}}$). The inclusion of bias information further improves constraints on $\sigma_8$ (< 10% at 95% confidence). Finally, robustness tests with two alternate ionization states and a variant with higher observational uncertainty show that the ionization fractions are still reliably recovered, even when cosmological constraints weaken. |
| 2025-09-05 | [Finding your MUSE: Mining Unexpected Solutions Engine](http://arxiv.org/abs/2509.05072v1) | Nir Sweed, Hanit Hakim et al. | Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research. |
| 2025-09-05 | [Deep Inverse Rosenblatt Transport for Structural Reliability Analysis](http://arxiv.org/abs/2509.05061v1) | Aryan Tyagi, Jan N. Fuhg | Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study. |
| 2025-09-05 | [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](http://arxiv.org/abs/2509.05060v1) | Patrick Amadeus Irawan, Ryandito Diandaru et al. | We introduce Entropy2Vec, a novel framework for deriving cross-lingual language representations by leveraging the entropy of monolingual language models. Unlike traditional typological inventories that suffer from feature sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in language models to capture typological relationships between languages. By training a language model on a single language, we hypothesize that the entropy of its predictions reflects its structural similarity to other languages: Low entropy indicates high similarity, while high entropy suggests greater divergence. This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values. Empirical evaluations demonstrate that Entropy2Vec embeddings align with established typological categories and achieved competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework. |
| 2025-09-05 | [Reconstruction of the Dipole Amplitude in the Dipole Picture as a mathematical Inverse Problem](http://arxiv.org/abs/2509.05005v1) | Henri HÃ¤nninen, Antti KykkÃ¤nen et al. | We show that the inference problem of constraining the dipole amplitude with inclusive deep inelastic scattering data can be written into a discrete linear inverse problem, in an analogous manner as can be done for computed tomography. To this formulation of the problem, we apply standard inverse problems methods and algorithms to reconstruct known dipole amplitudes from simulated reduced cross section data with realistic precision. The main difference of this approach to previous works is that this implementation does not require any fit parametrization of the dipole amplitude. The freedom from parametrization also enables us for the first time to quantify the uncertainties of the inferred dipole amplitude in a novel more general framework. This mathematical approach to small-$x$ phenomenology opens a path to parametrization bias free inference of the dipole amplitude from HERA and Electron--Ion Collider data. |
| 2025-09-04 | [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](http://arxiv.org/abs/2509.04430v1) | Nikolay Kartashev, Ivan Rubachev et al. | Recent advancements in tabular deep learning have demonstrated exceptional practical performance, yet the field often lacks a clear understanding of why these techniques actually succeed. To address this gap, our paper highlights the importance of the concept of data uncertainty for explaining the effectiveness of the recent tabular DL methods. In particular, we reveal that the success of many beneficial design choices in tabular DL, such as numerical feature embeddings, retrieval-augmented models and advanced ensembling strategies, can be largely attributed to their implicit mechanisms for managing high data uncertainty. By dissecting these mechanisms, we provide a unifying understanding of the recent performance improvements. Furthermore, the insights derived from this data-uncertainty perspective directly allowed us to develop more effective numerical feature embeddings as an immediate practical outcome of our analysis. Overall, our work paves the way to foundational understanding of the benefits introduced by modern tabular methods that results in the concrete advancements of existing techniques and outlines future research directions for tabular DL. |
| 2025-09-04 | [Generation of Lognormal Synthetic Lyman-$Î±$ Forest Spectra for $P_{1D}$ Analysis](http://arxiv.org/abs/2509.04405v1) | Meagan Herbold, Naim GÃ¶ksel KaraÃ§aylÄ± et al. | The one-dimensional flux power spectrum (P1D) of the Lyman-$\alpha$ forest probes small-scale structure in the intergalactic medium (IGM) and is therefore sensitive to a variety of cosmological and astrophysical parameters. These include the amplitude and shape of the matter power spectrum, the thermal history of the IGM, the sum of neutrino masses, and potential small-scale fluctuations due to the nature of dark matter. However, P1D is also highly sensitive to observational and instrumental systematics, making accurate synthetic spectra essential for validating analyses and quantifying these effects, especially in high-volume surveys like the Dark Energy Spectroscopic Instrument (DESI). We present an efficient lognormal mock framework for generating one-dimensional Lyman-$\alpha$ forest spectra tailored for P1D analysis. Our method captures the redshift evolution of the mean transmitted flux and the scale-dependent shape and amplitude of the one-dimensional flux power spectrum by tuning Gaussian field correlations and transformation parameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \leq z \leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \leq k \leq 1.0$ s km$^{-1}$), our mocks recover the mean flux evolution with redshift to sub-percent accuracy, and the P1D at the percent level. Additionally, we discuss potential extensions of this framework, such as the incorporation of astrophysical contaminants, continuum uncertainties, and instrumental effects. Such improvements would expand its utility in ongoing and upcoming surveys and enable a broader range of validation efforts and systematics studies for P1D inference and precision cosmology. |
| 2025-09-04 | [Revealing the origin of supermassive black holes with Taiji-TianQin network](http://arxiv.org/abs/2509.04396v1) | Ping Shen, Wen-Biao Han et al. | The origin of supermassive black holes (SMBHs) is a pivotal problem in modern cosmology. This work explores the potential of the Taiji-TianQin space-borne gravitational-wave (GW) detector network to identify the formation channels of massive black hole binaries (MBHBs) at high redshifts ($z \gtrsim 10$). The network substantially improves detection capability, boosting the signal-to-noise ratio by a factor of 2.2-3.0 (1.06-1.14) relative to TianQin (Taiji) alone. It increases the detection rate of MBHBs formed from light seeds (LS) by more than 2.2 times and achieves over 96\% detection efficiency for those originating from heavy seeds (HS). Furthermore, the network enables component mass estimation with relative uncertainties as low as $\sim 10^{-4}$ at the $2\sigma$ level. These improvements facilitate the assembly of a well-constrained population sample, allowing robust measurement of the fractional contributions from different formation pathways. The network achieves high precision in distinguishing between LS and HS origins (7.4\% relative uncertainty at $2\sigma$) and offers moderate discrimination between delay and no-delay channels in HS-origin binaries (24\%). However, classification remains challenging for delay versus no-delay scenarios in LS-origin systems (58\%) due to significant population overlap. In conclusion, the Taiji-TianQin network will serve as a powerful tool for unveiling the origins of SMBHs through GW population studies. |
| 2025-09-04 | [Prominence: A discriminator of gravitational wave signals](http://arxiv.org/abs/2509.04384v1) | JoÃ£o GonÃ§alves, Danny Marfatia et al. | The concept of prominence is familiar to signal engineers, topographers and mountaineers. We introduce Prominence $\mathcal P$ as a discriminator of gravitational wave (GW) signals. We treat black hole and neutron star binaries as astrophysical background sources, and show how $\mathcal P$ can be used to distinguish between GW spectra produced by first-order phase transitions, domain walls and cosmic strings, and combinations thereof. Prominence can also be used to discriminate between these and off-piste sources of GWs. The uncertainty in the measured energy density in GWs at Pulsar Timing Arrays needs to be smaller than $\sim 4\%$ for $\mathcal{P}$ to achieve discrimination at 3$\sigma$. LISA and ET data are expected to have sufficiently small uncertainties that Prominence can play a central role in their analysis. |
| 2025-09-04 | [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](http://arxiv.org/abs/2509.04363v1) | Paul Scherer, Andreas Kirsch et al. | Real-world experimental scenarios are characterized by the presence of heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated in batched settings. The bias--variance tradeoff can be used to write the expected mean squared error between a model distribution and a ground-truth random variable as the sum of an epistemic uncertainty term, the bias squared, and an aleatoric uncertainty term. We leverage this relationship to propose novel active learning strategies that directly reduce the bias between experimental rounds, considering model systems both with and without noise. Finally, we investigate methods to leverage historical data in a quadratic manner through the use of a novel cobias--covariance relationship, which naturally proposes a mechanism for batching through an eigendecomposition strategy. When our difference-based method leveraging the cobias--covariance relationship is utilized in a batched setting (with a quadratic estimator), we outperform a number of canonical methods including BALD and Least Confidence. |
| 2025-09-04 | [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](http://arxiv.org/abs/2509.04357v1) | Jiajun He, Naoki Sawada et al. | Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech. |
| 2025-09-04 | [History matching for functional data and its application to tsunami warnings in the Indian Ocean](http://arxiv.org/abs/2509.04342v1) | Ryuichi Kanai, NicolÃ¡s HernÃ¡ndez et al. | Traditional History Matching (HM) identifies implausible regions of the input parameter space by comparing scalar outputs of a computer model to observations. It offers higher computational efficiency than Bayesian calibration, making it suitable for high-dimensional problems. However, in real physical systems, outputs are often functional, such as time series or spatial fields, and conventional HM cannot fully exploit such information. We propose a novel method, Functional History Matching (FHM), which extends HM to handle functional data. FHM incorporates the Outer Product Emulator, an extension of the Gaussian Process emulator designed for time series, to enhance computational efficiency. FHM also leverages Random Projection to extract dynamic features from infinite-dimensional data, including derivatives. FHM supports uncertainty quantification essential for decision-making and naturally accommodates model discrepancies. To demonstrate its practical effectiveness, we apply FHM to a synthetic tsunami forecasting scenario in the Indian Ocean, assuming a realistic event in the Makran subduction zone. Wave elevation time series from offshore buoy data are used to predict wave elevations over the Indian coastline. Our results show that FHM significantly outperforms scalar-based HM in accuracy. FHM enables reliable forecasting from functional data within feasible computational constraints, offering a robust framework for early warning systems and beyond. |
| 2025-09-04 | [We Have It Covered: A Resampling-based Method for Uplift Model Comparison](http://arxiv.org/abs/2509.04315v1) | Yang Liu, Chaoyu Yuan | Uplift models play a critical role in modern marketing applications to help understand the incremental benefits of interventions and identify optimal targeting strategies. A variety of techniques exist for building uplift models, and it is essential to understand the model differences in the context of intended applications. The uplift curve is a widely adopted tool for assessing uplift model performance on the selection universe when observations are available for the entire population. However, when it is uneconomical or infeasible to select the entire population, it becomes difficult or even impossible to estimate the uplift curve without appropriate sampling design. To the best of our knowledge, no prior work has addressed uncertainty quantification of uplift curve estimates, which is essential for model comparisons. We propose a two-step sampling procedure and a resampling-based approach to compare uplift models with uncertainty quantification, examine the proposed method via simulations and real data applications, and conclude with a discussion. |
| 2025-09-04 | [Learning Optimal Crew Dispatch for Grid Restoration Following an Earthquake](http://arxiv.org/abs/2509.04308v1) | Farshad Amani, Faezeh Ardali et al. | Post-disaster crew dispatch is a critical but computationally intensive task. Traditional mixed-integer linear programming methods often require minutes to several hours to compute solutions, leading to delays that hinder timely decision-making in highly dynamic restoration environments. To address this challenge, we propose a novel learning-based framework that integrates transformer architectures with deep reinforcement learning (DRL) to deliver near real-time decision support without compromising solution quality. Crew dispatch is formulated as a sequential decision-making problem under uncertainty, where transformers capture high-dimensional system states and temporal dependencies, while DRL enables adaptive and scalable decision-making. Earthquake-induced distribution network damage is first characterized using established seismic standards, followed by a scenario generation and reduction pipeline that aggregates probable outcomes into a single geospatial impact map. Conditioned on this map, the proposed framework generates second-level dispatch strategies, trained offline on simulated and historical events and deployed online for rapid response. In addition to substantial runtime improvements, the proposed method enhances system resilience by enabling faster and more effective recovery and restoration. Case studies, particularly on the 2869-bus European gas and power network, demonstrate that the method substantially accelerates restoration while maintaining high-quality solutions, underscoring its potential for practical deployment in large-scale disaster response. |
| 2025-09-04 | [Energy Confinement Time Scaling for the Negative Triangularity Scenario in DIII-D](http://arxiv.org/abs/2509.04279v1) | P. Lunia | Results from the 2023 negative triangularity campaign on DIII-D demonstrate encouraging energy confinement properties, similar to or exceeding the scaling of the IPB98(y,2) law. This paper describes the procedure with which a new scaling law was regressed specifically from the data from the DIII-D campaign. Given the relatively small size of the single-machine dataset, measures were taken to minimize sampling bias and give a realistic estimate of the large uncertainties from the regression. The resulting power law shows a robustly stronger dependence on plasma current and more severe power degradation as compared to the H-mode scaling law. |
| 2025-09-03 | [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](http://arxiv.org/abs/2509.03516v1) | Ouxiang Li, Yuan Wang et al. | Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/. |
| 2025-09-03 | [Bayesian Multivariate Sparse Functional PCA](http://arxiv.org/abs/2509.03512v1) | Joseph Sartini, Scott Zeger et al. | Functional Principal Components Analysis (FPCA) provides a parsimonious, semi-parametric model for multivariate, sparsely-observed functional data. Frequentist FPCA approaches estimate principal components (PCs) from the data, then condition on these estimates in subsequent analyses. As an alternative, we propose a fully Bayesian inferential framework for multivariate, sparse functional data (MSFAST) which explicitly models the PCs and incorporates their uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate, densely-observed functional data. Like FAST, MSFAST represents PCs using orthonormal splines, samples the orthonormal spline coefficients using parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST extends FAST to multivariate, sparsely-observed data by (1) standardizing each functional covariate to mitigate poor posterior conditioning due to disparate scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing likelihood calculations over covariates; (4) updating parameterizations and priors for computational stability; (5) using a Procrustes-based posterior alignment procedure; and (6) providing efficient prediction routines. We evaluated MSFAST alongside existing implementations using simulations. MSFAST produces uniquely valid inferences and accurate estimates, particularly for smaller signals. MSFAST is motivated by and applied to a study of child growth, with an accompanying vignette illustrating the implementation step-by-step. |
| 2025-09-03 | [Quantitative Stability and Contraction Principles for Mean-Field G-SDEs](http://arxiv.org/abs/2509.03439v1) | Yunfan Zhao, Xiaojing Chen et al. | We study mean-field stochastic differential equations (SDEs) driven by G-Brownian motion, extending recent work on existence and uniqueness by developing a full quantitative stability framework. Our main contribution is the construction of an intrinsic stability modulus that provides explicit bounds on the sensitivity of solutions with respect to perturbations in initial data (and, indirectly, coefficients). Using Bihari-Osgood type inequalities under G-expectation, we establish sharp continuity estimates for the data-to-solution map and analyze the asymptotic properties of the stability modulus. In particular, we identify contraction behavior on short horizons, leading to a contraction principle that guarantees uniqueness and global propagation of stability. The results apply under non-Lipschitz, non-deterministic coefficients with square-integrable initial data, thereby significantly broadening the scope of mean-field G-SDEs. Beyond existence and uniqueness, our framework quantifies robustness of solutions under volatility uncertainty, opening new directions for applications in stochastic control, risk management, and mean-field models under ambiguity. |
| 2025-09-03 | [Bayesian analysis of properties of nuclear matter with the FOPI experimental data](http://arxiv.org/abs/2509.03406v1) | Guojun Wei, Manzi Nan et al. | Based on the ultra-relativistic quantum molecular dynamics (UrQMD) transport model, combined with experimental data of directed flow, elliptic flow, and nuclear stopping power measured by FOPI in $\rm ^{197}Au+^{197}Au$ collisions at beam energies ($E_{lab}$) of 0.25 and 0.4 GeV/nucleon, the incompressibility of the nuclear equation of state $K_0$, the nucleon effective mass $m^*$, and the in-medium correction factor ($F$, with respect to free-space values) on the nucleon-nucleon elastic cross sections are studied by Bayesian analysis. It is found that both $m^*$ and $F$ can be tightly constrained with the uncertainty $\le$ 15\%, however, $K_0$ cannot be constrained tightly. We deduce $m^*/m_0 = 0.78^{+0.09}_{-0.10}$ and $F = 0.75^{+0.08}_{-0.07}$ with experimental data at $E_{lab}$ = 0.25 GeV/nucleon, and the obtained values increased to $m^*/m_0 = 0.88^{+0.03}_{-0.03}$ and $F = 0.88^{+0.06}_{-0.07}$ at $E_{lab}$ = 0.4 GeV/nucleon. The obtained results are further verified with rapidity-dependent flow data. |
| 2025-09-03 | [Elemental and Isotopic Yields from T Coronae Borealis: Predictions and Uncertainties](http://arxiv.org/abs/2509.03395v1) | Emma Wallace, Christian Iliadis et al. | T Coronae Borealis (T CrB) is a symbiotic recurrent nova system expected to undergo its next outburst within the next two years. Recent hydrodynamic simulations have predicted the nucleosynthetic yields for both carbon-oxygen (CO) and oxygen-neon (ONe) white-dwarf models, but without accounting for thermonuclear reaction-rate uncertainties. We perform detailed Monte Carlo post-processing nucleosynthesis calculations based on updated thermonuclear reaction rates and uncertainties from the 2025 evaluation. We quantify the resulting abundance uncertainties and identify the key nuclear reactions that dominate them. Our results show that both the CO and ONe nova models robustly produce characteristic CNO isotopes. More pronounced abundance differences emerge for elements with A $\ge$ 20. Sulfur is the most robust observational discriminator between the CO and ONe nova models, with a model-to-model difference of a factor of $\approx$30 and minimal sensitivity to reaction rate uncertainties. Neon, silicon, and phosphorus exhibit even larger abundance differences (factors of $\approx$150-250), providing strong diagnostic potential. While their predicted yields are subject to larger uncertainties, these remain smaller than the model-to-model differences, allowing these elements to serve as useful, though less precise, tracers of white-dwarf composition. Chlorine, argon, and potassium also differ between models, but the 1$\sigma$-abundance ranges for the CO and ONe models overlap, reducing their present usefulness as composition tracers. We find that only nine nuclear reactions dominate the abundance uncertainties of the most diagnostically important isotopes, and their influence is largely independent of the underlying white-dwarf composition. These results provide guidance for future experimental efforts and for interpreting ejecta compositions in the next eruption of T CrB. |
| 2025-09-03 | [On the renormalization-group analysis of the SM: loops, uncertainties, and vacuum stability](http://arxiv.org/abs/2509.03369v1) | A. V. Bednyakov, A. S. Fedoruk et al. | Renormalization-group equations (RGE) is one of the key tools in studying high-energy behavior of the Standard Model (SM). We begin by reviewing one-loop RGE for the dimensionless couplings of the SM and proceed to the state-of-the-art results. Our study focuses on the RGE solutions at different loop orders. We compare not only the standard (``diagonal'') loop counting, when one considers gauge, Yukawa, and scalar self-coupling beta functions at the same order, but also ``non-diagonal'' ones, inspired by the so-called Weyl consistency conditions. We discuss the initial conditions for RGE (``matching'') for different loop configurations, and study the uncertainties of running coupling both related to the limited precision of the experimental input (``parametric'') and to the missing high-order corrections (``theoretical''). As an application of our analysis we also estimate the electroweak vacuum decay probability and study how the uncertainties in the running parameters affect the latter. We argue that the ``non-diagonal'' beta functions, if coupled with more consistent ``non-diagonal'' matching lead to larger theoretical uncertainty than the ``diagonal'' ones. |
| 2025-09-03 | [Search for Past Stellar Encounters and the Origin of 3I/ATLAS](http://arxiv.org/abs/2509.03361v1) | Yiyang Guo, Luyao Zhang et al. | 3I/ATLAS, the third discovered interstellar object, has a heliocentric speed of 58 km/s and exhibits cometary activity. To constrain the origin of 3I/ATLAS and its past dynamical evolution, we propagate the orbits of 3I/ATLAS and nearby stars to search for stellar encounters. Integrating orbits in the Galactic potential and propagating the astrometric and radial-velocity uncertainties of 30 million Gaia stars, we identify 25 encounters with median encounter distances less than 1 pc. However, because the encounter speeds between 3I/ATLAS and each encounter exceed 20 km/s, none is a plausible host under common ejection mechanisms. We infer stellar masses for most stars and quantify the gravitational perturbations exerted by each individual star or each binary system on 3I/ATLAS. The strongest gravitational scattering perturber is a wide M-dwarf binary. Among all past encounters, the binary's barycenter and 3I/ATLAS reach the small encounter distance of 0.242 pc and the encounter speed of 28.39 km/s,1.64 Myr ago. We further demonstrate that the cumulative influence of the stellar encounters on both the speed and direction of 3I/ATLAS is weak. Based on the present kinematics of 3I/ATLAS to assess its origin, we find that a thin-disk origin is strongly favored, because the thin disk both exhibits a velocity distribution closely matching that of 3I/ATLAS and provides the dominant local number density of stars. |
| 2025-09-03 | [Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner](http://arxiv.org/abs/2509.03348v1) | Yewen Li, Jingtong Gao et al. | Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers' bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges by focusing on trajectory-level accumulated rewards, as well as their explainable capability, i.e., planning a future trajectory of states and executing bids accordingly. However, diffusers struggle with generation uncertainty, particularly regarding dynamic legitimacy between adjacent states, which can lead to poor bids and further cause significant loss of ad impression opportunities when competing with other advertisers in a highly competitive auction environment. To address it, we propose a Causal auto-Bidding method based on a Diffusion completer-aligner framework, termed CBD. Firstly, we augment the diffusion training process with an extra random variable t, where the model observes t-length historical sequences with the goal of completing the remaining sequence, thereby enhancing the generated sequences' dynamic legitimacy. Then, we employ a trajectory-level return model to refine the generated trajectories, aligning more closely with advertisers' objectives. Experimental results across diverse settings demonstrate that our approach not only achieves superior performance on large-scale auto-bidding benchmarks, such as a 29.9% improvement in conversion value in the challenging sparse-reward auction setting, but also delivers significant improvements on the Kuaishou online advertising platform, including a 2.0% increase in target cost. |
| 2025-09-03 | [Bayesian Additive Regression Trees for functional ANOVA model](http://arxiv.org/abs/2509.03317v1) | Seokhun Park, Insung Kong et al. | Bayesian Additive Regression Trees (BART) is a powerful statistical model that leverages the strengths of Bayesian inference and regression trees. It has received significant attention for capturing complex non-linear relationships and interactions among predictors. However, the accuracy of BART often comes at the cost of interpretability. To address this limitation, we propose ANOVA Bayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART based on the functional ANOVA decomposition, which is used to decompose the variability of a function into different interactions, each representing the contribution of a different set of covariates or factors. Our proposed ANOVA-BART enhances interpretability, preserves and extends the theoretical guarantees of BART, and achieves superior predictive performance. Specifically, we establish that the posterior concentration rate of ANOVA-BART is nearly minimax optimal, and further provides the same convergence rates for each interaction that are not available for BART. Moreover, comprehensive experiments confirm that ANOVA-BART surpasses BART in both accuracy and uncertainty quantification, while also demonstrating its effectiveness in component selection. These results suggest that ANOVA-BART offers a compelling alternative to BART by balancing predictive accuracy, interpretability, and theoretical consistency. |
| 2025-09-03 | [Credible Uncertainty Quantification under Noise and System Model Mismatch](http://arxiv.org/abs/2509.03311v1) | Penggao Yan, Li-Ta Hsu | State estimators often provide self-assessed uncertainty metrics, such as covariance matrices, whose reliability is critical for downstream tasks. However, these self-assessments can be misleading due to underlying modeling violations like noise or system model mismatch. This letter addresses the problem of estimator credibility by introducing a unified, multi-metric evaluation framework. We construct a compact credibility portfolio that synergistically combines traditional metrics like the Normalized Estimation Error Squared (NEES) and the Noncredibility Index (NCI) with proper scoring rules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our key contributions are a novel energy distance-based location test to robustly detect system model misspecification and a method that leverages the asymmetric sensitivities of NLL and ES to distinguish optimism covariance scaling from system bias. Monte Carlo simulations across six distinct credibility scenarios demonstrate that our proposed method achieves high classification accuracy (80-100%), drastically outperforming single-metric baselines which consistently fail to provide a complete and correct diagnosis. This framework provides a practical tool for turning patterns of credibility indicators into actionable diagnoses of model deficiencies. |
| 2025-08-29 | [Sequential Fair Allocation With Replenishments: A Little Envy Goes An Exponentially Long Way](http://arxiv.org/abs/2508.21753v1) | Chido Onyeze, Sean R. Sinclair et al. | We study the trade-off between envy and inefficiency in repeated resource allocation settings with stochastic replenishments, motivated by real-world systems such as food banks and medical supply chains. Specifically, we consider a model in which a decision-maker faced with stochastic demand and resource donations must trade off between an equitable and efficient allocation of resources over an infinite horizon. The decision-maker has access to storage with fixed capacity $M$, and incurs efficiency losses when storage is empty (stockouts) or full (overflows). We provide a nearly tight (up to constant factors) characterization of achievable envy-inefficiency pairs. Namely, we introduce a class of Bang-Bang control policies whose inefficiency exhibits a sharp phase transition, dropping from $\Theta(1/M)$ when $\Delta = 0$ to $e^{-\Omega(\Delta M)}$ when $\Delta > 0$, where $\Delta$ is used to denote the target envy of the policy. We complement this with matching lower bounds, demonstrating that the trade-off is driven by supply, as opposed to demand uncertainty. Our results demonstrate that envy-inefficiency trade-offs not only persist in settings with dynamic replenishment, but are shaped by the decision-maker's available capacity, and are therefore qualitatively different compared to previously studied settings with fixed supply. |
| 2025-08-29 | [Uncertainties within Weather Regime definitions for the Euro-Atlantic sector in ERA5 and CMIP6](http://arxiv.org/abs/2508.21701v1) | Lotte Hompes, Swinda K. J. Falkena et al. | Certain Weather Regimes (WR) are associated with a higher risk of energy shortages, i.e. Blocking regimes for European winters. However, there are many uncertainties tied to the implementation of WRs and associated risks in the energy sector. Especially the impact of climate change is unknown.   We investigate these uncertainties by looking at three methodologically diverse Euro-Atlantic WR definitions. We carry out a thorough validation of these methods and analyse their methodological and spatio-temporal sensitivity using ERA5 data. Furthermore, we look into the suitability of CMIP6 models for WR based impact assessments.   Our sensitivity assessment showed that the persistence and occurrence of regimes are sensitive to small changes in the methodology. We show that the training period used has a very significant impact on the persistence and occurrence of the regimes found. For both WR4 and WR7, this results in instability of the regime patterns.   All CMIP6 models investigated show instability of the regimes. Meaning that the normalised distance between the CMIP6 model regimes and our baseline regimes exceeds 0.4 or are visually extremely dissimilar. Only the WR4 regimes clustered on historical CMIP6 model data consistently have a normalised distance to our baseline regimes smaller than 0.4 and are visually identifiable. The WR6 definition exceeds the normalised distance threshold for all investigated CMIP6 experiments. Though all CMIP6 model experiments clustered with the WR7 definition have a normalised distance to the baseline regimes below 0.4, visual inspection of the regimes indicates instability.   Great caution should be taken when applying WR's in impact models for the energy sector, due to this large instability and uncertainties associated with WR definitions. |
| 2025-08-29 | [Chance-Constrained DC Optimal Power Flow Using Constraint-Informed Statistical Estimation](http://arxiv.org/abs/2508.21687v1) | Tianyang Yi, D. Adrian Maldonado et al. | Chance-constrained optimization has emerged as a promising framework for managing uncertainties in power systems. This work advances its application to the DC Optimal Power Flow (DC-OPF) model, developing a novel approach to uncertainty modeling and estimation. Current methods typically tackle these problems by first modeling random nodal injections using high-dimensional statistical distributions that scale with the number of buses, followed by deriving deterministic reformulations of the probabilistic constraints. We propose an alternative methodology that exploits the constraint structure to inform the uncertainties to be estimated, enabling significant dimensionality reduction. Rather than learning joint distributions of net-load forecast errors across units, we instead directly model the one-dimensional aggregate system forecast error and two-dimensional line errors weighted by power transfer distribution factors. We evaluate our approach under both Gaussian and non-Gaussian distributions on synthetic and real-world datasets, demonstrating significant improvements in statistical accuracy and optimization performance compared to existing methods. |
| 2025-08-29 | [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](http://arxiv.org/abs/2508.21677v1) | Bernhard Wullt, Johannes KÃ¶hler et al. | Industrial manipulators are normally operated in cluttered environments, making safe motion planning important. Furthermore, the presence of model-uncertainties make safe motion planning more difficult. Therefore, in practice the speed is limited in order to reduce the effect of disturbances. There is a need for control methods that can guarantee safe motions that can be executed fast. We address this need by suggesting a novel model predictive control (MPC) solution for manipulators, where our two main components are a robust tube MPC and a corridor planning algorithm to obtain collision-free motion. Our solution results in a convex MPC, which we can solve fast, making our method practically useful. We demonstrate the efficacy of our method in a simulated environment with a 6 DOF industrial robot operating in cluttered environments with uncertainties in model parameters. We outperform benchmark methods, both in terms of being able to work under higher levels of model uncertainties, while also yielding faster motion. |
| 2025-08-29 | [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](http://arxiv.org/abs/2508.21648v1) | Farhad Abtahi, Mehdi Astaraki et al. | Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems. |
| 2025-08-29 | [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](http://arxiv.org/abs/2508.21620v1) | Agustinus Kristiadi | Decision theories offer principled methods for making choices under various types of uncertainty. Algorithms that implement these theories have been successfully applied to a wide range of real-world problems, including materials and drug discovery. Indeed, they are desirable since they can adaptively gather information to make better decisions in the future, resulting in data-efficient workflows. In scientific discovery, where experiments are costly, these algorithms can thus significantly reduce the cost of experimentation. Theoretical analyses of these algorithms are crucial for understanding their behavior and providing valuable insights for developing next-generation algorithms. However, theoretical analyses in the literature are often inaccessible to non-experts. This monograph aims to provide an accessible, self-contained introduction to the theoretical analysis of commonly used probabilistic decision-making algorithms, including bandit algorithms, Bayesian optimization, and tree search algorithms. Only basic knowledge of probability theory and statistics, along with some elementary knowledge about Gaussian processes, is assumed. |
| 2025-08-29 | [Universal Precision Limits in General Open Quantum Systems](http://arxiv.org/abs/2508.21567v1) | Tan Van Vu, Ryotaro Honma et al. | The intuition that the precision of observables is constrained by thermodynamic costs has recently been formalized through thermodynamic and kinetic uncertainty relations. While such trade-offs have been extensively studied in Markovian systems, corresponding constraints in the non-Markovian regime remain largely unexplored. In this Letter, we derive universal bounds on the precision of generic observables in open quantum systems coupled to environments of arbitrary strength and subjected to two-point measurements. By introducing an asymmetry term that quantifies the disparity between forward and backward processes, we show that the relative fluctuation of any time-antisymmetric current is constrained by both entropy production and this forward-backward asymmetry. For general observables, we prove that their relative fluctuation is always bounded from below by a generalized activity term. These results establish a comprehensive framework for understanding precision limits in broad classes of general open quantum systems. |
| 2025-08-29 | [Nuclear suppression in diffractive vector meson production within the color glass condensate framework](http://arxiv.org/abs/2508.21562v1) | Heikki MÃ¤ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions within a Color Glass Condensate based framework. Using data from HERA and the LHC, we find that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ observables is challenging. Introducing a global $K$-factor to account for theoretical uncertainties improves the agreement with data and enhances the framework's predictive power. We present predictions for integrated $\mathrm{J}/\psi$ cross sections at different photon-nucleus energies and study their $A$-dependence relative to a no-saturation baseline, quantifying nuclear suppression and providing insights into the onset of saturation effects. |
| 2025-08-29 | [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](http://arxiv.org/abs/2508.21550v1) | Yujin Park, Haejun Chung et al. | Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking. |
| 2025-08-29 | [Adaptive extended Kalman filter and laser link acquisition in the detection of gravitational waves in space](http://arxiv.org/abs/2508.21538v1) | Jinke Yang, Yong Xie et al. | An alternative, new laser link acquisition scheme for the triangular constellation of spacecraft (SCs) in deep space in the detection of gravitational waves is considered. In place of a wide field CCD camera in the initial stage of laser link acquisition adopted in the conventional scheme, an extended Kalman filter based on precision orbit determination is incorporated in the point ahead angle mechanism (PAAM) to steer the laser beam in such a way to narrow the uncertainty cone and at the same time avoids the heating problem generated by the CCD camera.A quadrant photodetector (QPD) based on the Differential Power Sensing (DPS) technique, which offers a higher dynamic range than differential wavefront sensing (DWS), is employed as the readout of the laser beam spot. The conventional two stages (coarse acquisition and fine acquisition) are integrated into a single control loop. The payload structure of the ATP control loop is simplified and numerical simulations, based on a colored measurement noise model that closely mimics the prospective on-orbit conditions, demonstrate that the AEKF significantly reduces the initial uncertainty region by predicting the point ahead angle (PAA) even when the worst case scenario in SC position (navigation) error is considered. |
| 2025-08-28 | [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](http://arxiv.org/abs/2508.21007v1) | Mateusz Jaszczuk, Nadia Figueroa | With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming-based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system. |
| 2025-08-28 | [Practical indistinguishability in a gene regulatory network inference problem, a case study](http://arxiv.org/abs/2508.21006v1) | Cody E. FitzGerald, Shelley Reich et al. | Computationally inferring mechanistic insights from typical biological data is a challenging pursuit. Even the highest-quality experimental data come with challenges. There are always sources of noise, a limit to how often we can measure the system, and we can rarely measure all the relevant states that participate in the underlying complexity. There are usually sources of uncertainty in model development, which give rise to multiple competing model structures. To underscore the need for further analysis of structural uncertainty in modeling, we use a meta-analysis across six journals covering mathematical biology and show that a huge number of models for biological systems are developed each year, but model selection and comparison across model structures appear to be less common. We walk through a case study involving inference of regulatory network structure involved in a developmental decision in the nematode, \textit{Pristonchus pacificus}. We use real biological data and compare across 13,824 models--each corresponding to a different regulatory network structure, to determine which regulatory features are supported by the data across three experimental conditions. We find that the best-fitting models for each experimental condition share a combination of features and identify a regulatory network that is common across the model sets for each condition. This model can describe the data across the experimental conditions we considered and exhibits a high degree of positive regulation and interconnectivity between the key regulators, \textit{eud-1}, $textit{sult-1}, and \textit{nhr-40}. While the biological results are specific to the molecular biology of development in \textit{Pristonchus pacificus}, the general modeling framework and underlying challenges we faced doing this analysis are widespread across biology, chemistry, physics, and many other scientific disciplines. |
| 2025-08-28 | [Bottomonium transport in a strongly coupled quark-gluon plasma](http://arxiv.org/abs/2508.20995v1) | Biaogang Wu, Ralf Rapp | Quarkonium production in high-energy heavy-ion collisions remains a key probe of the quark-gluon plasma formed in these reactions, but the development of a fully integrated nonperturbative approach remains a challenge. Toward this end, we set up a semiclassical transport approach that combines nonperturbative reaction rates rooted in lattice-constrained $T$-matrix interactions with a viscous hydrodynamic medium evolution. Bottomonium suppression is computed along trajectories in the hydrodynamic evolution while regeneration is evaluated via a rate equation extended to a medium with spatial gradients. The much larger reaction rates compared to previous calculations markedly enhance both dissociation and regeneration processes. This, in particular, requires a reliable assessment of bottomonium equilibrium limits and of the non-thermal distributions of the bottom quarks transported through the expanding medium. Within current uncertainties our approach can describe the centrality dependence of bottomonium yields measured in Pb-Pb ($\sqrt{s_{_{\rm NN}}}$=5.02\,TeV) collisions at the LHC, while discrepancies are found at large transverse momenta. |
| 2025-08-28 | [Polynomial Chaos Expansion for Operator Learning](http://arxiv.org/abs/2508.20886v1) | Himanshu Sharma, LukÃ¡Å¡ NovÃ¡k et al. | Operator learning (OL) has emerged as a powerful tool in scientific machine learning (SciML) for approximating mappings between infinite-dimensional functional spaces. One of its main applications is learning the solution operator of partial differential equations (PDEs). While much of the progress in this area has been driven by deep neural network-based approaches such as Deep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recent work has begun to explore traditional machine learning methods for OL. In this work, we introduce polynomial chaos expansion (PCE) as an OL method. PCE has been widely used for uncertainty quantification (UQ) and has recently gained attention in the context of SciML. For OL, we establish a mathematical framework that enables PCE to approximate operators in both purely data-driven and physics-informed settings. The proposed framework reduces the task of learning the operator to solving a system of equations for the PCE coefficients. Moreover, the framework provides UQ by simply post-processing the PCE coefficients, without any additional computational cost. We apply the proposed method to a diverse set of PDE problems to demonstrate its capabilities. Numerical results demonstrate the strong performance of the proposed method in both OL and UQ tasks, achieving excellent numerical accuracy and computational efficiency. |
| 2025-08-28 | [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](http://arxiv.org/abs/2508.20812v1) | Lorenzo Busellato, Federico Cunico et al. | To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art. |
| 2025-08-28 | [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](http://arxiv.org/abs/2508.20806v1) | Moriba Jah, Van Haslett | Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified. |
| 2025-08-28 | [Time Series Embedding and Combination of Forecasts: A Reinforcement Learning Approach](http://arxiv.org/abs/2508.20795v1) | Marcelo C. Medeiros, Jeronymo M. Pinro | The forecasting combination puzzle is a well-known phenomenon in forecasting literature, stressing the challenge of outperforming the simple average when aggregating forecasts from diverse methods. This study proposes a Reinforcement Learning - based framework as a dynamic model selection approach to address this puzzle. Our framework is evaluated through extensive forecasting exercises using simulated and real data. Specifically, we analyze the M4 Competition dataset and the Survey of Professional Forecasters (SPF). This research introduces an adaptable methodology for selecting and combining forecasts under uncertainty, offering a promising advancement in resolving the forecasting combination puzzle. |
| 2025-08-28 | [Surfel-based 3D Registration with Equivariant SE(3) Features](http://arxiv.org/abs/2508.20789v1) | Xueyang Kang, Hang Zhao et al. | Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods. |
| 2025-08-28 | [A predictive solution of the EPR paradox](http://arxiv.org/abs/2508.20788v1) | Henryk Gzyl | In this work an incorrect argument in EPR's paper is corrected. A predictive approach to further confirm the validity of quantum theory is also proposed. The essence of the detail that EPR missed is that in a state of given total momentum (in their example the total momentum is zero), since the total momentum operator $\hat{\bp}=\hat{\bp}_1+\hat{\bp}_2$ does not commute with any of the position operators $\hat{\bx}_1$ and $\hat{\bx}_2,$ then in an eigenstate of the total momentum operator, the standard deviation on the measurement of the position of any of the two particles has to be infinite. Below we compute the (quantum) conditional expectation of the momentum of any of the particles (say, the first) given the value of a measurement of the total momentum. Since the predictor of the momentum of the particle depends on the total momentum, and the total momentum does not commute with the position operator of any of the particles, the uncertainty principle is not violated, and no contradiction appears. We also prove, using the basic tenets of quantum measurement theory, that after measurement of the total momentum, the probability density (computed as the absolute square of the wave function), coincides with the (quantum) conditional density given the total momentum. |
| 2025-08-28 | [Update of the CODE catalogue and some aspects of the dynamical status of Oort Cloud comets](http://arxiv.org/abs/2508.20780v1) | Piotr A. DybczyÅ„ski, MaÅ‚gorzata KrÃ³likowska | Context. The outer Solar System is believed to host a vast reservoir of long-period comets (LPCs), but our understanding of their spatial distribution and dynamical history remains limited due to observational biases and uncertainties in orbital solutions for really observed comets. Aims. We aim to provide a comprehensive and dynamically homogeneous orbital database of LPCs to support the study of their origin, evolution, dynamical status, and 6D distribution of orbital elements. Methods. We updated the Catalogue of Cometary Orbits and their Dynamical Evolution (CODE catalogue) by computing original and future barycentric orbits, orbital parameters at previous and next perihelion, using full Monte Carlo swarms of real comets for the uncertainties estimation and taking into account the planetary, Galactic and passing stars perturbations according to the latest data and algorithms. Results. This update of the CODE catalogue focuses on the dynamical status of near-parabolic comets. Using current stellar data, we formulate new constraints for dynamically new comets. Now, the CODE database includes 983 orbital solutions for 369 comets with full uncertainty estimates and dynamical classifications, covering nearly all comets with original semi-major axes exceeding 10,000 au and discovered before 2022, as well as all LPCs discovered beyond 10 au from the Sun during during this period, and over 80% of the known LPCs with perihelion distances beyond 7 au. |
| 2025-08-27 | [A Partially Derivative-Free Proximal Method for Composite Multiobjective Optimization in the HÃ¶lder Setting](http://arxiv.org/abs/2508.20071v1) | V. S. Amaral, P. B. AssunÃ§Ã£o et al. | This paper presents an algorithm for solving multiobjective optimization problems involving composite functions, where we minimize a quadratic model that approximates $F(x) - F(x^k)$ and that can be derivative-free. We establish theoretical assumptions about the component functions of the composition and provide comprehensive convergence and complexity analysis. Specifically, we prove that the proposed method converges to a weakly $\varepsilon$-approximate Pareto point in at most $\mathcal{O}\left(\varepsilon^{-\frac{\beta+1}{\beta}}\right)$ iterations, where $\beta$ denotes the H\"{o}lder exponent of the gradient. The algorithm incorporates gradient approximations and a scaling matrix $B_k$ to achieve an optimal balance between computational accuracy and efficiency. Numerical experiments on robust biobjective instances with Lipschitz and H\"{o}lder-gradient components illustrate the method's behavior. In these tests, the proposed approach was able to approximate the Pareto front under different levels of uncertainty and consistently recovered distinct solutions, even in challenging cases where the objectives have only H\"{o}lder continuous gradients. |
| 2025-08-27 | [Neural Conditional Simulation for Complex Spatial Processes](http://arxiv.org/abs/2508.20067v1) | Julia Walchessen, Andrew Zammit-Mangion et al. | A key objective in spatial statistics is to simulate from the distribution of a spatial process at a selection of unobserved locations conditional on observations (i.e., a predictive distribution) to enable spatial prediction and uncertainty quantification. However, exact conditional simulation from this predictive distribution is intractable or inefficient for many spatial process models. In this paper, we propose neural conditional simulation (NCS), a general method for spatial conditional simulation that is based on neural diffusion models. Specifically, using spatial masks, we implement a conditional score-based diffusion model that evolves Gaussian noise into samples from a predictive distribution when given a partially observed spatial field and spatial process parameters as inputs. The diffusion model relies on a neural network that only requires unconditional samples from the spatial process for training. Once trained, the diffusion model is amortized with respect to the observations in the partially observed field, the number and locations of those observations, and the spatial process parameters, and can therefore be used to conditionally simulate from a broad class of predictive distributions without retraining the neural network. We assess the NCS-generated simulations against simulations from the true conditional distribution of a Gaussian process model, and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick process model for spatial extremes. In the latter case, we show that it is more efficient and accurate to conditionally simulate using NCS than classical MCMC techniques implemented in standard software. We conclude that NCS enables efficient and accurate conditional simulation from spatial predictive distributions that are challenging to sample from using traditional methods. |
| 2025-08-27 | [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](http://arxiv.org/abs/2508.20066v1) | Zheng Li, Yanming Guo et al. | Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios. |
| 2025-08-27 | [Robust Paths: Geometry and Computation](http://arxiv.org/abs/2508.20039v1) | Hao Hao, Peter Zhang | Applying robust optimization often requires selecting an appropriate uncertainty set both in shape and size, a choice that directly affects the trade-off between average-case and worst-case performances. In practice, this calibration is usually done via trial-and-error: solving the robust optimization problem many times with different uncertainty set shapes and sizes, and examining their performance trade-off. This process is computationally expensive and ad hoc. In this work, we take a principled approach to study this issue for robust optimization problems with linear objective functions, convex feasible regions, and convex uncertainty sets. We introduce and study what we define as the robust path: a set of robust solutions obtained by varying the uncertainty set's parameters. Our central geometric insight is that a robust path can be characterized as a Bregman projection of a curve (whose geometry is defined by the uncertainty set) onto the feasible region. This leads to a surprising discovery that the robust path can be approximated via the trajectories of standard optimization algorithms, such as the proximal point method, of the deterministic counterpart problem. We give a sharp approximation error bound and show it depends on the geometry of the feasible region and the uncertainty set. We also illustrate two special cases where the approximation error is zero: the feasible region is polyhedrally monotone (e.g., a simplex feasible region under an ellipsoidal uncertainty set), or the feasible region and the uncertainty set follow a dual relationship. We demonstrate the practical impact of this approach in two settings: portfolio optimization and adversarial deep learning. |
| 2025-08-27 | [Pruning Strategies for Backdoor Defense in LLMs](http://arxiv.org/abs/2508.20032v1) | Santosh Chapagain, Shah Muhammad Hamdi et al. | Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks. |
| 2025-08-27 | [Tip of the Red Giant Branch Distances to NGC 1316, NGC 1380, NGC 1404, & NGC 4457: A Pilot Study of a Parallel Distance Ladder Using Type Ia Supernovae in Early-Type Host Galaxies](http://arxiv.org/abs/2508.20023v1) | Max J. B. Newman, Conor Larison et al. | Though type-Ia supernovae (SNe Ia) are found in all types of galaxies, recent local Hubble constant measurements have disfavored using SNe Ia in early-type or quiescent galaxies, aiming instead for better consistency with SNe Ia in star-forming, late-type host galaxies calibrated by Cepheid distances. Here we investigate the feasibility of a parallel distance ladder using SNe Ia exclusively in quiescent, massive ($\log M_*/M_{\odot} \geq 10$) host galaxies, calibrated by tip of the red giant branch (TRGB) distances. We present TRGB measurements to four galaxies: three measured from the Hubble Space Telescope with the ACS F814W filter, and one measured from the JWST NIRCam F090W filter. Combined with literature measurements, we define a TRGB calibrator sample of five high-mass, early-type galaxies that hosted well-measured SNe Ia: NGC 1316 (SN 2006dd), NGC 1380 (SN 1992A), NGC 1404 (SN 2007on, SN 2011iv), NGC 4457 (SN 2020nvb), and NGC 4636 (SN 2020ue). We jointly standardize these calibrators with a fiducial sample of 124 Hubble-flow SNe Ia from the Zwicky Transient Facility that are matched in host-galaxy and light-curve properties. Our results with this homogenized subsample show a Hubble residual scatter of under 0.11 mag, lower than usually observed in cosmological samples of the full SN~Ia distribution. We obtain a measurement of the Hubble constant, $H_0 = 75.3 \pm 2.9$ km s$^{-1}$ Mpc$^{-1}$, including statistical and estimated systematic uncertainties, and discuss the potential to further improve the precision of this approach. As calibrator and supernova samples grow, we advocate that future cosmological applications of SNe Ia use subsamples matched in host-galaxy and supernova properties across redshift. |
| 2025-08-27 | [Communicating astrobiology and the search for life elsewhere: speculations and promises of a developing scientific field in newspapers, press releases and papers](http://arxiv.org/abs/2508.19984v1) | Danilo Albergaria, Pedro Russo et al. | This study examines the communication of astrobiology and the Search for Life Elsewhere (SLE) in academic papers, press releases, and news articles over three decades. Through a quantitative content analysis, it investigates the prevalence of speculations and promises/expectations in these sources, aiming to understand how research results are portrayed and their potential impact on public perception and future research directions. Findings reveal that speculations and promises/expectations are more frequent in news articles and press releases compared to academic papers. Speculations about conditions for life and the existence of life beyond Earth are common, particularly in news articles covering exoplanet research, while promises of life detection are rare. Press releases tend to emphasize the significance of research findings and the progress of the field. Speculations and promises/expectations in news articles often occur without attribution to scientists and in quotes of authors of the studies, and slightly less so in quotes of outside experts. The study highlights the complex dynamics of science communication in astrobiology, where speculations and promises can generate public excitement and influence research funding, but also risk misrepresenting scientific uncertainty and creating unrealistic expectations. It underscores the need for responsible communication practices that acknowledge the speculative dimension of the field while fostering public engagement and informed decision-making. |
| 2025-08-27 | [Comment on GarcÃ­a-Donato et al. (2025) "Model uncertainty and missing data: An objective Bayesian perspective"](http://arxiv.org/abs/2508.19939v1) | Joris Mulder | Garcia-Donato et al. (2025) present a methodology for handling missing data in a model selection problem using an objective Bayesian approach. The current comment discusses an alternative, existing objective Bayesian method for this problem. First, rather than using the g prior, O'Hagan's fractional Bayes factor (O'Hagan, 1995) is utilized based on a minimal fraction. Second, and more importantly due to the focus on missing data, Rubin's rules for multiple imputation can directly be used as the fractional Bayes factor can be written as a Savage-Dickey density ratio for a variable selection problem. The current comment derives the methodology for a variable selection problem. Moreover, its implied behavior is illustrated in a numerical experiment, showing competitive results as the method of Garcia-Donato et al. (2025). |
| 2025-08-27 | [Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition](http://arxiv.org/abs/2508.19933v1) | Sten Elling Tingstad Jacobsen, BalÃ¡zs KulcsÃ¡r et al. | The electrification and automation of mobility are reshaping how cities operate on-demand transport systems. Managing Electric Autonomous Mobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch, rebalancing, and charging decisions under multiple uncertainties, including travel demand, travel time, energy consumption, and charger availability. We address this challenge with a combined stochastic and robust model predictive control (MPC) framework. The framework integrates spatio-temporal Bayesian neural network forecasts with a multi-stage stochastic optimization model, formulated as a large-scale mixed-integer linear program. To ensure real-time applicability, we develop a tailored Nested Benders Decomposition that exploits the scenario tree structure and enables efficient parallelized solution. Stochastic optimization is employed to anticipate demand and infrastructure variability, while robust constraints on energy consumption and travel times safeguard feasibility under worst-case realizations. We evaluate the framework using high-fidelity simulations of San Francisco and Chicago. Compared with deterministic, reactive, and robust baselines, the combined stochastic and robust approach reduces median passenger waiting times by up to 36% and 95th-percentile delays by nearly 20%, while also lowering rebalancing distance by 27% and electricity costs by more than 35%. We also conduct a sensitivity analysis of battery size and vehicle efficiency, finding that energy-efficient vehicles maintain stable performance even with small batteries, whereas less efficient vehicles require larger batteries and greater infrastructure support. Our results emphasize the importance of jointly optimizing predictive control, vehicle capabilities, and infrastructure planning to enable scalable, cost-efficient EAMoD operations. |
| 2025-08-27 | [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](http://arxiv.org/abs/2508.19909v1) | Lechun You, Zhonghua Wu et al. | Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation. |
| 2025-08-26 | [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](http://arxiv.org/abs/2508.19191v1) | Yue Wang, Wenjie Deng et al. | Intraocular foreign body removal demands millimeter-level precision in confined intraocular spaces, yet existing robotic systems predominantly rely on manual teleoperation with steep learning curves. To address the challenges of autonomous manipulation (particularly kinematic uncertainties from variable motion scaling and variation of the Remote Center of Motion (RCM) point), we propose AutoRing, an imitation learning framework for autonomous intraocular foreign body ring manipulation. Our approach integrates dynamic RCM calibration to resolve coordinate-system inconsistencies caused by intraocular instrument variation and introduces the RCM-ACT architecture, which combines action-chunking transformers with real-time kinematic realignment. Trained solely on stereo visual data and instrument kinematics from expert demonstrations in a biomimetic eye model, AutoRing successfully completes ring grasping and positioning tasks without explicit depth sensing. Experimental validation demonstrates end-to-end autonomy under uncalibrated microscopy conditions. The results provide a viable framework for developing intelligent eye-surgical systems capable of complex intraocular procedures. |
| 2025-08-26 | [Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions](http://arxiv.org/abs/2508.19159v1) | Ersin Das, Rahal Nanayakkara et al. | Measurements and state estimates are often imperfect in control practice, posing challenges for safety-critical applications, where safety guarantees rely on accurate state information. In the presence of estimation errors, several prior robust control barrier function (R-CBF) formulations have imposed strict conditions on the input. These methods can be overly conservative and can introduce issues such as infeasibility, high control effort, etc. This work proposes a systematic method to improve R-CBFs, and demonstrates its advantages on a tracked vehicle that navigates among multiple obstacles. A primary contribution is a new optimization-based online parameter adaptation scheme that reduces the conservativeness of existing R-CBFs. In order to reduce the complexity of the parameter optimization, we merge several safety constraints into one unified numerical CBF via Poisson's equation. We further address the dual relative degree issue that typically causes difficulty in vehicle tracking. Experimental trials demonstrate the overall performance improvement of our approach over existing formulations. |
| 2025-08-26 | [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](http://arxiv.org/abs/2508.19150v1) | Juan Carlos SaborÃ­o, Marc Vinci et al. | Purposeful behavior in robotic assistants requires the integration of multiple components and technological advances. Often, the problem is reduced to recognizing explicit prompts, which limits autonomy, or is oversimplified through assumptions such as near-perfect information. We argue that a critical gap remains unaddressed -- specifically, the challenge of reasoning about the uncertain outcomes and perception errors inherent to human intention recognition. In response, we present a framework designed to be resilient to uncertainty and sensor noise, integrating real-time sensor data with a combination of planners. Centered around an intention-recognition POMDP, our approach addresses cooperative planning and acting under uncertainty. Our integrated framework has been successfully tested on a physical robot with promising results. |
| 2025-08-26 | [VPPE: Application of Scaled Vecchia Approximations to Parallel Partial Emulation](http://arxiv.org/abs/2508.19144v1) | Josh Seidman, Elaine T. Spiller | Computer models or simulators are widely used across scientific fields, but are computationally expensive limiting their use to explore possible scenarios/outcomes. Gaussian process emulators are statistical surrogates that can rapidly approximate the outputs of computer models at untested inputs and enable uncertainty quantification studies. The parallel partial emulation (PPE) was developed to model simulators with vector-valued outputs. While the PPE is adept at fitting simulator data with multidimensional outputs, the time to fit the PPE increases quickly as the number of training runs increases. The Scaled Vecchia approximation, a fast approximation to multivariate Gaussian likelihoods, makes fitting Gaussian process emulators with large training datasets tractable. Here we introduce the Vecchia Parallel Partial Emulation (VPPE) that utilizes the Scaled Vecchia approximation within the PPE framework to allow for parallel partial emulation with larger training datasets. The VPPE is applied to three computer experiments, a synthetic data set, a hydrology model, and a volcanic flow model, yielding comparable predictive accuracy to the PPE at a fraction of the runtime. |
| 2025-08-26 | [Using Machine Learning to Generate, Clarify, and Improve Economic Models](http://arxiv.org/abs/2508.19136v1) | Annie Liang | Machine learning algorithms can now outperform classic economic models in predicting quantities ranging from bargaining outcomes, to choice under uncertainty, to an individual's future jobs and wages. Yet this predictive accuracy comes at a cost: most machine learning algorithms function as black boxes, offering little insight into \emph{why} outcomes occur. This article asks whether machine learning can guide the development of new economic theories.   Economic models serve an important purpose beyond prediction -- they uncover the general mechanisms behind observed behaviors. A model that identifies the causal pathways of economic development is more valuable than one that merely predicts which countries will escape poverty, because it enables policymakers to encourage that development in countries where it might not have happened otherwise. Similarly, a model that predicts imperfectly across many domains can be more valuable than one that is highly accurate in a specific domain, since the former allows insights and data obtained from one setting to inform decisions and policy in another.   Applying machine learning algorithms off-the-shelf is unlikely to yield such models. But recent work shows that, when reconceived with the aims of an economic modeler in mind, machine learning methods can improve both prediction and understanding. These approaches range from adversarially training algorithms to expose the limits of existing models, to imposing economic theory as a constraint on algorithmic search. Advances in large language models complement these strategies and open new research directions. |
| 2025-08-26 | [Trustworthy Agents for Electronic Health Records through Confidence Estimation](http://arxiv.org/abs/2508.19096v1) | Yongwoo Song, Minbyul Jeong et al. | Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low. |
| 2025-08-26 | [Measurement of the branching fraction of $\psip \to Ï‰Î·Î·$](http://arxiv.org/abs/2508.19092v1) | BESIII Collaboration, M. Ablikim et al. | Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to \omega \eta \eta $ is observed for the first time. The branching fraction of the $\psi(3686)\to\omega\eta\eta$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $\omega(1420)$ and $f_{0}(1710)$ resonances are observed in the $\omega\eta$ and $\eta\eta$ invariant-mass spectra, respectively. |
| 2025-08-26 | [SN2023syz and SN2025cbj: Two Type IIn Supernovae Associated with IceCube High-energy Neutrinos](http://arxiv.org/abs/2508.19080v1) | Ming-Xuan Lu, Yun-Feng Liang et al. | Type IIn supernovae (SNe IIn) are a subclass of core-collapse SNe in which strong interactions occur between the ejecta and dense circumstellar material, creating ideal conditions for the production of high-energy neutrinos. This makes them promising candidate sources of neutrinos. In this work, we conduct an association study between 163 SNe IIn observed by the Zwicky Transient Facility and 138 neutrino alert events detected by the IceCube neutrino observatory. After excluding alerts with poor localization, we find two SNe that are spatiotemporally coincident with neutrino events. IC231027A and IC250421A coincide with the positions of SN2023syz and SN2025cbj, respectively, within their localization uncertainties, and the neutrino arrival times are delayed by 38 days and 61 days relative to the discovery times of the corresponding SNe. Using Monte Carlo simulations, we estimate that the probability of such two coincidences occurring by chance in our sample is $p \sim 0.67\%$, suggesting a high likelihood that they arise from genuine associations, though the result is not yet statistical significant. Furthermore, model calculations show that the expected numbers of neutrino events from these SNe IIn could be consistent with the actual observations. Our study provides possible evidence that interacting SNe may be potential neutrino-emitting sources. |
| 2025-08-26 | [A Quick Estimation of FrÃ©chet Quantizers for a Dynamic Solution to Flood Risk Management Problems](http://arxiv.org/abs/2508.19045v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization is a well-known quantitative tool for decision-making under uncertainty. It is broadly used in financial and investment planning, inventory control, and also natural disaster risk management. Theoretical solutions of multi-stage stochastic programs can be found explicitly only in very exceptional cases due to their variational form and interdependency of uncertainty in time. Nevertheless, numerical solutions are often inaccurate, as they rely on Monte-Carlo sampling, which requires the Law of Large Numbers to hold for the approximation quality. In this article, we introduce a new approximation scheme, which computes and groups together stage-wise optimal quantizers of conditional Fr\'echet distributions for optimal weighting of value functions in the dynamic programming. We consider optimality of scenario quantization methods in the sense of minimal Kantorovich-Wasserstein distance at each stage of the scenario tree. By this, we bound the approximation error with convergence guarantees. We also provide global solution guarantees under convexity and monotonicity conditions on the value function. We apply the developed methods to the governmental budget allocation problem for risk management of flood events in Austria. For this, we propose an extremely efficient way to approximate optimal quantizers for conditional Fr\'echet distributions. Our approach allows to enhance the overall efficiency of dynamic programming via the use of different parameter estimation methods for different groups of quantizers. The groups are distinguished by a particular risk threshold and are able to differentiate between higher- and lower-impact flood events. |
| 2025-08-26 | [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](http://arxiv.org/abs/2508.18937v1) | Wang Jiayin, Wei Yanran et al. | Autonomous control of the laparoscope in robot-assisted Minimally Invasive Surgery (MIS) has received considerable research interest due to its potential to improve surgical safety. Despite progress in pixel-level Image-Based Visual Servoing (IBVS) control, the requirement of continuous visibility and the existence of complex disturbances, such as parameterization error, measurement noise, and uncertainties of payloads, could degrade the surgeon's visual experience and compromise procedural safety. To address these limitations, this paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and uncertainty-adaptive framework for autonomous laparoscope control that guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian Process Regression (GPR) is utilized to perform hybrid (deterministic + stochastic) quantification of operational uncertainties including residual model uncertainties, stochastic uncertainties, and external disturbances. Based on uncertainty quantification, a novel safety aware trajectory optimization framework with probabilistic guarantees is proposed, where a uncertainty-adaptive safety Control Barrier Function (CBF) condition is given based on uncertainty propagation, and chance constraints are simultaneously formulated based on probabilistic approximation. This uncertainty aware formulation enables adaptive control effort allocation, minimizing unnecessary camera motion while maintaining robustness. The proposed method is validated through comparative simulations and experiments on a commercial surgical robot platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph node dissection. Compared with baseline methods, the framework maintains near-perfect target visibility (>99.9%), reduces tracking e |
| 2025-08-25 | [Flight-Ready Precise and Robust Carrier-Phase GNSS Navigation Software for Distributed Space Systems](http://arxiv.org/abs/2508.18246v1) | Samuel Y. W. Low, Toby Bell et al. | This paper presents the full requirements analysis, design, development, and testing of high-precision navigation flight software for Distributed Space Systems (DSS) using Carrier Phase Differential GNSS (CDGNSS). Five main contributions are made. First, a survey of flown and upcoming DSS missions with stringent precision requirements is conducted, from which a thorough requirements analysis is distilled to guide development and testing. Second, a real-time navigation functional architecture is designed, and adopts a sparse and regularized Consider Kalman Filter with options for numerical stability in-flight. The filter rigorously accounts for uncertainties in process noise, measurement noise, and biases. It tracks float ambiguities with integer resolution where possible. The covariance correlation structure is preserved under all navigation modes, including contingencies and outages. Third, a lightweight, memoryless Fault Detection, Isolation, and Recovery (FDIR) module is developed to guard against anomalous measurements, providing statistical screening and ensuring robust navigation. Fourth, the software architecture is proposed for ease of integration, with strategies presented for modularity and computational efficiency tailored to constrained flight systems. Fifth, a comprehensive test campaign is conducted, mapped to a requirements verification matrix, spanning unit, interface, software-in-the-loop, and real-time hardware-in-the-loop tests, emphasizing gradual test fidelity for efficient fault isolation. Finally, flight-like results are demonstrated using the VISORS mission, due to the generalizability of the VISORS navigation operations, and the stringency which demands sub-centimeter relative position and sub-millimeter-per-second velocity accuracy. This architecture aims to serve as a reference for next-generation DSS missions adopting CDGNSS. |
| 2025-08-25 | [Uncertain data assimilation for urban wind flow simulations with OpenLB-UQ](http://arxiv.org/abs/2508.18202v1) | Mingliang Zhong, Dennis Teutscher et al. | Accurate prediction of urban wind flow is essential for urban planning, pedestrian safety, and environmental management. Yet, it remains challenging due to uncertain boundary conditions and the high cost of conventional CFD simulations. This paper presents the use of the modular and efficient uncertainty quantification (UQ) framework OpenLB-UQ for urban wind flow simulations. We specifically use the lattice Boltzmann method (LBM) coupled with a stochastic collocation (SC) approach based on generalized polynomial chaos (gPC). The framework introduces a relative-error noise model for inflow wind speeds based on real measurements. The model is propagated through a non-intrusive SC LBM pipeline using sparse-grid quadrature. Key quantities of interest, including mean flow fields, standard deviations, and vertical profiles with confidence intervals, are efficiently computed without altering the underlying deterministic solver. We demonstrate this on a real urban scenario, highlighting how uncertainty localizes in complex flow regions such as wakes and shear layers. The results show that the SC LBM approach provides accurate, uncertainty-aware predictions with significant computational efficiency, making OpenLB-UQ a practical tool for real-time urban wind analysis. |
| 2025-08-25 | [New shell-model calculations of the $Î´_C$ correction to superallowed $0^+\rightarrow0^+$ nuclear $Î²$ decay and standard-model implications](http://arxiv.org/abs/2508.18189v1) | L. Xayavong, N. A. Smirnova et al. | Refined calculations of the radial mismatch correction, $\delta_{C2}$, to superallowed $0^+\rightarrow0^+$ nuclear $\beta$ decay are performed using the shell model with realistic Woods-Saxon radial wave functions. Two important improvements are introduced: i) charge radii used to constrain the length parameter are evaluated within a generalized formula, where proton occupation numbers are substituted by sums of spectroscopic factors, while radial wave functions are required to match separation energies with respect to the intermediate $(A-1)$-nucleon states by adjusting parameters such as the potential depth; ii) configuration mixing wave functions and energies for many-particle states are obtained through the diagonalization of well-established effective interactions in large configuration spaces without truncation. Furthermore, a variation of $\pm0.1$\,fm in the surface diffuseness parameter is now incorporated as a source of uncertainty. The present results are generally in fairly good agreement with those from previous studies. As an exception, the $\delta_{C2}$ value obtained for $^{18}$Ne is smaller by approximately a factor of two, principally due to the updated charge-radius treatment. A reduction is also observed in most cases with $A\ge38$, through the deviations generally remain within the newly assigned error bars. The smaller isospin-mixing counterpart, $\delta_{C1}$, is strongly interaction-dependent, roughly following an inverse-square law with respect to the energy separation between the lowest admixed levels. Therefore, an additional procedure to ensure isobaric displacements within the isospin multiplets appears to be indispensable. Our results for $\delta_{C2}$ lead to a new averaged $\overline{\mathcal{F}t}$ value of $3073.11(99)_{stat}(36)_{\delta_R'}(173)_{\delta_{NS}}$~s with $\chi^2/\nu=0.624$. The corresponding $|V_{ud}|$ value is 0.97359(33). |
| 2025-08-25 | [Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization](http://arxiv.org/abs/2508.18170v1) | Jonathan A. Moran, Pablo G. Morato | Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%. |
| 2025-08-25 | [Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation](http://arxiv.org/abs/2508.18142v1) | Tianjun Wei, Huizhong Guo et al. | User simulation is increasingly vital to develop and evaluate recommender systems (RSs). While Large Language Models (LLMs) offer promising avenues to simulate user behavior, they often struggle with the absence of specific domain alignment required for RSs and the efficiency demands of large-scale simulation. A vast yet underutilized resource for enhancing this alignment is the extensive user feedback inherent in RSs. However, directly leveraging such feedback presents two significant challenges. First, user feedback in RSs is often ambiguous and noisy, which negatively impacts effective preference alignment. Second, the massive volume of feedback largely hinders the efficiency of preference alignment, necessitating an efficient filtering mechanism to identify more informative samples. To overcome these hurdles, we introduce a novel data construction framework that leverages user feedback in RSs with advanced LLM capabilities to generate high-quality simulation data. Our framework unfolds in two key phases: (1) employing LLMs to generate cognitive decision-making processes on constructed simulation samples, reducing ambiguity in raw user feedback; (2) data distillation based on uncertainty estimation and behavior sampling to filter challenging yet denoised simulation samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using such high-quality dataset with corresponding decision-making processes. Extensive experiments verify that our framework significantly boosts the alignment with human preferences and in-domain reasoning capabilities of fine-tuned LLMs, and provides more insightful and interpretable signals when interacting with RSs. We believe our work will advance the RS community and offer valuable insights for broader human-centric AI research. |
| 2025-08-25 | [Forward-Backward Quantization of Scenario Processes in Multi-Stage Stochastic Optimization](http://arxiv.org/abs/2508.18112v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization lies at the core of decision-making under uncertainty. As the analytical solution is available only in exceptional cases, dynamic optimization aims to efficiently find approximations but often neglects non-Markovian time-interdependencies. Methods on scenario trees can represent such interdependencies but are subject to the curse of dimensionality. To ease this problem, researchers typically approximate the uncertainty by smaller but more accurate trees. In this article, we focus on multi-stage optimal tree quantization methods of time-interdependent stochastic processes, for which we develop novel bounds and demonstrate that the upper bound can be minimized via projected gradient descent incorporating the tree structure as linear constraints. Consequently, we propose an efficient quantization procedure, which improves forward-looking samples using a backward step on the tree.We apply the results to the multi-stage inventory control with time-interdependent demand. For the case with one product, we benchmark the approximation because the problem allows a solution in closed-form. For the multi-dimensional problem, our solution found by optimal discrete approximation demonstrates the importance of holding mitigation inventory in different phases of the product life cycle. |
| 2025-08-25 | [Measurement of the branching ratio of $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ isotopes through the nuclear muon capture reaction in the Super-Kamiokande detector](http://arxiv.org/abs/2508.18110v1) | Y. Maekawa, K. Abe et al. | The Super-Kamiokande detector has measured solar neutrinos for more than $25$ years. The sensitivity for solar neutrino measurement is limited by the uncertainties of energy scale and background modeling. Decays of unstable isotopes with relatively long half-lives through nuclear muon capture, such as $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$ and $\mathrm{^{13}B}$, are detected as background events for solar neutrino observations. In this study, we developed a method to form a pair of stopping muon and decay candidate events and evaluated the production rates of such unstable isotopes. We then measured their branching ratios considering both their production rates and the estimated number of nuclear muon capture processes as $Br(\mathrm{^{16}N})=(9.0 \pm 0.1)\%$, $Br(\mathrm{^{15}C})=(0.6\pm0.1)\%$, $Br(\mathrm{^{12}B})=(0.98 \pm 0.18)\%$, $Br(\mathrm{^{13}B})=(0.14 \pm 0.12)\%$, respectively. The result for $\mathrm{^{16}N}$ has world-leading precision at present and the results for $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ are the first branching ratio measurements for those isotopes. |
| 2025-08-25 | [The $Î›_{b} \to Î›$ transition form factors in perturbative QCD approach](http://arxiv.org/abs/2508.18069v1) | Lei Yang, Jia-Jie Han et al. | In this work, we investigate the $\Lambda_b \to \Lambda$ transition form factors in the perturbative QCD (PQCD) approach, incorporating higher-twist light-cone distribution amplitudes (LCDAs). The resulted form factors show that higher-twist LCDAs are dominant numerically. By combining our PQCD predictions at low-$q^2$ with lattice QCD results at high-$q^2$, $z$-series expansion fits are performed to parametrize the form factors over the full kinematic range. We also provide the prediction for physical observables in the rare decay $\Lambda_b \to \Lambda \mu^+ \mu^-$, including the differential branching fraction, dilepton longitudinal polarization fraction, and forward-backward asymmetries (lepton-side, hadron-side, and combined lepton-hadron). Our obtained form factors are consistent with those in other theoretical methods within the uncertainties. |
| 2025-08-25 | [Assessing the conditional calibration of interval forecasts using decompositions of the interval score](http://arxiv.org/abs/2508.18034v1) | Sam Allen, Julia Burnello et al. | Forecasts for uncertain future events should be probabilistic. Probabilistic forecasts are commonly issued as prediction intervals, which provide a measure of uncertainty in the unknown outcome whilst being easier to understand and communicate than full predictive distributions. The calibration of a $(1 - \alpha)$-level prediction interval can be assessed by checking whether the probability that the outcome falls within the interval is equal to $1 - \alpha$. However, such coverage checks are typically unconditional and therefore relatively weak. Although this is well known, there is a lack of methods to assess the conditional calibration of interval forecasts. In this work, we demonstrate how this can be achieved via decompositions of the well-known interval (or Winkler) score. We study notions of calibration for interval forecasts and then introduce a decomposition of the interval score based on isotonic distributional regression. This decomposition exhibits many desirable properties, both in theory and in practice, which allows users to accurately assess the conditional calibration of interval forecasts. This is illustrated on simulated data and in three applications to benchmark regression datasets. |
| 2025-08-25 | [Precision Measurements of the Electroweak Mixing Angle in the Region of the Z pole](http://arxiv.org/abs/2508.18022v1) | Arie Bodek, Hyon-San Seo et al. | We review the current status and techniques used in precision measurements of the effective leptonic weak mixing angle $\sin^2\theta^\ell_{\rm eff}$ (a fundamental parameter of the Standard Model (SM)) in the region of the Z pole with emphasis on hadron colliders. We also build on these techniques to extract the most precise single measurement to date of $\sin^2\theta^\ell_{\rm eff}$ from a new analysis of the published forward-backward asymmetry ($A_{\rm FB}$) in Drell-Yan dielpton production in proton-proton collisions at a center of mass energy of 13 TeV measured by the CMS collaboration at the large hadron collider. The uncertainty in $\sin^2\theta^\ell_{\rm eff}$ published by CMS is dominated by uncertainties in Parton Distribution Functions (PDFs), which are reduced by PDF profiling using the dilepton mass dependence of $A_{\rm FB}$. Our new extraction of $\sin^2\theta^\ell_{\rm eff}$ from the CMS values of $A_{\rm FB}$ includes profiling with additional new CMS measurements of the $W$-boson decay lepton asymmetry, and W/Z cross section ratio at 13 TeV. We obtain the most precise single measurement of $\sin^2\theta^\ell_{\rm eff}$ to date of 0.23153$\pm$0.00023, which is in excellent agreement with the SM prediction of 0.23161$\pm$0.00004. We also discuss outlook for future measurements at the LHC including more precise measurements of $\sin^2\theta^\ell_{\rm eff}$, a measurement of $\sin^2\theta^\ell_{\rm eff}$ for b-quarks in the initial state, and a measurement of the running of $\sin^2\theta^{\overline{\rm MS}}(\mu)$ up to 3 TeV. |
| 2025-08-22 | [A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer](http://arxiv.org/abs/2508.16569v1) | Yuhui Tao, Zhongwei Zhao et al. | The non-invasive assessment of increasingly incidentally discovered renal masses is a critical challenge in urologic oncology, where diagnostic uncertainty frequently leads to the overtreatment of benign or indolent tumors. In this study, we developed and validated RenalCLIP using a dataset of 27,866 CT scans from 8,809 patients across nine Chinese medical centers and the public TCIA cohort, a visual-language foundation model for characterization, diagnosis and prognosis of renal mass. The model was developed via a two-stage pre-training strategy that first enhances the image and text encoders with domain-specific knowledge before aligning them through a contrastive learning objective, to create robust representations for superior generalization and diagnostic precision. RenalCLIP achieved better performance and superior generalizability across 10 core tasks spanning the full clinical workflow of kidney cancer, including anatomical assessment, diagnostic classification, and survival prediction, compared with other state-of-the-art general-purpose CT foundation models. Especially, for complicated task like recurrence-free survival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726, representing a substantial improvement of approximately 20% over the leading baselines. Furthermore, RenalCLIP's pre-training imparted remarkable data efficiency; in the diagnostic classification task, it only needs 20% training data to achieve the peak performance of all baseline models even after they were fully fine-tuned on 100% of the data. Additionally, it achieved superior performance in report generation, image-text retrieval and zero-shot diagnosis tasks. Our findings establish that RenalCLIP provides a robust tool with the potential to enhance diagnostic accuracy, refine prognostic stratification, and personalize the management of patients with kidney cancer. |
| 2025-08-22 | [Exploring null-entropy events: What do we learn when nothing happens?](http://arxiv.org/abs/2508.16528v1) | Abhaya S. Hegde, AndrÃ© M. Timpanaro et al. | Fluctuation theorems establish that thermodynamic processes at the microscale can occasionally result in negative entropy production. At the microscale, another distinct possibility becomes more likely: processes where no entropy is produced overall. In this work, we explore the constraints imposed by such null-entropy events on the fluctuations of thermodynamic currents. By incorporating the probability of null-entropy events, we obtain tighter bounds on finite-time thermodynamic uncertainty relations derived from fluctuation theorems. We validate this framework using an example of a qudit SWAP engine. |
| 2025-08-22 | [Identifying Treatment Effect Heterogeneity with Bayesian Hierarchical Adjustable Random Partition in Adaptive Enrichment Trials](http://arxiv.org/abs/2508.16523v1) | Xianglin Zhao, Shirin Golchi et al. | Treatment effect heterogeneity refers to the systematic variation in treatment effects across subgroups. There is an increasing need for clinical trials that aim to investigate treatment effect heterogeneity and estimate subgroup-specific responses. While several statistical methods have been proposed to address this problem, existing partitioning-based methods often depend on auxiliary analysis, overlook model uncertainty, or impose inflexible borrowing strength. We propose the Bayesian Hierarchical Adjustable Random Partition (BHARP) model, a self-contained framework that applies a finite mixture model with an unknown number of components to explore the partition space accounting for model uncertainty. The BHARP model jointly estimates subgroup-specific effects and the heterogeneity patterns, and adjusts the borrowing strengths based on within-cluster cohesion without requiring manual calibration. Posterior sampling is performed via a custom reversible-jump Markov chain Monte Carlo sampler tailored to partitioning-based information borrowing in clinical trials. Simulation studies across a range of treatment effect heterogeneity patterns show that the BHARP model achieves better accuracy and precision compared to conventional and advanced methods. We showcase the utilities of the BHARP model in the context of a multi-arm adaptive enrichment trial investigating physical activity interventions in patients with type 2 diabetes. |
| 2025-08-22 | [Uncertainty Quantification and Propagation for ACORN, a geometric deep learning tracking pipeline for HEP experiments](http://arxiv.org/abs/2508.16518v1) | Lukas PÃ©ron, Paolo Calafiura et al. | We have developed an Uncertainty Quantification process for multistep pipelines and applied it to the ACORN particle tracking pipeline. All our experiments are made using the TrackML open dataset. Using the Monte Carlo Dropout method, we measure the data and model uncertainties of the pipeline steps, study how they propagate down the pipeline, and how they are impacted by the training dataset's size, the input data's geometry and physical properties. We will show that for our case study, as the training dataset grows, the overall uncertainty becomes dominated by aleatoric uncertainty, indicating that we had sufficient data to train the ACORN model we chose to its full potential. We show that the ACORN pipeline yields high confidence in the track reconstruction and does not suffer from the miscalibration of the GNN model. |
| 2025-08-22 | [Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean Modeling](http://arxiv.org/abs/2508.16489v1) | Yixuan Sun, Romain Egele et al. | Accurate simulations of the oceans are crucial in understanding the Earth system. Despite their efficiency, simulations at lower resolutions must rely on various uncertain parameterizations to account for unresolved processes. However, model sensitivity to parameterizations is difficult to quantify, making it challenging to tune these parameterizations to reproduce observations. Deep learning surrogates have shown promise for efficient computation of the parametric sensitivities in the form of partial derivatives, but their reliability is difficult to evaluate without ground truth derivatives. In this work, we leverage large-scale hyperparameter search and ensemble learning to improve both forward predictions, autoregressive rollout, and backward adjoint sensitivity estimation. Particularly, the ensemble method provides epistemic uncertainty of function value predictions and their derivatives, providing improved reliability of the neural surrogates in decision making. |
| 2025-08-22 | [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](http://arxiv.org/abs/2508.16476v1) | Maryam Ghasemzadeh, Anton van Beek | Multi-objective Bayesian optimization (MOBO) struggles with sparse (non-space-filling), scarce (limited observations) datasets affected by experimental uncertainty, where identical inputs can yield varying outputs. These challenges are common in physical and simulation experiments (e.g., randomized medical trials and, molecular dynamics simulations) and are therefore incompatible with conventional MOBO methods. As a result, experimental resources are inefficiently allocated, leading to suboptimal designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data Trust Region-based Optimization Algorithm), a novel sampling framework that integrates prior knowledge of experimental uncertainty to construct more accurate surrogate models while employing trust regions to focus sampling on promising areas of the design space. By strategically leveraging prior information and refining search regions, NOSTRA accelerates convergence to the Pareto frontier, enhances data efficiency, and improves solution quality. Through two test functions with varying levels of experimental uncertainty, we demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse, and scarce data. Specifically, we illustrate that, NOSTRA effectively prioritizes regions where samples enhance the accuracy of the identified Pareto frontier, offering a resource-efficient algorithm that is practical in scenarios with limited experimental budgets while ensuring efficient performance. |
| 2025-08-22 | [Scalable Bayesian inference on high-dimensional multivariate linear regression](http://arxiv.org/abs/2508.16446v1) | Xuan Cao, Kyoungjae Lee | We consider jointly estimating the coefficient matrix and the error precision matrix in high-dimensional multivariate linear regression models. Bayesian methods in this context often face computational challenges, leading to previous approaches that either utilize a generalized likelihood without ensuring the positive definiteness of the precision matrix or rely on maximization algorithms targeting only the posterior mode, thus failing to address uncertainty. In this work, we propose two Bayesian methods: an exact method and an approximate two-step method. We first propose an exact method based on spike and slab priors for the coefficient matrix and DAG-Wishart prior for the error precision matrix, whose computational complexity is comparable to the state-of-the-art generalized likelihood-based Bayesian method. To further enhance scalability, a two-step approach is developed by ignoring the dependency structure among response variables. This method estimates the coefficient matrix first, followed by the calculation of the posterior of the error precision matrix based on the estimated errors. We validate the two-step method by demonstrating (i) selection consistency and posterior convergence rates for the coefficient matrix and (ii) selection consistency for the directed acyclic graph (DAG) of errors. We demonstrate the practical performance of proposed methods through synthetic and real data analysis. |
| 2025-08-22 | [Dynamic Financial Analysis (DFA) of General Insurers under Climate Change](http://arxiv.org/abs/2508.16444v1) | Benjamin Avanzi, Yanfeng Li et al. | Climate change is expected to significantly affect the physical, financial, and economic environments over the long term, posing risks to the financial health of general insurers. While general insurers typically use Dynamic Financial Analysis (DFA) for a comprehensive view of financial impacts, traditional DFA as presented in the literature does not consider the impact of climate change. To address this gap, we introduce a climate-dependent DFA approach that integrates climate risk into DFA, providing a holistic assessment of the long-term impact of climate change on the general insurance industry. The proposed framework has three key features. First, it captures the long-term impact of climate change on the assets and liabilities of general insurers by considering both physical and economic dimensions across different climate scenarios within an interconnected structure. Second, it addresses the uncertainty of climate change impacts using stochastic simulations within climate scenario analysis that are useful for actuarial applications. Finally, the framework is tailored to the general insurance sector by addressing its unique characteristics. To demonstrate the practical application of our model, we conduct an extensive empirical study using Australian data to assess the long-term financial impact of climate change on the general insurance market under various climate scenarios. The results show that the interaction between economic growth and physical risk plays a key role in shaping general insurers' risk-return profiles. Limitations of our framework are thoroughly discussed. |
| 2025-08-22 | [Cost-optimized replacement strategies for water electrolysis systems affected by degradation](http://arxiv.org/abs/2508.16370v1) | Marie Arnold, Jonathan Brandt et al. | A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up. |
| 2025-08-22 | [Attenuation Models for Extensive Air Showers Derived from Simulations](http://arxiv.org/abs/2508.16360v1) | Fiona Ellwanger, Darko VeberiÄ | At ultra-high energies, the flux of cosmic rays is too low for direct measurements to be meaningful. When a cosmic ray enters the atmosphere, it initiates an extensive air shower, producing a cascade of secondary particles that propagate toward the ground. Large arrays of surface detectors are used to measure these secondary particles upon arrival.   The signal detected at a specific reference distance from the shower core serves as a proxy for the shower size and, consequently, as a reliable estimator of the energy of primary cosmic ray. However, shower development is influenced by attenuation effects: measured signals at the ground depend on the amount of traversed atmospheric density (column density) through which the shower evolves. Since the column density varies with the inclination of the shower, it is important to account for these attenuation effects to ensure accurate energy estimation.   In this study, we derive physics-and-geometry-based functional forms to describe attenuation and propose appropriate expansion terms using simple one-dimensional shower-development models, incorporating one or two main particle-cascade components. We then evaluate the applicability and effectiveness of these functional forms using a Monte-Carlo dataset that includes various primary cosmic-ray particles. By directly calibrating the the shower size derived from ground signals to the Monte-Carlo energy, we characterize attenuation behavior across different primary particles, assess the energy dependence of attenuation, and quantify systematic uncertainties introduced by different functional forms. |
| 2025-08-21 | [Bayesian Hierarchical Methods for Surveillance of Cervical Dystonia Treatments](http://arxiv.org/abs/2508.15762v1) | D. Baidoo, E. Kubuafor et al. | Cervical dystonia, a debilitating neurological disorder marked by involuntary muscle contractions and chronic pain, presents significant treatment challenges despite advances in botulinum toxin therapy. While botulinum toxin type B has emerged as one of the leading treatments, comparative efficacy across doses and the influence of demographic factors for personalized medicine remain understudied. This study aimed to: (1) compare the efficacy of different botulinum toxin type B doses using Bayesian methods, (2) evaluate demographic and clinical factors affecting treatment response, and (3) establish a probabilistic framework for personalized cervical dystonia management. We analyzed data from a multicenter randomized controlled trial involving 109 patients assigned to placebo, 5,000 units, or 10,000 units of botulinum toxin type B groups. The primary outcome was the Toronto Western Spasmodic Torticollis Rating Scale measured over 16 weeks. Bayesian hierarchical modeling assessed treatment effects while accounting for patient heterogeneity. Lower botulinum toxin type B doses (5,000 units) showed greater overall Toronto Western Spasmodic Torticollis Rating Scale score reductions (treatment effect: -2.39, 95% Probability Interval: -4.10 to -0.70). Male patients demonstrated better responses (5.2% greater improvement) than female patients. Substantial between-patient variability and site-specific effects were observed, highlighting the need for personalized protocols. The study confirms botulinum toxin type B's dose-dependent efficacy while identifying key modifiable factors in treatment response. Bayesian methods provided nuanced insights into uncertainty and heterogeneity, paving the way for personalized medicine in cervical dystonia management. |
| 2025-08-21 | [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](http://arxiv.org/abs/2508.15719v1) | Mohammed Elmusrati | Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning. |
| 2025-08-21 | [CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps](http://arxiv.org/abs/2508.15672v1) | Franz Hanke, Antonia Bieringer et al. | Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3 |
| 2025-08-21 | [Beyond the Nyquist frequency: Asteroseismic catalog of undersampled Kepler late subgiants and early red giants](http://arxiv.org/abs/2508.15654v1) | B. Liagre, R. A. GarcÃ­a et al. | Subgiants and early red giants are crucial for studying the first dredge-up, a key evolutionary phase where the convective envelope deepens, mixing previously interior-processed material and bringing it to the surface. Yet, very few have been seismically characterized with Kepler because their oscillation frequencies are close to the 30 minute sampling frequency of the mission. We developed a new method as part of the new PyA2Z code to identify super-Nyquist oscillators and infer their global seismic parameters, $\nu_\mathrm{max}$ and large separation, $\Delta\nu$. Applying PyA2Z to 2 065 Kepler targets, we seismically characterize 285 super-Nyquist and 168 close-to-Nyquist stars with masses from 0.8 to 1.6 M$_\odot$. In combination with APOGEE spectroscopy, Gaia spectro-photometry, and stellar models, we derive stellar ages for the sample. There is good agreement between the predicted and actual positions of stars on the HR diagram (luminosity vs. effective temperature) as a function of mass and composition. While the timing of dredge-up is consistent with predictions, the magnitude and mass dependence show discrepancies with models, possibly due to uncertainties in model physics or calibration issues in observed abundance scales. |
| 2025-08-21 | [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2508.15652v1) | Ardian Selmonaj, Miroslav Strupl et al. | To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems. |
| 2025-08-21 | [Label Uncertainty for Ultrasound Segmentation](http://arxiv.org/abs/2508.15635v1) | Malini Shivaram, Gautam Rajendrakumar Gare et al. | In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging. |
| 2025-08-21 | [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](http://arxiv.org/abs/2508.15588v1) | Ahmed Nasir, Abdelhafid Zenati | The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden "skeleton" governing the system's behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the system's convergence properties and potential failure modes, such as unintended "trap" states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policy's safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone. |
| 2025-08-21 | [LoUQAL: Low-fidelity informed Uncertainty Quantification for Active Learning in the chemical configuration space](http://arxiv.org/abs/2508.15577v1) | Vivin Vinod, Peter Zaspel | Uncertainty quantification is an important scheme in active learning techniques, including applications in predicting quantum chemical properties. In quantum chemical calculations, there exists the notion of a fidelity, a less accurate computation is accessible at a cheaper computational cost. This work proposes a novel low-fidelity informed uncertainty quantification for active learning with applications in predicting diverse quantum chemical properties such as excitation energies and \textit{ab initio} potential energy surfaces. Computational experiments are carried out in order to assess the proposed method with results demonstrating that models trained with the novel method outperform alternatives in terms of empirical error and number of iterations required. The effect of the choice of fidelity is also studied to perform a thorough benchmark. |
| 2025-08-21 | [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](http://arxiv.org/abs/2508.15569v1) | Xin Du, Sikun Yang et al. | Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal prediction's rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the framework's effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification. |
| 2025-08-21 | [Uncertainty Relation for the Wigner-Yanase Skew Information and Quantum Sobolev Inequalities](http://arxiv.org/abs/2508.15554v1) | Laurent Lafleche | This note explores uncertainty inequalities for quantum analogues of the Fisher information including the Wigner-Yanase skew information, and their connection to the quantum Sobolev inequalities proved by the author in [Journal of Functional Analysis, 286 (10) 2024]. Some additional inequalities concerning commutators are derived and others are left as open problems. |
| 2025-08-20 | [Squeezed Diffusion Models](http://arxiv.org/abs/2508.14871v1) | Jyotirmai Singh, Samar Khanna et al. | Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes. |
| 2025-08-20 | [Calibration offset estimation in mobile hearing tests via categorical loudness scaling](http://arxiv.org/abs/2508.14824v1) | Chen Xu, Birger Kollmeier | Objective: To enable reliable smartphone-based hearing assessments by developing methods to estimate device calibration offsets using categorical loudness scaling (CLS). Design: Calibration offsets were simulated from a Gaussian distribution. Two prediction models - a Bayesian regression model and a nearest neighbor model - were trained on CLS-derived parameters and data from the Oldenburg Hearing Health Repository (OHHR). CLS was chosen because it provides level-independent measures (e.g., dynamic range) that remain robust despite calibration errors. Study Sample: The dataset comprised CLS results from N = 847 participants with a mean age of 70.0 years (SD = 8.7), including 556 male and 291 female listeners with diverse hearing profiles. Results: The Bayesian regression model achieved correlations of up to 0.81 between estimated and true calibration offsets, enabling accurate individual-level correction. Compared to threshold-based approaches, calibration uncertainty was reduced by factors between 0.41 and 0.79, demonstrating greater robustness in uncontrolled environments. Conclusions: CLS-based models can effectively compensate for missing calibration in mobile hearing assessments. This approach provides a practical alternative to threshold-based methods, supporting the use of smartphone-based tests outside laboratory settings and expanding access to reliable hearing healthcare in everyday and resource-limited contexts. |
| 2025-08-20 | [Equipartition and the temperature of maximum density of TIP4/2005 water](http://arxiv.org/abs/2508.14800v1) | Dilipkumar N. Asthagiri, Thomas L. Beck | We simulate TIP4P/2005 water in the temperature range of 257 K to 318 K with time steps of $\delta =$ 0.25, 0.50, and 2.0 fs. Within the computed statistical uncertainties, the density-temperature behavior obtained using 0.25 fs and 0.50 fs are in excellent agreement with each other but differ from those obtained using $\delta t = 2.0$ fs, a choice that leads to a breakdown of equipartition. The temperature of maximum density (TMD) is 277.15 K with $\delta t = $0.25 fs or 0.50 fs, but is shifted to 273.15 K for simulations with $\delta t = 2.0$ fs. This shift is comparable in magnitude to the shift in TMD due to nuclear quantum effects, emphasizing the care required in the parameterization and classical statistical mechanical simulation of a fluid that displays nontrivial nuclear quantum effects under ambient conditions. Enhancing the water-water dispersion interaction, as has been recommended for modeling disordered solvated proteins, degrades the description of the liquid-vapor phase envelope. |
| 2025-08-20 | [Safe and Transparent Robots for Human-in-the-Loop Meat Processing](http://arxiv.org/abs/2508.14763v1) | Sagar Parekh, Casey Grothoff et al. | Labor shortages have severely affected the meat processing sector. Automated technology has the potential to support the meat industry, assist workers, and enhance job quality. However, existing automation in meat processing is highly specialized, inflexible, and cost intensive. Instead of forcing manufacturers to buy a separate device for each step of the process, our objective is to develop general-purpose robotic systems that work alongside humans to perform multiple meat processing tasks. Through a recently conducted survey of industry experts, we identified two main challenges associated with integrating these collaborative robots alongside human workers. First, there must be measures to ensure the safety of human coworkers; second, the coworkers need to understand what the robot is doing. This paper addresses both challenges by introducing a safety and transparency framework for general-purpose meat processing robots. For safety, we implement a hand-detection system that continuously monitors nearby humans. This system can halt the robot in situations where the human comes into close proximity of the operating robot. We also develop an instrumented knife equipped with a force sensor that can differentiate contact between objects such as meat, bone, or fixtures. For transparency, we introduce a method that detects the robot's uncertainty about its performance and uses an LED interface to communicate that uncertainty to the human. Additionally, we design a graphical interface that displays the robot's plans and allows the human to provide feedback on the planned cut. Overall, our framework can ensure safe operation while keeping human workers in-the-loop about the robot's actions which we validate through a user study. |
| 2025-08-20 | [Distributional Adversarial Attacks and Training in Deep Hedging](http://arxiv.org/abs/2508.14757v1) | Guangyi He, Tobias Sutter et al. | In this paper, we study the robustness of classical deep hedging strategies under distributional shifts by leveraging the concept of adversarial attacks. We first demonstrate that standard deep hedging models are highly vulnerable to small perturbations in the input distribution, resulting in significant performance degradation. Motivated by this, we propose an adversarial training framework tailored to increase the robustness of deep hedging strategies. Our approach extends pointwise adversarial attacks to the distributional setting and introduces a computationally tractable reformulation of the adversarial optimization problem over a Wasserstein ball. This enables the efficient training of hedging strategies that are resilient to distributional perturbations. Through extensive numerical experiments, we show that adversarially trained deep hedging strategies consistently outperform their classical counterparts in terms of out-of-sample performance and resilience to model misspecification. Our findings establish a practical and effective framework for robust deep hedging under realistic market uncertainties. |
| 2025-08-20 | [Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds](http://arxiv.org/abs/2508.14661v1) | Alexander Raab, Stephan Weiss et al. | Aiming to enhance the consistency and thus long-term accuracy of Extended Kalman Filters for terrestrial vehicle localization, this paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the robot's pose in a space with reduced dimensionality, the approach ensures feasible estimates on generic smooth surfaces, without introducing artificial constraints or simplifications that may degrade a filter's performance. The accompanying measurement models are compatible with common loosely- and tightly-coupled sensor modalities and also implicitly account for the ground geometry. We extend the formulation by introducing a novel correction scheme that embeds additional domain knowledge into the sensor data, giving more accurate uncertainty approximations and further enhancing filter consistency. The proposed estimator is seamlessly integrated into a validated modular state estimation framework, demonstrating compatibility with existing implementations. Extensive Monte Carlo simulations across diverse scenarios and dynamic sensor configurations show that the M-ESEKF outperforms classical filter formulations in terms of consistency and stability. Moreover, it eliminates the need for scenario-specific parameter tuning, enabling its application in a variety of real-world settings. |
| 2025-08-20 | [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](http://arxiv.org/abs/2508.14654v1) | Peilin Ji, Xiao Xue et al. | In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response. |
| 2025-08-20 | [Experimental validation of universal filtering and smoothing for linear system identification using adaptive tuning](http://arxiv.org/abs/2508.14629v1) | Zihao Liu, Sima Abolghasemi et al. | In Kalman filtering, unknown inputs are often estimated by augmenting the state vector, which introduces reliance on fictitious input models. In contrast, minimum-variance unbiased methods estimate inputs and states separately, avoiding fictitious models but requiring strict sensor configurations, such as full-rank feedforward matrices or without direct feedthrough. To address these limitations, two universal approaches have been proposed to handle systems with or without direct feedthrough, including cases of rank-deficient feedforward matrices. Numerical studies have shown their robustness and applicability, however, they have so far relied on offline tuning, and performance under physical sensor noise and structural uncertainties has not yet been experimentally validated. Contributing to this gap, this paper experimentally validates the universal methods on a five-storey shear frame subjected to shake table tests and multi-impact events. Both typical and rank-deficient conditions are considered. Furthermore, a self-tuning mechanism is introduced to replace impractical offline tuning and enable real-time adaptability. The findings of this paper provide strong evidence of the robustness and adaptability of the methods for structural health monitoring applications, particularly when sensor networks deviate from ideal configurations. |
| 2025-08-20 | [A Simple and Scalable Kernel Density Approach for Reliable Uncertainty Quantification in Atomistic Machine Learning](http://arxiv.org/abs/2508.14613v1) | Daniel Willimetz, LukÃ¡Å¡ Grajciar | Machine learning models are increasingly used to predict material properties and accelerate atomistic simulations, but the reliability of their predictions depends on the representativeness of the training data. We present a scalable, GPU-accelerated uncertainty quantification framework based on $k$-nearest-neighbor kernel density estimation (KDE) in a PCA-reduced descriptor space. This method efficiently detects sparsely sampled regions in large, high-dimensional datasets and provides a transferable, model-agnostic uncertainty metric without requiring retraining costly model ensembles. The framework is validated across diverse case studies varying in: i) chemistry, ii) prediction models (including foundational neural network), iii) descriptors used for KDE estimation, and iv) properties whose uncertainty is sought. In all cases, the KDE-based score reliably flags extrapolative configurations, correlates well with conventional ensemble-based uncertainties, and highlights regions of reduced prediction trustworthiness. The approach offers a practical route for improving the interpretability, robustness, and deployment readiness of ML models in materials science. |
| 2025-08-20 | [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](http://arxiv.org/abs/2508.14597v1) | Nitish Kumar Mahala, Muzammil Khan et al. | Fire outbreaks pose critical threats to human life and infrastructure, necessitating high-fidelity early-warning systems that detect combustion precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal dynamics influenced by illumination variability, flow kinematics, and environmental noise, undermining the reliability of traditional detectors. To address these challenges without the logistical complexity of multi-sensor arrays, we propose an information-fusion framework by integrating smoke feature representations extracted from monocular imagery. Specifically, a Two-Phase Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke detection, leveraging a novel smoke segmentation dataset, constructed via optical flow-based motion encoding, is proposed. The optical flow estimation is performed with a four-color-theorem-inspired dual-phase level-set fractional-order variational model, which preserves motion discontinuities. The resulting color-encoded optical flow maps are fused with appearance cues via a Gaussian Mixture Model to generate binary segmentation masks of the smoke regions. These fused representations are fed into the novel Shifted-Windows Transformer, which is augmented with a multi-scale uncertainty estimation head and trained under a two-phase learning regimen. First learning phase optimizes smoke detection accuracy, while during the second phase, the model learns to estimate plausibility confidence in its predictions by jointly modeling aleatoric and epistemic uncertainties. Extensive experiments using multiple evaluation metrics and comparative analysis with state-of-the-art approaches demonstrate superior generalization and robustness, offering a reliable solution for early fire detection in surveillance, industrial safety, and autonomous monitoring applications. |
| 2025-08-19 | [BLIPs: Bayesian Learned Interatomic Potentials](http://arxiv.org/abs/2508.14022v1) | Dario Coscia, Pim de Haan et al. | Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool in simulation-based chemistry. However, like most deep learning models, MLIPs struggle to make accurate predictions on out-of-distribution data or when trained in a data-scarce regime, both common scenarios in simulation-based chemistry. Moreover, MLIPs do not provide uncertainty estimates by construction, which are fundamental to guide active learning pipelines and to ensure the accuracy of simulation results compared to quantum calculations. To address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian framework for training or fine-tuning MLIPs, built on an adaptive version of Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and minimal computational overhead for energy and forces prediction at inference time, while integrating seamlessly with (equivariant) message-passing architectures. Empirical results on simulation-based computational chemistry tasks demonstrate improved predictive accuracy with respect to standard MLIPs, and trustworthy uncertainty estimates, especially in data-scarse or heavy out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP yields consistent performance gains and calibrated uncertainties. |
| 2025-08-19 | [Data Compression with Noise Suppression for Inference under Noisy Covariance](http://arxiv.org/abs/2508.14021v1) | Sunao Sugiyama, Minsu Park | In many fields including cosmology, statistical inference often relies on Gaussian likelihoods whose covariance matrices are estimated from a finite number of simulations. This finite-sample estimation introduces noise into the covariance, which propagates to parameter estimates, a phenomenon known as the Dodelson-Schneider (DS) effect, leading to inflated uncertainties. While the Massively Optimized Parameter Estimation and Data compression (MOPED) algorithm offers lossless Fisher information-preserving compression, it does not mitigate the DS effect when the compression matrix itself is derived from noisy covariances. In this paper, we propose a modified compression scheme, powered MOPED ($p$-MOPED), which suppresses noise propagation by balancing information retention and covariance estimate noise reduction through a tunable power-law transformation of the sample correlation matrix. We test $p$-MOPED against standard and diagonal MOPED on toy models and on cosmological data from the Subaru Hyper Suprime-Cam Year 3 weak lensing survey. Our results demonstrate that $p$-MOPED consistently outperforms other approaches, especially in regimes with limited simulations, offering a robust compression strategy for high-dimensional data analyses under practical constraints. |
| 2025-08-19 | [Dark Energy Survey Year 3 Results: Cosmological constraints from second and third-order shear statistics](http://arxiv.org/abs/2508.14018v1) | R. C. H. Gomes, S. Sugiyama et al. | We present a cosmological analysis of the third-order aperture mass statistic using Dark Energy Survey Year 3 (DES Y3) data. We perform a complete tomographic measurement of the three-point correlation function of the Y3 weak lensing shape catalog with the four fiducial source redshift bins. Building upon our companion methodology paper, we apply a pipeline that combines the two-point function $\xi_{\pm}$ with the mass aperture skewness statistic $\langle M_{\rm ap}^3\rangle$, which is an efficient compression of the full shear three-point function. We use a suite of simulated shear maps to obtain a joint covariance matrix. By jointly analyzing $\xi_\pm$ and $\langle M_{\rm ap}^3\rangle$ measured from DES Y3 data with a $\Lambda$CDM model, we find $S_8=0.780\pm0.015$ and $\Omega_{\rm m}=0.266^{+0.039}_{-0.040}$, yielding 111% of figure-of-merit improvement in $\Omega_m$-$S_8$ plane relative to $\xi_{\pm}$ alone, consistent with expectations from simulated likelihood analyses. With a $w$CDM model, we find $S_8=0.749^{+0.027}_{-0.026}$ and $w_0=-1.39\pm 0.31$, which gives an improvement of $22\%$ on the joint $S_8$-$w_0$ constraint. Our results are consistent with $w_0=-1$. Our new constraints are compared to CMB data from the Planck satellite, and we find that with the inclusion of $\langle M_{\rm ap}^3\rangle$ the existing tension between the data sets is at the level of $2.3\sigma$. We show that the third-order statistic enables us to self-calibrate the mean photometric redshift uncertainty parameter of the highest redshift bin with little degradation in the figure of merit. Our results demonstrate the constraining power of higher-order lensing statistics and establish $\langle M_{\rm ap}^3\rangle$ as a practical observable for joint analyses in current and future surveys. |
| 2025-08-19 | [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](http://arxiv.org/abs/2508.13990v1) | Daniel KlÃ¶tzl, Ozan Tastekin et al. | Multidimensional data is often associated with uncertainties that are not well-described by normal distributions. In this work, we describe how such distributions can be projected to a low-dimensional space using uncertainty-aware principal component analysis (UAPCA). We propose to model multidimensional distributions using Gaussian mixture models (GMMs) and derive the projection from a general formulation that allows projecting arbitrary probability density functions. The low-dimensional projections of the densities exhibit more details about the distributions and represent them more faithfully compared to UAPCA mappings. Further, we support including user-defined weights between the different distributions, which allows for varying the importance of the multidimensional distributions. We evaluate our approach by comparing the distributions in low-dimensional space obtained by our method and UAPCA to those obtained by sample-based projections. |
| 2025-08-19 | [GBEES-GPU: An efficient parallel GPU algorithm for high-dimensional nonlinear uncertainty propagation](http://arxiv.org/abs/2508.13986v1) | Benjamin L. Hanson, Carlos Rubio et al. | Eulerian nonlinear uncertainty propagation methods often suffer from finite domain limitations and computational inefficiencies. A recent approach to this class of algorithm, Grid-based Bayesian Estimation Exploiting Sparsity, addresses the first challenge by dynamically allocating a discretized grid in regions of phase space where probability is non-negligible. However, the design of the original algorithm causes the second challenge to persist in high-dimensional systems. This paper presents an architectural optimization of the algorithm for CPU implementation, followed by its adaptation to the CUDA framework for single GPU execution. The algorithm is validated for accuracy and convergence, with performance evaluated across distinct GPUs. Tests include propagating a three-dimensional probability distribution subject to the Lorenz '63 model and a six-dimensional probability distribution subject to the Lorenz '96 model. The results imply that the improvements made result in a speedup of over 1000 times compared to the original implementation. |
| 2025-08-19 | [Dynamics-independent bounds on state transformations and precision in open quantum systems](http://arxiv.org/abs/2508.13884v1) | Yoshihiko Hasegawa | We derive dynamics-independent upper bounds on achievable quantum state transformations. Modeling the evolution as a joint unitary on the system and its environment, we show that the R\'enyi divergence between the initial system state and any state reachable via the dynamics is bounded from above by a quantity determined solely by the eigenvalues of the initial system and environment density operators. As a consequence, we establish dynamics-independent lower bounds on the relative variance for arbitrary measurements, which parallel thermodynamic uncertainty relations. Moreover, we obtain dynamics- and measurement-independent lower bounds on the variance of parameter estimators. These results depend only on the initial eigenvalues of the system and environment and hold for any joint unitary, providing computable bounds for open quantum systems. |
| 2025-08-19 | [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](http://arxiv.org/abs/2508.13874v1) | Rouqaiah Al-Refai, Pankaja Priya Ramasamy et al. | The rapid advancement of authentication systems and their increasing reliance on biometrics for faster and more accurate user verification experience, highlight the critical need for a reliable framework to evaluate the suitability of biometric modalities for specific applications. Currently, the most widely known evaluation framework is a comparative table from 1998, which no longer adequately captures recent technological developments or emerging vulnerabilities in biometric systems. To address these challenges, this work revisits the evaluation of biometric modalities through an expert survey involving 24 biometric specialists. The findings indicate substantial shifts in property ratings across modalities. For example, face recognition, shows improved ratings due to technological progress, while fingerprint, shows decreased reliability because of emerging vulnerabilities and attacks. Further analysis of expert agreement levels across rated properties highlighted the consistency of the provided evaluations and ensured the reliability of the ratings. Finally, expert assessments are compared with dataset-level uncertainty across 55 biometric datasets, revealing strong alignment in most modalities and underscoring the importance of integrating empirical evidence with expert insight. Moreover, the identified expert disagreements reveal key open challenges and help guide future research toward resolving them. |
| 2025-08-19 | [OpenLB-UQ: An Uncertainty Quantification Framework for Incompressible Fluid Flow Simulations](http://arxiv.org/abs/2508.13867v1) | Mingliang Zhong, Adrian KummerlÃ¤nder et al. | Uncertainty quantification (UQ) is crucial in computational fluid dynamics to assess the reliability and robustness of simulations, given the uncertainties in input parameters. OpenLB is an open-source lattice Boltzmann method library designed for efficient and extensible simulations of complex fluid dynamics on high-performance computers. In this work, we leverage the efficiency of OpenLB for large-scale flow sampling with a dedicated and integrated UQ module. To this end, we focus on non-intrusive stochastic collocation methods based on generalized polynomial chaos and Monte Carlo sampling. The OpenLB-UQ framework is extensively validated in convergence tests with respect to statistical metrics and sample efficiency using selected benchmark cases, including two-dimensional Taylor--Green vortex flows with up to four-dimensional uncertainty and a flow past a cylinder. Our results confirm the expected convergence rates and show promising scalability, demonstrating robust statistical accuracy as well as computational efficiency. OpenLB-UQ enhances the capability of the OpenLB library, offering researchers a scalable framework for UQ in incompressible fluid flow simulations and beyond. |
| 2025-08-19 | [Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems](http://arxiv.org/abs/2508.13839v1) | Yue Xiu, Yang Zhao et al. | The cell-free integrated sensing and communication (CF-ISAC) architecture is a promising enabler for 6G, offering spectrum efficiency and ubiquitous coverage. However, real deployments suffer from hardware impairments, especially nonlinear distortion from power amplifiers (PAs), which degrades both communication and sensing. To address this, we propose a movable antenna (MA)-aided CF-ISAC system that mitigates distortion and enhances robustness. The PAs nonlinearities are modeled by a third-order memoryless polynomial, where the third-order distortion coefficients (3RDCs) vary across access points (APs) due to hardware differences, aging, and environmental conditions. We design a distributed distortion-aware worst-case robust optimization framework that explicitly incorporates uncertainty in 3RDCs. First, we analyze the worst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB) and communication rate. Then, to address the resulting non-convexity, we apply successive convex approximation (SCA) for estimating the 3RDCs. With these, we jointly optimize beamforming and MA positions under transmit power and sensing constraints. To efficiently solve this highly non-convex problem, we develop an MA-enabled self-attention convolutional graph neural network (SACGNN) algorithm. Simulations demonstrate that our method substantially enhances the communication-sensing trade-off under distortion and outperforms fixed-position antenna baselines in terms of robustness and capacity, thereby highlighting the advantages of MA-aided CF-ISAC systems. |
| 2025-08-19 | [Online Conformal Selection with Accept-to-Reject Changes](http://arxiv.org/abs/2508.13838v1) | Kangdao Liu, Huajun Xi et al. | Selecting a subset of promising candidates from a large pool is crucial across various scientific and real-world applications. Conformal selection offers a distribution-free and model-agnostic framework for candidate selection with uncertainty quantification. While effective in offline settings, its application to online scenarios, where data arrives sequentially, poses challenges. Notably, conformal selection permits the deselection of previously selected candidates, which is incompatible with applications requiring irreversible selection decisions. This limitation is particularly evident in resource-intensive sequential processes, such as drug discovery, where advancing a compound to subsequent stages renders reversal impractical. To address this issue, we extend conformal selection to an online Accept-to-Reject Changes (ARC) procedure: non-selected data points can be reconsidered for selection later, and once a candidate is selected, the decision is irreversible. Specifically, we propose a novel conformal selection method, Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC), which incorporates online Benjamini-Hochberg procedure into the candidate selection process. We provide theoretical guarantees that OCS-ARC controls the false discovery rate (FDR) at or below the nominal level at any timestep under both i.i.d. and exchangeable data assumptions. Additionally, we theoretically show that our approach naturally extends to multivariate response settings. Extensive experiments on synthetic and real-world datasets demonstrate that OCS-ARC significantly improves selection power over the baseline while maintaining valid FDR control across all examined timesteps. |
| 2025-08-18 | [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](http://arxiv.org/abs/2508.13144v1) | David Heineman, Valentin Hofmann et al. | Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances. |
| 2025-08-18 | [The ALPINE-CRISTAL-JWST survey: spatially resolved star formation relations at $z\sim5$](http://arxiv.org/abs/2508.13136v1) | C. Accard, M. BÃ©thermin et al. | Star formation governs galaxy evolution, shaping stellar mass assembly and gas consumption across cosmic time. The Kennicutt-Schmidt (KS) relation, linking star formation rate (SFR) and gas surface densities, is fundamental to understand star formation regulation, yet remains poorly constrained at $z > 2$ due to observational limitations and uncertainties in locally calibrated gas tracers. The [CII] $158 {\rm \mu m}$ line has recently emerged as a key probe of the cold ISM and star formation in the early Universe. We investigate whether the resolved [CII]-SFR and KS relations established at low redshift remain valid at $4 < z < 6$ by analysing 13 main-sequence galaxies from the ALPINE and CRISTAL surveys, using multi-wavelength data (HST, JWST, ALMA) at $\sim2$ kpc resolution. We perform pixel-by-pixel spectral energy distribution (SED) modelling with CIGALE on resolution-homogenised images. We develop a statistical framework to fit the [CII]-SFR relation that accounts for pixel covariance and compare our results to classical fitting methods. We test two [CII]-to-gas conversion prescriptions to assess their impact on inferred gas surface densities and depletion times. We find a resolved [CII]-SFR relation with a slope of $0.87 \pm 0.15$ and intrinsic scatter of $0.19 \pm 0.03$ dex, which is shallower and tighter than previous studies at $z\sim5$. The resolved KS relation is highly sensitive to the [CII]-to-gas conversion factor: using a fixed global $\alpha_{\rm [CII]}$ yields depletion times of $0.5$-$1$ Gyr, while a surface brightness-dependent $W_{\rm [CII]}$, places some galaxies with high gas density in the starburst regime ($<0.1$ Gyr). Future inputs from both simulations and observations are required to better understand how the [CII]-to-gas conversion factor depends on local ISM properties. We need to break this fundamental limit to properly study the KS relation at $z\gtrsim4$. |
| 2025-08-18 | [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](http://arxiv.org/abs/2508.13121v1) | Carlos Celemin | This work introduces an automated testing approach that employs agents controlling game characters to detect potential bugs within a game level. Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient search, the method determines the next sampling point by analyzing the data collected so far and calculates the data point that will maximize information acquisition. To support the BO process, we introduce a game testing-specific model built on top of a grid map, that features the smoothness and uncertainty estimation required by BO, however and most importantly, it does not suffer the scalability issues that traditional models carry. The experiments demonstrate that the approach significantly improves map coverage capabilities in both time efficiency and exploration distribution. |
| 2025-08-18 | [Reasonable uncertainty: Confidence intervals in empirical Bayes discrimination detection](http://arxiv.org/abs/2508.13110v1) | Jiaying Gu, Nikolaos Ignatiadis et al. | We revisit empirical Bayes discrimination detection, focusing on uncertainty arising from both partial identification and sampling variability. While prior work has mostly focused on partial identification, we find that some empirical findings are not robust to sampling uncertainty. To better connect statistical evidence to the magnitude of real-world discriminatory behavior, we propose a counterfactual odds-ratio estimand with a attractive properties and interpretation. Our analysis reveals the importance of careful attention to uncertainty quantification and downstream goals in empirical Bayes analyses. |
| 2025-08-18 | [Surrogate-based Bayesian calibration methods for climate models: a comparison of traditional and non-traditional approaches](http://arxiv.org/abs/2508.13071v1) | Maike F. Holthuijzen, Atlanta Chakraborty et al. | Parameter calibration is crucial for reducing uncertainty and improving simulation accuracy in physics-based models, yet computational constraints pose significant challenges. Bayesian calibration methods offer a principled framework for combining prior knowledge with data while rigorously quantifying uncertainty. In this work, we compare four emulator-based Bayesian calibration methods: Calibrate-Emulate-Sample (CES), History Matching (HM), Bayesian Optimal Experimental Design (BOED), and a novel Goal-Oriented BOED (GBOED) approach, using the Lorenz '96 multiscale system as a testbed. Our GBOED formulation explicitly targets calibration-relevant quantities and leverages information-theoretic criteria for data selection. We assess each method in terms of calibration accuracy, uncertainty quantification, computational cost, and convergence behavior. We evaluate each method's performance in balancing computational cost, implementation complexity, and uncertainty quantification (UQ), with additional insights into convergence behavior as model evaluations increase. We find CES offers excellent performance but at high computational expense, while GBOED achieves comparable accuracy using fewer model evaluations. Standard BOED underperforms with respect to calibration accuracy, and HM shows moderate effectiveness but can be useful as a precursor. Our results highlight trade-offs among Bayesian strategies and demonstrate the promise of goal-oriented design in calibration workflows. |
| 2025-08-18 | [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](http://arxiv.org/abs/2508.13057v1) | Adolfo GonzÃ¡lez, VÃ­ctor Parada | Demand forecasting is essential for strategic planning in competitive environments, enabling resource optimization and improved responsiveness to market dynamics. However, multivariate time series modeling faces challenges due to data complexity, uncertainty, and frequent regime shifts. Traditional evaluation metrics can introduce biases and limit generalization. This work compares two custom evaluation functions: FMAE (Focused Mean Absolute Error), focused on minimizing absolute errors, and HEF (Hierarchical Evaluation Function), designed to weight global metrics and penalize large deviations. Experiments were conducted under different data splits (91:9, 80:20, 70:30) using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative accuracy, robustness, and computational efficiency. Results show that HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE), enhancing model robustness and explanatory power. These findings were confirmed via visualizations and statistical tests. Conversely, FMAE offers advantages in local metrics (MAE, MASE) and execution time, making it suitable for short-term scenarios. The study highlights a methodological trade-off: HEF is ideal for strategic planning, while FMAE is better suited for operational efficiency. A replicable framework is proposed for optimizing predictive models in dynamic environments. |
| 2025-08-18 | [A test of Ca ii H & K photometry for isolating massive globular clusters below the metallicity floor](http://arxiv.org/abs/2508.13031v1) | Bas van Heumen, William E. Harris et al. | The serendipitous discovery of the M31 globular cluster (GC) EXT8 has presented a significant challenge to current theories for GC formation. By finding other GCs similar to EXT8, it should become clear if and/or how EXT8 can fit into our current understanding of GC formation. We aim to test the potential of integrated-light narrow-band Ca II H & K photometry as a proxy for the metallicity of GCs to be able to provide effective candidate selection for massive GCs below the GC metallicity floor ([Fe/H] $\leq$ -2.5), such as EXT8. We investigate the behaviour of two colours involving the CaHK filter employed by the Pristine survey, CaHK-u and CaHK-g, as a function of metallicity through CFHT MegaCam imaging of EXT8 and a wide set of M31 GCs covering the metallicity range of -2.9 $\leq$ [Fe/H] $\leq$ +0.4. Additionally, we investigate if the CaHK colours are strongly influenced by horizontal branch morphology through available morphology measurements. In both of the CaHK colours, EXT8 and two other potential GCs below the metallicity floor can be selected from other metal-poor GCs ([Fe/H] $\leq$ -1.5) with CaHK-g showing the larger metallicity sensitivity. The RMS of linear fits to the metal-poor GCs show an uncertainty of 0.3 dex on metallicity estimations for both colours. Comparisons with u-g and g-z/F450W-F850L colours reinforce the notion that CaHK photometry can be used for effective candidate selection as they reduce false positive selection rates by at least a factor of 2. We find no strong influence of the horizontal branch morphology on the CaHK colours that would interfere with candidate selection, although the assessment is limited by quantity and quality of available data. |
| 2025-08-18 | [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](http://arxiv.org/abs/2508.13021v1) | Pengcheng Huang, Shuhao Liu et al. | Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler. |
| 2025-08-18 | [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](http://arxiv.org/abs/2508.12997v1) | Haishun Chen, Cai Xu et al. | Multi-view evidential learning aims to integrate information from multiple views to improve prediction performance and provide trustworthy uncertainty esitimation. Most previous methods assume that view-specific evidence learning is naturally reliable. However, in practice, the evidence learning process tends to be biased. Through empirical analysis on real-world data, we reveal that samples tend to be assigned more evidence to support data-rich classes, thereby leading to unreliable uncertainty estimation in predictions. This motivates us to delve into a new Biased Evidential Multi-view Learning (BEML) problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning (FAML). FAML first introduces an adaptive prior based on training trajectory, which acts as a regularization strategy to flexibly calibrate the biased evidence learning process. Furthermore, we explicitly incorporate a fairness constraint based on class-wise evidence variance to promote balanced evidence allocation. In the multi-view fusion stage, we propose an opinion alignment mechanism to mitigate view-specific bias across views, thereby encouraging the integration of consistent and mutually supportive evidence. Extensive experiments on five real-world multi-view datasets demonstrate that FAML achieves more balanced evidence allocation and improves both prediction performance and the reliability of uncertainty estimation compared to state-of-the-art methods. |
| 2025-08-18 | [Likelihood-Based Heterogeneity Inference Reveals Non-Stationary Effects in Biohybrid Cell-Cargo Transport](http://arxiv.org/abs/2508.12976v1) | Jan Albrecht, Lara S. Dautzenberg et al. | Variability of motility behavior in populations of microbiological agents is an ubiquitous phenomenon even in the case of genetically identical cells. Accordingly, passive objects introduced into such biological systems and driven by them will also exhibit heterogeneous motion patterns. Here, we study a biohybrid system of passive beads driven by active ameboid cells and use a likelihood approach to estimate the heterogeneity of the bead dynamics from their discretely sampled trajectories. We showcase how this approach can deal with information-scarce situations and provides natural uncertainty bounds for heterogeneity estimates. Using these advantages we particularly uncover that the heterogeneity in the system is time-dependent. |
| 2025-08-15 | [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](http://arxiv.org/abs/2508.11573v1) | Mogens Plessen | Automatic Section Control (ASC) is a long-standing trend for spraying in agriculture. It promises to minimise spray overlap areas. The core idea is to (i) switch off spray nozzles on areas that have already been sprayed, and (ii) to dynamically adjust nozzle flow rates along the boom bar that holds the spray nozzles when velocities of boom sections vary during turn maneuvers. ASC is not possible without sensors, in particular for accurate positioning data. Spraying and the movement of modern wide boom bars are highly dynamic processes. In addition, many uncertainty factors have an effect such as cross wind drift, boom height, nozzle clogging in open-field conditions, and so forth. In view of this complexity, the natural question arises if a simpler alternative exist. Therefore, an Automatic Multi-Sections Control method is compared to a proposed simpler one- or two-sections alternative that uses predictive spray switching. The comparison is provided under nominal conditions. Agricultural spraying is intrinsically linked to area coverage path planning and spray switching logic. Combinations of two area coverage path planning and switching logics as well as three sections-setups are compared. The three sections-setups differ by controlling 48 sections, 2 sections or controlling all nozzles uniformly with the same control signal as one single section. Methods are evaluated on 10 diverse real-world field examples, including non-convex field contours, freeform mainfield lanes and multiple obstacle areas. A preferred method is suggested that (i) minimises area coverage pathlength, (ii) offers intermediate overlap, (iii) is suitable for manual driving by following a pre-planned predictive spray switching logic for an area coverage path plan, and (iv) and in contrast to ASC can be implemented sensor-free and therefore at low cost. |
| 2025-08-15 | [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](http://arxiv.org/abs/2508.11547v1) | Martin JirouÅ¡ek, TomÃ¡Å¡ BÃ¡Äa et al. | This paper addresses the problem of tracking the position of a cable-suspended payload carried by an unmanned aerial vehicle, with a focus on real-world deployment and minimal hardware requirements. In contrast to many existing approaches that rely on motion-capture systems, additional onboard cameras, or instrumented payloads, we propose a framework that uses only standard onboard sensors--specifically, real-time kinematic global navigation satellite system measurements and data from the onboard inertial measurement unit--to estimate and control the payload's position. The system models the full coupled dynamics of the aerial vehicle and payload, and integrates a linear Kalman filter for state estimation, a model predictive contouring control planner, and an incremental model predictive controller. The control architecture is designed to remain effective despite sensing limitations and estimation uncertainty. Extensive simulations demonstrate that the proposed system achieves performance comparable to control based on ground-truth measurements, with only minor degradation (< 6%). The system also shows strong robustness to variations in payload parameters. Field experiments further validate the framework, confirming its practical applicability and reliable performance in outdoor environments using only off-the-shelf aerial vehicle hardware. |
| 2025-08-15 | [Integrating Uncertainties for Koopman-Based Stabilization](http://arxiv.org/abs/2508.11533v1) | Yicheng Lin, Bingxian Wu et al. | Over the past decades, the Koopman operator has been widely applied in data-driven control, yet its theoretical foundations remain underexplored. This paper establishes a unified framework to address the robust stabilization problem in data-driven control via the Koopman operator, fully accounting for three uncertainties: projection error, estimation error, and process disturbance. It comprehensively investigates both direct and indirect data-driven control approaches, facilitating flexible methodology selection for analysis and control. For the direct approach, considering process disturbances, the lifted-state feedback controller, designed via a linear matrix inequality (LMI), robustly stabilizes all lifted bilinear systems consistent with noisy data. For the indirect approach requiring system identification, the feedback controller, designed using a nonlinear matrix inequality convertible to an LMI, ensures closed-loop stability under worst-case process disturbances. Numerical simulations via cross-validation validate the effectiveness of both approaches, highlighting their theoretical significance and practical utility. |
| 2025-08-15 | [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](http://arxiv.org/abs/2508.11460v1) | Aurora Grefsrud, Nello Blaser et al. | Rigorous statistical methods, including parameter estimation with accompanying uncertainties, underpin the validity of scientific discovery, especially in the natural sciences. With increasingly complex data models such as deep learning techniques, uncertainty quantification has become exceedingly difficult and a plethora of techniques have been proposed. In this case study, we use the unifying framework of approximate Bayesian inference combined with empirical tests on carefully created synthetic classification datasets to investigate qualitative properties of six different probabilistic machine learning algorithms for class probability and uncertainty estimation: (i) a neural network ensemble, (ii) neural network ensemble with conflictual loss, (iii) evidential deep learning, (iv) a single neural network with Monte Carlo Dropout, (v) Gaussian process classification and (vi) a Dirichlet process mixture model. We check if the algorithms produce uncertainty estimates which reflect commonly desired properties, such as being well calibrated and exhibiting an increase in uncertainty for out-of-distribution data points. Our results indicate that all algorithms are well calibrated, but none of the deep learning based algorithms provide uncertainties that consistently reflect lack of experimental evidence for out-of-distribution data points. We hope our study may serve as a clarifying example for researchers developing new methods of uncertainty estimation for scientific data-driven modeling. |
| 2025-08-15 | [Improving Air Shower Simulations by Tuning Pythia 8/Angantyr with Accelerator Data](http://arxiv.org/abs/2508.11458v1) | Michael Windau, ChloÃ© Gaudu et al. | We present a combined analysis of the Pythia 8 event generator using accelerator data and evaluate its impact on air shower observables. Reliable simulations with event generators are essential for particle physics analyses, achievable through advanced tuning to experimental data. Pythia 8 has emerged as a promising high-energy interaction model for cosmic ray air shower simulations, offering well-documented parameter settings and a user-friendly interface to enable automatic tuning efforts. Using data from collider and fixed-target experiments, we first derive tunes for each domain separately, before tuning both domains simultaneously. To achieve this, we define a core set of observables and quantify their dependence on selected parameters. The tuning efforts are based on gradient descent and Bayesian methods, the latter providing a full uncertainty propagation of the parameters to the observables. Results for the impact of a combined analysis for the Pythia 8/Angantyr event generator on air shower observables, such as particle densities at ground level and energy deposit profiles, are presented. |
| 2025-08-15 | [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](http://arxiv.org/abs/2508.11453v1) | Jiayue Jin, Lang Qian et al. | Recent years have witnessed remarkable progress in autonomous driving, with systems evolving from modular pipelines to end-to-end architectures. However, most existing methods are trained offline and lack mechanisms to adapt to new environments during deployment. As a result, their generalization ability diminishes when faced with unseen variations in real-world driving scenarios. In this paper, we break away from the conventional "train once, deploy forever" paradigm and propose EvoPSF, a novel online Evolution framework for autonomous driving based on Planning-State Feedback. We argue that planning failures are primarily caused by inaccurate object-level motion predictions, and such failures are often reflected in the form of increased planner uncertainty. To address this, we treat planner uncertainty as a trigger for online evolution, using it as a diagnostic signal to initiate targeted model updates. Rather than performing blind updates, we leverage the planner's agent-agent attention to identify the specific objects that the ego vehicle attends to most, which are primarily responsible for the planning failures. For these critical objects, we compute a targeted self-supervised loss by comparing their predicted waypoints from the prediction module with their actual future positions, selected from the perception module's outputs with high confidence scores. This loss is then backpropagated to adapt the model online. As a result, our method improves the model's robustness to environmental changes, leads to more precise motion predictions, and therefore enables more accurate and stable planning behaviors. Experiments on both cross-region and corrupted variants of the nuScenes dataset demonstrate that EvoPSF consistently improves planning performance under challenging conditions. |
| 2025-08-15 | [Conformal Prediction Meets Long-tail Classification](http://arxiv.org/abs/2508.11345v1) | Shuqi Liu, Jianguo Huang et al. | Conformal Prediction (CP) is a popular method for uncertainty quantification that converts a pretrained model's point prediction into a prediction set, with the set size reflecting the model's confidence. Although existing CP methods are guaranteed to achieve marginal coverage, they often exhibit imbalanced coverage across classes under long-tail label distributions, tending to over cover the head classes at the expense of under covering the remaining tail classes. This under coverage is particularly concerning, as it undermines the reliability of the prediction sets for minority classes, even with coverage ensured on average. In this paper, we propose the Tail-Aware Conformal Prediction (TACP) method to mitigate the under coverage of the tail classes by utilizing the long-tail structure and narrowing the head-tail coverage gap. Theoretical analysis shows that it consistently achieves a smaller head-tail coverage gap than standard methods. To further improve coverage balance across all classes, we introduce an extension of TACP: soft TACP (sTACP) via a reweighting mechanism. The proposed framework can be combined with various non-conformity scores, and experiments on multiple long-tail benchmark datasets demonstrate the effectiveness of our methods. |
| 2025-08-15 | [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](http://arxiv.org/abs/2508.11340v1) | Yuanlin Liu, Zhihan Zhou et al. | Information on the number and category of cervical cells is crucial for the diagnosis of cervical cancer. However, existing classification methods capable of automatically measuring this information require the training dataset to be representative, which consumes an expensive or even unaffordable human cost. We herein propose active labeling that enables us to construct a representative training dataset using a much smaller human cost for data-efficient cervical cell classification. This cost-effective method efficiently leverages the classifier's uncertainty on the unlabeled cervical cell images to accurately select images that are most beneficial to label. With a fast estimation of the uncertainty, this new algorithm exhibits its validity and effectiveness in enhancing the representative ability of the constructed training dataset. The extensive empirical results confirm its efficacy again in navigating the usage of human cost, opening the avenue for data-efficient cervical cell classification. |
| 2025-08-15 | [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](http://arxiv.org/abs/2508.11338v1) | Prathamesh Devadiga, Yashmitha Shailesh | We introduce RegimeNAS, a novel differentiable architecture search framework specifically designed to enhance cryptocurrency trading performance by explicitly integrating market regime awareness. Addressing the limitations of static deep learning models in highly dynamic financial environments, RegimeNAS features three core innovations: (1) a theoretically grounded Bayesian search space optimizing architectures with provable convergence properties; (2) specialized, dynamically activated neural modules (Volatility, Trend, and Range blocks) tailored for distinct market conditions; and (3) a multi-objective loss function incorporating market-specific penalties (e.g., volatility matching, transition smoothness) alongside mathematically enforced Lipschitz stability constraints. Regime identification leverages multi-head attention across multiple timeframes for improved accuracy and uncertainty estimation. Rigorous empirical evaluation on extensive real-world cryptocurrency data demonstrates that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving an 80.3% Mean Absolute Error reduction compared to the best traditional recurrent baseline and converging substantially faster (9 vs. 50+ epochs). Ablation studies and regime-specific analysis confirm the critical contribution of each component, particularly the regime-aware adaptation mechanism. This work underscores the imperative of embedding domain-specific knowledge, such as market regimes, directly within the NAS process to develop robust and adaptive models for challenging financial applications. |
| 2025-08-15 | [Demonstrating the velocity response of a table-top EPR Speedmeter](http://arxiv.org/abs/2508.11316v1) | S. L. Kranzhoff, S. L. Danilishin et al. | The sensitivity of gravitational-wave interferometers is fundamentally limited by quantum noise, as dictated by the Heisenberg uncertainty principle, due to their continuous position measurement of the end mirrors. Speedmeter configurations, which measure mirror velocity rather than position, have been proposed as a means to suppress quantum back-action noise, but practical implementations remain at an early stage. In this work, we present a table-top realisation of the Einstein- Podolsky-Rosen (EPR) Speedmeter concept, employing an optical readout scheme based on two orthogonal polarisation modes that probe the interferometer with different effective bandwidths. Using a triangular cavity, we demonstrate that the differential optical response between the linear p- and s-polarised modes exhibits a speed-like frequency dependence: vanishing at DC and increasing linearly with signal frequency, up to the bandwidth of the slower mode. With this we show that an optical system equivalent to the EPR Speedmeter indeed performs a velocity readout of the end mirror. |
| 2025-08-14 | [The Dark Energy Bedrock All-Sky Supernova Program: Cross Calibration, Simulations, and Cosmology Forecasts](http://arxiv.org/abs/2508.10877v1) | Maria Acevedo, Nora R. Sherman et al. | Type Ia supernovae (SNe Ia) have been essential for probing the nature of dark energy; however, most SN analyses rely on the same low-redshift sample, which may lead to shared systematics. In a companion paper (Sherman et al., submitted), we introduce the Dark Energy Bedrock All-Sky Supernova (DEBASS) program, which has already collected more than 500 low-redshift SNe Ia on the Dark Energy Camera (DECam), and present an initial release of 77 SNe Ia within the Dark Energy Survey (DES) footprint observed between 2021 and 2024. Here, we examine the systematics, including photometric calibration and selection effects. We find agreement at the 10 millimagnitude level among the tertiary standard stars of DEBASS, DES, and Pan-STARRS1. Our simulations reproduce the observed distributions of DEBASS SN light-curve properties, and we measure a bias-corrected Hubble residual scatter of $0.08$ mag, which, while small, is found in 10% of our simulations. We compare the DEBASS SN distances to the Foundation sample and find consistency with a median residual offset of $0.016 \pm 0.019$ mag. Selection effects have negligible impacts on distances, but a different photometric calibration solution shifts the median residual $-0.015 \pm 0.019$ mag, highlighting calibration sensitivity. Using conservative simulations, we forecast that replacing historical low-redshift samples with the full DEBASS sample (>400 SNe Ia) will improve the statistical uncertainties on dark energy parameters $w_0$ and $w_a$ by 30% and 24% respectively, enhance the dark energy Figure of Merit by up to 60%, and enable a measurement of $f\sigma_8$ at the 25% level. |
| 2025-08-14 | [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](http://arxiv.org/abs/2508.10867v1) | Yizhi Zhou, Ziwei Kang et al. | Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial odometry (VIO) systems. Consistency is crucial for ensuring the estimation accuracy of a UWBaided VIO system. An inconsistent estimator can degrade localization performance, where the inconsistency primarily arises from two main factors: (1) the estimator fails to preserve the correct system observability, and (2) UWB anchor positions are assumed to be known, leading to improper neglect of calibration uncertainty. In this paper, we propose a consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system based on the Lie group. Our method incorporates the UWB anchor state into the system state, explicitly accounting for UWB calibration uncertainty and enabling the joint and consistent estimation of both robot and anchor states. Furthermore, observability consistency is ensured by leveraging the invariant error properties of the Lie group. We analytically prove that the CVIRO algorithm naturally maintains the system's correct unobservable subspace, thereby preserving estimation consistency. Extensive simulations and experiments demonstrate that CVIRO achieves superior localization accuracy and consistency compared to existing methods. |
| 2025-08-14 | [Comparison of Data Reduction Criteria for Online Gaussian Processes](http://arxiv.org/abs/2508.10815v1) | Thore Wietzke, Knut Graichen | Gaussian Processes (GPs) are widely used for regression and system identification due to their flexibility and ability to quantify uncertainty. However, their computational complexity limits their applicability to small datasets. Moreover in a streaming scenario, more and more datapoints accumulate which is intractable even for Sparse GPs. Online GPs aim to alleviate this problem by e.g. defining a maximum budget of datapoints and removing redundant datapoints. This work provides a unified comparison of several reduction criteria, analyzing both their computational complexity and reduction behavior. The criteria are evaluated on benchmark functions and real-world datasets, including dynamic system identification tasks. Additionally, acceptance criteria are proposed to further filter out redundant datapoints. This work yields practical guidelines for choosing a suitable criterion for an online GP algorithm. |
| 2025-08-14 | [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](http://arxiv.org/abs/2508.10798v1) | Troi Williams | Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks. |
| 2025-08-14 | [When Experts Disagree: Characterizing Annotator Variability for Vessel Segmentation in DSA Images](http://arxiv.org/abs/2508.10797v1) | M. Geshvadi, G. So et al. | We analyze the variability among segmentations of cranial blood vessels in 2D DSA performed by multiple annotators in order to characterize and quantify segmentation uncertainty. We use this analysis to quantify segmentation uncertainty and discuss ways it can be used to guide additional annotations and to develop uncertainty-aware automatic segmentation methods. |
| 2025-08-14 | [Accelerating Stochastic Energy System Optimization Models: Temporally Split Benders Decomposition](http://arxiv.org/abs/2508.10789v1) | Shima Sasanpour, Manuel Wetzel et al. | Stochastic programming can be applied to consider uncertainties in energy system optimization models for capacity expansion planning. However, these models become increasingly large and time-consuming to solve, even without considering uncertainties. For two-stage stochastic capacity expansion planning problems, Benders decomposition is often applied to ensure that the problem remains solvable. Since stochastic scenarios can be optimized independently within subproblems, their optimization can be parallelized. However, hourly-resolved capacity expansion planning problems typically have a larger temporal than scenario cardinality. Therefore, we present a temporally split Benders decomposition that further exploits the parallelization potential of stochastic expansion planning problems. A compact reformulation of the storage level constraint into linking variables ensures that long-term storage operation can still be optimized despite the temporal decomposition. We demonstrate this novel approach with model instances of the German power system with up to 87 million rows and columns. Our results show a reduction in computing times of up to 60% and reduced memory requirements. Additional enhancement strategies and the use of distributed memory on high-performance computers further improve the computing time by over 80%. |
| 2025-08-14 | [Estimating Covariance for Global Minimum Variance Portfolio: A Decision-Focused Learning Approach](http://arxiv.org/abs/2508.10776v1) | Juchan Kim, Inwoo Tae et al. | Portfolio optimization constitutes a cornerstone of risk management by quantifying the risk-return trade-off. Since it inherently depends on accurate parameter estimation under conditions of future uncertainty, the selection of appropriate input parameters is critical for effective portfolio construction. However, most conventional statistical estimators and machine learning algorithms determine these parameters by minimizing mean-squared error (MSE), a criterion that can yield suboptimal investment decisions. In this paper, we adopt decision-focused learning (DFL) - an approach that directly optimizes decision quality rather than prediction error such as MSE - to derive the global minimum-variance portfolio (GMVP). Specifically, we theoretically derive the gradient of decision loss using the analytic solution of GMVP and its properties regarding the principal components of itself. Through extensive empirical evaluation, we show that prediction-focused estimation methods may fail to produce optimal allocations in practice, whereas DFL-based methods consistently deliver superior decision performance. Furthermore, we provide a comprehensive analysis of DFL's mechanism in GMVP construction, focusing on its volatility reduction capability, decision-driving features, and estimation characteristics. |
| 2025-08-14 | [On data-driven robust distortion risk measures for non-negative risks with partial information](http://arxiv.org/abs/2508.10682v1) | Xiangyu Han, Yijun Hu et al. | In this paper, by proposing two new kinds of distributional uncertainty sets, we explore robustness of distortion risk measures against distributional uncertainty. To be precise, we first consider a distributional uncertainty set which is characterized solely by a ball determined by general Wasserstein distance centered at certain empirical distribution function, and then further consider additional constraints of known first moment and any other higher moment of the underlying loss distribution function. Under the assumption that the distortion function is strictly concave and twice differentiable, and that the underlying loss random variable is non-negative and bounded, we derive closed-form expressions for the distribution functions which maximize a given distortion risk measure over the distributional uncertainty sets respectively. Moreover, we continue to study the general case of a concave distortion function and unbounded loss random variables. Comparisons with existing studies are also made. Finally, we provide a numerical study to illustrate the proposed models and results. Our work provides a novel generalization of several known achievements in the literature. |
| 2025-08-14 | [A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners](http://arxiv.org/abs/2508.10679v1) | Jinhua He, Tingzhe Pan et al. | With the continuous increase in the penetration of renewable energy in the emerging power systems, the pressure on system peak regulation has been significantly intensified. Against this backdrop, demand side resources particularly air conditioning loads have garnered considerable attention for their substantial regulation potential and fast response capabilities, making them promising candidates for providing auxiliary peak shaving services. This study focuses on fixed frequency air conditioners (FFACs) and proposes an optimization model and solution method for their participation in demand response (DR) programs. First, a probabilistic response model for FFACs is developed based on the Markov assumption. Second, by sampling this probabilistic model, the aggregate power consumption of an FFAC cluster under decentralized control is obtained. Subsequently, a robust optimization model is formulated to maximize the profit of an aggregator managing the FFAC cluster during DR events, taking into account the aggregated response power. The model explicitly considers temperature uncertainty to ensure user comfort in a robust sense. Finally, leveraging the structure of the proposed model, it is reformulated as a mixed-integer linear programming (MILP) problem and solved using a commercial optimization solver. Simulation results validate the effectiveness of the proposed model and solution approach. |
| 2025-08-14 | [On the Derivation of Equations of Motion from Symmetries in Quantum-Mechanical Systems via Heisenberg's Uncertainty](http://arxiv.org/abs/2508.10661v1) | Enrique Casanova, JosÃ© Rojas et al. | We propose the construction of equations of motion based on symmetries in quantum-mechanical systems, using Heisenberg's uncertainty principle as a minimal foundation. From canonical operators, two spaces of conjugate operators are constructed, along with a third space derived from the former, which includes the ``Symmetry-Dilation'' operator. When this operator commutes with the main equation of motion, it defines the set of observables compatible with a complete basis of operators (symmetry generators), organized into a Lie algebra dependent on Heisenberg's uncertainty principle within Minkowski spacetime. Furthermore, by requiring the dilation operator to commute with the central operator, the wavefunction is constrained, thereby constructing known structures. Specific cases are derived -- relativistic, non-relativistic, and a lesser-studied case: ``ultra-relativistic (Carroll-Schr\"odinger)''. Our work may open new avenues for understanding and classifying symmetries in quantum mechanics, as well as offer an alternative method for deriving equations of motion and applying them to complex scenarios involving exotic particles. |
| 2025-08-13 | [Data-driven analyses and model-independent fits for present $b\to s \ell \ell$ results](http://arxiv.org/abs/2508.09986v1) | T. Hurth, F. Mahmoudi et al. | We present a critical assessment of the present $B$ anomalies in the exclusive $b \to s \ell\ell$ mode based on the QCD factorisation approach (QCDf). In particular, we analyse the impact of different local form factor calculations and of the largest bin in the low-$q^2$ region.   We also present a model-independent analysis of the new results of the CMS experiment on the $B \to K^* \mu^+\mu^-$ angular observables and compare them with the corresponding LHCb data. In addition, we update the global fit by including all $b \to s$ observables incorporating the new data from CMS. In these analyses, we use 10% or higher guesstimates of the non-factorisable power corrections as additional uncertainties, serving as a placeholder for robust estimates of these contributions.   Updating earlier results, we also analyse the combined LHCb and CMS data on the $B \to K^* \mu^+\mu^-$ angular observables using data-driven approaches to find indications whether these tensions between the QCDf predictions and the present data are due to underestimated subleading hadronic contributions or due to new physics effects. |
| 2025-08-13 | [Laboratory Measurements of Ca XIX Dielectronic Recombination Satellites](http://arxiv.org/abs/2508.09975v1) | Filipe Grilo, Marc Botz et al. | We report measurements of the K$\alpha$ emission from the astrophysically very abundant Ca XIX (He-like ion) and its satellite lines resonantly excited by dielectronic recombination (DR). We achieve an electron-energy resolution of 8 eV in a cryogenic electron beam ion trap, and determine the energies of the exciting electrons and the emitted photons up to the KLn ($n\le 8$) manifold with $0.05\%$ and $0.1\%$ respective uncertainties. For the KLL satellites, energies agree very well with our predictions using the Flexible Atomic Code (FAC) and previous state-of-the-art calculations. Our calculations also agree with our experimental direct excitation cross-sections for K$\alpha$ within their $10\%$ uncertainty. We extract DR coefficient rates and find good agreement with values tabulated in the OPEN-ADAS database. As an application, we experimentally benchmark Ca XIX atomic data used to model high-temperature astrophysical plasmas by comparing FAC synthetic spectra with recent XRISM observations revealing the contributions of DR satellites to the Ca XIX lines. |
| 2025-08-13 | [Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems](http://arxiv.org/abs/2508.09908v1) | Haoshu Cheng, Martin Guay et al. | In this paper, we investigate the problem of tracking formations driven by bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty in the presence of multiple moving leaders. To estimate the leaders' velocities and accelerations, we first design a distributed observer for the leader system, utilizing a bearing-based localization condition in place of the conventional connectivity assumption. This observer, coupled with an adaptive mechanism, enables the synthesis of a novel distributed control law that guides the formation towards the target formation, without requiring prior knowledge of the system parameters. Furthermore, we establish a sufficient condition, dependent on the initial formation configuration, that ensures collision avoidance throughout the formation evolution. The effectiveness of the proposed approach is demonstrated through a numerical example. |
| 2025-08-13 | [Multi-head committees enable direct uncertainty prediction for atomistic foundation models](http://arxiv.org/abs/2508.09907v1) | Hubert Beck, Pavol Simko et al. | Machine learning potentials have become a standard tool for atomistic materials modelling. While models continue to become more generalisable, an open challenge relates to efficient uncertainty predictions for active learning and robust error analysis. In this work, we utilise MACE and its multi-head mechanism to implement a committee neural network potential for message-passing architectures, where the committee comprises multiple output modules attached to the same atomic environment descriptors. As with traditional committees of independent networks, the standard deviation of the predictions functions as an estimate of the model's uncertainty. We show for a range of datasets in custom-build models that the uncertainty of the force predictions correlates well with the true errors. We subsequently apply this concept to foundation models, specifically MACE-MP-0, where we train only the newly attached output heads while keeping the remaining part of the model fixed. We use this approach in an active learning workflow to condense the training set of the foundation model to just 5\% of its original size. The foundation model multi-head committee trained on the condensed training set enables reliable uncertainty estimation without any substantial decrease in prediction accuracy. |
| 2025-08-13 | [$gg \to ZH$ : updated predictions at NLO QCD](http://arxiv.org/abs/2508.09905v1) | Benjamin Campillo Aveleira, Long Chen et al. | We present state-of-the-art predictions for the inclusive cross section of gluon-initiated $ZH$ production, following the recommendations of the LHC Higgs Working Group. In particular, we include NLO QCD corrections, where the virtual corrections are obtained from the combination of a forward expansion and a high-energy expansion, and the real corrections are exact. The expanded results for the virtual corrections are compared in detail to full numerical results. The updated predictions show a reduction of the scale uncertainties to the level of 15%, and they include an estimate of the top-mass-scheme uncertainty. |
| 2025-08-13 | [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](http://arxiv.org/abs/2508.09732v1) | Romeo Valentin, Sydney M. Katz et al. | Recent advances in data-driven computer vision have enabled robust autonomous navigation capabilities for civil aviation, including automated landing and runway detection. However, ensuring that these systems meet the robustness and safety requirements for aviation applications remains a major challenge. In this work, we present a practical vision-based pipeline for aircraft pose estimation from runway images that represents a step toward the ability to certify these systems for use in safety-critical aviation applications. Our approach features three key innovations: (i) an efficient, flexible neural architecture based on a spatial Soft Argmax operator for probabilistic keypoint regression, supporting diverse vision backbones with real-time inference; (ii) a principled loss function producing calibrated predictive uncertainties, which are evaluated via sharpness and calibration metrics; and (iii) an adaptation of Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling runtime detection and rejection of faulty model outputs. We implement and evaluate our pose estimation pipeline on a dataset of runway images. We show that our model outperforms baseline architectures in terms of accuracy while also producing well-calibrated uncertainty estimates with sub-pixel precision that can be used downstream for fault detection. |
| 2025-08-13 | [CKFNet: Neural Network Aided Cubature Kalman filtering](http://arxiv.org/abs/2508.09727v1) | Jinhui Hu, Haiquan Zhao et al. | The cubature Kalman filter (CKF), while theoretically rigorous for nonlinear estimation, often suffers performance degradation due to model-environment mismatches in practice. To address this limitation, we propose CKFNet-a hybrid architecture that synergistically integrates recurrent neural networks (RNN) with the CKF framework while preserving its cubature principles. Unlike conventional model-driven approaches, CKFNet embeds RNN modules in the prediction phase to dynamically adapt to unmodeled uncertainties, effectively reducing cumulative error propagation through temporal noise correlation learning. Crucially, the architecture maintains CKF's analytical interpretability via constrained optimization of cubature point distributions. Numerical simulation experiments have confirmed that our proposed CKFNet exhibits superior accuracy and robustness compared to conventional model-based methods and existing KalmanNet algorithms. |
| 2025-08-13 | [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](http://arxiv.org/abs/2508.09639v1) | Akshat Dubey, Aleksandar AnÅ¾el et al. | Explainable Artificial Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP), have become essential tools for interpreting complex ensemble tree-based models, especially in high-stakes domains such as healthcare analytics. However, SHAP values are usually treated as point estimates, which disregards the inherent and ubiquitous uncertainty in predictive models and data. This uncertainty has two primary sources: aleatoric and epistemic. The aleatoric uncertainty, which reflects the irreducible noise in the data. The epistemic uncertainty, which arises from a lack of data. In this work, we propose an approach for decomposing uncertainty in SHAP values into aleatoric, epistemic, and entanglement components. This approach integrates Dempster-Shafer evidence theory and hypothesis sampling via Dirichlet processes over tree ensembles. We validate the method across three real-world use cases with descriptive statistical analyses that provide insight into the nature of epistemic uncertainty embedded in SHAP explanations. The experimentations enable to provide more comprehensive understanding of the reliability and interpretability of SHAP-based attributions. This understanding can guide the development of robust decision-making processes and the refinement of models in high-stakes applications. Through our experiments with multiple datasets, we concluded that features with the highest SHAP values are not necessarily the most stable. This epistemic uncertainty can be reduced through better, more representative data and following appropriate or case-desired model development techniques. Tree-based models, especially bagging, facilitate the effective quantification of epistemic uncertainty. |
| 2025-08-13 | [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](http://arxiv.org/abs/2508.09623v1) | Akshay Thakur, Sawan Kumar et al. | Solving partial differential equations (PDEs) within the framework of probabilistic numerics offers a principled approach to quantifying epistemic uncertainty arising from discretization. By leveraging Gaussian process regression and imposing the governing PDE as a constraint at a finite set of collocation points, probabilistic numerics delivers mesh-free solutions at arbitrary locations. However, the high computational cost, which scales cubically with the number of collocation points, remains a critical bottleneck, particularly for large-scale or high-dimensional problems. We propose a scalable enhancement to this paradigm through two key innovations. First, we develop a stochastic dual descent algorithm that reduces the per-iteration complexity from cubic to linear in the number of collocation points, enabling tractable inference. Second, we exploit a clustering-based active learning strategy that adaptively selects collocation points to maximize information gain while minimizing computational expense. Together, these contributions result in an $h$-adaptive probabilistic solver that can scale to a large number of collocation points. We demonstrate the efficacy of the proposed solver on benchmark PDEs, including two- and three-dimensional steady-state elliptic problems, as well as a time-dependent parabolic PDE formulated in a space-time setting. |
| 2025-08-13 | [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](http://arxiv.org/abs/2508.09561v1) | Changyuan Zhao, Guangyuan Liu et al. | Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems. |
| 2025-08-12 | [Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs](http://arxiv.org/abs/2508.09135v1) | Wenxin Zhang, Mark van der Laan | Adaptive experimental designs have gained popularity in clinical trials and online experiments. Unlike traditional, fixed experimental designs, adaptive designs can dynamically adjust treatment randomization probabilities and other design features in response to data accumulated sequentially during the experiment. These adaptations are useful to achieve diverse objectives, including reducing uncertainty in the estimation of causal estimands or increasing participants' chances of receiving better treatments during the experiment. At the end of the experiment, it is often desirable to answer causal questions from the observed data. However, the adaptive nature of such experiments and the resulting dependence among observations pose significant challenges to providing valid statistical inference and efficient estimation of causal estimands. Building upon the Targeted Maximum Likelihood Estimator (TMLE) framework tailored for adaptive designs (van der Laan, 2008), we introduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a variety of causal estimands from adaptive experiment data. We establish asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed positivity and design stabilization assumptions for adaptive experiments. Motivated by efficiency results, we further propose a novel adaptive design aimed at minimizing the variance of estimators based on data generated under that design. Using the average treatment effect as a representative example, simulation studies show that ADL-TMLE demonstrates superior variance-reduction performance across different types of adaptive experiments, and that the proposed adaptive design attains lower variance than the standard efficiency-oriented adaptive design. Finally, we generalize this estimation and design framework to broader settings with longitudinal structures. |
| 2025-08-12 | [A New Method of Deriving Doppler Velocities for Solar Orbiter SPICE](http://arxiv.org/abs/2508.09121v1) | J. E. Plowman, D. M. Hassler et al. | This paper presents a follow-up to previous work on correcting PSF-induced Doppler artifacts in observations by the SPICE spectrograph on Solar Orbiter. In a previous paper, we demonstrated correction of these artifacts in the $y-\lambda$ plane with PSF Regularization, treating the forward problem with a method based on large sparse matrix inversion. It has since been found that similar apparent artifacts are also present in the $x-\lambda$ direction, i.e., across adjacent slit positions. This is difficult (although not impossible) to correct with the previous matrix inversion method due to the time variation between slit positions. We have therefore devised a new method which addresses both $x-\lambda$ and $y-\lambda$ artifacts simultaneously by applying wavelength dependent shifts at each $x-y$ plane of the spectral cube. This paper demonstrates the SPICE data issue, describes the new method, and shows a comparison with the previous one. We explore the time variation of the correction parameters for the SPICE data and show a clear orbit dependence. The results of the method are significantly higher quality derived Doppler signals, which we estimate at less than $\sim$ 5 km/s uncertainty for brighter lines in the absence of other systematics. Furthermore, we show the new SPICE polar observation results as a demonstration. The correction codes are written in Python, publicly available on GitHub, and can be directly applied to SPICE level 2 datasets. |
| 2025-08-12 | [Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring](http://arxiv.org/abs/2508.09085v1) | Zihan Fang, Zheng Lin et al. | Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness. |
| 2025-08-12 | [CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks](http://arxiv.org/abs/2508.09054v1) | Debdeep Mukherjee, Eduardo Di Santi et al. | Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability. |
| 2025-08-12 | [Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2508.09014v1) | Kaiwen Huang, Tao Zhou et al. | Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at https://github.com/taozh2017/UCSeg. |
| 2025-08-12 | [Current-Enhanced Excited States in Lattice QCD Three-Point Functions](http://arxiv.org/abs/2508.09006v1) | Lorenzo Barca | Excited-state contamination remains one of the leading sources of systematic uncertainty in the precise determination of hadron structure observables from lattice QCD. In this letter, we present a general argument, inspired by current-meson dominance and implemented through the variational method, to identify which excited states are enhanced by the choice of the inserted current and kinematics. The argument is supported by numerical evidence across multiple hadronic channels and provides both a conceptual understanding and practical guidance to account for excited-state effects in hadron three-point function analyses. |
| 2025-08-12 | [Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty](http://arxiv.org/abs/2508.08992v1) | Rui Wang, Qihan Lin et al. | Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT. |
| 2025-08-12 | [Mutually equi-biased bases](http://arxiv.org/abs/2508.08969v1) | Seyed Javad Akhtarshenas, Saman Karimi et al. | In the framework of mutually unbiased bases (MUBs), a measurement in one basis gives \emph{no information} about the outcomes of measurements in another basis. Here, we relax the no-information condition by allowing the $d$ outcomes to be predicted according to a predefined probability distribution $q=(q_0,\cdots,q_{d-1})$. The notion of mutual unbiasedness, however, is preserved by requiring that the extracted information is the same for any preparation and any measurement; regardless of which state from which basis is chosen to prepare the system, the outcomes of measuring the system with respect to the other basis generate the same probability distribution. In the light of this, we define the notion of \emph{mutually equi-biased bases} (MEBs) such that within each basis the states are equi-biased with respect to the states of the other basis and that the bases are mutually equi-biased with respect to each other. For $d=2,3$, we derive a complete set of $d+1$ MEBs. The mutual equi-biasedness imposes nontrivial constraints on the distribution $q$, leading for $d=3$ to the restriction $1/3\le\mu \le 1/2$ where $\mu=\sum_{k=0}^{2}q_k^2$. To capture the incompatibility of the measurements in MEBs, we derive an inequality for the probabilities of projective measurements in a qudit system, which yields an associated entropic uncertainty inequality. Finally, we construct a class of positive maps and their associated entanglement witnesses based on MEBs. While an entanglement witness constructed from MUBs is generally finer than one based on MEBs when both use the same number of bases, for certain values of the index $\mu$, employing a larger set of MEBs can yield a finer witness. We illustrate this behavior using isotropic states of a $3\times 3$ system. |
| 2025-08-12 | [A comprehensive analysis of Drell-Yan production uncertainties and mass effects at moderate and low dilepton masses](http://arxiv.org/abs/2508.08956v1) | Ekta Chaubey, Claude Duhr et al. | We present a thorough investigation of the sources of uncertainties to the Drell-Yan production using state-of-the-art predictions for both neutral and charged current channels, focusing on the low invariant mass region. Differential predictions for the invariant mass spectrum are provided at N$^3$LO supplemented with exact charm and bottom quark mass effects calculated at $\mathcal{O}(\alpha_s^2)$. The impact of PDF choices (including approximate N$^3$LO), scale variations, the variation of the strong coupling constant, and impact heavy quark mass effects on the distributions is studied in detail. We also comment on the correlation of high-energy astrophysical processes with the low-mass DY region. |
| 2025-08-12 | [Hi-fi functional priors by learning activations](http://arxiv.org/abs/2508.08880v1) | Marcin Sendera, Amin Sorkhei et al. | Function-space priors in Bayesian Neural Networks (BNNs) provide a more intuitive approach to embedding beliefs directly into the model's output, thereby enhancing regularization, uncertainty quantification, and risk-aware decision-making. However, imposing function-space priors on BNNs is challenging. We address this task through optimization techniques that explore how trainable activations can accommodate higher-complexity priors and match intricate target function distributions. We investigate flexible activation models, including Pade functions and piecewise linear functions, and discuss the learning challenges related to identifiability, loss construction, and symmetries. Our empirical findings indicate that even BNNs with a single wide hidden layer when equipped with flexible trainable activation, can effectively achieve desired function-space priors. |
| 2025-08-11 | [Autonomous Air-Ground Vehicle Operations Optimization in Hazardous Environments: A Multi-Armed Bandit Approach](http://arxiv.org/abs/2508.08217v1) | Jimin Choi, Max Z. Li | Hazardous environments such as chemical spills, radiological zones, and bio-contaminated sites pose significant threats to human safety and public infrastructure. Rapid and reliable hazard mitigation in these settings often unsafe for humans, calling for autonomous systems that can adaptively sense and respond to evolving risks. This paper presents a decision-making framework for autonomous vehicle dispatch in hazardous environments with uncertain and evolving risk levels. The system integrates a Bayesian Upper Confidence Bound (BUCB) sensing strategy with task-specific vehicle routing problems with profits (VRPP), enabling adaptive coordination of unmanned aerial vehicles (UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning. Using VRPP allows selective site visits under resource constraints by assigning each site a visit value that reflects sensing or cleaning priorities. Site-level hazard beliefs are maintained through a time-weighted Bayesian update. BUCB scores guide UAV routing to balance exploration and exploitation under uncertainty, while UGV routes are optimized to maximize expected hazard reduction under resource constraints. Simulation results demonstrate that our framework reduces the number of dispatch cycles to resolve hazards by around 30% on average compared to baseline dispatch strategies, underscoring the value of uncertainty-aware vehicle dispatch for reliable hazard mitigation. |
| 2025-08-11 | [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](http://arxiv.org/abs/2508.08206v1) | Amirhossein Taherpour, Abbas Taherpour et al. | We propose a joint learning framework for Byzantine-resilient spectrum sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic access under channel state information (CSI) uncertainty. The sensing stage performs logit-domain Bayesian updates with trimmed aggregation and attention-weighted consensus, and the base station (BS) fuses network beliefs with a conservative minimum rule, preserving detection accuracy under a bounded number of Byzantine users. Conditioned on the sensing outcome, we pose downlink design as sum mean-squared error (MSE) minimization under transmit-power and signal-leakage constraints and jointly optimize the BS precoder, IRS phase shifts, and user equalizers. With partial (or known) CSI, we develop an augmented-Lagrangian alternating algorithm with projected updates and provide provable sublinear convergence, with accelerated rates under mild local curvature. With unknown CSI, we perform constrained Bayesian optimization (BO) in a geometry-aware low-dimensional latent space using Gaussian process (GP) surrogates; we prove regret bounds for a constrained upper confidence bound (UCB) variant of the BO module, and demonstrate strong empirical performance of the implemented procedure. Simulations across diverse network conditions show higher detection probability at fixed false-alarm rate under adversarial attacks, large reductions in sum MSE for honest users, strong suppression of eavesdropper signal power, and fast convergence. The framework offers a practical path to secure opportunistic communication that adapts to CSI availability while coherently coordinating sensing and transmission through joint learning. |
| 2025-08-11 | [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](http://arxiv.org/abs/2508.08204v1) | Kyle Moore, Jesse Roberts et al. | There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis. |
| 2025-08-11 | [Atomic thermometry in optical lattice clocks](http://arxiv.org/abs/2508.08164v1) | Irene Goti, Tommaso Petrucciani et al. | Accurate measurement of atomic temperature is essential for a wide range of applications, from quantum sensing to precision metrology. In optical lattice clocks, precise characterization of atomic temperature is required to minimize systematic uncertainties at the $10^{-18}$ level. In this work, we investigate atomic temperature measurements in the ytterbium optical lattice clock developed at INRIM, IT-Yb1, employing sideband and Doppler spectroscopy across a wide range of trapping conditions. By implementing clock-line-mediated Sisyphus cooling, we reduce the atomic temperature and enable operation at shallower lattice depths down to $D = 50E_{R}$. We compare temperature estimates obtained from the harmonic oscillator model with those derived using a Born-Oppenheimer-based approach, which is expected to provide a more accurate description of atomic motion in both longitudinal and radial directions, especially for hotter atoms whose motion deviates from the harmonic regime. Discrepancies up to a factor of two in extracted temperatures are observed depending on the chosen model. We assess the impact of these modeling differences on the evaluation of lattice frequency shifts and find deviations up to $8\times10^{-17}$. Even though extended Sisyphus cooling reduces these inconsistencies to the $1\times10^{-18}$ level or below, residual biases may still limit the accuracy of optical lattice clocks. |
| 2025-08-11 | [Precision Test of Bound-State QED at Intermediate-Z with Kaonic Neon](http://arxiv.org/abs/2508.08161v1) | Manti S., Sgaramella F. et al. | We report Dirac-Fock calculations of transition energies for kaonic neon (KNe). For the most intense line, the 7-6 transition, the calculated energy is 9450.28 eV, which includes a bound-state QED (BSQED) contribution of 12.66 eV. This is in excellent agreement with the recent SIDDHARTHA-2 measurement at DA$\Phi$NE of 9450.23 $\pm$ 0.37 (stat.) $\pm$ 1.50 (syst.) eV. With the QED shift far exceeding experimental uncertainty, these results establish kaonic atoms as powerful platforms for precision tests of BSQED in intermediate-Z systems. |
| 2025-08-11 | [Robust Adaptive Discrete-Time Control Barrier Certificate](http://arxiv.org/abs/2508.08153v1) | Changrui Liu, Anil Alan et al. | This work develops a robust adaptive control strategy for discrete-time systems using Control Barrier Functions (CBFs) to ensure safety under parametric model uncertainty and disturbances. A key contribution of this work is establishing a barrier function certificate in discrete time for general online parameter estimation algorithms. This barrier function certificate guarantees positive invariance of the safe set despite disturbances and parametric uncertainty without access to the true system parameters. In addition, real-time implementation and inherent robustness guarantees are provided. Our approach demonstrates that, using the proposed robust adaptive CBF framework, the parameter estimation module can be designed separately from the CBF-based safety filter, simplifying the development of safe adaptive controllers for discrete-time systems. The resulting safety filter guarantees that the system remains within the safe set while adapting to model uncertainties, making it a promising strategy for real-world applications involving discrete-time safety-critical systems. |
| 2025-08-11 | [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](http://arxiv.org/abs/2508.08139v1) | Tianyi Zhou, Johanne Medina et al. | Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation. |
| 2025-08-11 | [OFAL: An Oracle-Free Active Learning Framework](http://arxiv.org/abs/2508.08126v1) | Hadi Khorsand, Vahid Pourahmadi | In the active learning paradigm, using an oracle to label data has always been a complex and expensive task, and with the emersion of large unlabeled data pools, it would be highly beneficial If we could achieve better results without relying on an oracle. This research introduces OFAL, an oracle-free active learning scheme that utilizes neural network uncertainty. OFAL uses the model's own uncertainty to transform highly confident unlabeled samples into informative uncertain samples. First, we start with separating and quantifying different parts of uncertainty and introduce Monte Carlo Dropouts as an approximation of the Bayesian Neural Network model. Secondly, by adding a variational autoencoder, we go on to generate new uncertain samples by stepping toward the uncertain part of latent space starting from a confidence seed sample. By generating these new informative samples, we can perform active learning and enhance the model's accuracy. Lastly, we try to compare and integrate our method with other widely used active learning sampling methods. |
| 2025-08-11 | [Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors](http://arxiv.org/abs/2508.08097v1) | Muhammad Asif, Zain Ali et al. | This work explores the integration of rate-splitting multiple access (RSMA), simultaneous wireless information and power transfer (SWIPT), and beyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the spectral-efficiency, energy-efficiency, coverage, and connectivity of future sixth-generation (6G) communication networks. Specifically, with a multiuser BD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding vectors, the common rate proportion of users, the power-splitting ratios, and scattering matrix of BD-RIS node, under the assumption of imperfect channel state information (CSI). Additionally, to better capture practical hardware behavior, we incorporate a nonlinear energy harvesting model under energy harvesting constraints. We design a robust optimization framework to maximize the system sum-rate, while explicitly accounting for the worst-case impact of CSI uncertainties. Further, we introduce an alternating optimization framework that partitions the problem into several blocks, which are optimized iteratively. More specifically, the transmit precoding vectors are optimized by reformulating the problem as a convex semidefinite programming through successive-convex approximation (SCA), whereas the power-splitting problem is solved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the scattering matrix of the BD-RIS, we first employ SCA to reformulate the problem into a convex form, and then design a manifold optimization strategy based on the Conjugate-Gradient method. Finally, numerical simulation results reveal that the proposed scheme provides significant performance improvements over existing benchmarks and demonstrates rapid convergence within a reasonable number of iterations. |
| 2025-08-11 | [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](http://arxiv.org/abs/2508.08075v1) | Meishen He, Wenjun Ma et al. | The Dempster-Shafer theory of evidence has been widely applied in the field of information fusion under uncertainty. Most existing research focuses on combining evidence within the same frame of discernment. However, in real-world scenarios, trained algorithms or data often originate from different regions or organizations, where data silos are prevalent. As a result, using different data sources or models to generate basic probability assignments may lead to heterogeneous frames, for which traditional fusion methods often yield unsatisfactory results. To address this challenge, this study proposes an open-world information fusion method, termed Full Negation Belief Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a criterion is introduced to determine whether a given fusion task belongs to the open-world setting. Then, by extending the frames, the method can accommodate elements from heterogeneous frames. Finally, a full negation mechanism is employed to transform the mass functions, so that existing combination rules can be applied to the transformed mass functions for such information fusion. Theoretically, the proposed method satisfies three desirable properties, which are formally proven: mass function invariance, heritability, and essential conflict elimination. Empirically, FNBT demonstrates superior performance in pattern classification tasks on real-world datasets and successfully resolves Zadeh's counterexample, thereby validating its practical effectiveness. |
| 2025-08-08 | [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](http://arxiv.org/abs/2508.06468v1) | Jesutofunmi Ajayi, Antonio Di Maio et al. | Network Slicing has emerged as a powerful technique to enable cost-effective, multi-tenant communications and services over a shared physical mobile network infrastructure. One major challenge of service provisioning in slice-enabled networks is the uncertainty in the demand for the limited network resources that must be shared among existing slices and potentially new Network Slice Requests. In this paper, we consider admission control of Network Slice Requests in an online setting, with the goal of maximizing the long-term revenue received from admitted requests. We model the Slice Admission Control problem as an Online Multidimensional Knapsack Problem and present two reservation-based policies and their algorithms, which have a competitive performance for Online Multidimensional Knapsack Problems. Through Monte Carlo simulations, we evaluate the performance of our online admission control method in terms of average revenue gained by the Infrastructure Provider, system resource utilization, and the ratio of accepted slice requests. We compare our approach with those of the online First Come First Serve greedy policy. The simulation's results prove that our proposed online policies increase revenues for Infrastructure Providers by up to 12.9 % while reducing the average resource consumption by up to 1.7% In particular, when the tenants' economic inequality increases, an Infrastructure Provider who adopts our proposed online admission policies gains higher revenues compared to an Infrastructure Provider who adopts First Come First Serve. |
| 2025-08-08 | [Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials](http://arxiv.org/abs/2508.06456v1) | Yonatan Kurniawan, Mingjian Wen et al. | Machine learning interatomic potentials (MLIPs) enable atomistic simulations with near first-principles accuracy at substantially reduced computational cost, making them powerful tools for large-scale materials modeling. The accuracy of MLIPs is typically validated on a held-out dataset of \emph{ab initio} energies and atomic forces. However, accuracy on these small-scale properties does not guarantee reliability for emergent, system-level behavior -- precisely the regime where atomistic simulations are most needed, but for which direct validation is often computationally prohibitive. As a practical heuristic, predictive precision -- quantified as inverse uncertainty -- is commonly used as a proxy for accuracy, but its reliability remains poorly understood, particularly for system-level predictions. In this work, we systematically assess the relationship between predictive precision and accuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes, focusing on ensemble-based uncertainty quantification methods for neural network potentials, including bootstrap, dropout, random initialization, and snapshot ensembles. We use held-out cross-validation for ID assessment and calculate cold curve energies and phonon dispersion relations for OOD testing. These evaluations are performed across various carbon allotropes as representative test systems. We find that uncertainty estimates can behave counterintuitively in OOD settings, often plateauing or even decreasing as predictive errors grow. These results highlight fundamental limitations of current uncertainty quantification approaches and underscore the need for caution when using predictive precision as a stand-in for accuracy in large-scale, extrapolative applications. |
| 2025-08-08 | [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](http://arxiv.org/abs/2508.06452v1) | Mattia Litrico, Mario Valerio Giuffrida et al. | Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance. |
| 2025-08-08 | [$b \to c$ semileptonic sum rule: Current status and prospects](http://arxiv.org/abs/2508.06322v1) | Motoi Endo, Syuhei Iguro et al. | The $b \to c$ semileptonic sum rules provide relations between the decay rates of $B \to D^{(*)} \tau\bar\nu$ and $\Lambda_b \to \Lambda_c \tau\bar\nu$. Starting from the heavy quark and zero-recoil limits, we revisit the derivation of the sum rule for total decay rates. We then examine deviations from the limits and investigate corrections arising from realistic hadron masses and higher-order contributions to form factors, taking account of uncertainties. We show that these corrections are negligible compared to current experimental uncertainties, indicating that the sum rule is useful for cross-checking experimental consistency and testing the validity of the Standard Model predictions. In future, precise determinations of the form factors particularly for the tensor operator will be necessary to compare the sum rule predictions with $\Lambda_b \to \Lambda_c \tau\bar\nu$ data from the LHCb experiment and the Tera-Z projects. |
| 2025-08-08 | [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](http://arxiv.org/abs/2508.06317v1) | Jian Hu, Zixu Cheng et al. | Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic matching, they often struggle with fine-grained temporal localisation. Recently, Group Relative Policy Optimisation (GRPO) reformulates the inference process as a reinforcement learning task, enabling fine-grained grounding and achieving strong in-domain performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To overcome both problems, we introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce. Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using GRPO rollouts, averages them to form a pseudo label, and estimates confidence from the variance across these rollouts. This confidence then weights the training rewards, guiding the model to focus on reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. Codes will be released once published. |
| 2025-08-08 | [A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions](http://arxiv.org/abs/2508.06303v1) | Gerhard Kirsten, Bilgesu Bilgin et al. | Computing with discrete representations of high-dimensional probability distributions is fundamental to uncertainty quantification, Bayesian inference, and stochastic modeling. However, storing and manipulating such distributions suffers from the curse of dimensionality, as memory and computational costs grow exponentially with dimension. Monte Carlo methods require thousands to billions of samples, incurring high computational costs and producing inconsistent results due to stochasticity. We present an efficient tensor train method for performing exact arithmetic operations on discretizations of continuous probability distributions while avoiding exponential growth. Our approach leverages low-rank tensor train decomposition to represent latent random variables compactly using Dirac deltas, enabling deterministic addition, subtraction and multiplication operations directly in the compressed format. We develop an efficient implementation using sparse matrices and specialized data structures that further enhances performance. Theoretical analysis demonstrates polynomial scaling of memory and computational complexity under rank assumptions, and shows how statistics of latent variables can be computed with polynomial complexity. Numerical experiments spanning randomized linear algebra to stochastic differential equations demonstrate orders-of-magnitude improvements in memory usage and computational time compared to conventional approaches, enabling tractable deterministic computations on discretized random variables in previously intractable dimensions. |
| 2025-08-08 | [Situationally-aware Path Planning Exploiting 3D Scene Graphs](http://arxiv.org/abs/2508.06283v1) | Saad Ejaz, Marco Giberna et al. | 3D Scene Graphs integrate both metric and semantic information, yet their structure remains underutilized for improving path planning efficiency and interpretability. In this work, we present S-Path, a situationally-aware path planner that leverages the metric-semantic structure of indoor 3D Scene Graphs to significantly enhance planning efficiency. S-Path follows a two-stage process: it first performs a search over a semantic graph derived from the scene graph to yield a human-understandable high-level path. This also identifies relevant regions for planning, which later allows the decomposition of the problem into smaller, independent subproblems that can be solved in parallel. We also introduce a replanning mechanism that, in the event of an infeasible path, reuses information from previously solved subproblems to update semantic heuristics and prioritize reuse to further improve the efficiency of future planning attempts. Extensive experiments on both real-world and simulated environments show that S-Path achieves average reductions of 5.7x in planning time while maintaining comparable path optimality to classical sampling-based planners and surpassing them in complex scenarios, making it an efficient and interpretable path planner for environments represented by indoor 3D Scene Graphs. |
| 2025-08-08 | [Thermodynamic uncertainty relation for feedback cooling](http://arxiv.org/abs/2508.06174v1) | Kousuke Kumasaki, Kaito Tojo et al. | Feedback cooling enables a system to achieve low temperatures through measurement-based control. Determining the thermodynamic cost required to achieve the ideal cooling efficiency within a finite time remains an important problem. In this work, we establish a thermodynamic uncertainty relation (TUR) for feedback cooling in classical underdamped Langevin systems, thereby deriving a trade-off between the cooling efficiency and the entropy reduction rate. The obtained TUR implies that simultaneous achievement of the ideal cooling efficiency and finite entropy reduction rate is asymptotically possible by letting the fluctuation of the reversible local mean velocity diverge. This is shown to be feasible by using a feedback control based on the Kalman filter. Our results clarify the thermodynamic costs of achieving the fundamental cooling limit of feedback control from the perspective of the TUR. |
| 2025-08-08 | [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](http://arxiv.org/abs/2508.06169v1) | Wenpeng Xing, Jie Chen et al. | Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts. |
| 2025-08-08 | [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](http://arxiv.org/abs/2508.06145v1) | Byeonghun Bang, Jongsuk Yoon et al. | The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information. |
| 2025-08-07 | [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](http://arxiv.org/abs/2508.05634v1) | Jianpeng Yao, Xiaopan Zhang et al. | Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/. |
| 2025-08-07 | [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](http://arxiv.org/abs/2508.05544v1) | Guang Yang, Xinyang Liu | Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications. |
| 2025-08-07 | [Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy](http://arxiv.org/abs/2508.05466v1) | Yun Li, Jicheng Shi et al. | This paper studies the finite-horizon robust optimal control of linear systems subject to model mismatch and additive stochastic disturbances. Utilizing the system level synthesis (SLS) parameterization, we propose a novel SLS design using output-feedback affine control policy and extend it to a distributionally robust setting to improve system resilience by minimizing the cost function while ensuring constraint satisfaction against the worst-case uncertainty distribution. The scopes of model mismatch and stochastic disturbances are quantified using the 1-norm and a Wasserstein metric-based ambiguity set, respectively. For the closed-loop dynamics, we analyze the distributional shift between the predicted output-input response -- computed using nominal parameters and empirical disturbance samples -- and the actual closed-loop distribution, highlighting its dependence on model mismatch and SLS parameterization. Assuming convex and Lipschitz continuous cost functions and constraints, we derive a tractable reformulation of the distributionally robust SLS (DR-SLS) problem by leveraging tools from robust control and distributionally robust optimization (DRO). Numerical experiments validate the performance and robustness of the proposed approach. |
| 2025-08-07 | [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](http://arxiv.org/abs/2508.05454v1) | Wei Li, Zixin Wang et al. | Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field. |
| 2025-08-07 | [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](http://arxiv.org/abs/2508.05441v1) | Zuyuan Zhang, Arnob Ghosh et al. | Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\alpha$ achieves explicit tail-risk control over the expected loss in the "worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability. |
| 2025-08-07 | [LLM-based Multi-Agent Copilot for Quantum Sensor](http://arxiv.org/abs/2508.05421v1) | Rong Sha, Binglin Wang et al. | Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\rm{8}}$ sub-$\rm{\mu}$K atoms without any human intervention within a few hours, representing $\sim$100$\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems. |
| 2025-08-07 | [Metallicity of Active Galactic Nuclei from ultraviolet and optical emission lines-II. Revisiting the $C43$ metallicity calibration and its implications](http://arxiv.org/abs/2508.05397v1) | O. L. Dors, C. B. Oliveira et al. | In this study, a new semi-empirical calibration is proposed between ultraviolet emission lines (\ion{C}{iii}]$\lambda1909$, \ion{C}{iv}$\lambda1549$, \ion{He}{ii}]$\lambda1640$) of type~2 AGNs and their metallicity ($Z$). This calibration is derived by comparing a large sample of 106 objects (data taken from the literature) located over a wide range of redshifts ($0 \: \lesssim \: z \: \lesssim \: 4.0$) with predictions from photoionization models that adopt a recent C/O-O/H relation derived via estimates using the $T_{\rm e}$ method, which is considered the most reliable method. We found that the new calibration produces $Z$ values in agreement (within an uncertainty of $\pm 0.1$ dex) with those from other calibrations and from estimates via the $T_{\rm e}$-method. We find also that AGN metallicities are already high at early epochs, with no evidence for monotonic evolution across the redshift range $0 \: \lesssim \: z \: \lesssim \: 12$. Notably, the highest metallicities in our sample, reaching up to $\rm 4\: Z_{\odot}$, are found in objects at $2 \lesssim z \lesssim 3$. This redshift range coincides with the peak of the cosmic star formation rate history, suggesting a strong connection between the major epoch of star formation, black hole growth, and rapid metal enrichment in the host galaxies of AGNs. Furthermore, our analysis reveals no significant correlation between AGN metallicity and radio properties (radio spectral index or radio luminosity) or host galaxy stellar mass. The lack of a clear mass-metallicity relation, consistent with findings for local AGNs, suggests that the chemical evolution of the nuclear gas is decoupled from the global properties of the host galaxy. |
| 2025-08-07 | [Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games](http://arxiv.org/abs/2508.05378v1) | Zhisen Jiang, Saverio Bolognani et al. | The integration of distributed energy resources into transmission grid operations presents a complex challenge, particularly in the context of reactive power procurement for voltage support. This paper addresses this challenge by formulating the voltage regulation problem as a Stackelberg game, where the Transmission System Operator (TSO) designs incentives to guide the reactive power responses of Distribution System Operators (DSOs). We utilize a gradient-based iterative algorithm that updates the incentives to ensure that DSOs adjust their reactive power injections to maintain voltage stability. We incorporate principles from online feedback optimization to enable real-time implementation, utilizing voltage measurements in both TSO's and DSOs' policies. This approach not only enhances the robustness against model uncertainties and changing operating conditions but also facilitates the co-design of incentives and automation. Numerical experiments on a 5-bus transmission grid demonstrate the effectiveness of our approach in achieving voltage regulation while accommodating the strategic interactions of self-interested DSOs. |
| 2025-08-07 | [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](http://arxiv.org/abs/2508.05342v1) | Shunlei Li, Longsen Gao et al. | Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. |
| 2025-08-07 | [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](http://arxiv.org/abs/2508.05310v1) | Jelle Luijkx, Zlatan AjanoviÄ‡ et al. | Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io. |
| 2025-08-06 | [Occupancy Learning with Spatiotemporal Memory](http://arxiv.org/abs/2508.04705v1) | Ziyang Leng, Jiawei Yang et al. | 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%. |
| 2025-08-06 | [Open Scene Graphs for Open-World Object-Goal Navigation](http://arxiv.org/abs/2508.04678v1) | Joel Loo, Zhanxin Wu et al. | How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments. |
| 2025-08-06 | [Stochastic Calculus for Pathwise Observables of Markov-Jump Processes: Unification of Diffusion and Jump Dynamics](http://arxiv.org/abs/2508.04647v1) | Lars TorbjÃ¸rn Stutzer, Cai Dieball et al. | Path-wise observables--functionals of stochastic trajectories--are at the heart of time-average statistical mechanics and are central to thermodynamic inequalities such as uncertainty relations, speed limits, and correlation-bounds. They provide a means of thermodynamic inference in the typical situation, when not all dissipative degrees of freedom in a system are experimentally accessible. So far, theories focusing on path-wise observables have been developing in two major directions, diffusion processes and Markov-jump dynamics, in a virtually disjoint manner. Moreover, even the respective results for diffusion and jump dynamics were derived with a patchwork of different approaches that are predominantly indirect. Stochastic calculus was recently shown to provide a direct approach to path-wise observables of diffusion processes, while a corresponding framework for jump dynamics remained elusive. In our work we develop, in an exact parallelism with continuous-space diffusion, a complete stochastic calculus for path-wise observables of Markov-jump processes. We formulate a "Langevin equation" for jump processes, define general path-wise observables, and establish their covariation structure, whereby we fully account for transients and time-inhomogeneous dynamics. We prove the known kinds of thermodynamic inequalities in their most general form and discus saturation conditions. We determine the response of path-wise observables to general (incl. thermal) perturbations and carry out the continuum limit to achieve the complete unification of diffusion and jump dynamics. Our results open new avenues in the direction of discrete-state analogs of generative diffusion models and the learning of stochastic thermodynamics from fluctuating trajectories. |
| 2025-08-06 | [Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials](http://arxiv.org/abs/2508.04633v1) | James P. Long, Abhishikta Roy et al. | In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model. |
| 2025-08-06 | [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](http://arxiv.org/abs/2508.04588v1) | Nicola Casali, Alessandro Brusaferri et al. | Accurate estimation of intravoxel incoherent motion (IVIM) parameters from diffusion-weighted MRI remains challenging due to the ill-posed nature of the inverse problem and high sensitivity to noise, particularly in the perfusion compartment. In this work, we propose a probabilistic deep learning framework based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling estimation of total predictive uncertainty and decomposition into aleatoric (AU) and epistemic (EU) components. The method was benchmarked against non probabilistic neural networks, a Bayesian fitting approach and a probabilistic network with single Gaussian parametrization. Supervised training was performed on synthetic data, and evaluation was conducted on both simulated and two in vivo datasets. The reliability of the quantified uncertainties was assessed using calibration curves, output distribution sharpness, and the Continuous Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper predictive distributions for the D and f parameters, although slight overconfidence was observed in D*. The Robust Coefficient of Variation (RCV) indicated smoother in vivo estimates for D* with MDNs compared to Gaussian model. Despite the training data covering the expected physiological range, elevated EU in vivo suggests a mismatch with real acquisition conditions, highlighting the importance of incorporating EU, which was allowed by DE. Overall, we present a comprehensive framework for IVIM fitting with uncertainty quantification, which enables the identification and interpretation of unreliable estimates. The proposed approach can also be adopted for fitting other physical models through appropriate architectural and simulation adjustments. |
| 2025-08-06 | [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](http://arxiv.org/abs/2508.04570v1) | A. Tarik Leblebici, Sumeyra Hassan et al. | This paper proposes a joint communication and indoor positioning (JCP) system based on visible light communication (VLC) designed for high-precision indoor environments. The framework supports 2D and 3D positioning using received signal strength (RSS) from pilot transmissions, enhanced by the radical axis theorem to improve accuracy under measurement uncertainties. Communication is achieved using spatial modulation (SM) with M-ary pulse amplitude modulation (PAM), where data is conveyed through the modulation symbol and the active light-emitting diode (LED) index, improving spectral efficiency while maintaining low complexity. A pilot-aided least squares (LS) estimator is employed for joint channel and dimming coefficient estimation, enabling robust symbol detection in multipath environments characterized by both line-of-sight (LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician fading. The proposed system incorporates a dimming control mechanism to meet lighting requirements while maintaining reliable communication and positioning performance. Simulation results demonstrate sub-centimeter localization accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below 10^{-6} for low-order PAM schemes. Additionally, comparative analysis across user locations reveals that positioning and communication performance improve significantly near the geometric center of the LED layout. These findings validate the effectiveness of the proposed system for future 6G indoor networks requiring integrated localization and communication under practical channel conditions. |
| 2025-08-06 | [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](http://arxiv.org/abs/2508.04537v1) | Alkesh K. Srivastava, Aamodh Suresh et al. | We address the challenge of multi-robot autonomous hazard mapping in high-risk, failure-prone, communication-denied environments such as post-disaster zones, underground mines, caves, and planetary surfaces. In these missions, robots must explore and map hazards while minimizing the risk of failure due to environmental threats or hardware limitations. We introduce a behavior-adaptive, information-theoretic planning framework for multi-robot teams grounded in the concept of Behavioral Entropy (BE), that generalizes Shannon entropy (SE) to capture diverse human-like uncertainty evaluations. Building on this formulation, we propose the Behavior-Adaptive Path Planning (BAPP) framework, which modulates information gathering strategies via a tunable risk-sensitivity parameter, and present two planning algorithms: BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for safe deployment under high risk. We provide theoretical insights on the informativeness of the proposed BAPP framework and validate its effectiveness through both single-robot and multi-robot simulations. Our results show that the BAPP stack consistently outperforms Shannon-based and random strategies: BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot survivability with minimal loss in information gain. In multi-agent deployments, BAPP scales effectively through spatial partitioning, mobile base relocation, and role-aware heterogeneity. These findings underscore the value of behavior-adaptive planning for robust, risk-sensitive exploration in complex, failure-prone environments. |
| 2025-08-06 | [Metric Learning in an RKHS](http://arxiv.org/abs/2508.04476v1) | Gokcan Tatli, Yi Chen et al. | Metric learning from a set of triplet comparisons in the form of "Do you think item h is more similar to item i or item j?", indicating similarity and differences between items, plays a key role in various applications including image retrieval, recommendation systems, and cognitive psychology. The goal is to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric learning using kernel methods and neural networks have shown great empirical promise. While previous works have addressed certain aspects of this problem, there is little or no theoretical understanding of such methods. The exception is the special (linear) case in which the RKHS is the standard Euclidean space $\mathbb{R}^d$; there is a comprehensive theory for metric learning in $\mathbb{R}^d$. This paper develops a general RKHS framework for metric learning and provides novel generalization guarantees and sample complexity bounds. We validate our findings through a set of simulations and experiments on real datasets. Our code is publicly available at https://github.com/RamyaLab/metric-learning-RKHS. |
| 2025-08-06 | [Case Studies of Generative Machine Learning Models for Dynamical Systems](http://arxiv.org/abs/2508.04459v1) | Nachiket U. Bapat, Randy C. Paffenroth et al. | Systems like aircraft and spacecraft are expensive to operate in the real world. The design, validation, and testing for such systems therefore relies on a combination of mathematical modeling, abundant numerical simulations, and a relatively small set of real-world experiments. Due to modeling errors, simplifications, and uncertainties, the data synthesized by simulation models often does not match data from the system's real-world operation. We consider the broad research question of whether this model mismatch can be significantly reduced by generative artificial intelligence models (GAIMs). Unlike text- or image-processing, where generative models have attained recent successes, GAIM development for aerospace engineering applications must not only train with scarce operational data, but their outputs must also satisfy governing equations based on natural laws, e.g., conservation laws. The scope of this paper primarily focuses on two case studies of optimally controlled systems that are commonly understood and employed in aircraft guidance, namely: minimum-time navigation in a wind field and minimum-exposure navigation in a threat field. We report GAIMs that are trained with a relatively small set, of the order of a few hundred, of examples and with underlying governing equations. By focusing on optimally controlled systems, we formulate training loss functions based on invariance of the Hamiltonian function along system trajectories. We investigate three GAIM architectures, namely: the generative adversarial network (GAN) and two variants of the variational autoencoder (VAE). We provide architectural details and thorough performance analyses of these models. The main finding is that our new models, especially the VAE-based models, are able to synthesize data that satisfy the governing equations and are statistically similar to the training data despite small volumes of training data. |
| 2025-08-06 | [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](http://arxiv.org/abs/2508.04457v1) | Simon Baur, Wojciech Samek et al. | Reliable uncertainty quantification is crucial for trustworthy decision-making and the deployment of AI models in medical imaging. While prior work has explored the ability of neural networks to quantify predictive, epistemic, and aleatoric uncertainties using an information-theoretical approach in synthetic or well defined data settings like natural image classification, its applicability to real life medical diagnosis tasks remains underexplored. In this study, we provide an extensive uncertainty quantification benchmark for multi-label chest X-ray classification using the MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for convolutional (ResNet) and transformer-based (Vision Transformer) architectures across a wide range of tasks. Additionally, we extend Evidential Deep Learning, HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting. Our analysis provides insights into uncertainty estimation effectiveness and the ability to disentangle epistemic and aleatoric uncertainties, revealing method- and architecture-specific strengths and limitations. |
| 2025-08-05 | [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2508.03692v1) | Ao Liang, Youquan Liu et al. | Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community. |
| 2025-08-05 | [La La LiDAR: Large-Scale Layout Generation from LiDAR Data](http://arxiv.org/abs/2508.03691v1) | Youquan Liu, Lingdong Kong et al. | Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation. |
| 2025-08-05 | [Streaming Generated Gaussian Process Experts for Online Learning and Control](http://arxiv.org/abs/2508.03679v1) | Zewen Yang, Dongfa Zhang et al. | Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \underline{s}treaming \underline{k}ernel-induced progressivel\underline{y} generated expert framework of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches. |
| 2025-08-05 | [Theoretical framework for lattice QCD computations of $B\to K \ell^+ \ell^-$ and $\bar{B}_s\to \ell^+\ell^- Î³$ decays rates, including contributions from "Charming Penguins"](http://arxiv.org/abs/2508.03655v1) | R. Frezzotti, G. Gagliardi et al. | We develop a strategy for computing the $B\to K\ell^+\ell^-$ and $\bar{B}_s\to\gamma\ell^+\ell^-$ decay amplitudes using lattice QCD (where $\ell^\pm$ are charged leptons). We focus on those terms which contain complex contributions to the amplitude, due to on-shell intermediate states propagating between the weak operator and electromagnetic current(s). Such terms, which are generally estimated using model calculations and represent significant uncertainties in the phenomenological predictions for these decays, cannot be computed using standard lattice QCD techniques. It has recently been shown that such contributions can be computed using spectral-density methods and our proposed strategy, which we discuss in detail, is built on this approach. The complex contributions include the ``charming penguins" (matrix elements of the current-current operators $O_1^{(c)}$ and $O_2^{(c)}$ defined in Eq. (6) below), in which the charm-quark loop can propagate long distances, particularly close to the region of charmonium resonances. They also include the contributions from the chromomagnetic operator ($O_8$ in standard notation, defined in Eq. (8) below). We discuss the renormalization of the ultra-violet divergences, and in particular those which arise due to ``contact" terms, and explain how those which appear as inverse powers of the lattice spacing can be subtracted non-perturbatively. We apply the spectral density methods in an instructive exploratory computation of the charming penguin diagram in $B\to K\ell^+\ell^-$ decays in which the virtual photon is emitted from the charm-quark loop (the diagram in Fig. 1(a) below) and discuss the prospects and strategies for the reliable determination of the amplitudes in future dedicated computations. |
| 2025-08-05 | [RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data](http://arxiv.org/abs/2508.03578v1) | Jonas Leo Mueller, Lukas Engel et al. | Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications. |
| 2025-08-05 | [Machine learning the single-$Î›$ hypernuclei with neural-network quantum states](http://arxiv.org/abs/2508.03575v1) | Zi-Xiao Zhang, Yi-Long Yang et al. | Single-$\Lambda$ hypernuclei are the most straightforward extension of atomic nuclei. A thorough description of baryonic system beyond first-generation quark sector is indispensable for the maturation of nuclear $ab$ $initio$ methods. This study pioneers the application of neural-network quantum states to hypernuclei, with trainable parameters determined by variational Monte Carlo approach (VMC-NQS). In order to reduce the numerical uncertainty and treat the nucleons and hyperons in a unified manner, spinor grouping (SG) method is proposed to analytically integrate out isospin degrees of freedom. A novel spin purification scheme is developed to address the severe spin contamination occurring in standard energy minimization due to the weakly bound characteristic of light single-$\Lambda$ hypernuclei. The energy spectrum of $s$-shell hypernuclei is computed with one-thousandth level accuracy and benchmarked against existing stochastic variational results, showing superior performance. By comparing two different sets of Hamiltonian based on pionless effective field theory (pionless EFT), we choose an optimal model and further carry out calculations of selected $p$-shell charge-symmetric hypernuclei with mass number up to 13, exhibiting satisfactory consistency with experimental results. Our findings underscore the potential of VMC-NQS family in approaching exact solution of few-body systems and the accuracy of pionless EFT in modeling hypernuclei. This is crucial for understanding hyperon-nucleon-nucleon and hyperon-hyperon-nucleon interactions, providing a powerful tool for precisely predicting the properties of multi-strangeness hypernuclei. |
| 2025-08-05 | [Regulator and gauge dependence of the Abelian gauge coupling in asymptotically safe quantum gravity](http://arxiv.org/abs/2508.03563v1) | Maksym Riabokon, Marc Schiffer et al. | Both General Relativity and the Standard Model of particle physics are not UV complete. General Relativity is perturbatively non-renormalizable, while the Standard Model features Landau poles, where couplings are predicted to diverge at finite energies, e.g., in the Abelian gauge sector. Asymptotically safe quantum gravity may resolve both of these issues at the same time. In this paper, we assess the systematic uncertainties associated with this scenario, in particular with the gravitationally induced UV-completion of the Abelian gauge sector. Specifically, we study the dependence of this qualitative feature, namely the existence of a UV-complete gauge sector, on unphysical choices like the gauge, and the regulator function. Intriguingly, in some scenarios, we find simultaneous points of minimal sensitivity relative to both the regulator and gauge parameters, which allow for a UV completion. This provides further indications that the simultaneous UV-completion of quantum gravity and matter via an asymptotically safe fixed point is a robust physical feature, and that physical quantities, like scaling exponents, can become independent of unphysical choices. |
| 2025-08-05 | [An Evolutionary Analysis of Narrative Selection](http://arxiv.org/abs/2508.03540v1) | Federico Innocenti, Roberto Rozzi | We study the performance of different methods for processing information, incorporating narrative selection within an evolutionary model. All agents update their beliefs according to Bayes' Rule, but some strategically choose the narrative they use in updating according to heterogeneous criteria. We simulate the endogenous composition of the population, considering different laws of motion for the underlying state of the world. We find that conformists -- that is, agents that choose the narrative to conform to the average belief in the population -- have an evolutionary advantage over other agents across all specifications. The survival chances of the remaining types depend on the uncertainty regarding the state of the world. Agents who tend to develop mild beliefs perform better when the uncertainty is high, whereas agents who tend to develop extreme beliefs perform better when the uncertainty is low. |
| 2025-08-05 | [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](http://arxiv.org/abs/2508.03520v1) | Md Rakibul Hasan, Md Zakir Hossain et al. | Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems. |
| 2025-08-05 | [MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation](http://arxiv.org/abs/2508.03511v1) | Yazhou Zhu, Haofeng Zhang | Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: https://github.com/YazhouZhu19/MAUP. |
| 2025-08-04 | [DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting](http://arxiv.org/abs/2508.02616v1) | Ali Forootani, Mohammad Khosravi et al. | Time series forecasting plays a vital role across scientific, industrial, and environmental domains, especially when dealing with high-dimensional and nonlinear systems. While Transformer-based models have recently achieved state-of-the-art performance in long-range forecasting, they often suffer from interpretability issues and instability in the presence of noise or dynamical uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting framework that combines the representational power of Transformers with the theoretical rigor of Koopman operator theory. Our model features a modular encoder-propagator-decoder structure, where temporal dynamics are learned via a spectrally constrained, linear Koopman operator in a latent space. We impose structural guarantees-such as bounded spectral radius, Lyapunov based energy regularization, and orthogonal parameterization to ensure stability and interpretability. Comprehensive evaluations are conducted on both synthetic dynamical systems, real-world climate dataset (wind speed and surface pressure), financial time series (cryptocurrency), and electricity generation dataset using the Python package that is prepared for this purpose. Across all experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline Transformer models in terms of accuracy, robustness to noise, and long-term forecasting stability. These results establish DeepKoopFormer as a flexible, interpretable, and robust framework for forecasting in high dimensional and dynamical settings. |
| 2025-08-04 | [Quark mass corrections in di-Higgs production amplitude at high-energy](http://arxiv.org/abs/2508.02589v1) | Sebastian Jaskiewicz | A large theoretical uncertainty due to the choice of the top-quark mass renormalisation scheme is present in QCD predictions for Higgs boson pair production. In these proceedings, we report on the recent progress in tackling these uncertainties for the $gg\to HH$ amplitude in the high-energy limit $s,|t|,|u| \gg m_t^2 \gg m_H^2$. Using the Method of Regions and Soft-Collinear Effective Theory, the leading power in $m_t$ behaviour of the amplitude is understood to all orders in the strong coupling expansion, and leading logarithmic resummation leads to a significant reduction in the scheme choice uncertainty in the virtual amplitude for di-Higgs production at high energies. |
| 2025-08-04 | [Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification](http://arxiv.org/abs/2508.02566v1) | Javier Fumanal-Idocin, Raquel Fernandez-Peralta et al. | Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. Unlike classical methods that apply a uniform feature set, DFS customizes feature selection per sample, providing insight into the decision-making process for each case. DFS is especially significant in settings where decision transparency is key, i.e., clinical decisions; however, existing methods use opaque models, which hinder their applicability in real-life scenarios. This paper introduces a novel approach leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. We also show how this method provides a quantitative measure of uncertainty for each feature query and can make the feature selection process computationally lighter by constraining the feature search space. We also discuss when greedy selection of conditional mutual information is equivalent to selecting features that minimize the difference with respect to the global model predictions. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and RL methods, which are mostly considered opaque, compared to our explainable rule-based system. |
| 2025-08-04 | [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](http://arxiv.org/abs/2508.02528v1) | Jingsong Liu, Xiaofeng Deng et al. | Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis. |
| 2025-08-04 | [Quantitative and Predictive Folding Models from Limited Single-Molecule Data Using Simulation-Based Inference](http://arxiv.org/abs/2508.02509v1) | Lars Dingeldein, Aaron Lyons et al. | The study of biomolecular folding has been greatly advanced by single-molecule force spectroscopy (SMFS), which enables the observation of the dynamics of individual molecules. However, extracting quantitative models of fundamental properties such as folding landscapes from SNFS data is very challenging due to instrumental noise, linker artifacts, and the inherent stochasticity of the process, often requiring extensive datasets and complex calibration experiments. Here, we introduce a framework based on simulation-based inference (SBI) that overcomes these limitations by integrating physics-based modeling with deep learning. We apply this framework to analyze constant-force measurements of a DNA hairpin. From a single, short experimental trajectory of only two seconds, we successfully reconstruct the hairpin's free energy landscape and folding dynamics, obtaining results that are in close agreement with established deconvolution methods that require approximately 100 times more data. Furthermore, the Bayesian nature of this approach robustly quantifies uncertainties for inferred parameter values, including the free-energy profile, diffusion coefficients, and linker stiffness, without needing independent measurements of instrumental properties. The inferred model is predictive, generating simulated trajectories that quantitatively reproduce the thermodynamic and kinetic properties of the experimental data. This work establishes SBI as a highly efficient and powerful tool for analyzing single-molecule experiments. The ability to derive statistically robust models from minimal datasets is crucial for investigating complex biomolecular systems where extensive data collection is impractical or impossible. Consequently, our SBI framework enables the rigorous quantitative analysis of previously intractable biomolecular systems, paving the way for novel applications of SMFS. |
| 2025-08-04 | [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](http://arxiv.org/abs/2508.02503v1) | Maxime Bouscary, Saurabh Amin | LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\% to 92\% on the most complex problems. |
| 2025-08-04 | [Pre-discovery TESS Observations of Interstellar Object 3I/ATLAS](http://arxiv.org/abs/2508.02499v1) | Jorge Martinez-Palomera, Amy Tuson et al. | 3I/ATLAS, also known as C/2025 N1 (ATLAS), is the third known interstellar object to pass through our Solar System. We report serendipitous Transiting Exoplanet Survey Satellite (TESS) observations of 3I/ATLAS taken between 2025-05-07 and 2025-06-02,, 55 days prior to the discovery date (2025-07-01) and 14 days prior to the current earliest observation (2025-05-21). We retrieve the TESS pixel data, perform a robust background correction and use a data-driven approach to refine the object's ephemeris. We find a statistically significant offset between the target's observed and predicted positions and we show that this is dominated by uncertainty in the TESS World Coordinate System (WCS) rather than the ephemeris. 3I/ATLAS is too faint to be detected in the individual 200\,second TESS integrations, so we perform image stacking to improve detectability. After co-adding the TESS image data, we performed aperture and Pixel Response Function (PRF) photometry to create two light curves for 3I/ATLAS. Each light curve consists of 15 measurements with $\text{SNR}>3$, collected across two different TESS cameras during the 26\,days that the object was observed, but the PRF light curve is more robust against image noise. The PRF light curve in the TESS bandpass shows a gradual increase in brightness from $T_{\text{mag}} = 20.9 \pm 0.29$ to $T_{\text{mag}} = 19.57 \pm 0.15$. This is expected as 3I/ATLAS approaches the inner Solar System. This paper highlights the power of using TESS for Solar System science; by increasing the photometric observing baseline, future studies will be able to investigate the long-term behavior of 3I/ATLAS |
| 2025-08-04 | [Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning](http://arxiv.org/abs/2508.02495v1) | Kunyu Zhang, Lin Gu et al. | Many previous studies have proposed extracting image labels from clinical notes to create large-scale medical image datasets at a low cost. However, these approaches inherently suffer from label noise due to uncertainty from the clinical experts. When radiologists and physicians analyze medical images to make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or ``not excluded''. Unfortunately, current text-mining methods overlook these nuances, resulting in the creation of noisy labels. Existing methods for handling noisy labels in medical image analysis, which typically address the problem through post-processing techniques, have largely ignored the important issue of expert-driven uncertainty contributing to label noise. To better incorporate the expert-written uncertainty in clinical notes into medical image analysis and address the label noise issue, we first examine the impact of clinical expert uncertainty on label noise. We then propose a clinical expert uncertainty-aware benchmark, along with a label smoothing method, which significantly improves performance compared to current state-of-the-art approaches. |
| 2025-08-04 | [Uncertainty-Aware Perception-Based Control for Autonomous Racing](http://arxiv.org/abs/2508.02494v1) | Jelena Trisovic, Andrea Carron et al. | Autonomous systems operating in unknown environments often rely heavily on visual sensor data, yet making safe and informed control decisions based on these measurements remains a significant challenge. To facilitate the integration of perception and control in autonomous vehicles, we propose a novel perception-based control approach that incorporates road estimation, quantification of its uncertainty, and uncertainty-aware control based on this estimate. At the core of our method is a parametric road curvature model, optimized using visual measurements of the road through a constrained nonlinear optimization problem. This process ensures adherence to constraints on both model parameters and curvature. By leveraging the Frenet frame formulation, we embed the estimated track curvature into the system dynamics, allowing the controller to explicitly account for perception uncertainty and enhancing robustness to estimation errors based on visual input. We validate our approach in a simulated environment, using a high-fidelity 3D rendering engine, and demonstrate its effectiveness in achieving reliable and uncertainty-aware control for autonomous racing. |
| 2025-08-04 | [Gauge theory approach to describe ice crystals habit evolution in ice clouds](http://arxiv.org/abs/2508.02472v1) | Gianluca Di Natale, Francesco Pio De Cosmo et al. | Ice clouds, particularly cirrus clouds, significantly influence Earth's radiative balance but remain poorly characterized in current climate models. A major uncertainty arises from the variability of their microphysical properties, especially the evolution of ice crystal habits under depositional growth. We propose a heuristic method to describe habit evolution based on four fundamental shapes identified in the literature and from in situ observations: droxtals, plates, columns, and rosettes. These represent the primary forms that are relevant under depositional growth, excluding aggregation. In this study, we employ a non-Abelian gauge theory within a field-theoretical framework, imposing an SU(2) $\otimes$ U(1) symmetry on the fields associated with each habit probability growth. This symmetry enables the derivation of a modified system of coupled Fokker-Planck equations, capturing the stochastic growth dynamics of ice crystals while incorporating phenomenological mutual influences among habits. This framework outlines a novel theoretical direction for integrating symmetry principles and field-theoretical tools into the modelling of habit dynamics in ice clouds. |
| 2025-08-01 | [Numerical Uncertainty in Linear Registration: An Experimental Study](http://arxiv.org/abs/2508.00781v1) | Niusha Mirhakimi, Yohan Chatelain et al. | While linear registration is a critical step in MRI preprocessing pipelines, its numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA) simulations, we assessed the most commonly used linear registration tools within major software packages (SPM, FSL, and ANTs) across multiple image similarity measures, two brain templates, and both healthy control (HC, n=50) and Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the influence of linear registration tools and similarity measures on numerical stability. Among the evaluated tools and with default similarity measures, SPM exhibited the highest stability. FSL and ANTs showed greater and similar ranges of variability, with ANTs demonstrating particular sensitivity to numerical perturbations that occasionally led to registration failure. Furthermore, no significant differences were observed between healthy and PD cohorts, suggesting that numerical stability analyses obtained with healthy subjects may generalise to clinical populations. Finally, we also demonstrated how numerical uncertainty measures may support automated quality control (QC) of linear registration results. Overall, our experimental results characterize the numerical stability of linear registration experimentally and can serve as a basis for future uncertainty analyses. |
| 2025-08-01 | [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](http://arxiv.org/abs/2508.00754v1) | Yaxin Ma, Benjamin Colburn et al. | Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models. |
| 2025-08-01 | [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](http://arxiv.org/abs/2508.00750v1) | Prerana Ramkumar | Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications. |
| 2025-08-01 | [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](http://arxiv.org/abs/2508.00734v1) | Liuyun Xu, Seymour M. J. Spence | Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches. |
| 2025-08-01 | [Efficient Solution and Learning of Robust Factored MDPs](http://arxiv.org/abs/2508.00707v1) | Yannik Schnitzer, Alessandro Abate et al. | Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods. |
| 2025-08-01 | [Chemical abundances of seven stars in the GD-1 stream](http://arxiv.org/abs/2508.00671v1) | Jing-Kun Zhao, Guang-Wei Li et al. | We present the first detailed chemical abundances for seven GD-1 stream stars from Subaru/HDS spectroscopy. Atmospheric parameters were derived via color calibrations ($T\rm_{eff}$) and iterative spectroscopic analysis. LTE abundances for 14 elements ($\alpha$, odd-Z, iron-peak, n-capture) were measured. Six stars trace the main orbit, one resides in a `blob'. All exhibit tightly clustered metallicities ([Fe/H] = -2.38, {\bf intrinsic dispersion smaller than 0.05 dex, average uncertainty is about 0.13 dex}). While one star shows binary mass transfer signatures, the other six display consistent abundance patterns (dispersions $<$ uncertainties). Their iron-peak elements (Sc, Cr, Mn, Ni) match Milky Way halo stars. In contrast, Y and Sr are systematically lower than halo stars of similar [Fe/H]. Significantly, six stars show consistently enhanced [Eu/Fe] $\sim$ 0.60 ($\sigma$ = 0.08). A tight Ba-Eu correlation (r = 0.83, p=0.04) exists, with [Ba/Fe] = -0.03 $\pm$ 0.05, indicating a common r-process origin. This extreme chemical homogeneity strongly supports an origin from a single disrupted globular cluster. The lack of light-element anti-correlations may stem from our sample size or the progenitor's low mass. |
| 2025-08-01 | [Uncertainty Relation for Pseudo-Hermitian Quantum Systems](http://arxiv.org/abs/2508.00648v1) | Boubakeur Khantoul, Bilel Hamil et al. | This study investigates pseudo-Hermitian quantum mechanics, where the Hamiltonian satisfies a modified Hermiticity condition. We extend the uncertainty relation for such systems, demonstrating its equivalence to the standard Hermitian case within a pseudo-Hermitian inner product. Analytical solutions to the time-dependent Schr\"odinger equation with a linearly evolving potential are derived. Furthermore, we show that the uncertainty relation for position and momentum remains real and greater than 1/2, highlighting the significance of non-Hermitian systems in quantum mechanics. |
| 2025-08-01 | [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](http://arxiv.org/abs/2508.00643v1) | Albert Matveev, Sanmitra Ghosh et al. | Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification. |
| 2025-08-01 | [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](http://arxiv.org/abs/2508.00600v1) | Mingruo Yuan, Shuyi Zhang et al. | Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines. |
| 2025-08-01 | [Output-recurrent gated state space model for multiphase flows modeling and uncertainty quantification of exhaust vehicles](http://arxiv.org/abs/2508.00588v1) | Ruilin Chen, Ming Fang et al. | This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for complex multiphase flows modeling and uncertainty quantification of exhaust vehicles during motion. By establishing the state-space formulation of the gas-liquid Navier-Stokes equations applying semigroup theory and Galerkin projection, explicitly characterizing the dynamic coupling evolution between the velocity, pressure, and volume fraction fields. A novel Gated State Space Transition (GSST) unit is designed to learn parameterized transition and input matrices with adaptive timescales, enhancing physical interpretability and computational efficiency. The output recursion mechanism aligns with the numerical solution characteristics of state-space equations, mitigating long-term error accumulation and addressing training-inference pattern mismatch issues inherent in teacher forcing and scheduled sampling. Validations on the underwater cone-head and water-exit hemisphere-head vehicles demonstrate that: OR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and computational efficiency through its physics-informed adaptive state-space unit design and parallel matrix operations; The output recursion mechanism ensures more stable training, better generalization, and higher prediction accuracy than teacher forcing and scheduled sampling; OR-GSSM accurately captures the gas-phase expansion, gas-liquid mixing formation, backflow jet generation, bubble shedding, and entire water-exit process, etc, showcasing outstanding modeling capability; Its uncertainty quantification effectively characterizes flow features and uncertainty distributions, validating prediction reliability. The proposed method resolves the accuracy-real-time trade-off in traditional computational fluid dynamics, advancing machine learning for multiphase flow modeling and uncertainty quantification in exhaust vehicles. |
| 2025-07-31 | [Formal Bayesian Transfer Learning via the Total Risk Prior](http://arxiv.org/abs/2507.23768v1) | Nathan Wycoff, Ali Arab et al. | In analyses with severe data-limitations, augmenting the target dataset with information from ancillary datasets in the application domain, called source datasets, can lead to significantly improved statistical procedures. However, existing methods for this transfer learning struggle to deal with situations where the source datasets are also limited and not guaranteed to be well-aligned with the target dataset. A typical strategy is to use the empirical loss minimizer on the source data as a prior mean for the target parameters, which places the estimation of source parameters outside of the Bayesian formalism. Our key conceptual contribution is to use a risk minimizer conditional on source parameters instead. This allows us to construct a single joint prior distribution for all parameters from the source datasets as well as the target dataset. As a consequence, we benefit from full Bayesian uncertainty quantification and can perform model averaging via Gibbs sampling over indicator variables governing the inclusion of each source dataset. We show how a particular instantiation of our prior leads to a Bayesian Lasso in a transformed coordinate system and discuss computational techniques to scale our approach to moderately sized datasets. We also demonstrate that recently proposed minimax-frequentist transfer learning techniques may be viewed as an approximate Maximum a Posteriori approach to our model. Finally, we demonstrate superior predictive performance relative to the frequentist baseline on a genetics application, especially when the source data are limited. |
| 2025-07-31 | [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](http://arxiv.org/abs/2507.23756v1) | Diana Mortagua | This study centers on overcoming the challenge of selecting the best annotators for each query in Active Learning (AL), with the objective of minimizing misclassifications. AL recognizes the challenges related to cost and time when acquiring labeled data, and decreases the number of labeled data needed. Nevertheless, there is still the necessity to reduce annotation errors, aiming to be as efficient as possible, to achieve the expected accuracy faster. Most strategies for query-annotator pairs do not consider internal factors that affect productivity, such as mood, attention, motivation, and fatigue levels. This work addresses this gap in the existing literature, by not only considering how the internal factors influence annotators (mood and fatigue levels) but also presenting a new query-annotator pair strategy, using a Knowledge-Based Recommendation System (RS). The RS ranks the available annotators, allowing to choose one or more to label the queried instance using their past accuracy values, and their mood and fatigue levels, as well as information about the instance queried. This work bases itself on existing literature on mood and fatigue influence on human performance, simulating annotators in a realistic manner, and predicting their performance with the RS. The results show that considering past accuracy values, as well as mood and fatigue levels reduces the number of annotation errors made by the annotators, and the uncertainty of the model through its training, when compared to not using internal factors. Accuracy and F1-score values were also better in the proposed approach, despite not being as substantial as the aforementioned. The methodologies and findings presented in this study begin to explore the open challenge of human cognitive factors affecting AL. |
| 2025-07-31 | [DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction](http://arxiv.org/abs/2507.23736v1) | Kyle Naddeo, Nikolas Koutsoubis et al. | Access to medical imaging and associated text data has the potential to drive major advances in healthcare research and patient outcomes. However, the presence of Protected Health Information (PHI) and Personally Identifiable Information (PII) in Digital Imaging and Communications in Medicine (DICOM) files presents a significant barrier to the ethical and secure sharing of imaging datasets. This paper presents a hybrid de-identification framework developed by Impact Business Information Solutions (IBIS) that combines rule-based and AI-driven techniques, and rigorous uncertainty quantification for comprehensive PHI/PII removal from both metadata and pixel data.   Our approach begins with a two-tiered rule-based system targeting explicit and inferred metadata elements, further augmented by a large language model (LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of synthetic datasets simulating realistic clinical PHI/PII. For pixel data, we employ an uncertainty-aware Faster R-CNN model to localize embedded text, extract candidate PHI via Optical Character Recognition (OCR), and apply the NER pipeline for final redaction. Crucially, uncertainty quantification provides confidence measures for AI-based detections to enhance automation reliability and enable informed human-in-the-loop verification to manage residual risks.   This uncertainty-aware deidentification framework achieves robust performance across benchmark datasets and regulatory standards, including DICOM, HIPAA, and TCIA compliance metrics. By combining scalable automation, uncertainty quantification, and rigorous quality assurance, our solution addresses critical challenges in medical data de-identification and supports the secure, ethical, and trustworthy release of imaging data for research. |
| 2025-07-31 | [Distributed AI Agents for Cognitive Underwater Robot Autonomy](http://arxiv.org/abs/2507.23735v1) | Markus Buchholz, Ignacio Carlucho et al. | Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications. |
| 2025-07-31 | [High-resolution eikonal imaging and uncertainty quantification of the Kilauea caldera](http://arxiv.org/abs/2507.23692v1) | Angela F. Gao, John D. Wilding et al. | Images of the Earth's interior can provide us with insight into the underlying properties of the Earth, such as how seismic activity might emerge and the interplay between seismic and volcanic activity. Understanding these systems requires reliable high-resolution images to understand mechanisms and estimate physical quantities. However, reliable images are often difficult to obtain due to the non-linear nature of seismic wave propagation and the ill-posedness of the related inverse problem. Reconstructions rely on good initial estimates as well as hand-crafted priors, which can ultimately bias solutions. In our work, we present a 3D reconstruction of Kilauea's magmatic system at a previously unattained resolution. Our eikonal tomography procedure improves upon prior imaging results of Kilauea through increased resolution and per-pixel uncertainties estimated through variational inference. In particular, solving eikonal imaging using variational inference with stochastic gradient descent enables stable inversion and uncertainty quantification in the absence of strong prior knowledge of the velocity structure. Our work makes two key contributions: developing a stochastic eikonal tomography scheme with uncertainty quantification and illuminating the structure and melt quantity of the magmatic system that underlies Kilauea. |
| 2025-07-31 | [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](http://arxiv.org/abs/2507.23607v1) | Tien Huu Do, Antoine Masquelier et al. | Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models. |
| 2025-07-31 | [Branching ratios and CP asymmetries of $B^0 \to Î·_c f_0$ in the improved perturbative QCD formalism](http://arxiv.org/abs/2507.23578v1) | Min-Qi Li, Xin Liu et al. | Motivated by the idea of fragmented scalar glueball, we investigate the decays $B^0 \to \eta_c f_0$ within the improved perturbative QCD (iPQCD) framework by including the known next-to-leading order corrections. Here, $B^0$ and $f_0$ denote the neutral $B_{d,s}^0$ mesons and the light scalar mesons $f_0(500, 980, 1370, 1500)$ under the $q\bar q$ assignment. The {\it CP}-averaged branching ratios (BRs) and the {\it CP} asymmetries of $B^0 \to \eta_c f_0$ are evaluated with the $f_0(500)[f_0(1370)]-f_0(980)[f_0(1500)]$ mixing in quark-flavor basis. For effective comparisons with the near-future measurements, we further derive the $B^0 \to \eta_c f_0 (\to \pi^+ \pi^-/K^+ K^-)$ BRs under the narrow-width approximation. ${\rm BR}(B_s^0 \to \eta_c f_0(980) (\to \pi^+ \pi^-))= (2.87^{+1.38}_{-1.29}) \times 10^{-4}$ obtained in the iPQCD formalism agrees with the available measurements and predictions within uncertainties. Large BRs of $B_s^0 \to \eta_c f_0(1500) (\to \pi^+ \pi^-/K^+ K^-)$ and large direct {\it CP} asymmetries of $B^0 \to \eta_c f_0(1370, 1500)$ are accessible in the LHCb and Belle-II experiments. The experimental tests of these iPQCD predictions would help us to understand the nature of these light scalars more deeply and provide evidences to decipher $f_0(1500)$ as a primary or fragmented scalar glueball potentially. |
| 2025-07-31 | [Subthreshold parameters of $Ï€Ï€$ scattering revisited](http://arxiv.org/abs/2507.23566v1) | MariÃ¡n KolesÃ¡r, Jaroslav Å˜Ã­ha | Using the most recent experimental data and lattice QCD calculations of $\pi\pi$ scattering lengths, while employing dispersive representations of the amplitude based on Roy equations, we compute the subthreshold parameters of this process. We use Monte Carlo sampling to numerically model the probability distribution of the results based on all uncertainties in the inputs. We also investigate the dependence of the results on a theoretical correlation between the $\pi\pi$ scattering lengths $a^0_0$ and $a^2_0$, which was previously established in the framework of two-flavour chiral perturbation theory. |
| 2025-07-31 | [Latest neutrino results from the FASER experiment and their implications for forward hadron production](http://arxiv.org/abs/2507.23552v1) | FASER Collaboration, Roshan Mammen Abraham et al. | The muon puzzle -- an excess of muons relative to simulation predictions in ultra-high-energy cosmic-ray air showers -- has been reported by many experiments. This suggests that forward particle production in hadronic interactions is not fully understood. Some of the scenarios proposed to resolve this predict reduced production of forward neutral pions and enhanced production of forward kaons (or other particles). The FASER experiment at the LHC is located 480 m downstream of the ATLAS interaction point and is sensitive to neutrinos and muons, which are the decay products of forward charged pions and kaons. In this study, the latest measurements of electron and muon neutrino fluxes are presented using the data corresponding to 9.5 $\mathrm{fb^{-1}}$ and 65.6 $\mathrm{fb^{-1}}$ of proton-proton collisions with $\sqrt{s}=13.6~\mathrm{TeV}$ by the FASER$\nu$ and the FASER electronic detector, respectively. These fluxes are compared with predictions from recent hadronic interaction models, including EPOS-LHCr, SIBYLL 2.3e, and QGSJET 3. The predictions are generally consistent with the measured fluxes from FASER, although some discrepancies appear in certain energy bins. More precise flux measurements with additional data will follow soon, enabling validation of pion, kaon, and charm meson production with finer energy binning, reduced uncertainties, and multi-differential analyses. |
| 2025-07-31 | [A decomposition of Fisher's information to inform sample size for developing or updating fair and precise clinical prediction models -- Part 3: continuous outcomes](http://arxiv.org/abs/2507.23548v1) | Rebecca Whittle, Richard D Riley et al. | Clinical prediction models enable healthcare professionals to estimate individual outcomes using patient characteristics. Current sample size guidelines for developing or updating models with continuous outcomes aim to minimise overfitting and ensure accurate estimation of population-level parameters, but do not explicitly address the precision of predictions. This is a critical limitation, as wide confidence intervals around predictions can undermine clinical utility and fairness, particularly if precision varies across subgroups. We propose methodology for calculating the sample size required to ensure precise and fair predictions in models with continuous outcomes. Building on linear regression theory and the Fisher's unit information matrix, our approach calculates how sample size impacts the epistemic (model-based) uncertainty of predictions and allows researchers to either (i) evaluate whether an existing dataset is sufficiently large, or (ii) determine the sample size needed to target a particular confidence interval width around predictions. The method requires real or synthetic data representing the target population. To assess fairness,the approach can evaluate prediction precision across subgroups. Extensions to prediction intervals are included to additionally address aleatoric uncertainty. Our methodology provides a practical framework for examining required sample sizes when developing or updating prediction models with continuous outcomes, focusing on achieving precise and equitable predictions. It supports the development of more reliable and fair models, enhancing their clinical applicability and trustworthiness. |
| 2025-07-30 | [Solitons, chaos, and quantum phenomena: a deterministic approach to the SchrÃ¶dinger equation](http://arxiv.org/abs/2507.22868v1) | DamiÃ  Gomila | We show that the Schr\"odinger equation describes the ensemble mean dynamics of solitons in a Galilean invariant field theory where we interpret solitons as particles. On a zero background, solitons move classically, following Newton`s second law, however, on a non-zero amplitude chaotic background, their momentum and position fluctuate fulfilling an exact uncertainty relation, which give rise to the emergence of quantum phenomena. The Schrodinger equation for the ensemble of solitons is obtained from this exact uncertainty relation, and the amplitude of the background fluctuations is what corresponds to the value of $\hbar$. We confirm our analytical results running simulations of solitons moving against a potential barrier and comparing the ensemble probabilities with the predictions of the time dependent Schr\"odinger equation, providing a deterministic version of the quantum tunneling effect. We conclude with a discussion of how our theory does not present statistical independence between measurement and experiment outcome. |
| 2025-07-30 | [A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model](http://arxiv.org/abs/2507.22854v1) | Andris Ambainis, Joao F. Doriguello et al. | We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces. |
| 2025-07-30 | [Robust Contract with Career Concerns](http://arxiv.org/abs/2507.22852v1) | Tan Gan, Hongcheng Li | An employer contracts with a worker to incentivize efforts whose productivity depends on ability; the worker then enters a market that pays him contingent on ability evaluation. With non-additive monitoring technology, the interdependence between market expectations and worker efforts can lead to multiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy (1992)). We identify a sufficient and necessary criterion for the employer to face such strategic uncertainty--one linked to skill-effort complementarity, a pervasive feature of labor markets. To fully implement work, the employer optimally creates private wage discrimination to iteratively eliminate pessimistic market expectations and low worker efforts. Our result suggests that present contractual privacy, employers' coordination motives generate within-group pay inequality. The comparative statics further explain several stylized facts about residual wage dispersion. |
| 2025-07-30 | [An Uncertainty Principle for Probabilistic Computation in the Retina](http://arxiv.org/abs/2507.22785v1) | Jayanth R Taranath, Salim M'Jahad | We introduce a probabilistic model of early visual processing, beginning with the interaction between a light wavefront and the retina. We argue that perception originates not with deterministic transduction, but with probabilistic threshold crossings shaped by quantum photon arrival statistics and biological variability. We formalize this with an uncertainty relation, \( \Delta \alpha \cdot \Delta t \geq \eta \), through the transformation of light into symbolic neural code through the layered retinal architecture. Our model is supported by previous experimental results, which show intrinsic variability in retinal responses even under fixed stimuli. We contrast this with a classical null hypothesis of deterministic encoding and propose experiments to further test our uncertainty relation. By re-framing the retina as a probabilistic measurement device, we lay the foundation for future models of cortical dynamics rooted in quantum-like computation. We are not claiming that the brain could be working as a quantum-system, but rather putting forth the argument that the brain as a classical system could still implement quantum-inspired computations. We define quantum-inspired computation as a scheme that includes both probabilistic and time-sensitive computation, clearly separating it from classically implementable probabilistic systems. |
| 2025-07-30 | [A comparison of variable selection methods and predictive models for postoperative bowel surgery complications](http://arxiv.org/abs/2507.22771v1) | Ã–zge Åžahin, Annemiek Kwast et al. | Accurate prediction of postoperative complications can support personalized perioperative care. However, in surgical settings, data collection is often constrained, and identifying which variables to prioritize remains an open question. We analyzed 767 elective bowel surgeries performed under an Enhanced Recovery After Surgery protocol at Medisch Spectrum Twente (Netherlands) between March 2020 and December 2023. Although hundreds of variables were available, most had substantial missingness or near-constant values and were therefore excluded. After data preprocessing, 34 perioperative predictors were selected for further analysis. Surgeries from 2020 to 2022 ($n=580$) formed the development set, and 2023 cases ($n=187$) provided temporal validation. We modeled two binary endpoints: any and serious postoperative complications (Clavien Dindo $\ge$ IIIa). We compared weighted logistic regression, stratified random forests, and Naive Bayes under class imbalance (serious complication rate $\approx$11\%; any complication rate $\approx$35\%). Probabilistic performance was assessed using class-specific Brier scores. We advocate reporting probabilistic risk estimates to guide monitoring based on uncertainty. Random forests yielded better calibration across outcomes. Variable selection modestly improved weighted logistic regression and Naive Bayes but had minimal effect on random forests. Despite single-center data, our findings underscore the value of careful preprocessing and ensemble methods in perioperative risk modeling. |
| 2025-07-30 | [Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models](http://arxiv.org/abs/2507.22766v1) | Felix Kronenwett, Georg Maier et al. | Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters. |
| 2025-07-30 | [Pionic gluons from global QCD analysis of experimental and lattice data](http://arxiv.org/abs/2507.22730v1) | William Good, Patrick C. Barry et al. | We perform the first global QCD analysis of parton distribution functions (PDFs) in the pion, with lattice-QCD data on gluonic pseudo--Ioffe-time distributions fitted simultaneously with experimental Drell-Yan and leading neutron electroproduction data. Inclusion of the lattice results with parametrized systematic corrections significantly reduces the uncertainties on the gluon PDF at parton momentum fractions $x \gtrsim 0.2$, revealing a higher gluon density in the pion at large $x$ than in the proton. The similar gluon momentum fractions in the pion and proton further suggests a relative suppression of the pion gluon density at small $x$. |
| 2025-07-30 | [Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection](http://arxiv.org/abs/2507.22693v1) | Naoki Aizawa, Keita Emura et al. | This study proposes an encrypted PID control system with a disturbance observer (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to achieve control performance while providing resistance to malleability-based attacks. The controller integrates a DOB with a PID structure to compensate for modeling uncertainties by estimating and canceling external disturbances. To enhance security, the system is designed to output error symbols when ciphertexts are falsified during decryption or evaluation, enabling real-time detection of malleability-based signal or parameter falsification. To validate the proposed method, we conduct stage positioning control experiments and attack detection tests using an industrial linear stage. The results show that the encrypted DOB-based PID controller outperforms a conventional encrypted PID controller in terms of tracking accuracy. Furthermore, the system successfully detects two types of malleability-based attacks: one that destabilizes the control system, and another that degrades its performance. The primary contributions of this study are: (i) the implementation of a KHE-based encrypted DOB-PID controller, (ii) the improvement of control performance under uncertainties, and (iii) the experimental demonstration of attack detection capabilities in encrypted control systems. |
| 2025-07-30 | [Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach](http://arxiv.org/abs/2507.22671v1) | Sami Saeed Alghamdi, Christopher Bull et al. | Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools. |
| 2025-07-30 | [Robust Voting under Uncertainty](http://arxiv.org/abs/2507.22655v1) | Satoshi Nakada, Shmuel Nitzan et al. | This paper proposes normative criteria for voting rules under uncertainty about individual preferences. The criteria emphasize the importance of responsiveness, i.e., the probability that the social outcome coincides with the realized individual preferences. Given a convex set of probability distributions of preferences, denoted by $P$, a voting rule is said to be $P$-robust if, for each probability distribution in $P$, at least one individual's responsiveness exceeds one-half. Our main result establishes that a voting rule is $P$-robust if and only if there exists a nonnegative weight vector such that the weighted average of individual responsiveness is strictly greater than one-half under every extreme point of $P$. In particular, if the set $P$ includes all degenerate distributions, a $P$-robust rule is a weighted majority rule without ties. |
| 2025-07-29 | [Planning Persuasive Trajectories Based on a Leader-Follower Game Model](http://arxiv.org/abs/2507.22022v1) | Chaozhe R. He, Yichen Dong et al. | We propose a framework that enables autonomous vehicles (AVs) to proactively shape the intentions and behaviors of interacting human drivers. The framework employs a leader-follower game model with an adaptive role mechanism to predict human interaction intentions and behaviors. It then utilizes a branch model predictive control (MPC) algorithm to plan the AV trajectory, persuading the human to adopt the desired intention. The proposed framework is demonstrated in an intersection scenario. Simulation results illustrate the effectiveness of the framework for generating persuasive AV trajectories despite uncertainties. |
| 2025-07-29 | [Uncertainty Estimation of the Optimal Decision with Application to Cure Process Optimization](http://arxiv.org/abs/2507.21995v1) | Yezhuo Li, Qiong Zhang et al. | Decision-making in manufacturing often involves optimizing key process parameters using data collected from simulation experiments. Gaussian processes are widely used to surrogate the underlying system and guide optimization. Uncertainty often inherent in the decisions given by the surrogate model due to limited data and model assumptions. This paper proposes a surrogate model-based framework for estimating the uncertainty of optimal decisions and analyzing its sensitivity with respect to the objective function. The proposed approach is applied to the composite cure process simulation in manufacturing. |
| 2025-07-29 | [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](http://arxiv.org/abs/2507.21931v1) | Carel van Niekerk, Renato Vukovic et al. | Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.   RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.   By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training. |
| 2025-07-29 | [Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs](http://arxiv.org/abs/2507.21893v1) | Saeed Ghorbani | We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences. |
| 2025-07-29 | [Multi-Gap superconductivity in HgS under pressure](http://arxiv.org/abs/2507.21869v1) | Pietro Maria Forcella, Cesare Tresca et al. | Mercury chalcogenides are a class of materials that exhibit diverse structural phases under pressure, leading to a range of exotic physical properties, including topological phases and chiral phonons. In particular, the phase diagram of mercury sulfide (HgS) remains difficult to characterize, with significant uncertainty surrounding the transition pressure between phases. Based on recent experimental results, we employ Density Functional Theory and Superconducting Density Functional Theory to investigate the pressure-induced structural phase transition in HgS and its interplay with the emergence of superconductivity as the crystal transitions from the cinnabar phase (space group P3$_1$21) to the rock salt phase (space group Fm$\bar{3}$m). Remarkably, the rocksalt phase hosts a multigap superconducting state driven by distinct Fermi surface sheets, with two dominant gaps; the unusually high critical temperature of $\sim$11 K emerges naturally within this multiband scenario, highlighting the role of interband coupling beyond isotropic models. These results place HgS among the few systems where multiband superconducting gap structures emerge under pressure. |
| 2025-07-29 | [Two-neutrino $Î²Î²$ decay to excited states at next-to-leading order](http://arxiv.org/abs/2507.21868v1) | Daniel Castillo, Dorian Frycz et al. | We study two-neutrino double-beta decay ($2\nu\beta\beta$) into first-excited $0^+_2$ states of nuclei used in $\beta\beta$ decay experiments, including $^{76}$Ge, $^{82}$Se, $^{130}$Te, and $^{136}$Xe. We calculate the corresponding nuclear matrix elements (NMEs) within the nuclear shell model, using various Hamiltonians that describe well the spectroscopy of the initial and final nuclei. We evaluate the next-to-leading order (NLO) long-range NMEs recently introduced within chiral effective field theory, keeping three terms in the expansion of the energy denominator. In most cases, NLO contributions to the half-life are below 5%, but they can increase to 25% due to cancellations in the leading-order Gamow-Teller NME. A detailed analysis in terms of nuclear deformation, including triaxiality, indicates that larger deformation differences between the initial and final states generally lead to smaller NMEs, but the seniority structure of the states also plays a relevant role. The lower range of our predicted half-lives, with uncertainties dominated by the nuclear Hamiltonian used, are slightly longer than the current experimental limit in $^{76}$Ge and the very recent half-life indication in $^{82}$Se. |
| 2025-07-29 | [The Lyman-$Î±$ Forest from LBGs: First 3D Correlation Measurement with DESI and Prospects for Cosmology](http://arxiv.org/abs/2507.21852v1) | Hiram K. Herrera-Alcantar, Eric Armengaud et al. | The Lyman-$\alpha$ (Ly$\alpha$) forest is a key tracer of large-scale structure at redshifts z > 2, traditionally studied using spectra of quasars. Here, we explore the viability Lyman Break Galaxies (LBGs) as alternative background sources for Ly$\alpha$ forest studies. We analyze 4,151 Ly$\alpha$ forest skewers extracted from LBG spectra obtained in the DESI pilot surveys in the COSMOS and XMM-LSS fields. We present the first measurement of the Ly$\alpha$ forest auto-correlation function derived exclusively from LBG spectra, probing comoving separations up to 48 $h^{-1}$Mpc at an effective redshift of $z_\mathrm{eff}$ = 2.70. The measured signal is consistent with that from DESI DR2 quasar Ly$\alpha$ forest spectra at a comparable redshift, validating LBGs as reliable background sources. We also measure the cross-correlation between the LBG Ly$\alpha$ forest and 13,362 galaxy positions, showing that this observable serves as a sensitive diagnostic for galaxy redshift uncertainties and systematic offsets. Finally, using synthetic LBG spectra and Fisher forecasts, we show that a future wide-area survey over 5000 deg$^2$, targeting 1000 LBGs per deg$^2$ at similar signal-to-noise than our dataset, could enable Ly$\alpha$ forest baryon acoustic oscillation (BAO) measurements with 0.4% precision on the isotropic BAO scale and 1.3% on the anisotropic (Alcock-Paczynski) scale. Combining BAO with a Ly$\alpha$ forest full-shape analysis improves the AP constraint to 0.6%. These results open a new path for precision cosmology at high redshift using dense LBG samples. |
| 2025-07-29 | [Probabilistic Active Goal Recognition](http://arxiv.org/abs/2507.21846v1) | Chenyuan Zhang, Cristian Rojas Cardenas et al. | In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems. |
| 2025-07-29 | [Distribution-Based Masked Medical Vision-Language Model Using Structured Reports](http://arxiv.org/abs/2507.21794v1) | Shreyank N Gowda, Ruichi Zhang et al. | Medical image-language pre-training aims to align medical images with clinically relevant text to improve model performance on various downstream tasks. However, existing models often struggle with the variability and ambiguity inherent in medical data, limiting their ability to capture nuanced clinical information and uncertainty. This work introduces an uncertainty-aware medical image-text pre-training model that enhances generalization capabilities in medical image analysis. Building on previous methods and focusing on Chest X-Rays, our approach utilizes structured text reports generated by a large language model (LLM) to augment image data with clinically relevant context. These reports begin with a definition of the disease, followed by the `appearance' section to highlight critical regions of interest, and finally `observations' and `verdicts' that ground model predictions in clinical semantics. By modeling both inter- and intra-modal uncertainty, our framework captures the inherent ambiguity in medical images and text, yielding improved representations and performance on downstream tasks. Our model demonstrates significant advances in medical image-text pre-training, obtaining state-of-the-art performance on multiple downstream tasks. |
| 2025-07-29 | [The impact of large-scale EV charging on the real-time operation of distribution systems: A comprehensive review](http://arxiv.org/abs/2507.21759v1) | Zhe Yu, Chuang Yang et al. | With the large-scale integration of electric vehicles (EVs) in the distribution grid, the unpredictable nature of EV charging introduces considerable uncertainties to the grid's real-time operations. This can exacerbate load fluctuations, compromise power quality, and pose risks to the grid's stability and security. However, due to their dual role as controllable loads and energy storage devices, EVs have the potential to mitigate these fluctuations, balance the variability of renewable energy sources, and provide ancillary services that support grid stability. By leveraging the bidirectional flow of information and energy in smart grids, the adverse effects of EV charging can be minimized and even converted into beneficial outcomes through effective real-time management strategies. This paper explores the negative impacts of EV charging on the distribution system's real-time operations and outlines methods to transform these challenges into positive contributions. Additionally, it provides an in-depth analysis of the real-time management system for EV charging, focusing on state estimation and management strategies. |
| 2025-07-28 | [Locally Adaptive Conformal Inference for Operator Models](http://arxiv.org/abs/2507.20975v1) | Trevor Harris, Yan Liu | Operator models are regression algorithms for functional data and have become a key tool for emulating large-scale dynamical systems. Recent advances in deep neural operators have dramatically improved the accuracy and scalability of operator modeling, but lack an inherent notion of predictive uncertainty. We introduce Local Spectral Conformal Inference (LSCI), a new framework for locally adaptive, distribution-free uncertainty quantification for neural operator models. LSCI uses projection-based depth scoring and localized conformal inference to generate function-valued prediction sets with statistical guarantees. We prove approximate finite-sample marginal coverage under local exchangeability, and demonstrate significant gains in adaptivity and coverage across synthetic and real-world operator learning tasks. |
| 2025-07-28 | [The Concordance of Weak Lensing and Escape Velocity Mass Estimates for Galaxy Clusters](http://arxiv.org/abs/2507.20938v1) | Alexander Rodriguez, Christopher J. Miller | In the $\Lambda$CDM paradigm, the masses of the galaxy clusters inferred using background galaxies via weak-lensing shear should agree with the masses measured using the galaxy projected radius-velocity phase-space data via the escape velocity profile. However, prior work indicates that the correlation between caustic-inferred escape masses and weak lensing masses is statistically consistent with zero. Based on recent advancements in the measurement of the escape edge and its physical interpretation, we conduct a revised comparison between these two independent mass inference techniques for 46 galaxy clusters between $0.05 \le z \le 0.3$ and over an order of magnitude in mass, $14.4 \le {\rm log}_{10} M/M_{\odot} \le 15.4$. We find excellent agreement, with a correlation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between the two mass measurements consistent with zero (0.02 $\pm$ 0.02 dex). The observed scatter between these direct mass estimates is 0.17 dex and is consistent with the reported individual mass errors, suggesting that there is no need for an additional intrinsic component. We discuss the important practical consequences of these results, focusing on the systematic uncertainties inherent to each technique, and their implications for cosmology. |
| 2025-07-28 | [Target-density formation in swarms with stochastic sensing and dynamics](http://arxiv.org/abs/2507.20911v1) | Jason Hindes, George Stantchev et al. | An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target. |
| 2025-07-28 | [DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](http://arxiv.org/abs/2507.20879v1) | Weicheng Zheng, Xiaofei Mao et al. | Vision-Language Models (VLMs) are advancing autonomous driving, yet their potential is constrained by myopic decision-making and passive perception, limiting reliability in complex environments. We introduce DriveAgent-R1 to tackle these challenges in long-horizon, high-level behavioral decision-making. DriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that adaptively switches between efficient text-based and in-depth tool-based reasoning, and an Active Perception mechanism with a vision toolkit to proactively resolve uncertainties, thereby balancing decision-making efficiency and reliability. The agent is trained using a novel, three-stage progressive reinforcement learning strategy designed to master these hybrid capabilities. Extensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art performance, outperforming even leading proprietary large multimodal models, such as Claude Sonnet 4. Ablation studies validate our approach and confirm that the agent's decisions are robustly grounded in actively perceived visual evidence, paving a path toward safer and more intelligent autonomous systems. |
| 2025-07-28 | [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](http://arxiv.org/abs/2507.20861v1) | Marco Faroni, Carlo Odesco et al. | Physics-based simulations and learning-based models are vital for complex robotics tasks like deformable object manipulation and liquid handling. However, these models often struggle with accuracy due to epistemic uncertainty or the sim-to-real gap. For instance, accurately pouring liquid from one container to another poses challenges, particularly when models are trained on limited demonstrations and may perform poorly in novel situations. This paper proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed to mitigate these inaccuracies. By incorporating estimates of model uncertainty, the proposed MCTS strategy biases the search towards actions with lower predicted uncertainty. This approach enhances the reliability of planning under uncertain conditions. Applied to a liquid pouring task, our method demonstrates improved success rates even with models trained on minimal data, outperforming traditional methods and showcasing its potential for robust decision-making in robotics. |
| 2025-07-28 | [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](http://arxiv.org/abs/2507.20850v1) | Meiting Dang, Yanping Wu et al. | Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method. |
| 2025-07-28 | [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](http://arxiv.org/abs/2507.20804v1) | Xueyao Wan, Hang Yu | Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths. |
| 2025-07-28 | [Physical Constraints on the Rhythmicity of the Biological Clock](http://arxiv.org/abs/2507.20750v1) | YeongKyu Lee, Changbong Hyeon | Circadian rhythms in living organisms are temporal orders emerging from biochemical circuits driven out of equilibrium. Here, we study how the rhythmicity of a biochemical clock is shaped using the KaiABC system. A phase diagram constructed as a function of KaiC and KaiA concentrations reveals a sharply bounded limit-cycle region, which naturally explains arrhythmia upon protein over-expression. Beyond the Hopf bifurcation, intrinsic noise enables regular oscillation via coherence resonance. Within the limit-cycle region, greater rhythmic precision incurs a higher energetic cost, following the thermodynamic uncertainty relation. The cost-minimizing period of the KaiABC clock ($\sim$21-hr) is close enough to entrain to 24-hr cycle of environment. Our study substantiates universal physical constraints on the robustness, precision, and efficiency of noisy biological clocks. |
| 2025-07-28 | [Generalized Uncertainty Principle as a Mechanism for CP Violation](http://arxiv.org/abs/2507.20727v1) | Hector Gisbert, Victor Ilisie et al. | Within quantum electrodynamics we show that the Generalized Uncertainty Principle induces higher-derivative corrections that promote the topological invariant $F_{\mu\nu}\,\widetilde F^{\mu\nu}$ to the dynamical, non-topological operator $\partial^\lambda F_{\mu\nu}\,\partial_\lambda \widetilde F^{\mu\nu}$. We explore the resulting phenomenology, focusing on the generation of electric dipole moments. Our findings open a new low-energy window for testing quantum-gravity scenarios through precision measurements of charge-parity violation. |
| 2025-07-28 | [Uncertainty-driven Embedding Convolution](http://arxiv.org/abs/2507.20718v1) | Sungjun Lim, Kangjun Noh et al. | Text embeddings are essential components in modern NLP pipelines. While numerous embedding models have been proposed, their performance varies across domains, and no single model consistently excels across all tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble weights based on embedding uncertainty, grounded in a Bayes-optimal solution under a surrogate loss. Additionally, UEC introduces an uncertainty-aware similarity function that directly incorporates uncertainty into similarity scoring. Extensive experiments on retrieval, classification, and semantic similarity benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling. |
| 2025-07-25 | [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](http://arxiv.org/abs/2507.19458v1) | Amir Fard, Arnold X. -X. Yuan | Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows. |
| 2025-07-25 | [DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment](http://arxiv.org/abs/2507.19418v1) | Yiwei Lou, Yuanpeng He et al. | Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios. |
| 2025-07-25 | [Measurement of the Inelastic Proton-Proton Cross-Section at $\sqrt{s} \geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory](http://arxiv.org/abs/2507.19326v1) | Olena Tkachenko | Measuring proton-proton interaction cross-sections at center-of-mass energies above 40 TeV remains a significant challenge in particle physics. The Pierre Auger Observatory provides a unique opportunity to study the interactions at the highest energies through the distribution of the depth of maximum shower development ($X_\mathrm{max}$) observed by its Fluorescence Detector. In previous studies, the determination of the interaction cross-section at ultrahigh energies has relied on the assumption that the tail of the $X_\mathrm{max}$ distribution is proton-dominated, which restricts the analysis to a limited energy range below the ankle and introduces related systematic uncertainties. In this contribution, we adopt a novel method for the simultaneous estimation of the proton-proton interaction cross-section and the primary cosmic-ray mass composition using data from the Pierre Auger Observatory, avoiding assumptions about one quantity to infer the other and thus improving the accuracy and robustness of our analysis. In addition, a systematic shift in the $X_\mathrm{max}$ scale is fitted to account for both experimental uncertainties and theoretical constraints on the modeling of particle interactions. The obtained results are consistent with previous analyses and provide additional constraints on hadronic interaction models. The measured proton-proton inelastic cross-section at ultra-high energies agrees well with extrapolations of accelerator data. The inferred cosmic-ray composition and the $X_\mathrm{max}$-scale shift are also compatible with previous estimates. |
| 2025-07-25 | [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](http://arxiv.org/abs/2507.19263v1) | Achille Morenville, Ã‰ric Piette | In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings. |
| 2025-07-25 | [Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation](http://arxiv.org/abs/2507.19246v1) | Robert Hahn, Sebastian SchÃ¶ps | While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort. |
| 2025-07-25 | [Constraining the origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory: a three-dimensional approach](http://arxiv.org/abs/2507.19216v1) | Marta Bianciotto | Unveiling the sources of ultra-high-energy cosmic rays remains one of the main challenges of high-energy astrophysics. Measurements of anisotropies in their arrival directions are key to identifying their sources, yet magnetic deflections obscure direct associations. In this work, we reconstruct the sky regions of possible origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory by tracing their trajectories through Galactic magnetic fields using up-to-date models, while fully accounting for energy and directional uncertainties. A mixed composition at injection is assumed to model the detected charge distributions of such events. Different classes of astrophysical sources are investigated and tested for a correlation with the inferred regions of origin of the events. By incorporating constraints on the maximum propagation distances, we also allow for a three-dimensional localization of the possible source regions. Our findings provide new constraints on the sources of the highest-energy cosmic particles and offer fresh insights into the role of Galactic magnetic fields in shaping the observed ultra-high-energy cosmic-ray sky. |
| 2025-07-25 | [Emulating redshift-mixing due to blending in weak gravitational lensing](http://arxiv.org/abs/2507.19130v1) | Zekang Zhang, Daniel Gruen et al. | Galaxies whose images overlap in the focal plane of a telescope, commonly referred to as blends, are often located at different redshifts. Blending introduces a challenge to weak lensing cosmology probes, as such blends are subject to shear signals from multiple redshifts. This effect can be described by joining shear bias and redshift characterisation in the effective redshift distribution, $n_{\gamma}(z)$, which includes the response of apparent shapes of detected objects to shear of galaxies at redshift $z$. In this work, we propose a novel method to correct $n_{\gamma}(z)$ for redshift-mixed blending by emulating the shear response to neighbouring galaxies. Specifically, we design a ``half-sky-shearing'' simulation with HSC-Wide-like specifications, in which we extract the response of a detected object's measured ellipticity to shear of neighbouring galaxies among numerous galaxy pairs. We demonstrate the feasibility of accurately emulating these pairwise responses and validate the robustness of our approach under varying observing conditions and galaxy population uncertainties. We find that the effective redshift of sources at the high-redshift tail of the distribution is about 0.05 lower than expected when not modelling the effect. Given appropriately processed image simulations, our correction method can be readily incorporated into future cosmological analyses to mitigate this source of systematic error. |
| 2025-07-25 | [Boundary-layer transition in the age of data: from a comprehensive dataset to fine-grained prediction](http://arxiv.org/abs/2507.19120v1) | Wenhui Chang, Hongyuan Hu et al. | The laminar-to-turbulent transition remains a fundamental and enduring challenge in fluid mechanics. Its complexity arises from the intrinsic nonlinearity and extreme sensitivity to external disturbances. This transition is critical in a wide range of applications, including aerospace, marine engineering, geophysical flows, and energy systems. While the governing physics can be well described by the Navier-Stokes equations, practical prediction efforts often fall short due to the lack of comprehensive models for perturbation initialization and turbulence generation in numerical simulations. To address the uncertainty introduced by unforeseeable environmental perturbations, we propose a fine-grained predictive framework that accurately predicts the transition location. The framework generates an extensive dataset using nonlinear parabolized stability equations (NPSE). NPSE simulations are performed over a wide range of randomly prescribed initial conditions for the generic zero-pressure-gradient flat-plate boundary-layer flow, resulting in a large dataset that captures the nonlinear evolution of instability waves across three canonical transition pathways (Type-K, -H, and -O). From a database of 3000 simulation cases, we extract diagnostic quantities (e.g., wall pressure signals and skin-friction coefficients) from each simulation to construct a feature set that links pre-transition flow characteristics to transition onset locations. Machine learning models are systematically evaluated, with ensemble methods-particularly XGBoost-demonstrating exceptional predictive accuracy (mean relative error of approximately 0.001). Compared to methods currently available (e.g., N-factor, transitional turbulence model), this approach accounts for the physical process and achieves transition prediction without relying on any empirical parameters. |
| 2025-07-25 | [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](http://arxiv.org/abs/2507.19117v1) | Swapnil Hingmire, Ze Shi Li et al. | Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed. |
| 2025-07-25 | [Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks](http://arxiv.org/abs/2507.19111v1) | Bowen Li, Junting Chen | Dynamic low altitude networks offer significant potential for efficient and reliable data transport via unmanned aerial vehicles (UAVs) relays which usually operate with predetermined trajectories. However, it is challenging to optimize the data routing and resource allocation due to the time-varying topology and the need to control interference with terrestrial systems. Traditional schemes rely on time-expanded graphs with uniform and fine time subdivisions, making them impractical for interference-aware applications. This paper develops a dynamic space-time graph model with a cross-layer optimization framework that converts a joint routing and predictive resource allocation problem into a joint bottleneck path planning and resource allocation problem. We develop explicit deterministic bounds to handle the channel uncertainty and prove a monotonicity property in the problem structure that enables us to efficiently reach the globally optimal solution to the predictive resource allocation subproblem. Then, this approach is extended to multi-commodity transmission tasks through time-frequency allocation, and a bisection search algorithm is developed to find the optimum solution by leveraging the monotonicity of the feasible set family. Simulations verify that the single-commodity algorithm approaches global optimality with more than 30 dB performance gain over the classical graph-based methods for delay-sensitive and large data transportation. At the same time, the multi-commodity method achieves 100X improvements in dense service scenarios and enables an additional 20 dB performance gain by data segmenting. |
| 2025-07-24 | [Partial-State DADS Control for Matched Unmodeled Dynamics](http://arxiv.org/abs/2507.18609v1) | Iasson Karafyllis, Miroslav Krstic | We extend the Deadzone-Adapted Disturbance Suppression (DADS) control to time-invariant systems with dynamic uncertainties that satisfy the matching condition and for which no bounds for the disturbance and the unknown parameters are known. This problem is equivalent to partial-state adaptive feedback, where the states modeling the dynamic uncertainty are unmeasured. We show that the DADS controller can bypass small-gain conditions and achieve robust regulation for systems in spite of the fact that the strength of the interconnections has no known bound. Moreover, no gain and state drift arise, regardless of the size of the disturbances and unknown parameters. Finally, the paper provides the detailed analysis of a control system where the unmeasured state (or the dynamic uncertainty) is infinite-dimensional and described by a reaction-diffusion Partial Differential Equation, where the diffusion coefficient and the reaction term are unknown. It is shown that even in the infinite-dimensional case, a DADS controller can be designed and guarantees robust regulation of the plant state. |
| 2025-07-24 | [Searching for Gravitational Waves with Gaia and its Cross-Correlation with PTA: Absolute vs Relative Astrometry](http://arxiv.org/abs/2507.18593v1) | Massimo Vaglio, Mikel Falxa et al. | Astrometric missions like Gaia provide exceptionally precise measurements of stellar positions and proper motions. Gravitational waves traveling between the observer and distant stars can induce small, correlated shifts in these apparent positions, a phenomenon known as astrometric deflection. The precision and scale of astrometric datasets make them well-suited for searching for a stochastic gravitational wave background, whose signature appears in the two-point correlation function of the deflection field across the sky. Although Gaia achieves high accuracy in measuring angular separations in its focal plane, systematic uncertainties in the satellite's absolute orientation limit the precision of absolute position measurements. These orientation errors can be mitigated by focusing on relative angles between star pairs, which effectively cancel out common-mode orientation noise. In this work, we compute the astrometric response and the overlap reduction functions for this relative astrometry approach, correcting previous expressions presented in the literature. We use a Fisher matrix analysis to compare the sensitivity of relative astrometry to that of conventional absolute astrometry. Our analysis shows that while the relative method is theoretically sound, its sensitivity is limited for closely spaced star pairs within a single Gaia field of view. Pairs with large angular separations could provide competitive sensitivity, but are practically inaccessible due to Gaia's scanning law. Finally, we demonstrate that combining astrometric data with observations from pulsar timing arrays leads to slight improvements in sensitivity at frequencies greater than approximately 10^-7 Hz. |
| 2025-07-24 | [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](http://arxiv.org/abs/2507.18562v1) | Jiafeng Xiong, Yuting Zhao | Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. |
| 2025-07-24 | [Large deviations of ionic currents in dilute electrolytes](http://arxiv.org/abs/2507.18556v1) | Jafar Farhadi, David T. Limmer | We evaluate the exponentially rare fluctuations of the ionic current for a dilute electrolyte by means of macroscopic fluctuation theory. We consider the fluctuating hydrodynamics of a fluid electrolyte described by a stochastic Poisson-Nernst-Planck equation. We derive the Euler-Lagrange equations that dictate the optimal concentration profiles of ions conditioned on exhibiting a given current, whose form determines the likelihood of that current in the long-time limit. For a symmetric electrolyte under small applied voltages, number density fluctuations are small, and ionic current fluctuations are Gaussian with a variance determined by the Nernst-Einstein conductivity. Under large applied potentials, where number densities vary, the ionic current distribution is generically non-Gaussian. Its structure is constrained thermodynamically by Gallavotti-Cohen symmetry and the thermodynamic uncertainty principle. |
| 2025-07-24 | [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](http://arxiv.org/abs/2507.18498v1) | Zongzheng Zhang, Xuchong Qiu et al. | Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction. |
| 2025-07-24 | [A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks](http://arxiv.org/abs/2507.18492v1) | Mirhan Ãœrkmez, Carsten KallesÃ¸e et al. | Water utilities aim to reduce the high electrical costs of Water Distribution Networks (WDNs), primarily driven by pumping. However, pump scheduling is challenging due to model uncertainties and water demand forecast errors. This paper presents a Robust Model Predictive Control (RMPC) method for optimal and reliable pump scheduling, extending a previous efficient robust control method tailored to our model. A linear model with bounded additive disturbances is used to represent tank water level evolution, with uncertainty bounds derived from WDN simulation and demand data. At each time step, a pump scheduling policy, affine in past disturbances, is optimized to satisfy system constraints over a prediction horizon. The resulting policies are then applied in a receding horizon fashion. The optimization problem is formulated to require $\mathcal{O}(N^6)$ computations per iteration with an interior-point method, which is reduced to $\mathcal{O}(N^3)$ by reformulating it into a sparse form. When evaluated on a model representing the water distribution network of Randers, a medium-sized town in Denmark, the method surpasses nominal and constraint-tightening model predictive control (MPC) approaches in terms of meeting constraints and provides comparable economic outcomes. |
| 2025-07-24 | [Relativistic Calculations of Energy Levels, Field Shift Factors, and Polarizabilities of Mercury and Copernicium](http://arxiv.org/abs/2507.18490v1) | Hongxu Liu, Jize Han et al. | Mercury (Hg) and superheavy element copernicium (Cn) are investigated using equation-of-motion relativistic coupled-cluster (EOM-RCC) and configuration interaction plus many-body perturbation theory (CI+MBPT) methods. Key atomic properties including ionization potentials (IP), excitation energies (EEs), isotope field shift factors (F), and static electric dipole polarizabilities ({\alpha}) are calculated for ground and low-lying excited states. To evaluate the theoretical accuracy, calculations for both Hg and Cn are performed, with experimental data of Hg serving as benchmarks. Furthermore, basis set dependence has been systematically evaluated in the EOM-RCC calculations, with corresponding uncertainty estimates having been provided. The calculated atomic properties could provide valuable insights into the electronic structure and chemical behavior of superheavy elements. |
| 2025-07-24 | [INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data](http://arxiv.org/abs/2507.18488v1) | Mario Figueira, Michela Cameletti et al. | Environmental processes often exhibit complex, non-linear patterns and discontinuities across space and time, posing significant challenges for traditional geostatistical modeling approaches. In this paper, we propose a hybrid spatio-temporal modeling framework that combines the interpretability and uncertainty quantification of Bayesian models -- estimated using the INLA-SPDE approach -- with the predictive power and flexibility of Random Forest (RF). Specifically, we introduce two novel algorithms, collectively named INLA-RF, which integrate a statistical spatio-temporal model with RF in an iterative two-stage framework. The first algorithm (INLA-RF1) incorporates RF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2) uses RF to directly correct selected latent field nodes. Both hybrid strategies enable uncertainty propagation between modeling stages, an aspect often overlooked in existing hybrid approaches. In addition, we propose a Kullback-Leibler divergence-based stopping criterion. We evaluate the predictive performance and uncertainty quantification capabilities of the proposed algorithms through two simulation studies. Results suggest that our hybrid approach enhances spatio-temporal prediction while maintaining interpretability and coherence in uncertainty estimates. |
| 2025-07-24 | [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](http://arxiv.org/abs/2507.18484v1) | Xiao Yang, Lingxuan Wu et al. | Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving. |
| 2025-07-24 | [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](http://arxiv.org/abs/2507.18423v1) | Mizuki Funato, Yohei Sawada | Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions. |
| 2025-07-23 | [A narrowband burst from FRB 20190520B simultaneously observed by FAST and Parkes](http://arxiv.org/abs/2507.17696v1) | Yuhao Zhu, Chenhui Niu et al. | Fast Radio Bursts (FRBs) are short-duration radio transients with mysterious origins. Since its uncertainty, there are very few FRBs that are observed by different instruments, simultaneously. This study presents a detailed analysis of a burst from FRB 20190520B observed by FAST and Parkes at the same time. The spectrum of this individual burst ended at the upper limit of the FAST frequency band and was simultaneously detected by the Parkes telescope in the 1.5-1.8 GHz range. By employing spectral energy distribution (SED) and spectral sharpness methods, we confirmed the presence of narrowband radiation in FRB 20190520B, which is crucial for understanding its radiation mechanisms. Our findings support the narrowband characteristics that most repeaters exhibit. This work also highlights the necessity of continued multiband observations to explore its periodicity and frequency-dependent properties, contributing to an in-depth understanding of FRB phenomena. |
| 2025-07-23 | [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](http://arxiv.org/abs/2507.17695v1) | Ilias Chatzistefanidis, Navid Nikaein | Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance. |
| 2025-07-23 | [The Joint Asymptotic Distribution of Entropy and Complexity](http://arxiv.org/abs/2507.17625v1) | Angelika Silbernagel, Christian WeiÃŸ | We derive the asymptotic distribution of ordinal-pattern frequencies under weak dependence conditions and investigate the long-run covariance matrix not only analytically for moving-average, Gaussian, and the novel generalized coin-tossing processes, but also approximately by a simulation-based approach. Then, we deduce the asymptotic distribution of the entropy-complexity pair, which emerged as a popular tool for summarizing the time-series dynamics. Here, we make the necessary distinction between a uniform and a non-uniform ordinal pattern distribution and, thus, obtain two different limit theorems. On this basis, we consider a test for serial dependence and check its finite-sample performance. Moreover, we use our asymptotic results to approximate the estimation uncertainty of entropy-complexity pairs. |
| 2025-07-23 | [Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling](http://arxiv.org/abs/2507.17526v1) | Leandro Von Krannichfeldt, Kristina Orehounig et al. | Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling. |
| 2025-07-23 | [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](http://arxiv.org/abs/2507.17477v1) | Haoran Sun, Zekun Zhang et al. | Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance. |
| 2025-07-23 | [Analysing Models for Volatility Clustering with Subordinated Processes: VGSA and Beyond](http://arxiv.org/abs/2507.17431v1) | Sourojyoti Barick, Sudip Ratan Chandra | This paper explores a comprehensive class of time-changed stochastic processes constructed by subordinating Brownian motion with Levy processes, where the subordination is further governed by stochastic arrival mechanisms such as the Cox Ingersoll Ross (CIR) and Chan Karolyi Longstaff Sanders (CKLS) processes. These models extend classical jump frameworks like the Variance Gamma (VG) and CGMY processes, allowing for more flexible modeling of market features such as jump clustering, heavy tails, and volatility persistence. We first revisit the theory of Levy subordinators and establish strong consistency results for the VG process under Gamma subordination. Building on this, we prove asymptotic normality for both the VG and VGSA (VG with stochastic arrival) processes when the arrival process follows CIR or CKLS dynamics. The analysis is then extended to the more general CGMY process under stochastic arrival, for which we derive analogous consistency and limit theorems under positivity and regularity conditions on the arrival process. A simulation study accompanies the theoretical work, confirming our results through Monte Carlo experiments, with visualizations and normality testing (via Shapiro-Wilk statistics) that show approximate Gaussian behavior even for processes driven by heavy-tailed jumps. This work provides a rigorous and unified probabilistic framework for analyzing subordinated models with stochastic time changes, with applications to financial modeling and inference under uncertainty. |
| 2025-07-23 | [Confidence Calibration in Vision-Language-Action Models](http://arxiv.org/abs/2507.17383v1) | Thomas P Zollo, Richard Zemel | Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification. |
| 2025-07-23 | [Exploring Spatial Diversity for Region-based Active Learning](http://arxiv.org/abs/2507.17367v1) | Lile Cai, Xun Xu et al. | State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves $95\%$ performance of fully supervised methods with only $5-9\%$ of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation. |
| 2025-07-23 | [On Distributionally Robust Lossy Source Coding](http://arxiv.org/abs/2507.17366v1) | Giuseppe Serra, Photios A. Stavrou et al. | In this paper, we investigate the problem of distributionally robust source coding, i.e., source coding under uncertainty in the source distribution, discussing both the coding and computational aspects of the problem. We propose two extensions of the so-called Strong Functional Representation Lemma (SFRL), considering the cases where, for a fixed conditional distribution, the marginal inducing the joint coupling belongs to either a finite set of distributions or a Kullback-Leibler divergence sphere (KL-Sphere) centered at a fixed nominal distribution. Using these extensions, we derive distributionally robust coding schemes for both the one-shot and asymptotic regimes, generalizing previous results in the literature. Focusing on the case where the source distribution belongs to a given KL-Sphere, we derive an implicit characterization of the points attaining the robust rate-distortion function (R-RDF), which we later exploit to implement a novel algorithm for computing the R-RDF. Finally, we characterize the analytical expression of the R-RDF for Bernoulli sources, providing a theoretical benchmark to evaluate the estimation performance of the proposed algorithm. |
| 2025-07-23 | [Integrating Belief Domains into Probabilistic Logic Programs](http://arxiv.org/abs/2507.17291v1) | Damiano Azzolini, Fabrizio Riguzzi et al. | Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications. |
| 2025-07-22 | [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](http://arxiv.org/abs/2507.16806v1) | Mehul Damani, Isha Puri et al. | When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models. |
| 2025-07-22 | [Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2507.16796v1) | Mian Ibad Ali Shah, Enda Barrett et al. | This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities. |
| 2025-07-22 | [Quantum teleportation of an elemental silicon nanophotonic CNOT gate](http://arxiv.org/abs/2507.16783v1) | Kai-Chi Chang, Xiang Cheng et al. | Large-scale quantum computers possess the capacity to effectively tackle practical problems that can be insurmountable for classical computers. The main challenge in building these quantum computers is to realize scalable modules for remote qubits and entanglement. By assembling small, specialized parts into a larger architecture, the modular approach mitigates complexity and uncertainty. Such a distributed architecture requires non-local quantum gate operations between remote qubits. An essential method for implementing such operations, known as quantum gate teleportation, requires only local operations, classical communication, and shared entanglement. Till today, the quantum gate teleportation using a photonic chip has remained elusive. Here we experimentally demonstrate the quantum teleportation of an on-chip controlled-NOT (CNOT) gate, assisted with the scalable silicon chip platform, high-fidelity local quantum logic gates, linear optical components, post-selected entanglement, and coincidence measurements from photonic qubits. First, we measure and characterize our teleported chip-scale CNOT gate with an average truth table fidelity of 93.1 +- 0.3%. Second, for different input polarization states, we obtain an average quantum state fidelity of 87.0 +- 2.2% with our teleported on-chip CNOT gate. Third, we use our non-local CNOT gate for remote entanglement creation of four Bell states, with an average quantum state fidelity of 86.2 +- 0.8%. Fourthly, we fully characterize our teleported on-chip CNOT gate with a quantum process fidelity 83.1 +- 2.0%, and an average non-local CNOT gate fidelity of 86.5 +- 2.2%. Our teleported photonic on-chip quantum logic gate could be extended both to multiple qubits and chip-scale modules towards fault-tolerant and large-scale distributed quantum computation. |
| 2025-07-22 | [A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling](http://arxiv.org/abs/2507.16771v1) | Michael Grosskopf, Kellin Rumsey et al. | The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case. |
| 2025-07-22 | [Planck constraints on the scale dependence of isotropic cosmic birefringence](http://arxiv.org/abs/2507.16714v1) | M. Ballardini, A. Gruppuso et al. | The rotation of the linear polarisation plane of photons during propagation, also known as cosmic birefringence, is a powerful probe of parity-violating extensions of standard electromagnetism. Using Planck legacy data, we confirm previous estimates of the isotropic birefringence angle, finding $\beta \simeq 0.30 \pm 0.05$ [deg] at 68% CL, not including the systematic error from the instrumental polarisation angle. If this is a genuine signal, it could be explained by theories of Chern--Simons-type coupled to electromagnetism, which could lead to a harmonic scale-dependent birefringence signal, if the hypothesis of an ultra-light (pseudo) scalar field does not hold. To investigate these models, we pursue two complementary approaches: first, we fit the birefringence angle estimated at different multipoles, $\beta_{\ell}$, with a power-law model and second, we perform a non-parametric Bayesian reconstruction of it. Both methods yield results consistent with a non-vanishing constant birefringence angle. The first method shows no significant dependence on the harmonic scale (up to $1.8\sigma$ CL), while the second method demonstrates that a constant model is favored by Bayesian evidence. This conclusion is robust across all four published Planck CMB solutions. Finally, we forecast that upcoming CMB observations by Simons Observatory, LiteBIRD and a wishful CMB-Stage 4 experiment could reduce current uncertainties by a factor of approximately 7. |
| 2025-07-22 | [Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains](http://arxiv.org/abs/2507.16670v1) | Amandeep Kaur, Gyan Prakash | Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty. |
| 2025-07-22 | [FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture](http://arxiv.org/abs/2507.16668v1) | Somayeh Sobati-M | Modern smart grids demand fast, intelligent, and energy-aware computing at the edge to manage real time fluctuations and ensure reliable operation. This paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration and Twin based Execution a next-generation fog cloud framework designed to enhance autonomy, resilience, and efficiency in distributed energy systems. FOGNITE combines three core components: federated learning, reinforcement learning, and digital twin validation. Each fog node trains a local CNN LSTM model on private energy consumption data, enabling predictive intelligence while preserving data privacy through federated aggregation. A reinforcement learning agent dynamically schedules tasks based on current system load and energy conditions, optimizing for performance under uncertainty.   To prevent unsafe or inefficient decisions, a hierarchical digital twin layer simulates potential actions before deployment, significantly reducing execution errors and energy waste. We evaluate FOGNITE on a real world testbed of Raspberry Pi devices, showing up to a 93.7% improvement in load balancing accuracy and a 63.2% reduction in energy waste compared to conventional architectures. By shifting smart grid control from reactive correction to proactive optimization, FOGNITE represents a step toward more intelligent, adaptive, and sustainable energy infrastructures |
| 2025-07-22 | [A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market](http://arxiv.org/abs/2507.16655v1) | Mahdi Goldani | In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis. |
| 2025-07-22 | [Bayesian Variational Inference for Mixed Data Mixture Models](http://arxiv.org/abs/2507.16545v1) | Junyang Wang, James Bennett et al. | Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. In addition, we show that the posterior means from CAVI for this model converge to the true parameter value as the sample size n tends to infinity, providing theoretical justification for our method. |
| 2025-07-22 | [A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in Nonlinear Multi-Agent Systems](http://arxiv.org/abs/2507.16520v1) | Aria Delshad, Maryam Babazadeh | This paper proposes a reinforcement learning (RL)-based backstepping control strategy to achieve fixed time consensus in nonlinear multi-agent systems with strict feedback dynamics. Agents exchange only output information with their neighbors over a directed communication graph, without requiring full state measurements or symmetric communication. Achieving fixed time consensus, where convergence occurs within a pre-specified time bound that is independent of initial conditions is faced with significant challenges due to the presence of unknown nonlinearities, inter-agent couplings, and external disturbances. This work addresses these challenges by integrating actor critic reinforcement learning with a novel fixed time adaptation mechanism. Each agent employs an actor critic architecture supported by two estimator networks designed to handle system uncertainties and unknown perturbations. The adaptation laws are developed to ensure that all agents track the leader within a fixed time regardless of their initial conditions. The consensus and tracking errors are guaranteed to converge to a small neighborhood of the origin, with the convergence radius adjustable through control parameters. Simulation results demonstrate the effectiveness of the proposed approach and highlight its advantages over state-of-the-art methods in terms of convergence speed and robustness. |
| 2025-07-21 | [Euclid preparation: Expected constraints on initial conditions](http://arxiv.org/abs/2507.15819v1) | Euclid Collaboration, F. Finelli et al. | The Euclid mission of the European Space Agency will deliver galaxy and cosmic shear surveys, which will be used to constrain initial conditions and statistics of primordial fluctuations. We present highlights for the Euclid scientific capability to test initial conditions beyond LCDM with the main probes, i.e. 3D galaxy clustering from the spectroscopic survey, the tomographic approach to 3x2pt statistics from photometric galaxy survey, and their combination. We provide Fisher forecasts from the combination of Euclid spectroscopic and photometric surveys for spatial curvature, running of the spectral index of the power spectrum of curvature perturbations, isocurvature perturbations, and primordial features. For the parameters of these models we also provide the combination of Euclid forecasts (pessimistic and optimistic) with current and future measurements of the cosmic microwave background (CMB) anisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide Fisher forecasts for how the power spectrum and bispectrum from the Euclid spectroscopic survey will constrain the local, equilateral, and orthogonal shapes of primordial non-Gaussianity. We also review how Bayesian field-level inference of primordial non-Gaussianity can constrain local primordial non-Gaussianity. We show how Euclid, with its unique combination of the main probes, will provide the tightest constraints on low redshift to date. By targeting a markedly different range in redshift and scale, Euclid's expected uncertainties are complementary to those obtained by CMB primary anisotropy, returning the tightest combined constraints on the physics of the early Universe. |
| 2025-07-21 | [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](http://arxiv.org/abs/2507.15803v1) | Danhui Chen, Ziquan Liu et al. | Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in. |
| 2025-07-21 | [Deterministic Quantum Search via Recursive Oracle Expansion](http://arxiv.org/abs/2507.15797v1) | John Burke, Ciaran McGoldrick | We introduce a novel deterministic quantum search algorithm that provides a practical alternative to conventional probabilistic search approaches. Our scheme eliminates the inherent uncertainty of quantum search without relying on arbitrary phase rotations, a key limitation of other deterministic methods. The algorithm achieves certainty by recursively expanding the base oracle so that it marks all states prefixed by the same two bits as the target, encompassing exactly one-quarter of the search space. This enables a step-by-step reduction of the superposition until the target state can be measured with certainty. The algorithm achieves deterministic success with a query complexity of $O(N^{\log_2(3)/2}) \approx O(N^{0.7925})$, falling between Grover's $O(\sqrt{N})$ scaling and the classical $O(N)$. Our approach relies exclusively on two-qubit nearest-neighbour diffusion operators, avoiding global diffusion entirely. We show that, despite the increased query complexity, this design reduces the total number of two-qubit gates required for diffusion by more than an order of magnitude for search spaces up to at least 18 qubits, with even greater advantages on hardware with limited qubit connectivity. The scheme's inherent determinism, reliance on simple nearest-neighbour, low-depth operations, and scalable recursive structure make it well-suited for hardware implementation. Additionally, we show that the algorithm naturally supports partial database search, enabling deterministic identification of selected target bits without requiring a full search, further broadening its applicability. |
| 2025-07-21 | [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](http://arxiv.org/abs/2507.15782v1) | Ruochu Yang, Yu Zhou et al. | Household robots have been a longstanding research topic, but they still lack human-like intelligence, particularly in manipulating open-set objects and navigating large environments efficiently and accurately. To push this boundary, we consider a generalized multi-object collection problem in large scene graphs, where the robot needs to pick up and place multiple objects across multiple locations in a long mission of multiple human commands. This problem is extremely challenging since it requires long-horizon planning in a vast action-state space under high uncertainties. To this end, we propose a novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a multimodal action cost similarity function, our algorithm can both reflect the history and look into the future to optimize plans, striking a good balance of quality and efficiency. Simulation experiments demonstrate that compared with latest works, our algorithm improves the overall mission performance by 30% in terms of fulfilling human commands, maximizing mission success rates, and minimizing mission costs. |
| 2025-07-21 | [Learning Climate Sensitivity from Future Observations, Fast and Slow](http://arxiv.org/abs/2507.15767v1) | Adam Michael Bauer, Cristian Proistosescu et al. | Climate sensitivity has remained stubbornly uncertain since the Charney Report was published some 45 years ago. Two factors in future climate projections could alter this dilemma: (i) an increased ratio of CO$_2$ forcing relative to aerosol cooling, owing to both continued accumulation of CO$_2$ and declining aerosol emissions, and (ii) a warming world, whereby CO$_2$-induced warming becomes more pronounced relative to climate variability. Here, we develop a novel modeling approach to explore the rates of learning about equilibrium climate sensitivity and the transient climate response (TCR) and identify the physical drivers underpinning these learning rates. Our approach has the advantage over past work by accounting for the full spectrum of parameter uncertainties and covariances, while also taking into account serially correlated internal climate variability. Moreover, we provide a physical explanation of how quickly we may hope to learn about climate sensitivity. We find that, although we are able to constrain future TCR regardless of the true underlying value, constraining ECS is more difficult, with low values of ECS being more easily ascertained than high values. This asymmetry can be explained by most of the warming this century being attributable to the fast climate mode, which is more useful for constraining TCR than it is for ECS. We further show that our inability to constrain the deep ocean response is what limits our ability to learn high values of ECS. |
| 2025-07-21 | [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](http://arxiv.org/abs/2507.15741v1) | GÃ¡bor Lugosi, Marcos Matabuena | This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces. Leveraging a newly defined notion of homoscedasticity, we develop a conformal prediction algorithm that offers finite-sample coverage guarantees and fast convergence rates of the oracle estimator. In heteroscedastic settings, we forgo these non-asymptotic guarantees to gain statistical efficiency, proposing a local $k$--nearest--neighbor method without conformal calibration that is adaptive to the geometry of each particular nonlinear space. Both procedures work with any regression algorithm and are scalable to large data sets, allowing practitioners to plug in their preferred models and incorporate domain expertise. We prove consistency for the proposed estimators under minimal conditions. Finally, we demonstrate the practical utility of our approach in personalized--medicine applications involving random response objects such as probability distributions and graph Laplacians. |
| 2025-07-21 | [Symplectic coherence: a measure of position-momentum correlations in quantum states](http://arxiv.org/abs/2507.15738v1) | Varun Upreti, Ulysse Chabaud | The interdependence of position and momentum, as highlighted by the Heisenberg uncertainty principle, is a cornerstone of quantum physics. Yet, position-momentum correlations have received little systematic attention. Motivated by recent developments in bosonic quantum physics that underscore their relevance in quantum thermodynamics, metrology, and computing, we establish a general framework to study and quantify position-momentum correlations in quantum states. We introduce symplectic coherence, a faithful and easily computable measure defined as the Frobenius norm of the block of the covariance matrix encoding position-momentum correlations, and demonstrate that symplectic coherence is monotone under relevant operations and robust under small perturbations. Furthermore, using a recent mapping by Barthe et al. (Phys. Rev. Lett. 134, 070604) which relates the covariance matrix of a bosonic state to the density matrix of a finite-dimensional system, we show that position-momentum correlations correspond to beyond-classical correlations in a virtual finite-dimensional quantum state, with symplectic coherence mapping naturally to geometric quantum discord. Taking energy constraints into account, we determine the maximal position-momentum correlations achievable at fixed energy, revealing structural insights about the corresponding optimal states. Finally, we illustrate the operational relevance of symplectic coherence through several examples in quantum information tasks and quantum thermodynamics. In the process, we establish new technical results on matrix norms and quantum covariance matrices, and demonstrate the conceptual significance of viewing covariance matrices as density matrices of virtual quantum states. |
| 2025-07-21 | [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](http://arxiv.org/abs/2507.15727v1) | Xuchuang Wang, Bo Sun et al. | This paper introduces a novel multi-agent ski-rental problem that generalizes the classical ski-rental dilemma to a group setting where agents incur individual and shared costs. In our model, each agent can either rent at a fixed daily cost, or purchase a pass at an individual cost, with an additional third option of a discounted group pass available to all. We consider scenarios in which agents' active days differ, leading to dynamic states as agents drop out of the decision process. To address this problem from different perspectives, we define three distinct competitive ratios: overall, state-dependent, and individual rational. For each objective, we design and analyze optimal deterministic and randomized policies. Our deterministic policies employ state-aware threshold functions that adapt to the dynamic states, while our randomized policies sample and resample thresholds from tailored state-aware distributions. The analysis reveals that symmetric policies, in which all agents use the same threshold, outperform asymmetric ones. Our results provide competitive ratio upper and lower bounds and extend classical ski-rental insights to multi-agent settings, highlighting both theoretical and practical implications for group decision-making under uncertainty. |
| 2025-07-21 | [Evaluation of hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles using electrochemical permeation and thermal desorption spectroscopy](http://arxiv.org/abs/2507.15711v1) | Nicholas Winzer | Hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles was investigated using electrochemical permeation (EP) and thermal desorption spectroscopy (TDS). The trapping parameters for the test materials were evaluated by fitting the measurements with a finite element model based on the McNabb-Foster equations using least-squares optimisation. The measurements showed that hydrogen diffusion in ferrite is slowed significantly by the presence of fine (<5 nm) (Ti,Cr)C particles; coarser particles had little or no effect. The TDS measurements were consistent with hydrogen traps with a high energy barrier. The uniqueness of the hydrogen trapping parameters obtained using the fitting procedure was evaluated. It was found that the system was overdetermined; the measurements could be fitted with multiple combinations of trapping parameters. Consequently, it was not possible to determine the individual trapping parameters using this procedure. Trapping parameters were also evaluated from TDS measurements by applying Kissinger's equation. Using this procedure a trap binding energy of 0.24 eV was calculated for all materials, albeit with a high degree of uncertainty. |
| 2025-07-21 | [Ubiquity of Uncertainty in Neuron Systems](http://arxiv.org/abs/2507.15702v1) | Brandon B. Le, Bennett Lamb et al. | We demonstrate that final-state uncertainty is ubiquitous in multistable systems of coupled neuronal maps, meaning that predicting whether one such system will eventually be chaotic or nonchaotic is often nearly impossible. We propose a "chance synchronization" mechanism that governs the emergence of unpredictability in neuron systems and support it by using basin classification, uncertainty exponent, and basin entropy techniques to analyze five simple discrete-time systems, each consisting of a different neuron model. Our results illustrate that uncertainty in neuron systems is not just a product of noise or high-dimensional complexity; it is also a fundamental property of low-dimensional, deterministic models, which has profound implications for understanding brain function, modeling cognition, and interpreting unpredictability in general multistable systems. |
| 2025-07-18 | [Missing baryons recovered: a measurement of the gas fraction in galaxies and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing](http://arxiv.org/abs/2507.14136v1) | Boryana Hadzhiyska, Simone Ferraro et al. | We present new constraints on the halo masses and matter density profiles of DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs) and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing convergence map from ACT DR6. This provides an independent, lensing-based calibration of halo masses, complementary to methods relying on clustering or dynamics. We derive constraints on the mean halo mass for three DESI-selected samples, finding $\log(M_{\rm halo}/(M_\odot/h)) \approx 13.18$, 13.03 and 13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a halo model approach, we also compare the projected galaxy-matter density profiles with previously reported gas profiles inferred from measurements of the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the key uncertainties in interpreting kSZ signals -- the unknown host halo mass distribution -- by providing an independent and consistent mass calibration. The agreement between the gas and total mass profiles at large aperture suggests that sufficiently far from the group center (2--3 virial radii), we recover all the baryons, offering a resolution to the 'missing baryon' problem. We further study the cumulative gas fractions for all galaxies as well as for the most massive galaxy groups in the sample ($\log(M_{\rm halo}/(M_\odot/h)) \approx 13.5$), finding values that are physically sensible and in agreement with previous findings using kSZ and X-ray data: compared to the TNG300 simulation, the observed gas fractions are systematically lower at fixed radius by $\gtrsim$4$\sigma$, providing compelling, independent evidence for stronger baryonic feedback in the real Universe. These findings highlight the power of combining CMB lensing with galaxy surveys to probe the interplay between baryons and dark matter in group-sized halos. |
| 2025-07-18 | [On the relation between perspective-neutral, algebraic, and effective quantum reference frames](http://arxiv.org/abs/2507.14131v1) | Philipp A. Hoehn, Julian De Vuyst et al. | The framework of internal quantum reference frames (QRFs) constitutes a universal toolset for dealing with symmetries in quantum theory and has led to new revelations in quantum gravity, gauge theories and foundational physics. Multiple approaches have emerged, sometimes differing in scope and the way symmetries are implemented, raising the question as to their relation. Here, we investigate the relation between three approaches to QRFs for gauge symmetries, namely the effective semiclassical, algebraic, and perspective-neutral (PN) approaches. Rather than constructing Hilbert spaces, as the PN approach, the effective approach is based on a quantum phase space parametrized by expectation values and fluctuations, while the emphasis of the algebraic approach is on the state space of complex linear functionals on a kinematical algebra. Nevertheless, external frame information is treated as gauge in all three formalisms, manifested in constraints on states and algebra. We show that these three approaches are, in fact, equivalent for ideal QRFs, distinguished by sharp orientations, which is the previous setting of the first two approaches. Our demonstration pertains to single constraints, including relativistic ones, and encompasses QRF changes. In particular, the QRF transformations of the PN framework agree semiclassically with those of the older effective approach, by which it was inspired. As a physical application, we explore the QRF covariance of uncertainties and fluctuations, which turn out to be frame-dependent. This is particularly well-suited for the effective and algebraic approaches, for which these quantities form a natural basis. Finally, we pave the way towards extending these two approaches to non-ideal QRFs by studying the projection and gauge-fixing operations of the Page-Wootters formalism, built into the PN framework, on algebraic states. |
| 2025-07-18 | [Integrating Forecasting Models Within Steady-State Analysis and Optimization](http://arxiv.org/abs/2507.14117v1) | Aayushya Agarwal, Larry Pileggi | Extreme weather variations and the increasing unpredictability of load behavior make it difficult to determine power grid dispatches that are robust to uncertainties. While machine learning (ML) methods have improved the ability to model uncertainty caused by loads and renewables, accurately integrating these forecasts and their sensitivities into steady-state analyses and decision-making strategies remains an open challenge. Toward this goal, we present a generalized methodology that seamlessly embeds ML-based forecasting engines within physics-based power flow and grid optimization tools. By coupling physics-based grid modeling with black-box ML methods, we accurately capture the behavior and sensitivity of loads and weather events by directly integrating the inputs and outputs of trained ML forecasting models into the numerical methods of power flow and grid optimization. Without fitting surrogate load models, our approach obtains the sensitivities directly from data to accurately predict the response of forecasted devices to changes in the grid. Our approach combines the sensitivities of forecasted devices attained via backpropagation and the sensitivities of physics-defined grid devices. We demonstrate the efficacy of our method by showcasing improvements in sensitivity calculations and leveraging them to design a robust power dispatch that improves grid reliability under stochastic weather events. Our approach enables the computation of system sensitivities to exogenous factors which supports broader analyses that improve grid reliability in the presence of load variability and extreme weather conditions. |
| 2025-07-18 | [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](http://arxiv.org/abs/2507.14102v1) | Shravan Venkatraman, Pavan Kumar S et al. | Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL |
| 2025-07-18 | [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](http://arxiv.org/abs/2507.14099v1) | Markus Buchholz, Ignacio Carlucho et al. | Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges. |
| 2025-07-18 | [Global Bayesian Analysis of $\mathrm{J}/Ïˆ$ Photoproduction on Proton and Lead Targets](http://arxiv.org/abs/2507.14087v1) | Heikki MÃ¤ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions using a color glass condensate (CGC) based calculation framework. As past calculations have shown that CGC-based models typically overpredict the $\mathrm{J}/\psi$ production in $\gamma+\mathrm{Pb}$ collisions at high center of mass energy, we address the question of whether it is possible to describe coherent and incoherent diffractive $\mathrm{J}/\psi$ data from $\gamma+p$ collisions at HERA and the LHC, and from $\gamma+\mathrm{Pb}$ collisions at the LHC simultaneously. Our results indicate that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ data is challenging, with results improving when an overall $K$-factor -- scaling $\gamma+p$ and $\gamma+\mathrm{Pb}$ cross sections to absorb model uncertainties -- is introduced. |
| 2025-07-18 | [Direct Measurement of the Accretion Disk Formed in Prompt Collapse Mergers with Future Gravitational-Wave Observatories](http://arxiv.org/abs/2507.14071v1) | Arnab Dhani, Alessandro Camilletti et al. | The production site of heavy r-process elements, such as Gold and Uranium, is uncertain. Neutron star mergers are the only astrophysical phenomenon in which we have witnessed their formation. However, the amount of heavy elements resulting from the merger remains poorly constrained, mainly due to uncertainties on the mass and angular momentum of the disk formed in the merger remnant. Matter accretion from the disk is also thought to power gamma ray-bursts. We discover from numerical relativity simulations that the accretion disk influences the ringdown gravitational-wave signal produced by binaries that promptly collapse to black-hole at merger. We propose a method to \emph{directly} measure the mass of the accretion disk left during black hole formation in binary mergers using observatories such as the Einstein Telescope or Cosmic Explorer with a relative error of 10\% for binaries at a distance of up to 30~Mpc, corresponding to an event rate of 0.001 to 0.25 events per year. |
| 2025-07-18 | [VLA-Mark: A cross modal watermark for large vision-language alignment model](http://arxiv.org/abs/2507.14067v1) | Shuliang Liu, Qi Zheng et al. | Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking |
| 2025-07-18 | [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](http://arxiv.org/abs/2507.14056v1) | Alejandro Rodriguez-Garcia, Anindya Ghosh et al. | Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks. |
| 2025-07-18 | [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](http://arxiv.org/abs/2507.14034v1) | Jochen Wulf, Jurg Meierhofer et al. | Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems. |
| 2025-07-17 | [$\texttt{raccoon}$: A Python package for removing wiggle artifacts in the JWST NIRSpec integral field spectroscopy](http://arxiv.org/abs/2507.13341v1) | Anowar J. Shajib | $\texttt{raccoon}$ is a Python package for removing resampling noise - commonly referred to as "wiggles'' - from spaxel-level spectra in datacubes obtained from the JWST Near Infrared Spectrograph's (NIRSpec) integral field spectroscopy (IFS) mode. These wiggles arise as artifacts during resampling of the 2D raw data into 3D datacubes, due to the point spread function (PSF) being undersampled. The standard JWST data reduction pipeline does not correct for this noise. The wiggle artifacts can significantly degrade the scientific usability of the data, particularly at the spaxel level, undermining the exquisite spatial resolution of NIRSpec. $\texttt{raccoon}$ provides an empirical correction by modeling and removing these artifacts, thereby restoring the fidelity of the extracted spectra. $\texttt{raccoon}$ forward-models the wiggles as a chirp function impacting one or more template spectra that are directly fit to the original data across the entire wavelength range. The best-fit wiggle model is then used to clean the data while propagating the associated uncertainties. |
| 2025-07-17 | [A Framework for Waterfall Pricing Using Simulation-Based Uncertainty Modeling](http://arxiv.org/abs/2507.13324v1) | Nicola Jean, Giacomo Le Pera et al. | We present a novel framework for pricing waterfall structures by simulating the uncertainty of the cashflow generated by the underlying assets in terms of value, time, and confidence levels. Our approach incorporates various probability distributions calibrated on the market price of the tranches at inception. The framework is fully implemented in PyTorch, leveraging its computational efficiency and automatic differentiation capabilities through Adjoint Algorithmic Differentiation (AAD). This enables efficient gradient computation for risk sensitivity analysis and optimization. The proposed methodology provides a flexible and scalable solution for pricing complex structured finance instruments under uncertainty |
| 2025-07-17 | [Testing halo models for constraining astrophysical feedback with multi-probe modeling: I. 3D Power spectra and mass fractions](http://arxiv.org/abs/2507.13317v1) | Pranjal R. S., Shivam Pandey et al. | Upcoming Stage-IV surveys will deliver measurements of distribution of matter with unprecedented precision, demanding highly accurate theoretical models for cosmological parameter inference. A major source of modeling uncertainty lies in astrophysical processes associated with galaxy formation and evolution, which remain poorly understood. Probes such as the thermal and kinematic Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio bursts offer a promising avenue for mapping the distribution and thermal properties of cosmic baryons. A unified analytical framework capable of jointly modeling these observables is essential for fully harnessing the complementary information while mitigating probe-specific systematics. In this work, we present a detailed assessment of existing analytical models, which differ in their assumptions and prescriptions for simultaneously describing the distribution of matter and baryons in the universe. Using the Magneticum hydrodynamical simulation, we test these models by jointly analyzing the 3D auto- and cross-power spectra of the matter and baryonic fields that underpin the above probes. We find that all models can reproduce the power spectra at sub-percent to few-percent accuracy, depending on the tracer combination and number of free parameters. Their ability to recover underlying halo properties, such as the evolution of gas abundance and thermodynamic profiles with halo mass, varies considerably. Our results suggest that these models require further refinement and testing for reliable interpretation of multi-wavelength datasets. |
| 2025-07-17 | [Systematic study of the validity of the eikonal model including uncertainties](http://arxiv.org/abs/2507.13291v1) | Daniel Shiu, ChloÃ« Hebborn et al. | Nuclear reactions at intermediate beam energies are often interpreted using the eikonal model. In the analysis of complex reaction probes, where few-body reaction methods are needed, the eikonal method may be used as an efficient way for describing the fragment-target reaction process. In this work, we perform a systematic study to test the validity of the eikonal approximation for nucleon-nucleus reactions. We also quantify uncertainties due to the nucleon optical potential on reaction observables. We inspect the validity of the eikonal model and its semiclassical correction by comparing it to exact solutions (obtained from solving the optical model equation with a finite differences method) for a wide range of reactions. We also study the effect of relativistic corrections, both kinematic and dynamic, by effectively incorporating the relativistic effects at intermediate energies. The uncertainties from a Bayesian global optical potential (KDUQ) are propagated to the observables of interest. Our study includes neutron and proton reactions on $^{27}$Al, $^{40}$Ca, $^{90}$Zr and $^{208}$Pb, for a wide range of energies $E_{lab}=0-400$ MeV. Our results show that for the proton absorption cross section, the eikonal model can be used down to around $60$ MeV and the semiclassical correction extends its use to $30$ MeV. However, the validity of the eikonal model for the neutron total cross section only goes down to $\approx120$ MeV, a range extended to $\approx 50$ MeV when using the semiclassical correction. We find the semi-classical correction to the eikonal model to be less effective in describing the angular distributions. The $1\sigma$ uncertainty intervals on the observables we studied is less than $5$% for most of the energies considered, but increases rapidly for higher energies, namely energies outside the range of KDUQ ($E_{lab}>200$ MeV). |
| 2025-07-17 | [Multi-wavelength study of the high Galactic latitude supernova remnant candidate \snr\ associated with the Calvera pulsar](http://arxiv.org/abs/2507.13210v1) | Emanuele Greco, Michela Rigoselli et al. | The candidate supernova remnant (SNR) G118.4+37.0 (Calvera's SNR), discovered as a faint radio ring at high Galactic latitude and coincident with extended Fermi/LAT gamma-ray emission, is likely associated to the X-ray pulsar 1RXS J141256.0+792204 (Calvera). Previous XMM-Newton data hinted at soft diffuse X-ray emission inside the ring but lacked sufficient exposure for detailed characterisation. We obtained new XMM-Newton observations, and produced count-rate images, equivalent width and median photon energy maps to identify optimal regions for spectral analysis. We complemented these observations with a reanalysis of Fermi/LAT gamma-ray data and new Telescopio Nazionale Galileo observations aimed to search for Halpha emission. The X-ray diffuse emission is well described by a model of shock-heated plasma with temperature kT \sim 0.15 keV, mildly under-solar N and o abundances and densities ne=0.1-0.7 cm-3. According to our estimates, Calvera's SNR is 10-20 kya old and lies at a distance of 4-5 kpc. A distinti "Clump" region shows hared emission equally well described by a thermal (kT\sim 1.7 keV) or a non thermal model (Gamma \sim 2.7). The brightest X-ray area is close to the gamma-ray peak and to an isolated Alpha filament. G118.4+37.0 is a middle-aged remnant which expands in a tenuous medium and encountered a denser phase, likely the relic of the wind activity of the massive progenitor star. The estimated SNR distance is consistent within the uncertainties with that estimated for Calvera, confirming that this peculiar pulsar was born in the explosion of a massive star high above the Galactic disk. Our measured ambient density, together with the patchy morphology of the gamma-ray emission and the detection of Halpha filaments indicates that a hadronic origin is compatible with the gamma-ray flux, though a mixed leptonic-hadronic cannot be excluded |
| 2025-07-17 | [Performance Portable Gradient Computations Using Source Transformation](http://arxiv.org/abs/2507.13204v1) | Kim Liegeois, Brian Kelley et al. | Derivative computation is a key component of optimization, sensitivity analysis, uncertainty quantification, and nonlinear solvers. Automatic differentiation (AD) is a powerful technique for evaluating such derivatives, and in recent years, has been integrated into programming environments such as Jax, PyTorch, and TensorFlow to support derivative computations needed for training of machine learning models, resulting in widespread use of these technologies. The C++ language has become the de facto standard for scientific computing due to numerous factors, yet language complexity has made the adoption of AD technologies for C++ difficult, hampering the incorporation of powerful differentiable programming approaches into C++ scientific simulations. This is exacerbated by the increasing emergence of architectures such as GPUs, which have limited memory capabilities and require massive thread-level concurrency. Portable scientific codes rely on domain specific programming models such as Kokkos making AD for such codes even more complex. In this paper, we will investigate source transformation-based automatic differentiation using Clad to automatically generate portable and efficient gradient computations of Kokkos-based code. We discuss the modifications of Clad required to differentiate Kokkos abstractions. We will illustrate the feasibility of our proposed strategy by comparing the wall-clock time of the generated gradient code with the wall-clock time of the input function on different cutting edge GPU architectures such as NVIDIA H100, AMD MI250x, and Intel Ponte Vecchio GPU. For these three architectures and for the considered example, evaluating up to 10 000 entries of the gradient only took up to 2.17x the wall-clock time of evaluating the input function. |
| 2025-07-17 | [Medium-induced modification of azimuthal correlations of electrons from heavy-flavor hadron decays with charged particles in Pb-Pb collisions at $\sqrt{s_{\rm{NN}} = 5.02}$ TeV](http://arxiv.org/abs/2507.13197v1) | ALICE Collaboration | The azimuthal-correlation distributions between electrons from the decays of heavy-flavor hadrons and associated charged particles in Pb-Pb collisions at $\sqrt{s_{\rm NN}} = 5.02$ TeV are reported for the 0-10% and 30-50% centrality classes. This is the first measurement to provide access to the azimuthal-correlation observables in the heavy-flavor sector in Pb-Pb collisions. The analysis is performed for trigger electrons from heavy-flavor hadron decays with transverse momentum $4 < p_{\rm T}^{\rm e} < 16$ GeV/$c$, considering associated particles within the transverse-momentum range $1 < p_{\rm T}^{\rm assoc} < 7$ GeV/$c$, and a pseudorapidity difference of $|\Delta\eta| < 1$ between the trigger electron and associated particles. The per-trigger nuclear modification factor ($I_{\rm AA}$) is calculated to compare the near- and away-side peak yields to those in pp collisions at $\sqrt{s} = 5.02$ TeV. In 0-10% central collisions, the $I_{\rm AA}$ indicates a hint of enhancement of associated-particle yields with $p_{\rm T} < 3$ GeV/$c$ on the near side, and a suppression of yields with $p_{\rm T} > 4$ GeV/$c$ on the away side. The $I_{\rm AA}$ for electron triggers from heavy-flavor hadron decays is compared with that for light-flavor and strange-particle triggers to investigate the dependence on different fragmentation processes and parton-medium dynamics, and is found to be the same within uncertainties. |
| 2025-07-17 | [$\overlineÎ£^{\pm}$ production in pp and p-Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02 TeV with ALICE](http://arxiv.org/abs/2507.13183v1) | ALICE Collaboration | The transverse momentum spectra and integrated yields of $\overline{\Sigma}^{\pm}$ have been measured in pp and p-Pb collisions at $\sqrt{s_{\mathrm{NN}}} = 5.02$ TeV with the ALICE experiment. Measurements are performed via the newly accessed decay channel $\overline{\Sigma}^{\pm} \rightarrow {\rm\overline{n}}\pi^{\pm}$. A new method of antineutron reconstruction with the PHOS electromagnetic spectrometer is developed and applied to this analysis. The $p_{\rm T}$ spectra of $\overline{\Sigma}^{\pm}$ are measured in the range $0.5 < p_{\rm T} < 3$ GeV/$c$ and compared to predictions of the PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4 models. The EPOS LHC and EPOS4 models provide the best descriptions of the measured spectra both in pp and p-Pb collisions, while models which do not account for multiparton interactions provide a considerably worse description at high $p_{\rm T}$. The total yields of $\overline{\Sigma}^{\pm}$ in both pp and p-Pb collisions are compared to predictions of the Thermal-FIST model and dynamical models PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4. All models reproduce the total yields in both colliding systems within uncertainties. The nuclear modification factors $R_{\rm pPb}$ for both $\overline{\Sigma}^{+}$ and $\overline{\Sigma}^{-}$ are evaluated and compared to those of protons, $\Lambda$ and $\Xi$ hyperons, and predictions of EPOS LHC and EPOS4 models. No deviations of $R_{\rm pPb}$ for $\overline{\Sigma}^{\pm}$ from the model predictions or measurements for other hadrons are found within uncertainties. |
| 2025-07-17 | [The Time-Energy Principle in Algebraic Geometry](http://arxiv.org/abs/2507.13134v1) | Renaud Gauthier | We consider the time-energy uncertainty principle from Quantum Mechanics and provide its Algebro-Geometric interpretation within the context of stacks. |
| 2025-07-17 | [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](http://arxiv.org/abs/2507.13092v1) | Hyo-Jeong Jang, Hye-Bin Shin et al. | Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications. |
| 2025-07-16 | [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](http://arxiv.org/abs/2507.12461v1) | Trong-Thang Pham, Anh Nguyen et al. | Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets. |
| 2025-07-16 | [Precision measurement of the ${\itÎž}_b^0$ baryon lifetime](http://arxiv.org/abs/2507.12402v1) | LHCb collaboration, R. Aaij et al. | A sample of $pp$ collision data, corresponding to an integrated luminosity of 5.4 fb$^{-1}$ and collected by the LHCb experiment during LHC Run 2, is used to measure the ratio of the lifetime of the ${\it{\Xi}}_b^0$ baryon to that of the ${\it{\Lambda}}_b^0$ baryon, $r_{\tau}\equiv\tau_{{\it{\Xi}}_b^0}/\tau_{{\it{\Lambda}}_b^0}$. The value ${r_{\tau}^{\rm Run\,2}=1.004\pm0.009\pm0.006}$ is obtained, where the first uncertainty is statistical and the second systematic. This value is averaged with the corresponding value from Run 1 to obtain ${r_{\tau} = 1.004\pm0.008\pm0.005}$. Multiplying by the known value of the ${\it{\Lambda}}_b^0$ lifetime yields ${{\tau_{{\it{\Xi}}_b^0}} = 1.475\pm0.012\pm0.008\pm0.009~{\rm ps}}$, where the last uncertainty is due to the limited knowledge of the ${\it{\Lambda}}_b^0$ lifetime. This measurement improves the precision of the current world average of the ${\it{\Xi}}_b^0$ lifetime by about a factor of two, and is in good agreement with the most recent theoretical predictions. |
| 2025-07-16 | [Surrogate modeling for uncertainty quantification in nonlinear dynamics](http://arxiv.org/abs/2507.12358v1) | S. Marelli, S. SchÃ¤r et al. | Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use. |
| 2025-07-16 | [The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models](http://arxiv.org/abs/2507.12335v1) | Morgan Deal, GaÃ«l Buldgen et al. | The transport of chemical elements in stellar interiors is one of the greatest sources of uncertainties of solar and stellar modelling. The Sun, with its exquisite spectroscopic, helioseismic and neutrino observations, offers a prime environment to test the prescriptions used for both microscopic and macroscopic transport processes. We study in detail the impact of various formalisms for atomic diffusion on helioseismic constraints in both CLES (Scuflaire et al., 2008a) and Cesam2k2 (Morel and Lebreton 2008; Marques et al. 2013; Deal et al. 2018) models and compare both codes in detail. Moreover, due to the inability of standard models using microscopic diffusion to reproduce light element depletion in the Sun (Li, Be), another efficient process must be included to reproduce these constraints (rotation-induced: Eggenberger et al. 2022, overshooting -- or penetrative convection -- below the convective envelope: Th\'evenin et al. 2017, or ad hoc turbulence: Lebreton and Maeder 1987; Richer, Michaud, and Turcotte 2000). However, introducing such an extra mixing leads to issues with the CNO neutrino fluxes (see Buldgen et al. 2023), which seem to be systematically lower than the Borexino observations (Appel et al., 2022. Another key aspect to consider when reconciling models with neutrino fluxes is the impact of electronic screening (Mussack and D\"appen, 2011). |
| 2025-07-16 | [Uncertainty and entropies of classical channels](http://arxiv.org/abs/2507.12310v1) | Takla Nateeboon | In this thesis, I studied a mathematical development to define and quantify the uncertainty inherent in classical channels. This thesis starts with the introduction and background on how to formally think about uncertainty in the domain of classical states. The concept of probability vector majorization and its variants, relative majorization and conditional majorization, are reviewed. This thesis introduces three conceptually distinct approaches to formalize the notion of uncertainty inherent in classical channels. These three approaches define the same preordering on the domain of classical channels, leading to characterizations from many perspectives. With the solid foundation of uncertainty comparison, classical channel entropy is then defined to be an additive monotone with respect to the majorization relation. The well-known entropies in the domain of classical states are uniquely extended to the domain of channels via the optimal extensions, providing not only a solid foundation but also the quantifiers of uncertainty inherent in classical channels. |
| 2025-07-16 | [Forecasting Climate Policy Uncertainty: Evidence from the United States](http://arxiv.org/abs/2507.12276v1) | Donia Besher, Anirban Sengupta et al. | Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers strive to balance economic growth with environmental goals. High levels of CPU can slow down investments in green technologies, make regulatory planning more difficult, and increase public resistance to climate reforms, especially during times of economic stress. This study addresses the challenge of forecasting the US CPU index by building the Bayesian Structural Time Series (BSTS) model with a large set of covariates, including economic indicators, financial cycle data, and public sentiments captured through Google Trends. The key strength of the BSTS model lies in its ability to efficiently manage a large number of covariates through its dynamic feature selection mechanism based on the spike-and-slab prior. To validate the effectiveness of the selected features of the BSTS model, an impulse response analysis is performed. The results show that macro-financial shocks impact CPU in different ways over time. Numerical experiments are performed to evaluate the performance of the BSTS model with exogenous variables on the US CPU dataset over different forecasting horizons. The empirical results confirm that BSTS consistently outperforms classical and deep learning frameworks, particularly for semi-long-term and long-term forecasts. |
| 2025-07-16 | [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](http://arxiv.org/abs/2507.12262v1) | Zachary James, Joseph Guinness | Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset. |
| 2025-07-16 | [What are we talking about when we discuss the Born-Oppenheimer approximation?](http://arxiv.org/abs/2507.12223v1) | Olimpia Lombardi, Sebastian Fortin et al. | Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a comprehensive article examining the Born-Oppenheimer Approximation (BOA). Their central objective is to challenge our position on the matter-namely, that the BOA incorporates a classical assumption incompatible with the Heisenberg Uncertainty Principle. In contrast, HLT contend that the BOA involves no such classical assumption and, as a result, supports the view that chemistry can be reduced to physics. The purpose of this paper is to offer a critical analysis of the HLT article and to clarify why we consider their arguments unpersuasive. |
| 2025-07-16 | [Explainable Evidential Clustering](http://arxiv.org/abs/2507.12192v1) | Victor F. Lopes de Souza, Karima Bakhti et al. | Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of "tolerable" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time. |
| 2025-07-16 | [Learning, fast and slow: a two-fold algorithm for data-based model adaptation](http://arxiv.org/abs/2507.12187v1) | Laura Boca de Giuli, Alessio La Bella et al. | This article addresses the challenge of adapting data-based models over time. We propose a novel two-fold modelling architecture designed to correct plant-model mismatch caused by two types of uncertainty. Out-of-domain uncertainty arises when the system operates under conditions not represented in the initial training dataset, while in-domain uncertainty results from real-world variability and flaws in the model structure or training process. To handle out-of-domain uncertainty, a slow learning component, inspired by the human brain's slow thinking process, learns system dynamics under unexplored operating conditions, and it is activated only when a monitoring strategy deems it necessary. This component consists of an ensemble of models, featuring (i) a combination rule that weights individual models based on the statistical proximity between their training data and the current operating condition, and (ii) a monitoring algorithm based on statistical control charts that supervises the ensemble's reliability and triggers the offline training and integration of a new model when a new operating condition is detected. To address in-domain uncertainty, a fast learning component, inspired by the human brain's fast thinking process, continuously compensates in real time for the mismatch of the slow learning model. This component is implemented as a Gaussian process (GP) model, trained online at each iteration using recent data while discarding older samples. The proposed methodology is tested on a benchmark energy system referenced in the literature, demonstrating that the combined use of slow and fast learning components improves model accuracy compared to standard adaptation approaches. |
| 2025-07-15 | [Canonical Bayesian Linear System Identification](http://arxiv.org/abs/2507.11535v1) | Andrey Bryutkin, Matthew E. Levine et al. | Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data. |
| 2025-07-15 | [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](http://arxiv.org/abs/2507.11488v1) | Pakizar Shamoi, Nuray Toganas et al. | Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical. |
| 2025-07-15 | [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](http://arxiv.org/abs/2507.11420v1) | Mingcong Li | Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies. |
| 2025-07-15 | [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](http://arxiv.org/abs/2507.11413v1) | Christos N. Efrem, Ioannis Krikidis | We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average. |
| 2025-07-15 | [Towards NNLO QCD predictions for off-shell top-quark pair production and decays](http://arxiv.org/abs/2507.11410v1) | Luca Buonocore, Massimiliano Grazzini et al. | We consider QCD radiative corrections to $W^+W^-b {\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\Gamma_t\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\%$ level $\unicode{x2013}$ significantly smaller than the $5\%$ residual perturbative uncertainties. |
| 2025-07-15 | [Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions](http://arxiv.org/abs/2507.11394v1) | Syed Afrid Jahan, Hendrik Roch et al. | We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\sqrt{s_\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions. |
| 2025-07-15 | [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](http://arxiv.org/abs/2507.11385v1) | George D. Pasparakis, Ioannis A. Kougioumtzoglou et al. | A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics. |
| 2025-07-15 | [The miniJPAS survey quasar selection V: combined algorithm](http://arxiv.org/abs/2507.11380v1) | Ignasi PÃ©rez-RÃ fols, L. Raul Abramo et al. | Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\sigma_{\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\sigma_{\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration. |
| 2025-07-15 | [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](http://arxiv.org/abs/2507.11357v1) | Emile van Krieken, Pasquale Minervini et al. | The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons. |
| 2025-07-15 | [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](http://arxiv.org/abs/2507.11352v1) | Yunhao Yang, Neel P. Bhatt et al. | Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics. |
| 2025-07-14 | [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](http://arxiv.org/abs/2507.10510v1) | Jiangkai Wu, Zhiyuan Ren et al. | AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. |
| 2025-07-14 | [Referential ambiguity and clarification requests: comparing human and LLM behaviour](http://arxiv.org/abs/2507.10445v1) | Chris Madge, Matthew Purver et al. | In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy. |
| 2025-07-14 | [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](http://arxiv.org/abs/2507.10425v1) | Alvaro H. C. Correia, Christos Louizos | Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift. |
| 2025-07-14 | [Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions](http://arxiv.org/abs/2507.10377v1) | T. Dumont, A. Bonhomme et al. | The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\alpha},{\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\alpha},{\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\alpha},{\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition. |
| 2025-07-14 | [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](http://arxiv.org/abs/2507.10355v1) | Bo Jiang, Xueyang Ze et al. | Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach. |
| 2025-07-14 | [Gaussian Process Methods for Very Large Astrometric Data Sets](http://arxiv.org/abs/2507.10317v1) | Timothy Hapitas, Lawrence M. Widrow et al. | We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics. |
| 2025-07-14 | [How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models](http://arxiv.org/abs/2507.10314v1) | Nina Mackensen, Nicole Reindl et al. | PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\text{RX J}0122.9\text{ -}7521$. We find $T_\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\text{RX J}0122.9\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\text{RX J}0122.9\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\text{RX J}0122.9\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\text{RX J}0122.9\text{ -}7521$. |
| 2025-07-14 | [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](http://arxiv.org/abs/2507.10291v1) | Kristine V. Ung, Connor A. Roncaioli et al. | The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\mathrm{^{1}E}$ singlet state in the $\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\mathrm{^{1}E}$ into the neutral $\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited. |
| 2025-07-14 | [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](http://arxiv.org/abs/2507.10201v1) | Gleb Shishaev, Vasily Demyanov et al. | The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure. |
| 2025-07-14 | [Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture](http://arxiv.org/abs/2507.10166v1) | Filip Surmaa, Anahita Jamshidnejad | This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems. |
| 2025-03-10 | [Controllable 3D Outdoor Scene Generation via Scene Graphs](http://arxiv.org/abs/2503.07152v1) | Yuheng Liu, Xinke Li et al. | Three-dimensional scene generation is crucial in computer vision, with applications spanning autonomous driving, gaming and the metaverse. Current methods either lack user control or rely on imprecise, non-intuitive conditions. In this work, we propose a method that uses, scene graphs, an accessible, user friendly control format to generate outdoor 3D scenes. We develop an interactive system that transforms a sparse scene graph into a dense BEV (Bird's Eye View) Embedding Map, which guides a conditional diffusion model to generate 3D scenes that match the scene graph description. During inference, users can easily create or modify scene graphs to generate large-scale outdoor scenes. We create a large-scale dataset with paired scene graphs and 3D semantic scenes to train the BEV embedding and diffusion models. Experimental results show that our approach consistently produces high-quality 3D urban scenes closely aligned with the input scene graphs. To the best of our knowledge, this is the first approach to generate 3D outdoor scenes conditioned on scene graphs. |
| 2024-09-18 | [Generation of Uncertainty-Aware Emergent Concepts in Factorized 3D Scene Graphs via Graph Neural Networks](http://arxiv.org/abs/2409.11972v2) | Jose Andres Millan-Romera, Muhammad Shaheer et al. | Enabling robots to autonomously discover emergent spatial concepts (e.g., rooms) from primitive geometric observations (e.g., planar surfaces) within 3D Scene Graphs is essential for robust indoor navigation and mapping. These graphs provide a hierarchical metric-semantic representation in which such concepts are organized. To further enhance graph-SLAM performance, Factorized 3D Scene Graphs incorporate these concepts as optimization factors that constrain relative geometry and enforce global consistency. However, both stages of this process remain largely manual: concepts are typically derived using hand-crafted, concept-specific heuristics, while factors and their covariances are likewise manually designed. This reliance on manual specification limits generalization across diverse environments and scalability to new concept classes. This paper presents, for the first time, a learning-based method to generate online spatial emergent concepts as optimizable factors within a SLAM backend, reducing the need to handcraft both concept generation and the definition of their corresponding factors and covariances. In both simulated and real indoor scenarios, our approach improves complex concept detection by 20.7% and 5.3%, trajectory estimation by 19.2%, and map reconstruction by 12.3% and 3.8%, respectively, highlighting the benefits of this integration for robust and adaptive spatial understanding. |
| 2023-12-15 | [nuScenes Knowledge Graph -- A comprehensive semantic representation of traffic scenes for trajectory prediction](http://arxiv.org/abs/2312.09676v1) | Leon Mlodzian, Zhigang Sun et al. | Trajectory prediction in traffic scenes involves accurately forecasting the behaviour of surrounding vehicles. To achieve this objective it is crucial to consider contextual information, including the driving path of vehicles, road topology, lane dividers, and traffic rules. Although studies demonstrated the potential of leveraging heterogeneous context for improving trajectory prediction, state-of-the-art deep learning approaches still rely on a limited subset of this information. This is mainly due to the limited availability of comprehensive representations. This paper presents an approach that utilizes knowledge graphs to model the diverse entities and their semantic connections within traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a knowledge graph for the nuScenes dataset, that models explicitly all scene participants and road elements, as well as their semantic and spatial relationships. To facilitate the usage of the nSKG via graph neural networks for trajectory prediction, we provide the data in a format, ready-to-use by the PyG library. All artefacts can be found here: https://github.com/boschresearch/nuScenes_Knowledge_Graph |
| 2023-11-28 | [Panoptic Video Scene Graph Generation](http://arxiv.org/abs/2311.17058v1) | Jingkang Yang, Wenxuan Peng et al. | Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG relates to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects grounded with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG to miss key details crucial for comprehensive video understanding. In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding. To advance research in this new area, we contribute the PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with a total of 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs. We also provide a variety of baseline methods and share useful design practices for future work. |
| 2023-08-10 | [Informative Scene Graph Generation via Debiasing](http://arxiv.org/abs/2308.05286v2) | Lianli Gao, Xinyu Lyu et al. | Scene graph generation aims to detect visual relationship triplets, (subject, predicate, object). Due to biases in data, current models tend to predict common predicates, e.g. "on" and "at", instead of informative ones, e.g. "standing on" and "looking at". This tendency results in the loss of precise information and overall performance. If a model only uses "stone on road" rather than "stone blocking road" to describe an image, it may be a grave misunderstanding. We argue that this phenomenon is caused by two imbalances: semantic space level imbalance and training sample level imbalance. For this problem, we propose DB-SGG, an effective framework based on debiasing but not the conventional distribution fitting. It integrates two components: Semantic Debiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD utilizes a confusion matrix and a bipartite graph to construct predicate relationships. BPL adopts a random undersampling strategy and an ambiguity removing strategy to focus on informative predicates. Benefiting from the model-agnostic process, our method can be easily applied to SGG models and outperforms Transformer by 136.3%, 119.5%, and 122.6% on mR@20 at three SGG sub-tasks on the SGG-VG dataset. Our method is further verified on another complex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph retrieval and image captioning). |
| 2023-08-09 | [Generalized Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.04802v2) | Xinyu Lyu, Lianli Gao et al. | Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. Hence, we introduce a novel research problem: Generalized Unbiased Scene Graph Generation (G-USGG), which takes into account both predicate-level and concept-level imbalance. To the end, we propose the Multi-Concept Learning (MCL) framework, which ensures a balanced learning process across rare/ uncommon/ common concepts. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. It then effectively learns concept-prototypes by applying the Concept Regularization (CR) technique. Furthermore, to achieve balanced learning over different concepts, we introduce the Balanced Prototypical Memory (BPM), which guides SGG models to generate balanced representations for concept-prototypes. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. |
| 2023-08-09 | [Bird's-Eye-View Scene Graph for Vision-Language Navigation](http://arxiv.org/abs/2308.04758v2) | Rui Liu, Xiaohan Wang et al. | Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human instructions, has shown great advances. However, current agents are built upon panoramic observations, which hinders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment under the supervision of 3D detection. During navigation, BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accurate action prediction. Our approach significantly outperforms state-of-the-art methods on REVERIE, R2R, and R4R, showing the potential of BEV perception in VLN. |
| 2020-11-20 | [Neural Scene Graphs for Dynamic Scenes](http://arxiv.org/abs/2011.10379v3) | Julian Ost, Fahim Mannan et al. | Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses. |
| 2020-02-27 | [Unbiased Scene Graph Generation from Biased Training](http://arxiv.org/abs/2002.11949v4) | Kaihua Tang, Yulei Niu et al. | Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse "human walk on / sit on / lay on beach" into "human on beach". Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., "person read book" rather than "eat") and bad long-tailed bias (e.g., "near" dominating "behind / in front of"). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect (TDE) as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods. |
| 2019-09-19 | [Triplet-Aware Scene Graph Embeddings](http://arxiv.org/abs/1909.09256v1) | Brigit Schroeder, Subarna Tripathi et al. | Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model. |

</details>

<!-- HISTORICAL_PAPERS_END -->
---



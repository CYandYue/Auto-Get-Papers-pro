## ðŸ“° Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-07-16 | [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](http://arxiv.org/abs/2507.12461v1) | Trong-Thang Pham, Anh Nguyen et al. | Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets. |
| 2025-07-16 | [Precision measurement of the ${\itÎž}_b^0$ baryon lifetime](http://arxiv.org/abs/2507.12402v1) | LHCb collaboration, R. Aaij et al. | A sample of $pp$ collision data, corresponding to an integrated luminosity of 5.4 fb$^{-1}$ and collected by the LHCb experiment during LHC Run 2, is used to measure the ratio of the lifetime of the ${\it{\Xi}}_b^0$ baryon to that of the ${\it{\Lambda}}_b^0$ baryon, $r_{\tau}\equiv\tau_{{\it{\Xi}}_b^0}/\tau_{{\it{\Lambda}}_b^0}$. The value ${r_{\tau}^{\rm Run\,2}=1.004\pm0.009\pm0.006}$ is obtained, where the first uncertainty is statistical and the second systematic. This value is averaged with the corresponding value from Run 1 to obtain ${r_{\tau} = 1.004\pm0.008\pm0.005}$. Multiplying by the known value of the ${\it{\Lambda}}_b^0$ lifetime yields ${{\tau_{{\it{\Xi}}_b^0}} = 1.475\pm0.012\pm0.008\pm0.009~{\rm ps}}$, where the last uncertainty is due to the limited knowledge of the ${\it{\Lambda}}_b^0$ lifetime. This measurement improves the precision of the current world average of the ${\it{\Xi}}_b^0$ lifetime by about a factor of two, and is in good agreement with the most recent theoretical predictions. |
| 2025-07-16 | [Surrogate modeling for uncertainty quantification in nonlinear dynamics](http://arxiv.org/abs/2507.12358v1) | S. Marelli, S. SchÃ¤r et al. | Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use. |
| 2025-07-16 | [The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models](http://arxiv.org/abs/2507.12335v1) | Morgan Deal, GaÃ«l Buldgen et al. | The transport of chemical elements in stellar interiors is one of the greatest sources of uncertainties of solar and stellar modelling. The Sun, with its exquisite spectroscopic, helioseismic and neutrino observations, offers a prime environment to test the prescriptions used for both microscopic and macroscopic transport processes. We study in detail the impact of various formalisms for atomic diffusion on helioseismic constraints in both CLES (Scuflaire et al., 2008a) and Cesam2k2 (Morel and Lebreton 2008; Marques et al. 2013; Deal et al. 2018) models and compare both codes in detail. Moreover, due to the inability of standard models using microscopic diffusion to reproduce light element depletion in the Sun (Li, Be), another efficient process must be included to reproduce these constraints (rotation-induced: Eggenberger et al. 2022, overshooting -- or penetrative convection -- below the convective envelope: Th\'evenin et al. 2017, or ad hoc turbulence: Lebreton and Maeder 1987; Richer, Michaud, and Turcotte 2000). However, introducing such an extra mixing leads to issues with the CNO neutrino fluxes (see Buldgen et al. 2023), which seem to be systematically lower than the Borexino observations (Appel et al., 2022. Another key aspect to consider when reconciling models with neutrino fluxes is the impact of electronic screening (Mussack and D\"appen, 2011). |
| 2025-07-16 | [Uncertainty and entropies of classical channels](http://arxiv.org/abs/2507.12310v1) | Takla Nateeboon | In this thesis, I studied a mathematical development to define and quantify the uncertainty inherent in classical channels. This thesis starts with the introduction and background on how to formally think about uncertainty in the domain of classical states. The concept of probability vector majorization and its variants, relative majorization and conditional majorization, are reviewed. This thesis introduces three conceptually distinct approaches to formalize the notion of uncertainty inherent in classical channels. These three approaches define the same preordering on the domain of classical channels, leading to characterizations from many perspectives. With the solid foundation of uncertainty comparison, classical channel entropy is then defined to be an additive monotone with respect to the majorization relation. The well-known entropies in the domain of classical states are uniquely extended to the domain of channels via the optimal extensions, providing not only a solid foundation but also the quantifiers of uncertainty inherent in classical channels. |

<!-- LATEST_PAPERS_END -->


<!-- HISTORICAL_PAPERS_START -->

<details>
<summary>ðŸ“š View Historical Papers (25 entries)</summary>



| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-07-16 | [Forecasting Climate Policy Uncertainty: Evidence from the United States](http://arxiv.org/abs/2507.12276v1) | Donia Besher, Anirban Sengupta et al. | Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers strive to balance economic growth with environmental goals. High levels of CPU can slow down investments in green technologies, make regulatory planning more difficult, and increase public resistance to climate reforms, especially during times of economic stress. This study addresses the challenge of forecasting the US CPU index by building the Bayesian Structural Time Series (BSTS) model with a large set of covariates, including economic indicators, financial cycle data, and public sentiments captured through Google Trends. The key strength of the BSTS model lies in its ability to efficiently manage a large number of covariates through its dynamic feature selection mechanism based on the spike-and-slab prior. To validate the effectiveness of the selected features of the BSTS model, an impulse response analysis is performed. The results show that macro-financial shocks impact CPU in different ways over time. Numerical experiments are performed to evaluate the performance of the BSTS model with exogenous variables on the US CPU dataset over different forecasting horizons. The empirical results confirm that BSTS consistently outperforms classical and deep learning frameworks, particularly for semi-long-term and long-term forecasts. |
| 2025-07-16 | [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](http://arxiv.org/abs/2507.12262v1) | Zachary James, Joseph Guinness | Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset. |
| 2025-07-16 | [What are we talking about when we discuss the Born-Oppenheimer approximation?](http://arxiv.org/abs/2507.12223v1) | Olimpia Lombardi, Sebastian Fortin et al. | Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a comprehensive article examining the Born-Oppenheimer Approximation (BOA). Their central objective is to challenge our position on the matter-namely, that the BOA incorporates a classical assumption incompatible with the Heisenberg Uncertainty Principle. In contrast, HLT contend that the BOA involves no such classical assumption and, as a result, supports the view that chemistry can be reduced to physics. The purpose of this paper is to offer a critical analysis of the HLT article and to clarify why we consider their arguments unpersuasive. |
| 2025-07-16 | [Explainable Evidential Clustering](http://arxiv.org/abs/2507.12192v1) | Victor F. Lopes de Souza, Karima Bakhti et al. | Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of "tolerable" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time. |
| 2025-07-16 | [Learning, fast and slow: a two-fold algorithm for data-based model adaptation](http://arxiv.org/abs/2507.12187v1) | Laura Boca de Giuli, Alessio La Bella et al. | This article addresses the challenge of adapting data-based models over time. We propose a novel two-fold modelling architecture designed to correct plant-model mismatch caused by two types of uncertainty. Out-of-domain uncertainty arises when the system operates under conditions not represented in the initial training dataset, while in-domain uncertainty results from real-world variability and flaws in the model structure or training process. To handle out-of-domain uncertainty, a slow learning component, inspired by the human brain's slow thinking process, learns system dynamics under unexplored operating conditions, and it is activated only when a monitoring strategy deems it necessary. This component consists of an ensemble of models, featuring (i) a combination rule that weights individual models based on the statistical proximity between their training data and the current operating condition, and (ii) a monitoring algorithm based on statistical control charts that supervises the ensemble's reliability and triggers the offline training and integration of a new model when a new operating condition is detected. To address in-domain uncertainty, a fast learning component, inspired by the human brain's fast thinking process, continuously compensates in real time for the mismatch of the slow learning model. This component is implemented as a Gaussian process (GP) model, trained online at each iteration using recent data while discarding older samples. The proposed methodology is tested on a benchmark energy system referenced in the literature, demonstrating that the combined use of slow and fast learning components improves model accuracy compared to standard adaptation approaches. |
| 2025-07-15 | [Canonical Bayesian Linear System Identification](http://arxiv.org/abs/2507.11535v1) | Andrey Bryutkin, Matthew E. Levine et al. | Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data. |
| 2025-07-15 | [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](http://arxiv.org/abs/2507.11488v1) | Pakizar Shamoi, Nuray Toganas et al. | Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical. |
| 2025-07-15 | [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](http://arxiv.org/abs/2507.11420v1) | Mingcong Li | Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies. |
| 2025-07-15 | [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](http://arxiv.org/abs/2507.11413v1) | Christos N. Efrem, Ioannis Krikidis | We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average. |
| 2025-07-15 | [Towards NNLO QCD predictions for off-shell top-quark pair production and decays](http://arxiv.org/abs/2507.11410v1) | Luca Buonocore, Massimiliano Grazzini et al. | We consider QCD radiative corrections to $W^+W^-b {\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\Gamma_t\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\%$ level $\unicode{x2013}$ significantly smaller than the $5\%$ residual perturbative uncertainties. |
| 2025-07-15 | [Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions](http://arxiv.org/abs/2507.11394v1) | Syed Afrid Jahan, Hendrik Roch et al. | We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\sqrt{s_\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions. |
| 2025-07-15 | [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](http://arxiv.org/abs/2507.11385v1) | George D. Pasparakis, Ioannis A. Kougioumtzoglou et al. | A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics. |
| 2025-07-15 | [The miniJPAS survey quasar selection V: combined algorithm](http://arxiv.org/abs/2507.11380v1) | Ignasi PÃ©rez-RÃ fols, L. Raul Abramo et al. | Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\sigma_{\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\sigma_{\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration. |
| 2025-07-15 | [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](http://arxiv.org/abs/2507.11357v1) | Emile van Krieken, Pasquale Minervini et al. | The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons. |
| 2025-07-15 | [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](http://arxiv.org/abs/2507.11352v1) | Yunhao Yang, Neel P. Bhatt et al. | Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics. |
| 2025-07-14 | [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](http://arxiv.org/abs/2507.10510v1) | Jiangkai Wu, Zhiyuan Ren et al. | AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. |
| 2025-07-14 | [Referential ambiguity and clarification requests: comparing human and LLM behaviour](http://arxiv.org/abs/2507.10445v1) | Chris Madge, Matthew Purver et al. | In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy. |
| 2025-07-14 | [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](http://arxiv.org/abs/2507.10425v1) | Alvaro H. C. Correia, Christos Louizos | Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift. |
| 2025-07-14 | [Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions](http://arxiv.org/abs/2507.10377v1) | T. Dumont, A. Bonhomme et al. | The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\alpha},{\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\alpha},{\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\alpha},{\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition. |
| 2025-07-14 | [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](http://arxiv.org/abs/2507.10355v1) | Bo Jiang, Xueyang Ze et al. | Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach. |
| 2025-07-14 | [Gaussian Process Methods for Very Large Astrometric Data Sets](http://arxiv.org/abs/2507.10317v1) | Timothy Hapitas, Lawrence M. Widrow et al. | We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics. |
| 2025-07-14 | [How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models](http://arxiv.org/abs/2507.10314v1) | Nina Mackensen, Nicole Reindl et al. | PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\text{RX J}0122.9\text{ -}7521$. We find $T_\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\text{RX J}0122.9\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\text{RX J}0122.9\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\text{RX J}0122.9\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\text{RX J}0122.9\text{ -}7521$. |
| 2025-07-14 | [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](http://arxiv.org/abs/2507.10291v1) | Kristine V. Ung, Connor A. Roncaioli et al. | The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\mathrm{^{1}E}$ singlet state in the $\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\mathrm{^{1}E}$ into the neutral $\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited. |
| 2025-07-14 | [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](http://arxiv.org/abs/2507.10201v1) | Gleb Shishaev, Vasily Demyanov et al. | The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure. |
| 2025-07-14 | [Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture](http://arxiv.org/abs/2507.10166v1) | Filip Surmaa, Anahita Jamshidnejad | This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems. |

</details>

<!-- HISTORICAL_PAPERS_END -->
---



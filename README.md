## ðŸ“° Latest arXiv Papers (Auto-Updated)
<!-- LATEST_PAPERS_START -->


| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-10-20 | [Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks](http://arxiv.org/abs/2510.17798v1) | Samuel Talkington, Cameron Khanpour et al. | This paper presents probabilistic bounds for the spectrum of the admittance matrix and classical linear power flow models under uncertain network parameters; for example, probabilistic line contingencies. Our proposed approach imports tools from probability theory, such as concentration inequalities for random matrices with independent entries. It yields error bounds for common approximations of the AC power flow equations under parameter uncertainty, including the DC and LinDistFlow approximations. |
| 2025-10-20 | [QUIJOTE scientific results XIX. New constraints on the synchrotron spectral index using a semi-blind component separation method](http://arxiv.org/abs/2510.17761v1) | Debabrata Adak, J. A. RubiÃ±o-MartÃ­n et al. | We introduce a novel approach to estimate the spectral index, $\beta_s$, of polarised synchrotron emission, combining the moment expansion of CMB foregrounds and the constrained-ILC method. We reconstruct the maps of the first two synchrotron moments, combining multi-frequency data, and apply the `T-T plot' technique between two moment maps to estimate the synchrotron spectral index. This approach offers a new technique for mapping the foreground spectral parameters, complementing the model-based parametric component separation methods. Applying this technique, we derive a new constraint on the spectral index of polarised synchrotron emission using QUIJOTE MFI wide-survey 11 and 13 GHz data, Wilkinson Microwave Anisotropy Probe (WMAP) data at K and Ka bands, and Planck LFI 30 GHz data. In the Galactic plane and North Polar Spur regions, we obtain an inverse-variance-weighted mean synchrotron index of $\beta_s = -3.11$ with a standard deviation of $0.21$ due to intrinsic scatter, consistent with previous results based on parametric methods using the same dataset. We find that the inverse-variance-weighted mean spectral index, including both statistical and systematic uncertainties, is $\beta_s^{\rm plane} = -3.05 \pm 0.01$ in the Galactic plane and $\beta_s^{\rm high\text{-}lat} = -3.13 \pm 0.02$ at high latitudes, indicating a moderate steepening of the spectral index from low to high Galactic latitudes. Our analysis indicates that, within the current upper limit on the AME polarisation fraction, our results are not subject to any appreciable bias. Furthermore, we infer the spectral index over the entire QUIJOTE survey region, partitioning the sky into 21 patches. This technique can be further extended to constrain the synchrotron spectral curvature by reconstructing higher-order moments when better-quality data become available. |
| 2025-10-20 | [Enabling Fine-Grained Operating Points for Black-Box LLMs](http://arxiv.org/abs/2510.17727v1) | Ege Beyazit, KL Navaneet et al. | Black-box Large Language Models (LLMs) provide practical and accessible alternatives to other machine learning methods, as they require minimal labeled data and machine learning expertise to develop solutions for various decision making problems. However, for applications that need operating with constraints on specific metrics (e.g., precision $\geq$ 95%), decision making with black-box LLMs remains unfavorable, due to their low numerical output cardinalities. This results in limited control over their operating points, preventing fine-grained adjustment of their decision making behavior. In this paper, we study using black-box LLMs as classifiers, focusing on efficiently improving their operational granularity without performance loss. Specifically, we first investigate the reasons behind their low-cardinality numerical outputs and show that they are biased towards generating rounded but informative verbalized probabilities. Then, we experiment with standard prompt engineering, uncertainty estimation and confidence elicitation techniques, and observe that they do not effectively improve operational granularity without sacrificing performance or increasing inference cost. Finally, we propose efficient approaches to significantly increase the number and diversity of available operating points. Our proposed approaches provide finer-grained operating points and achieve comparable to or better performance than the benchmark methods across 11 datasets and 3 LLMs. |
| 2025-10-20 | [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](http://arxiv.org/abs/2510.17690v1) | Xihong Su | This dissertation makes three main contributions. First, We identify a new connection between policy gradient and dynamic programming in MMDPs and propose the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov policy that maximizes the discounted return averaged over the uncertain models. CADP adjusts model weights iteratively to guarantee monotone policy improvements to a local maximum. Second, We establish sufficient and necessary conditions for the exponential ERM Bellman operator to be a contraction and prove the existence of stationary deterministic optimal policies for ERM-TRC and EVaR-TRC. We also propose exponential value iteration, policy iteration, and linear programming algorithms for computing optimal stationary policies for ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the optimal risk-averse value functions. The proposed Q-learning algorithms compute the optimal stationary policy for ERM-TRC and EVaR-TRC. |
| 2025-10-20 | [LILO: Bayesian Optimization with Interactive Natural Language Feedback](http://arxiv.org/abs/2510.17671v1) | Katarzyna Kobalczyk, Zhiyuan Jerry Lin et al. | For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space. Unlike preferential BO, which only accepts restricted feedback formats and requires customized models for each domain-specific problem, our approach leverages LLMs to turn varied types of textual feedback into consistent utility signals and to easily include flexible user priors without manual kernel design. At the same time, our method maintains the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes. |

<!-- LATEST_PAPERS_END -->


<!-- HISTORICAL_PAPERS_START -->

<details>
<summary>ðŸ“š View Historical Papers (676 entries)</summary>



| Date       | Title                                      | Authors           | Abstract Summary          |
|------------|--------------------------------------------|-------------------|---------------------------|
| 2025-10-20 | [Space-Time Rate-Splitting Multiple Access for Multibeam LEO Satellite Networks](http://arxiv.org/abs/2510.17625v1) | Jaehyup Seong, Byungju Lee et al. | This paper proposes a novel space-time rate-splitting multiple access (ST-RSMA) framework for multibeam low Earth orbit (LEO) satellite communications (SATCOM) systems, where space-time coding is integrated into the common stream transmission. This design enables full diversity gain in the common stream transmission for all users, regardless of the uncertainty of the channel state information (CSI) and network load conditions, thereby overcoming the performance limitations of conventional RSMA that employs a single beamforming vector for all users. To further enhance performance, we develop a weighted minimum mean square error (WMMSE)-based algorithm tailored to ST-RSMA that jointly optimizes the power allocation for the common stream and the power/beamforming vectors for private streams, aiming to maximize the minimum user rate. Numerical results show that ST-RSMA significantly outperforms conventional RSMA and other multiple access techniques, offering a robust and scalable solution for LEO SATCOM. |
| 2025-10-20 | [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](http://arxiv.org/abs/2510.17614v1) | Praphul Singh, Corey Barrett et al. | Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection. |
| 2025-10-20 | [Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials](http://arxiv.org/abs/2510.17550v1) | Andrew Hall, Duncan Wilson et al. | The main objective of dose finding trials is to find an optimal dose amongst a candidate set for further research. The trial design in oncology proceeds in stages with a decision as to how to treat the next group of patients made at every stage until a final sample size is reached or the trial stopped early.   This work applies a Bayesian decision-theoretic approach to the problem, proposing a new utility function based on both efficacy and toxicity and grounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed framework seeks to better capture real clinical judgements by allowing attitudes to risk to vary when the judgements are of gains or losses, which are defined with respect to an intermediate outcome known as a reference point. We call this method Reference Dependent Decision Theoretic dose finding (R2DT).   A simulation study demonstrates that the framework can perform well and produce good operating characteristics. The simulation results demonstrate that R2DT is better at detecting the optimal dose in scenarios where candidate doses are around minimum acceptable efficacy and maximum acceptable toxicity thresholds.   The proposed framework shows that a flexible utility function, which better captures clinician beliefs, can lead to trials with good operating characteristics, including a high probability of finding the optimal dose. Our work demonstrates proof-of-concept for this framework, which should be evaluated in a broader range of settings. |
| 2025-10-20 | [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](http://arxiv.org/abs/2510.17548v1) | Nisrine Rair, Alban Goupil et al. | Language models are often evaluated with scalar metrics like accuracy, but such measures fail to capture how models internally represent ambiguity, especially when human annotators disagree. We propose a topological perspective to analyze how fine-tuned models encode ambiguity and more generally instances.   Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from topological data analysis, reveals that fine-tuning restructures embedding space into modular, non-convex regions aligned with model predictions, even for highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$ prediction purity, yet alignment with ground-truth labels drops in ambiguous data, surfacing a hidden tension between structural confidence and label uncertainty.   Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry directly uncovering decision regions, boundary collapses, and overconfident clusters. Our findings position Mapper as a powerful diagnostic tool for understanding how models resolve ambiguity. Beyond visualization, it also enables topological metrics that may inform proactive modeling strategies in subjective NLP tasks. |
| 2025-10-20 | [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](http://arxiv.org/abs/2510.17496v1) | Giacomo Camposampiero, Michael Hersche et al. | We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes. |
| 2025-10-17 | [A Unifying Convexification Framework for Chance-Constrained Programs via Bilinear Extended Formulations over a Simplex](http://arxiv.org/abs/2510.15861v1) | Danial Davarnia, Hamed Rahimian | Chance-constrained programming is a widely used framework for decision-making under uncertainty, yet its mixed-integer reformulations involve nonconvex mixing sets with a knapsack constraint, leading to weak relaxations and computational challenges. Most existing approaches for strengthening the relaxations of these sets rely primarily on extensions of a specific class of valid inequalities, limiting both convex hull coverage and the discovery of fundamentally new structures. In this paper, we develop a novel convexification framework that reformulates chance-constrained sets as bilinear sets over a simplex in a lifted space and employs a step-by-step aggregation procedure to derive facet-defining inequalities in the original space of variables. Our approach generalizes and unifies established families of valid inequalities in the literature while introducing new ones that capture substantially larger portions of the convex hull. Main contributions include: (i) the development of a new aggregation-based convexification technique for bilinear sets over a simplex in a lower-dimensional space; (ii) the introduction of a novel bilinear reformulation of mixing sets with a knapsack constraint -- arising from single-row relaxations of chance constraints -- over a simplex, which enables the systematic derivation of strong inequalities in the original variable space; and (iii) the characterization of facet-defining inequalities within a unified framework that contains both existing and new families. Preliminary computational experiments demonstrate that our inequalities describe over 90\% of the facet-defining inequalities of the convex hull of benchmark instances, significantly strengthening existing relaxations and advancing the polyhedral understanding of chance-constrained programs. |
| 2025-10-17 | [Quantum Monte Carlo Calculations of Light Nuclei with Fully Propagated Theoretical Uncertainties](http://arxiv.org/abs/2510.15860v1) | Ryan Curry, Kai Hebeler et al. | We report on the first quantum Monte Carlo calculations of helium isotopes with fully propagated theoretical uncertainties from the interaction to the many-body observables. To achieve this, we build emulators for solutions to the Faddeev equations for the binding energy and Gamow-Teller matrix element of $^3\text{H}$, as well as for auxiliary-field diffusion Monte Carlo calculations of the $^4\text{He}$ charge radius, employing local two- and three-body interactions up to next-to-next-to-leading order in chiral effective field theory. We use these emulators to determine the posterior distributions for all low-energy couplings that appear in the interaction up to this order using Bayesian inference while accounting for theoretical uncertainties. We then build emulators for auxiliary-field diffusion Monte Carlo for helium isotopes and propagate the full posterior distributions to these systems. Our approach serves as a framework for $\textit{ab initio}$ studies of atomic nuclei with consistently treated and correlated theoretical uncertainties. |
| 2025-10-17 | [Bio-inspired Microgrid Management based on Brain's Sensorimotor Gating](http://arxiv.org/abs/2510.15847v1) | Panos C. Papageorgiou, Anastasios E. Giannopoulos et al. | Microgrids are emerging as key enablers of resilient, sustainable, and intelligent power systems, but they continue to face challenges in dynamic disturbance handling, protection coordination, and uncertainty. Recent efforts have explored Brain Emotional Learning (BEL) controllers as bio-inspired solutions for microgrid control. Building on this growing trajectory, this article introduces a new paradigm for Neuro-Microgrids, inspired by the brain's sensorimotor gating mechanisms, specifically the Prepulse Inhibition (PPI) and Prepulse Facilitation (PPF). Sensorimotor gating offers a biological model for selectively suppressing or amplifying responses depending on contextual relevance. By mapping these principles onto the hierarchical control architecture of microgrids, we propose a Sensorimotor Gating-Inspired Neuro-Microgrid (SG-NMG) framework. In this architecture, PPI-like control decisions correspond to protective damping in primary and secondary management of microgrids, whereas PPF-like decisions correspond to adaptive amplification of corrective control actions. The framework is presented through analytical workflow design, neuro-circuitry analogies, and integration with machine learning methods. Finally, open challenges and research directions are outlined, including the mathematical modeling of gating, digital twin validation, and cross-disciplinary collaboration between neuroscience and industrial power systems. The resulting paradigm highlights sensorimotor gating as a promising framework for designing self-protective, adaptive, and resilient microgrids. |
| 2025-10-17 | [From Localization to Discovery: Bayesian Ranking of Electromagnetic Counterparts to Gravitational-Wave Events](http://arxiv.org/abs/2510.15836v1) | Kendall Ackley | The robust association of electromagnetic candidates discovered during follow-up of gravitational-wave alerts is challenging, not only due to the large sky areas and broad distance uncertainties, but also due to the tens to hundreds of unrelated optical transients that are observed per event. We present a Bayesian ranking method to identify electromagnetic counterparts to GW events using only location information. The framework combines three-dimensional gravitational wave skymaps with host-galaxy information, a morphology-aware host association, empirical offset priors, and peculiar velocity corrections. We apply the method to GW170817 where it ranks AT2017gfo as the top candidate and correctly selects NGC\,4993 as the host. The approach is directly applicable to transient candidates with only location information and enables more efficient follow-up with prioritized candidates and leads to more reliable counterpart identification in current and future observing runs. |
| 2025-10-17 | [The kinetic Sunyaev Zeldovich effect as a benchmark for AGN feedback models in hydrodynamical simulations: insights from DESI + ACT](http://arxiv.org/abs/2510.15822v1) | Leah Bigwood, Masaya Yamamoto et al. | Baryonic feedback remains one of the largest uncertainties in cosmological hydrodynamical simulations, with different prescriptions producing divergent predictions for the fraction of gas expelled from halos, the radial extent of the gas expulsion and the impact on large scale matter clustering. We present the first systematic study of the kinetic Sunyaev-Zel'dovich (kSZ) effect across a wide range of simulations (FLAMINGO, ANTILLES, BAHAMAS, SIMBA, FABLE and their variants), and compare them directly to DESI Year 1 + ACT kSZ measurements. We ensure a like-for-like comparison with observations by developing a robust methodology that accounts for the halo mass selection using galaxy-galaxy lensing, cosmic variance, miscentering and satellites, establishing the kSZ effect as a new benchmark for the simulations. We find that fiducial feedback models are disfavoured by >3 sigma, while simulations with more powerful AGN feedback within the FLAMINGO and BAHAMAS suites, as well as SIMBA, reproduce the observed kSZ signal within <2 sigma. We use the ANTILLES simulation suite to demonstrate that the amplitude of the kSZ effect is a strong predictor of matter power spectrum suppression, competitive with baryon fraction metrics. These results establish the kSZ as a critical probe for evaluating feedback physics and for advancing the fidelity of cosmological simulations. |
| 2025-10-17 | [Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction](http://arxiv.org/abs/2510.15780v1) | Alireza Moradi, Mathieu Tanneau et al. | Accurate forecasting is critical for reliable power grid operations, particularly as the share of renewable generation, such as wind and solar, continues to grow. Given the inherent uncertainty and variability in renewable generation, probabilistic forecasts have become essential for informed operational decisions. However, such forecasts frequently suffer from calibration issues, potentially degrading decision-making performance. Building on recent advances in Conformal Predictions, this paper introduces a tailored calibration framework that constructs context-aware calibration sets using a novel weighting scheme. The proposed framework improves the quality of probabilistic forecasts at the site and fleet levels, as demonstrated by numerical experiments on large-scale datasets covering several systems in the United States. The results demonstrate that the proposed approach achieves higher forecast reliability and robustness for renewable energy applications compared to existing baselines. |
| 2025-10-17 | [Multiphysics inversion with variable complexity of receiver-function, surface-wave dispersion and magnetotelluric data reduces uncertainty for lithosphere structure](http://arxiv.org/abs/2510.15779v1) | P. Shahsavari, J. Dettmer et al. | We present a probabilistic multiphysics inversion based on Bayesian inference with trans-dimensional models. We jointly consider magnetotelluric, receiver function, and Rayleigh-wave dispersion data to infer one-dimensional lithospheric structure in the vicinity of Athabasca, Canada. The location is on the North American Craton with a cover of sediments from the Western Canada Sedimentary Basin. The trans-dimensional model uses layer nodes that include parameters that are activated or deactivated based on data information. Furthermore, the number of nodes is based on data information. Hence, the parameterization uncertainty is included in the uncertainty estimates. Furthermore, the layer nodes permit trans-dimensional decoupling such that some discontinuities may be represented by only some of the parameters. In probabilistic multiphysics inversion, it is important that the various data types are weighed objectively. Here, the weights are the data covariance matrices of the various data types. We apply empirical estimation of data covariance matrices and employ hierarchical scaling parameters to reduce the dependence on some assumptions required by the empirical approach. Hence, we account for noise variances and covariances, which is crucial for successful probabilistic multiphysics inversion. The parameter estimates and data covariance matrices are obtained with the reversible-jump Markov chain Monte Carlo algorithm with parallel tempering to enhance the efficiency. Since covariance matrix estimation changes data weights, the estimation process is carried out while samples are not recorded for inference. The results at the Athabasca site fit the data and produce plausible data covariance matrices for the data weights. |
| 2025-10-17 | [Integrating Conductor Health into Dynamic Line Rating and Unit Commitment under Uncertainty](http://arxiv.org/abs/2510.15740v1) | Geon Roh, Jip Kim | Dynamic line rating (DLR) enables greater utilization of existing transmission lines by leveraging real-time weather data. However, the elevated temperature operation (ETO) of conductors under DLR is often overlooked, despite its long-term impact on conductor health. This paper addresses this issue by 1) quantifying depreciation costs associated with ETO and 2) proposing a Conductor Health-Aware Unit Commitment (CHA-UC) that internalizes these costs in operational decisions. The CHA-UC incorporates a robust linear approximation of conductor temperature and integration of expected depreciation costs due to hourly ETO into the objective function. Case studies on the Texas 123-bus backbone test system using NOAA weather data demonstrate that the proposed CHA-UC model reduces the total cost by 0.8% and renewable curtailment by 84%compared to static line rating (SLR), while conventional DLR operation without risk consideration resulted in higher costs due to excessive ETO. Further analysis of the commitment decisions and the line temperature statistics confirms that the CHA-UC achieves safer line flows by shifting generator commitments. Finally, we examine the emergent correlation between wind generation and DLR forecast errors, and show that CHA-UC adaptively manages this effect by relaxing flows for risk-hedging conditions while tightening flows for risk-amplifying ones. |
| 2025-10-17 | [Robust Insurance Pricing and Liquidity Management](http://arxiv.org/abs/2510.15709v1) | Shunzhi Pang | With the rise of emerging risks, model uncertainty poses a fundamental challenge in the insurance industry, making robust pricing a first-order question. This paper investigates how insurers' robustness preferences shape competitive equilibrium in a dynamic insurance market. Insurers optimize their underwriting and liquidity management strategies to maximize shareholder value, leading to equilibrium outcomes that can be analytically derived and numerically solved. Compared to a benchmark without model uncertainty, robust insurance pricing results in significantly higher premiums and equity valuations. Notably, our model yields three novel insights: (1) The minimum, maximum, and admissible range of aggregate capacity all expand, indicating that insurers' liquidity management becomes more conservative. (2) The expected length of the underwriting cycle increases substantially, far exceeding the range commonly reported in earlier empirical studies. (3) While the capacity process remains ergodic in the long run, the stationary density becomes more concentrated in low-capacity states, implying that liquidity-constrained insurers require longer to recover. Together, these findings provide a potential explanation for recent skepticism regarding the empirical evidence of underwriting cycles, suggesting that such cycles may indeed exist but are considerably longer than previously assumed. |
| 2025-10-17 | [Two-Stage Data-Driven Contextual Robust Optimization: An End-to-End Learning Approach for Online Energy Applications](http://arxiv.org/abs/2510.15696v1) | Carlos Gamboa, Alexandre Street et al. | Traditional end-to-end contextual robust optimization models are trained for specific contextual data, requiring complete retraining whenever new contextual information arrives. This limitation hampers their use in online decision-making problems such as energy scheduling, where multiperiod optimization must be solved every few minutes. In this paper, we propose a novel Data-Driven Contextual Uncertainty Set, which gives rise to a new End-to-End Data-Driven Contextual Robust Optimization model. For right-hand-side uncertainty, we introduce a reformulation scheme that enables the development of a variant of the Column-and-Constraint Generation (CCG) algorithm. This new CCG method explicitly considers the contextual vector within the cut expressions, allowing previously generated cuts to remain valid for new contexts, thereby significantly accelerating convergence in online applications. Numerical experiments on energy and reserve scheduling problems, based on classical test cases and large-scale networks (with more than 10,000 nodes), demonstrate that the proposed method reduces computation times and operational costs while capturing context-dependent risk structures. The proposed framework (model and method), therefore, offers a unified, scalable, and prescriptive approach to robust decision-making under uncertainty, effectively bridging data-driven learning and optimization. |
| 2025-10-16 | [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](http://arxiv.org/abs/2510.14959v1) | Lizhi Yang, Blake Werner et al. | Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter. |
| 2025-10-16 | [Current fluctuations in nonequilibrium open quantum systems beyond weak coupling: a reaction coordinate approach](http://arxiv.org/abs/2510.14926v1) | Khalak Mahadeviya, Saulo V. Moreira et al. | We investigate current fluctuations in open quantum systems beyond the weak-coupling and Markovian regimes, focusing on a coherently driven qubit strongly coupled to a structured bosonic environment. By combining full counting statistics with the reaction coordinate mapping, we develop a framework that enables the calculation of steady-state current fluctuations and their temporal correlations in the strong-coupling regime. Our analysis reveals that, unlike in weak coupling, both the average current and its fluctuations exhibit nonmonotonic dependence on the system-environment interaction strength. Notably, we identify a regime where current noise is suppressed below the classical thermodynamic uncertainty bound, coinciding with enhanced anticorrelations in quantum jump trajectories and faster system relaxation. We further show that these features are linked to nonclassical properties of the reaction coordinate mode, such as non-Gaussianity and quantum coherence. Our results provide new insights and design principles for controlling current fluctuations in quantum devices operating beyond the standard weak-coupling paradigm. |
| 2025-10-16 | [Astrophysical uncertainties challenge 21-cm forecasts: A primordial black hole case study](http://arxiv.org/abs/2510.14877v1) | Dominic Agius, Rouven Essig et al. | The 21-cm signal is a powerful probe of the early Universe's thermal history and could provide a unique avenue for constraining exotic physics. Previous studies have forecasted stringent constraints on energy injections from exotic sources that heat, excite, and ionize the background gas and thereby modify the 21-cm signal. In this work, we quantify the substantial impact that astrophysical uncertainties have on the projected sensitivity to exotic energy injection. In particular, there are significant uncertainties in the minimum star-forming dark matter halo mass, the Lyman-$\alpha$ emission, and the X-ray emission, whose values characterize the fiducial astrophysical model when projecting bounds. As a case study, we investigate the energy injection of accreting primordial black holes of mass $\sim 1~M_\odot-10^3~M_\odot$, also taking into account uncertainties in the accretion model. We show that, depending on the chosen fiducial model and accretion uncertainties, the sensitivity of future 21-cm data could constrain the abundance of primordial black holes to be either slightly stronger, or significantly weaker, than current limits from the Cosmic Microwave Background. |
| 2025-10-16 | [A new photometric ephemeris for the 2M1510 AB double brown dwarf eclipsing binary system](http://arxiv.org/abs/2510.14801v1) | Seb T. Millward, Vedad Kunovac | Eclipsing brown dwarfs are important calibrators of sub-stellar evolution models used to infer the characteristics of directly imaged brown dwarfs and giant exoplanets. Only two double brown dwarf eclipsing binary systems are known, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a poorly constrained orbital period. Here we analyse TESS full-frame image (FFI) photometry of this faint ($T=15.9$) binary and detect a significant (${>}10\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with previous data. We refine the orbital period to $20.897782 \pm 0.000036$ d, reducing its present-day uncertainty from 18 h to 8 min. Our work is crucial for scheduling follow-up observations of this system for detailed study with other photometric facilities. We also find that a recent orbital solution from Doppler data is inconsistent with existing photometry. A timing offset in the Doppler data may have produced a spurious signal mimicking retrograde apsidal precession, from which the claimed circumbinary planet 2M1510 ABb was inferred. From our best attempt at correcting the data we were unable to reconcile the radial velocity data with the photometry, suggesting that the radial velocity uncertainties are underestimated, and that the circumbinary planet 2M1510 ABb may be a false positive. |
| 2025-10-16 | [Measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays with the LHCb Upgrade I detector](http://arxiv.org/abs/2510.14732v1) | LHCb collaboration, R. Aaij et al. | A measurement of $C\!P$ asymmetry in $D^0 \to K^0_{\rm S} K^0_{\rm S}$ decays is reported, based on a data sample of proton-proton collisions collected with the LHCb Upgrade I detector in 2024 at a centre-of-mass energy of $13.6\,$TeV, corresponding to an integrated luminosity of $6.2\,\mathrm{fb}^{-1}$. The $D^0 \to K^0_{\rm S} \pi^+ \pi^-$ decay is used as calibration channel to cancel residual detection and production asymmetries. The time-integrated $C\!P$ asymmetry for the $D^0 \to K^0_{\rm S} K^0_{\rm S}$ mode is measured to be $$ {\cal A}^{C\!P} (D^0 \to K^0_{\rm S} K^0_{\rm S}) = (1.86 \pm 1.04\pm 0.41)\%, $$ where the first uncertainty is statistical, and the second is systematic. This is the most precise determination of this quantity to date. |
| 2025-10-16 | [Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock](http://arxiv.org/abs/2510.14715v1) | Guobin Mou | Recent long-term radio monitoring of tidal disruption events (TDEs) suggests that radio afterglows are common. Most studies argue that these afterglows may arise from forward shocks (FS) produced by the interaction between the TDE outflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical models do not model the evolution of relativistic electrons in space, which introduces uncertainties. Here we conducted hydrodynamic simulations to study the hydrodynamic evolution of relativistic electrons, and calculated the synchrotron spectra via radiative transfer. We focus on the FS scenario with non-relativistic outflows, and various parameters of the outflow and CNM are explored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a Galactic center - like CNM can produce mJy-level radio afterglows at a distance of 100 Mpc. The self-absorption frequency exhibits a slow decline at early times and a rapid decrease at late times. We derived the temporal evolution of the high-frequency radio flux, revealing its characteristic rise and decline pattern. We also find that: (1) the radio spectra for narrow outflows are clearly anisotropic along different sight lines; (2) the FS parameters inferred from radio spectra using conventional analytical formulas deviate significantly from those in simulations, in which the inferred shock radii are half of those from simulations, and the inferred energies are an order of magnitude lower. |
| 2025-10-16 | [Fast and Scalable Score-Based Kernel Calibration Tests](http://arxiv.org/abs/2510.14711v1) | Pierre Glaser, David Widmann et al. | We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD test), a non-parametric, kernel-based test for assessing the calibration of probabilistic models with well-defined scores. In contrast to previous methods, our test avoids the need for possibly expensive expectation approximations while providing control over its type-I error. We achieve these improvements by using a new family of kernels for score-based probabilities that can be estimated without probability density samples, and by using a conditional goodness-of-fit criterion for the KCCSD test's U-statistic. We demonstrate the properties of our test on various synthetic settings. |
| 2025-10-16 | [Practical, Utilitarian Algorithm Configuration](http://arxiv.org/abs/2510.14683v1) | Devon Graham, Kevin Leyton-Brown | Utilitarian algorithm configuration identifies a parameter setting for a given algorithm that maximizes a user's utility. Utility functions offer a theoretically well-grounded approach to optimizing decision-making under uncertainty and are flexible enough to capture a user's preferences over algorithm runtimes (e.g., they can describe a sharp cutoff after which a solution is no longer required, a per-hour cost for compute, or diminishing returns from algorithms that take longer to run). COUP is a recently-introduced utilitarian algorithm configuration procedure which was designed mainly to offer strong theoretical guarantees about the quality of the configuration it returns, with less attention paid to its practical performance. This paper closes that gap, bringing theoretically-grounded, utilitarian algorithm configuration to the point where it is competitive with widely used, heuristic configuration procedures that offer no performance guarantees. We present a series of improvements to COUP that improve its empirical performance without degrading its theoretical guarantees and demonstrate their benefit experimentally. Using a case study, we also illustrate ways of exploring the robustness of a given solution to the algorithm selection problem to variations in the utility function. |
| 2025-10-16 | [Precision of an autonomous demon exploiting nonthermal resources and information](http://arxiv.org/abs/2510.14578v1) | Juliette Monsel, Matteo Acciai et al. | Quantum-dot systems serve as nanoscale heat engines exploiting thermal fluctuations to perform a useful task. Here, we investigate a multi-terminal triple-dot system, operating as a refrigerator that extracts heat from a cold electronic contact. In contrast to standard heat engines, this system exploits a nonthermal resource. This has the intriguing consequence that cooling can occur without extracting energy from the resource on average -- a seemingly demonic action -- while, however, requiring the resource to fluctuate. Using full counting statistics and stochastic trajectories, we analyze the performance of the device in terms of the cooling-power precision, employing performance quantifiers motivated by the thermodynamic and kinetic uncertainty relations. We focus on two regimes with large output power, which are based on two operational principles: exploiting information on one hand and the nonthermal properties of the resource on the other. We show that these regimes significantly differ in precision. In particular, the regime exploiting the nonthermal properties of the resource can have cooling-power fluctuations that are suppressed with respect to the input fluctuations by an order of magnitude. We also substantiate the interpretation of the two different working principles by analyzing cross-correlations between input and output heat currents and information flow. |
| 2025-10-16 | [Co-Investment under Revenue Uncertainty Based on Stochastic Coalitional Game Theory](http://arxiv.org/abs/2510.14555v1) | Amal Sakr, Andrea Araldo et al. | The introduction of new services, such as Mobile Edge Computing (MEC), requires a massive investment that cannot be assumed by a single stakeholder, for instance the Infrastructure Provider (InP). Service Providers (SPs) however also have an interest in the deployment of such services. We hence propose a co-investment scheme in which all stakeholders, i.e., the InP and the SPs, form the so-called grand coalition composed of all the stakeholders with the aim of sharing costs and revenues and maximizing their payoffs. The challenge comes from the fact that future revenues are uncertain. We devise in this case a novel stochastic coalitional game formulation which builds upon robust game theory and derive a lower bound on the probability of the stability of the grand coalition, wherein no player can be better off outside of it. In the presence of some correlated fluctuations of revenues however, stability can be too conservative. In this case, we make use also of profitability, in which payoffs of players are non-negative, as a necessary condition for co-investment. The proposed framework is showcased for MEC deployment, where computational resources need to be deployed in nodes at the edge of a telecommunication network. Numerical results show high lower bound on the probability of stability when the SPs' revenues are of similar magnitude and the investment period is sufficiently long, even with high levels of uncertainty. In the case where revenues are highly variable however, the lower bound on stability can be trivially low whereas co-investment is still profitable. |
| 2025-10-15 | [$\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF](http://arxiv.org/abs/2510.13805v1) | David Gebauer, Anik Halder et al. | We present $\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework for analysing a higher-order weak lensing statistic, the integrated 3-point correlation function (i3PCF). Our approach forward-models the cosmic shear field using the $\texttt{CosmoGridV1}$ suite of N-body simulations, including a comprehensive set of systematic effects such as intrinsic alignment, baryonic feedback, photometric redshift uncertainty, shear calibration bias, and shape noise. Using this, we have produced a set of DES Y3-like synthetic measurements for 2-point shear correlation functions $\xi_{\pm}$ (2PCFs) and i3PCFs $\zeta_{\pm}$ across 6 cosmological and 11 systematic parameters. Having validated these measurements against theoretical predictions and thoroughly examined for potential systematic biases, we have found that the impact of source galaxy clustering and reduced shear on the i3PCF is negligible for Stage-III surveys. Furthermore, we have tested the Gaussianity assumption for the likelihood of our data vector and found that while the sampling distribution of the 2PCF can be well approximated by a Gaussian function, the likelihood of the combined 2PCF + i3PCF data vector including filter sizes of $90'$ and larger can deviate from this assumption. Our SBI pipeline employs masked autoregressive flows to perform neural likelihood estimation and is validated to give statistically accurate posterior estimates. On mock data, we find that including the i3PCF yields a substantial $63.8\%$ median improvement in the figure of merit for $\Omega_m - \sigma_8 - w_0$. These findings are consistent with previous works on the i3PCF and demonstrate that our SBI framework can achieve the accuracy and realism needed to analyse the i3PCF in wide-area weak lensing surveys. |
| 2025-10-15 | [Splitting Isotope Shift in the $1s2p\,^3\!P_{0,1,2}$ Fine-Structure Triplet in $^{12,13,14}$C$^{4+}$: Experiment and Theory](http://arxiv.org/abs/2510.13779v1) | Patrick MÃ¼ller, Kristian KÃ¶nig et al. | We report measurements and theoretical calculations of the fine-structure splittings in all three $1s2s\,^3\!S_1\rightarrow\,1s2p\,^3\!P_{0,1,2}$ transitions in the heliumlike systems of the isotopes $^{12,13,14}$C. The metastable triplet state was efficiently populated in an electron beam ion source and the C$^{4+}$ ions were electrostatically accelerated to 50\,keV to perform collinear laser spectroscopy. From the determined transition frequencies, the splitting isotope shift (SIS), i.e., the difference in fine-structure splittings between different isotopes of the same element, was extracted. In the SIS, theoretical uncertainties due to higher-order quantum electrodynamic corrections are strongly suppressed since they are independent of both nuclear mass and the fine-structure quantum number $J$ in lowest order. Comparison with theory provides an important test of experimental accuracy, particularly in the $^{13}$C$^{4+}$ case, for which the nuclear spin leads to hyperfine-induced fine-structure mixing. At the same time, the even-even isotopes $^{12,14}$C$^{4+}$ without nuclear spin can be used to confirm theory. Theoretical values of the SIS are given for all the heliumlike ions with $2\le Z\le 10$. |
| 2025-10-15 | [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](http://arxiv.org/abs/2510.13750v1) | Zhiqi Huang, Vivek Datla et al. | We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment. |
| 2025-10-15 | [VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions](http://arxiv.org/abs/2510.13705v1) | Fan Chang, Yijia Fang | In this paper, we uncover a new uncertainty principle that governs the complexity of Boolean functions. This principle manifests as a fundamental trade-off between two central measures of complexity: a combinatorial complexity of its supported set, captured by its Vapnik-Chervonenkis dimension ($\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial degree over various fields. We establish two primary inequalities that formalize this trade-off:$\mathrm{VC}(f)+\deg(f)\ge n,$ and $\mathrm{VC}(f)+\deg_{\mathbb{F}_2}(f)\ge n$. In particular, these results recover the classical uncertainty principle on the discrete hypercube, as well as the Sziklai--Weiner's bound in the case of $\mathbb{F}_2$. |
| 2025-10-15 | [Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping](http://arxiv.org/abs/2510.13672v1) | MarcÃ­lio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo | Dengue remains one of Brazil's major epidemiological challenges, marked by strong intra-urban inequalities and the influence of climatic and socio-environmental factors. This study analyzed confirmed dengue cases in Recife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model implemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal component. Covariates included population density, household size, income, drainage channels, lagged precipitation, and mean temperature. Population density and household size had positive effects on dengue risk, while income and channel presence were protective. Lagged precipitation increased risk, and higher temperatures showed an inverse association, suggesting thermal thresholds for vector activity. The model achieved good fit (DIC=65817; WAIC=64506) and stable convergence, with moderate residual spatial autocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019. Spatio-temporal estimates revealed persistent high-risk clusters in northern and western Recife, overlapping with areas of higher density and social vulnerability. Beyond reproducing historical patterns, the Bayesian model supports probabilistic forecasting and early warning systems. Compared with classical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty and spatial-temporal dependence, offering credible interval inference for decision-making in urban health management. |
| 2025-10-15 | [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](http://arxiv.org/abs/2510.13643v1) | Akib Mohammed Khan, Bartosz Krawczyk | Foundation models such as DINOv2 have shown strong performance in few-shot anomaly detection, yet two key questions remain unexamined: (i) how susceptible are these detectors to adversarial perturbations; and (ii) how well do their anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a training-free deep nearest-neighbor detector over DINOv2 features, we present one of the first systematic studies of adversarial attacks and uncertainty estimation in this setting. To enable white-box gradient attacks while preserving test-time behavior, we attach a lightweight linear head to frozen DINOv2 features only for crafting perturbations. Using this heuristic, we evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible perturbations can flip nearest-neighbor relations in feature space to induce confident misclassification. Complementing robustness, we probe reliability and find that raw anomaly scores are poorly calibrated, revealing a gap between confidence and correctness that limits safety-critical use. As a simple, strong baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly scores for uncertainty estimation. The resulting calibrated posteriors yield significantly higher predictive entropy on adversarially perturbed inputs than on clean ones, enabling a practical flagging mechanism for attack detection while reducing calibration error (ECE). Our findings surface concrete vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an evaluation protocol and baseline for robust, uncertainty-aware anomaly detection. We argue that adversarial robustness and principled uncertainty quantification are not optional add-ons but essential capabilities if anomaly detection systems are to be trustworthy and ready for real-world deployment. |
| 2025-10-15 | [Radiative decays of the $Î©(2012)$ as a hadronic molecule](http://arxiv.org/abs/2510.13623v1) | Qing-Hua Shen, Jun-Xu Lu et al. | We present a theoretical investigation of the radiative decay process $\Omega(2012) \to \gamma \Omega$, where the $\Omega(2012)$ resonance with spin-parity $J^P=\frac{3}{2}^-$, is treated as a dynamically generated state from $\bar{K}\Xi(1530)$ and $\eta \Omega$ in $s$-wave and $\bar{K}\Xi$ in $d$-wave. The radiative decay width of the $\Omega(2012)$ is calculated using a triangular loop mechanism, where the $\Omega(2012)$ couples to the $\bar{K} \Xi(1530)$ channel. Subsequently, the final state interactions between $\Xi(1530)$ and $\bar{K}$ transition to a photon and $\Omega$ through the exchange of a $\Xi$ baryon. Our calculations yield a radiative decay width of $13.2 ^{+4.5}_{-3.9}$ KeV, with uncertainties arising from the model parameters. This result provides valuable insights into the nature of the $\Omega(2012)$ resonance and its decay dynamics. It is expected that the calculations presented here could be verified by future experiments, which would open a new door for studying the still elusive nature of the $\Omega(2012)$. |
| 2025-10-15 | [Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity](http://arxiv.org/abs/2510.13609v1) | Ahmad Awad, Erik ScharwÃ¤chter | Soil organic carbon (SOC) sequestration projects require unbiased, precise and cost-effective Monitoring, Reporting, and Verification (MRV) systems that balance sampling costs against uncertainty deductions imposed by regulatory frameworks. Design-based estimators guarantee unbiasedness but cannot exploit auxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can improve precision but require independent validation for each project. Model-assisted estimation offers a robust compromise, combining model predictions with probability sampling to retain design-based guarantees while improving precision. We evaluate the scientific integrity and efficiency of the simple regression estimator (SRE), a well-known model-assisted estimator, via an extensive simulation study. Our simulations span diverse SOC stock variances, sample sizes, and model performances. We assess three core properties: empirical bias, empirical confidence interval coverage, and precision gain relative to the design-based Horvitz-Thompson estimator (HTE). Results show negligible bias and valid coverage probabilities for n > 40, regardless of SOC stock variance. Below this threshold, variance approximations and normality assumptions yield unreliable uncertainty estimates. With correlated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains over HTE. With uncorrelated variables, no gains are observed, but performance converges to HTE for n >= 40. Model-assisted estimation can enhance project economics without compromising scientific rigor. Regulators should permit such estimators while mandating minimum sample size thresholds. Project proponents should routinely employ such estimators when correlated ancillary variables exist. The industry should prioritize the retrieval of high-quality, project-specific covariates to maximize precision gains and thereby the project economics. |
| 2025-10-15 | [Unsupervised Constitutive Model Discovery from Sparse and Noisy Data](http://arxiv.org/abs/2510.13559v1) | Vahab Knauf Narouie, Jorge-Humberto Urrea-Quintero et al. | Recently, unsupervised constitutive model discovery has gained attention through frameworks based on the Virtual Fields Method (VFM), most prominently the EUCLID approach. However, the performance of VFM-based approaches, including EUCLID, is affected by measurement noise and data sparsity, which are unavoidable in practice. The statistical finite element method (statFEM) offers a complementary perspective by providing a Bayesian framework for assimilating noisy and sparse measurements to reconstruct the full-field displacement response, together with quantified uncertainty. While statFEM recovers displacement fields under uncertainty, it does not strictly enforce consistency with constitutive relations or aim to yield interpretable constitutive models. In this work, we couple statFEM with unsupervised constitutive model discovery in the EUCLID framework, yielding statFEM--EUCLID. The framework is demonstrated for isotropic hyperelastic materials. The results show that this integration reduces sensitivity to noise and data sparsity, while ensuring that the reconstructed fields remain consistent with both equilibrium and constitutive laws. |
| 2025-10-15 | [Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage](http://arxiv.org/abs/2510.13514v1) | Andreas C. Makrides, Adam Suski et al. | The transition to a fully decarbonised electricity system depends on integrating new technologies that ensure reliability alongside sustainability. However, missing risk markets hinder investment in reliability-enhancing technologies by exposing investors to revenue uncertainty. This study provides the first quantitative assessment of how missing risk markets affect investment decisions in power systems that depend on long-duration energy storage (LDES) for reliability. We develop a two-stage stochastic equilibrium model with risk-averse market participants, which independently sizes power and energy capacity. We apply the method to a case study of a deeply decarbonised power system in Great Britain. The results show that incomplete risk markets reduce social welfare, harm reliability, and discourage investment in LDES and other technologies with volatile revenue streams. Revenue volatility leads to substantial risk premiums and higher financing costs for LDES, creating a barrier to its large-scale deployment. These findings demonstrate the importance of policy mechanisms that hedge revenue risk to lower the cost of capital and accelerate investment in reliability-enhancing, zero-carbon technologies |
| 2025-10-14 | [What If : Understanding Motion Through Sparse Interactions](http://arxiv.org/abs/2510.12777v1) | Stefan Andreas Baumann, Nick Stracke et al. | Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer. |
| 2025-10-14 | [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](http://arxiv.org/abs/2510.12768v1) | Fengzhi Guo, Chih-Chuan Hsu et al. | Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints. |
| 2025-10-14 | [(Non-Parametric) Bootstrap Robust Optimization for Portfolios and Trading Strategies](http://arxiv.org/abs/2510.12725v1) | Daniel Cunha Oliveira, Grover Guzman et al. | Robust optimization provides a principled framework for decision-making under uncertainty, with broad applications in finance, engineering, and operations research. In portfolio optimization, uncertainty in expected returns and covariances demands methods that mitigate estimation error, parameter instability, and model misspecification. Traditional approaches, including parametric, bootstrap-based, and Bayesian methods, enhance stability by relying on confidence intervals or probabilistic priors but often impose restrictive assumptions. This study introduces a non-parametric bootstrap framework for robust optimization in financial decision-making. By resampling empirical data, the framework constructs flexible, data-driven confidence intervals without assuming specific distributional forms, thus capturing uncertainty in statistical estimates, model parameters, and utility functions. Treating utility as a random variable enables percentile-based optimization, naturally suited for risk-sensitive and worst-case decision-making. The approach aligns with recent advances in robust optimization, reinforcement learning, and risk-aware control, offering a unified perspective on robustness and generalization. Empirically, the framework mitigates overfitting and selection bias in trading strategy optimization and improves generalization in portfolio allocation. Results across portfolio and time-series momentum experiments demonstrate that the proposed method delivers smoother, more stable out-of-sample performance, offering a practical, distribution-free alternative to traditional robust optimization methods. |
| 2025-10-14 | [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](http://arxiv.org/abs/2510.12699v1) | Sunny Yu, Ahmad Jabbar et al. | Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs. |
| 2025-10-14 | [EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels](http://arxiv.org/abs/2510.12687v1) | Kunyu Peng, Di Wen et al. | Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM. |
| 2025-10-14 | [Measurement of the tau anomalous magnetic moment using Ultra-peripheral collisions with the ALICE detector in Run 3 Pb-Pb data](http://arxiv.org/abs/2510.12661v1) | Roman LaviÄka, Paul Alois BÃ¼hler | The anomalous magnetic moment of the tau lepton ($a_{\tau}$) is a sensitive probe for the search for deviations from the Standard Model predictions and thus for new physics. This study investigates the feasibility of measuring $a_{\tau}$ using ultra-peripheral collisions (UPCs) at the LHC, where photon-photon interactions ($\gamma\gamma \to \tau^+ \tau^-$) produce tau lepton pairs. We focus on events recorded by the ALICE detector during Run 3 Pb-Pb data-taking. Events are selected in the decay channel where one tau decays into an electron and neutrinos, and the other decays into a charged pion, or three charged pions, and neutrinos. These samples are enhanced with decays into muons, which are inseparable in the ALICE detector. The clean environment of UPCs minimizes hadronic background, while the advanced particle identification capabilities of the ALICE Time Projection Chamber (TPC) and Time-of-Flight (TOF) systems allow for efficient separation of electrons, pions, and background particles. In this talk, prospects for measuring this process by ALICE in Run 3, which benefits from high statistics and improved systematics uncertainties, will be discussed. Results will provide tighter constraints on $a_{\tau}$, contributing to the broader effort to test the Standard Model's robustness and explore physics beyond it. |
| 2025-10-14 | [Moment-based Posterior Sampling for Multi-reference Alignment](http://arxiv.org/abs/2510.12651v1) | Axel Janson, Joakim AndÃ©n | We propose a Bayesian approach to the problem of multi-reference alignment -- the recovery of signals from noisy, randomly shifted observations. While existing frequentist methods accurately recover the signal at arbitrarily low signal-to-noise ratios, they require a large number of samples to do so. In contrast, our proposed method leverages diffusion models as data-driven plug-and-play priors, conditioning these on the sample power spectrum (a shift-invariant statistic) enabling both accurate posterior sampling and uncertainty quantification. The use of an appropriate prior significantly reduces the required number of samples, as illustrated in simulation experiments with comparisons to state-of-the-art methods such as expectation--maximization and bispectrum inversion. These findings establish our approach as a promising framework for other orbit recovery problems, such as cryogenic electron microscopy (cryo-EM). |
| 2025-10-14 | [Designing Tools with Control Confidence](http://arxiv.org/abs/2510.12630v1) | Ajith Anil Meera, Abian Torres et al. | Prehistoric humans invented stone tools for specialized tasks by not just maximizing the tool's immediate goal-completion accuracy, but also increasing their confidence in the tool for later use under similar settings. This factor contributed to the increased robustness of the tool, i.e., the least performance deviations under environmental uncertainties. However, the current autonomous tool design frameworks solely rely on performance optimization, without considering the agent's confidence in tool use for repeated use. Here, we take a step towards filling this gap by i) defining an optimization framework for task-conditioned autonomous hand tool design for robots, where ii) we introduce a neuro-inspired control confidence term into the optimization routine that helps the agent to design tools with higher robustness. Through rigorous simulations using a robotic arm, we show that tools designed with control confidence as the objective function are more robust to environmental uncertainties during tool use than a pure accuracy-driven objective. We further show that adding control confidence to the objective function for tool design provides a balance between the robustness and goal accuracy of the designed tools under control perturbations. Finally, we show that our CMAES-based evolutionary optimization strategy for autonomous tool design outperforms other state-of-the-art optimizers by designing the optimal tool within the fewest iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence. |
| 2025-10-14 | [Learning-To-Measure: In-context Active Feature Acquisition](http://arxiv.org/abs/2510.12624v1) | Yuta Kobayashi, Zilin Jing et al. | Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness. |
| 2025-10-14 | [Enhancing Robust Multi-Market Participation of Renewable-Based VPPs through Flexible Resources](http://arxiv.org/abs/2510.12589v1) | Hadi Nemati, Ãlvaro Ortega et al. | In the transition toward a sustainable power system, renewable-based Virtual Power Plants (RVPPs) have emerged as a promising solution to the challenges of integrating renewable energy sources into electricity markets. Their viability, however, depends on effective market participation strategies and the ability to manage uncertainties while leveraging flexible resources. This paper analyzes the impact of different flexible resources - such as concentrated solar power plants, hydro plants, biomass plants, and flexible demand - on the participation of RVPPs in energy and reserve markets. Multiple sources of uncertainty in generation, consumption, and electricity prices are addressed using a two-stage robust optimization approach. The contribution of different technologies to RVPP profitability is evaluated through a marginal contribution method, ensuring fair allocation of profits among them according to their actual role in energy and reserve provision across markets. Simulations for an RVPP in southern Spain demonstrate how strategic decisions and the availability of flexible resources influence viability, market participation, and unit scheduling. |
| 2025-10-13 | [BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe](http://arxiv.org/abs/2510.11719v1) | M. Grayling, S. Thorp et al. | We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia supernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged, gravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for magnifications and time-delays across multiple images while marginalising over an achromatic, Gaussian process-based treatment of microlensing, to allow for time-dependent deviations from a typical SN Ia SED caused by gravitational lensing by stars in the lensing system. BayeSN-TD is able to robustly infer time delays and produce well-calibrated uncertainties, even when applied to simulations based on a different SED model and incorporating chromatic microlensing, strongly validating its suitability for time-delay cosmography. We then apply BayeSN-TD to publicly available photometry of the glSN Ia SN H0pe, inferring time delays between images BA and BC of $\Delta T_{BA}=121.9^{+9.5}_{-7.5}$ days and $\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days along with absolute magnifications $\beta$ for each image, $\beta_A = 2.38^{+0.72}_{-0.54}$, $\beta_B=5.27^{+1.25}_{-1.02}$ and $\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and magnifications with existing lens models of this system, we infer $H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous analysis of this system; incorporating additional constraints based on spectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While this is not yet precise enough to draw a meaningful conclusion with regard to the `Hubble tension', upcoming analysis of SN H0pe with more accurate photometry enabled by template images, and other glSNe, will provide stronger constraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses. |
| 2025-10-13 | [Bayesian Topological Convolutional Neural Nets](http://arxiv.org/abs/2510.11704v1) | Sarah Harkins Dayton, Hayden Everett et al. | Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification. |
| 2025-10-13 | [Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation](http://arxiv.org/abs/2510.11689v1) | Maggie Wang, Stephen Tian et al. | Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying center of mass (CoM) and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster average task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success. Project website: https://phys2real.github.io/ . |
| 2025-10-13 | [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](http://arxiv.org/abs/2510.11654v1) | Daniel Berhane Araya, Duoduo Liao | Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches. |
| 2025-10-13 | [StatTestCalculator: A New General Tool for Statistical Analysis in High Energy Physics](http://arxiv.org/abs/2510.11637v1) | Emil Abasov, Lev Dudko et al. | We present StatTestCalculator (STC), a new open-source statistical analysis tool designed for analysis high energy physics experiments. STC provides both asymptotic calculations and Monte Carlo simulations for computing the exact statistical significance of a discovery or for setting upper limits on signal model parameters. We review the underlying statistical formalism, including profile likelihood ratio test statistics for discovery and exclusion hypotheses, and the asymptotic distributions that allow quick significance estimates. We explain the relevant formulas for the likelihood functions, test statistic distributions, and significance metrics (both with and without incorporating systematic uncertainties). The implementation and capabilities of STC are described, and we validate its performance against the widely-used CMS Combine tool. We find excellent agreement in both the expected discovery significances and upper limit calculations. STC is a flexible framework that can accommodate systematic uncertainties and user-defined statistical models, making it suitable for a broad range of analyses. |
| 2025-10-13 | [The Fractional Two-Sided Quaternionic Dunkl Transform and Heisenberg-Type Inequalities](http://arxiv.org/abs/2510.11597v1) | Mohamed Essenhajy | This report investigates the main definitions and fundamental properties of the fractional two-sided quaternionic Dunkl transform in two dimensions. We present key results concerning its structure and emphasize its connections to classical harmonic analysis. Special attention is given to inversion, boundedness, spectral behavior, and explicit formulas for structured functions such as radial or harmonic functions. Within this framework, we establish a generalized form of the classical Heisenberg-type uncertainty principle. Building on this foundation, we further extend the result by proving a higher-order Heisenberg-type inequality valid for arbitrary moments $p \geq 1$, with sharp constants characterized through generalized Hermite functions. Finally, by analyzing the interplay between the two-sided fractional quaternionic Dunkl transform and the two-sided fractional quaternionic Fourier transform, we derive a corresponding uncertainty principle for the latter. |
| 2025-10-13 | [Reproducibility: The New Frontier in AI Governance](http://arxiv.org/abs/2510.11595v1) | Israel Mason-Williams, Gabryel Mason-Williams | AI policymakers are responsible for delivering effective governance mechanisms that can provide safe, aligned and trustworthy AI development. However, the information environment offered to policymakers is characterised by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and creating deep uncertainty and divides on which risks should be prioritised from a governance perspective. We posit that the current publication speeds in AI combined with the lack of strong scientific standards, via weak reproducibility protocols, effectively erodes the power of policymakers to enact meaningful policy and governance protocols. Our paper outlines how AI research could adopt stricter reproducibility guidelines to assist governance endeavours and improve consensus on the AI risk landscape. We evaluate the forthcoming reproducibility crisis within AI research through the lens of crises in other scientific domains; providing a commentary on how adopting preregistration, increased statistical power and negative result publication reproducibility protocols can enable effective AI governance. While we maintain that AI governance must be reactive due to AI's significant societal implications we argue that policymakers and governments must consider reproducibility protocols as a core tool in the governance arsenal and demand higher standards for AI research. Code to replicate data and figures: https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance |
| 2025-10-13 | [Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization](http://arxiv.org/abs/2510.11539v1) | Denglin Cheng, Jiarong Kang et al. | Accurate state estimation is critical for legged and aerial robots operating in dynamic, uncertain environments. A key challenge lies in specifying process and measurement noise covariances, which are typically unknown or manually tuned. In this work, we introduce a bi-level optimization framework that jointly calibrates covariance matrices and kinematic parameters in an estimator-in-the-loop manner. The upper level treats noise covariances and model parameters as optimization variables, while the lower level executes a full-information estimator. Differentiating through the estimator allows direct optimization of trajectory-level objectives, resulting in accurate and consistent state estimates. We validate our approach on quadrupedal and humanoid robots, demonstrating significantly improved estimation accuracy and uncertainty calibration compared to hand-tuned baselines. Our method unifies state estimation, sensor, and kinematics calibration into a principled, data-driven framework applicable across diverse robotic platforms. |
| 2025-10-13 | [Characterizing planetary systems with SPIRou: questions about the magnetic cycle of 55 Cnc A and two new planets around B](http://arxiv.org/abs/2510.11523v1) | C. Moutou, P. Petit et al. | One of the first exoplanet hosts discovered thirty years ago, the star 55 Cnc has been constantly observed ever since. It is now known to host at least five planets with orbital periods ranging from 17 hours to 15 years. It is also one of the most extreme metal rich stars in the neighbourhood and it has a low-mass secondary star. In this article, we present data obtained at the Canada-France-Hawai'i Telescope with the SPIRou spectropolarimeter on both components of the 55 Cnc stellar system. We revisit the long-period radial-velocity signals of 55 Cnc A, with a focus on the role of the magnetic cycle, and propose the existence of a sixth planet candidate, whose period falls close to that of the magnetic cycle, or half of it. The other massive outer planet has a revised period of 13.15 years and a minimum mass of 3.8 MJup. Although some uncertainty remains on these outer planets, the characterization of the four inner planets is very robust through the combination of many different data sets, and all signals are consistent in the nIR and optical domains. In addition, the magnetic topology of the solar-type primary component of the system is observed by SPIRou at the minimum of its activity cycle, characterized by an amplitude ten times smaller than observed during its maximum in 2017. For the low-mass component 55 Cnc B, we report the discovery of two exoplanets in the system, with a period of 6.799+-0.0014 and 33.75+-0.04 days and a minimum mass of 3.5+-0.8 and 5.3+-1.4 MEarth, respectively. The secondary magnetic field is very weak and the current data set does not allow its precise characterization, setting an upper limit of 10 G. The system 55 Cnc stands out as the sixth binary system with planetary systems around both components, and the first one with non equal-mass stellar components. |
| 2025-10-13 | [Uncertainty Quantification for Retrieval-Augmented Reasoning](http://arxiv.org/abs/2510.11483v1) | Heydar Soudani, Hamed Zamani et al. | Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods. |
| 2025-10-10 | [Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference](http://arxiv.org/abs/2510.09574v1) | Daria de tinguy, Tim Verbelen et al. | Autonomous navigation in unfamiliar environments requires robots to simultaneously explore, localise, and plan under uncertainty, without relying on predefined maps or extensive training. We present a biologically inspired, Active Inference-based framework, Active Inference MAPping and Planning (AIMAPP). This model unifies mapping, localisation, and decision-making within a single generative model. Inspired by hippocampal navigation, it uses topological reasoning, place-cell encoding, and episodic memory to guide behaviour. The agent builds and updates a sparse topological map online, learns state transitions dynamically, and plans actions by minimising Expected Free Energy. This allows it to balance goal-directed and exploratory behaviours. We implemented a ROS-compatible navigation system that is sensor and robot-agnostic, capable of integrating with diverse hardware configurations. It operates in a fully self-supervised manner, is resilient to drift, and supports both exploration and goal-directed navigation without any pre-training. We demonstrate robust performance in large-scale real and simulated environments against state-of-the-art planning models, highlighting the system's adaptability to ambiguous observations, environmental changes, and sensor noise. The model offers a biologically inspired, modular solution to scalable, self-supervised navigation in unstructured settings. AIMAPP is available at https://github.com/decide-ugent/AIMAPP. |
| 2025-10-10 | [Yukawa interactions in Quantum Gravity](http://arxiv.org/abs/2510.09572v1) | Gustavo P. de Brito, Manuel Reichert et al. | We present the first complete next-to-leading-order analysis of a Yukawa system within the framework of asymptotically safe quantum gravity. Our results are obtained through a systematic resummation of higher-order operators, revealing two distinct resummation mechanisms -- one of which has not been explored previously. In addition, we introduce a novel approach to estimate systematic uncertainties by simulating the impact of neglected higher-order contributions.   We demonstrate that quantum gravity fluctuations anti-screen Yukawa interactions, thereby resolving previously inconclusive leading-order results. This anti-screening mechanism enables the generation of finite interactions from an asymptotically free Yukawa fixed point. Consequently, our findings provide strong evidence that non-vanishing Yukawa couplings are compatible with asymptotically safe quantum gravity, which is a necessary requirement for the Standard Model to emerge from an asymptotically safe ultraviolet completion. |
| 2025-10-10 | [Low-redshift constraints on structure growth from CMB lensing tomography](http://arxiv.org/abs/2510.09563v1) | Andrea Rubiola, Matteo Zennaro et al. | We present constraints on the amplitude of matter fluctuations from the clustering of galaxies and their cross-correlation with the gravitational lensing convergence of the cosmic microwave background (CMB), focusing on low redshifts ($z\lesssim0.3$), where potential deviations from a perfect cosmological constant dominating the growth of structure could be more prominent. Specifically, we make use of data from the 2MASS photometric survey (\tmpz) and the \wisc galaxy survey, in combination with CMB lensing data from \planck. Using a hybrid effective field theory (HEFT) approach to model galaxy bias we obtain constraints on the combination $S_8=\sigma_8\sqrt{\Omega_m/0.3}$, where $\sigma_8$ is the amplitude of matter fluctuations, and $\Omega_m$ is the non-relativistic matter fraction. Using a prior on $\Omega_m$ based on the baryon acoustic oscillation measurements of DESI, we find $S_8=0.79\pm0.06$, in reasonable agreement with CMB constraints. We also find that, in the absence of this prior, the data favours a value of $\Omega_m=0.245\pm0.024$, that is 2.8$\sigma$ lower than \planck. This result is driven by the broadband shape of the galaxy auto-correlation, and may be affected by theoretical uncertainties in the HEFT power spectrum templates. We further reconstruct the low-redshift growth history, finding it to be compatible with the \planck predictions, as well as existing constraints from lensing tomography. Finally, we study our constraints on the HEFT bias parameters of the galaxy samples studied, finding them to be in reasonable agreement with coevolution predictions. |
| 2025-10-10 | [A LENS on DUNE-PRISM: Characterizing a Neutrino Beam with Off-Axis Measurements](http://arxiv.org/abs/2510.09546v1) | Julia Gehrlein, Joachim Kopp et al. | Upcoming precision long-baseline neutrino oscillation experiments will be severely limited by the large systematic uncertainties associated with neutrino flux predictions and neutrino--nucleus cross sections. A promising remedy is the PRISM (Precision Reaction Independent Spectrum Measurement) technique, whereby the near detector measures the neutrino spectrum at different angles with respect to the beam axis. These measurements are then linearly combined into a prediction of the oscillated neutrino flux at the far detector. This prediction is data-driven, but still dependent on some theoretical knowledge about the neutrino flux. In this paper, we study to what extent off-axis measurements themselves can be used to directly constrain neutrino flux models. In particular, we use them to extract separately the fluxes and spectra of different meson species in the beam. We call this measurement LENS (Lateral Extraction of Neutrino Spectra). Second, we demonstrate how the thus improved flux model helps to further constrain the far detector flux prediction, thereby ultimately improving oscillation measurements. |
| 2025-10-10 | [Hierarchical Progressive Survey (HiPS) format: moving from visualisation to scientific analysis](http://arxiv.org/abs/2510.09533v1) | Fabrizio Giordano, Yago Ascasibar et al. | Context. In the current era of multi-wavelength and multi-messenger astronomy, international organisations are actively working on the definition of new standards for the publication of astronomical data, and substantial effort is devoted to make them available through public archives. Aims. We present a set of tools that allow user-friendly access and basic scientific analysis of observations in Hierarchical Progressive Survey (HiPS) format, and we use them to gauge the quality of representative skymaps at ultraviolet, optical, and infrared wavelengths. Methods. We apply a fully-automatic procedure to derive aperture photometry in 10 different bands for the 323 nearby galaxies in the Herschel Reference Sample (HRS), and compare its results with the rigorous analyses involving specialised knowledge and human intervention carried out by the HRS team. Results. Our experiment shows that 9 of the 10 skymaps considered preserve the original quality of the data, and the photometric fluxes returned by our pipeline are consistent with the HRS measurements within a few per cent. In the case of Herschel PACS maps at 100 {\mu}m, we uncovered a systematic error that we ascribe to an inconsistent combination of data products with different spatial resolution. For the remaining skymaps, the estimated statistical uncertainties provide a realistic indication of the differences with respect to the HRS catalogue. Conclusions. In principle, the currently available HiPS skymaps in Flexible Image Transport System (FITS) format allow to carry out broadband photometric analyses with an accuracy of the order of a few percent, but some level human intervention is still required. In addition to assessing data quality, we also propose a series of recommendations to realise the full potential of the HiPS format for the scientific analysis of large astronomical data sets. |
| 2025-10-10 | [Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study](http://arxiv.org/abs/2510.09498v1) | Rogier P. Krijnen, Akshay Joshi et al. | Fully capturing this behavior in traditional homogenized tissue testing requires the excitation of multiple deformation modes, i.e. combined triaxial shear tests and biaxial stretch tests. Inherently, such multimodal experimental protocols necessitate multiple tissue samples and extensive sample manipulations. Intrinsic inter-sample variability and manipulation-induced tissue damage might have an adverse effect on the inversely identified tissue behavior. In this work, we aim to overcome this gap by focusing our attention to the use of heterogeneous deformation profiles in a parameter estimation problem. More specifically, we adapt EUCLID, an unsupervised method for the automated discovery of constitutive models, towards the purpose of parameter identification for highly nonlinear, orthotropic constitutive models using a Bayesian inference approach and three-dimensional continuum elements. We showcase its strength to quantitatively infer, with varying noise levels, the material model parameters of synthetic myocardial tissue slabs from a single heterogeneous biaxial stretch test. This method shows good agreement with the ground-truth simulations and with corresponding credibility intervals. Our work highlights the potential for characterizing highly nonlinear and orthotropic material models from a single biaxial stretch test with uncertainty quantification. |
| 2025-10-10 | [FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents](http://arxiv.org/abs/2510.09483v1) | Lars Ohnemus, Nils Hantke et al. | Dynamic Scene Graphs (DSGs) provide a structured representation of hierarchical, interconnected environments, but current approaches struggle to capture stochastic dynamics, partial observability, and multi-agent activity. These aspects are critical for embodied AI, where agents must act under uncertainty and delayed perception. We introduce FOGMACHINE , an open-source framework that fuses DSGs with discrete-event simulation to model object dynamics, agent observations, and interactions at scale. This setup enables the study of uncertainty propagation, planning under limited perception, and emergent multi-agent behavior. Experiments in urban scenarios illustrate realistic temporal and spatial patterns while revealing the challenges of belief estimation under sparse observations. By combining structured representations with efficient simulation, FOGMACHINE establishes an effective tool for benchmarking, model training, and advancing embodied AI in complex, uncertain environments. |
| 2025-10-10 | [Failure Prediction at Runtime for Generative Robot Policies](http://arxiv.org/abs/2510.09459v1) | Ralf RÃ¶mer, Adrian Kobras et al. | Imitation learning (IL) with generative models, such as diffusion and flow matching, has enabled robots to perform complex, long-horizon tasks. However, distribution shifts from unseen environments or compounding action errors can still cause unpredictable and unsafe behavior, leading to task failure. Early failure prediction during runtime is therefore essential for deploying robots in human-centered and safety-critical environments. We propose FIPER, a general framework for Failure Prediction at Runtime for generative IL policies that does not require failure data. FIPER identifies two key indicators of impending failure: (i) out-of-distribution (OOD) observations detected via random network distillation in the policy's embedding space, and (ii) high uncertainty in generated actions measured by a novel action-chunk entropy score. Both failure prediction scores are calibrated using a small set of successful rollouts via conformal prediction. A failure alarm is triggered when both indicators, aggregated over short time windows, exceed their thresholds. We evaluate FIPER across five simulation and real-world environments involving diverse failure modes. Our results demonstrate that FIPER better distinguishes actual failures from benign OOD situations and predicts failures more accurately and earlier than existing methods. We thus consider this work an important step towards more interpretable and safer generative robot policies. Code, data and videos are available at https://tum-lsy.github.io/fiper_website. |
| 2025-10-10 | [Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior](http://arxiv.org/abs/2510.09401v1) | Matthew R. Williams, F. Hunter McGuire et al. | Parameter estimation and inference from complex survey samples typically focuses on global model parameters whose estimators have asymptotic properties, such as from fixed effects regression models. We present a motivating example of Bayesian inference for a multi-level or mixed effects model in which both the local parameters (e.g. group level random effects) and the global parameters may need to be adjusted for the complex sampling design. We evaluate the limitations of the survey-weighted pseudo-posterior and an existing automated post-processing method to incorporate the complex survey sample design for a wide variety of Bayesian models. We propose modifications to the automated process and demonstrate their improvements for multi-level models via a simulation study and a motivating example from the National Survey on Drug Use and Health. Reproduction examples are available from the authors and the updated R package is available via github:https://github.com/RyanHornby/csSampling |
| 2025-10-10 | [Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation](http://arxiv.org/abs/2510.09390v1) | Mert Ä°nan, Anthony Sicilia et al. | Establishing shared goals is a fundamental step in human-AI communication. However, ambiguities can lead to outputs that seem correct but fail to reflect the speaker's intent. In this paper, we explore this issue with a focus on the data visualization domain, where ambiguities in natural language impact the generation of code that visualizes data. The availability of multiple views on the contextual (e.g., the intended plot and the code rendering the plot) allows for a unique and comprehensive analysis of diverse ambiguity types. We develop a taxonomy of types of ambiguity that arise in this task and propose metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, we demonstrate that our ambiguity metrics better correlate with human annotations than uncertainty baselines. Our work also explores how multi-turn dialogue can reduce ambiguity, therefore, improve code accuracy by better matching user goals. We evaluate three pragmatic models to inform our dialogue strategies: Gricean Cooperativity, Discourse Representation Theory, and Questions under Discussion. A simulated user study reveals how pragmatic dialogues reduce ambiguity and enhance code accuracy, highlighting the value of multi-turn exchanges in code generation. |
| 2025-10-09 | [Scalable Offline Metrics for Autonomous Driving](http://arxiv.org/abs/2510.08571v1) | Animikh Aich, Adwait Kulkarni et al. | Real-World evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e., by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed. |
| 2025-10-09 | [Cleaning Galactic foregrounds with spatially varying spectral dependence from CMB observations with \texttt{fgbuster}](http://arxiv.org/abs/2510.08534v1) | Arianna Rizzieri, ClÃ©ment Leloup et al. | In the context of maximum-likelihood parametric component separation for next-generation full-sky CMB polarization experiments, we study the impact of fitting different spectral parameters of Galactic foregrounds in distinct subsets of pixels on the sky, with the goal of optimizing the search for primordial B modes. Using both simulations and analytical arguments, we highlight how the post-component separation uncertainty and systematic foreground residuals in the cleaned CMB power spectrum depend on spatial variations in the spectral parameters. We show that allowing spectral parameters to vary across subsets of the sky pixels is essential to achieve competitive S/N on the reconstructed CMB after component separation while keeping residual foreground bias under control. Although several strategies exist to define pixel subsets for the spectral parameters, each with its advantages and limitations, we show using current foreground simulations in the context of next-generation space-borne missions that there are satisfactory configurations in which both statistical and systematic residuals become negligible. The exact magnitude of these residuals, however, depends on the mission's specific characteristics, especially its frequency coverage and sensitivity. We also show that the post-component separation statistical uncertainty is only weakly dependent on the properties of the foregrounds and propose a semi-analytical framework to estimate it. On the contrary, the systematic foreground residuals highly depend on both the properties of the foregrounds and the chosen spatial resolution of the spectral parameters. |
| 2025-10-09 | [Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression](http://arxiv.org/abs/2510.08512v1) | Nikolaos Stathoulopoulos, Christoforos Kanellakis et al. | Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans. |
| 2025-10-09 | [Constraining the new contributions to electron $g-2$ in a radiative neutrino mass model](http://arxiv.org/abs/2510.08504v1) | Bayu Dirgantara, J. Julio | We examine electron and muon anomalous magnetic dipole moments within a radiative neutrino mass model featuring TeV-scale scalar leptoquarks $S(3,1,-1/3)$ and $R(3,2,1/6)$. We utilize textures with decoupling electron and muon sectors, so that both electron and muon anomalous magnetic dipole moments could receive internal chiral enhancements from different heavy up-type quarks while in the same time evading the stringent $\mu\to e\gamma$ constraint. A successful fit to neutrino oscillation data requires the simultaneous presence of one- and two-loop neutrino mass contributions. This severely constrains the parameter space of the model, which results in a negligible new physics correction to the muon $g-2$. The electron $g-2$ discrepancy implied by the rubidium experiment, on the other hand, can be resolved within $2\sigma$ uncertainty provided that neutrino mass ordering is inverted. Lepton-flavor-violating tau decay rates, such as $\tau\to e\gamma$ and $\tau\to 3e$, are predicted to be within the sensitivities of next-generation experiments. |
| 2025-10-09 | [Diffusion-Based Probabilistic Modeling for Hourly Streamflow Prediction and Assimilation](http://arxiv.org/abs/2510.08488v1) | Wencong Yang, Haoyu Ji et al. | Hourly predictions are critical for issuing flood warnings as the flood peaks on the hourly scale can be distinctly higher than the corresponding daily ones. Currently a popular hourly data-driven prediction scheme is multi-time-scale long short-term memory (MTS-LSTM), yet such models face challenges in probabilistic forecasts or integrating observations when available. Diffusion artificial intelligence (AI) models represent a promising method to predict high-resolution information, e.g., hourly streamflow. Here we develop a denoising diffusion probabilistic model (h-Diffusion) for hourly streamflow prediction that conditions on either observed or simulated daily discharge from hydrologic models to generate hourly hydrographs. The model is benchmarked on the CAMELS hourly dataset against record-holding MTS-LSTM and multi-frequency LSTM (MF-LSTM) baselines. Results show that h-Diffusion outperforms baselines in terms of general performance and extreme metrics. Furthermore, the h-Diffusion model can utilize the inpainting technique and recent observations to accomplish data assimilation that largely improves flood forecasting performance. These advances can greatly reduce flood forecasting uncertainty and provide a unified probabilistic framework for downscaling, prediction, and data assimilation at the hourly scale, representing risks where daily models cannot. |
| 2025-10-09 | [Stochastic Volatility-in-mean VARs with Time-Varying Skewness](http://arxiv.org/abs/2510.08415v1) | Leonardo N. Ferreira, Haroon Mumtaz et al. | This paper introduces a Bayesian vector autoregression (BVAR) with stochastic volatility-in-mean and time-varying skewness. Unlike previous approaches, the proposed model allows both volatility and skewness to directly affect macroeconomic variables. We provide a Gibbs sampling algorithm for posterior inference and apply the model to quarterly data for the US and the UK. Empirical results show that skewness shocks have economically significant effects on output, inflation and spreads, often exceeding the impact of volatility shocks. In a pseudo-real-time forecasting exercise, the proposed model outperforms existing alternatives in many cases. Moreover, the model produces sharper measures of tail risk, revealing that standard stochastic volatility models tend to overstate uncertainty. These findings highlight the importance of incorporating time-varying skewness for capturing macro-financial risks and improving forecast performance. |
| 2025-10-09 | [Revisiting Hallucination Detection with Effective Rank-based Uncertainty](http://arxiv.org/abs/2510.08389v1) | Rui Wang, Zeming Wei et al. | Detecting hallucinations in large language models (LLMs) remains a fundamental challenge for their trustworthy deployment. Going beyond basic uncertainty-driven hallucination detection frameworks, we propose a simple yet powerful method that quantifies uncertainty by measuring the effective rank of hidden states derived from multiple model outputs and different layers. Grounded in the spectral analysis of representations, our approach provides interpretable insights into the model's internal reasoning process through semantic variations, while requiring no extra knowledge or additional modules, thus offering a combination of theoretical elegance and practical efficiency. Meanwhile, we theoretically demonstrate the necessity of quantifying uncertainty both internally (representations of a single response) and externally (different responses), providing a justification for using representations among different layers and responses from LLMs to detect hallucinations. Extensive experiments demonstrate that our method effectively detects hallucinations and generalizes robustly across various scenarios, contributing to a new paradigm of hallucination detection for LLM truthfulness. |
| 2025-10-09 | [The Two-Sided Clifford Dunkl Transform and Miyachi's Theorem](http://arxiv.org/abs/2510.08347v1) | Mohamed Essenhajy, Said Fahlaoui | Recent advances have extended the Dunkl transform to the setting of Clifford algebras. In particular, the two-sided quaternionic Dunkl transform has been introduced as a Dunkl analogue of the two-dimensional quaternionic Fourier transform. In this paper, we develop the two-sided Clifford Dunkl transform, defined using two square roots of -1 in Cl_{p,q}. We establish its fundamental properties, including the inversion and Plancherel formulas, and provide two explicit expressions for the associated translation operator. Moreover, we prove an analogue of Miyachi's theorem for this transform, thereby extending a classical result in harmonic analysis to the Clifford-Dunkl framework. |
| 2025-10-09 | [Design of chemical recycling processes for PUR foam under uncertainty](http://arxiv.org/abs/2510.08301v1) | Patrick Lotz, Luca Bosetti et al. | Optimization problems in chemical process design involve a significant number of discrete and continuous decisions. When taking into account uncertainties, the search space is very difficult to explore, even for experienced engineers. Moreover, it should be taken into account that while some decisions are fixed at the design stage, other parameters can be adapted to the realization of the uncertainty during the operation of the plant. This leads to a two-stage optimization problem which is difficult to solve. To address this challenge, we propose to combine commercial process simulation software with an evolutionary strategy. This approach is applied to designing a downstream process to isolate valuable products from pyrolysis oil produced by the catalytic pyrolysis of rigid polyurethane foam. The suggested algorithm consistently performed better than a manually designed robust process. Additionally, the analysis of different scenarios provided insight into promising changes in the overall layout of the recycling process. |
| 2025-10-09 | [Adversarial Thermodynamics](http://arxiv.org/abs/2510.08298v1) | Maite Arcos, Philippe Faist et al. | In thermodynamics, an agent's ability to extract work is fundamentally constrained by their environment. Traditional frameworks struggle to capture how strategic decision-making under uncertainty -- particularly an agent's tolerance for risk -- determines the trade-off between extractable work and probability of success in finite-scale experiments. Here, we develop a framework for non-equilibrium thermodynamics based on adversarial resource theories, in which work extraction is modelled as an adversarial game for an agent extracting work. Within this perspective, we recast the Szilard engine as a game isomorphic to Kelly gambling, an information-theoretic model of optimal betting under uncertainty -- but with a thermodynamic utility function. Extending the framework to finite-size regimes, we apply a risk-reward trade-off to find an interpretation of the Renyi-divergences, in terms of extractable work for a given failure probability. By incorporating risk sensitivity via utility functions, we show that the guaranteed amount of work a rational agent would accept instead of undertaking a risky protocol is given by a R\'enyi divergence. This provides a unified picture of thermodynamics and gambling, and highlights how generalized free energies emerge from an adversarial setup. |
| 2025-10-08 | [Probing evolution of Long GRB properties through their cosmic formation history aided by Machine Learning predicted redshifts](http://arxiv.org/abs/2510.07306v1) | Dhruv S. Bal, Aditya Narendra et al. | Gamma-ray Bursts (GRBs) are valuable probes of cosmic star formation reaching back into the epoch of reionization, and a large dataset with known redshifts ($z$) is an important ingredient for these studies. Usually, $z$ is measured using spectroscopy or photometry, but $\sim80\%$ of GRBs lack such data. Prompt and afterglow correlations can provide estimates in these cases, though they suffer from systematic uncertainties due to assumed cosmologies and due to detector threshold limits. We use a sample with $z$ estimated via machine learning models, based on prompt and afterglow parameters, without relying on cosmological assumptions. We then use an augmented sample of GRBs with measured and predicted redshifts, forming a larger dataset. We find that the predicted redshifts are a crucial step forward in understanding the evolution of GRB properties. We test three cases: no evolution, an evolution of the beaming factor, and an evolution of all terms captured by an evolution factor $(1+z)^\delta$. We find that these cases can explain the density rate in the redshift range between 1-2, but neither of the cases can explain the derived rate densities at smaller and higher redshifts, which may point towards an evolution term different than a simple power law. Another possibility is that this mismatch is due to the non-homogeneity of the sample, e.g., a non-collapsar origin of some long GRB within the sample. |
| 2025-10-08 | [On the Convergence of Moral Self-Correction in Large Language Models](http://arxiv.org/abs/2510.07290v1) | Guangliang Liu, Haitao Mao et al. | Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance. |
| 2025-10-08 | [Muonium HFS Uncertainty Revisited](http://arxiv.org/abs/2510.07281v1) | Michael I. Eides | Uncertainty of the quantum electrodynamics theoretical prediction for the hyperfine splitting in the ground state of muonium is considered. It is compared with the respective discussion in the two most recent CODATA adjustments of the fundamental physical constants. |
| 2025-10-08 | [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](http://arxiv.org/abs/2510.07210v1) | Donald Pfaffmann, Matthias Klusch et al. | We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proximal policy optimization and approximated online POMDP planning with heuristic confidence-based vertical pruning to reduce its execution time without compromising safety of driving. Our experimental performance analysis on the CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed that HyPlan may navigate safer than selected relevant baselines and perform significantly faster than considered alternative online POMDP planners. |
| 2025-10-08 | [A Broader View of Thompson Sampling](http://arxiv.org/abs/2510.07208v1) | Yanlin Qu, Hongseok Namkoong et al. | Thompson Sampling is one of the most widely used and studied bandit algorithms, known for its simple structure, low regret performance, and solid theoretical guarantees. Yet, in stark contrast to most other families of bandit algorithms, the exact mechanism through which posterior sampling (as introduced by Thompson) is able to "properly" balance exploration and exploitation, remains a mystery. In this paper we show that the core insight to address this question stems from recasting Thompson Sampling as an online optimization algorithm. To distill this, a key conceptual tool is introduced, which we refer to as "faithful" stationarization of the regret formulation. Essentially, the finite horizon dynamic optimization problem is converted into a stationary counterpart which "closely resembles" the original objective (in contrast, the classical infinite horizon discounted formulation, that leads to the Gittins index, alters the problem and objective in too significant a manner). The newly crafted time invariant objective can be studied using Bellman's principle which leads to a time invariant optimal policy. When viewed through this lens, Thompson Sampling admits a simple online optimization form that mimics the structure of the Bellman-optimal policy, and where greediness is regularized by a measure of residual uncertainty based on point-biserial correlation. This answers the question of how Thompson Sampling balances exploration-exploitation, and moreover, provides a principled framework to study and further improve Thompson's original idea. |
| 2025-10-08 | [Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks](http://arxiv.org/abs/2510.07199v1) | Shirin Shoushtari, Edward P. Chandler et al. | Poisson denoising plays a central role in photon-limited imaging applications such as microscopy, astronomy, and medical imaging. It is common to train deep learning models for denoising using the mean-squared error (MSE) loss, which corresponds to computing the posterior mean $\mathbb{E}[x \mid y]$. When the noise is Gaussian, Tweedie's formula enables approximation of the posterior distribution through its higher-order moments. However, this connection no longer holds for Poisson denoising: while $ \mathbb{E}[x \mid y] $ still minimizes MSE, it fails to capture posterior uncertainty. We propose a new strategy for Poisson denoising based on training a log-network. Instead of predicting the posterior mean $ \mathbb{E}[x \mid y] $, the log-network is trained to learn $\mathbb{E}[\log x \mid y]$, leveraging the logarithm as a convenient parameterization for the Poisson distribution. We provide a theoretical proof that the proposed log-network enables recovery of higher-order posterior moments and thus supports posterior approximation. Experiments on simulated data show that our method matches the denoising performance of standard MMSE models while providing access to the posterior. |
| 2025-10-08 | [Bayesian Portfolio Optimization by Predictive Synthesis](http://arxiv.org/abs/2510.07180v1) | Masahiro Kato, Kentaro Baba et al. | Portfolio optimization is a critical task in investment. Most existing portfolio optimization methods require information on the distribution of returns of the assets that make up the portfolio. However, such distribution information is usually unknown to investors. Various methods have been proposed to estimate distribution information, but their accuracy greatly depends on the uncertainty of the financial markets. Due to this uncertainty, a model that could well predict the distribution information at one point in time may perform less accurately compared to another model at a different time. To solve this problem, we investigate a method for portfolio optimization based on Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for meta-learning. We assume that investors have access to multiple asset return prediction models. By using BPS with dynamic linear models to combine these predictions, we can obtain a Bayesian predictive posterior about the mean rewards of assets that accommodate the uncertainty of the financial markets. In this study, we examine how to construct mean-variance portfolios and quantile-based portfolios based on the predicted distribution information. |
| 2025-10-08 | [Optimal network pricing with oblivious users: a new model and algorithm](http://arxiv.org/abs/2510.07157v1) | Yixuan Li, Andersen Ang et al. | Traffic modeling is important in modern society. In this work we propose a new model on the optimal network pricing (Onp) with the assumption of oblivious users, in which the users remain oblivious to real-time traffic conditions and others' behavior. Inspired by works on transportation research and network pricing for selfish traffic, we mathematically derive and prove a new formulation of Onp with decision-dependent modeling that relax certain existing modeling constraints in the literature. Then, we express the Onp formulation as a constrained nonconvex stochastic quadratic program with uncertainty, and we propose an efficient algorithm to solve the problem, utilizing graph theory, sparse linear algebra and stochastic approximation. Lastly, we showcase the effectiveness of the proposed algorithm and the usefulness of the new Onp formulation. The proposed algorithm achieves a 5x speedup by exploiting the sparsity structure of the model. |
| 2025-10-08 | [CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized Cluster Strong Lens Modeling](http://arxiv.org/abs/2510.07131v1) | Yushan Xie, Huanyuan Shan et al. | Strongly lensed supernovae (glSNe) provide a powerful, independent method to measure the Hubble constant, $H_{0}$, through time delays between their multiple images. The accuracy of this measurement depends critically on both the precision of time delay estimation and the robustness of lens modeling. In many current cluster-scale modeling algorithms, all multiple images used for modeling are simplified as point sources to reduce computational costs. In the first paper of the CURLING program, we demonstrated that such a point-like approximation can introduce significant uncertainties and biases in both magnification reconstruction and cosmological inference. In this study, we explore how such simplifications affect $H_0$ measurements from glSNe. We simulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at $z=0.336$, assuming time delays are measured from LSST-like light curves. The lens model is constructed using JWST-like imaging data, utilizing both Lenstool and a pixelated method developed in CURLING. Under a fiducial cosmology with $H_0=70\rm \ km \ s^{-1}\ Mpc^{-1}$, the Lenstool model yields $H_0=69.91^{+6.27}_{-5.50}\rm \ km\ s^{-1}\ Mpc^{-1}$, whereas the pixelated framework improves the precision by over an order of magnitude, $H_0=70.39^{+0.82}_{-0.60}\rm \ km \ s^{-1}\ Mpc^{-1}$. Our results indicate that in the next-generation observations (e.g., JWST), uncertainties from lens modeling dominate the error budget for $H_0$ inference, emphasizing the importance of incorporating the extended surface brightness of multiple images to fully leverage the potential of glSNe for cosmology. |
| 2025-10-08 | [Mitigating Increase-Decrease Gaming with Alternative Connection Agreements: A Defender-Attacker-Defender Game](http://arxiv.org/abs/2510.07102v1) | Bart van der Holst, Thomas Swarts et al. | Redispatch markets are widely used by system operators to manage network congestion. A well-known drawback, however, is that Flexibility Service Providers (FSPs) may strategically adjust their baselines in anticipation of redispatch actions, thereby aggravating congestion and raising system costs. To address this increase-decrease gaming, Distribution System Operators (DSOs) could use Alternative Connection Agreements (ACAs) to conditionally limit the available connection capacity of market participants in the day-ahead stage. In this paper, we present a novel Defender-Attacker-Defender game to investigate the potential of this approach in distribution networks under load and price uncertainty. We solve the resulting trilevel optimization model using a custom branch-and-bound algorithm, and we demonstrate that it efficiently solves the problem without exploring many nodes in the branch-and-bound search tree for most simulated scenarios. The case study demonstrates that applying ACAs can substantially lower redispatch costs (e.g. by 25%) for the DSO with only a limited impact on FSP profits. The effectiveness of the approach critically depends on how often the DSO can invoke ACAs and on the extent to which the DSO can anticipate strategic bidding behavior of the FSP. |
| 2025-10-07 | [Studying the gravitational-wave population without looking that FAR out](http://arxiv.org/abs/2510.06220v1) | Noah E. Wolfe, Matthew Mould et al. | From catalogs of gravitational-wave transients, the population-level properties of their sources and the formation channels of merging compact binaries can be constrained. However, astrophysical conclusions can be biased by misspecification or misestimation of the population likelihood. Despite detection thresholds on the false-alarm rate (FAR) or signal-to-noise ratio (SNR), the current catalog is likely contaminated by noise transients. Further, computing the population likelihood becomes less accurate as the catalog grows. Current methods to address these challenges often scale poorly with the number of events and potentially become infeasible for future catalogs. Here, we evaluate a simple remedy: increasing the significance threshold for including events in population analyses. To determine the efficacy of this approach, we analyze simulated catalogs of up to 1600 gravitational-wave signals from black-hole mergers using full Bayesian parameter estimation with current detector sensitivities. We show that the growth in statistical uncertainty about the black-hole population, as we analyze fewer events but with higher SNR, depends on the source parameters of interest. When the SNR threshold is raised from 11 to 15 -- reducing our catalog size by two--thirds -- we find that statistical uncertainties on the mass distribution only grow by a few 10% and constraints on the spin distribution are essentially unchanged; meanwhile, uncertainties on the high-redshift cosmic merger rate more than double. Simultaneously, numerical uncertainty in the estimate of the population likelihood more than halves, allowing us to ensure unbiased inference without additional computational expense. Our results demonstrate that focusing on higher-significance events is an effective way to facilitate robust astrophysical inference with growing gravitational-wave catalogs. |
| 2025-10-07 | [Conformalized Gaussian processes for online uncertainty quantification over graphs](http://arxiv.org/abs/2510.06181v1) | Jinwen Xu, Qin Lu et al. | Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP. |
| 2025-10-07 | [Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques](http://arxiv.org/abs/2510.06136v1) | Jieyun Wang, Anna L. Smith | Latent space models assume that network ties are more likely between nodes that are closer together in an underlying latent space. Euclidean space is a popular choice for the underlying geometry, but hyperbolic geometry can mimic more realistic patterns of ties in complex networks. To identify the underlying geometry, past research has applied non-Euclidean extensions of multidimensional scaling (MDS) to the observed geodesic distances: the shortest path lengths between nodes. The difference in stress, a standard goodness-of-fit metric for MDS, across the geometries is then used to select a latent geometry with superior model fit (lower stress). The effectiveness of this method is assessed through simulations of latent space networks in Euclidean and hyperbolic geometries. To better account for uncertainty, we extend permutation-based hypothesis tests for MDS to the latent network setting. However, these tests do not incorporate any network structure. We propose a parametric bootstrap distribution of networks, conditioned on observed geodesic distances and the Gaussian Latent Position Model (GLPM). Our method extends the Davidson-MacKinnon J-test to latent space network models with differing latent geometries. We pay particular attention to large and sparse networks, and both the permutation test and the bootstrapping methods show an improvement in detecting the underlying geometry. |
| 2025-10-07 | [Multiprobe constraints on early and late time dark energy](http://arxiv.org/abs/2510.06114v1) | Alexander Reeves, Simone Ferraro et al. | We perform a multiprobe analysis combining cosmic microwave background (CMB) data from Planck and the Atacama Cosmology Telescope (ACT), ACT CMB lensing, and large-scale structure (LSS) measurements from the Dark Energy Spectroscopic Instrument (DESI), including DESI Legacy Imaging Survey (LS) galaxies and baryon acoustic oscillations (BAOs). We present the first $5\times2$pt analysis of ACT DR6 lensing, DESI LS, and Planck ISW. Within $\Lambda$CDM, this yields $S_8 = \sigma_8(\Omega_m/0.3)^{0.5} = 0.819 \pm 0.016$, in good agreement with primary CMB inferences and provides a sound-horizon-free Hubble constant constraint of $H_0 = 70.0 \pm 4.4$ km s$^{-1}$ Mpc$^{-1}$. Then, combining with CMB primary and BAO, we reconfirm a CMB-BAO discrepancy in the $\Omega_m$-$\frac{D_v}{r_d}$ plane, which is heightened when combining BAO with the $5\times2$pt data vector. We explore two dark-energy extensions that may reconcile this: an early-time modification, early dark energy (EDE), and late-time dynamical dark energy (DDE) parameterized by $w_0w_a$. For CMB primary+BAO+$5\times2$pt, we find a $3.3\sigma$ preference for DDE over $\Lambda$CDM, while EDE is modestly favoured at $2.3\sigma$. The models address different shortcomings of $\Lambda$CDM: DDE relaxes the neutrino mass bound ($M_\nu<0.17$eV vs. $<0.050$eV under $\Lambda$CDM), making it compatible with neutrino oscillation measurements, while EDE raises the Hubble constant to $H_0=70.5\pm1.2\,\mathrm{km\,s^{-1}\,Mpc^{-1}}$, easing the discrepancy with SH0ES. However, neither model resolves both issues simultaneously. Our analysis indicates that both DDE and EDE remain viable extensions of $\Lambda$CDM within current uncertainties and demonstrates the capacity of combined probes to place increasingly stringent constraints on cosmological parameters. |
| 2025-10-07 | [Mass loading of outflows from evolving Young Massive Clusters](http://arxiv.org/abs/2510.06100v1) | C. J. K. Larkin, C. Hawcroft et al. | Feedback from Young Massive Clusters (YMCs) is an important driver of galaxy evolution. In the first few Myr, mechanical feedback is dominated by collective effects of the massive stellar winds in the YMC. The mass-loss rates and terminal wind velocities of these stars change by orders of magnitude over pre-SN timescales as the massive stars evolve, and mass-loss rates of Cool Supergiant (CSG) stars in particular are uncertain by a factor $\sim~20$ or more. In this work we perform a first study of the time evolution of average cluster wind velocity $\bar{V}_{\mathrm{cl}}$ as a function of stellar metallicity $Z$, assuming single star evolution. We also check the validity of assuming Wolf-Rayet stars dominate the feedback effects of a YMC, as often done when interpreting X-ray and $\gamma$-ray observations, and test how sensitive $\bar{V}_{\mathrm{cl}}$ is to current uncertainties in mass-loss rates. We use pySTARBURST99 to calculate integrated properties of YMCs for $Z$ in the range of $0.0004-0.02$, representing a range of environments from IZw18 to the Galactic Centre. We find that $\bar{V}_{\mathrm{cl}}$ drops off rapidly for sub-LMC $Z$, and we recommend a value of $500-1000\,~\textrm{km~s}^{-1}$ be used in this regime. We show accounting only for WR stars can overestimate $\bar{V}_{\mathrm{cl}}$ by $500-2000\,~\textrm{km~s}^{-1}$ at $Z \geq Z_\text{LMC}$. We also find that different RSG mass-loss assumptions can change the inferred $\bar{V}_{\mathrm{cl}}$ by $\sim1000\,~\textrm{km~s}^{-1}$, highlighting the need for improved observational constraints for RSGs in YMCs. |
| 2025-10-07 | [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](http://arxiv.org/abs/2510.06096v1) | Matthieu Bou, Nyal Patel et al. | The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI. |
| 2025-10-07 | [Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data](http://arxiv.org/abs/2510.06080v1) | Nga Nguyen, Olivier Bonnefon et al. | Biological control strategies against mosquito-borne diseases--such as the sterile insect technique (SIT), RIDL, and Wolbachia-based releases--require reliable estimates of dispersal and survival of released males. We propose a mechanistic--statistical framework for mark--release--recapture (MRR) data linking an individual-based 2D diffusion model with its reaction--diffusion limit. Inference is based on solving the macroscopic system and embedding it in a Poisson observation model for daily trap counts, with uncertainty quantified via a parametric bootstrap. We validate identifiability using simulated data and apply the model to an urban MRR campaign in El Cano (Havana, Cuba) involving four weekly releases of sterile Aedes aegypti males. The best-supported model suggests a mean life expectancy of about five days and a typical displacement of about 180 m. Unlike empirical fits of survival or dispersal, our mechanistic approach jointly estimates movement, mortality, and capture, yielding biologically interpretable parameters and a principled framework for designing and evaluating SIT-based interventions. |
| 2025-10-07 | [Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition](http://arxiv.org/abs/2510.06033v1) | Jim Dai, Manxi Wu et al. | Stochastic processing networks (SPNs) have broad applications in healthcare, transportation, and communication networks. The control of SPN is to dynamically assign servers in batches under uncertainty to optimize long-run performance. This problem is challenging as the policy dimension grows exponentially with the number of servers, making standard reinforcement learning and policy optimization methods intractable at scale. We propose an atomic action decomposition framework that addresses this scalability challenge by breaking joint assignments into sequential single-server assignments. This yields policies with constant dimension, independent of the number of servers. We study two classes of atomic policies, the step-dependent and step-independent atomic policies, and prove that both achieve the same optimal long-run average reward as the original joint policies. These results establish that computing the optimal SPN control can be made scalable without loss of optimality using the atomic framework. Our results offer theoretical justification for the strong empirical success of the atomic framework in large-scale applications reported in previous articles. |
| 2025-10-07 | [Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers](http://arxiv.org/abs/2510.06025v1) | Kevin Raina, Tanya Schmah | Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST and CIFAR-10 In-Distributions, with 5000 training samples or less, show that the Bayesian methods outperform corresponding deterministic methods. |
| 2025-10-07 | [Uncertainty in Machine Learning](http://arxiv.org/abs/2510.06007v1) | Hans Weytjens, Wouter Verbeke | This book chapter introduces the principles and practical applications of uncertainty quantification in machine learning. It explains how to identify and distinguish between different types of uncertainty and presents methods for quantifying uncertainty in predictive models, including linear regression, random forests, and neural networks. The chapter also covers conformal prediction as a framework for generating predictions with predefined confidence intervals. Finally, it explores how uncertainty estimation can be leveraged to improve business decision-making, enhance model reliability, and support risk-aware strategies. |
| 2025-10-06 | [Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A Particle-Tracking Study](http://arxiv.org/abs/2510.05084v1) | Ethan Kahn | Electrospray thrusters are emerging as a leading propulsion technology for CubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power requirements. However, the divergent ion plumes can impinge on spacecraft surfaces, particularly body-mounted solar arrays, causing contamination and thrust efficiency losses. This study presents a validated particle-tracking simulation to quantify the effects of thruster placement on thrust efficiency and surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs a cosine power distribution ($k=1.8$) with half-angle $46^\circ$, validated against experimental data with errors below 7%. Results show that thrust efficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted configurations to 100% for side-mounted configurations with deployable arrays. CubeSat size significantly affects impingement: 3U platforms experience 46.4% contamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable solar arrays reduce contamination by 77% compared to body-mounted arrays, while side-mounted thrusters eliminate impingement entirely at the cost of only 1.6% efficiency loss. Corner-mounted configurations at $30^\circ$ cant provide intermediate performance with 88.9% efficiency and 11.1% contamination. These quantitative design guidelines enable mission planners to optimize thruster integration based on power budget and propellant mass constraints, with statistical uncertainty below 0.15% across all configurations. |
| 2025-10-06 | [The role of entropy production and thermodynamic uncertainty relations in the thermalization of open quantum systems](http://arxiv.org/abs/2510.05072v1) | Ãlvaro Tejero | The asymmetry between heating and cooling in open quantum systems is a hallmark of nonequilibrium dynamics, yet its thermodynamic origin has remained unclear. Here, we investigate the thermalization of a quantum system weakly coupled to a thermal bath, focusing on the entropy production rate and the quantum thermokinetic uncertainty relation (TKUR). We derive an analytical expression for the entropy production rate, showing that heating begins with a higher entropy production, which drives faster thermalization than cooling. The quantum TKUR links this asymmetry to heat current fluctuations, demonstrating that larger entropy production suppresses fluctuations, making heating more stable than cooling. Our results reveal the thermodynamic basis of asymmetric thermalization and highlight uncertainty relations as key to nonequilibrium quantum dynamics. |
| 2025-10-06 | [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](http://arxiv.org/abs/2510.05054v1) | Peter Van Katwyk, Karianne J. Bergen | Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework. |
| 2025-10-06 | [Inferring the spins of merging black holes in the presence of data-quality issues](http://arxiv.org/abs/2510.05029v1) | Rhiannon Udall, Sophie Bini et al. | Gravitational waves from black hole binary mergers carry information about the component spins, but inference is sensitive to analysis assumptions, which may be broken by terrestrial noise transients known as glitches. Using a variety of simulated glitches and gravitational wave signals, we study the conditions under which glitches can bias spin measurements. We confirm the theoretical expectation that inference and subtraction of glitches invariably leaves behind residual power due to statistical uncertainty, no matter the strength (signal-to-noise ratio; SNR) of the original glitch. Next we show that low-SNR glitches - including those below the threshold for flagging data-quality issues - can still significantly bias spin inference. Such biases occur for a range of glitch morphologies, even in cases where glitches and signals are not precisely aligned in phase. Furthermore, we find that residuals of glitch subtraction can result in biases as well. Our results suggest that joint inference of the glitch and gravitational wave parameters, with appropriate models and priors, is required to address these uncertainties inherent in glitch mitigation via subtraction. |
| 2025-10-06 | [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](http://arxiv.org/abs/2510.05027v1) | Ethan Davis | We introduce a framework for applying metaheuristic algorithms, such as ant colony optimization (ACO), to combinatorial optimization problems (COPs) like the traveling salesman problem (TSP). The framework consists of three sequential stages: broad exploration of the parameter space, exploitation of top-performing parameters, and uncertainty quantification (UQ) to assess the reliability of results. As a case study, we apply ACO to the TSPLIB berlin52 dataset, which has a known optimal tour length of 7542. Using our framework, we calculate that the probability of ACO finding the global optimum is approximately 1/40 in a single run and improves to 1/5 when aggregated over ten runs. |
| 2025-10-06 | [Risk-Adjusted Policy Learning and the Social Cost of Uncertainty: Theory and Evidence from CAP evaluation](http://arxiv.org/abs/2510.05007v1) | Giovanni Cerulli, Francesco Caracciolo | This paper develops a risk-adjusted alternative to standard optimal policy learning (OPL) for observational data by importing Roy's (1952) safety-first principle into the treatment assignment problem. We formalize a welfare functional that maximizes the probability that outcomes exceed a socially required threshold and show that the associated pointwise optimal rule ranks treatments by the ratio of conditional means to conditional standard deviations. We implement the framework using microdata from the Italian Farm Accountancy Data Network to evaluate the allocation of subsidies under the EU Common Agricultural Policy. Empirically, risk-adjusted optimal policies systematically dominate the realized allocation across specifications, while risk aversion lowers overall welfare relative to the risk-neutral benchmark, making transparent the social cost of insurance against uncertainty. The results illustrate how safety-first OPL provides an implementable, interpretable tool for risk-sensitive policy design, quantifying the efficiency-insurance trade-off that policymakers face when outcomes are volatile. |
| 2025-10-06 | [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](http://arxiv.org/abs/2510.05006v1) | Koen Vellenga, H. Joe Steinhauer et al. | Deep neural networks (DNNs) are increasingly applied to safety-critical tasks in resource-constrained environments, such as video-based driver action and intention recognition. While last layer probabilistic deep learning (LL-PDL) methods can detect out-of-distribution (OOD) instances, their performance varies. As an alternative to last layer approaches, we propose extending pre-trained DNNs with transformation layers to produce multiple latent representations to estimate the uncertainty. We evaluate our latent uncertainty representation (LUR) and repulsively trained LUR (RLUR) approaches against eight PDL methods across four video-based driver action and intention recognition datasets, comparing classification performance, calibration, and uncertainty-based OOD detection. We also contribute 28,000 frame-level action labels and 1,194 video-level intention labels for the NuScenes dataset. Our results show that LUR and RLUR achieve comparable in-distribution classification performance to other LL-PDL approaches. For uncertainty-based OOD detection, LUR matches top-performing PDL methods while being more efficient to train and easier to tune than approaches that require Markov-Chain Monte Carlo sampling or repulsive training procedures. |
| 2025-10-06 | [Doubly Discordant SH0ES NGC4258 Cepheid Relations (HVI), and Impactful Extinction Laws](http://arxiv.org/abs/2510.04998v1) | Daniel Majaess | S$H_0$ES 2016-2022 $HVI$ data for classical Cepheids in the keystone galaxy NGC4258 yield doubly discordant Wesenheit Leavitt functions:~$\Delta W_{0,H-VI} = -0.13\pm0^{m}.02$ ($-0^{m}.17$ unweighted) and that is paired with a previously noted $\Delta W_{0,I-VI}\simeq-0^{m}.3$, which in concert with complimentary evidence suggest the 2016 S$H_0$ES NGC4258-anchored $H_0 \pm \sigma_{H_0}$ warrants scrutiny (i.e., $\sigma_{H_0}/{H_0}\gtrsim 6$\%). Cepheid distance uncertainties are further exacerbated by extinction law ambiguities endemic to such Leavitt relations (e.g., NGC4258), particularly for comparatively obscured variables (e.g., $\Delta d \gtrsim 4$\%, reddened Cepheid subsamples in the Milky Way, M31, NGC2442, NGC4424, NGC5643, NGC7250). Lastly, during the analysis it was identified that the 2022 S$H_0$ES database relays incorrect SMC Cepheid photometry. |
| 2025-10-06 | [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](http://arxiv.org/abs/2510.04996v1) | Wei Xiong, Chenlu Ye et al. | Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada. |
| 2025-10-06 | [Line emission search from DM annihilation in the Galactic Center with LST-1](http://arxiv.org/abs/2510.04977v1) | Abhishek Abhishek, Shotaro Abe et al. | Dark Matter remains a great mystery in modern physics. Among various candidates, the weakly interacting massive particles (WIMPs) scenario stands out and is under extensive study. The detection of the hypothetical gamma-ray emission from WIMP annihilation could act as a direct probe of electroweak-scale interactions, complementing DM collider searches and other direct DM detection techniques. At very high energies (VHE), WIMP self-annihilation is expected to produce gamma rays together with other Standard Model particles. The galactic center (GC), due to its relative proximity to the Earth and its high expected DM density, is a prime target for monoenergetic line searches. IACTs have placed strong constraints on the DM properties at the GC, with the MAGIC providing the most stringent limits from 20 TeV to 100 TeV, exploiting large zenith angle (LZA) observations. However, the limited field of view (FoV) of the MAGIC telescopes (< 3.5{\deg} ) prevented a detailed study of the extended region around the GC in which an enhanced DM density is expected. The LST-1 of the CTAO, located at the Roque de Los Muchachos Observatory (La Palma, Spain), close to the MAGIC site, has been observing the GC since 2021. With its wide FoV of 4.5{\deg}, LST-1 could contribute significantly to the WIMPs search at the GC. The observations are performed at LZA (ZA > 58{\deg}), which, while required due to the source's low altitude, also optimizes the detection of gamma rays up to 100 TeV and beyond. We present a study of the systematic uncertainties in WIMP line emission searches with LST-1. Our work examines the instrument response functions for LZA observations, background rejection in monoscopic mode, and includes updated results from simulations, highlighting new methods for spectral line searches. |
| 2025-10-03 | [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](http://arxiv.org/abs/2510.03181v1) | Ha Manh Bui, Felix Parker et al. | We study the Non-Stationary Reinforcement Learning (RL) under distribution shifts in both finite-horizon episodic and infinite-horizon discounted Markov Decision Processes (MDPs). In the finite-horizon case, the transition functions may suddenly change at a particular episode. In the infinite-horizon setting, such changes can occur at an arbitrary time step during the agent's interaction with the environment. While the Q-learning Upper Confidence Bound algorithm (QUCB) can discover a proper policy during learning, due to the distribution shifts, this policy can exploit sub-optimal rewards after the shift happens. To address this issue, we propose Density-QUCB (DQUCB), a shift-aware Q-learning~UCB algorithm, which uses a transition density function to detect distribution shifts, then leverages its likelihood to enhance the uncertainty estimation quality of Q-learning~UCB, resulting in a balance between exploration and exploitation. Theoretically, we prove that our oracle DQUCB achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the computational efficiency of model-free RL and outperforms QUCB baselines by having a lower regret across RL tasks, as well as a real-world COVID-19 patient hospital allocation task using a Deep-Q-learning architecture. |
| 2025-10-03 | [Calibrated Uncertainty Sampling for Active Learning](http://arxiv.org/abs/2510.03162v1) | Ha Manh Bui, Iliana Maifeld-Carucci et al. | We study the problem of actively learning a classifier with a low calibration error. One of the most popular Acquisition Functions (AFs) in pool-based Active Learning (AL) is querying by the model's uncertainty. However, we recognize that an uncalibrated uncertainty model on the unlabeled pool may significantly affect the AF effectiveness, leading to sub-optimal generalization and high calibration error on unseen data. Deep Neural Networks (DNNs) make it even worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty. Specifically, we utilize a kernel calibration error estimator under the covariate shift and formally show that AL with this AF eventually leads to a bounded calibration error on the unlabeled pool and unseen test data. Empirically, our proposed method surpasses other AF baselines by having a lower calibration and generalization error across pool-based AL settings. |
| 2025-10-03 | [Stimulus-Voltage-Based Prediction of Action Potential Onset Timing: Classical vs. Quantum-Inspired Approaches](http://arxiv.org/abs/2510.03155v1) | Stevens Johnson, Varun Puram et al. | Accurate modeling of neuronal action potential (AP) onset timing is crucial for understanding neural coding of danger signals. Traditional leaky integrate-and-fire (LIF) models, while widely used, exhibit high relative error in predicting AP onset latency, especially under strong or rapidly changing stimuli. Inspired by recent experimental findings and quantum theory, we present a quantum-inspired leaky integrate-and-fire (QI-LIF) model that treats AP onset as a probabilistic event, represented by a Gaussian wave packet in time. This approach captures the biological variability and uncertainty inherent in neuronal firing. We systematically compare the relative error of AP onset predictions between the classical LIF and QI-LIF models using synthetic data from hippocampal and sensory neurons subjected to varying stimulus amplitudes. Our results demonstrate that the QI-LIF model significantly reduces prediction error, particularly for high-intensity stimuli, aligning closely with observed biological responses. This work highlights the potential of quantum-inspired computational frameworks in advancing the accuracy of neural modeling and has implications for quantum engineering approaches to brain-inspired computing. |
| 2025-10-03 | [Statistical framework for nuclear parameter uncertainties in nucleosynthesis modeling of r- and i-process](http://arxiv.org/abs/2510.03138v1) | S. Martinet, G. Goriely et al. | Propagating nuclear uncertainties to nucleosynthesis simulations is key to understand the impact of theoretical uncertainties on the predictions, especially for processes far from the stability region, where nuclear properties are scarcely known. While systematic (model) uncertainties have been thoroughly studied, the statistical (parameter) ones have been more rarely explored, as constraining them is more challenging. We present here a methodology to determine coherently parameter uncertainties by anchoring the theoretical uncertainties to the experimentally known nuclear properties through the use of the Backward Forward Monte Carlo method. We use this methodology for two nucleosynthesis processes: the intermediate neutron capture process (i-process) and the rapid neutron capture process (r-process). We determine coherently for the i-process the uncertainties from the (n,$\gamma$) rates while we explore the impact of nuclear mass uncertainties for the r-process. The effect of parameter uncertainties on the final nucleosynthesis is in the same order as model uncertainties, suggesting the crucial need for more experimental constraints on key nuclei of interest. We show how key nuclear properties, such as relevant (n,$\gamma$) rates impacting the i-process tracers, could enhance tremendously the prediction of stellar evolution models by experimentally constraining them. |
| 2025-10-03 | [A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control](http://arxiv.org/abs/2510.03100v1) | Tianhua Gao | Quadrotor stability under complex dynamic disturbances and model uncertainties poses significant challenges. One of them remains the underfitting problem in high-dimensional features, which limits the identification capability of current learning-based methods. To address this, we introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from which we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for geometric control. Specifically, the high-dimensional mapping for identification is axially ``sliced" into multiple low-dimensional submappings (``slices"). In this way, the complex high-dimensional problem is decomposed into a set of simple low-dimensional tasks addressed by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation without any pre-training or persistent excitation (PE) condition. To enhance the interpretability of the proposed approach, we prove that the full-state closed-loop system exhibits arbitrarily close to exponential stability despite multi-dimensional time-varying disturbances and model uncertainties. This result is novel as it demonstrates exponential convergence without requiring pre-training for unknown disturbances and specific knowledge of the model. |
| 2025-10-03 | [Polarization dependence of spin-electric transitions in molecular exchange qubits](http://arxiv.org/abs/2510.03099v1) | Filippo Troiani, Athanassios K. Boudalis | Quasi-optical experiments are emerging as a powerful technique to probe magnetic transitions in molecular spin systems. However, the simultaneous presence of the electric- and magnetic-dipole induced transitions poses the challenge of discriminating between these two contributions. Besides, the identification of the spin-electric transitions can hardly rely on the peak intensity, because of the current uncertainties on the value of the spin-electric coupling in most molecular compounds. Here, we compute the polarizations required for electric- and magnetic-dipole induced transitions through spin-Hamiltonian models of molecular spin triangles. We show that the polarization allows a clear discrimination between the two kinds of transitions. In addition, it allows one to identify the physical origin of the zero-field splitting in the ground multiplet, a debated issue with significant implications on the coherence properties of the spin qubit implemented in molecular spin triangles. |
| 2025-10-03 | [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](http://arxiv.org/abs/2510.03046v1) | Soohaeng Yoo Willow, Tae Hyeon Park et al. | Machine learning potentials (MLPs) have become essential for large-scale atomistic simulations, enabling ab initio-level accuracy with computational efficiency. However, current MLPs struggle with uncertainty quantification, limiting their reliability for active learning, calibration, and out-of-distribution (OOD) detection. We address these challenges by developing Bayesian E(3) equivariant MLPs with iterative restratification of many-body message passing. Our approach introduces the joint energy-force negative log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models uncertainty in both energies and interatomic forces, yielding superior accuracy compared to conventional NLL losses. We systematically benchmark multiple Bayesian approaches, including deep ensembles with mean-variance estimation, stochastic weight averaging Gaussian, improved variational online Newton, and laplace approximation by evaluating their performance on uncertainty prediction, OOD detection, calibration, and active learning tasks. We further demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by quantifying energy and force uncertainties. Using Bayesian active learning by disagreement (BALD), our framework outperforms random sampling and energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs achieve competitive accuracy with state-of-the-art models while enabling uncertainty-guided active learning, OOD detection, and energy/forces calibration. This work establishes Bayesian equivariant neural networks as a powerful framework for developing uncertainty-aware MLPs for atomistic simulations at scale. |
| 2025-10-03 | [Distributional Inverse Reinforcement Learning](http://arxiv.org/abs/2510.03013v1) | Feiyang Wu, Ye Zhao et al. | We propose a distributional framework for offline Inverse Reinforcement Learning (IRL) that jointly models uncertainty over reward functions and full distributions of returns. Unlike conventional IRL approaches that recover a deterministic reward estimate or match only expected returns, our method captures richer structure in expert behavior, particularly in learning the reward distribution, by minimizing first-order stochastic dominance (FSD) violations and thus integrating distortion risk measures (DRMs) into policy learning, enabling the recovery of both reward distributions and distribution-aware policies. This formulation is well-suited for behavior analysis and risk-aware imitation learning. Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks demonstrate that our method recovers expressive reward representations and achieves state-of-the-art imitation performance. |
| 2025-10-03 | [Real-Time Peer-to-Peer Energy Trading for Multi-Microgrids: Improved Double Auction Mechanism and Prediction-Free Online Trading Approach](http://arxiv.org/abs/2510.02985v1) | Kaidi Huang, Lin Cheng et al. | Peer-to-peer energy trading offers a promising solution for enhancing renewable energy utilization and economic benefits within interconnected microgrids. However, existing real-time P2P markets face two key challenges: high computational complexity in trading mechanisms, and suboptimal participant decision-making under diverse uncertainties. Existing prediction-based decision-making methods rely heavily on accurate forecasts, which are typically unavailable for microgrids, while prediction-free methods suffer from myopic behaviors. To address these challenges, this paper proposes an improved double auction mechanism combined with an adaptive step-size search algorithm to reduce computational burden, and a data-driven dual-reference online optimization (DDOO) framework to enhance participant decision-making. The improved mechanism simplifies bidding procedures, significantly reducing computational burden and ensuring rapid convergence to the market equilibrium. Additionally, the prediction-free DDOO framework mitigates myopic decision-making by introducing two informative reference signals. Case studies on a 20-microgrid system demonstrate the effectiveness and scalability of the proposed mechanism and approach. The improved mechanism significantly decreases the computational time while increasing local energy self-sufficiency periods from 0.01% to 29.86%, reducing reverse power flow periods from 24.51% to 3.96%, and lowering average operating costs by 19.20%. Compared with conventional approaches such as Lyapunov optimization and model predictive control, the DDOO framework achieves a 10%-13% reduction in operating costs with an optimality gap of only 5.76%. |
| 2025-10-03 | [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](http://arxiv.org/abs/2510.02976v1) | Alvaro Paz, Pauli Mustalahti et al. | This paper presents a framework for real-time optimal controlling of a heavy-duty skid-steered mobile platform for trajectory tracking. The importance of accurate real-time performance of the controller lies in safety considerations of situations where the dynamic system under control is affected by uncertainties and disturbances, and the controller should compensate for such phenomena in order to provide stable performance. A multiple-shooting nonlinear model-predictive control framework is proposed in this paper. This framework benefits from suitable algorithm along with readings from various sensors for genuine real-time performance with extremely high accuracy. The controller is then tested for tracking different trajectories where it demonstrates highly desirable performance in terms of both speed and accuracy. This controller shows remarkable improvement when compared to existing nonlinear model-predictive controllers in the literature that were implemented on skid-steered mobile platforms. |
| 2025-10-02 | [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](http://arxiv.org/abs/2510.02279v1) | Mykyta Ielanskyi, Kajetan Schweighofer et al. | Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings. |
| 2025-10-02 | [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](http://arxiv.org/abs/2510.02216v1) | Zeqi Ye, Minshuo Chen | Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance. |
| 2025-10-02 | [Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale](http://arxiv.org/abs/2510.02189v1) | Boris Kriuk | Arctic warming threatens over 100 billion in permafrost-dependent infrastructure across Northern territories, yet existing risk assessment frameworks lack spatiotemporal validation, uncertainty quantification, and operational decision-support capabilities. We present a hybrid physics-machine learning framework integrating 2.9 million observations from 171,605 locations (2005-2021) combining permafrost fraction data with climate reanalysis. Our stacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic Net) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal cross-validation preventing data leakage. To address machine learning limitations in extrapolative climate scenarios, we develop a hybrid approach combining learned climate-permafrost relationships (60%) with physical permafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over 10 years), we project mean permafrost fraction decline of -20.3 pp (median: -20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point loss. Infrastructure risk classification identifies 15% high-risk zones (25% medium-risk) with spatially explicit uncertainty maps. Our framework represents the largest validated permafrost ML dataset globally, provides the first operational hybrid physics-ML forecasting system for Arctic infrastructure, and delivers open-source tools enabling probabilistic permafrost projections for engineering design codes and climate adaptation planning. The methodology is generalizable to other permafrost regions and demonstrates how hybrid approaches can overcome pure data-driven limitations in climate change applications. |
| 2025-10-02 | [On the uncertainty in predicting the stochastic gravitational wave background from compact binary coalescences](http://arxiv.org/abs/2510.02163v1) | Michael Ebersold, Tania Regimbau | The stochastic gravitational-wave background from compact binary coalescences is expected to be the first detectable stochastic signal via cross-correlation searches with terrestrial detectors. It encodes the cumulative merger history of stellar-mass binaries across cosmic time, offering a unique probe of the high-redshift Universe. However, predicting the background spectrum is challenging due to numerous modeling choices, each with distinct uncertainties. In this work, we present a comprehensive forecast of the astrophysical gravitational-wave background from binary black holes, binary neutron stars, and neutron star-black hole systems. We systematically assess the impact of uncertainties in population properties, waveform features, and the modeling of the merger rate evolution. By combining all uncertainties, we derive credible bands for the background spectrum, spanning approximately an order of magnitude in the fractional energy density. These results provide thorough predictions to facilitate the interpretation of current upper limits and future detections. |
| 2025-10-02 | [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](http://arxiv.org/abs/2510.02109v1) | Jong Bum Won, Wesley De Neve et al. | Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast. |
| 2025-10-02 | [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](http://arxiv.org/abs/2510.02088v1) | Alexander Schmitz, Andreas Petersen et al. | The analysis of the size of nanoparticles is an essential task in plasma technology and dusty plasmas. Light scattering techniques, based on Mie theory, can be used as a non-invasive and in-situ diagnostic tool for this purpose. However, the standard back-calculation methods require expertise from the user. To address this, we introduce a neural network that performs the same task. We discuss how we set up and trained the network to analyze the size of plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up to several hundred nanometers, depending on the used wavelength. The diagnostic approach is kinetic, which means that the particles need to change in size due to growth or etching. An uncertainty analysis as well as a test with experimental data are presented. Our neural network achieves results that agree with those of prior fitting algorithms while offering higher methodical stability. The model also holds a major advantage in terms of computing speed and automation. |
| 2025-10-02 | [Event-triggered control and communication for single-master multi-slave teleoperation systems with Try-Once-Discard protocol](http://arxiv.org/abs/2510.02072v1) | Yuling Li, Chenxi Li et al. | Single-master multi-slave (SMMS) teleoperation systems can perform multiple tasks remotely in a shorter time, cover large-scale areas, and adapt more easily to single-point failures, thereby effectively encompassing a broader range of applications. As the number of slave manipulators sharing a communication network increases, the limitation of communication bandwidth becomes critical. To alleviate bandwidth usage, the Try-Once-Discard (TOD) scheduling protocol and event-triggered mechanisms are often employed separately. In this paper, we combine both strategies to optimize network bandwidth and energy consumption for SMMS teleoperation systems. Specifically, we propose event-triggered control and communication schemes for a class of SMMS teleoperation systems using the TOD scheduling protocol. Considering dynamic uncertainties, the unavailability of relative velocities, and time-varying delays, we develop adaptive controllers with virtual observers based on event-triggered schemes to achieve master-slave synchronization. Stability criteria for the SMMS teleoperation systems under these event-triggered control and communication schemes are established, demonstrating that Zeno behavior is excluded. Finally, experiments are conducted to validate the effectiveness of the proposed algorithms. |
| 2025-10-02 | [Strong-lensing rates of massive black hole binaries in LISA](http://arxiv.org/abs/2510.02061v1) | Juan GutiÃ©rrez, Macarena Lagos | Similarly to electromagnetic (EM) signals, gravitational lensing by intervening galaxies can also affect gravitational waves (GWs). In this paper, we estimate the strong-lensing rate of massive black hole mergers observed with LISA. Given the uncertainties in the source populations as well as in the population of galaxies at high redshift, we consider: six different source population models, including light and heavy seeds, as well as three lens population models, including redshift-independent and redshift-dependent evolution properties. Among all the scenarios explored, the expected number of strong lensed events detected in a 4-year observation time in LISA ranges between 0.13-231 with most of them having two (one) images detectable in the heavy (light) seed scenarios. The event numbers obtained correspond to 0.2%-0.9% of all detected unlensed events. Out of all the detectable strong-lensed events, up to 61% (in the light-seed scenario) and 1% (in the heavy-seed scenario) of them are above the detectability threshold solely due to strong lensing effects and would otherwise be undetectable. For detectable pairs of strong-lensed events by galaxy lenses, we also find between 72%-81% of them to have time delays from 1 week to 1 year. |
| 2025-10-02 | [Chemical transport by weakly nonlinear internal gravity waves in stars](http://arxiv.org/abs/2510.02031v1) | Yifeng Mao, Daniel Lecoanet | While it is well-known that internal gravity waves (IGWs) transport chemicals in the radiative zones of stars, there remains substantial uncertainty on the amount of, and physical mechanism behind, this transport. Most previous studies have relied on heuristic theories, or numerical simulations that may be hard to extrapolate to stellar parameters. In this work, we present the first rigorous asymptotic calculation of (passive) chemical transport by IGWs, in the limit of small wave amplitude. We find that the net transport by a coherent packet of waves scales like wave amplitude to the fourth power, and verify these analytic calculations with numerical simulations. Because the transport is equally likely to be positive as negative, the transport by a random superposition of waves is expected to scale as wave amplitude to the eighth power. These results show that closer comparisons between theoretical arguments and numerical calculations are essential for interpreting numerical simulations of chemical transport by IGWs, and making accurate predictions of this process for stellar evolution modeling. |
| 2025-10-02 | [A Large Sample of JWST/NIRSpec Brown Dwarfs: New Distant Discoveries](http://arxiv.org/abs/2510.02026v1) | Zhijun Tu, Shu Wang et al. | Brown dwarfs are essential probes of stellar and planetary formation, yet their low luminosities pose challenges for detection at large Galactic distances. The James Webb Space Telescope (JWST), with its unprecedented near-infrared sensitivity, enables the discovery and characterization of distant substellar objects, including those in the Milky Way's thick disk and halo. We conducted a systematic search using over 40,000 publicly available JWST/NIRSpec PRISM/CLEAR spectra and identified 68 brown dwarfs through spectral template matching and visual inspection. Among them, 12 are newly identified candidates, including 8 T dwarfs and 4 M/L dwarfs, most at distances exceeding 1 kpc. Remarkably, two sources -- JWST J001418.22-302223.2 and JWST J033240.07-274907.8 -- are found at distances greater than 5 kpc, making them the most distant brown dwarfs within the Milky Way. Spectral fits were performed using a nested sampling Monte Carlo algorithm with three model grids: Sonora Elf Owl, LOWZ, and SAND. The analysis reveals that cloud-free models are unable to reproduce L/T transition spectra, whereas the SAND model provides a more accurate representation of cloud effects in metal-poor environments. With the newly identified distant brown dwarfs, we also investigated the vertical metallicity gradient of brown dwarfs. Overall, the metallicities do not show an evident trend with Galactic height $|Z|$, due to the limited sample size and the uncertainties in metallicity measurements. |
| 2025-09-30 | [Uncertainty Quantification for Regression using Proper Scoring Rules](http://arxiv.org/abs/2509.26610v1) | Alexander Fishkov, Kajetan Schweighofer et al. | Quantifying uncertainty of machine learning model predictions is essential for reliable decision-making, especially in safety-critical applications. Recently, uncertainty quantification (UQ) theory has advanced significantly, building on a firm basis of learning with proper scoring rules. However, these advances were focused on classification, while extending these ideas to regression remains challenging. In this work, we introduce a unified UQ framework for regression based on proper scoring rules, such as CRPS, logarithmic, squared error, and quadratic scores. We derive closed-form expressions for the resulting uncertainty measures under practical parametric assumptions and show how to estimate them using ensembles of models. In particular, the derived uncertainty measures naturally decompose into aleatoric and epistemic components. The framework recovers popular regression UQ measures based on predictive variance and differential entropy. Our broad evaluation on synthetic and real-world regression datasets provides guidance for selecting reliable UQ measures. |
| 2025-09-30 | [Beyond Suboptimality: Resource-Rationality and Task Demands Shape the Complexity of Perceptual Representations](http://arxiv.org/abs/2509.26606v1) | Andrew Jun Lee, Daniel Turek et al. | Early theories of perception as probabilistic inference propose that uncertainty about the interpretation of sensory input is represented as a probability distribution over many interpretations -- a relatively complex representation. However, critics argue that persistent demonstrations of suboptimal perceptual decision-making indicate limits in representational complexity. We contend that suboptimality arises not from genuine limits, but participants' resource-rational adaptations to task demands. For example, when tasks are solvable with minimal attention to stimuli, participants may neglect information needed for complex representations, relying instead on simpler ones that engender suboptimality. Across three experiments, we progressively reduced the efficacy of resource-rational strategies on a carefully controlled decision task. Model fits favored simple representations when resource-rational strategies were effective, and favored complex representations when ineffective, suggesting that perceptual representations can be simple or complex depending on task demands. We conclude that resource-rationality is an epistemic constraint for experimental design and essential to a complete theory of perception. |
| 2025-09-30 | [Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning](http://arxiv.org/abs/2509.26605v1) | MaÃ«l Macuglia, Paul Friedrich et al. | Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents. |
| 2025-09-30 | [The JWST EXCELS Survey: A spectroscopic investigation of the ionizing properties of star-forming galaxies at 1<z<8](http://arxiv.org/abs/2509.26591v1) | R. Begley, R. J. McLure et al. | Charting the Epoch of Reionization demands robust assessments of what drives the production of ionizing photons in high-redshift star-forming galaxies (SFGs), and requires better predictive capabilities from current observations. Using a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution spectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical analysis of their ionizing photon production efficiencies ($\xi_\rm{ion}$). We consider $\xi_\rm{ion}$, measured with Balmer line measurements, in relation to a number of key galaxy properties including; nebular emission line strengths ($W_\lambda(\rm{H\alpha})$ and $W_\lambda$( [OIII])), UV luminosity ($M_\rm{UV}$) and UV slope ($\beta_\rm{UV}$), as well as dust attenuation ($E(B-V)_\rm{neb}$) and redshift. Implementing a Bayesian linear regression methodology, we fit $\xi_\rm{ion}$ against the principal observables while fully marginalising over all measurement uncertainties, mitigating against the impact of outliers and determining the intrinsic scatter. Significant relations between $\xi_\rm{ion}$ and $ W_\lambda(\rm{H\alpha})$, $W_\lambda$([OIII]) and $\beta_\rm{UV}$ are recovered. Moreover, the weak trends with $M_\rm{UV}$ and redshift can be fully explained by the remaining property dependencies. Expanding our analysis to multivariate regression, we determine that $W_\lambda(\rm{H\alpha})$ or $W_\lambda$([OIII]), along with $\beta_\rm{UV}$ and $E(B-V)_\rm{neb}$, are the most important observables for accurately predicting $\xi_\rm{ion,0}$. The latter identifies the most common outliers as SFGs with relatively high $E(B-V)_\rm{neb}\gtrsim0.5$, possibly indicative of obscured star-formation or strong differential attenuation. Combining these properties enable $\xi_\rm{ion,0}$ to be inferred with an accuracy of $\sim0.15\,$dex, with a population intrinsic scatter of $\sigma_\rm{int}\sim0.035\,$dex. |
| 2025-09-30 | [Signal-Aware Workload Shifting Algorithms with Uncertainty-Quantified Predictors](http://arxiv.org/abs/2509.26511v1) | Ezra Johnson, Adam Lechowicz et al. | A wide range of sustainability and grid-integration strategies depend on workload shifting, which aligns the timing of energy consumption with external signals such as grid curtailment events, carbon intensity, or time-of-use electricity prices. The main challenge lies in the online nature of the problem: operators must make real-time decisions (e.g., whether to consume energy now) without knowledge of the future. While forecasts of signal values are typically available, prior work on learning-augmented online algorithms has relied almost exclusively on simple point forecasts. In parallel, the forecasting research has made significant progress in uncertainty quantification (UQ), which provides richer and more fine-grained predictive information. In this paper, we study how online workload shifting can leverage UQ predictors to improve decision-making. We introduce $\texttt{UQ-Advice}$, a learning-augmented algorithm that systematically integrates UQ forecasts through a $\textit{decision uncertainty score}$ that measures how forecast uncertainty affects optimal future decisions. By introducing $\textit{UQ-robustness}$, a new metric that characterizes how performance degrades with forecast uncertainty, we establish theoretical performance guarantees for $\texttt{UQ-Advice}$. Finally, using trace-driven experiments on carbon intensity and electricity price data, we demonstrate that $\texttt{UQ-Advice}$ consistently outperforms robust baselines and existing learning-augmented methods that ignore uncertainty. |
| 2025-09-30 | [Nondestructive characterization of laser-cooled atoms using machine learning](http://arxiv.org/abs/2509.26479v1) | G. De Sousa, M. Doris et al. | We develop machine learning techniques for estimating physical properties of laser-cooled potassium-39 atoms in a magneto-optical trap using only the scattered light -- i.e., fluorescence -- that is intrinsic to the cooling process. In-situ snap-shot images of fluorescing atomic ensembles directly reveal the spatial structure of these millimeter-scale objects but contain no obvious information regarding internal properties such as the temperature. We first assembled and labeled a balanced dataset sampling $8\times10^3$ different experimental parameters that includes examples with: large and dense atomic ensembles, a complete absence of atoms, and everything in between. We describe a range of models trained to predict atom number and temperature solely from fluorescence images. These run the gamut from a poorly performing linear regression model based only on integrated fluorescence to deep neural networks that give number and temperature with fractional uncertainties of $0.1$ and $0.2$ respectively. |
| 2025-09-30 | [Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification](http://arxiv.org/abs/2509.26457v1) | Artur Barros, Carlos Caetano et al. | Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Î² spectrum](http://arxiv.org/abs/2509.26390v1) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete \beta decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full \beta decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of \beta particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision \beta decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {\sigma}) enhancement in \beta emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for \beta decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [Precision measurement and modelling of the threshold-free 210Pb Î² spectrum](http://arxiv.org/abs/2509.26390v2) | Shuo Zhang, Hao-Ran Liu et al. | Beta decay is a fundamental process that governs nuclear stability and serves as a sensitive probe of the weak interaction and possible physics beyond the Standard Model of particle physics. However, precise measurements of complete $\beta$ decay spectra, particularly at low energies, remain experimentally and theoretically challenging. Here we report a high-precision, threshold-free measurement of the full $\beta$ decay spectrum of 210Pb to excited states of 210Bi, using a transition-edge sensor (TES)-based micro-calorimeter. This approach enables the detection of $\beta$ particle energies from 0 keV up to their endpoint by coincidence summing with subsequent de-excitation energy, thereby eliminating reconstruction artifacts near zero energy that have traditionally limited low-energy spectral accuracy. To our knowledge, this is the first complete, high-precision $\beta$ decay spectrum from 0 keV. The data resolve theoretical uncertainties associated with the atomic quantum exchange (AQE) effect. An accompanying ab initio theoretical framework, incorporating atomic, leptonic, and nuclear components, predicts a statistically significant (7.2 {$\sigma$}) enhancement in $\beta$ emission probability near zero energy, in agreement with the measurement and in contrast to models that omit AQE corrections. These results provide a new benchmark for $\beta$ decay theory at low energies, deepen our understanding of the weak interaction, and establish a critical foundation for searches for new physics, including dark matter interactions and precision studies of neutrinos. |
| 2025-09-30 | [An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition](http://arxiv.org/abs/2509.26385v1) | Zejin Gao, Ksheera Sagar et al. | We consider the problem of fully Bayesian posterior estimation and uncertainty quantification in undirected Gaussian graphical models via Markov chain Monte Carlo (MCMC) under recently-developed element-wise graphical priors, such as the graphical horseshoe. Unlike the conjugate Wishart family, these priors are non-conjugate; but have the advantage that they naturally allow one to encode a prior belief of sparsity in the off-diagonal elements of the precision matrix, without imposing a structure on the entire matrix. Unfortunately, for a graph with $p$ nodes and with $n$ samples, the state-of-the-art MCMC approaches for the element-wise priors achieve a per iteration complexity of $O(p^4),$ which is prohibitive when $p\gg n$. In this regime, we develop a suitably reparameterized MCMC with per iteration complexity of $O(p^3)$, providing a one-order of magnitude improvement, and consequently bringing the computational cost at par with the conjugate Wishart family, which is also $O(p^3)$ due to a use of the classical Bartlett decomposition, but this decomposition does not apply outside the Wishart family. Importantly, the proposed benefit is obtained solely due to our reparameterization in an MCMC scheme targeting the true posterior, that reverses the recently developed telescoping block decomposition of Bhadra et al. (2024), in a suitable sense. There is no variational or any other approximate Bayesian computation scheme considered in this paper that compromises targeting the true posterior. Simulations and the analysis of a breast cancer data set confirm both the correctness and better algorithmic scaling of the proposed reverse telescoping sampler. |
| 2025-09-30 | [Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography](http://arxiv.org/abs/2509.26382v1) | Shijie Lin, Bin Hu et al. | Time-delay cosmography, by monitoring the multiply imaged gravitational lenses in the time domain, offers a promising and independent method for measuring cosmological distances. However, in addition to the main deflector that produces the multiple images, the large-scale structure along the line-of-sight (LoS) will also deflect the traveling light rays, known as weak lensing (WL). Due to resolution limitations, accurately measuring WL on arcsecond scales is highly challenging. In this work, we evaluate the LoS effects on both lensing images and time-delay measurements using a more straightforward, high-resolution N-body simulation that provides a more realistic matter distribution compared to the traditional, computationally cheaper halo rendering method. We employ the multi-plane ray tracing technique, which is traditionally utilized to compute WL effects at the arcminute scale, extending its application to the strong lensing regime at the arcsecond scale. We focus on the quadruple-image system and present the following findings: 1. In addition to a constant external convergence, large-scale structures within a region approximately 2 arcminutes in angular size act as external perturbers, inducing inhomogeneous fluctuations on the arcsecond scale; 2. These fluctuations cannot be fully accounted for by external shear alone, necessitating the inclusion of external flexion; 3. While incorporating flexion provides a reasonably good fit to the lensing image, the time-delay distance still exhibits a $6.2$\textperthousand~bias and a $2.5\%$ uncertainty. This underscores the limitations of the single-plane approximation, as time-delay errors accumulate along the LoS. |
| 2025-09-29 | [Safe Planning in Unknown Environments using Conformalized Semantic Maps](http://arxiv.org/abs/2509.25124v1) | David Smith Sundarsingh, Yifei Li et al. | This paper addresses semantic planning problems in unknown environments under perceptual uncertainty. The environment contains multiple unknown semantically labeled regions or objects, and the robot must reach desired locations while maintaining class-dependent distances from them. We aim to compute robot paths that complete such semantic reach-avoid tasks with user-defined probability despite uncertain perception. Existing planning algorithms either ignore perceptual uncertainty - thus lacking correctness guarantees - or assume known sensor models and noise characteristics. In contrast, we present the first planner for semantic reach-avoid tasks that achieves user-specified mission completion rates without requiring any knowledge of sensor models or noise. This is enabled by quantifying uncertainty in semantic maps - constructed on-the-fly from perceptual measurements - using conformal prediction in a model- and distribution-free manner. We validate our approach and the theoretical mission completion rates through extensive experiments, showing that it consistently outperforms baselines in mission success rates. |
| 2025-09-29 | [New insights from the flavor dependence of quark transverse momentum distributions in the pion](http://arxiv.org/abs/2509.25098v1) | Lorenzo Rossi, Alessandro Bacchetta et al. | We update our previous extraction of transverse momentum distributions of unpolarized quarks in the pion by implementing a more comprehensive description of theoretical uncertainties and, for the first time, by exploring possible differences among quark flavors. We extract such distributions from all available data for unpolarized pion-nucleus Drell-Yan processes, where the cross section is differential in the transverse momentum of the final lepton pair. The cross section involves transverse momentum distributions in the nucleon, that we consistently take from our previous studies. |
| 2025-09-29 | [Curriculum Imitation Learning of Distributed Multi-Robot Policies](http://arxiv.org/abs/2509.25097v1) | JesÃºs Roche, Eduardo SebastiÃ¡n et al. | Learning control policies for multi-robot systems (MRS) remains a major challenge due to long-term coordination and the difficulty of obtaining realistic training data. In this work, we address both limitations within an imitation learning framework. First, we shift the typical role of Curriculum Learning in MRS, from scalability with the number of robots, to focus on improving long-term coordination. We propose a curriculum strategy that gradually increases the length of expert trajectories during training, stabilizing learning and enhancing the accuracy of long-term behaviors. Second, we introduce a method to approximate the egocentric perception of each robot using only third-person global state demonstrations. Our approach transforms idealized trajectories into locally available observations by filtering neighbors, converting reference frames, and simulating onboard sensor variability. Both contributions are integrated into a physics-informed technique to produce scalable, distributed policies from observations. We conduct experiments across two tasks with varying team sizes and noise levels. Results show that our curriculum improves long-term accuracy, while our perceptual estimation method yields policies that are robust to realistic uncertainty. Together, these strategies enable the learning of robust, distributed controllers from global demonstrations, even in the absence of expert actions or onboard measurements. |
| 2025-09-29 | [Finite-Size Security Bounds in Semi-Quantum Key Distribution: Spectral, Operator-Theoretic, and Entropic Perspectives](http://arxiv.org/abs/2509.25078v1) | Zahidur Rezwan Ratul | We study Semi-Quantum Key Distribution (SQKD) with a focus on finite-size security bounds, developed through three complementary perspectives. (i) Spectral disturbance: wrong-basis L\"uders updates produce closed-form spectra and purity loss, which serve as basis-independent indicators of disturbance. (ii) Operator-theoretic reduction: in Z/Z-sifted rounds, intercept-resend attacks can be represented as an effective depolarizing channel, characterized by a fidelity-QBER relation. (iii) Entropic trade-offs: Maassen-Uffink and memory-assisted uncertainty relations certify security through X tests and reflection rounds, even when the sifted QBER is low. The exposition provides step-by-step derivations supported by physically interpretable figures, and the framework concludes with finite-size estimates based on concentration inequalities that are suited for practical parameter estimation. |
| 2025-09-29 | [Confidence-Guided Error Correction for Disordered Speech Recognition](http://arxiv.org/abs/2509.25048v1) | Abner Hernandez, TomÃ¡s Arias Vergara et al. | We investigate the use of large language models (LLMs) as post-processing modules for automatic speech recognition (ASR), focusing on their ability to perform error correction for disordered speech. In particular, we propose confidence-informed prompting, where word-level uncertainty estimates are embedded directly into LLM training to improve robustness and generalization across speakers and datasets. This approach directs the model to uncertain ASR regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare our approach to both transcript-only fine-tuning and post hoc confidence-based filtering. Evaluations show that our method achieves a 10% relative WER reduction compared to naive LLM correction on the Speech Accessibility Project spontaneous speech and a 47% reduction on TORGO, demonstrating the effectiveness of confidence-aware fine-tuning for impaired speech. |
| 2025-09-29 | [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](http://arxiv.org/abs/2509.25034v1) | Heming Fu, Guojun Xiong et al. | As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management. |
| 2025-09-29 | [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](http://arxiv.org/abs/2509.25031v1) | Sophia V. Kuhn, Rafael Bischof et al. | Aging infrastructure portfolios pose a critical resource allocation challenge: deciding which structures require intervention and which can safely remain in service. Structural assessments must balance the trade-off between cheaper, conservative analysis methods and accurate but costly simulations that do not scale portfolio-wide. We propose Bayesian neural network (BNN) surrogates for rapid structural pre-assessment of worldwide common bridge types, such as reinforced concrete frame bridges. Trained on a large-scale database of non-linear finite element analyses generated via a parametric pipeline and developed based on the Swiss Federal Railway's bridge portfolio, the models accurately and efficiently estimate high-fidelity structural analysis results by predicting code compliance factors with calibrated epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware triage: flagging likely critical structures and providing guidance where refined analysis is pertinent. We demonstrate the framework's effectiveness in a real-world case study of a railway underpass, showing its potential to significantly reduce costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios. |
| 2025-09-29 | [The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools](http://arxiv.org/abs/2509.25028v1) | Eric Browne | Randomness plays a pivotal yet paradoxical role in computational music creativity: it can spark novelty, but unchecked chance risks incoherence. This paper presents a thematic review of contemporary AI music systems, examining how designers incorporate randomness and uncertainty into creative practice. I draw on the concept of structured uncertainty to analyse how stochastic processes are constrained within musical and interactive frameworks. Through a comparative analysis of six systems - Musika (Pasini and Schl\"uter, 2022), MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) - we identify recurring design patterns that support musical coherence, user control, and co-creativity. To my knowledge, this is the first thematic review examining randomness in AI music through structured uncertainty, offering practical insights for designers and artists aiming to support expressive, collaborative, or improvisational interactions. |
| 2025-09-29 | [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](http://arxiv.org/abs/2509.25017v1) | Spyros Kondylatos, Gustau Camps-Valls et al. | Wildfires are among the most severe natural hazards, posing a significant threat to both humans and natural ecosystems. The growing risk of wildfires increases the demand for forecasting models that are not only accurate but also reliable. Deep Learning (DL) has shown promise in predicting wildfire danger; however, its adoption is hindered by concerns over the reliability of its predictions, some of which stem from the lack of uncertainty quantification. To address this challenge, we present an uncertainty-aware DL framework that jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance short-term wildfire danger forecasting. In the next-day forecasting, our best-performing model improves the F1 Score by 2.3% and reduces the Expected Calibration Error by 2.1% compared to a deterministic baseline, enhancing both predictive skill and calibration. Our experiments confirm the reliability of the uncertainty estimates and illustrate their practical utility for decision support, including the identification of uncertainty thresholds for rejecting low-confidence predictions and the generation of well-calibrated wildfire danger maps with accompanying uncertainty layers. Extending the forecast horizon up to ten days, we observe that aleatoric uncertainty increases with time, showing greater variability in environmental conditions, while epistemic uncertainty remains stable. Finally, we show that although the two uncertainty types may be redundant in low-uncertainty cases, they provide complementary insights under more challenging conditions, underscoring the value of their joint modeling for robust wildfire danger prediction. In summary, our approach significantly improves the accuracy and reliability of wildfire danger forecasting, advancing the development of trustworthy wildfire DL systems. |
| 2025-09-29 | [Addressing Methodological Uncertainty in MCDM with a Systematic Pipeline Approach to Data Transformation Sensitivity Analysis](http://arxiv.org/abs/2509.24996v1) | Juan B. Cabral, Alvaro Roy Schachner | Multicriteria decision-making methods exhibit critical dependence on the choice of normalization techniques, where different selections can alter 20-40% of the final rankings. Current practice is characterized by the ad-hoc selection of methods without systematic robustness evaluation. We present a framework that addresses this methodological uncertainty through automated exploration of the scaling transformation space. The implementation leverages the existing Scikit-Criteria infrastructure to automatically generate all possible methodological combinations and provide robust comparative analysis. |
| 2025-09-26 | [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](http://arxiv.org/abs/2509.22633v1) | Gen Li, Yuling Yan | Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters. |
| 2025-09-26 | [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](http://arxiv.org/abs/2509.22612v1) | Jonne SÃ¤levÃ¤, Duygu Ataman et al. | In this paper, we introduce a set of resampling-based methods for quantifying uncertainty and statistical precision of evaluation metrics in multilingual and/or multitask NLP benchmarks. We show how experimental variation in performance scores arises from both model- and data-related sources, and that accounting for both of them is necessary to avoid substantially underestimating the overall variability over hypothetical replications. Using multilingual question answering, machine translation, and named entity recognition as example tasks, we also demonstrate how resampling methods are useful for computing sampling distributions for various quantities used in leaderboards such as the average/median, pairwise differences between models, and rankings. |
| 2025-09-26 | [Likelihood-free inference for gravitational-wave data analysis and public alerts](http://arxiv.org/abs/2509.22561v1) | Ethan Marx, Deep Chatterjee et al. | Rapid and reliable detection and dissemination of source parameter estimation data products from gravitational-wave events, especially sky localization, is critical for maximizing the potential of multi-messenger astronomy. Machine learning based detection and parameter estimation algorithms are emerging as production ready alternatives to traditional approaches. Here, we report validation studies of AMPLFI, a likelihood-free inference solution to low-latency parameter estimation of binary black holes. We use simulated signals added into data from the LIGO-Virgo-KAGRA's (LVK's) third observing run (O3) to compare sky localization performance with BAYESTAR, the algorithm currently in production for rapid sky localization of candidates from matched-filter pipelines. We demonstrate sky localization performance, measured by searched area and volume, to be equivalent with BAYESTAR. We show accurate reconstruction of source parameters with uncertainties for use distributing low-latency coarse-grained chirp mass information. In addition, we analyze several candidate events reported by the LVK in the third gravitational-wave transient catalog (GWTC-3) and show consistency with the LVK's analysis. Altogether, we demonstrate AMPLFI's ability to produce data products for low-latency public alerts. |
| 2025-09-26 | [Event Generator Tuning as a Robustness Test](http://arxiv.org/abs/2509.22526v1) | Jean Wolfs, Chris M. Marshall | Neutrino oscillation experiments use Monte Carlo event generators to predict neutrino-nucleus interactions. Cross section uncertainties are typically implemented by varying the parameters of the model(s) used in the generator. We study the performance of two commonly-used model configurations of the GENIE generator (G18_10a_02_11a and AR23_0i_00_000) and their uncertainties by tuning parameters to cross section data, and then comparing the resulting tuned prediction to a suite of other measurements from T2K, MicroBooNE, and MINERvA. This reveals whether the model can simultaneously describe several datasets, as well as whether the uncertainties are adequately robust. We find that G18 and especially AR23 are reasonable in predicting lower-energy measurements from T2K and MicroBooNE, but unable to describe MINERvA data, and discuss the implications for short-baseline oscillation searches. We attempt to replicate a tuning procedure developed by MicroBooNE using several different measurements, and find substantially different results depending on which measurement is used, and that the MicroBooNE tune does not agree with other measurements. We conclude that the SBN experiment should not tune its generator to external data. |
| 2025-09-26 | [A high-stability optical clock based on a continuously ground-state cooled Al$^+$ ion without compromising its accuracy](http://arxiv.org/abs/2509.22525v1) | Fabian Dawel, Lennart Pelzer et al. | Single ion optical clocks have shown systematic frequency uncertainties below $10^{-18}$, but typically require more than one week of averaging to achieve a corresponding statistical uncertainty. This time can be reduced with longer probe times, but comes at the cost of a higher time-dilation shift due to motional heating of the ions in the trap. We show that sympathetic ground-state cooling using electromagnetically-induced transparency (EIT) of an \Al clock ion via a co-trapped \Ca ion during clock interrogation suppresses the heating of the ions. \Al can be kept close to the motional ground state, independent from the chosen interrogation time, at a relative time dilation shift of $(-1.69\pm0.20)\times10^{-18}$. The \Ca cooling light introduces an additional light shift on the \Al clock transition of $(-9.27\pm 1.03)\times10^{-18}$. We project that the uncertainty of this light shift can be further reduced by nearly an order of magnitude. This sympathetic cooling enables seconds of interrogation time with $10^{-19}$ motional and cooling laser-induced uncertainties for \Al and can be employed in other ion clocks as well. |
| 2025-09-26 | [Metric response of relative entropy: a universal indicator of quantum criticality](http://arxiv.org/abs/2509.22515v1) | Pritam Sarkar, Diptiman Sen et al. | The information-geometric origin of fidelity susceptibility and its utility as a universal probe of quantum criticality in many-body settings have been widely discussed. Here we explore the metric response of quantum relative entropy (QRE), by tracing out all but $n$ adjacent sites from the ground state of spin chains of finite length $N$, as a parameter of the corresponding Hamiltonian is varied. The diagonal component of this metric defines a susceptibility of the QRE that diverges at quantum critical points (QCPs) in the thermodynamic limit. We study two spin-$1/2$ models as examples, namely the integrable transverse field Ising model (TFIM) and a non-integrable Ising chain with three-spin interactions. We demonstrate distinct scaling behaviors for the peak of the QRE susceptibility as a function of $N$: namely a square logarithmic divergence in TFIM and a power-law divergence in the non-integrable chain. This susceptibility encodes uncertainty of entanglement Hamiltonian gradients and is also directly connected to other information measures such as Petz-R\'enyi entropies. We further show that this susceptibility diverges even at finite $N$ if the subsystem size, $n$, exceeds a certain value when the Hamiltonian is tuned to its classical limits due to the rank of the RDMs being finite; unlike the divergence associated with the QCPs which require $N \rightarrow \infty$. |
| 2025-09-26 | [A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random](http://arxiv.org/abs/2509.22499v1) | Yunshu Zhang, Chan Park et al. | Instrumental variable (IV) methods offer a valuable approach to account for outcome data missing not-at-random. A valid missing data instrument is a measured factor which (i) predicts the nonresponse process and (ii) is independent of the outcome in the underlying population. For point identification, all existing IV methods for missing data including the celebrated Heckman selection model, a priori restrict the extent of selection bias on the outcome scale, therefore potentially understating uncertainty due to missing data. In this work, we introduce an IV framework which allows the degree of selection bias on the outcome scale to remain completely unrestricted. The new approach instead relies for identification on (iii) a key multiplicative selection model, which posits that the instrument and any hidden common correlate of selection and the outcome, do not interact on the multiplicative scale. Interestingly, we establish that any regular statistical functional of the missing outcome is nonparametrically identified under (i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald ratio estimand in causal inference. For estimation and inference, we characterize the influence function for any functional defined on a nonparametric model for the observed data, which we leverage to develop semiparametric multiply robust IV estimators. Several extensions of the methods are also considered, including the important practical setting of polytomous and continuous instruments. Simulation studies illustrate the favorable finite sample performance of proposed methods, which we further showcase in an HIV study nested within a household health survey study we conducted in Mochudi, Botswana, in which interviewer characteristics are used as instruments to correct for selection bias due to dependent nonresponse in the HIV component of the survey study. |
| 2025-09-26 | [On an optimization framework for damage localization in structures](http://arxiv.org/abs/2509.22492v1) | Owais Saleem, Tim Suchan et al. | Efficient structural damage localization remains a challenge in structural health monitoring (SHM), particularly when the problem is coupled with uncertainty of conditions and complexity of structures. Traditional methods simply based on experimental data processing are often not sufficiently reliable, while complex models often struggle with computational inefficiency given the tremendous amount of model parameters. This paper focuses on closing the gap between data-driven SHM and physics-based model updating by offering a solution for real-world infrastructure. We first concentrate on fusing multi-source damage-sensitive features (DSF) based on experimental modal data into spatially mapped belief masses to pre-screen candidate damage locations. The resulting candidate damage locations are integrated into an inverse Finite Element method (FEM) model calibration process. We propose an optimization framework to identify the most probable damage scenario with single and multi-damage cases. We present the corresponding numerical results in this paper, which open the door to extend the application of the framework to a complex real bridge structure. |
| 2025-09-26 | [BÃ©zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](http://arxiv.org/abs/2509.22476v1) | Chen Li, Meilong Xu et al. | Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, B\'ezier Meets Diffusion, for cross-domain image generation. First, we introduce a B\'ezier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance. |
| 2025-09-26 | [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](http://arxiv.org/abs/2509.22469v1) | Ben Rossano, Jaein Lim et al. | This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation. |
| 2025-09-25 | [Efficient Digital Methods to Quantify Sensor Output Uncertainty](http://arxiv.org/abs/2509.21311v1) | Orestis Kaparounakis, Phillip Stanley-Marbell | Accurate characterization of sensor output uncertainty is important for reliable data interpretation in many applications. Here, we investigate the impact of transducer-level measurement uncertainty on overall sensor measurement accuracy due to limited-precision information about sensor components. We explain our method using thermopile-based sensors as an example class of sensors. We show how sensor calibration and conversion equations, which are an essential part of all sensing systems, propagate uncertainties resulting from the quantization of calibration parameters, to the final, compensated sensor output. The experimental results show that the epistemic uncertainty of calibration-related quantities leads to absolute error in the sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for one commonly-used thermopile sensor. In one instance of using the epistemic uncertainty information in edge detection, we show reduction of false-positives edges to zero for the conventional Canny operator, while maintaining accuracy. We show these ideas are practical and possible on actual embedded sensor systems by prototyping them on two commercially-available uncertainty tracking hardware platforms, one with average power dissipation 16.7 mW and 42.9x speedup compared to the equal-confidence Monte Carlo computation (the status quo), and the other with average power dissipation 147.15 mW and 94.4x speedup, paving the way for use in real time. |
| 2025-09-25 | [Emission line tracers of galactic outflows driven by stellar feedback in simulations of isolated disk galaxies](http://arxiv.org/abs/2509.21295v1) | Elliot L. Howatson, Alexander J. Richings et al. | Hydrodynamic simulations can connect outflow observables to the physical conditions of outflowing gas. Here, we use simulations of isolated disk galaxies ranging from dwarf mass ($M_{200} = 10^{10}\mathrm{M}_{\odot}$) to Milky Way mass ($M_{200} = 10^{12}\mathrm{M}_{\odot}$), based on the FIRE-2 subgrid models to investigate multiphase galactic outflows. We use the CHIMES non-equilibrium chemistry module to create synthetic spectra of common outflow tracers ([CII]$_{158\rm{\mu m}}$, $\mathrm{CO}_{J(1-0)}$, H$\alpha$ and $[\mathrm{OIII}]_{5007\text{A}}$). Using our synthetic spectra we measure the mass outflow rate, kinetic power and momentum flux using observational techniques. In [CII]$_{158\rm{\mu m}}$ we measure outflow rates of $10^{-4}$ to $1$ $\mathrm{M_{\odot}yr^{-1}}$ across an SFR range of $10^{-3}$ to $1$ $\text{M}_{\odot}\text{yr}^{-1}$, which is in reasonable agreement with observations. The significant discrepancy is in $\mathrm{CO}_{J(1-0)}$, with the simulations lying $\approx1$ dex below the observational sample. We test observational assumptions used to derive outflow properties from synthetic spectra. We find the greatest uncertainty lies in measurements of electron density, as estimates using the SII doublet can overestimate the actual electron density by up to 2 dex, which changes mass outflow rates by up to 4 dex. We also find that molecular outflows are especially sensitive to the conversion factor between CO luminosity and H2 mass, with outflow rates changing by up to 4 dex in our least massive galaxy. Comparing the outflow properties derived from the synthetic spectra to those derived directly from the simulation, we find that [CII]$_{158\rm{\mu m}}$ probes outflows at greater distances from the disk, whilst we find that molecular gas does not survive at large distances within outflows within our modestly star-forming disk galaxies simulated in this work. |
| 2025-09-25 | [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](http://arxiv.org/abs/2509.21210v1) | Ali Kafili Gavgani, Amin Talaeizadeh et al. | Conventional multi-rotors are under-actuated systems, hindering them from independently controlling attitude from position. In this study, we present several distinct configurations that incorporate additional control inputs for manipulating the angles of the propeller axes. This addresses the mentioned limitations, making the systems "omniorientational". We comprehensively derived detailed dynamic models for all introduced configurations and validated by a methodology using Simscape Multibody simulations. Two controllers are designed: a sliding mode controller for robust handling of disturbances and a novel PID-based controller with gravity compensation integrating linear and non-linear allocators, designed for computational efficiency. A custom control allocation strategy is implemented to manage the input-non-affine nature of these systems, seeking to maximize battery life by minimizing the "Power Consumption Factor" defined in this study. Moreover, the controllers effectively managed harsh disturbances and uncertainties. Simulations compare and analyze the proposed configurations and controllers, majorly considering their power consumption. Furthermore, we conduct a qualitative comparison to evaluate the impact of different types of uncertainties on the control system, highlighting areas for potential model or hardware improvements. The analysis in this study provides a roadmap for future researchers to design omniorientational drones based on their design objectives, offering practical insights into configuration selection and controller design. This research aligns with the project SAC-1, one of the objectives of Sharif AgRoLab. |
| 2025-09-25 | [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](http://arxiv.org/abs/2509.21201v1) | Yang Fu, Peng Qin et al. | The vision of 6G networks aims to enable edge inference by leveraging ubiquitously deployed artificial intelligence (AI) models, facilitating intelligent environmental perception for a wide range of applications. A critical operation in edge inference is for an edge node (EN) to aggregate multi-view sensory features extracted by distributed agents, thereby boosting perception accuracy. Over-the-air computing (AirComp) emerges as a promising technique for rapid feature aggregation by exploiting the waveform superposition property of analog-modulated signals, which is, however, incompatible with existing digital communication systems. Meanwhile, hybrid reconfigurable intelligent surface (RIS), a novel RIS architecture capable of simultaneous signal amplification and reflection, exhibits potential for enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital AirComp (HRD-AirComp) scheme, which employs vector quantization to map high-dimensional features into discrete codewords that are digitally modulated into symbols for wireless transmission. By judiciously adjusting the AirComp transceivers and hybrid RIS reflection to control signal superposition across agents, the EN can estimate the aggregated features from the received signals. To endow HRD-AirComp with a task-oriented design principle, we derive a surrogate function for inference accuracy that characterizes the impact of feature quantization and over-the-air aggregation. Based on this surrogate, we formulate an optimization problem targeting inference accuracy maximization, and develop an efficient algorithm to jointly optimize the quantization bit allocation, agent transmission coefficients, EN receiving beamforming, and hybrid RIS reflection beamforming. Experimental results demonstrate that the proposed HRD-AirComp outperforms baselines in terms of both inference accuracy and uncertainty. |
| 2025-09-25 | [The depletion of the asteroid belt and the impact history of the Earth](http://arxiv.org/abs/2509.21194v1) | Julio A. Fernandez | We have evaluated the rate at which the asteroid belt is losing material, and how it splits between macroscopic bodies and meteoritic dust. The mass loss process is due to the injection of asteroid fragments into unstable dynamical regions, associated to mean-motion resonances with Jupiter, Saturn and Mars or secular resonances, from where they are scattered either to the region of the terrestrial planets or to the vicinity of Jupiter's orbit. Asteroid fragments that do not escape from the belt are ground down by mutual collisions to meteoritic dust. Under the assumption that 25\% of the zodiacal dust mass is of asteroidal origin, we find that the asteroid belt is currently losing a fraction of about $\mu_o \simeq 8.8 \times 10^{-5}$ Ma$^{-1}$ of its collisionally-active mass (without the primordial objects Ceres, Vesta and Pallas), about 20\% as macroscopic bodies, and 80\% as dust particles that feed the zodiacal dust cloud. Extrapolation of the current mass loss rate to the past suggests only a moderate increase of the asteroid belt mass and the mass loss rate around 3.0 - 3.5 Ga ago (by about 50\% and a factor of two respectively). Yet, should the computed $\mu_o$ be somewhat underestimated owing to the different uncertainties associated to its computation, the extrapolation to the past would lead to quite different results. For instance, a moderate increase in the computed $\mu_o$, say by a factor of three, would lead to an exponential increase of the asteroid mass and mass loss rate about 3.5 Ga ago. A greater asteroid mass loss rate in the past should be correlated with a more intense impact rate of the Earth, Moon and the other terrestrial planets, which is indeed what suggests the geologic record (Hartmann 2007). |
| 2025-09-25 | [$Î›_{c}(2910)$ and $Î›_{c}(2940)$ productions in $p \bar{p}$ annihilation and $K^{-}p$ scattering processes](http://arxiv.org/abs/2509.21176v1) | Quan-Yun Guo, Dian-Yong Chen | In this work, we investigate the productions of $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ in the $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes by utilizing an effective Lagrangian approach, where both $\Lambda_{c}(2910)$ and $\Lambda_{c}(2940)$ are considered as $D^{\ast}N$ molecular states with $J^{P}=1/2^{-}$ and $3/2^{-}$, respectively. The cross sections and $D^0 p$ invariant mass distributions at exemplified center-of-mass energy for the involved processes are estimated. At $\sqrt{s}=10$ $\mathrm{GeV}$, our estimations indicate that the cross sections for $p \bar{p} \rightarrow \bar{\Lambda}_{c} D^{0} p$, $K^{-}p \rightarrow D^{-}_{s} D^{0} p$, and $K^{-}p \rightarrow D^{\ast-}_{s} D^{0} p$ processes are $(2.1^{+5.6}_{-1.6})$ nb, $(2.2^{+5.9}_{-1.7})$ nb, and $(0.6^{+1.6}_{-0.4})$ nb, respectively, where the uncertainties are resulted from the variation of model parameter. Furthermore, Our estimations of the $D^{0}p$ invariant mass spectra reveal that the peak structure around 2.9 GeV primarily originates from $\Lambda_c(2910)$ across all three processes, however the contributions from $\Lambda_c(2940)$ makes the structure asymmetric. It is expected that the total cross sections and the $D^0 p$ invariant mass distributions estimated in the present work can be tested by future experiments at $\mathrm{\bar{P}ANDA}$ and J-PARC. |
| 2025-09-25 | [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](http://arxiv.org/abs/2509.21161v1) | Giuseppe Serra, Florian Buettner | Continual Learning (CL) is recently gaining increasing attention for its ability to enable a single model to learn incrementally from a sequence of new classes. In this scenario, it is important to keep consistent predictive performance across all the classes and prevent the so-called Catastrophic Forgetting (CF). However, in safety-critical applications, predictive performance alone is insufficient. Predictive models should also be able to reliably communicate their uncertainty in a calibrated manner - that is, with confidence scores aligned to the true frequencies of target events. Existing approaches in CL address calibration primarily from a data-centric perspective, relying on a single temperature shared across all tasks. Such solutions overlook task-specific differences, leading to large fluctuations in calibration error across tasks. For this reason, we argue that a more principled approach should adapt the temperature according to the distance to the current task. However, the unavailability of the task information at test time/during deployment poses a major challenge to achieve the intended objective. For this, we propose Distance-Aware Temperature Scaling (DATS), which combines prototype-based distance estimation with distance-aware calibration to infer task proximity and assign adaptive temperatures without prior task information. Through extensive empirical evaluation on both standard benchmarks and real-world, imbalanced datasets taken from the biomedical domain, our approach demonstrates to be stable, reliable and consistent in reducing calibration error across tasks compared to state-of-the-art approaches. |
| 2025-09-25 | [Normalizing Flows are Capable Visuomotor Policy Learning Models](http://arxiv.org/abs/2509.21073v1) | Simon Kristoffersson Lind, Jialong Li et al. | The field of general purpose robotics has recently embraced powerful probabilistic models, such as diffusion models, to model and learn complex behaviors. However, these models often come with significant trade-offs, namely high computational costs for inference and a fundamental inability to quantify output uncertainty. We argue that a model's trustworthiness, a critical factor for reliable, general-purpose robotics, is inherently linked to its ability to provide confidence measures.   In this work, we introduce Normalizing Flows Policy, a novel visuomotor policy learning model based on Normalizing Flows. We show that Normalizing Flows are a natural and powerful alternative to diffusion models, providing both a statistically sound measure of confidence and a highly efficient inference process. Through comprehensive experiments across four distinct simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves performance comparable to, and often surpassing, Diffusion Policy, and it does so not only with improved sample efficiency but also with up to 30 times faster inference. Additionally, our ablation study validates several key architectural and training techniques that enable Normalizing Flows to perform well in this domain. |
| 2025-09-25 | [A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data](http://arxiv.org/abs/2509.21041v1) | Maeve Upton, Eamonn Organ et al. | Accurate estimation of solar irradiance is essential for reliable modelling of solar photovoltaic (PV) power production. In Ireland's highly variable maritime climate, where ground-based measurement stations are sparsely distributed, selecting an appropriate solar irradiance dataset presents a significant challenge. This study introduces a novel Bayesian spatio-temporal modelling framework for predicting solar irradiance at hourly and sub-hourly (10-minute) resolutions across Ireland. Cross-validation demonstrates that our model is statistically robust across all temporal resolutions with hourly showing highest prediction precision whereas 10-minute resolution encounters higher errors but better uncertainty quantification. In separate evaluations, we compare our model against alternative data sources, including reanalysis datasets and nearest-station interpolation, and find that it consistently provides superior site-specific accuracy. At the hourly scale, our model outperforms ERA5 in agreement with ground-based observations. At the sub-hourly scale, 10-minute resolution estimates provide solar PV power outputs consistent with residential and industrial solar PV installations in Ireland. Beyond surpassing existing datasets, our model delivers full uncertainty quantification, scalability and the capacity for real-time implementation, offering a powerful tool for solar energy prediction and the estimation of losses due to overload clipping from inverter undersizing. |
| 2025-09-25 | [Study on Locomotive Epidemic Dynamics in a Stochastic Spatio-Temporal Simulation Model on a Multiplex Network](http://arxiv.org/abs/2509.21017v1) | H. M. Shadman Tabib, Jaber Ahmed Deedar et al. | This study presents an integrated approach to understanding epidemic dynamics through a stochastic spatio-temporal simulation model on a multiplex network, blending physical and informational layers. The physical layer maps the geographic movement of individuals, while the information layer tracks the spread of knowledge and health behavior via social interactions. We explore the interplay between physical mobility, information flow, and epidemic outcomes by simulating disease spread within this dual-structured network. Our model employs stochastic elements to mirror human behavior, mobility, and information dissemination uncertainties. Through simulations, we assess the impact of network structure, mobility patterns, and information spread speed on epidemic dynamics. The findings highlight the crucial role of effective communication in curbing disease transmission, even in highly mobile societies. Additionally, our agent-based simulation allows for real-time scenario analysis through a user interface, offering insights into leveraging physical and informational networks for epidemic control. This research sheds light on designing strategic interventions in complex social systems to manage disease outbreaks. |
| 2025-09-24 | [Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing](http://arxiv.org/abs/2509.20349v1) | Ramona Rubini, Siavash Khodakarami et al. | Accurate time-series forecasting for complex physical systems is the backbone of modern industrial monitoring and control. While deep learning models excel at capturing complex dynamics, currently, their deployment is limited due to physical inconsistency and robustness, hence constraining their reliability in regulated environments. We introduce process-informed forecasting (PIF) models for temperature in pharmaceutical lyophilization. We investigate a wide range of models, from classical ones such as Autoregressive Integrated Moving Average Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning architectures, including Kolmogorov-Arnold Networks (KANs). We compare three different loss function formulations that integrate a process-informed trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a Residual-Based Attention (RBA) mechanism. We evaluate all models not only for accuracy and physical consistency but also for robustness to sensor noise. Furthermore, we test the practical generalizability of the best model in a transfer learning scenario on a new process. Our results show that PIF models outperform their data-driven counterparts in terms of accuracy, physical plausibility and noise resilience. This work provides a roadmap for developing reliable and generalizable forecasting solutions for critical applications in the pharmaceutical manufacturing landscape. |
| 2025-09-24 | [Quantum speed limits based on Jensen-Shannon and Jeffreys divergences for general physical processes](http://arxiv.org/abs/2509.20347v1) | Jucelino Ferreira de Sousa, Diego Paiva Pires | We discuss quantum speed limits (QSLs) for finite-dimensional quantum systems undergoing a general physical process. These QSLs were obtained using two families of entropic measures, namely the square root of the Jensen-Shannon divergence, which in turn defines a faithful distance of quantum states, and the square root of the quantum Jeffreys divergence. The results apply to both closed and open quantum systems, and are evaluated in terms of the Schatten speed of the evolved state, as well as cost functions that depend on the smallest and largest eigenvalues of both initial and instantaneous states of the quantum system. To illustrate our findings, we focus on the unitary and nonunitary dynamics of mixed single-qubit states. In the first case, we obtain speed limits $\textit{\`{a} la}$ Mandelstam-Tamm that are inversely proportional to the variance of the Hamiltonian driving the evolution. In the second case, we set the nonunitary dynamics to be described by the noisy operations: depolarizing channel, phase damping channel, and generalized amplitude damping channel. We provide analytical results for the two entropic measures, present numerical simulations to support our results on the speed limits, comment on the tightness of the bounds, and provide a comparison with previous QSLs. Our results may find applications in the study of quantum thermodynamics, entropic uncertainty relations, and also complexity of many-body systems. |
| 2025-09-24 | [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](http://arxiv.org/abs/2509.20293v1) | Benjamin Feuer, Chiung-Yi Tseng et al. | LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md |
| 2025-09-24 | [4D Driving Scene Generation With Stereo Forcing](http://arxiv.org/abs/2509.20251v1) | Hao Lu, Zhuang Ma et al. | Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}. |
| 2025-09-24 | [$S_8$ from Tully-Fisher, fundamental plane, and supernova distances agree with Planck](http://arxiv.org/abs/2509.20235v1) | Richard Stiskalek | Peculiar velocity measurements constrain the parameter combination $f\sigma_8$, the product of the linear growth rate $f$ and the fluctuation amplitude $\sigma_8$. Under the approximation that $f$ is a monotonic function of $\Omega_{\rm m}$, this can be related to $S_8 \equiv \sigma_8 \sqrt{\Omega_{\rm m}/0.3}$, enabling direct comparison with weak lensing and cosmic microwave background results. We exploit this by using three classes of direct-distance tracers -- the Tully-Fisher relation, the fundamental plane, and Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical forward model jointly calibrates each distance indicator and a linear theory reconstruction of the local Universe. This is the first consistent Bayesian analysis to combine all three major classes of distance indicators within a common framework, enabling cross-checks of systematics across diverse galaxy populations. All three tracers yield consistent values of $S_8$ that are also in agreement with Planck. Our joint constraint is $S_8 = 0.819 \pm 0.030$, with the uncertainty dominated by the 2M++ galaxy field. These results demonstrate that peculiar velocity surveys provide a robust, consistent measurement of $S_8$, and support concordance with the cosmic microwave background. |
| 2025-09-24 | [InconVAD: A Two-Stage Dual-Tower Framework for Multimodal Emotion Inconsistency Detection](http://arxiv.org/abs/2509.20140v1) | Zongyi Li, Junchuan Zhao et al. | Detecting emotional inconsistency across modalities is a key challenge in affective computing, as speech and text often convey conflicting cues. Existing approaches generally rely on incomplete emotion representations and employ unconditional fusion, which weakens performance when modalities are inconsistent. Moreover, little prior work explicitly addresses inconsistency detection itself. We propose InconVAD, a two-stage framework grounded in the Valence/Arousal/Dominance (VAD) space. In the first stage, independent uncertainty-aware models yield robust unimodal predictions. In the second stage, a classifier identifies cross-modal inconsistency and selectively integrates consistent signals. Extensive experiments show that InconVAD surpasses existing methods in both multimodal emotion inconsistency detection and modeling, offering a more reliable and interpretable solution for emotion analysis. |
| 2025-09-24 | [Causal Understanding by LLMs: The Role of Uncertainty](http://arxiv.org/abs/2509.20088v1) | Oscar Lithgow-Serrano, Vani Kanjirangat et al. | Recent papers show LLMs achieve near-random accuracy in causal relation classification, raising questions about whether such failures arise from limited pretraining exposure or deeper representational gaps. We investigate this under uncertainty-based evaluation, testing whether pretraining exposure to causal examples improves causal understanding >18K PubMed sentences -- half from The Pile corpus, half post-2024 -- across seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model behavior through: (i) causal classification, where the model identifies causal relationships in text, and (ii) verbatim memorization probing, where we assess whether the model prefers previously seen causal statements over their paraphrases. Models perform four-way classification (direct/conditional/correlational/no-relationship) and select between originals and their generated paraphrases. Results show almost identical accuracy on seen/unseen sentences (p > 0.05), no memorization bias (24.8% original selection), and output distribution over the possible options is almost flat, with entropic values near the maximum (1.35/1.39), confirming random guessing. Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence, 32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11% vs. direct). These findings suggest that failures in causal understanding arise from the lack of structured causal representation, rather than insufficient exposure to causal examples during pretraining. |
| 2025-09-24 | [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](http://arxiv.org/abs/2509.20077v1) | Xun Li, Rodrigo Santa Cruz et al. | To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution. We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments. |
| 2025-09-24 | [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](http://arxiv.org/abs/2509.20065v1) | Maggie Mi, Aline Villavicencio et al. | Language models often struggle with idiomatic, figurative, or context-sensitive inputs, not because they produce flawed outputs, but because they misinterpret the input from the outset. We propose an input-only method for anticipating such failures using token-level likelihood features inspired by surprisal and the Uniform Information Density hypothesis. These features capture localized uncertainty in input comprehension and outperform standard baselines across five linguistically challenging datasets. We show that span-localized features improve error detection for larger models, while smaller models benefit from global patterns. Our method requires no access to outputs or hidden activations, offering a lightweight and generalizable approach to pre-generation error prediction. |
| 2025-09-24 | [Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors](http://arxiv.org/abs/2509.20059v1) | Koki Kanzaki, Koya Sato | This paper proposes a high-accuracy radio map construction method tailored for environments where location information is affected by bursty errors. Radio maps are an effective tool for visualizing wireless environments. Although extensive research has been conducted on accurate radio map construction, most existing approaches assume noise-free location information during sensing. In practice, however, positioning errors ranging from a few to several tens of meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring such errors during inference can lead to significant degradation in radio map accuracy. This study highlights that these errors often tend to be biased when using mobile devices as sensors. We introduce a novel framework that models these errors together with spatial correlation in radio propagation by embedding them as tunable parameters in the marginal log-likelihood function. This enables ex-post calibration of location uncertainty during radio map construction. Numerical results based on practical human mobility data demonstrate that the proposed method can limit RMSE degradation to approximately 0.25-0.29 dB, compared with Gaussian process regression using noise-free location data, whereas baseline methods suffer performance losses exceeding 1 dB. |
| 2025-09-23 | [Recalibration of the H$Î±$ surface brightness-radius relation for planetary nebulae using Gaia DR3: new distances and the Milky Way oxygen radial gradient](http://arxiv.org/abs/2509.19239v1) | Oscar Cavichia, Hektor Monteiro et al. | The spatial distribution of chemical elements in the Galactic disk provides key constraints on models of galaxy evolution. However, studies using planetary nebulae (PNe) as tracers have been historically limited by large uncertainties in their distances. To overcome the long-standing distance uncertainties, we recalibrated the H$\alpha$ surface brightness-radius relation (Frew et al. 2016) with Gaia DR3 parallaxes, deriving statistical distances for 1,200 PNe and Bayesian distances for 419 objects with reliable parallaxes. Adopting Bayesian values preferentially, we determined the O/H radial gradient for 230 disk PNe. We tested three models: a single linear gradient, a segmented fit with one break, and a segmented fit with two breaks. Although model selection is statistically inconclusive, segmented fits indicate a change in slope near the solar radius ($R \sim 8$ kpc), with a flatter or slightly positive gradient inward and a steeper negative gradient outward. This feature may reflect changes in star formation efficiency driven by the Galactic bar or the corotation resonance of the spiral arms. Comparison with other tracers - Cepheids, red giants, and open clusters - shows qualitative consistency. The two-dimensional O/H distribution in the Galactic plane supports the adopted distances and reveals modest azimuthal asymmetry, with enhanced abundances near the bar at positive longitudes, and a bimodal abundance structure between the inner and outer solar regions. Our results provide new constraints on the chemical evolution of the Milky Way, the impact of non-axisymmetric structures, and the possible existence of distinct radial abundance regimes across the Galactic disk. |
| 2025-09-23 | [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](http://arxiv.org/abs/2509.19233v1) | Milad Leyli-abadi, Antoine Marot et al. | In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page. |
| 2025-09-23 | [A decentralized future for the open-science databases](http://arxiv.org/abs/2509.19206v1) | Gaurav Sharma, Viorel Munteanu et al. | Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come. |
| 2025-09-23 | [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](http://arxiv.org/abs/2509.19185v1) | Mohammed Mehedi Hasan, Hao Li et al. | Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents. |
| 2025-09-23 | [Bayesian Neural Networks versus deep ensembles for uncertainty quantification in machine learning interatomic potentials](http://arxiv.org/abs/2509.19180v1) | Riccardo Farris, Emanuele Telari et al. | Neural-network-based machine learning interatomic potentials have emerged as powerful tools for predicting atomic energies and forces, enabling accurate and efficient simulations in atomistic modeling. A key limitation of traditional deep learning approaches, however, is their inability to provide reliable estimates of predictive uncertainty. Such uncertainty quantification is critical for assessing model reliability, especially in materials science, where often the model is applied on out-of-distribution data. Different strategies have been proposed to address this challenge, with deep ensembles and Bayesian neural networks being among the most widely used. In this work, we introduce an implementation of Bayesian neural networks with variational inference in the aenet-PyTorch framework. To evaluate their applicability to machine learning interatomic potentials, we systematically compare the performance of variational BNNs and deep ensembles on a dataset of 7,815 TiO$_{2}$ structures. The models are trained on both the full dataset and a subset to assess how variations in data representation influence predictive accuracy and uncertainty estimation. This analysis provides insights into the strengths and limitations of each approach, offering practical guidance for the development of uncertainty-aware machine learning interatomic potentials. |
| 2025-09-23 | [Intrinsic Heisenberg Lower Bounds on Schwarzschild and Weyl-Class Spacelike Slices](http://arxiv.org/abs/2509.19099v1) | Thomas SchÃ¼rmann | We establish a coordinate-invariant Heisenberg-type lower bound for quantum states strictly localized in geodesic balls of radius $r_g$ on horizon-regular spacelike slices of static, spherically symmetric, asymptotically flat (AF) black-holes. Via a variance-eigenvalue equivalence the momentum uncertainty reduces to the first Dirichlet eigenvalue of the Laplace-Beltrami operator, yielding a slice-uniform Hardy baseline $\sigma_p r_g \ge \hbar/2$ under mild convexity assumptions on the balls; the bound is never attained and admits a positive gap both on compact interior regions and uniformly far out. For the Schwarzschild Painlev\'e-Gullstrand (PG) slice, whose induced 3-geometry is Euclidean, one recovers the exact Euclidean scale $\sigma_p r_g \ge \pi\hbar$, which is optimal among all admissible slices. The entire construction extends across the black-hole horizon, and it transfers to the static axisymmetric Weyl class, where the Hardy floor, strict gap, and AF $\pi$-scale persist (a global PG-like optimum need not exist). |
| 2025-09-23 | [Hybrid Adaptive Robust Stochastic Optimization Model for the Design of a Photovoltaic Battery Energy Storage System](http://arxiv.org/abs/2509.19054v1) | Alba Lun Mora Pous, Fernando Garcia-MuÃ±oz et al. | Future energy projections and their inherent uncertainty play a key role in the design of photovoltaic-battery energy storage systems (PV-BESS) for household use. In this study, both stochastic and robust optimization techniques are simultaneously integrated into a Hybrid Adaptive Robust-Stochastic Optimization (HARSO) model. Uncertainty in future PV generation is addressed using a stochastic approach, while uncertainty in power demand is handled through robust optimization. To solve the tri-level structure emerging from the hybrid approach, a Column-and-Constraint Generation (CCG) algorithm is implemented. The model also accounts for battery degradation by considering multiple commercially available battery chemistries, enabling a more realistic evaluation of long-term system costs and performance. To demonstrate its applicability, the model is applied to a case study involving the optimal design of a PV-BESS system for a household in Spain. The empirical analysis includes both first-life (FL) and second-life (SL) batteries with different chemistries, providing a comprehensive evaluation of design alternatives under uncertainty. Results indicate that the optimal solution is highly dependent on the level of robustness considered, leading to a shift in design strategy. Under less conservative settings, robustness is achieved by increasing battery capacity, while higher levels of conservatism favor expanding PV capacity to meet demand. |
| 2025-09-23 | [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](http://arxiv.org/abs/2509.18998v1) | Christina Schenk, Jacobo Ayensa JimÃ©nez et al. | Computational models provide crucial insights into complex biological processes such as cancer evolution, but their mechanistic nature often makes them nonlinear and parameter-rich, complicating calibration. We systematically evaluate parameter probability distributions in cell migration models using Bayesian calibration across four complementary strategies: parametric and surrogate models, each with and without explicit model discrepancy. This approach enables joint analysis of parameter uncertainty, predictive performance, and interpretability. Applied to a real data experiment of glioblastoma progression in microfluidic devices, surrogate models achieve higher computational efficiency and predictive accuracy, whereas parametric models yield more reliable parameter estimates due to their mechanistic grounding. Incorporating model discrepancy exposes structural limitations, clarifying where model refinement is necessary. Together, these comparisons offer practical guidance for calibrating and improving computational models of complex biological systems. |
| 2025-09-23 | [Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints](http://arxiv.org/abs/2509.18988v1) | Ziliang Lyu, Miroslav Krstic et al. | This paper considers the problem of adaptively overriding unsafe actions of a nominal controller in the presence of high-relative-degree nonovershooting constraints and parametric uncertainties. To prevent the design from being coupled with high-order derivatives of the parameter estimation error, we adopt a modular design approach in which the controller and the parameter identifier are designed separately. The controller module ensures that any safety violations caused by parametric uncertainties remain bounded, provided that the parameter estimation error and its first-order derivative are either bounded or square-integrable. The identifier module, in turn, guarantees that these requirements on the parameter estimation error are satisfied. Both theoretical analysis and simulation results demonstrate that the closed-loop safety violation is bounded by a tunable function of the initial estimation error. Moreover, as time increases, the parameter estimate converges to the true value, and the amount of safety violation decreases accordingly. |
| 2025-09-23 | [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](http://arxiv.org/abs/2509.18954v1) | Minoo Dolatabadi, Fardin Ayar et al. | LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map, which may not always be available, or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty. In this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness. |
| 2025-09-22 | [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](http://arxiv.org/abs/2509.18090v1) | Jiahe Li, Jiawei Zhang et al. | Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR. |
| 2025-09-22 | [Robust, Online, and Adaptive Decentralized Gaussian Processes](http://arxiv.org/abs/2509.18011v1) | Fernando Llorente, Daniel Waxman et al. | Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling. |
| 2025-09-22 | [Cosmic inventory of the background fields of relativistic particles in the Universe](http://arxiv.org/abs/2509.17954v1) | Jonathan Biteau | The extragalactic background is composed of the emission from all astrophysical sources, both resolved and unresolved, in addition to any diffuse components. In the last decade, there has been significant progress in our understanding of the cosmic history of extragalactic emissions associated with stellar evolution and accretion onto supermassive black holes, largely enabled by the extensive body of multi-wavelength data. The brightness of the extragalactic sky is now measured in photons, neutrinos, and cosmic rays, using observatories on the ground, in the sea, and in the ice, satellites in Earth orbit, and probes at the edge of the solar system. This wealth of disparate data is essential to unraveling the mysteries of the source populations that contribute to the extragalactic background.   In this contribution, we present an open database containing the most comprehensive collection of measurements of the extragalactic background spectrum to date. The combination of multi-messenger measurements over 27 frequency decades allows us to estimate the energy density of most extragalactic background components with an uncertainty of less than 30%. We explore the consistency of this cosmic inventory of the observed fields of relativistic particles populating the Universe with the cosmic history of star formation and accretion around supermassive black holes. Models incorporating these cosmic histories, as well as the redshift-dependent luminosity functions of extragalactic sources, currently match the electromagnetic component of the extragalactic background spectrum over 14 frequency decades, from the near UV to sub-TeV gamma rays. The knowledge gained from synthetic population models in the electromagnetic bands may become a crucial tool for understanding the origin of the most energetic extragalactic messengers, neutrinos and ultrahigh-energy cosmic rays. |
| 2025-09-22 | [Deep Hierarchical Learning with Nested Subspace Networks](http://arxiv.org/abs/2509.17874v1) | Paulius Rauba, Mihaela van der Schaar | Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models. |
| 2025-09-22 | [Modeling Scintillation Photon Transport and Reconstruction Algorithms for the Time-of-Flight Detector in the T2K Neutrino Experiment](http://arxiv.org/abs/2509.17860v1) | C. Alt, A. Blanchet et al. | The T2K ND280 upgrade aims to reduce the systematic uncertainty of the CP-violating phase, $\delta_{CP}$, to reject non-CP violation hypothesis at $3\sigma$ confidence level. A crucial component of the ND280 upgrade, alongside the Super Fine Grained Detector (SuperFGD) and two High-Angle Time Projection Chambers (TPCs), is the Time-of-Flight (ToF) detector, which significantly enhances background rejection and particle identification capabilities. The ToF detector features six modules in a cube configuration, each with 20 plastic scintillator bars measuring $\text{220}\times\text{12}\times\text{1}\,\text{cm}^3$ and is equipped with Silicon Photomultiplier (SiPM) arrays at both ends to capture scintillation light. This letter outlines the modelling of the detector response and the signal reconstruction process. |
| 2025-09-22 | [How Realistic are Idealized Copper Surfaces? A Machine Learning Study of Rough Copper-Water Interfaces](http://arxiv.org/abs/2509.17833v1) | Linus C. Erhard, Johannes SchÃ¶rghuber et al. | Copper is a highly promising catalyst for the electrochemical CO$_2$ reduction reaction (CO2RR) since it is the only pure metal that can form highly added-value products such as ethylene and ethanol. Since the CO2RR takes place in aqueous solution, the detailed atomic structure of the water-copper interface is essential for unraveling the key reaction mechanisms. In this study, we investigate copper-water interfaces exhibiting nanometer-scale roughnesses. We introduce two molecular dynamics protocols to create rough copper surfaces, which are subsequently brought into contact with water. From these interfaces, we sample additional training configurations from machine-learning-interatomic-potential-driven molecular dynamics simulations containing hundreds of thousands of atoms. An active learning workflow is developed to identify regions with high spatially resolved uncertainty and convert them into DFT-feasible cells through a modified amorphous matrix embedding approach. Finally, we analyze the local environments at the interface using unsupervised machine-learning techniques. Unique environments emerge on the rough copper surfaces absent from model systems, including stacking-fault-induced configurations and undercoordinated corner atoms. Notably, corner atoms consistently feature chemisorbed water molecules in our simulations, indicating their potential importance in catalytic processes. |
| 2025-09-22 | [On Fast Attitude Filtering Based on Matrix Fisher Distribution with Stability Guarantee](http://arxiv.org/abs/2509.17827v1) | Shijie Wang, Haichao Gui et al. | This paper addresses two interrelated problems of the nonlinear filtering mechanism and fast attitude filtering with the matrix Fisher distribution (MFD) on the special orthogonal group. By analyzing the distribution evolution along Bayes' rule, we reveal two essential properties that enhance the performance of Bayesian attitude filters with MFDs, particularly in challenging conditions, from a theoretical viewpoint.   Benefiting from the new understanding of the filtering mechanism associated with MFDs, two closed-form filters with MFDs is then proposed. These filters avoid the burdensome computations in previous MFD-based filters by introducing linearized error systems with right-invariant errors but retaining the two advantageous properties. Moreover, we leverage the two properties and closed-form filtering iteration to prove the almost-global exponential stability of the proposed filter with right-invariant error for the single-axis rotation, which, to our knowledge, is not achieved by existing directional statistics-based filters. Numerical simulations demonstrate that the proposed filters are significantly more accurate than the classic invariant Kalman filter. Besides, they are also as accurate as recent MFD-based Bayesian filters in challenging circumstances with large initial error and measurement uncertainty but consumes far less computation time (about 1/5 to 1/100 of previous MFD-based attitude filters). |
| 2025-09-22 | [RoboSeek: You Need to Interact with Your Objects](http://arxiv.org/abs/2509.17783v1) | Yibo Peng, Jiahao Yang et al. | Optimizing and refining action execution through   exploration and interaction is a promising way for robotic   manipulation. However, practical approaches to interaction driven robotic learning are still underexplored, particularly for   long-horizon tasks where sequential decision-making, physical   constraints, and perceptual uncertainties pose significant chal lenges. Motivated by embodied cognition theory, we propose   RoboSeek, a framework for embodied action execution that   leverages interactive experience to accomplish manipulation   tasks. RoboSeek optimizes prior knowledge from high-level   perception models through closed-loop training in simulation   and achieves robust real-world execution via a real2sim2real   transfer pipeline. Specifically, we first replicate real-world   environments in simulation using 3D reconstruction to provide   visually and physically consistent environments., then we train   policies in simulation using reinforcement learning and the   cross-entropy method leveraging visual priors. The learned   policies are subsequently deployed on real robotic platforms   for execution. RoboSeek is hardware-agnostic and is evaluated   on multiple robotic platforms across eight long-horizon ma nipulation tasks involving sequential interactions, tool use, and   object handling. Our approach achieves an average success rate   of 79%, significantly outperforming baselines whose success   rates remain below 50%, highlighting its generalization and   robustness across tasks and platforms. Experimental results   validate the effectiveness of our training framework in complex,   dynamic real-world settings and demonstrate the stability of the   proposed real2sim2real transfer mechanism, paving the way for   more generalizable embodied robotic learning. Project Page:   https://russderrick.github.io/Roboseek/ |
| 2025-09-22 | [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](http://arxiv.org/abs/2509.17726v1) | Javier Bisbal, Patrick Winter et al. | Accurate anatomical labeling of intracranial arteries is essential for cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming and subject to interoperator variability. We present a deep learning-based framework for automated artery labeling from 3D Time-of-Flight Magnetic Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating uncertainty quantification to enhance interpretability and reliability. We evaluated three convolutional neural network architectures: (1) a UNet with residual encoder blocks, reflecting commonly used baselines in vascular labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and spatial attention mechanisms for enhanced curvilinear structure recognition; and (3) nnUNet, a self-configuring framework that automates preprocessing, training, and architectural adaptation based on dataset characteristics. Among these, nnUNet achieved the highest labeling performance (average Dice score: 0.922; average surface distance: 0.387 mm), with improved robustness in anatomically complex vessels. To assess predictive confidence, we implemented test-time augmentation (TTA) and introduced a novel coordinate-guided strategy to reduce interpolation errors during augmented inference. The resulting uncertainty maps reliably indicated regions of anatomical ambiguity, pathological variation, or manual labeling inconsistency. We further validated clinical utility by comparing flow velocities derived from automated and manual labels in co-registered 4D Flow MRI datasets, observing close agreement with no statistically significant differences. Our framework offers a scalable, accurate, and uncertainty-aware solution for automated cerebrovascular labeling, supporting downstream hemodynamic analysis and facilitating clinical integration. |
| 2025-09-22 | [Enhanced fill probability estimates in institutional algorithmic bond trading using statistical learning algorithms with quantum computers](http://arxiv.org/abs/2509.17715v1) | Axel Ciceri, Austin Cottrell et al. | The estimation of fill probabilities for trade orders represents a key ingredient in the optimization of algorithmic trading strategies. It is bound by the complex dynamics of financial markets with inherent uncertainties, and the limitations of models aiming to learn from multivariate financial time series that often exhibit stochastic properties with hidden temporal patterns. In this paper, we focus on algorithmic responses to trade inquiries in the corporate bond market and investigate fill probability estimation errors of common machine learning models when given real production-scale intraday trade event data, transformed by a quantum algorithm running on IBM Heron processors, as well as on noiseless quantum simulators for comparison. We introduce a framework to embed these quantum-generated data transforms as a decoupled offline component that can be selectively queried by models in low-latency institutional trade optimization settings. A trade execution backtesting method is employed to evaluate the fill prediction performance of these models in relation to their input data. We observe a relative gain of up to ~ 34% in out-of-sample test scores for those models with access to quantum hardware-transformed data over those using the original trading data or transforms by noiseless quantum simulation. These empirical results suggest that the inherent noise in current quantum hardware contributes to this effect and motivates further studies. Our work demonstrates the emerging potential of quantum computing as a complementary explorative tool in quantitative finance and encourages applied industry research towards practical applications in trading. |
| 2025-09-19 | [CAPOS: The bulge Cluster APOGEE Survey VI. Characterizing multiple stellar populations and chemical abundances in the bulge globular cluster NGC 6569](http://arxiv.org/abs/2509.16168v1) | NicolÃ¡s Barrera, Sandro Villanova et al. | Context. The CAPOS project aims to obtain accurate mean abundances and radial velocities for many elements, and it explores the multiple population (MP) phenomenon in Galactic bulge globular clusters (BGCs). NGC 6569 is one of the CAPOS targets. Aims. This study provides a detailed high-resolution spectroscopic analysis of NGC 6569 to derive precise abundances for elements of different nucleosynthetic origins and to unveil its MPs by focusing on key spectral features. Methods. We analyzed APOGEE-2 near-infrared spectra of 11 giant members with the BACCHUS code, deriving abundances for 12 elements (C, N, O, Mg, Si, Ca, Ti, Fe, Ni, Al, Ce, Nd). Isochrone fitting with Gaia+2MASS photometry was used to estimate atmospheric parameters, cluster distance, and extinction. Results. We obtained [Fe/H] = -0.91 $\pm$ 0.06, consistent with APOGEE pipeline values; the scatter lies within uncertainties. The cluster shows [$\alpha$/Fe] = 0.36 $\pm$ 0.06 dex, similar to other GCs. Al appears homogeneous, while N is strongly enriched ([N/Fe] = 0.68 $\pm$ 0.34) with a spread of 0.90 dex, yielding two populations anticorrelated in C. The n-capture elements Ce and Nd are overabundant compared to the Sun but consistent with GCs of similar metallicity, with $\langle$[Ce/Nd]$\rangle$ = -0.17 $\pm$ 0.12. We also measure RV = -49.75 $\pm$ 3.68 km s$^{-1}$, consistent with previous works, while d$_\odot$ = 12.4 $\pm$ 1.45 kpc and E(B-V) = 0.68 are both higher than literature values. Conclusions. MPs in NGC 6569 are confirmed through a clear C-N anticorrelation. The cluster shows [$\alpha$/Fe] enhancement from Type II SNe and no Mg-Al-Si anticorrelation, suggesting rapid and homogeneous formation. The $\langle$[Ce/Nd]$\rangle$ ratio points to contributions from r-process events such as neutron star mergers, while overall Ce and Nd abundances are reported here for the first time in this cluster. |
| 2025-09-19 | [Lensed stars in galaxy-galaxy strong lensing -- a JWST prediction for the Cosmic Horseshoe](http://arxiv.org/abs/2509.16154v1) | Sung Kei Li, Luke Weisenbach et al. | We explore for the first time the possibility of detecting lensed star transients in galaxy-galaxy strong lensing systems upon repeated, deep imaging using the {\it James-Webb Space Telescope} ({\it JWST}). Our calculation predicts that the extremely high recent star formation rate of $\sim 100\,M_{\odot}\textrm{yr}^{-1}$ over the last 50 Myr (not accounting for image multiplicity) in the ``Cosmic Horseshoe'' lensed system ($z = 2.381$) generates many young, bright stars, of which their large abundance is expected to lead to a detection rate of $\sim 60$ transients per pointing in {\it JWST} observations with a $5\sigma$ limiting magnitude of $\sim 29\,m_{AB}$. With the high expected detection rate and little room for uncertainty for the lens model compared with cluster lenses, our result suggests that the Cosmic Horseshoe could be an excellent tool to test the nature of Dark Matter based on the spatial distribution of transients, and can be used to constrain axion mass if Dark Matter is constituted of ultra-light axions. We also argue that the large distance modulus of $\sim46.5\,$mag at $z \approx 2.4$ can act as a filter to screen out less massive stars as transients and allow one to better constrain the high-mass end of the stellar initial mass function based on the transient detection rate. Follow-up {\it JWST} observations of the Cosmic Horseshoe with would allow one to better probe the nature of Dark Matter and the star formation properties, such as the initial mass function at the cosmic noon, via lensed star transients. |
| 2025-09-19 | [First evidence of $CP$ violation in beauty baryon to charmonium decays](http://arxiv.org/abs/2509.16103v1) | LHCb collaboration, R. Aaij et al. | A study of the difference in the $CP$ asymmetries between ${\Lambda}^0_b \rightarrow J / \psi p \pi^-$ and ${\Lambda}^0_b \rightarrow J / \psi p K^-$ decays, $\Delta {\cal A}_{CP}$, is performed using proton-proton collision data collected by the LHCb experiment in the years 2015--2018, corresponding to an integrated luminosity of $6 {\rm fb}^{-1}$. This quantity is measured to be $ \Delta {\cal A}_{CP}=(4.03\pm 1.18\pm 0.23)\%$, where the first uncertainty is statistical and the second is systematic. When combined with the previous LHCb result, a value of $\Delta {\cal A}_{CP} = (4.31 \pm 1.06 \pm 0.28)\%$ is obtained, corresponding to a significance of $3.9\sigma$ against the $CP$ symmetry hypothesis. Studies of triple-product asymmetries, which provide an additional probe of $CP$ violation, show no significant deviation from $CP$ symmetry. |
| 2025-09-19 | [Compose by Focus: Scene Graph-based Atomic Skills](http://arxiv.org/abs/2509.16053v1) | Han Qi, Changhe Chen et al. | A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine "focused" scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks. |
| 2025-09-19 | [Automated Model Tuning for Multifidelity Uncertainty Propagation in Trajectory Simulation](http://arxiv.org/abs/2509.16007v1) | James E. Warner, Geoffrey F. Bomarito et al. | Multifidelity uncertainty propagation combines the efficiency of low-fidelity models with the accuracy of a high-fidelity model to construct statistical estimators of quantities of interest. It is well known that the effectiveness of such methods depends crucially on the relative correlations and computational costs of the available computational models. However, the question of how to automatically tune low-fidelity models to maximize performance remains an open area of research. This work investigates automated model tuning, which optimizes model hyperparameters to minimize estimator variance within a target computational budget. Focusing on multifidelity trajectory simulation estimators, the cost-versus-precision tradeoff enabled by this approach is demonstrated in a practical, online setting where upfront tuning costs cannot be amortized. Using a real-world entry, descent, and landing example, it is shown that automated model tuning largely outperforms hand-tuned models even when the overall computational budget is relatively low. Furthermore, for scenarios where the computational budget is large, model tuning solutions can approach the best-case multifidelity estimator performance where optimal model hyperparameters are known a priori. Recommendations for applying model tuning in practice are provided and avenues for enabling adoption of such approaches for budget-constrained problems are highlighted. |
| 2025-09-19 | [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](http://arxiv.org/abs/2509.15987v1) | AurÃ©lien Cecille, Stefan Duffner et al. | Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines. |
| 2025-09-19 | [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](http://arxiv.org/abs/2509.15981v1) | Yujie Zhu, Charles A. Hepburn et al. | In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD. |
| 2025-09-19 | [Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems](http://arxiv.org/abs/2509.15961v1) | Nicole Aretz, Thomas Lynn et al. | We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition. |
| 2025-09-19 | [Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration](http://arxiv.org/abs/2509.15959v1) | Zhuoyue Zhang, Haitong Xu | Autonomous navigation in maritime domains is accelerating alongside advances in artificial intelligence, sensing, and connectivity. Opaque decision-making and poorly calibrated human-automation interaction remain key barriers to safe adoption. This article synthesizes 100 studies on automation transparency for Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA), human factors, interface design, and regulation. We (i) map the Guidance-Navigation-Control stack to shore-based operational modes -- remote supervision (RSM) and remote control (RCM) -- and identify where human unsafe control actions (Human-UCAs) concentrate in handover and emergency loops; (ii) summarize evidence that transparency features (decision rationales, alternatives, confidence/uncertainty, and rule-compliance indicators) improve understanding and support trust calibration, though reliability and predictability often dominate trust; (iii) distill design strategies for transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI presentation (textual/graphical overlays, color coding, conversational and immersive UIs), and engineer-facing processes (resilient interaction design, validation, and standardization). We integrate methods for Human-UCA identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and operator workload monitoring, and outline regulatory and rule-based implications including COLREGs formalization and route exchange. We conclude with an adaptive transparency framework that couples operator state estimation with explainable decision support to reduce cognitive overload and improve takeover timeliness. The review highlights actionable figure-of-merit displays (e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs (rule traceability, confidence), and training pipelines (HIL/MIL, simulation) as near-term levers for safer MASS operations. |
| 2025-09-19 | [Quantum Metric Spaces: Replacing Fuzzy Metrics with the Hilbert Space Structure of Quantum States](http://arxiv.org/abs/2509.15945v1) | Nicola Fabiano | Fuzzy metric spaces, grounded in t-norms and membership functions, have been widely proposed to model uncertainty in machine learning, decision systems, and artificial intelligence. Yet these frameworks treat uncertainty as an external layer of imprecision imposed upon classical, point-like entities - a conceptual mismatch for domains where indeterminacy is intrinsic, such as quantum systems or cognitive representations. We argue that fuzzy metrics are unnecessary for modeling such uncertainty: instead, the well-established structure of complex Hilbert spaces - the foundational language of quantum mechanics for over a century - provides a natural, rigorous, and non-contradictory metric space where the ``points'' are quantum states themselves. The distance between states is given by the Hilbert norm, which directly encodes state distinguishability via the Born rule. This framework inherently captures the non-classical nature of uncertainty without requiring fuzzy logic, t-norms, or membership degrees. We demonstrate its power by modeling AI concepts as Gaussian wavefunctions and classifying ambiguous inputs via quantum overlap integrals. Unlike fuzzy methods, our approach naturally handles interference, distributional shape, and concept compositionality through the geometry of state vectors. We conclude that fuzzy metric spaces, while historically useful, are obsolete for representing intrinsic uncertainty - superseded by the more robust, predictive, and ontologically coherent framework of quantum state geometry. |
| 2025-09-18 | [Explaining deep learning for ECG using time-localized clusters](http://arxiv.org/abs/2509.15198v1) | AhcÃ¨ne Boubekki, Konstantinos Patlatzoglou et al. | Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns. |
| 2025-09-18 | [Parallel Simulation of Contact and Actuation for Soft Growing Robots](http://arxiv.org/abs/2509.15180v1) | Yitian Gao, Lucas Chen et al. | Soft growing robots, commonly referred to as vine robots, have demonstrated remarkable ability to interact safely and robustly with unstructured and dynamic environments. It is therefore natural to exploit contact with the environment for planning and design optimization tasks. Previous research has focused on planning under contact for passively deforming robots with pre-formed bends. However, adding active steering to these soft growing robots is necessary for successful navigation in more complex environments. To this end, we develop a unified modeling framework that integrates vine robot growth, bending, actuation, and obstacle contact. We extend the beam moment model to include the effects of actuation on kinematics under growth and then use these models to develop a fast parallel simulation framework. We validate our model and simulator with real robot experiments. To showcase the capabilities of our framework, we apply our model in a design optimization task to find designs for vine robots navigating through cluttered environments, identifying designs that minimize the number of required actuators by exploiting environmental contacts. We show the robustness of the designs to environmental and manufacturing uncertainties. Finally, we fabricate an optimized design and successfully deploy it in an obstacle-rich environment. |
| 2025-09-18 | [To CLEAN or not to CLEAN: Data Processing in the ngVLA era](http://arxiv.org/abs/2509.15176v1) | Hendrik MÃ¼ller | Radio interferometric imaging has long relied on the CLEAN algorithm, valued for its speed, robustness, and integration with calibration pipelines. However, next-generation facilities such as the ngVLA, SKA, and ALMAs Wideband Sensitivity Upgrade will produce data volumes and dynamic ranges that exceed the scalability of traditional methods. CLEAN remains dominant due to its simplicity and accumulated expertise, yet its assumption of modeling the sky as point sources limits its ability to recover extended emission and hampers automation. We review CLEANs limitations and survey alternatives, including multiscale extensions, compressive sensing, Regularized Maximum Likelihood, Bayesian inference, and AI-driven approaches. Forward-modeling methods enable higher fidelity, flexible priors, and uncertainty quantification, albeit at greater computational cost. Hybrid approaches such as Autocorr-CLEAN, CG-CLEAN, and PolyCLEAN retain CLEANs workflow while incorporating modern optimization. We argue hybrids are best suited for the near term, while Bayesian and AI-based frameworks represent the long-term future of interferometric imaging. |
| 2025-09-18 | [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](http://arxiv.org/abs/2509.15147v1) | Viktor Kovalchuk, Nikita Kotelevskii et al. | Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training. |
| 2025-09-18 | [A model-independent measurement of the CKM angle $Î³$ in the decays $B^\pm\to[K^+K^-Ï€^+Ï€^-]_D h^\pm$ and $B^\pm\to[Ï€^+Ï€^-Ï€^+Ï€^-]_D h^\pm$ ($h = K, Ï€$)](http://arxiv.org/abs/2509.15139v1) | LHCb collaboration, R. Aaij et al. | A model-independent determination of the CKM angle $\gamma$ is presented, using the $B^\pm\to[K^+K^-\pi^+\pi^-]_D h^\pm$ and $B^\pm\to[\pi^+\pi^-\pi^+\pi^-]_D h^\pm$ decays, with $h=K,\pi$. This measurement is the first phase-space-binned study of these decay modes, and uses a sample of proton-proton collision data collected by the LHCb experiment, corresponding to an integrated luminosity of $9$fb$^{-1}$. The phase-space bins are optimised for sensitivity to $\gamma$, and in each bin external inputs from the BESIII experiment are used to constrain the charm strong-phase parameters. The result of this binned analysis is $\gamma = (53.9_{-8.9}^{+9.5})^\circ$, where the uncertainty includes both statistical and systematic contributions. Furthermore, when combining with existing phase-space-integrated measurements of the same decay modes, a value of $\gamma = (52.6_{-6.4}^{+8.5})^\circ$ is obtained, which is one of the most precise determinations of $\gamma$ to date. |
| 2025-09-18 | [Sequential sample size calculations and learning curves safeguard the robust development of a clinical prediction model for individuals](http://arxiv.org/abs/2509.15134v1) | Amardeep Legha, Joie Ensor et al. | When prospectively developing a new clinical prediction model (CPM), fixed sample size calculations are typically conducted before data collection based on sensible assumptions. But if the assumptions are inaccurate the actual sample size required to develop a reliable model may be very different. To safeguard against this, adaptive sample size approaches have been proposed, based on sequential evaluation of a models predictive performance. Aim: illustrate and extend sequential sample size calculations for CPM development by (i) proposing stopping rules based on minimising uncertainty (instability) and misclassification of individual-level predictions, and (ii) showcasing how it safeguards against inaccurate fixed sample size calculations. Using the sequential approach repeats the pre-defined model development strategy every time a chosen number (e.g., 100) of participants are recruited and adequately followed up. At each stage, CPM performance is evaluated using bootstrapping, leading to prediction and classification stability statistics and plots, alongside optimism-adjusted measures of calibration and discrimination. Our approach is illustrated for development of acute kidney injury using logistic regression CPMs. The fixed sample size calculation, based on perceived sensible assumptions suggests recruiting 342 patients to minimise overfitting; however, the sequential approach reveals that a much larger sample size of 1100 is required to minimise overfitting (targeting population-level stability). If the stopping rule criteria also target small uncertainty and misclassification probability of individual predictions, the sequential approach suggests an even larger sample size (n=1800). Our sequential sample size approach allows users to dynamically monitor individual-level prediction and classification instability and safeguard against using inaccurate assumptions. |
| 2025-09-18 | [LOFAR 58 MHz Legacy Survey of the 3CRR Catalog](http://arxiv.org/abs/2509.15115v1) | J. M. Boxelaar, F. De Gasperin et al. | The Low Frequency Array (LOFAR) is uniquely able to perform deep, 15" resolutions imaging at frequencies below 100 MHz. Observations in this regime, using the Low Band Antenna (LBA) system, are significantly affected by instrumental and ionospheric distortions. Recent developments in calibration techniques have enabled routine production of high-fidelity images at these challenging frequencies. The aim of this paper is to obtain images of the radio sources included in the Third Cambridge catalog, second revised version (3CRR), at an observing frequency of 58 MHz, with an angular resolution of 15"and sensitivity to both compact and diffuse radio emission. This work also aims to produce accurate flux measurements for all sources. This dataset is designed to serve as a reference for low-frequency radio galaxy studies and future spectral aging analyses. We deliver 58. MHz radio images for the complete 3CRR sample including flux density measurements. We determined that the LBA has an accurate flux density scale with an average flux uncertainty of 10%. This is an important confirmation for any future works using the LOFAR LBA system. With these results we characterize the bright radio galaxy population with new high-resolution low-frequency images. We also provide high-resolution models of these sources which will be useful for calibrating future surveys. This legacy survey significantly expands the available high-resolution data at low frequencies and is the first fully imaged high-resolution sample at ultra low frequencies (< 100 MHz). It lays the foundation for future studies of radio galaxy physics, low-energy cosmic-ray populations, and the interplay between radio jets and their environments. |
| 2025-09-18 | [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](http://arxiv.org/abs/2509.15107v1) | Amy Rafferty, Rishi Ramaesh et al. | Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks. |
| 2025-09-18 | [Precise measurement of the $t\bar{t}$ production cross-section and lepton differential distributions in $eÎ¼$ dilepton events from $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector](http://arxiv.org/abs/2509.15066v1) | ATLAS Collaboration | The inclusive top quark pair ($t\bar{t}$) cross-section $\sigma_{t\bar{t}}$ has been measured in $\sqrt{s}=13$ TeV proton-proton collisions, using 140 fb$^{-1}$ of data collected by the ATLAS experiment at the Large Hadron Collider. Using events with an opposite-charge $e\mu$ pair and $b$-tagged jets, the cross-section is measured to be: $\sigma_{t\bar{t}} = 829.3 \pm 1.3\,\mathrm{(stat)}\ \pm 8.0\,\mathrm{(syst)}\ \pm 7.3\,\mathrm{(lumi)}\ \pm 1.9\,\mathrm{(beam)}\,\mathrm{pb},$ where the uncertainties reflect the limited size of the data sample, experimental and theoretical systematic effects, the integrated luminosity, and the proton beam energy, giving a total uncertainty of 1.3%. The result is used to determine the top quark pole mass via the dependence of the predicted cross-section on $m_t^\mathrm{pole}$, giving $m_t^\mathrm{pole}=172.8^{+1.5}_{-1.7}$ GeV. The same event sample is used to measure absolute and normalised differential cross-sections for the $t\bar{t}\rightarrow e\mu\nu\bar{\nu}b\bar{b}$ process as a function of single-lepton and dilepton kinematic variables. Complementary measurements of $e\mu b\bar{b}$ production, treating both $t\bar{t}$ and $Wt$ events as signal, are also provided. Both sets of differential cross-sections are compared to the predictions of various Monte Carlo event generators, demonstrating that the state-of-the-art generators Powheg MiNNLO and Powheg $bb4l$ describe the data better than Powheg hvq. |
| 2025-09-18 | [Constraining Cosmology with Double-Source-Plane Strong Gravitational Lenses From the AGEL Survey](http://arxiv.org/abs/2509.15012v1) | Duncan J. Bowden, Nandini Sahu et al. | Double-source-plane strong gravitational lenses (DSPLs), with two sources at different redshifts, are independent cosmological probes of the dark energy equation of state parameter $w$ and the matter density parameter $\Omega_{\rm m}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer cosmological constraints from this system for flat $\Lambda$CDM and flat $w$CDM cosmologies. From the joint posterior of $w$ and $\Omega_{\rm m}$ in the flat $w$CDM cosmology, we extract the following median values and 1$\sigma$ uncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\Omega_{\rm m} = 0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with two previously analyzed DSPLs, we present the joint constraint on these parameters from a sample of three, the largest galaxy-scale DSPL sample used for cosmological measurement to date. The combined precision of $w$ from three DSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave background (CMB) measurements improves the precision of $w$ from CMB-only constraints by 39%, demonstrating the complementarity of DSPLs with the CMB. Despite their promising constraining power, DSPLs are limited by sample size, with only a handful discovered so far. Although ongoing and near-future wide-area sky surveys will increase the number of known DSPLs by up to two orders of magnitude, these systems will still require dedicated high-resolution imaging and spectroscopic follow-ups like those presented in this paper. Our ASTRO 3D Galaxy Evolution with Lenses (AGEL) collaboration is undertaking such follow-up campaigns for several newly discovered DSPLs and will provide cosmological measurements from larger samples of DSPLs in the future. |
| 2025-09-17 | [Large deviations for probability graphons](http://arxiv.org/abs/2509.14204v1) | Pierfrancesco Dionigi, Giulio Zucal | We establish a large deviation principle (LDP) for probability graphons, which are symmetric functions from the unit square into the space of probability measures. This notion extends classical graphons and provides a flexible framework for studying the limit behavior of large dense weighted graphs. In particular, our result generalizes the seminal work of Chatterjee and Varadhan (2011), who derived an LDP for Erd\H{o}s-R\'enyi random graphs via graphon theory. We move beyond their binary (Bernoulli) setting to encompass arbitrary edge-weight distributions. Specifically, we analyze the distribution on probability graphons induced by random weighted graphs in which edges are sampled independently from a common reference probability measure supported on a compact Polish space. We prove that this distribution satisfies an LDP with a good rate function, expressed as an extension of the Kullback-Leibler divergence between probability graphons and the reference measure. This theorem can also be viewed as a Sanov-type result in the graphon setting. Our work provides a rigorous foundation for analyzing rare events in weighted networks and supports statistical inference in structured random graph models under distributional edge uncertainty. |
| 2025-09-17 | [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](http://arxiv.org/abs/2509.14167v1) | Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes et al. | Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains. |
| 2025-09-17 | [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](http://arxiv.org/abs/2509.14151v1) | Rongyu Zhang, Jiaming Liu et al. | Vision-centric Bird's Eye View (BEV) perception holds considerable promise for autonomous driving. Recent studies have prioritized efficiency or accuracy enhancements, yet the issue of domain shift has been overlooked, leading to substantial performance degradation upon transfer. We identify major domain gaps in real-world cross-domain scenarios and initiate the first effort to address the Domain Adaptation (DA) challenge in multi-view 3D object detection for BEV perception. Given the complexity of BEV perception approaches with their multiple components, domain shift accumulation across multi-geometric spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain adaptation. In this paper, we introduce an innovative geometric-aware teacher-student framework, BEVUDA++, to diminish this issue, comprising a Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model. Specifically, RDT effectively blends target LiDAR with dependable depth predictions to generate depth-aware information based on uncertainty estimation, enhancing the extraction of Voxel and BEV features that are essential for understanding the target domain. To collaboratively reduce the domain shift, GCS maps features from multiple spaces into a unified geometric embedding space, thereby narrowing the gap in data distribution between the two domains. Additionally, we introduce a novel Uncertainty-guided Exponential Moving Average (UEMA) to further reduce error accumulation due to domain shifts informed by previously obtained uncertainty guidance. To demonstrate the superiority of our proposed method, we execute comprehensive experiments in four cross-domain scenarios, securing state-of-the-art performance in BEV 3D object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night adaptation. |
| 2025-09-17 | [Safe Sliding Mode Control in Position for Double Integrator Systems](http://arxiv.org/abs/2509.14121v1) | Marco A. Gomez, Christopher D. Cruz-Ancona | We address the problem of robust safety control design for double integrator systems. We show that, when the constraints are defined only on position states, it is possible to construct a safe sliding domain from the dynamic of a simple integrator that is already safe. On this domain, the closed-loop trajectories remain robust and safe against uncertainties and disturbances. Furthermore, we design a controller gain that guarantees convergence to the safe sliding domain while avoiding the given unsafe set. The concept is initially developed for first-order sliding mode and is subsequently generalized to an adaptive framework, ensuring that trajectories remain confined to a predefined vicinity of the sliding domain, outside the unsafe region. |
| 2025-09-17 | [Online Bayesian Risk-Averse Reinforcement Learning](http://arxiv.org/abs/2509.14077v1) | Yuhao Wang, Enlu Zhou | In this paper, we study the Bayesian risk-averse formulation in reinforcement learning (RL). To address the epistemic uncertainty due to a lack of data, we adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the parameter uncertainty of the unknown underlying model. We derive the asymptotic normality that characterizes the difference between the Bayesian risk value function and the original value function under the true unknown distribution. The results indicate that the Bayesian risk-averse approach tends to pessimistically underestimate the original value function. This discrepancy increases with stronger risk aversion and decreases as more data become available. We then utilize this adaptive property in the setting of online RL as well as online contextual multi-arm bandits (CMAB), a special case of online RL. We provide two procedures using posterior sampling for both the general RL problem and the CMAB problem. We establish a sub-linear regret bound, with the regret defined as the conventional regret for both the RL and CMAB settings. Additionally, we establish a sub-linear regret bound for the CMAB setting with the regret defined as the Bayesian risk regret. Finally, we conduct numerical experiments to demonstrate the effectiveness of the proposed algorithm in addressing epistemic uncertainty and verifying the theoretical properties. |
| 2025-09-17 | [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](http://arxiv.org/abs/2509.14054v1) | Weihao Yan, Christoph Brune et al. | Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications. |
| 2025-09-17 | [Network representations reveal structured uncertainty in music](http://arxiv.org/abs/2509.14053v1) | Lluc Bono RossellÃ³, Robert Jankowski et al. | Music, as a structured yet perceptually rich experience, can be modeled as a network to uncover how humans encode and process auditory information. While network-based representations of music are increasingly common, the impact of feature selection on structural properties and cognitive alignment remains underexplored. In this study, we evaluated eight network models, each constructed from symbolic representations of piano compositions using distinct combinations of pitch, octave, duration, and interval, designed to be representative of existing approaches in the literature. By comparing these models through topological metrics, entropy analysis, and divergence with respect to inferred cognitive representations, we assessed both their structural and perceptual efficiency. Our findings reveal that simpler, feature-specific models better match human perception, whereas complex, multidimensional representations introduce cognitive inefficiencies. These results support the view that humans rely on modular, parallel cognitive networks--an architecture consistent with theories of predictive processing and free energy minimization. Moreover, we find that musical networks are structurally organized to guide attention toward transitions that are both uncertain and inferable. The resulting structure concentrates uncertainty in a few frequently visited nodes, creating local entropy gradients that alternate between stable and unpredictable regions, thereby enabling the expressive dynamics of tension and release that define the musical experience. These findings show that network structures make the organization of uncertainty in music observable, offering new insight into how patterned flows of expectation shape perception, and open new directions for studying how musical structures evolve across genres, cultures, and historical periods through the lens of network science. |
| 2025-09-17 | [Day-Ahead Transmission Grid Topology Optimization Considering Renewable Energy Sources' Uncertainty](http://arxiv.org/abs/2509.13994v1) | Giacomo Bastianel, Dirk Van Hertem et al. | The increasing renewable penetration introduces significant uncertainty in power system operations. At the same time, the existing transmission grid is often already congested, and urgently needed reinforcements are frequently delayed due to several constraints. To address these challenges, adjusting the grid topology based on congestion patterns is considered a non-costly remedy to guarantee efficient power transmission. Based on this idea, this paper proposes a grid topology optimization model combining optimal transmission switching and busbar splitting for AC and hybrid AC/DC grids. The methodology incorporates RES forecast uncertainty through a scenario-based stochastic optimization approach, using real offshore wind data and K-means clustering to generate representative forecast error scenarios. The proposed model includes several formulations to be compared with a plain optimal power flow (OPF) model: hourly optimizing the topology, one topology for 24 hours, or a limited number of switching actions over a day. The grid topology optimization model is formulated as a Mixed-Integer Quadratic Convex Problem, optimized based on the day-ahead (D-1) RES forecast and validated for AC-feasibility via an AC-OPF formulation. Based on the generation setpoints of the feasibility check, a redispatch simulation based on the measured (D) RES realization is then computed. The methodology is tested on an AC 30-bus test case and a hybrid AC/DC 50-bus test case, for a 24-hours (30-bus) and a 14-days (both test cases) time series. The results highlight the economic benefits brought by grid topology optimization for congested test cases with high penetration of RES. In addition, the results demonstrate that accounting for RES uncertainty with at least 6 to 8 scenarios leads to lower or comparable total costs to deterministic day-ahead forecasts, even when limiting the frequency of topological actions. |
| 2025-09-17 | [Improved systematic evaluation of a strontium optical clock with uncertainty below $1\times 10^{-18}$](http://arxiv.org/abs/2509.13991v1) | Zhi-Peng Jia, Jie Li et al. | We report a systematic uncertainty of $9.2\times 10^{-19}$ for the USTC Sr1 optical lattice clock, achieving accuracy at the level required for the roadmap of the redefinition of the SI second. A finite-element model with {\it in situ}-validated, spatially-resolved chamber emissivity reduced blackbody radiation shift uncertainty to $6.3\times 10^{-19}$. Concurrently, an externally mounted lattice cavity combined with a larger beam waist suppressed density shifts. Enhanced lattice depth modulation consolidated lattice light shift uncertainty to $6.3\times 10^{-19}$ by enabling simultaneous determination of key polarizabilities and magic wavelength. Magnetic shifts were resolved below $10^{-18}$ via precision characterization of the second-order Zeeman coefficient. Supported by a crystalline-coated ultra-low-expansion cavity-stabilized laser and refined temperature control suppressing BBR fluctuations, the clock also achieves a frequency stability better than $1\times10^{-18}$ at 30,000-s averaging time. These developments collectively establish a new benchmark in USTC Sr1 clock performance and pave the way for high-accuracy applications in metrology and fundamental physics. |
| 2025-09-17 | [Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game](http://arxiv.org/abs/2509.13985v1) | Yixun Wen, Yulong Gao et al. | Generalized Nash equilibrium problem (GNEP) is fundamental for practical applications where multiple self-interested agents work together to make optimal decisions. In this work, we study GNEP with shared distributionally robust chance constraints (DRCCs) for incorporating inevitable uncertainties. The DRCCs are defined over the Wasserstein ball, which can be explicitly characterized even with limited sample data. To determine the equilibrium of the GNEP, we propose an exact approach to transform the original computationally intractable problem into a deterministic formulation using the Nikaido-Isoda function. Specifically, we show that when all agents' objectives are quadratic in their respective variables, the equilibrium can be obtained by solving a typical mixed-integer nonlinear programming (MINLP) problem, where the integer and continuous variables are decoupled in both the objective function and the constraints. This structure significantly improves computational tractability, as demonstrated through a case study on the charging station pricing problem. |
| 2025-09-16 | [Axion-photon conversion in transient compact stars: Systematics, constraints, and opportunities](http://arxiv.org/abs/2509.13322v1) | Damiano F. G. Fiorillo, Ãngel Gil Muyor et al. | We study magnetic conversion of ultra-relativistic axion-like particles (ALPs) into photons in compact-star environments, focusing on the hot, transient conditions of core-collapse supernova (SN) remnants and neutron-star mergers (NSMs). We address previously overlooked uncertainties, particularly the suppression caused by ejected matter near the stellar surface, a region crucial to the conversion process. We derive analytical expressions for the transition rate; they reveal the influence of key parameters and their uncertainties. We update constraints using historical gamma-ray data from SN~1987A and find $g_{a\gamma}<5\times10^{-12}~{\rm GeV}^{-1}$ for $m_a\lesssim10^{-9}$ meV. We also forecast sensitivities for a future Galactic SN and for NSMs, assuming observations with Fermi-LAT or similar gamma-ray instruments. We distinguish ALPs -- defined as coupling only to photons and produced via Primakoff scattering -- from axions, which also couple to nucleons and emerge through nuclear bremsstrahlung. We omit pionic axion production due to its large uncertainties and inconsistencies, though it could contribute comparably to bremsstrahlung under optimistic assumptions. For the compact sources, we adopt time-averaged one-zone models, guided by numerical simulations, to enable clear and reproducible parametric studies. |
| 2025-09-16 | [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](http://arxiv.org/abs/2509.13313v1) | Xixi Wu, Kuan Li et al. | Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents. |
| 2025-09-16 | [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](http://arxiv.org/abs/2509.13305v1) | Kuan Li, Zhongwang Zhang et al. | Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap. |
| 2025-09-16 | [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](http://arxiv.org/abs/2509.13262v1) | Zhizhong Zhao, Ke Chen | Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet existing methods are either computationally intensive, such as Bayesian or ensemble methods, or provide only partial, task-specific estimates, such as single-forward-pass techniques. In this paper, we propose a post-hoc single-forward-pass framework that jointly captures aleatoric and epistemic uncertainty without modifying or retraining pretrained models. Our method applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs) on each side. We prove that, under ideal conditions, the total MAR equals the harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency Discrepancy Score} (SDS) for fine-grained epistemic estimation across regression and classification. For regression, side-specific quantile regression yields prediction intervals with improved empirical coverage, which are further calibrated via SDS. For classification, when calibration data are available, we apply SPA-based calibration identities to adjust the softmax outputs and then compute predictive entropy on these calibrated probabilities. Extensive experiments on diverse regression and classification benchmarks demonstrate that our framework matches or exceeds several state-of-the-art UQ methods while incurring minimal overhead.   Our source code is available at https://github.com/zzz0527/SPC-UQ. |
| 2025-09-16 | [SURGIN: SURrogate-guided Generative INversion for subsurface multiphase flow with quantified uncertainty](http://arxiv.org/abs/2509.13189v1) | Zhao Feng, Bicheng Yan et al. | We present a direct inverse modeling method named SURGIN, a SURrogate-guided Generative INversion framework tailed for subsurface multiphase flow data assimilation. Unlike existing inversion methods that require adaptation for each new observational configuration, SURGIN features a zero-shot conditional generation capability, enabling real-time assimilation of unseen monitoring data without task-specific retraining. Specifically, SURGIN synergistically integrates a U-Net enhanced Fourier Neural Operator (U-FNO) surrogate with a score-based generative model (SGM), framing the conditional generation as a surrogate prediction-guidance process in a Bayesian perspective. Instead of directly learning the conditional generation of geological parameters, an unconditional SGM is first pretrained in a self-supervised manner to capture the geological prior, after which posterior sampling is performed by leveraging a differentiable U-FNO surrogate to enable efficient forward evaluations conditioned on unseen observations. Extensive numerical experiments demonstrate SURGIN's capability to decently infer heterogeneous geological fields and predict spatiotemporal flow dynamics with quantified uncertainty across diverse measurement settings. By unifying generative learning with surrogate-guided Bayesian inference, SURGIN establishes a new paradigm for inverse modeling and uncertainty quantification in parametric functional spaces. |
| 2025-09-16 | [Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments](http://arxiv.org/abs/2509.13176v1) | Qiushi Bu, Wen Su et al. | We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility. |
| 2025-09-16 | [Evaluation of Objective Image Quality Metrics for High-Fidelity Image Compression](http://arxiv.org/abs/2509.13150v1) | Shima Mohammadi, Mohsen Jenadeleh et al. | Nowadays, image compression solutions are increasingly designed to operate within high-fidelity quality ranges, where preserving even the most subtle details of the original image is essential. In this context, the ability to detect and quantify subtle compression artifacts becomes critically important, as even slight degradations can impact perceptual quality in professional or quality sensitive applications, such as digital archiving, professional editing and web delivery. However, the performance of current objective image quality assessment metrics in this range has not been thoroughly investigated. In particular, it is not well understood how reliably these metrics estimate distortions at or below the threshold of Just Noticeable Difference (JND). This study directly addresses this issue by proposing evaluation methodologies for assessing the performance of objective quality metrics and performing a comprehensive evaluation using the JPEG AIC-3 dataset which is designed for high-fidelity image compression. Beyond conventional criteria, the study introduces Z-RMSE to incorporate subjective score uncertainty and applies novel statistical tests to assess significant differences between metrics. The analysis spans the full JPEG AIC-3 range and its high- and medium-fidelity subsets, examines the impact of cropping in subjective tests, and a public dataset with benchmarks and evaluation tools is released to support further research. |
| 2025-09-16 | [An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios](http://arxiv.org/abs/2509.13132v1) | Zhihao Zhang, Chengyang Peng et al. | Autonomous driving in dense, dynamic environments requires decision-making systems that can exploit both spatial structure and long-horizon temporal dependencies while remaining robust to uncertainty. This work presents a novel framework that integrates multi-channel bird's-eye-view occupancy grids with transformer-based sequence modeling for tactical driving in complex roundabout scenarios. To address the imbalance between frequent low-risk states and rare safety-critical decisions, we propose the Uncertainty-Weighted Decision Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate per-token predictive entropy, which is then used as a weight in the student model's loss function. This mechanism amplifies learning from uncertain, high-impact states while maintaining stability across common low-risk transitions. Experiments in a roundabout simulator, across varying traffic densities, show that UWDT consistently outperforms other baselines in terms of reward, collision rate, and behavioral stability. The results demonstrate that uncertainty-aware, spatial-temporal transformers can deliver safer and more efficient decision-making for autonomous driving in complex traffic environments. |
| 2025-09-16 | [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](http://arxiv.org/abs/2509.13084v1) | Yunyao Lu, Yihang Wu et al. | Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments. |
| 2025-09-16 | [Physics potential of the IceCube Upgrade for atmospheric neutrino oscillations](http://arxiv.org/abs/2509.13066v1) | IceCube Collaboration | The IceCube Upgrade is an extension of the existing IceCube Neutrino Observatory and will be deployed in the 2025-2026 austral summer. It will significantly improve the sensitivity of the detector to atmospheric neutrino oscillations. The existing 86-string IceCube array contains a dense in-fill known as DeepCore which is optimized to measure neutrinos with energies down to a few GeV. The IceCube Upgrade will consist of seven new densely-instrumented strings placed within the DeepCore volume to further enhance the performance in the GeV energy range. The additional strings will feature new optical modules, each containing multiple PMTs, in contrast to the existing modules that each contain a single PMT. This will more than triple the number of PMT channels with respect to the current IceCube configuration, allowing for improved detection efficiency and reconstruction performance at GeV energies. We describe necessary updates to simulation, event selection, and reconstruction to accommodate the higher data rates observed by the upgraded detector and the addition of multi-PMT modules. We determine the expected sensitivity of the IceCube Upgrade to the atmospheric neutrino oscillation parameters sin$^2\theta_{23}$ and $\Delta m^2_{32}$, the appearance of tau neutrinos and the neutrino mass ordering. The IceCube Upgrade will provide neutrino oscillation measurements that are of similar precision to those from accelerator experiments, while providing complementarity by probing higher energies and longer baselines, and with different sources of systematic uncertainties. |
| 2025-09-15 | [Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks](http://arxiv.org/abs/2509.12199v1) | Asif Iqbal, Subhabrata Majumdar et al. | Precise determination of galaxy cluster masses is crucial for establishing reliable mass-observable scaling relations in cluster cosmology. We employ graph neural networks (GNNs) to estimate galaxy cluster masses from radially sampled profiles of the intra-cluster medium (ICM) inferred from X-ray observations. GNNs naturally handle inputs of variable length and resolution by representing each ICM profile as a graph, enabling accurate and flexible modeling across diverse observational conditions. We trained and tested GNN model using state-of-the-art hydrodynamical simulations of galaxy clusters from The Three Hundred Project. The mass estimates using our method exhibit no systematic bias compared to the true cluster masses in the simulations. Additionally, we achieve a scatter in recovered mass versus true mass of about 6\%, which is a factor of six smaller than obtained from a standard hydrostatic equilibrium approach. Our algorithm is robust to both data quality and cluster morphology and it is capable of incorporating model uncertainties alongside observational uncertainties. Finally, we apply our technique to XMM-Newton observed galaxy cluster samples and compare the GNN derived mass estimates with those obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our results provide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZ derived masses, with higher mass clusters exhibiting a greater degree of deviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$, albeit with significant dispersion due to its mass dependence. This work takes a significant step towards establishing unbiased observable mass scaling relations by integrating X-ray, SZ and optical datasets using deep learning techniques, thereby enhancing the role of galaxy clusters in precision cosmology. |
| 2025-09-15 | [Approaches to Analysis and Design of AI-Based Autonomous Vehicles](http://arxiv.org/abs/2509.12169v1) | Tao Yan, Zheyu Zhang et al. | Artificial intelligence (AI) models are becoming key components in an autonomous vehicle (AV), especially in handling complicated perception tasks. However, closing the loop through AI-based feedback may pose significant risks on reliability of autonomous driving due to very limited understanding about the mechanism of AI-driven perception processes. To overcome it, this paper aims to develop tools for modeling, analysis, and synthesis for a class of AI-based AV; in particular, their closed-loop properties, e.g., stability, robustness, and performance, are rigorously studied in the statistical sense. First, we provide a novel modeling means for the AI-driven perception processes by looking at their error characteristics. Specifically, three fundamental AI-induced perception uncertainties are recognized and modeled by Markov chains, Gaussian processes, and bounded disturbances, respectively. By means of that, the closed-loop stochastic stability (SS) is established in the sense of mean square, and then, an SS control synthesis method is presented within the framework of linear matrix inequalities (LMIs). Besides the SS properties, the robustness and performance of AI-based AVs are discussed in terms of a stochastic guaranteed cost, and criteria are given to test the robustness level of an AV when in the presence of AI-induced uncertainties. Furthermore, the stochastic optimal guaranteed cost control is investigated, and an efficient design procedure is developed innovatively based on LMI techniques and convex optimization. Finally, to illustrate the effectiveness, the developed results are applied to an example of car following control, along with extensive simulation. |
| 2025-09-15 | [Superresolving Non-linear PDE Dynamics with Reduced-Order Autodifferentiable Ensemble Kalman Filtering For Turbulence Modeling and Flow Regulation](http://arxiv.org/abs/2509.12149v1) | Mrigank Dhingra, Omer San | Accurately reconstructing and forecasting high-resolution (HR) states from computationally cheap low-resolution (LR) observations is central to estimation-and-control of spatio-temporal PDE systems. We develop a unified superresolution pipeline based on the reduced-order autodifferentiable Ensemble Kalman filter (ROAD-EnKF). The method learns a low-dimensional latent dynamics model and a nonlinear decoder from latent variables to HR fields; the learned pair is embedded in an EnKF, enabling simultaneous state estimation and control-oriented forecasting with quantified uncertainty. We evaluate on three benchmarks: 1-D viscous Burgers equation (shock formation), Kuramoto-Sivashinsky (KS) equation (chaotic dynamics), and 2-D Navier-Stokes-Kraichnan turbulence (NSKT) (vortex decaying dynamics at Re 16,000). LR data are obtained by factors of 4-8 downsampling per spatial dimension and are corrupted with noise. On Burgers and KS, the latent models remain stable far beyond the observation window, accurately predicting shock propagation and chaotic attractor statistics up to 150 steps. On 2-D NSKT, the approach preserves the kinetic-energy spectrum and enstrophy budget of the HR data, indicating suitability for control scenarios that depend on fine-scale flow features. These results position ROAD-EnKF as a principled and efficient framework for physics-constrained superresolution, bridging LR sensing and HR actuation across diverse PDE regimes. |
| 2025-09-15 | [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](http://arxiv.org/abs/2509.12094v1) | Tianqi Zhao, Russa Biswas et al. | Graph machine learning models often achieve similar overall performance yet behave differently at the node level, failing on different subsets of nodes with varying reliability. Standard evaluation metrics such as accuracy obscure these fine grained differences, making it difficult to diagnose when and where models fail. We introduce NodePro, a node profiling framework that enables fine-grained diagnosis of model behavior by assigning interpretable profile scores to individual nodes. These scores combine data-centric signals, such as feature dissimilarity, label uncertainty, and structural ambiguity, with model-centric measures of prediction confidence and consistency during training. By aligning model behavior with these profiles, NodePro reveals systematic differences between models, even when aggregate metrics are indistinguishable. We show that node profiles generalize to unseen nodes, supporting prediction reliability without ground-truth labels. Finally, we demonstrate the utility of NodePro in identifying semantically inconsistent or corrupted nodes in a structured knowledge graph, illustrating its effectiveness in real-world settings. |
| 2025-09-15 | [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](http://arxiv.org/abs/2509.12043v1) | Mayur Patil, Qadeer Ahmed et al. | Traffic flow forecasting is essential for managing congestion, improving safety, and optimizing various transportation systems. However, it remains a prevailing challenge due to the stochastic nature of urban traffic and environmental factors. Better predictions require models capable of accommodating the traffic variability influenced by multiple dynamic and complex interdependent factors. In this work, we propose a Graph Neural Network (GNN) framework to address the stochasticity by leveraging adaptive adjacency matrices using log-normal distributions and Coefficient of Variation (CV) values to reflect real-world travel time variability. Additionally, weather factors such as temperature, wind speed, and precipitation adjust edge weights and enable GNN to capture evolving spatio-temporal dependencies across traffic stations. This enhancement over the static adjacency matrix allows the model to adapt effectively to traffic stochasticity and changing environmental conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP) framework to provide reliable uncertainty quantification, achieving target coverage while maintaining acceptable prediction intervals. Experimental results demonstrate that the proposed model, in comparison with baseline methods, showed better prediction accuracy and uncertainty bounds. We, then, validate this method by constructing traffic scenarios in SUMO and applying Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under Test (VUT) to reflect real-world variability. The simulated mean travel time of the VUT falls within the intervals defined by INRIX historical data, verifying the model's robustness. |
| 2025-09-15 | [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](http://arxiv.org/abs/2509.12034v1) | Emmanuel Adjei Domfeh, Christopher L. Dancy | In high-stakes disaster scenarios, timely and informed decision-making is critical yet often challenged by uncertainty, dynamic environments, and limited resources. This paper presents a systematic review of Human-AI collaboration patterns that support decision-making across all disaster management phases. Drawing from 51 peer-reviewed studies, we identify four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Within these, we analyze sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. Our review highlights how AI systems may enhance situational awareness, improves response efficiency, and support complex decision-making, while also surfacing critical limitations in scalability, interpretability, and system interoperability. We conclude by outlining key challenges and future research directions, emphasizing the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and equitable recovery outcomes. |
| 2025-09-15 | [Learning from Uncertain Similarity and Unlabeled Data](http://arxiv.org/abs/2509.11984v1) | Meng Wei, Zhongnian Li et al. | Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches. |
| 2025-09-15 | [Probabilistic modelling of atmosphere-surface coupling with a copula Bayesian network](http://arxiv.org/abs/2509.11975v1) | Laura Mack, Marvin KÃ¤hnert et al. | Land-atmosphere coupling is an important process for correctly modelling near-surface temperature profiles, but it involves various uncertainties due to subgrid-scale processes, such as turbulent fluxes or unresolved surface heterogeneities, suggesting a probabilistic modelling approach. We develop a copula Bayesian network (CBN) to interpolate temperature profiles, acting as alternative to T2m-diagnostics used in numerical weather prediction (NWP) systems. The new CBN results in (1) a reduction of the warm bias inherent to NWP predictions of wintertime stable boundary layers allowing cold temperature extremes to be better represented, and (2) consideration of uncertainty associated with subgrid-scale spatial variability. The use of CBNs combines the advantages of uncertainty propagation inherent to Bayesian networks with the ability to model complex dependence structures between random variables through copulas. By combining insights from copula modelling and information entropy, criteria for the applicability of CBNs in the further development of parameterizations in NWP models are derived. |
| 2025-09-15 | [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](http://arxiv.org/abs/2509.11966v1) | Sangjoon Park, Yeonjong Shin et al. | Poroelasticity -- coupled fluid flow and elastic deformation in porous media -- often involves spatially variable permeability, especially in subsurface systems. In such cases, simulations with random permeability fields are widely used for probabilistic analysis, uncertainty quantification, and inverse problems. These simulations require repeated forward solves that are often prohibitively expensive, motivating the development of efficient surrogate models. However, efficient surrogate modeling techniques for poroelasticity with random permeability fields remain scarce. In this study, we propose a surrogate modeling framework based on the deep operator network (DeepONet), a neural architecture designed to learn mappings between infinite-dimensional function spaces. The proposed surrogate model approximates the solution operator that maps random permeability fields to transient poroelastic responses. To enhance predictive accuracy and stability, we integrate three strategies: nondimensionalization of the governing equations, input dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step training procedure that decouples the optimization of branch and trunk networks. The methodology is evaluated on two benchmark problems in poroelasticity: soil consolidation and ground subsidence induced by groundwater extraction. In both cases, the DeepONet achieves substantial speedup in inference while maintaining high predictive accuracy across a wide range of permeability statistics. These results highlight the potential of the proposed approach as a scalable and efficient surrogate modeling technique for poroelastic systems with random permeability fields. |
| 2025-09-15 | [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](http://arxiv.org/abs/2509.11964v1) | Junyoung Kim, Minsik Jeon et al. | Semantic mapping aims to construct a 3D semantic representation of the environment, providing essential knowledge for robots operating in complex outdoor settings. While Bayesian Kernel Inference (BKI) addresses discontinuities of map inference from sparse sensor data, existing semantic mapping methods suffer from various sources of uncertainties in challenging outdoor environments. To address these issues, we propose an uncertainty-aware semantic mapping framework that handles multiple sources of uncertainties, which significantly degrade mapping performance. Our method estimates uncertainties in semantic predictions using Evidential Deep Learning and incorporates them into BKI for robust semantic inference. It further aggregates noisy observations into coherent Gaussian representations to mitigate the impact of unreliable points, while employing geometry-aligned kernels that adapt to complex scene structures. These Gaussian primitives effectively fuse local geometric and semantic information, enabling robust, uncertainty-aware mapping in complex outdoor scenarios. Comprehensive evaluation across diverse off-road and urban outdoor environments demonstrates consistent improvements in mapping quality, uncertainty calibration, representational flexibility, and robustness, while maintaining real-time efficiency. |
| 2025-09-12 | [The CHARA Array Polarization Model and Prospects for Spectropolarimetry](http://arxiv.org/abs/2509.10451v1) | Linling Shuai, John D. Monnier et al. | Polarimetric data provide key insights into infrared emission mechanisms in the inner disks of YSOs and the details of dust formation around AGB stars. While polarization measurements are well-established in radio interferometry, they remain challenging at visible and near-infrared due to the significant time-variable birefringence introduced by the complex optical beamtrain. In this study, we characterize instrumental polarization effects within the optical path of the CHARA Array, focusing on the H-band MIRC-X and K-band MYSTIC beam combiners. Using Jones matrix formalism, we developed a comprehensive model describing diattenuation and retardance across the array. By applying this model to an unpolarized calibrator, we derived the instrumental parameters for both MIRC-X and MYSTIC. Our results show differential diattenuation consistent with >= 97% reflectivity per aluminum-coated surface at 45 deg incidence. The differential retardance exhibits small wavelength-dependent variations, in some cases larger than we expected. Notably, telescope W2 exhibits a significantly larger phase shift in the Coude path, attributable to a fixed aluminum mirror (M4) used in place of deformable mirrors present on the other telescopes during the observing run. We also identify misalignments in the LiNbO_3 birefringent compensator plates on S1 (MIRC-X) and W2 (MYSTIC). After correcting for night-to-night offsets, we achieve calibration accuracies of $\pm$ 3.4% in visibility ratio and $\pm$ 1.4 deg in differential phase for MIRC-X, and $\pm$ 5.9% and $\pm$ 2.4 deg, respectively, for MYSTIC. Given that the differential intrinsic polarization of spatially resolved sources, such as AGB stars and YSOs, typically greater than these instrumental uncertainties, our results demonstrate that CHARA is now capable of achieving high-accuracy measurements of intrinsic polarization in astrophysical targets. |
| 2025-09-12 | [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](http://arxiv.org/abs/2509.10419v1) | Francesco Vitale, Tommaso Zoppi et al. | Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability. |
| 2025-09-12 | [A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives](http://arxiv.org/abs/2509.10393v1) | ClÃ©mentine Chazal, Heishiro Kanagawa et al. | Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called 'gradient discrepancy', and in particular a 'kernel gradient discrepancy' (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel charasterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained. To illustrate this point several novel algorithms are proposed, including a natural generalisation of Stein variational gradient descent, with applications to mean-field neural networks and prediction-centric uncertainty quantification presented. On the theoretical side, our principal contribution is to establish sufficient conditions for desirable properties of KGD, such as continuity and convergence control. |
| 2025-09-12 | [Vendi Information Gain for Active Learning and its Application to Ecology](http://arxiv.org/abs/2509.10390v1) | Quan Nguyen, Adji Bousso Dieng | While monitoring biodiversity through camera traps has become an important endeavor for ecological research, identifying species in the captured image data remains a major bottleneck due to limited labeling resources. Active learning -- a machine learning paradigm that selects the most informative data to label and train a predictive model -- offers a promising solution, but typically focuses on uncertainty in the individual predictions without considering uncertainty across the entire dataset. We introduce a new active learning policy, Vendi information gain (VIG), that selects images based on their impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG achieves impressive predictive accuracy close to full supervision using less than 10% of the labels. It consistently outperforms standard baselines across metrics and batch sizes, collecting more diverse data in the feature space. VIG has broad applicability beyond ecology, and our results highlight its value for biodiversity monitoring in data-limited environments. |
| 2025-09-12 | [Theory uncertainties of the irreducible background to VBF Higgs production](http://arxiv.org/abs/2509.10368v1) | Xuan Chen, Silvia Ferrario Ravasio et al. | Higgs boson production through gluon fusion in association with two jets is an irreducible background to Higgs boson production through vector boson fusion, one of the most important channels for analyzing and understanding the Higgs boson properties at the Large Hadron Collider. Despite a range of available simulation tools, precise predictions for the corresponding final states are notoriously hard to achieve. Using state-of-the-art fixed-order calculations as the baseline for a comparison, we perform a detailed study of similarities and differences in existing event generators. We provide consistent setups for the simulations that can be used to obtain identical parametric precision in various programs used by experiments. We find that NLO calculations for the two-jet final state are essential to achieve reliable predictions. |
| 2025-09-12 | [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](http://arxiv.org/abs/2509.10348v1) | Yehudit Aperstein, Amit Tzahar et al. | Overconfidence in deep learning models poses a significant risk in high-stakes medical imaging tasks, particularly in multi-label classification of chest X-rays, where multiple co-occurring pathologies must be detected simultaneously. This study introduces an uncertainty-aware framework for chest X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection. Both methods enable the model to abstain from uncertain predictions, improving reliability by deferring ambiguous cases to clinical experts. A quantile-based calibration procedure is employed to tune rejection thresholds using either global or class-specific strategies. Experiments conducted on three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR) demonstrate that selective rejection improves the trade-off between diagnostic accuracy and coverage, with entropy-based rejection yielding the highest average AUC across all pathologies. These results support the integration of selective prediction into AI-assisted diagnostic workflows, providing a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings. |
| 2025-09-12 | [OpenCSP: A Deep Learning Framework for Crystal Structure Prediction from Ambient to High Pressure](http://arxiv.org/abs/2509.10293v1) | Yinan Wang, Xiaoyang Wang et al. | High-pressure crystal structure prediction (CSP) underpins advances in condensed matter physics, planetary science, and materials discovery. Yet, most large atomistic models are trained on near-ambient, equilibrium data, leading to degraded stress accuracy at tens to hundreds of gigapascals and sparse coverage of pressure-stabilized stoichiometries and dense coordination motifs. Here, we introduce OpenCSP, a machine learning framework for CSP tasks spanning ambient to high-pressure conditions. This framework comprises an open-source pressure-resolved dataset alongside a suite of publicly available atomistic models that are jointly optimized for accuracy in energy, force, and stress predictions. The dataset is constructed via randomized high-pressure sampling and iteratively refined through an uncertainty-guided concurrent learning strategy, which enriches underrepresented compression regimes while suppressing redundant DFT labeling. Despite employing a training corpus one to two orders of magnitude smaller than those of leading large models, OpenCSP achieves comparable or superior performance in high-pressure enthalpy ranking and stability prediction. Across benchmark CSP tasks spanning a wide pressure window, our models match or surpass MACE-MPA-0, MatterSim v1 5M, and GRACE-2L-OAM, with the largest gains observed at elevated pressures. These results demonstrate that targeted, pressure-aware data acquisition coupled with scalable architectures enables data-efficient, high-fidelity CSP, paving the way for autonomous materials discovery under ambient and extreme conditions. |
| 2025-09-12 | [Astroclimes -- measuring the abundance of CO$_2$ and CH$_4$ in the Earth's atmosphere using astronomical observations](http://arxiv.org/abs/2509.10258v1) | Marcelo Aron Fetzner Keniger, David Armstrong et al. | Monitoring the abundance of greenhouse gases (GHGs) such as carbon dioxide (CO$_2$) and methane (CH$_4$) is necessary to quantify their impact on global warming and climate change. Although a number of satellites and ground-based networks measure the total column volume mixing ratio (VMR) of these gases, they rely on sunlight, and column measurements at night are comparatively scarce. We present a new algorithm, Astroclimes, that hopes to complement and extend nighttime CO$_2$ and CH4 column measurements. Astroclimes can measure the abundance of GHGs on Earth by generating a model telluric transmission spectra and fitting it to the spectra of telluric standard stars in the near-infrared taken by ground-based telescopes. A Markov Chain Monte Carlo (MCMC) analysis on an extensive dataset from the CARMENES spectrograph showed that Astroclimes was able to recover the long term trend known to be present in the molecular abundances of both CO$_2$ and CH$_4$, but not their seasonal cycles. Using the Copernicus Atmosphere Monitoring Service (CAMS) global greenhouse gas reanalysis model (EGG4) as a benchmark, we identified an overall vertical shift in our data and quantified the long term scatter in our retrievals. The scatter on a 1 hour timescale, however, is much lower, and is on par with the uncertainties on individual measurements. Although currently the precision of the method is not in line with state of the art techniques using dedicated instrumentation, it shows promise for further development. |
| 2025-09-12 | [Learning Constraint Surrogate Model for Two-stage Stochastic Unit Commitment](http://arxiv.org/abs/2509.10246v1) | Amir Bahador Javadi, Amin Kargarian et al. | The increasing penetration of renewable energy sources introduces significant uncertainty in power system operations, making traditional deterministic unit commitment approaches computationally expensive. This paper presents a machine learning surrogate modeling approach designed to reformulate the feasible design space of the two-stage stochastic unit commitment (TSUC) problem, reducing its computational complexity. The proposed method uses a support vector machine (SVM) to construct a surrogate model based on the governing equations of the learner. This model replaces the original 2|L| * |S| transmission line flow constraints, where |S| is the number of uncertainty scenarios and |L| is the number of transmission lines with |S| much less than |L|, with a significantly reduced set of 1 * |S| linear inequality constraints. The approach is theoretically grounded in the polyhedral structure of the feasible region under the DC power flow approximation, enabling the transformation of 2|L| line flow limit constraints into a single linear constraint. The surrogate model is trained using data generated from computationally efficient DC optimal power flow simulations. Simulation results on the IEEE 57-bus and 118-bus systems demonstrate SVM halfspace constraint accuracy of 99.72% and 99.88%, respectively, with TSUC computational time reductions of 46% and 31% and negligible generation cost increases (0.63% and 0.88% on average for IEEE 57- and 118-bus systems, respectively). This shows the effectiveness of the proposed approach for practical power system operations under renewable energy uncertainty. |
| 2025-09-12 | [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](http://arxiv.org/abs/2509.10227v1) | Ãngel LadrÃ³n, Miguel SÃ¡nchez-DomÃ­nguez et al. | Fatigue life prediction is essential in both the design and operational phases of any aircraft, and in this sense safety in the aerospace industry requires early detection of fatigue cracks to prevent in-flight failures. Robust and precise fatigue life predictors are thus essential to ensure safety. Traditional engineering methods, while reliable, are time consuming and involve complex workflows, including steps such as conducting several Finite Element Method (FEM) simulations, deriving the expected loading spectrum, and applying cycle counting techniques like peak-valley or rainflow counting. These steps often require collaboration between multiple teams and tools, added to the computational time and effort required to achieve fatigue life predictions. Machine learning (ML) offers a promising complement to traditional fatigue life estimation methods, enabling faster iterations and generalization, providing quick estimates that guide decisions alongside conventional simulations.   In this paper, we present a ML-based pipeline that aims to estimate the fatigue life of different aircraft wing locations given the flight parameters of the different missions that the aircraft will be operating throughout its operational life. We validate the pipeline in a realistic use case of fatigue life estimation, yielding accurate predictions alongside a thorough statistical validation and uncertainty quantification. Our pipeline constitutes a complement to traditional methodologies by reducing the amount of costly simulations and, thereby, lowering the required computational and human resources. |
| 2025-09-11 | [Cosmology inference with perturbative forward modeling at the field level: a comparison with joint power spectrum and bispectrum analyses](http://arxiv.org/abs/2509.09673v1) | Kazuyuki Akitsu, Marko SimonoviÄ‡ et al. | We extend field-level inference to jointly constrain the cosmological parameters $\{A,\omega_{\rm cdm},H_0\}$, in both real and redshift space. Our analyses are based on mock data generated using a perturbative forward model, with noise drawn from a Gaussian distribution with a constant power spectrum. This idealized setting, where the field-level likelihood is exactly Gaussian, allows us to precisely quantify the information content in the nonlinear field on large scales. We find that field-level inference accurately recovers all cosmological parameters in both real and redshift space, with uncertainties consistent with perturbation theory expectations. We show that these error bars are comparable to those obtained from a joint power spectrum and bispectrum analysis using the same perturbative model. Finally, we perform several tests using the Gaussian field-level likelihood to fit the mock data where the true noise model is non-Gaussian, and find significant biases in the inferred cosmological parameters. These results highlight that the success of field-level inference critically depends on using the correct likelihood, which may be the primary challenge for applying this method to smaller scales even in the perturbative regime. |
| 2025-09-11 | [Measuring Epistemic Humility in Multimodal Large Language Models](http://arxiv.org/abs/2509.09658v1) | Bingkui Tong, Jiaer Xia et al. | Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a "None of the above" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench. |
| 2025-09-11 | [Reconstructing the origin of black hole mergers using sparse astrophysical models](http://arxiv.org/abs/2509.09647v1) | V. Gayathri, Giuliano Iorio et al. | The astrophysical origin of binary black hole mergers discovered by LIGO and Virgo remains uncertain. Efforts to reconstruct the processes that lead to mergers typically rely on either astrophysical models with fixed parameters, or continuous analytical models that can be fit to observations. Given the complexity of astrophysical formation mechanisms, these methods typically cannot fully take into account model uncertainties, nor can they fully capture the underlying processes. Here, we present a merger population analysis that can take a discrete set of simulated model distributions as its input to interpret observations. The analysis can take into account multiple formation scenarios as fractional contributors to the total set of observations, and can naturally account for model uncertainties. We apply this technique to investigate the origin of black hole mergers observed by LIGO Virgo. Specifically, we consider a model of AGN assisted black hole merger distributions, exploring a range of AGN parameters along with several {{SEVN}} population synthesis models that vary in common envelope efficiency parameter ($\alpha$) and metallicity ($Z$). We estimate the posterior distributions for AGN+SEVN models using $87$ BBH detections from the $O1--O3$ observation runs. The inferred total merger rate is $46.2 {Gpc}^{-3} {yr}^{-1}$, with the AGN sub-population contributing $21.2{Gpc}^{-3}{yr}^{-1}$ and the SEVN sub-population contributing $25.0 {Gpc}^{-3} {yr}^{-1}$. |
| 2025-09-11 | [Constraints on Ultra-heavy DM from TeV-PeV gamma-ray diffuse measurements](http://arxiv.org/abs/2509.09609v1) | Manuel Rocamora, Pedro De La Torre Luque et al. | Recent experiments have measured the Galactic $\gamma$-ray diffuse emission up to PeV energies, opening a window to study acceleration of Galactic cosmic rays and their propagation up to the cosmic-ray knee. Furthermore, these observations provide a powerful tool to set strong constraints into very-heavy dark matter particles, with masses in the TeV-PeV range. In this paper, we explore the potential of the newest observations of diffuse emissions at the Galactic plane from HAWC and LHAASO to probe this kind of dark matter over a wide mass range. Here, we model secondary emissions (inverse-Compton) from the electrons and positrons produced in the annihilation/decay of dark matter, on top of their prompt $\gamma$-ray emission, including the effects of absorption of high-energy photons via pair production. Furthermore, we show that including the astrophysical backgrounds (namely diffuse emission from cosmic-ray collisions or emission from unresolved sources) can significantly improve these limits. We find that the new measurements provided, specially by LHAASO with the combination of the WCDA and KM2A detectors, allow us to set strong constraints in decaying dark matter, being competitive and even improving the strongest constraints at the moment. We also highlight that these regions lead to constraints that are less affected by uncertainties from the dark matter distribution and discuss how CTA north and SWGO will be able to improve limits in this mass range. |
| 2025-09-11 | [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](http://arxiv.org/abs/2509.09599v1) | Ira J. S. Shokar, Rich R. Kerswell et al. | We present a deep learning emulator for stochastic and chaotic spatio-temporal systems, explicitly conditioned on the parameter values of the underlying partial differential equations (PDEs). Our approach involves pre-training the model on a single parameter domain, followed by fine-tuning on a smaller, yet diverse dataset, enabling generalisation across a broad range of parameter values. By incorporating local attention mechanisms, the network is capable of handling varying domain sizes and resolutions. This enables computationally efficient pre-training on smaller domains while requiring only a small additional dataset to learn how to generalise to larger domain sizes. We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, showcasing its ability to capture phenomena at interpolated parameter values. The emulator provides significant computational speed-ups over conventional numerical integration, facilitating efficient exploration of parameter space, while a probabilistic variant of the emulator provides uncertainty quantification, allowing for the statistical study of rare events. |
| 2025-09-11 | [ObjectReact: Learning Object-Relative Control for Visual Navigation](http://arxiv.org/abs/2509.09594v1) | Sourav Garg, Dustin Craggs et al. | Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/ |
| 2025-09-11 | [Unified Framework for Hybrid Aleatory and Epistemic Uncertainty Propagation via Decoupled Multi-Probability Density Evolution Method](http://arxiv.org/abs/2509.09535v1) | Yi Luo, Meng-Ze Lyu et al. | This paper presents a unified framework for uncertainty propagation in dynamical systems involving hybrid aleatory and epistemic uncertainties. The framework accommodates precise probabilistic, imprecise probabilistic, and non-probabilistic representations, including the distribution-free probability-box (p-box). A central aspect of the framework involves transforming the original uncertainty inputs into an augmented random space, yielding the primary challenge of determining the conditional probability density function (PDF) of the response quantity of interest given epistemic uncertainty parameters. The recently proposed decoupled multi-probability density evolution method (decoupled M-PDEM) is employed to numerically solve the conditional PDF for complex dynamical systems. Several numerical examples illustrate the applicability, efficiency, and accuracy of the proposed framework. These include a linear single-degree-of-freedom (SDOF) system subject to Gaussian white noise with its natural frequency modeled as a p-box, a 10-DOF hysteretic structure subject to imprecise seismic loads, and a crash box model with mixed random and interval system parameters. |
| 2025-09-11 | [Mapping of discrete range modulated proton radiograph to water-equivalent path length using machine learning](http://arxiv.org/abs/2509.09514v1) | Atiq Ur Rahman, Chun-Chieh Wang et al. | Objective. Proton beams enable localized dose delivery. Accurate range estimation is essential, but planning still relies on X-ray CT, which introduces uncertainty in stopping power and range. Proton CT measures water equivalent thickness directly but suffers resolution loss from multiple Coulomb scattering. We develop a data driven method that reconstructs water equivalent path length (WEPL) maps from energy resolved proton radiographs, bypassing intermediate reconstructions. Approach. We present a machine learning pipeline for WEPL from high dimensional radiographs. Data were generated with the TOPAS Monte Carlo toolkit, modeling a clinical nozzle and a patient CT. Proton energies spanned 70-230 MeV across 72 projection angles. Principal component analysis reduced input dimensionality while preserving signal. A conditional GAN with gradient penalty was trained for WEPL prediction using a composite loss (adversarial, MSE, SSIM, perceptual) to balance sharpness, accuracy, and stability. Main results. The model reached a mean relative WEPL deviation of 2.5 percent, an SSIM of 0.97, and a proton radiography gamma index passing rate of 97.1 percent (2 percent delta WEPL, 3 mm distance-to-agreement) on a simulated head phantom. Results indicate high spatial fidelity and strong structural agreement. Significance. WEPL can be mapped directly from proton radiographs with deep learning while avoiding intermediate steps. The method mitigates limits of analytic techniques and may improve treatment planning. Future work will tune the number of PCA components, include detector response, explore low dose settings, and extend multi angle data toward full proton CT reconstruction; it is compatible with clinical workflows. |
| 2025-09-11 | [Spin Constraints on 4U 1630-47 via combined Continuum Fitting and Reflection methods: a comparative study using Frequentist and Bayesian statistics](http://arxiv.org/abs/2509.09481v1) | Debtroy Das, Honghui Liu et al. | We present a comprehensive Bayesian spectral analysis of the black hole X-ray binary 4U 1630-47 during its 2022 outburst, using simultaneous \textit{NICER} and \textit{NuSTAR} observations. Using the traditional frequentist approach, we build our model combining reflection spectroscopy with continuum fitting techniques and analyse the data. In the Bayesian framework, we jointly constrain the black hole's spin, mass, inclination, and distance within a unified framework. Employing nested sampling, we capture parameter degeneracies and rigorously propagate both statistical and systematic uncertainties. Our results yield robust and precise spin measurements from both approaches. Our Bayesian analysis fetches spin $a_*= 0.93_{-0.04}^{+0.05}$, mass $M_{\rm BH} = 9.0_{-2.0}^{+2.0} \, M_\odot$, distance $d_{\rm BH} = 10.5_{-1.2}^{+1.3}$~kpc, and inclination angle $i=53.8_{-1.3}^{+1.3}$~deg. It also demonstrates the power of Bayesian inference in fetching valuable insights into the complex physics of black hole accretion and enabling high-confidence measurements of fundamental parameters. |
| 2025-09-11 | [Dark Vector Boson Bremsstrahlung: New Form Factors for a Broader Class of Models](http://arxiv.org/abs/2509.09437v1) | Felix Kling, Peter Reimitz et al. | We explore the sensitivity of collider experiments to a broad class of GeV-scale dark vector models of new physics via production in proton and neutron bremsstrahlung and initial state radiation. This is achieved using a new physically motivated model for timelike vector form factors with generic charges for both protons and neutrons, which is fit to a variety of timelike and spacelike data with quantified uncertainties. The production model for both proton and neutron bremsstrahlung is applied to re-cast and extend the reach of existing FASER data to GeV-mass dark photons, $U(1)_B$, $U(1)_{B-L}$, and photophobic vectors, as well as forecasts for millicharged particles at FORMOSA. |
| 2025-09-10 | [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](http://arxiv.org/abs/2509.08777v1) | Eric Slyman, Mehrab Tanjim et al. | Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge's true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation. |
| 2025-09-10 | [FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model](http://arxiv.org/abs/2509.08742v1) | Yanlong Wang, Jian Xu et al. | Financial time series forecasting is both highly significant and challenging. Previous approaches typically standardized time series data before feeding it into forecasting models, but this encoding process inherently leads to a loss of important information. Moreover, past time series models generally require fixed numbers of variables or lookback window lengths, which further limits the scalability of time series forecasting. Besides, the interpretability and the uncertainty in forecasting remain areas requiring further research, as these factors directly impact the reliability and practical value of predictions. To address these issues, we first construct a diverse financial image-text dataset (FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization (UARPO) method to enable the model not only output predictions but also analyze the uncertainty of those predictions. We then proposed FinZero, a multimodal pre-trained model finetuned by UARPO to perform reasoning, prediction, and analytical understanding on the FVLDB financial time series. Extensive experiments validate that FinZero exhibits strong adaptability and scalability. After fine-tuning with UARPO, FinZero achieves an approximate 13.48\% improvement in prediction accuracy over GPT-4o in the high-confidence group, demonstrating the effectiveness of reinforcement learning fine-tuning in multimodal large model, including in financial time series forecasting tasks. |
| 2025-09-10 | [Quantifying model prediction sensitivity to model-form uncertainty](http://arxiv.org/abs/2509.08708v1) | Teresa Portone, Rebekah D. White et al. | Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration). |
| 2025-09-10 | [Delving into the depths of NGC 3783 with XRISM II. Cross-calibration of X-ray instruments used in the large, multi-mission observational campaign](http://arxiv.org/abs/2509.08649v1) | XRISM collaboration | Accurate X-ray spectroscopic measurements are fundamental for deriving basic physical parameters of the most abundant baryon components in the Universe. The plethora of X-ray observatories currently operational enables a panchromatic view of the high-energy emission of celestial sources. However, uncertainties in the energy-dependent calibration of the instrument transfer functions (e.g. the effective area, energy redistribution, or gain) can limit - and historically, did limit - the accuracy of X-ray spectroscopic measurements.   We revised the status of the cross-calibration among the scientific payload on board four operation missions: Chandra, NuSTAR, XMM-Newton, and the recently launched XRISM. XRISM carries the micro-calorimeter Resolve, which yields the best energy resolution at energies above 2 keV. For this purpose, we used the data from a 10-day-long observational campaign targeting the nearby active galactic nucleus NGC 3783, carried out in July 2024.   We present a novel model-independent method for assessing the cross-calibration status that is based on a multi-node spline of the spectra with the highest-resolving power (XRISM/Resolve in our campaign). We also estimated the impact of the intrinsic variability of NGC 3783 on the cross-calibration status due to the different time coverages of participating observatories and performed an empirical reassessment of the Resolve throughput at low energies.   Based on this analysis, we derived a set of energy-dependent correction factors of the observed responses, enabling a statistically robust analysis of the whole spectral dataset. They will be employed in subsequent papers describing the astrophysical results of the campaign. |
| 2025-09-10 | [AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models](http://arxiv.org/abs/2509.08638v1) | Rebecca Martin, Jay Patrikar et al. | Specialized machine learning models, regardless of architecture and training, are susceptible to failures in deployment. With their increasing use in high risk situations, the ability to audit these models by determining their operational design domain (ODD) is crucial in ensuring safety and compliance. However, given the high-dimensional input spaces, this process often requires significant human resources and domain expertise. To alleviate this, we introduce \coolname, an LLM-Agent centric framework for automated generation of semantically relevant test cases to search for failure modes in specialized black-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit a uncertainty-aware failure distribution model on a learned text-embedding manifold by projecting the high-dimension input space to low-dimension text-embedding latent space. The LLM-Agent is tasked with iteratively building the failure landscape by leveraging tools for generating test-cases to probe the model-under-test (MUT) and recording the response. The agent also guides the search using tools to probe uncertainty estimate on the low dimensional manifold. We demonstrate this process in a simple case using models trained with missing digits on the MNIST dataset and in the real world setting of vision-based intruder detection for aerial vehicles. |
| 2025-09-10 | [Isotope shift spectroscopy in mercury vapors: a valid alternative to ytterbium for new physics search](http://arxiv.org/abs/2509.08622v1) | Stefania Gravina, Antonio Castrillo et al. | Precision isotope shift metrology in the deep-UV region has been performed for all bosonic isotopes of mercury with a zero nuclear spin, by using the technique of frequency-comb referenced, wavelength-modulated, saturated absorption spectroscopy. The absolute center frequencies of the 6s$^2$ $^1$S$_0$ $\rightarrow$ 6s6p $^3$P$_1$ intercombination line have been measured with precision in the range of 2.5 - 5.9 10$^{-12}$, in temperature-stabilized mercury vapor samples with natural abundances. Frequency shifts in four isotope pairs have been determined with unprecedented accuracy, the global uncertainty being improved by a factor greater than 20 with respect to the best experimental data of the past literature. Our data set, when combined with previous measurements on the 6s6p $^3$P$_2$$\rightarrow$6s7s $^3$S$_1$ transition at 546 nm, allows us to build a King plot that reveals a nonlinearity with a statistical significance of 4.6$\sigma$. |
| 2025-09-10 | [No-Knowledge Alarms for Misaligned LLMs-as-Judges](http://arxiv.org/abs/2509.08593v1) | AndrÃ©s Corrada-Emmanuel | If we use LLMs as judges to evaluate the complex decisions of other LLMs, who or what monitors the judges? Infinite monitoring chains are inevitable whenever we do not know the ground truth of the decisions by experts and we do not want to trust them. One way to ameliorate our evaluation uncertainty is to exploit the use of logical consistency between disagreeing experts. By observing how LLM judges agree and disagree while grading other LLMs, we can compute the only possible evaluations of their grading ability. For example, if two LLM judges disagree on which tasks a third one completed correctly, they cannot both be 100\% correct in their judgments. This logic can be formalized as a Linear Programming problem in the space of integer response counts for any finite test. We use it here to develop no-knowledge alarms for misaligned LLM judges. The alarms can detect, with no false positives, that at least one member or more of an ensemble of judges are violating a user specified grading ability requirement. |
| 2025-09-10 | [Interpretability as Alignment: Making Internal Understanding a Design Principle](http://arxiv.org/abs/2509.08592v1) | Aadit Sengupta, Pratinav Seth et al. | Large neural models are increasingly deployed in high-stakes settings, raising concerns about whether their behavior reliably aligns with human values. Interpretability provides a route to internal transparency by revealing the computations that drive outputs. We argue that interpretability especially mechanistic approaches should be treated as a design principle for alignment, not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer intuitive but correlational explanations, while mechanistic techniques like circuit tracing or activation patching yield causal insight into internal failures, including deceptive or misaligned reasoning that behavioral methods like RLHF, red teaming, or Constitutional AI may overlook. Despite these advantages, interpretability faces challenges of scalability, epistemic uncertainty, and mismatches between learned representations and human concepts. Our position is that progress on safe and trustworthy AI will depend on making interpretability a first-class objective of AI research and development, ensuring that systems are not only effective but also auditable, transparent, and aligned with human intent. |
| 2025-09-10 | [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](http://arxiv.org/abs/2509.08578v1) | Hong Liu | Timely and robust influenza incidence forecasting is critical for public health decision-making. To address this, we present MAESTRO, a Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves robustness by adaptively fusing multi-modal inputs-including surveillance, web search trends, and meteorological data-and leveraging a comprehensive spectro-temporal architecture. The model first decomposes time series into seasonal and trend components. These are then processed through a hybrid feature enhancement pipeline combining Transformer-based encoders, a Mamba state-space model for long-range dependencies, multi-scale temporal convolutions, and a frequency-domain analysis module. A cross-channel attention mechanism further integrates information across the different data modalities. Finally, a temporal projection head performs sequence-to-sequence forecasting, with an optional estimator to quantify prediction uncertainty. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO shows strong competitive performance, demonstrating a superior model fit and relative accuracy, achieving a state-of-the-art R-square of 0.956. Extensive ablations confirm the significant contributions of both multi-modal fusion and the spectro-temporal components. Our modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens.Our publicly available pipeline presents a powerful, unified framework, demonstrating the critical synergy of advanced spectro-temporal modeling and multi-modal data fusion for robust epidemiological forecasting. |
| 2025-09-10 | [Accelerating first-principles molecular-dynamics thermal conductivity calculations for complex systems](http://arxiv.org/abs/2509.08573v1) | Sandro Wieser, YuJie Cen et al. | Atomistic simulations of heat transport in complex materials are costly and hard to converge. This has led to the development of several noise reduction techniques applicable to equilibrium molecular-dynamics simulations. We analyze the performance of those strategies, taking InAs nanowires as our benchmark due to the diverse structures and complex phonon spectra of these quasi-1D systems. We demonstrate how, for low-thermal-conductivity systems, cepstral analysis can reduce computational demands while still delivering accurate results that do not require discarding arbitrary parts of the dataset. However, issues with this approach are revealed when treating high-thermal-conductivity systems, where the thermal conductivity is significantly underestimated. We discuss alternative methods to be used in that situation, relying on uncertainty propagation from independent simulations. We show that the contributions of the covariance matrix have to be included for a quantitative assessment of the error. The combination of these strategies with machine-learning interatomic potentials (MLIPs) provides an accelerated, robust workflow applicable to a diverse set of systems, as our examples using a highly transferable MACE potential illustrate. |
| 2025-09-09 | [Advanced Weights for IXPE Polarization Analysis](http://arxiv.org/abs/2509.07981v1) | Jack T. Dinsmore, Roger W. Romani | As the Imaging X-ray Polarimetry Explorer (IXPE) measures increasingly faint sources, the need for precise polarimetry extraction becomes paramount. In addition to previously described neural-net (NN) weights, we introduce here point-spread function weights and particle background weights, which can be critical for faint sources. In some cases these can be augmented by time/phase and energy weights. We provide a publicly available analysis tool to incorporate these new weights, validate our method on simulated data, and test it on archival IXPE observations. Together these weights decrease the area of the polarization uncertainty contour by a factor of two and will be essential for background-limited IXPE observations. |
| 2025-09-09 | [Early warning for lensed gravitational wave counterparts from time delays of their host galaxies observed in the optical](http://arxiv.org/abs/2509.07967v1) | Sourabh Magare, Anupreeta More et al. | Gravitational lensing of gravitational waves (GWs) can be leveraged to provide early-warning times of $\mathcal{O}({\rm hours})$ to $\mathcal{O}({\rm days})$ before the merger of Binary Neutron Stars (BNSs) and Neutron Star Black Holes (NSBHs). This in turn could enable electromagnetic (EM) telescopes to capture emissions surrounding the time of the merger. In this work, we assess the practicability of lensing-driven early-warning by analysing optical images of the lensed host galaxy to predict the arrival time of subsequent BNS/NSBH signals following the observation of the first signal. We produce mock lenses with image quality and resolution similar to images taken with the Hubble Space Telescope (HST) and the ground-based Hyper Suprime-Cam (HSC) on the Subaru telescope. We compare the time delay uncertainties between these two cases for typical lensed image configurations and multiplicity. These include doubles and quads, and among quads: the fold, cusp, cross image configurations. We find that time delay uncertainties for doubles are comparable for both HST and HSC mocks. On the other hand, quads tend to provide accurate time-delay predictions (typical relative error $\sim0.1$) with HST. Analysis of a real lens led to a difference in time-delay estimates of $\mathcal{O}(\rm days)$ between the predictions derived from HST and HSC data. Our work therefore strongly advocates the need for high-resolution EM observations of lensed host galaxies to feasibly enable lensing-driven early-warning. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: Redshift Calibration of the MagLim++ Lens Sample](http://arxiv.org/abs/2509.07964v1) | G. Giannini, A. Alarcon et al. | In this work, we derive and calibrate the redshift distribution of the MagLim++ lens galaxy sample used in the Dark Energy Survey Year 6 (DES Y6) 3x2pt cosmology analysis. The 3x2pt analysis combines galaxy clustering from the lens galaxy sample and weak gravitational lensing. The redshift distributions are inferred using the SOMPZ method - a Self-Organizing Map framework that combines deep-field multi-band photometry, wide-field data, and a synthetic source injection (Balrog) catalog. Key improvements over the DES Year 3 (Y3) calibration include a noise-weighted SOM metric, an expanded Balrog catalogue, and an improved scheme for propagating systematic uncertainties, which allows us to generate O($10^8$) redshift realizations that collectively span the dominant sources of uncertainty. These realizations are then combined with independent clustering-redshift measurements via importance sampling. The resulting calibration achieves typical uncertainties on the mean redshift of 1-2%, corresponding to a 20-30% average reduction relative to DES Y3. We compress the $n(z)$ uncertainties into a small number of orthogonal modes for use in cosmological inference. Marginalizing over these modes leads to only a minor degradation in cosmological constraints. This analysis establishes the MagLim++ sample as a robust lens sample for precision cosmology with DES Y6 and provides a scalable framework for future surveys. |
| 2025-09-09 | [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](http://arxiv.org/abs/2509.07961v1) | Valen Tagliabue, Leonard Dung | We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration. |
| 2025-09-09 | [Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation](http://arxiv.org/abs/2509.07957v1) | Shunlei Li, Longsen Gao et al. | Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations. |
| 2025-09-09 | [Dark Energy Survey Year 6 Results: improved mitigation of spatially varying observational systematics with masking](http://arxiv.org/abs/2509.07943v1) | M. RodrÃ­guez-Monroy, N. Weaverdyck et al. | As photometric surveys reach unprecedented statistical precision, systematic uncertainties increasingly dominate large-scale structure probes relying on galaxy number density. Defining the final survey footprint is critical, as it excludes regions affected by artefacts or suboptimal observing conditions. For galaxy clustering, spatially varying observational systematics, such as seeing, are a leading source of bias. Template maps of contaminants are used to derive spatially dependent corrections, but extreme values may fall outside the applicability range of mitigation methods, compromising correction reliability. The complexity and accuracy of systematics modelling depend on footprint conservativeness, with aggressive masking enabling simpler, robust mitigation. We present a unified approach to define the DES Year 6 joint footprint, integrating observational systematics templates and artefact indicators that degrade mitigation performance. This removes extreme values from an initial seed footprint, leading to the final joint footprint. By evaluating the DES Year 6 lens sample MagLim++ plus plus on this footprint, we enhance the Iterative Systematics Decontamination (ISD) method, detecting non-linear systematic contamination and improving correction accuracy. While the mask's impact on clustering is less significant than systematics decontamination, it remains non-negligible, comparable to statistical uncertainties in certain w(theta) scales and redshift bins. Supporting coherent analyses of galaxy clustering and cosmic shear, the final footprint spans 4031.04 deg2, setting the basis for DES Year 6 1x2pt, 2x2pt, and 3x2pt analyses. This work highlights how targeted masking strategies optimise the balance between statistical power and systematic control in Stage-III and -IV surveys. |
| 2025-09-09 | [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](http://arxiv.org/abs/2509.07925v1) | Tuo Wang, Adithya Kulkarni et al. | Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ. |
| 2025-09-09 | [Forecasting dementia incidence](http://arxiv.org/abs/2509.07874v1) | JÃ©rÃ´me R. Simons, Yuntao Chen et al. | This paper estimates the stochastic process of how dementia incidence evolves over time. We proceed in two steps: first, we estimate a time trend for dementia using a multi-state Cox model. The multi-state model addresses problems of both interval censoring arising from infrequent measurement and also measurement error in dementia. Second, we feed the estimated mean and variance of the time trend into a Kalman filter to infer the population level dementia process. Using data from the English Longitudinal Study of Aging (ELSA), we find that dementia incidence is no longer declining in England. Furthermore, our forecast is that future incidence remains constant, although there is considerable uncertainty in this forecast. Our two-step estimation procedure has significant computational advantages by combining a multi-state model with a time series method. To account for the short sample that is available for dementia, we derive expressions for the Kalman filter's convergence speed, size, and power to detect changes and conclude our estimator performs well even in short samples. |
| 2025-09-09 | [Are Humans as Brittle as Large Language Models?](http://arxiv.org/abs/2509.07869v1) | Jiahui Li, Sean Papay et al. | The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs. |
| 2025-09-09 | [Jet cone size dependence of single inclusive jet suppression due to jet quenching in Pb+Pb collisions at $\sqrt{s_{\rm NN}}=5.02$ TeV](http://arxiv.org/abs/2509.07842v1) | Qing-Fei Han, Man Xie et al. | Jet suppression in high-energy heavy-ion collisions results from jet energy loss and transverse-momentum broadening during jet propagation through the quark-gluon plasma (QGP). The jet cone size ($R$) dependence of this suppression offers crucial insights into the energy loss mechanisms and QGP transport properties. In our study, we implement a comprehensive approach within the perturbative QCD parton model that incorporates both elastic and inelastic energy loss mechanisms. For elastic processes the contribution from recoiling thermal partons reduces the net in-cone energy loss for a given jet radius. For inelastic processes, we account for the angular distribution of radiated gluons, the thermalization of soft gluons, and transverse-momentum broadening. Using this framework, we calculate the jet nuclear modification factors ($R_{AA}$) and their double ratios $R_{AA}(R=0.2-1.0)/R_{AA}(R=0.2)$, and systematically compare with ALICE, ATLAS and CMS data in 0-10\% and 30-50\% Pb+Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02~TeV. Numerical results show that $R_{AA}$ increases with the cone size $R$ because the in-cone energy loss decreases at larger radii. Specifically, as the radius $R$ grows, the probability for elastically scattered partons to escape the jet cone and the likelihood for radiated gluons to fall outside the cone both decrease, resulting in a net reduction of energy loss. The $R_{AA}$ double ratios are approximately unity for small radii ($R=0.4$ relative to $R=0.2$) and at high $p_{\rm T}\gtrsim200$ GeV$/c$, in agreement with the data within uncertainties. |
| 2025-09-08 | [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](http://arxiv.org/abs/2509.06938v1) | Praneet Suresh, Jack Stanley et al. | As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk. |
| 2025-09-08 | [Black-hole mass estimation through accretion disk spectral fitting for high-redshift blazars](http://arxiv.org/abs/2509.06933v1) | G. Kyriopoulos, M. Petropoulou et al. | High-redshift ($z>2$) blazars, with relativistic jets aligned toward us, probe the most powerful end of the active galactic nuclei (AGN) population. We aim at determining the black hole masses and mass accretion rates of high-$z$ blazars in a common framework that utilizes a Markov Chain Monte Carlo (MCMC) fitting method and the Shakura-Sunayev multi-temperature accretion disk model, accounting also for attenuation due to neutral hydrogen gas in the intergalactic medium (IGM). We compiled a sample of 23 high-redshift blazars from the literature with publicly available infrared-to-ultraviolet photometric data. We performed a Bayesian fit to the spectral energy distribution (SED) of the accretion disk, accounting for upper limits, and determined the black hole masses and mass accretion rates with their uncertainties. We also examined the impact of optical-ultraviolet attenuation due to gas in the IGM. We find that neglecting IGM attenuation in SED fits leads to systematically larger black-hole mass estimates and correspondingly lower Eddington ratios, with the bias becoming more severe at higher redshift. Our MCMC fits yield median black-hole masses in the range $\sim (10^{8}-10^{10})\,M_{\odot}$ and a broad distribution of median Eddington ratios ($\lambda_{\rm Edd}\sim 0.04 - 1$). Comparison with previous literature shows no clear method-dependent systematic offsets, although individual mass estimates can differ by up to a factor of a few. We also demonstrate that assumptions about black-hole spin introduce a systematic degeneracy. This work is to our knowledge the first systematic study to model the accretion-disk emission of a large sample of high-$z$ blazars within a single, consistent statistical framework. Our results emphasize the importance of accounting for IGM attenuation and of using uniform fitting methods when comparing disk-based black hole estimates across samples. |
| 2025-09-08 | [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](http://arxiv.org/abs/2509.06902v1) | Aivin V. Solatorio | Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty. |
| 2025-09-08 | [Stochastic modelling of cosmic-ray sources for Galactic diffuse emissions](http://arxiv.org/abs/2509.06857v1) | Anton Stall, Philipp Mertsch | Galactic diffuse emissions in gamma rays and neutrinos arise from interactions of cosmic rays with the interstellar medium and probe the cosmic-ray intensity away from the Solar system. Model predictions for those are influenced by the properties of cosmic-ray sources, and understanding the impact of cosmic-ray sources on Galactic diffuse emissions is key for interpreting measurements by LHAASO, Tibet AS-gamma, IceCube, and the upcoming SWGO. We consider supernova remnants as prototypical cosmic-ray sources and study the impact of their discreteness on the Galactic diffuse emissions in different source injection and near-source transport models in a stochastic Monte Carlo study. Three lessons exemplify the results of our simulations: First, the distributions of Galactic diffuse emission intensities can be described by a mixture model of stable laws and Gaussian distributions. Second, the maximal deviations caused by discrete sources across the sky depend on energy, reaching typically tens of percent in burst-like and energy-dependent escape scenarios but order unity or larger in a time-dependent diffusion scenario. Third, the additional model uncertainty from source stochasticity is subdominant in burst-like and energy-dependent escape scenarios, but becomes sizeable above some tens of TeV in the time-dependent diffusion scenario, where it can help reconcile model predictions with LHAASO measurements. With increased spatial resolution, especially at energies beyond tens of TeV, measurements of Galactic diffuse emissions can be expected to constrain source models and locate cosmic ray sources. |
| 2025-09-08 | [Adversarial Obstacle Placement with Spatial Point Processes for Optimal Path Disruption](http://arxiv.org/abs/2509.06837v1) | Li Zhou, Elvan Ceyhan et al. | We investigate the Optimal Obstacle Placement (OOP) problem under uncertainty, framed as the dual of the Optimal Traversal Path problem in the Stochastic Obstacle Scene paradigm. We consider both continuous domains, discretized for analysis, and already discrete spatial grids that form weighted geospatial networks using 8-adjacency lattices. Our unified framework integrates OOP with stochastic geometry, modeling obstacle placement via Strauss (regular) and Mat\'ern (clustered) processes, and evaluates traversal using the Reset Disambiguation algorithm. Through extensive Monte Carlo experiments, we show that traversal cost increases by up to 40% under strongly regular placements, while clustered configurations can decrease traversal costs by as much as 25% by leaving navigable corridors compared to uniform random layouts. In mixed (with both true and false obstacles) scenarios, increasing the proportion of true obstacles from 30% to 70% nearly doubles the traversal cost. These findings are further supported by statistical analysis and stochastic ordering, providing rigorous insights into how spatial patterns and obstacle compositions influence navigation under uncertainty. |
| 2025-09-08 | [Uncertainty Principle from Operator Asymmetry](http://arxiv.org/abs/2509.06760v1) | Xingze Qiu | The uncertainty principle is fundamentally rooted in the algebraic asymmetry between observables. We introduce a new class of uncertainty relations grounded in the resource theory of asymmetry, where incompatibility is quantified by an observable's intrinsic, state-independent capacity to break the symmetry associated with another. This ``operator asymmetry,'' formalized as the asymmetry norm, leads to a variance-based uncertainty relation for pure states that can be tighter than the standard Robertson bound, especially in the near-compatible regime. Most significantly, this framework resolves a long-standing open problem in quantum information theory: the formulation of a universally valid, product-form uncertainty relation for the Wigner-Yanase skew information. We demonstrate the practical power of our framework by deriving tighter quantum speed limits for the dynamics of nearly conserved quantities, which are crucial for understanding non-equilibrium phenomena such as prethermalization and many-body localization. This work provides both a new conceptual lens for understanding quantum uncertainty and a powerful toolkit for its application. |
| 2025-09-08 | [Nested Optimal Transport Distances](http://arxiv.org/abs/2509.06702v1) | Ruben Bontorno, Songyan Hou | Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches. |
| 2025-09-08 | [Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties](http://arxiv.org/abs/2509.06697v1) | Tanujit Chakraborty, Donia Besher et al. | Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \texttt{narfima} \textbf{R} package provides an implementation of our approach. |
| 2025-09-08 | [Port-Hamiltonian Neural Networks: From Theory to Simulation of Interconnected Stochastic Systems](http://arxiv.org/abs/2509.06674v1) | Luca Di Persio, Matthias Ehrhardt et al. | This work introduces a new framework integrating port-Hamiltonian systems (PHS) and neural network architectures. This framework bridges the gap between deterministic and stochastic modeling of complex dynamical systems. We introduce new mathematical formulations and computational methods that expand the geometric structure of PHS to account for uncertainty, environmental noise, and random perturbations. Building on these advances, we introduce stochastic port-Hamiltonian neural networks (pHNNs), which facilitate the accurate learning and prediction of non-autonomous and interconnected stochastic systems. Our proposed framework generalizes passivity concepts to the stochastic regime, ensuring stability while maintaining the system's energy-consistent structure. Extensive simulations, including those involving damped mass-spring systems, Duffing oscillators, and robotic control tasks, demonstrate the capability of pHNNs to capture complex dynamics with high fidelity, even under noise and uncertainty. This unified approach establishes a foundation for the robust, data-driven modeling and control of nonlinear stochastic systems. |
| 2025-09-08 | [The complementary of CTAO, direct detection and collider searches for dark matter in Effective Field Theories and Simplified models](http://arxiv.org/abs/2509.06628v1) | Igor Reis, Andre Scaffidi et al. | This paper explores the sensitivity of the Cherenkov Telescope Array Observatory to dark matter annihilation in the Galactic Center, within the frameworks of Effective Field Theory and Simplified Models. We present sensitivity forecasts, utilizing an up-to-date instrument configuration and incorporating the latest models for Galactic Diffuse Emission. A key aspect of our work is the inclusion of updated dark matter density profiles, J-factors, and velocity dispersion distributions derived from the FIRE-2 cosmological hydrodynamical simulations, which significantly impact the expected indirect detection signals. Furthermore, we update the constraints from direct detection experiments (Xenon1T and LZ) taking into account the astrophysical uncertainties informed by the FIRE-2 simulations, and also investigate limits coming from collider searches (ATLAS and CMS). Our analysis reveals improved constraints on the effective suppression scale ($M_*$) in the Effective Field Theory framework and on the mediator mass ($M_{med}$) in Simplified Models compared to previous studies, highlighting the complementarity of the Cherenkov Telescope Array Observatory with direct and collider searches in probing a wide range of dark matter scenarios. We discuss the implications of these results for various dark matter interaction types, including scalar, pseudoscalar, vector, and axial-vector mediators, and emphasize the importance of considering realistic astrophysical inputs in interpreting dark matter search results across different experimental fronts. |
| 2025-09-05 | [Testing Magnetic Field Configurations in Spider Pulsar PSR J1723-2837 with IXPE](http://arxiv.org/abs/2509.05240v1) | Michela Negro, Haocheng Zhang et al. | We present the first X-ray polarimetry observations of a redback millisecond pulsar binary, \src, with the Imaging X-ray Polarimetry Explorer (IXPE). Redbacks are compact binaries in which a rotation-powered millisecond pulsar interacts with a non-degenerate companion via an intrabinary shock, forming ideal laboratories for probing pulsar winds and relativistic shock physics, where ordered magnetic fields and particle acceleration shape the observed radiation. We conduct a spectro-polarimetric analysis combining IXPE data with archival Chandra, XMM-Newton, NuSTAR, and Swift observations. We explore two limiting magnetic field configurations, parallel and perpendicular to the bulk flow, and simulate their expected polarization signatures using the {\tt 3DPol} radiative transport code. To account for the rapid rotation of the polarization angle predicted by these models, we implement a phase-dependent Stokes alignment procedure that preserves the polarization degree while correcting for phase-rotating PA. We also devise a new maximum-likelihood fitting strategy to determine the phase-dependence of the polarization angle by minimizing the polarization degree uncertainty. This technique shows a hint the binary may be rotating clockwise relative to the celestial north pole. We find no significant detection of polarization in the IXPE data, with PD<51% at 99% confidence level. Our results excludes the high-polarization degree scenario predicted by the perpendicular field model during the brightest orbital phase bin. Simulations show that doubling the current exposure would make the parallel configuration detectable. The new PA rotation technique is also applicable to IXPE data of many sources whose intrinsic PA variation is apriori not known but is strictly periodic. |
| 2025-09-05 | [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](http://arxiv.org/abs/2509.05201v1) | Nariman Niknejad, Gokul S. Sankar et al. | This paper presents a robust model predictive control (MPC) framework that explicitly addresses the non-Gaussian noise inherent in deep learning-based perception modules used for state estimation. Recognizing that accurate uncertainty quantification of the perception module is essential for safe feedback control, our approach departs from the conventional assumption of zero-mean noise quantification of the perception error. Instead, it employs set-based state estimation with constrained zonotopes to capture biased, heavy-tailed uncertainties while maintaining bounded estimation errors. To improve computational efficiency, the robust MPC is reformulated as a linear program (LP), using a Minkowski-Lyapunov-based cost function with an added slack variable to prevent degenerate solutions. Closed-loop stability is ensured through Minkowski-Lyapunov inequalities and contractive zonotopic invariant sets. The largest stabilizing terminal set and its corresponding feedback gain are then derived via an ellipsoidal approximation of the zonotopes. The proposed framework is validated through both simulations and hardware experiments on an omnidirectional mobile robot along with a camera and a convolutional neural network-based perception module implemented within a ROS2 framework. The results demonstrate that the perception-aware MPC provides stable and accurate control performance under heavy-tailed noise conditions, significantly outperforming traditional Gaussian-noise-based designs in terms of both state estimation error bounding and overall control performance. |
| 2025-09-05 | [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](http://arxiv.org/abs/2509.05186v1) | Benjamin J. Zhang, Siting Liu et al. | In-context operator networks (ICON) are a class of operator learning methods based on the novel architectures of foundation models. Trained on a diverse set of datasets of initial and boundary conditions paired with corresponding solutions to ordinary and partial differential equations (ODEs and PDEs), ICON learns to map example condition-solution pairs of a given differential equation to an approximation of its solution operator. Here, we present a probabilistic framework that reveals ICON as implicitly performing Bayesian inference, where it computes the mean of the posterior predictive distribution over solution operators conditioned on the provided context, i.e., example condition-solution pairs. The formalism of random differential equations provides the probabilistic framework for describing the tasks ICON accomplishes while also providing a basis for understanding other multi-operator learning methods. This probabilistic perspective provides a basis for extending ICON to \emph{generative} settings, where one can sample from the posterior predictive distribution of solution operators. The generative formulation of ICON (GenICON) captures the underlying uncertainty in the solution operator, which enables principled uncertainty quantification in the solution predictions in operator learning. |
| 2025-09-05 | [Orlicz spaces and the uncertainty principle](http://arxiv.org/abs/2509.05185v1) | A. Iosevich, I. Li et al. | Let $f$ be a finite signal. The classical uncertainty principle tells us that the product of the support of $f$ and the support of $\hat{f}$, the Fourier transform of $f$, must satisfy $|supp(f)|\cdot|supp(\hat{f})|\geq |G|$. Recently, Iosevich and Mayeli improved the uncertainty principle for signals with Fourier supported on generic sets. This was done by employing the Fourier restriction theory in $L^p$ spaces. In this paper, we extended the $(p,q)$-restriction setting to Orlicz spaces. Then we apply uncertainty principles to the problem of exact recovery, which again extends and recovers the result that Iosevich and Mayeli obtained in Lebesgue spaces. |
| 2025-09-05 | [Deep-Field Analytical Calibration](http://arxiv.org/abs/2509.05152v1) | Andy Park, Xiangchong Li et al. | The next generation of imaging surveys, including the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), Euclid, and the Nancy Grace Roman Space Telescope, will place unprecedented constraints on cosmology using weak gravitational lensing. To fully exploit their statistical power, shear measurement methods must achieve sub-percent accuracy while mitigating systematic biases from noise, the point-spread function (PSF), blending, and shear-dependent detection. The analytical calibration framework (\texttt{AnaCal}) has demonstrated such accuracy but requires adding noise to images, reducing their effective depth. We introduce Deep-Field Analytical Calibration (\textsc{deep-field~}\texttt{AnaCal}), an extension of \texttt{AnaCal} that leverages deep-field images to compute shear responses while preserving wide-field statistical power. We validate the method on isolated and blended galaxy simulations with LSST-like seeing and noise, showing it meets the stringent requirement of multiplicative bias $|m| < 3\times10^{-3}$ at 99.7% confidence. Relative to standard \texttt{AnaCal} on wide-field images, this method improves effective galaxy number density from $17$ to $30$ arcmin$^{-2}$ for simulated 10-year LSST data. Assuming deep fields with $10\times$ the exposure of wide fields, we find the pixel noise variance in shear estimation is reduced by $30%$ and the overall shear uncertainty by $\sim 25%$. Finally, we assess sample variance impacts using the LSST Deep Drilling Fields strategy, finding an equivalent calibration uncertainty of $\lesssim 0.3%$. These results establish \textsc{deep-field~}\texttt{AnaCal} as a promising approach for shear calibration in upcoming weak lensing surveys. |
| 2025-09-05 | [An emulator-based forecasting on astrophysics and cosmology with 21 cm and density cross-correlations during EoR](http://arxiv.org/abs/2509.05096v1) | Barun Maity | The 21 cm signal arising from fluctuations in the neutral hydrogen field, and its cross-correlation with other tracers of cosmic density, are promising probes of the high-redshift Universe. In this study, we assess the potential of the 21 cm power spectrum, along with its cross power spectrum with dark matter density and associated bias, to constrain both astrophysics during the reionization era and the underlying cosmology. Our methodology involves emulating these estimators using an Artificial Neural Network (ANN), enabling efficient exploration of the parameter space. Utilizing a photon-conserving semi-numerical reionization model, we construct emulators at a fixed redshift ($z = 7.0$) for $k$-modes relevant to upcoming telescopes such as SKA-Low. We generate $\sim7000$ training samples by varying both cosmological and astrophysical parameters along with initial conditions, achieving high accuracy when compared to true simulation outputs. While forecasting, the model involves five free parameters: three cosmological ($\Omega_m$, $h$, $\sigma_8$) and two astrophysical (ionizing efficiency, $\zeta$, and minimum halo mass, $M_{\mathrm{min}}$). Using a fiducial model at the mid-reionization stage, we create a mock dataset and perform forecasting with the trained emulators. Assuming a 5% observational uncertainty combined with emulator error, we find that the 21 cm and 21 cm-density cross power spectra can constrain the Hubble parameter ($h$) to better than 6% at a confidence interval of 95%, with tight constraints on the global neutral fraction ($Q_{\mathrm{HI}}$). The inclusion of bias information further improves constraints on $\sigma_8$ (< 10% at 95% confidence). Finally, robustness tests with two alternate ionization states and a variant with higher observational uncertainty show that the ionization fractions are still reliably recovered, even when cosmological constraints weaken. |
| 2025-09-05 | [Finding your MUSE: Mining Unexpected Solutions Engine](http://arxiv.org/abs/2509.05072v1) | Nir Sweed, Hanit Hakim et al. | Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research. |
| 2025-09-05 | [Deep Inverse Rosenblatt Transport for Structural Reliability Analysis](http://arxiv.org/abs/2509.05061v1) | Aryan Tyagi, Jan N. Fuhg | Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study. |
| 2025-09-05 | [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](http://arxiv.org/abs/2509.05060v1) | Patrick Amadeus Irawan, Ryandito Diandaru et al. | We introduce Entropy2Vec, a novel framework for deriving cross-lingual language representations by leveraging the entropy of monolingual language models. Unlike traditional typological inventories that suffer from feature sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in language models to capture typological relationships between languages. By training a language model on a single language, we hypothesize that the entropy of its predictions reflects its structural similarity to other languages: Low entropy indicates high similarity, while high entropy suggests greater divergence. This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values. Empirical evaluations demonstrate that Entropy2Vec embeddings align with established typological categories and achieved competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework. |
| 2025-09-05 | [Reconstruction of the Dipole Amplitude in the Dipole Picture as a mathematical Inverse Problem](http://arxiv.org/abs/2509.05005v1) | Henri HÃ¤nninen, Antti KykkÃ¤nen et al. | We show that the inference problem of constraining the dipole amplitude with inclusive deep inelastic scattering data can be written into a discrete linear inverse problem, in an analogous manner as can be done for computed tomography. To this formulation of the problem, we apply standard inverse problems methods and algorithms to reconstruct known dipole amplitudes from simulated reduced cross section data with realistic precision. The main difference of this approach to previous works is that this implementation does not require any fit parametrization of the dipole amplitude. The freedom from parametrization also enables us for the first time to quantify the uncertainties of the inferred dipole amplitude in a novel more general framework. This mathematical approach to small-$x$ phenomenology opens a path to parametrization bias free inference of the dipole amplitude from HERA and Electron--Ion Collider data. |
| 2025-09-04 | [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](http://arxiv.org/abs/2509.04430v1) | Nikolay Kartashev, Ivan Rubachev et al. | Recent advancements in tabular deep learning have demonstrated exceptional practical performance, yet the field often lacks a clear understanding of why these techniques actually succeed. To address this gap, our paper highlights the importance of the concept of data uncertainty for explaining the effectiveness of the recent tabular DL methods. In particular, we reveal that the success of many beneficial design choices in tabular DL, such as numerical feature embeddings, retrieval-augmented models and advanced ensembling strategies, can be largely attributed to their implicit mechanisms for managing high data uncertainty. By dissecting these mechanisms, we provide a unifying understanding of the recent performance improvements. Furthermore, the insights derived from this data-uncertainty perspective directly allowed us to develop more effective numerical feature embeddings as an immediate practical outcome of our analysis. Overall, our work paves the way to foundational understanding of the benefits introduced by modern tabular methods that results in the concrete advancements of existing techniques and outlines future research directions for tabular DL. |
| 2025-09-04 | [Generation of Lognormal Synthetic Lyman-$Î±$ Forest Spectra for $P_{1D}$ Analysis](http://arxiv.org/abs/2509.04405v1) | Meagan Herbold, Naim GÃ¶ksel KaraÃ§aylÄ± et al. | The one-dimensional flux power spectrum (P1D) of the Lyman-$\alpha$ forest probes small-scale structure in the intergalactic medium (IGM) and is therefore sensitive to a variety of cosmological and astrophysical parameters. These include the amplitude and shape of the matter power spectrum, the thermal history of the IGM, the sum of neutrino masses, and potential small-scale fluctuations due to the nature of dark matter. However, P1D is also highly sensitive to observational and instrumental systematics, making accurate synthetic spectra essential for validating analyses and quantifying these effects, especially in high-volume surveys like the Dark Energy Spectroscopic Instrument (DESI). We present an efficient lognormal mock framework for generating one-dimensional Lyman-$\alpha$ forest spectra tailored for P1D analysis. Our method captures the redshift evolution of the mean transmitted flux and the scale-dependent shape and amplitude of the one-dimensional flux power spectrum by tuning Gaussian field correlations and transformation parameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \leq z \leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \leq k \leq 1.0$ s km$^{-1}$), our mocks recover the mean flux evolution with redshift to sub-percent accuracy, and the P1D at the percent level. Additionally, we discuss potential extensions of this framework, such as the incorporation of astrophysical contaminants, continuum uncertainties, and instrumental effects. Such improvements would expand its utility in ongoing and upcoming surveys and enable a broader range of validation efforts and systematics studies for P1D inference and precision cosmology. |
| 2025-09-04 | [Revealing the origin of supermassive black holes with Taiji-TianQin network](http://arxiv.org/abs/2509.04396v1) | Ping Shen, Wen-Biao Han et al. | The origin of supermassive black holes (SMBHs) is a pivotal problem in modern cosmology. This work explores the potential of the Taiji-TianQin space-borne gravitational-wave (GW) detector network to identify the formation channels of massive black hole binaries (MBHBs) at high redshifts ($z \gtrsim 10$). The network substantially improves detection capability, boosting the signal-to-noise ratio by a factor of 2.2-3.0 (1.06-1.14) relative to TianQin (Taiji) alone. It increases the detection rate of MBHBs formed from light seeds (LS) by more than 2.2 times and achieves over 96\% detection efficiency for those originating from heavy seeds (HS). Furthermore, the network enables component mass estimation with relative uncertainties as low as $\sim 10^{-4}$ at the $2\sigma$ level. These improvements facilitate the assembly of a well-constrained population sample, allowing robust measurement of the fractional contributions from different formation pathways. The network achieves high precision in distinguishing between LS and HS origins (7.4\% relative uncertainty at $2\sigma$) and offers moderate discrimination between delay and no-delay channels in HS-origin binaries (24\%). However, classification remains challenging for delay versus no-delay scenarios in LS-origin systems (58\%) due to significant population overlap. In conclusion, the Taiji-TianQin network will serve as a powerful tool for unveiling the origins of SMBHs through GW population studies. |
| 2025-09-04 | [Prominence: A discriminator of gravitational wave signals](http://arxiv.org/abs/2509.04384v1) | JoÃ£o GonÃ§alves, Danny Marfatia et al. | The concept of prominence is familiar to signal engineers, topographers and mountaineers. We introduce Prominence $\mathcal P$ as a discriminator of gravitational wave (GW) signals. We treat black hole and neutron star binaries as astrophysical background sources, and show how $\mathcal P$ can be used to distinguish between GW spectra produced by first-order phase transitions, domain walls and cosmic strings, and combinations thereof. Prominence can also be used to discriminate between these and off-piste sources of GWs. The uncertainty in the measured energy density in GWs at Pulsar Timing Arrays needs to be smaller than $\sim 4\%$ for $\mathcal{P}$ to achieve discrimination at 3$\sigma$. LISA and ET data are expected to have sufficiently small uncertainties that Prominence can play a central role in their analysis. |
| 2025-09-04 | [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](http://arxiv.org/abs/2509.04363v1) | Paul Scherer, Andreas Kirsch et al. | Real-world experimental scenarios are characterized by the presence of heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated in batched settings. The bias--variance tradeoff can be used to write the expected mean squared error between a model distribution and a ground-truth random variable as the sum of an epistemic uncertainty term, the bias squared, and an aleatoric uncertainty term. We leverage this relationship to propose novel active learning strategies that directly reduce the bias between experimental rounds, considering model systems both with and without noise. Finally, we investigate methods to leverage historical data in a quadratic manner through the use of a novel cobias--covariance relationship, which naturally proposes a mechanism for batching through an eigendecomposition strategy. When our difference-based method leveraging the cobias--covariance relationship is utilized in a batched setting (with a quadratic estimator), we outperform a number of canonical methods including BALD and Least Confidence. |
| 2025-09-04 | [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](http://arxiv.org/abs/2509.04357v1) | Jiajun He, Naoki Sawada et al. | Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech. |
| 2025-09-04 | [History matching for functional data and its application to tsunami warnings in the Indian Ocean](http://arxiv.org/abs/2509.04342v1) | Ryuichi Kanai, NicolÃ¡s HernÃ¡ndez et al. | Traditional History Matching (HM) identifies implausible regions of the input parameter space by comparing scalar outputs of a computer model to observations. It offers higher computational efficiency than Bayesian calibration, making it suitable for high-dimensional problems. However, in real physical systems, outputs are often functional, such as time series or spatial fields, and conventional HM cannot fully exploit such information. We propose a novel method, Functional History Matching (FHM), which extends HM to handle functional data. FHM incorporates the Outer Product Emulator, an extension of the Gaussian Process emulator designed for time series, to enhance computational efficiency. FHM also leverages Random Projection to extract dynamic features from infinite-dimensional data, including derivatives. FHM supports uncertainty quantification essential for decision-making and naturally accommodates model discrepancies. To demonstrate its practical effectiveness, we apply FHM to a synthetic tsunami forecasting scenario in the Indian Ocean, assuming a realistic event in the Makran subduction zone. Wave elevation time series from offshore buoy data are used to predict wave elevations over the Indian coastline. Our results show that FHM significantly outperforms scalar-based HM in accuracy. FHM enables reliable forecasting from functional data within feasible computational constraints, offering a robust framework for early warning systems and beyond. |
| 2025-09-04 | [We Have It Covered: A Resampling-based Method for Uplift Model Comparison](http://arxiv.org/abs/2509.04315v1) | Yang Liu, Chaoyu Yuan | Uplift models play a critical role in modern marketing applications to help understand the incremental benefits of interventions and identify optimal targeting strategies. A variety of techniques exist for building uplift models, and it is essential to understand the model differences in the context of intended applications. The uplift curve is a widely adopted tool for assessing uplift model performance on the selection universe when observations are available for the entire population. However, when it is uneconomical or infeasible to select the entire population, it becomes difficult or even impossible to estimate the uplift curve without appropriate sampling design. To the best of our knowledge, no prior work has addressed uncertainty quantification of uplift curve estimates, which is essential for model comparisons. We propose a two-step sampling procedure and a resampling-based approach to compare uplift models with uncertainty quantification, examine the proposed method via simulations and real data applications, and conclude with a discussion. |
| 2025-09-04 | [Learning Optimal Crew Dispatch for Grid Restoration Following an Earthquake](http://arxiv.org/abs/2509.04308v1) | Farshad Amani, Faezeh Ardali et al. | Post-disaster crew dispatch is a critical but computationally intensive task. Traditional mixed-integer linear programming methods often require minutes to several hours to compute solutions, leading to delays that hinder timely decision-making in highly dynamic restoration environments. To address this challenge, we propose a novel learning-based framework that integrates transformer architectures with deep reinforcement learning (DRL) to deliver near real-time decision support without compromising solution quality. Crew dispatch is formulated as a sequential decision-making problem under uncertainty, where transformers capture high-dimensional system states and temporal dependencies, while DRL enables adaptive and scalable decision-making. Earthquake-induced distribution network damage is first characterized using established seismic standards, followed by a scenario generation and reduction pipeline that aggregates probable outcomes into a single geospatial impact map. Conditioned on this map, the proposed framework generates second-level dispatch strategies, trained offline on simulated and historical events and deployed online for rapid response. In addition to substantial runtime improvements, the proposed method enhances system resilience by enabling faster and more effective recovery and restoration. Case studies, particularly on the 2869-bus European gas and power network, demonstrate that the method substantially accelerates restoration while maintaining high-quality solutions, underscoring its potential for practical deployment in large-scale disaster response. |
| 2025-09-04 | [Energy Confinement Time Scaling for the Negative Triangularity Scenario in DIII-D](http://arxiv.org/abs/2509.04279v1) | P. Lunia | Results from the 2023 negative triangularity campaign on DIII-D demonstrate encouraging energy confinement properties, similar to or exceeding the scaling of the IPB98(y,2) law. This paper describes the procedure with which a new scaling law was regressed specifically from the data from the DIII-D campaign. Given the relatively small size of the single-machine dataset, measures were taken to minimize sampling bias and give a realistic estimate of the large uncertainties from the regression. The resulting power law shows a robustly stronger dependence on plasma current and more severe power degradation as compared to the H-mode scaling law. |
| 2025-09-03 | [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](http://arxiv.org/abs/2509.03516v1) | Ouxiang Li, Yuan Wang et al. | Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/. |
| 2025-09-03 | [Bayesian Multivariate Sparse Functional PCA](http://arxiv.org/abs/2509.03512v1) | Joseph Sartini, Scott Zeger et al. | Functional Principal Components Analysis (FPCA) provides a parsimonious, semi-parametric model for multivariate, sparsely-observed functional data. Frequentist FPCA approaches estimate principal components (PCs) from the data, then condition on these estimates in subsequent analyses. As an alternative, we propose a fully Bayesian inferential framework for multivariate, sparse functional data (MSFAST) which explicitly models the PCs and incorporates their uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate, densely-observed functional data. Like FAST, MSFAST represents PCs using orthonormal splines, samples the orthonormal spline coefficients using parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST extends FAST to multivariate, sparsely-observed data by (1) standardizing each functional covariate to mitigate poor posterior conditioning due to disparate scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing likelihood calculations over covariates; (4) updating parameterizations and priors for computational stability; (5) using a Procrustes-based posterior alignment procedure; and (6) providing efficient prediction routines. We evaluated MSFAST alongside existing implementations using simulations. MSFAST produces uniquely valid inferences and accurate estimates, particularly for smaller signals. MSFAST is motivated by and applied to a study of child growth, with an accompanying vignette illustrating the implementation step-by-step. |
| 2025-09-03 | [Quantitative Stability and Contraction Principles for Mean-Field G-SDEs](http://arxiv.org/abs/2509.03439v1) | Yunfan Zhao, Xiaojing Chen et al. | We study mean-field stochastic differential equations (SDEs) driven by G-Brownian motion, extending recent work on existence and uniqueness by developing a full quantitative stability framework. Our main contribution is the construction of an intrinsic stability modulus that provides explicit bounds on the sensitivity of solutions with respect to perturbations in initial data (and, indirectly, coefficients). Using Bihari-Osgood type inequalities under G-expectation, we establish sharp continuity estimates for the data-to-solution map and analyze the asymptotic properties of the stability modulus. In particular, we identify contraction behavior on short horizons, leading to a contraction principle that guarantees uniqueness and global propagation of stability. The results apply under non-Lipschitz, non-deterministic coefficients with square-integrable initial data, thereby significantly broadening the scope of mean-field G-SDEs. Beyond existence and uniqueness, our framework quantifies robustness of solutions under volatility uncertainty, opening new directions for applications in stochastic control, risk management, and mean-field models under ambiguity. |
| 2025-09-03 | [Bayesian analysis of properties of nuclear matter with the FOPI experimental data](http://arxiv.org/abs/2509.03406v1) | Guojun Wei, Manzi Nan et al. | Based on the ultra-relativistic quantum molecular dynamics (UrQMD) transport model, combined with experimental data of directed flow, elliptic flow, and nuclear stopping power measured by FOPI in $\rm ^{197}Au+^{197}Au$ collisions at beam energies ($E_{lab}$) of 0.25 and 0.4 GeV/nucleon, the incompressibility of the nuclear equation of state $K_0$, the nucleon effective mass $m^*$, and the in-medium correction factor ($F$, with respect to free-space values) on the nucleon-nucleon elastic cross sections are studied by Bayesian analysis. It is found that both $m^*$ and $F$ can be tightly constrained with the uncertainty $\le$ 15\%, however, $K_0$ cannot be constrained tightly. We deduce $m^*/m_0 = 0.78^{+0.09}_{-0.10}$ and $F = 0.75^{+0.08}_{-0.07}$ with experimental data at $E_{lab}$ = 0.25 GeV/nucleon, and the obtained values increased to $m^*/m_0 = 0.88^{+0.03}_{-0.03}$ and $F = 0.88^{+0.06}_{-0.07}$ at $E_{lab}$ = 0.4 GeV/nucleon. The obtained results are further verified with rapidity-dependent flow data. |
| 2025-09-03 | [Elemental and Isotopic Yields from T Coronae Borealis: Predictions and Uncertainties](http://arxiv.org/abs/2509.03395v1) | Emma Wallace, Christian Iliadis et al. | T Coronae Borealis (T CrB) is a symbiotic recurrent nova system expected to undergo its next outburst within the next two years. Recent hydrodynamic simulations have predicted the nucleosynthetic yields for both carbon-oxygen (CO) and oxygen-neon (ONe) white-dwarf models, but without accounting for thermonuclear reaction-rate uncertainties. We perform detailed Monte Carlo post-processing nucleosynthesis calculations based on updated thermonuclear reaction rates and uncertainties from the 2025 evaluation. We quantify the resulting abundance uncertainties and identify the key nuclear reactions that dominate them. Our results show that both the CO and ONe nova models robustly produce characteristic CNO isotopes. More pronounced abundance differences emerge for elements with A $\ge$ 20. Sulfur is the most robust observational discriminator between the CO and ONe nova models, with a model-to-model difference of a factor of $\approx$30 and minimal sensitivity to reaction rate uncertainties. Neon, silicon, and phosphorus exhibit even larger abundance differences (factors of $\approx$150-250), providing strong diagnostic potential. While their predicted yields are subject to larger uncertainties, these remain smaller than the model-to-model differences, allowing these elements to serve as useful, though less precise, tracers of white-dwarf composition. Chlorine, argon, and potassium also differ between models, but the 1$\sigma$-abundance ranges for the CO and ONe models overlap, reducing their present usefulness as composition tracers. We find that only nine nuclear reactions dominate the abundance uncertainties of the most diagnostically important isotopes, and their influence is largely independent of the underlying white-dwarf composition. These results provide guidance for future experimental efforts and for interpreting ejecta compositions in the next eruption of T CrB. |
| 2025-09-03 | [On the renormalization-group analysis of the SM: loops, uncertainties, and vacuum stability](http://arxiv.org/abs/2509.03369v1) | A. V. Bednyakov, A. S. Fedoruk et al. | Renormalization-group equations (RGE) is one of the key tools in studying high-energy behavior of the Standard Model (SM). We begin by reviewing one-loop RGE for the dimensionless couplings of the SM and proceed to the state-of-the-art results. Our study focuses on the RGE solutions at different loop orders. We compare not only the standard (``diagonal'') loop counting, when one considers gauge, Yukawa, and scalar self-coupling beta functions at the same order, but also ``non-diagonal'' ones, inspired by the so-called Weyl consistency conditions. We discuss the initial conditions for RGE (``matching'') for different loop configurations, and study the uncertainties of running coupling both related to the limited precision of the experimental input (``parametric'') and to the missing high-order corrections (``theoretical''). As an application of our analysis we also estimate the electroweak vacuum decay probability and study how the uncertainties in the running parameters affect the latter. We argue that the ``non-diagonal'' beta functions, if coupled with more consistent ``non-diagonal'' matching lead to larger theoretical uncertainty than the ``diagonal'' ones. |
| 2025-09-03 | [Search for Past Stellar Encounters and the Origin of 3I/ATLAS](http://arxiv.org/abs/2509.03361v1) | Yiyang Guo, Luyao Zhang et al. | 3I/ATLAS, the third discovered interstellar object, has a heliocentric speed of 58 km/s and exhibits cometary activity. To constrain the origin of 3I/ATLAS and its past dynamical evolution, we propagate the orbits of 3I/ATLAS and nearby stars to search for stellar encounters. Integrating orbits in the Galactic potential and propagating the astrometric and radial-velocity uncertainties of 30 million Gaia stars, we identify 25 encounters with median encounter distances less than 1 pc. However, because the encounter speeds between 3I/ATLAS and each encounter exceed 20 km/s, none is a plausible host under common ejection mechanisms. We infer stellar masses for most stars and quantify the gravitational perturbations exerted by each individual star or each binary system on 3I/ATLAS. The strongest gravitational scattering perturber is a wide M-dwarf binary. Among all past encounters, the binary's barycenter and 3I/ATLAS reach the small encounter distance of 0.242 pc and the encounter speed of 28.39 km/s,1.64 Myr ago. We further demonstrate that the cumulative influence of the stellar encounters on both the speed and direction of 3I/ATLAS is weak. Based on the present kinematics of 3I/ATLAS to assess its origin, we find that a thin-disk origin is strongly favored, because the thin disk both exhibits a velocity distribution closely matching that of 3I/ATLAS and provides the dominant local number density of stars. |
| 2025-09-03 | [Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner](http://arxiv.org/abs/2509.03348v1) | Yewen Li, Jingtong Gao et al. | Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers' bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges by focusing on trajectory-level accumulated rewards, as well as their explainable capability, i.e., planning a future trajectory of states and executing bids accordingly. However, diffusers struggle with generation uncertainty, particularly regarding dynamic legitimacy between adjacent states, which can lead to poor bids and further cause significant loss of ad impression opportunities when competing with other advertisers in a highly competitive auction environment. To address it, we propose a Causal auto-Bidding method based on a Diffusion completer-aligner framework, termed CBD. Firstly, we augment the diffusion training process with an extra random variable t, where the model observes t-length historical sequences with the goal of completing the remaining sequence, thereby enhancing the generated sequences' dynamic legitimacy. Then, we employ a trajectory-level return model to refine the generated trajectories, aligning more closely with advertisers' objectives. Experimental results across diverse settings demonstrate that our approach not only achieves superior performance on large-scale auto-bidding benchmarks, such as a 29.9% improvement in conversion value in the challenging sparse-reward auction setting, but also delivers significant improvements on the Kuaishou online advertising platform, including a 2.0% increase in target cost. |
| 2025-09-03 | [Bayesian Additive Regression Trees for functional ANOVA model](http://arxiv.org/abs/2509.03317v1) | Seokhun Park, Insung Kong et al. | Bayesian Additive Regression Trees (BART) is a powerful statistical model that leverages the strengths of Bayesian inference and regression trees. It has received significant attention for capturing complex non-linear relationships and interactions among predictors. However, the accuracy of BART often comes at the cost of interpretability. To address this limitation, we propose ANOVA Bayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART based on the functional ANOVA decomposition, which is used to decompose the variability of a function into different interactions, each representing the contribution of a different set of covariates or factors. Our proposed ANOVA-BART enhances interpretability, preserves and extends the theoretical guarantees of BART, and achieves superior predictive performance. Specifically, we establish that the posterior concentration rate of ANOVA-BART is nearly minimax optimal, and further provides the same convergence rates for each interaction that are not available for BART. Moreover, comprehensive experiments confirm that ANOVA-BART surpasses BART in both accuracy and uncertainty quantification, while also demonstrating its effectiveness in component selection. These results suggest that ANOVA-BART offers a compelling alternative to BART by balancing predictive accuracy, interpretability, and theoretical consistency. |
| 2025-09-03 | [Credible Uncertainty Quantification under Noise and System Model Mismatch](http://arxiv.org/abs/2509.03311v1) | Penggao Yan, Li-Ta Hsu | State estimators often provide self-assessed uncertainty metrics, such as covariance matrices, whose reliability is critical for downstream tasks. However, these self-assessments can be misleading due to underlying modeling violations like noise or system model mismatch. This letter addresses the problem of estimator credibility by introducing a unified, multi-metric evaluation framework. We construct a compact credibility portfolio that synergistically combines traditional metrics like the Normalized Estimation Error Squared (NEES) and the Noncredibility Index (NCI) with proper scoring rules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our key contributions are a novel energy distance-based location test to robustly detect system model misspecification and a method that leverages the asymmetric sensitivities of NLL and ES to distinguish optimism covariance scaling from system bias. Monte Carlo simulations across six distinct credibility scenarios demonstrate that our proposed method achieves high classification accuracy (80-100%), drastically outperforming single-metric baselines which consistently fail to provide a complete and correct diagnosis. This framework provides a practical tool for turning patterns of credibility indicators into actionable diagnoses of model deficiencies. |
| 2025-08-29 | [Sequential Fair Allocation With Replenishments: A Little Envy Goes An Exponentially Long Way](http://arxiv.org/abs/2508.21753v1) | Chido Onyeze, Sean R. Sinclair et al. | We study the trade-off between envy and inefficiency in repeated resource allocation settings with stochastic replenishments, motivated by real-world systems such as food banks and medical supply chains. Specifically, we consider a model in which a decision-maker faced with stochastic demand and resource donations must trade off between an equitable and efficient allocation of resources over an infinite horizon. The decision-maker has access to storage with fixed capacity $M$, and incurs efficiency losses when storage is empty (stockouts) or full (overflows). We provide a nearly tight (up to constant factors) characterization of achievable envy-inefficiency pairs. Namely, we introduce a class of Bang-Bang control policies whose inefficiency exhibits a sharp phase transition, dropping from $\Theta(1/M)$ when $\Delta = 0$ to $e^{-\Omega(\Delta M)}$ when $\Delta > 0$, where $\Delta$ is used to denote the target envy of the policy. We complement this with matching lower bounds, demonstrating that the trade-off is driven by supply, as opposed to demand uncertainty. Our results demonstrate that envy-inefficiency trade-offs not only persist in settings with dynamic replenishment, but are shaped by the decision-maker's available capacity, and are therefore qualitatively different compared to previously studied settings with fixed supply. |
| 2025-08-29 | [Uncertainties within Weather Regime definitions for the Euro-Atlantic sector in ERA5 and CMIP6](http://arxiv.org/abs/2508.21701v1) | Lotte Hompes, Swinda K. J. Falkena et al. | Certain Weather Regimes (WR) are associated with a higher risk of energy shortages, i.e. Blocking regimes for European winters. However, there are many uncertainties tied to the implementation of WRs and associated risks in the energy sector. Especially the impact of climate change is unknown.   We investigate these uncertainties by looking at three methodologically diverse Euro-Atlantic WR definitions. We carry out a thorough validation of these methods and analyse their methodological and spatio-temporal sensitivity using ERA5 data. Furthermore, we look into the suitability of CMIP6 models for WR based impact assessments.   Our sensitivity assessment showed that the persistence and occurrence of regimes are sensitive to small changes in the methodology. We show that the training period used has a very significant impact on the persistence and occurrence of the regimes found. For both WR4 and WR7, this results in instability of the regime patterns.   All CMIP6 models investigated show instability of the regimes. Meaning that the normalised distance between the CMIP6 model regimes and our baseline regimes exceeds 0.4 or are visually extremely dissimilar. Only the WR4 regimes clustered on historical CMIP6 model data consistently have a normalised distance to our baseline regimes smaller than 0.4 and are visually identifiable. The WR6 definition exceeds the normalised distance threshold for all investigated CMIP6 experiments. Though all CMIP6 model experiments clustered with the WR7 definition have a normalised distance to the baseline regimes below 0.4, visual inspection of the regimes indicates instability.   Great caution should be taken when applying WR's in impact models for the energy sector, due to this large instability and uncertainties associated with WR definitions. |
| 2025-08-29 | [Chance-Constrained DC Optimal Power Flow Using Constraint-Informed Statistical Estimation](http://arxiv.org/abs/2508.21687v1) | Tianyang Yi, D. Adrian Maldonado et al. | Chance-constrained optimization has emerged as a promising framework for managing uncertainties in power systems. This work advances its application to the DC Optimal Power Flow (DC-OPF) model, developing a novel approach to uncertainty modeling and estimation. Current methods typically tackle these problems by first modeling random nodal injections using high-dimensional statistical distributions that scale with the number of buses, followed by deriving deterministic reformulations of the probabilistic constraints. We propose an alternative methodology that exploits the constraint structure to inform the uncertainties to be estimated, enabling significant dimensionality reduction. Rather than learning joint distributions of net-load forecast errors across units, we instead directly model the one-dimensional aggregate system forecast error and two-dimensional line errors weighted by power transfer distribution factors. We evaluate our approach under both Gaussian and non-Gaussian distributions on synthetic and real-world datasets, demonstrating significant improvements in statistical accuracy and optimization performance compared to existing methods. |
| 2025-08-29 | [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](http://arxiv.org/abs/2508.21677v1) | Bernhard Wullt, Johannes KÃ¶hler et al. | Industrial manipulators are normally operated in cluttered environments, making safe motion planning important. Furthermore, the presence of model-uncertainties make safe motion planning more difficult. Therefore, in practice the speed is limited in order to reduce the effect of disturbances. There is a need for control methods that can guarantee safe motions that can be executed fast. We address this need by suggesting a novel model predictive control (MPC) solution for manipulators, where our two main components are a robust tube MPC and a corridor planning algorithm to obtain collision-free motion. Our solution results in a convex MPC, which we can solve fast, making our method practically useful. We demonstrate the efficacy of our method in a simulated environment with a 6 DOF industrial robot operating in cluttered environments with uncertainties in model parameters. We outperform benchmark methods, both in terms of being able to work under higher levels of model uncertainties, while also yielding faster motion. |
| 2025-08-29 | [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](http://arxiv.org/abs/2508.21648v1) | Farhad Abtahi, Mehdi Astaraki et al. | Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems. |
| 2025-08-29 | [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](http://arxiv.org/abs/2508.21620v1) | Agustinus Kristiadi | Decision theories offer principled methods for making choices under various types of uncertainty. Algorithms that implement these theories have been successfully applied to a wide range of real-world problems, including materials and drug discovery. Indeed, they are desirable since they can adaptively gather information to make better decisions in the future, resulting in data-efficient workflows. In scientific discovery, where experiments are costly, these algorithms can thus significantly reduce the cost of experimentation. Theoretical analyses of these algorithms are crucial for understanding their behavior and providing valuable insights for developing next-generation algorithms. However, theoretical analyses in the literature are often inaccessible to non-experts. This monograph aims to provide an accessible, self-contained introduction to the theoretical analysis of commonly used probabilistic decision-making algorithms, including bandit algorithms, Bayesian optimization, and tree search algorithms. Only basic knowledge of probability theory and statistics, along with some elementary knowledge about Gaussian processes, is assumed. |
| 2025-08-29 | [Universal Precision Limits in General Open Quantum Systems](http://arxiv.org/abs/2508.21567v1) | Tan Van Vu, Ryotaro Honma et al. | The intuition that the precision of observables is constrained by thermodynamic costs has recently been formalized through thermodynamic and kinetic uncertainty relations. While such trade-offs have been extensively studied in Markovian systems, corresponding constraints in the non-Markovian regime remain largely unexplored. In this Letter, we derive universal bounds on the precision of generic observables in open quantum systems coupled to environments of arbitrary strength and subjected to two-point measurements. By introducing an asymmetry term that quantifies the disparity between forward and backward processes, we show that the relative fluctuation of any time-antisymmetric current is constrained by both entropy production and this forward-backward asymmetry. For general observables, we prove that their relative fluctuation is always bounded from below by a generalized activity term. These results establish a comprehensive framework for understanding precision limits in broad classes of general open quantum systems. |
| 2025-08-29 | [Nuclear suppression in diffractive vector meson production within the color glass condensate framework](http://arxiv.org/abs/2508.21562v1) | Heikki MÃ¤ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions within a Color Glass Condensate based framework. Using data from HERA and the LHC, we find that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ observables is challenging. Introducing a global $K$-factor to account for theoretical uncertainties improves the agreement with data and enhances the framework's predictive power. We present predictions for integrated $\mathrm{J}/\psi$ cross sections at different photon-nucleus energies and study their $A$-dependence relative to a no-saturation baseline, quantifying nuclear suppression and providing insights into the onset of saturation effects. |
| 2025-08-29 | [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](http://arxiv.org/abs/2508.21550v1) | Yujin Park, Haejun Chung et al. | Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking. |
| 2025-08-29 | [Adaptive extended Kalman filter and laser link acquisition in the detection of gravitational waves in space](http://arxiv.org/abs/2508.21538v1) | Jinke Yang, Yong Xie et al. | An alternative, new laser link acquisition scheme for the triangular constellation of spacecraft (SCs) in deep space in the detection of gravitational waves is considered. In place of a wide field CCD camera in the initial stage of laser link acquisition adopted in the conventional scheme, an extended Kalman filter based on precision orbit determination is incorporated in the point ahead angle mechanism (PAAM) to steer the laser beam in such a way to narrow the uncertainty cone and at the same time avoids the heating problem generated by the CCD camera.A quadrant photodetector (QPD) based on the Differential Power Sensing (DPS) technique, which offers a higher dynamic range than differential wavefront sensing (DWS), is employed as the readout of the laser beam spot. The conventional two stages (coarse acquisition and fine acquisition) are integrated into a single control loop. The payload structure of the ATP control loop is simplified and numerical simulations, based on a colored measurement noise model that closely mimics the prospective on-orbit conditions, demonstrate that the AEKF significantly reduces the initial uncertainty region by predicting the point ahead angle (PAA) even when the worst case scenario in SC position (navigation) error is considered. |
| 2025-08-28 | [Rapid Mismatch Estimation via Neural Network Informed Variational Inference](http://arxiv.org/abs/2508.21007v1) | Mateusz Jaszczuk, Nadia Figueroa | With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming-based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system. |
| 2025-08-28 | [Practical indistinguishability in a gene regulatory network inference problem, a case study](http://arxiv.org/abs/2508.21006v1) | Cody E. FitzGerald, Shelley Reich et al. | Computationally inferring mechanistic insights from typical biological data is a challenging pursuit. Even the highest-quality experimental data come with challenges. There are always sources of noise, a limit to how often we can measure the system, and we can rarely measure all the relevant states that participate in the underlying complexity. There are usually sources of uncertainty in model development, which give rise to multiple competing model structures. To underscore the need for further analysis of structural uncertainty in modeling, we use a meta-analysis across six journals covering mathematical biology and show that a huge number of models for biological systems are developed each year, but model selection and comparison across model structures appear to be less common. We walk through a case study involving inference of regulatory network structure involved in a developmental decision in the nematode, \textit{Pristonchus pacificus}. We use real biological data and compare across 13,824 models--each corresponding to a different regulatory network structure, to determine which regulatory features are supported by the data across three experimental conditions. We find that the best-fitting models for each experimental condition share a combination of features and identify a regulatory network that is common across the model sets for each condition. This model can describe the data across the experimental conditions we considered and exhibits a high degree of positive regulation and interconnectivity between the key regulators, \textit{eud-1}, $textit{sult-1}, and \textit{nhr-40}. While the biological results are specific to the molecular biology of development in \textit{Pristonchus pacificus}, the general modeling framework and underlying challenges we faced doing this analysis are widespread across biology, chemistry, physics, and many other scientific disciplines. |
| 2025-08-28 | [Bottomonium transport in a strongly coupled quark-gluon plasma](http://arxiv.org/abs/2508.20995v1) | Biaogang Wu, Ralf Rapp | Quarkonium production in high-energy heavy-ion collisions remains a key probe of the quark-gluon plasma formed in these reactions, but the development of a fully integrated nonperturbative approach remains a challenge. Toward this end, we set up a semiclassical transport approach that combines nonperturbative reaction rates rooted in lattice-constrained $T$-matrix interactions with a viscous hydrodynamic medium evolution. Bottomonium suppression is computed along trajectories in the hydrodynamic evolution while regeneration is evaluated via a rate equation extended to a medium with spatial gradients. The much larger reaction rates compared to previous calculations markedly enhance both dissociation and regeneration processes. This, in particular, requires a reliable assessment of bottomonium equilibrium limits and of the non-thermal distributions of the bottom quarks transported through the expanding medium. Within current uncertainties our approach can describe the centrality dependence of bottomonium yields measured in Pb-Pb ($\sqrt{s_{_{\rm NN}}}$=5.02\,TeV) collisions at the LHC, while discrepancies are found at large transverse momenta. |
| 2025-08-28 | [Polynomial Chaos Expansion for Operator Learning](http://arxiv.org/abs/2508.20886v1) | Himanshu Sharma, LukÃ¡Å¡ NovÃ¡k et al. | Operator learning (OL) has emerged as a powerful tool in scientific machine learning (SciML) for approximating mappings between infinite-dimensional functional spaces. One of its main applications is learning the solution operator of partial differential equations (PDEs). While much of the progress in this area has been driven by deep neural network-based approaches such as Deep Operator Networks (DeepONet) and Fourier Neural Operator (FNO), recent work has begun to explore traditional machine learning methods for OL. In this work, we introduce polynomial chaos expansion (PCE) as an OL method. PCE has been widely used for uncertainty quantification (UQ) and has recently gained attention in the context of SciML. For OL, we establish a mathematical framework that enables PCE to approximate operators in both purely data-driven and physics-informed settings. The proposed framework reduces the task of learning the operator to solving a system of equations for the PCE coefficients. Moreover, the framework provides UQ by simply post-processing the PCE coefficients, without any additional computational cost. We apply the proposed method to a diverse set of PDE problems to demonstrate its capabilities. Numerical results demonstrate the strong performance of the proposed method in both OL and UQ tasks, achieving excellent numerical accuracy and computational efficiency. |
| 2025-08-28 | [Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting](http://arxiv.org/abs/2508.20812v1) | Lorenzo Busellato, Federico Cunico et al. | To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art. |
| 2025-08-28 | [The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation](http://arxiv.org/abs/2508.20806v1) | Moriba Jah, Van Haslett | Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified. |
| 2025-08-28 | [Time Series Embedding and Combination of Forecasts: A Reinforcement Learning Approach](http://arxiv.org/abs/2508.20795v1) | Marcelo C. Medeiros, Jeronymo M. Pinro | The forecasting combination puzzle is a well-known phenomenon in forecasting literature, stressing the challenge of outperforming the simple average when aggregating forecasts from diverse methods. This study proposes a Reinforcement Learning - based framework as a dynamic model selection approach to address this puzzle. Our framework is evaluated through extensive forecasting exercises using simulated and real data. Specifically, we analyze the M4 Competition dataset and the Survey of Professional Forecasters (SPF). This research introduces an adaptable methodology for selecting and combining forecasts under uncertainty, offering a promising advancement in resolving the forecasting combination puzzle. |
| 2025-08-28 | [Surfel-based 3D Registration with Equivariant SE(3) Features](http://arxiv.org/abs/2508.20789v1) | Xueyang Kang, Hang Zhao et al. | Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods. |
| 2025-08-28 | [A predictive solution of the EPR paradox](http://arxiv.org/abs/2508.20788v1) | Henryk Gzyl | In this work an incorrect argument in EPR's paper is corrected. A predictive approach to further confirm the validity of quantum theory is also proposed. The essence of the detail that EPR missed is that in a state of given total momentum (in their example the total momentum is zero), since the total momentum operator $\hat{\bp}=\hat{\bp}_1+\hat{\bp}_2$ does not commute with any of the position operators $\hat{\bx}_1$ and $\hat{\bx}_2,$ then in an eigenstate of the total momentum operator, the standard deviation on the measurement of the position of any of the two particles has to be infinite. Below we compute the (quantum) conditional expectation of the momentum of any of the particles (say, the first) given the value of a measurement of the total momentum. Since the predictor of the momentum of the particle depends on the total momentum, and the total momentum does not commute with the position operator of any of the particles, the uncertainty principle is not violated, and no contradiction appears. We also prove, using the basic tenets of quantum measurement theory, that after measurement of the total momentum, the probability density (computed as the absolute square of the wave function), coincides with the (quantum) conditional density given the total momentum. |
| 2025-08-28 | [Update of the CODE catalogue and some aspects of the dynamical status of Oort Cloud comets](http://arxiv.org/abs/2508.20780v1) | Piotr A. DybczyÅ„ski, MaÅ‚gorzata KrÃ³likowska | Context. The outer Solar System is believed to host a vast reservoir of long-period comets (LPCs), but our understanding of their spatial distribution and dynamical history remains limited due to observational biases and uncertainties in orbital solutions for really observed comets. Aims. We aim to provide a comprehensive and dynamically homogeneous orbital database of LPCs to support the study of their origin, evolution, dynamical status, and 6D distribution of orbital elements. Methods. We updated the Catalogue of Cometary Orbits and their Dynamical Evolution (CODE catalogue) by computing original and future barycentric orbits, orbital parameters at previous and next perihelion, using full Monte Carlo swarms of real comets for the uncertainties estimation and taking into account the planetary, Galactic and passing stars perturbations according to the latest data and algorithms. Results. This update of the CODE catalogue focuses on the dynamical status of near-parabolic comets. Using current stellar data, we formulate new constraints for dynamically new comets. Now, the CODE database includes 983 orbital solutions for 369 comets with full uncertainty estimates and dynamical classifications, covering nearly all comets with original semi-major axes exceeding 10,000 au and discovered before 2022, as well as all LPCs discovered beyond 10 au from the Sun during during this period, and over 80% of the known LPCs with perihelion distances beyond 7 au. |
| 2025-08-27 | [A Partially Derivative-Free Proximal Method for Composite Multiobjective Optimization in the HÃ¶lder Setting](http://arxiv.org/abs/2508.20071v1) | V. S. Amaral, P. B. AssunÃ§Ã£o et al. | This paper presents an algorithm for solving multiobjective optimization problems involving composite functions, where we minimize a quadratic model that approximates $F(x) - F(x^k)$ and that can be derivative-free. We establish theoretical assumptions about the component functions of the composition and provide comprehensive convergence and complexity analysis. Specifically, we prove that the proposed method converges to a weakly $\varepsilon$-approximate Pareto point in at most $\mathcal{O}\left(\varepsilon^{-\frac{\beta+1}{\beta}}\right)$ iterations, where $\beta$ denotes the H\"{o}lder exponent of the gradient. The algorithm incorporates gradient approximations and a scaling matrix $B_k$ to achieve an optimal balance between computational accuracy and efficiency. Numerical experiments on robust biobjective instances with Lipschitz and H\"{o}lder-gradient components illustrate the method's behavior. In these tests, the proposed approach was able to approximate the Pareto front under different levels of uncertainty and consistently recovered distinct solutions, even in challenging cases where the objectives have only H\"{o}lder continuous gradients. |
| 2025-08-27 | [Neural Conditional Simulation for Complex Spatial Processes](http://arxiv.org/abs/2508.20067v1) | Julia Walchessen, Andrew Zammit-Mangion et al. | A key objective in spatial statistics is to simulate from the distribution of a spatial process at a selection of unobserved locations conditional on observations (i.e., a predictive distribution) to enable spatial prediction and uncertainty quantification. However, exact conditional simulation from this predictive distribution is intractable or inefficient for many spatial process models. In this paper, we propose neural conditional simulation (NCS), a general method for spatial conditional simulation that is based on neural diffusion models. Specifically, using spatial masks, we implement a conditional score-based diffusion model that evolves Gaussian noise into samples from a predictive distribution when given a partially observed spatial field and spatial process parameters as inputs. The diffusion model relies on a neural network that only requires unconditional samples from the spatial process for training. Once trained, the diffusion model is amortized with respect to the observations in the partially observed field, the number and locations of those observations, and the spatial process parameters, and can therefore be used to conditionally simulate from a broad class of predictive distributions without retraining the neural network. We assess the NCS-generated simulations against simulations from the true conditional distribution of a Gaussian process model, and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick process model for spatial extremes. In the latter case, we show that it is more efficient and accurate to conditionally simulate using NCS than classical MCMC techniques implemented in standard software. We conclude that NCS enables efficient and accurate conditional simulation from spatial predictive distributions that are challenging to sample from using traditional methods. |
| 2025-08-27 | [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](http://arxiv.org/abs/2508.20066v1) | Zheng Li, Yanming Guo et al. | Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios. |
| 2025-08-27 | [Robust Paths: Geometry and Computation](http://arxiv.org/abs/2508.20039v1) | Hao Hao, Peter Zhang | Applying robust optimization often requires selecting an appropriate uncertainty set both in shape and size, a choice that directly affects the trade-off between average-case and worst-case performances. In practice, this calibration is usually done via trial-and-error: solving the robust optimization problem many times with different uncertainty set shapes and sizes, and examining their performance trade-off. This process is computationally expensive and ad hoc. In this work, we take a principled approach to study this issue for robust optimization problems with linear objective functions, convex feasible regions, and convex uncertainty sets. We introduce and study what we define as the robust path: a set of robust solutions obtained by varying the uncertainty set's parameters. Our central geometric insight is that a robust path can be characterized as a Bregman projection of a curve (whose geometry is defined by the uncertainty set) onto the feasible region. This leads to a surprising discovery that the robust path can be approximated via the trajectories of standard optimization algorithms, such as the proximal point method, of the deterministic counterpart problem. We give a sharp approximation error bound and show it depends on the geometry of the feasible region and the uncertainty set. We also illustrate two special cases where the approximation error is zero: the feasible region is polyhedrally monotone (e.g., a simplex feasible region under an ellipsoidal uncertainty set), or the feasible region and the uncertainty set follow a dual relationship. We demonstrate the practical impact of this approach in two settings: portfolio optimization and adversarial deep learning. |
| 2025-08-27 | [Pruning Strategies for Backdoor Defense in LLMs](http://arxiv.org/abs/2508.20032v1) | Santosh Chapagain, Shah Muhammad Hamdi et al. | Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks. |
| 2025-08-27 | [Tip of the Red Giant Branch Distances to NGC 1316, NGC 1380, NGC 1404, & NGC 4457: A Pilot Study of a Parallel Distance Ladder Using Type Ia Supernovae in Early-Type Host Galaxies](http://arxiv.org/abs/2508.20023v1) | Max J. B. Newman, Conor Larison et al. | Though type-Ia supernovae (SNe Ia) are found in all types of galaxies, recent local Hubble constant measurements have disfavored using SNe Ia in early-type or quiescent galaxies, aiming instead for better consistency with SNe Ia in star-forming, late-type host galaxies calibrated by Cepheid distances. Here we investigate the feasibility of a parallel distance ladder using SNe Ia exclusively in quiescent, massive ($\log M_*/M_{\odot} \geq 10$) host galaxies, calibrated by tip of the red giant branch (TRGB) distances. We present TRGB measurements to four galaxies: three measured from the Hubble Space Telescope with the ACS F814W filter, and one measured from the JWST NIRCam F090W filter. Combined with literature measurements, we define a TRGB calibrator sample of five high-mass, early-type galaxies that hosted well-measured SNe Ia: NGC 1316 (SN 2006dd), NGC 1380 (SN 1992A), NGC 1404 (SN 2007on, SN 2011iv), NGC 4457 (SN 2020nvb), and NGC 4636 (SN 2020ue). We jointly standardize these calibrators with a fiducial sample of 124 Hubble-flow SNe Ia from the Zwicky Transient Facility that are matched in host-galaxy and light-curve properties. Our results with this homogenized subsample show a Hubble residual scatter of under 0.11 mag, lower than usually observed in cosmological samples of the full SN~Ia distribution. We obtain a measurement of the Hubble constant, $H_0 = 75.3 \pm 2.9$ km s$^{-1}$ Mpc$^{-1}$, including statistical and estimated systematic uncertainties, and discuss the potential to further improve the precision of this approach. As calibrator and supernova samples grow, we advocate that future cosmological applications of SNe Ia use subsamples matched in host-galaxy and supernova properties across redshift. |
| 2025-08-27 | [Communicating astrobiology and the search for life elsewhere: speculations and promises of a developing scientific field in newspapers, press releases and papers](http://arxiv.org/abs/2508.19984v1) | Danilo Albergaria, Pedro Russo et al. | This study examines the communication of astrobiology and the Search for Life Elsewhere (SLE) in academic papers, press releases, and news articles over three decades. Through a quantitative content analysis, it investigates the prevalence of speculations and promises/expectations in these sources, aiming to understand how research results are portrayed and their potential impact on public perception and future research directions. Findings reveal that speculations and promises/expectations are more frequent in news articles and press releases compared to academic papers. Speculations about conditions for life and the existence of life beyond Earth are common, particularly in news articles covering exoplanet research, while promises of life detection are rare. Press releases tend to emphasize the significance of research findings and the progress of the field. Speculations and promises/expectations in news articles often occur without attribution to scientists and in quotes of authors of the studies, and slightly less so in quotes of outside experts. The study highlights the complex dynamics of science communication in astrobiology, where speculations and promises can generate public excitement and influence research funding, but also risk misrepresenting scientific uncertainty and creating unrealistic expectations. It underscores the need for responsible communication practices that acknowledge the speculative dimension of the field while fostering public engagement and informed decision-making. |
| 2025-08-27 | [Comment on GarcÃ­a-Donato et al. (2025) "Model uncertainty and missing data: An objective Bayesian perspective"](http://arxiv.org/abs/2508.19939v1) | Joris Mulder | Garcia-Donato et al. (2025) present a methodology for handling missing data in a model selection problem using an objective Bayesian approach. The current comment discusses an alternative, existing objective Bayesian method for this problem. First, rather than using the g prior, O'Hagan's fractional Bayes factor (O'Hagan, 1995) is utilized based on a minimal fraction. Second, and more importantly due to the focus on missing data, Rubin's rules for multiple imputation can directly be used as the fractional Bayes factor can be written as a Savage-Dickey density ratio for a variable selection problem. The current comment derives the methodology for a variable selection problem. Moreover, its implied behavior is illustrated in a numerical experiment, showing competitive results as the method of Garcia-Donato et al. (2025). |
| 2025-08-27 | [Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition](http://arxiv.org/abs/2508.19933v1) | Sten Elling Tingstad Jacobsen, BalÃ¡zs KulcsÃ¡r et al. | The electrification and automation of mobility are reshaping how cities operate on-demand transport systems. Managing Electric Autonomous Mobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch, rebalancing, and charging decisions under multiple uncertainties, including travel demand, travel time, energy consumption, and charger availability. We address this challenge with a combined stochastic and robust model predictive control (MPC) framework. The framework integrates spatio-temporal Bayesian neural network forecasts with a multi-stage stochastic optimization model, formulated as a large-scale mixed-integer linear program. To ensure real-time applicability, we develop a tailored Nested Benders Decomposition that exploits the scenario tree structure and enables efficient parallelized solution. Stochastic optimization is employed to anticipate demand and infrastructure variability, while robust constraints on energy consumption and travel times safeguard feasibility under worst-case realizations. We evaluate the framework using high-fidelity simulations of San Francisco and Chicago. Compared with deterministic, reactive, and robust baselines, the combined stochastic and robust approach reduces median passenger waiting times by up to 36% and 95th-percentile delays by nearly 20%, while also lowering rebalancing distance by 27% and electricity costs by more than 35%. We also conduct a sensitivity analysis of battery size and vehicle efficiency, finding that energy-efficient vehicles maintain stable performance even with small batteries, whereas less efficient vehicles require larger batteries and greater infrastructure support. Our results emphasize the importance of jointly optimizing predictive control, vehicle capabilities, and infrastructure planning to enable scalable, cost-efficient EAMoD operations. |
| 2025-08-27 | [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](http://arxiv.org/abs/2508.19909v1) | Lechun You, Zhonghua Wu et al. | Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation. |
| 2025-08-26 | [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](http://arxiv.org/abs/2508.19191v1) | Yue Wang, Wenjie Deng et al. | Intraocular foreign body removal demands millimeter-level precision in confined intraocular spaces, yet existing robotic systems predominantly rely on manual teleoperation with steep learning curves. To address the challenges of autonomous manipulation (particularly kinematic uncertainties from variable motion scaling and variation of the Remote Center of Motion (RCM) point), we propose AutoRing, an imitation learning framework for autonomous intraocular foreign body ring manipulation. Our approach integrates dynamic RCM calibration to resolve coordinate-system inconsistencies caused by intraocular instrument variation and introduces the RCM-ACT architecture, which combines action-chunking transformers with real-time kinematic realignment. Trained solely on stereo visual data and instrument kinematics from expert demonstrations in a biomimetic eye model, AutoRing successfully completes ring grasping and positioning tasks without explicit depth sensing. Experimental validation demonstrates end-to-end autonomy under uncalibrated microscopy conditions. The results provide a viable framework for developing intelligent eye-surgical systems capable of complex intraocular procedures. |
| 2025-08-26 | [Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions](http://arxiv.org/abs/2508.19159v1) | Ersin Das, Rahal Nanayakkara et al. | Measurements and state estimates are often imperfect in control practice, posing challenges for safety-critical applications, where safety guarantees rely on accurate state information. In the presence of estimation errors, several prior robust control barrier function (R-CBF) formulations have imposed strict conditions on the input. These methods can be overly conservative and can introduce issues such as infeasibility, high control effort, etc. This work proposes a systematic method to improve R-CBFs, and demonstrates its advantages on a tracked vehicle that navigates among multiple obstacles. A primary contribution is a new optimization-based online parameter adaptation scheme that reduces the conservativeness of existing R-CBFs. In order to reduce the complexity of the parameter optimization, we merge several safety constraints into one unified numerical CBF via Poisson's equation. We further address the dual relative degree issue that typically causes difficulty in vehicle tracking. Experimental trials demonstrate the overall performance improvement of our approach over existing formulations. |
| 2025-08-26 | [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](http://arxiv.org/abs/2508.19150v1) | Juan Carlos SaborÃ­o, Marc Vinci et al. | Purposeful behavior in robotic assistants requires the integration of multiple components and technological advances. Often, the problem is reduced to recognizing explicit prompts, which limits autonomy, or is oversimplified through assumptions such as near-perfect information. We argue that a critical gap remains unaddressed -- specifically, the challenge of reasoning about the uncertain outcomes and perception errors inherent to human intention recognition. In response, we present a framework designed to be resilient to uncertainty and sensor noise, integrating real-time sensor data with a combination of planners. Centered around an intention-recognition POMDP, our approach addresses cooperative planning and acting under uncertainty. Our integrated framework has been successfully tested on a physical robot with promising results. |
| 2025-08-26 | [VPPE: Application of Scaled Vecchia Approximations to Parallel Partial Emulation](http://arxiv.org/abs/2508.19144v1) | Josh Seidman, Elaine T. Spiller | Computer models or simulators are widely used across scientific fields, but are computationally expensive limiting their use to explore possible scenarios/outcomes. Gaussian process emulators are statistical surrogates that can rapidly approximate the outputs of computer models at untested inputs and enable uncertainty quantification studies. The parallel partial emulation (PPE) was developed to model simulators with vector-valued outputs. While the PPE is adept at fitting simulator data with multidimensional outputs, the time to fit the PPE increases quickly as the number of training runs increases. The Scaled Vecchia approximation, a fast approximation to multivariate Gaussian likelihoods, makes fitting Gaussian process emulators with large training datasets tractable. Here we introduce the Vecchia Parallel Partial Emulation (VPPE) that utilizes the Scaled Vecchia approximation within the PPE framework to allow for parallel partial emulation with larger training datasets. The VPPE is applied to three computer experiments, a synthetic data set, a hydrology model, and a volcanic flow model, yielding comparable predictive accuracy to the PPE at a fraction of the runtime. |
| 2025-08-26 | [Using Machine Learning to Generate, Clarify, and Improve Economic Models](http://arxiv.org/abs/2508.19136v1) | Annie Liang | Machine learning algorithms can now outperform classic economic models in predicting quantities ranging from bargaining outcomes, to choice under uncertainty, to an individual's future jobs and wages. Yet this predictive accuracy comes at a cost: most machine learning algorithms function as black boxes, offering little insight into \emph{why} outcomes occur. This article asks whether machine learning can guide the development of new economic theories.   Economic models serve an important purpose beyond prediction -- they uncover the general mechanisms behind observed behaviors. A model that identifies the causal pathways of economic development is more valuable than one that merely predicts which countries will escape poverty, because it enables policymakers to encourage that development in countries where it might not have happened otherwise. Similarly, a model that predicts imperfectly across many domains can be more valuable than one that is highly accurate in a specific domain, since the former allows insights and data obtained from one setting to inform decisions and policy in another.   Applying machine learning algorithms off-the-shelf is unlikely to yield such models. But recent work shows that, when reconceived with the aims of an economic modeler in mind, machine learning methods can improve both prediction and understanding. These approaches range from adversarially training algorithms to expose the limits of existing models, to imposing economic theory as a constraint on algorithmic search. Advances in large language models complement these strategies and open new research directions. |
| 2025-08-26 | [Trustworthy Agents for Electronic Health Records through Confidence Estimation](http://arxiv.org/abs/2508.19096v1) | Yongwoo Song, Minbyul Jeong et al. | Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low. |
| 2025-08-26 | [Measurement of the branching fraction of $\psip \to Ï‰Î·Î·$](http://arxiv.org/abs/2508.19092v1) | BESIII Collaboration, M. Ablikim et al. | Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to \omega \eta \eta $ is observed for the first time. The branching fraction of the $\psi(3686)\to\omega\eta\eta$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $\omega(1420)$ and $f_{0}(1710)$ resonances are observed in the $\omega\eta$ and $\eta\eta$ invariant-mass spectra, respectively. |
| 2025-08-26 | [SN2023syz and SN2025cbj: Two Type IIn Supernovae Associated with IceCube High-energy Neutrinos](http://arxiv.org/abs/2508.19080v1) | Ming-Xuan Lu, Yun-Feng Liang et al. | Type IIn supernovae (SNe IIn) are a subclass of core-collapse SNe in which strong interactions occur between the ejecta and dense circumstellar material, creating ideal conditions for the production of high-energy neutrinos. This makes them promising candidate sources of neutrinos. In this work, we conduct an association study between 163 SNe IIn observed by the Zwicky Transient Facility and 138 neutrino alert events detected by the IceCube neutrino observatory. After excluding alerts with poor localization, we find two SNe that are spatiotemporally coincident with neutrino events. IC231027A and IC250421A coincide with the positions of SN2023syz and SN2025cbj, respectively, within their localization uncertainties, and the neutrino arrival times are delayed by 38 days and 61 days relative to the discovery times of the corresponding SNe. Using Monte Carlo simulations, we estimate that the probability of such two coincidences occurring by chance in our sample is $p \sim 0.67\%$, suggesting a high likelihood that they arise from genuine associations, though the result is not yet statistical significant. Furthermore, model calculations show that the expected numbers of neutrino events from these SNe IIn could be consistent with the actual observations. Our study provides possible evidence that interacting SNe may be potential neutrino-emitting sources. |
| 2025-08-26 | [A Quick Estimation of FrÃ©chet Quantizers for a Dynamic Solution to Flood Risk Management Problems](http://arxiv.org/abs/2508.19045v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization is a well-known quantitative tool for decision-making under uncertainty. It is broadly used in financial and investment planning, inventory control, and also natural disaster risk management. Theoretical solutions of multi-stage stochastic programs can be found explicitly only in very exceptional cases due to their variational form and interdependency of uncertainty in time. Nevertheless, numerical solutions are often inaccurate, as they rely on Monte-Carlo sampling, which requires the Law of Large Numbers to hold for the approximation quality. In this article, we introduce a new approximation scheme, which computes and groups together stage-wise optimal quantizers of conditional Fr\'echet distributions for optimal weighting of value functions in the dynamic programming. We consider optimality of scenario quantization methods in the sense of minimal Kantorovich-Wasserstein distance at each stage of the scenario tree. By this, we bound the approximation error with convergence guarantees. We also provide global solution guarantees under convexity and monotonicity conditions on the value function. We apply the developed methods to the governmental budget allocation problem for risk management of flood events in Austria. For this, we propose an extremely efficient way to approximate optimal quantizers for conditional Fr\'echet distributions. Our approach allows to enhance the overall efficiency of dynamic programming via the use of different parameter estimation methods for different groups of quantizers. The groups are distinguished by a particular risk threshold and are able to differentiate between higher- and lower-impact flood events. |
| 2025-08-26 | [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](http://arxiv.org/abs/2508.18937v1) | Wang Jiayin, Wei Yanran et al. | Autonomous control of the laparoscope in robot-assisted Minimally Invasive Surgery (MIS) has received considerable research interest due to its potential to improve surgical safety. Despite progress in pixel-level Image-Based Visual Servoing (IBVS) control, the requirement of continuous visibility and the existence of complex disturbances, such as parameterization error, measurement noise, and uncertainties of payloads, could degrade the surgeon's visual experience and compromise procedural safety. To address these limitations, this paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and uncertainty-adaptive framework for autonomous laparoscope control that guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian Process Regression (GPR) is utilized to perform hybrid (deterministic + stochastic) quantification of operational uncertainties including residual model uncertainties, stochastic uncertainties, and external disturbances. Based on uncertainty quantification, a novel safety aware trajectory optimization framework with probabilistic guarantees is proposed, where a uncertainty-adaptive safety Control Barrier Function (CBF) condition is given based on uncertainty propagation, and chance constraints are simultaneously formulated based on probabilistic approximation. This uncertainty aware formulation enables adaptive control effort allocation, minimizing unnecessary camera motion while maintaining robustness. The proposed method is validated through comparative simulations and experiments on a commercial surgical robot platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph node dissection. Compared with baseline methods, the framework maintains near-perfect target visibility (>99.9%), reduces tracking e |
| 2025-08-25 | [Flight-Ready Precise and Robust Carrier-Phase GNSS Navigation Software for Distributed Space Systems](http://arxiv.org/abs/2508.18246v1) | Samuel Y. W. Low, Toby Bell et al. | This paper presents the full requirements analysis, design, development, and testing of high-precision navigation flight software for Distributed Space Systems (DSS) using Carrier Phase Differential GNSS (CDGNSS). Five main contributions are made. First, a survey of flown and upcoming DSS missions with stringent precision requirements is conducted, from which a thorough requirements analysis is distilled to guide development and testing. Second, a real-time navigation functional architecture is designed, and adopts a sparse and regularized Consider Kalman Filter with options for numerical stability in-flight. The filter rigorously accounts for uncertainties in process noise, measurement noise, and biases. It tracks float ambiguities with integer resolution where possible. The covariance correlation structure is preserved under all navigation modes, including contingencies and outages. Third, a lightweight, memoryless Fault Detection, Isolation, and Recovery (FDIR) module is developed to guard against anomalous measurements, providing statistical screening and ensuring robust navigation. Fourth, the software architecture is proposed for ease of integration, with strategies presented for modularity and computational efficiency tailored to constrained flight systems. Fifth, a comprehensive test campaign is conducted, mapped to a requirements verification matrix, spanning unit, interface, software-in-the-loop, and real-time hardware-in-the-loop tests, emphasizing gradual test fidelity for efficient fault isolation. Finally, flight-like results are demonstrated using the VISORS mission, due to the generalizability of the VISORS navigation operations, and the stringency which demands sub-centimeter relative position and sub-millimeter-per-second velocity accuracy. This architecture aims to serve as a reference for next-generation DSS missions adopting CDGNSS. |
| 2025-08-25 | [Uncertain data assimilation for urban wind flow simulations with OpenLB-UQ](http://arxiv.org/abs/2508.18202v1) | Mingliang Zhong, Dennis Teutscher et al. | Accurate prediction of urban wind flow is essential for urban planning, pedestrian safety, and environmental management. Yet, it remains challenging due to uncertain boundary conditions and the high cost of conventional CFD simulations. This paper presents the use of the modular and efficient uncertainty quantification (UQ) framework OpenLB-UQ for urban wind flow simulations. We specifically use the lattice Boltzmann method (LBM) coupled with a stochastic collocation (SC) approach based on generalized polynomial chaos (gPC). The framework introduces a relative-error noise model for inflow wind speeds based on real measurements. The model is propagated through a non-intrusive SC LBM pipeline using sparse-grid quadrature. Key quantities of interest, including mean flow fields, standard deviations, and vertical profiles with confidence intervals, are efficiently computed without altering the underlying deterministic solver. We demonstrate this on a real urban scenario, highlighting how uncertainty localizes in complex flow regions such as wakes and shear layers. The results show that the SC LBM approach provides accurate, uncertainty-aware predictions with significant computational efficiency, making OpenLB-UQ a practical tool for real-time urban wind analysis. |
| 2025-08-25 | [New shell-model calculations of the $Î´_C$ correction to superallowed $0^+\rightarrow0^+$ nuclear $Î²$ decay and standard-model implications](http://arxiv.org/abs/2508.18189v1) | L. Xayavong, N. A. Smirnova et al. | Refined calculations of the radial mismatch correction, $\delta_{C2}$, to superallowed $0^+\rightarrow0^+$ nuclear $\beta$ decay are performed using the shell model with realistic Woods-Saxon radial wave functions. Two important improvements are introduced: i) charge radii used to constrain the length parameter are evaluated within a generalized formula, where proton occupation numbers are substituted by sums of spectroscopic factors, while radial wave functions are required to match separation energies with respect to the intermediate $(A-1)$-nucleon states by adjusting parameters such as the potential depth; ii) configuration mixing wave functions and energies for many-particle states are obtained through the diagonalization of well-established effective interactions in large configuration spaces without truncation. Furthermore, a variation of $\pm0.1$\,fm in the surface diffuseness parameter is now incorporated as a source of uncertainty. The present results are generally in fairly good agreement with those from previous studies. As an exception, the $\delta_{C2}$ value obtained for $^{18}$Ne is smaller by approximately a factor of two, principally due to the updated charge-radius treatment. A reduction is also observed in most cases with $A\ge38$, through the deviations generally remain within the newly assigned error bars. The smaller isospin-mixing counterpart, $\delta_{C1}$, is strongly interaction-dependent, roughly following an inverse-square law with respect to the energy separation between the lowest admixed levels. Therefore, an additional procedure to ensure isobaric displacements within the isospin multiplets appears to be indispensable. Our results for $\delta_{C2}$ lead to a new averaged $\overline{\mathcal{F}t}$ value of $3073.11(99)_{stat}(36)_{\delta_R'}(173)_{\delta_{NS}}$~s with $\chi^2/\nu=0.624$. The corresponding $|V_{ud}|$ value is 0.97359(33). |
| 2025-08-25 | [Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization](http://arxiv.org/abs/2508.18170v1) | Jonathan A. Moran, Pablo G. Morato | Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%. |
| 2025-08-25 | [Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation](http://arxiv.org/abs/2508.18142v1) | Tianjun Wei, Huizhong Guo et al. | User simulation is increasingly vital to develop and evaluate recommender systems (RSs). While Large Language Models (LLMs) offer promising avenues to simulate user behavior, they often struggle with the absence of specific domain alignment required for RSs and the efficiency demands of large-scale simulation. A vast yet underutilized resource for enhancing this alignment is the extensive user feedback inherent in RSs. However, directly leveraging such feedback presents two significant challenges. First, user feedback in RSs is often ambiguous and noisy, which negatively impacts effective preference alignment. Second, the massive volume of feedback largely hinders the efficiency of preference alignment, necessitating an efficient filtering mechanism to identify more informative samples. To overcome these hurdles, we introduce a novel data construction framework that leverages user feedback in RSs with advanced LLM capabilities to generate high-quality simulation data. Our framework unfolds in two key phases: (1) employing LLMs to generate cognitive decision-making processes on constructed simulation samples, reducing ambiguity in raw user feedback; (2) data distillation based on uncertainty estimation and behavior sampling to filter challenging yet denoised simulation samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using such high-quality dataset with corresponding decision-making processes. Extensive experiments verify that our framework significantly boosts the alignment with human preferences and in-domain reasoning capabilities of fine-tuned LLMs, and provides more insightful and interpretable signals when interacting with RSs. We believe our work will advance the RS community and offer valuable insights for broader human-centric AI research. |
| 2025-08-25 | [Forward-Backward Quantization of Scenario Processes in Multi-Stage Stochastic Optimization](http://arxiv.org/abs/2508.18112v1) | Anna Timonina-Farkas | Multi-stage stochastic optimization lies at the core of decision-making under uncertainty. As the analytical solution is available only in exceptional cases, dynamic optimization aims to efficiently find approximations but often neglects non-Markovian time-interdependencies. Methods on scenario trees can represent such interdependencies but are subject to the curse of dimensionality. To ease this problem, researchers typically approximate the uncertainty by smaller but more accurate trees. In this article, we focus on multi-stage optimal tree quantization methods of time-interdependent stochastic processes, for which we develop novel bounds and demonstrate that the upper bound can be minimized via projected gradient descent incorporating the tree structure as linear constraints. Consequently, we propose an efficient quantization procedure, which improves forward-looking samples using a backward step on the tree.We apply the results to the multi-stage inventory control with time-interdependent demand. For the case with one product, we benchmark the approximation because the problem allows a solution in closed-form. For the multi-dimensional problem, our solution found by optimal discrete approximation demonstrates the importance of holding mitigation inventory in different phases of the product life cycle. |
| 2025-08-25 | [Measurement of the branching ratio of $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ isotopes through the nuclear muon capture reaction in the Super-Kamiokande detector](http://arxiv.org/abs/2508.18110v1) | Y. Maekawa, K. Abe et al. | The Super-Kamiokande detector has measured solar neutrinos for more than $25$ years. The sensitivity for solar neutrino measurement is limited by the uncertainties of energy scale and background modeling. Decays of unstable isotopes with relatively long half-lives through nuclear muon capture, such as $\mathrm{^{16}N}$, $\mathrm{^{15}C}$, $\mathrm{^{12}B}$ and $\mathrm{^{13}B}$, are detected as background events for solar neutrino observations. In this study, we developed a method to form a pair of stopping muon and decay candidate events and evaluated the production rates of such unstable isotopes. We then measured their branching ratios considering both their production rates and the estimated number of nuclear muon capture processes as $Br(\mathrm{^{16}N})=(9.0 \pm 0.1)\%$, $Br(\mathrm{^{15}C})=(0.6\pm0.1)\%$, $Br(\mathrm{^{12}B})=(0.98 \pm 0.18)\%$, $Br(\mathrm{^{13}B})=(0.14 \pm 0.12)\%$, respectively. The result for $\mathrm{^{16}N}$ has world-leading precision at present and the results for $\mathrm{^{15}C}$, $\mathrm{^{12}B}$, and $\mathrm{^{13}B}$ are the first branching ratio measurements for those isotopes. |
| 2025-08-25 | [The $Î›_{b} \to Î›$ transition form factors in perturbative QCD approach](http://arxiv.org/abs/2508.18069v1) | Lei Yang, Jia-Jie Han et al. | In this work, we investigate the $\Lambda_b \to \Lambda$ transition form factors in the perturbative QCD (PQCD) approach, incorporating higher-twist light-cone distribution amplitudes (LCDAs). The resulted form factors show that higher-twist LCDAs are dominant numerically. By combining our PQCD predictions at low-$q^2$ with lattice QCD results at high-$q^2$, $z$-series expansion fits are performed to parametrize the form factors over the full kinematic range. We also provide the prediction for physical observables in the rare decay $\Lambda_b \to \Lambda \mu^+ \mu^-$, including the differential branching fraction, dilepton longitudinal polarization fraction, and forward-backward asymmetries (lepton-side, hadron-side, and combined lepton-hadron). Our obtained form factors are consistent with those in other theoretical methods within the uncertainties. |
| 2025-08-25 | [Assessing the conditional calibration of interval forecasts using decompositions of the interval score](http://arxiv.org/abs/2508.18034v1) | Sam Allen, Julia Burnello et al. | Forecasts for uncertain future events should be probabilistic. Probabilistic forecasts are commonly issued as prediction intervals, which provide a measure of uncertainty in the unknown outcome whilst being easier to understand and communicate than full predictive distributions. The calibration of a $(1 - \alpha)$-level prediction interval can be assessed by checking whether the probability that the outcome falls within the interval is equal to $1 - \alpha$. However, such coverage checks are typically unconditional and therefore relatively weak. Although this is well known, there is a lack of methods to assess the conditional calibration of interval forecasts. In this work, we demonstrate how this can be achieved via decompositions of the well-known interval (or Winkler) score. We study notions of calibration for interval forecasts and then introduce a decomposition of the interval score based on isotonic distributional regression. This decomposition exhibits many desirable properties, both in theory and in practice, which allows users to accurately assess the conditional calibration of interval forecasts. This is illustrated on simulated data and in three applications to benchmark regression datasets. |
| 2025-08-25 | [Precision Measurements of the Electroweak Mixing Angle in the Region of the Z pole](http://arxiv.org/abs/2508.18022v1) | Arie Bodek, Hyon-San Seo et al. | We review the current status and techniques used in precision measurements of the effective leptonic weak mixing angle $\sin^2\theta^\ell_{\rm eff}$ (a fundamental parameter of the Standard Model (SM)) in the region of the Z pole with emphasis on hadron colliders. We also build on these techniques to extract the most precise single measurement to date of $\sin^2\theta^\ell_{\rm eff}$ from a new analysis of the published forward-backward asymmetry ($A_{\rm FB}$) in Drell-Yan dielpton production in proton-proton collisions at a center of mass energy of 13 TeV measured by the CMS collaboration at the large hadron collider. The uncertainty in $\sin^2\theta^\ell_{\rm eff}$ published by CMS is dominated by uncertainties in Parton Distribution Functions (PDFs), which are reduced by PDF profiling using the dilepton mass dependence of $A_{\rm FB}$. Our new extraction of $\sin^2\theta^\ell_{\rm eff}$ from the CMS values of $A_{\rm FB}$ includes profiling with additional new CMS measurements of the $W$-boson decay lepton asymmetry, and W/Z cross section ratio at 13 TeV. We obtain the most precise single measurement of $\sin^2\theta^\ell_{\rm eff}$ to date of 0.23153$\pm$0.00023, which is in excellent agreement with the SM prediction of 0.23161$\pm$0.00004. We also discuss outlook for future measurements at the LHC including more precise measurements of $\sin^2\theta^\ell_{\rm eff}$, a measurement of $\sin^2\theta^\ell_{\rm eff}$ for b-quarks in the initial state, and a measurement of the running of $\sin^2\theta^{\overline{\rm MS}}(\mu)$ up to 3 TeV. |
| 2025-08-22 | [A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer](http://arxiv.org/abs/2508.16569v1) | Yuhui Tao, Zhongwei Zhao et al. | The non-invasive assessment of increasingly incidentally discovered renal masses is a critical challenge in urologic oncology, where diagnostic uncertainty frequently leads to the overtreatment of benign or indolent tumors. In this study, we developed and validated RenalCLIP using a dataset of 27,866 CT scans from 8,809 patients across nine Chinese medical centers and the public TCIA cohort, a visual-language foundation model for characterization, diagnosis and prognosis of renal mass. The model was developed via a two-stage pre-training strategy that first enhances the image and text encoders with domain-specific knowledge before aligning them through a contrastive learning objective, to create robust representations for superior generalization and diagnostic precision. RenalCLIP achieved better performance and superior generalizability across 10 core tasks spanning the full clinical workflow of kidney cancer, including anatomical assessment, diagnostic classification, and survival prediction, compared with other state-of-the-art general-purpose CT foundation models. Especially, for complicated task like recurrence-free survival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726, representing a substantial improvement of approximately 20% over the leading baselines. Furthermore, RenalCLIP's pre-training imparted remarkable data efficiency; in the diagnostic classification task, it only needs 20% training data to achieve the peak performance of all baseline models even after they were fully fine-tuned on 100% of the data. Additionally, it achieved superior performance in report generation, image-text retrieval and zero-shot diagnosis tasks. Our findings establish that RenalCLIP provides a robust tool with the potential to enhance diagnostic accuracy, refine prognostic stratification, and personalize the management of patients with kidney cancer. |
| 2025-08-22 | [Exploring null-entropy events: What do we learn when nothing happens?](http://arxiv.org/abs/2508.16528v1) | Abhaya S. Hegde, AndrÃ© M. Timpanaro et al. | Fluctuation theorems establish that thermodynamic processes at the microscale can occasionally result in negative entropy production. At the microscale, another distinct possibility becomes more likely: processes where no entropy is produced overall. In this work, we explore the constraints imposed by such null-entropy events on the fluctuations of thermodynamic currents. By incorporating the probability of null-entropy events, we obtain tighter bounds on finite-time thermodynamic uncertainty relations derived from fluctuation theorems. We validate this framework using an example of a qudit SWAP engine. |
| 2025-08-22 | [Identifying Treatment Effect Heterogeneity with Bayesian Hierarchical Adjustable Random Partition in Adaptive Enrichment Trials](http://arxiv.org/abs/2508.16523v1) | Xianglin Zhao, Shirin Golchi et al. | Treatment effect heterogeneity refers to the systematic variation in treatment effects across subgroups. There is an increasing need for clinical trials that aim to investigate treatment effect heterogeneity and estimate subgroup-specific responses. While several statistical methods have been proposed to address this problem, existing partitioning-based methods often depend on auxiliary analysis, overlook model uncertainty, or impose inflexible borrowing strength. We propose the Bayesian Hierarchical Adjustable Random Partition (BHARP) model, a self-contained framework that applies a finite mixture model with an unknown number of components to explore the partition space accounting for model uncertainty. The BHARP model jointly estimates subgroup-specific effects and the heterogeneity patterns, and adjusts the borrowing strengths based on within-cluster cohesion without requiring manual calibration. Posterior sampling is performed via a custom reversible-jump Markov chain Monte Carlo sampler tailored to partitioning-based information borrowing in clinical trials. Simulation studies across a range of treatment effect heterogeneity patterns show that the BHARP model achieves better accuracy and precision compared to conventional and advanced methods. We showcase the utilities of the BHARP model in the context of a multi-arm adaptive enrichment trial investigating physical activity interventions in patients with type 2 diabetes. |
| 2025-08-22 | [Uncertainty Quantification and Propagation for ACORN, a geometric deep learning tracking pipeline for HEP experiments](http://arxiv.org/abs/2508.16518v1) | Lukas PÃ©ron, Paolo Calafiura et al. | We have developed an Uncertainty Quantification process for multistep pipelines and applied it to the ACORN particle tracking pipeline. All our experiments are made using the TrackML open dataset. Using the Monte Carlo Dropout method, we measure the data and model uncertainties of the pipeline steps, study how they propagate down the pipeline, and how they are impacted by the training dataset's size, the input data's geometry and physical properties. We will show that for our case study, as the training dataset grows, the overall uncertainty becomes dominated by aleatoric uncertainty, indicating that we had sufficient data to train the ACORN model we chose to its full potential. We show that the ACORN pipeline yields high confidence in the track reconstruction and does not suffer from the miscalibration of the GNN model. |
| 2025-08-22 | [Ensembles of Neural Surrogates for Parametric Sensitivity in Ocean Modeling](http://arxiv.org/abs/2508.16489v1) | Yixuan Sun, Romain Egele et al. | Accurate simulations of the oceans are crucial in understanding the Earth system. Despite their efficiency, simulations at lower resolutions must rely on various uncertain parameterizations to account for unresolved processes. However, model sensitivity to parameterizations is difficult to quantify, making it challenging to tune these parameterizations to reproduce observations. Deep learning surrogates have shown promise for efficient computation of the parametric sensitivities in the form of partial derivatives, but their reliability is difficult to evaluate without ground truth derivatives. In this work, we leverage large-scale hyperparameter search and ensemble learning to improve both forward predictions, autoregressive rollout, and backward adjoint sensitivity estimation. Particularly, the ensemble method provides epistemic uncertainty of function value predictions and their derivatives, providing improved reliability of the neural surrogates in decision making. |
| 2025-08-22 | [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](http://arxiv.org/abs/2508.16476v1) | Maryam Ghasemzadeh, Anton van Beek | Multi-objective Bayesian optimization (MOBO) struggles with sparse (non-space-filling), scarce (limited observations) datasets affected by experimental uncertainty, where identical inputs can yield varying outputs. These challenges are common in physical and simulation experiments (e.g., randomized medical trials and, molecular dynamics simulations) and are therefore incompatible with conventional MOBO methods. As a result, experimental resources are inefficiently allocated, leading to suboptimal designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data Trust Region-based Optimization Algorithm), a novel sampling framework that integrates prior knowledge of experimental uncertainty to construct more accurate surrogate models while employing trust regions to focus sampling on promising areas of the design space. By strategically leveraging prior information and refining search regions, NOSTRA accelerates convergence to the Pareto frontier, enhances data efficiency, and improves solution quality. Through two test functions with varying levels of experimental uncertainty, we demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse, and scarce data. Specifically, we illustrate that, NOSTRA effectively prioritizes regions where samples enhance the accuracy of the identified Pareto frontier, offering a resource-efficient algorithm that is practical in scenarios with limited experimental budgets while ensuring efficient performance. |
| 2025-08-22 | [Scalable Bayesian inference on high-dimensional multivariate linear regression](http://arxiv.org/abs/2508.16446v1) | Xuan Cao, Kyoungjae Lee | We consider jointly estimating the coefficient matrix and the error precision matrix in high-dimensional multivariate linear regression models. Bayesian methods in this context often face computational challenges, leading to previous approaches that either utilize a generalized likelihood without ensuring the positive definiteness of the precision matrix or rely on maximization algorithms targeting only the posterior mode, thus failing to address uncertainty. In this work, we propose two Bayesian methods: an exact method and an approximate two-step method. We first propose an exact method based on spike and slab priors for the coefficient matrix and DAG-Wishart prior for the error precision matrix, whose computational complexity is comparable to the state-of-the-art generalized likelihood-based Bayesian method. To further enhance scalability, a two-step approach is developed by ignoring the dependency structure among response variables. This method estimates the coefficient matrix first, followed by the calculation of the posterior of the error precision matrix based on the estimated errors. We validate the two-step method by demonstrating (i) selection consistency and posterior convergence rates for the coefficient matrix and (ii) selection consistency for the directed acyclic graph (DAG) of errors. We demonstrate the practical performance of proposed methods through synthetic and real data analysis. |
| 2025-08-22 | [Dynamic Financial Analysis (DFA) of General Insurers under Climate Change](http://arxiv.org/abs/2508.16444v1) | Benjamin Avanzi, Yanfeng Li et al. | Climate change is expected to significantly affect the physical, financial, and economic environments over the long term, posing risks to the financial health of general insurers. While general insurers typically use Dynamic Financial Analysis (DFA) for a comprehensive view of financial impacts, traditional DFA as presented in the literature does not consider the impact of climate change. To address this gap, we introduce a climate-dependent DFA approach that integrates climate risk into DFA, providing a holistic assessment of the long-term impact of climate change on the general insurance industry. The proposed framework has three key features. First, it captures the long-term impact of climate change on the assets and liabilities of general insurers by considering both physical and economic dimensions across different climate scenarios within an interconnected structure. Second, it addresses the uncertainty of climate change impacts using stochastic simulations within climate scenario analysis that are useful for actuarial applications. Finally, the framework is tailored to the general insurance sector by addressing its unique characteristics. To demonstrate the practical application of our model, we conduct an extensive empirical study using Australian data to assess the long-term financial impact of climate change on the general insurance market under various climate scenarios. The results show that the interaction between economic growth and physical risk plays a key role in shaping general insurers' risk-return profiles. Limitations of our framework are thoroughly discussed. |
| 2025-08-22 | [Cost-optimized replacement strategies for water electrolysis systems affected by degradation](http://arxiv.org/abs/2508.16370v1) | Marie Arnold, Jonathan Brandt et al. | A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up. |
| 2025-08-22 | [Attenuation Models for Extensive Air Showers Derived from Simulations](http://arxiv.org/abs/2508.16360v1) | Fiona Ellwanger, Darko VeberiÄ | At ultra-high energies, the flux of cosmic rays is too low for direct measurements to be meaningful. When a cosmic ray enters the atmosphere, it initiates an extensive air shower, producing a cascade of secondary particles that propagate toward the ground. Large arrays of surface detectors are used to measure these secondary particles upon arrival.   The signal detected at a specific reference distance from the shower core serves as a proxy for the shower size and, consequently, as a reliable estimator of the energy of primary cosmic ray. However, shower development is influenced by attenuation effects: measured signals at the ground depend on the amount of traversed atmospheric density (column density) through which the shower evolves. Since the column density varies with the inclination of the shower, it is important to account for these attenuation effects to ensure accurate energy estimation.   In this study, we derive physics-and-geometry-based functional forms to describe attenuation and propose appropriate expansion terms using simple one-dimensional shower-development models, incorporating one or two main particle-cascade components. We then evaluate the applicability and effectiveness of these functional forms using a Monte-Carlo dataset that includes various primary cosmic-ray particles. By directly calibrating the the shower size derived from ground signals to the Monte-Carlo energy, we characterize attenuation behavior across different primary particles, assess the energy dependence of attenuation, and quantify systematic uncertainties introduced by different functional forms. |
| 2025-08-21 | [Bayesian Hierarchical Methods for Surveillance of Cervical Dystonia Treatments](http://arxiv.org/abs/2508.15762v1) | D. Baidoo, E. Kubuafor et al. | Cervical dystonia, a debilitating neurological disorder marked by involuntary muscle contractions and chronic pain, presents significant treatment challenges despite advances in botulinum toxin therapy. While botulinum toxin type B has emerged as one of the leading treatments, comparative efficacy across doses and the influence of demographic factors for personalized medicine remain understudied. This study aimed to: (1) compare the efficacy of different botulinum toxin type B doses using Bayesian methods, (2) evaluate demographic and clinical factors affecting treatment response, and (3) establish a probabilistic framework for personalized cervical dystonia management. We analyzed data from a multicenter randomized controlled trial involving 109 patients assigned to placebo, 5,000 units, or 10,000 units of botulinum toxin type B groups. The primary outcome was the Toronto Western Spasmodic Torticollis Rating Scale measured over 16 weeks. Bayesian hierarchical modeling assessed treatment effects while accounting for patient heterogeneity. Lower botulinum toxin type B doses (5,000 units) showed greater overall Toronto Western Spasmodic Torticollis Rating Scale score reductions (treatment effect: -2.39, 95% Probability Interval: -4.10 to -0.70). Male patients demonstrated better responses (5.2% greater improvement) than female patients. Substantial between-patient variability and site-specific effects were observed, highlighting the need for personalized protocols. The study confirms botulinum toxin type B's dose-dependent efficacy while identifying key modifiable factors in treatment response. Bayesian methods provided nuanced insights into uncertainty and heterogeneity, paving the way for personalized medicine in cervical dystonia management. |
| 2025-08-21 | [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](http://arxiv.org/abs/2508.15719v1) | Mohammed Elmusrati | Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning. |
| 2025-08-21 | [CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps](http://arxiv.org/abs/2508.15672v1) | Franz Hanke, Antonia Bieringer et al. | Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3 |
| 2025-08-21 | [Beyond the Nyquist frequency: Asteroseismic catalog of undersampled Kepler late subgiants and early red giants](http://arxiv.org/abs/2508.15654v1) | B. Liagre, R. A. GarcÃ­a et al. | Subgiants and early red giants are crucial for studying the first dredge-up, a key evolutionary phase where the convective envelope deepens, mixing previously interior-processed material and bringing it to the surface. Yet, very few have been seismically characterized with Kepler because their oscillation frequencies are close to the 30 minute sampling frequency of the mission. We developed a new method as part of the new PyA2Z code to identify super-Nyquist oscillators and infer their global seismic parameters, $\nu_\mathrm{max}$ and large separation, $\Delta\nu$. Applying PyA2Z to 2 065 Kepler targets, we seismically characterize 285 super-Nyquist and 168 close-to-Nyquist stars with masses from 0.8 to 1.6 M$_\odot$. In combination with APOGEE spectroscopy, Gaia spectro-photometry, and stellar models, we derive stellar ages for the sample. There is good agreement between the predicted and actual positions of stars on the HR diagram (luminosity vs. effective temperature) as a function of mass and composition. While the timing of dredge-up is consistent with predictions, the magnitude and mass dependence show discrepancies with models, possibly due to uncertainties in model physics or calibration issues in observed abundance scales. |
| 2025-08-21 | [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2508.15652v1) | Ardian Selmonaj, Miroslav Strupl et al. | To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems. |
| 2025-08-21 | [Label Uncertainty for Ultrasound Segmentation](http://arxiv.org/abs/2508.15635v1) | Malini Shivaram, Gautam Rajendrakumar Gare et al. | In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging. |
| 2025-08-21 | [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](http://arxiv.org/abs/2508.15588v1) | Ahmed Nasir, Abdelhafid Zenati | The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden "skeleton" governing the system's behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the system's convergence properties and potential failure modes, such as unintended "trap" states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policy's safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone. |
| 2025-08-21 | [LoUQAL: Low-fidelity informed Uncertainty Quantification for Active Learning in the chemical configuration space](http://arxiv.org/abs/2508.15577v1) | Vivin Vinod, Peter Zaspel | Uncertainty quantification is an important scheme in active learning techniques, including applications in predicting quantum chemical properties. In quantum chemical calculations, there exists the notion of a fidelity, a less accurate computation is accessible at a cheaper computational cost. This work proposes a novel low-fidelity informed uncertainty quantification for active learning with applications in predicting diverse quantum chemical properties such as excitation energies and \textit{ab initio} potential energy surfaces. Computational experiments are carried out in order to assess the proposed method with results demonstrating that models trained with the novel method outperform alternatives in terms of empirical error and number of iterations required. The effect of the choice of fidelity is also studied to perform a thorough benchmark. |
| 2025-08-21 | [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](http://arxiv.org/abs/2508.15569v1) | Xin Du, Sikun Yang et al. | Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal prediction's rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the framework's effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification. |
| 2025-08-21 | [Uncertainty Relation for the Wigner-Yanase Skew Information and Quantum Sobolev Inequalities](http://arxiv.org/abs/2508.15554v1) | Laurent Lafleche | This note explores uncertainty inequalities for quantum analogues of the Fisher information including the Wigner-Yanase skew information, and their connection to the quantum Sobolev inequalities proved by the author in [Journal of Functional Analysis, 286 (10) 2024]. Some additional inequalities concerning commutators are derived and others are left as open problems. |
| 2025-08-20 | [Squeezed Diffusion Models](http://arxiv.org/abs/2508.14871v1) | Jyotirmai Singh, Samar Khanna et al. | Diffusion models typically inject isotropic Gaussian noise, disregarding structure in the data. Motivated by the way quantum squeezed states redistribute uncertainty according to the Heisenberg uncertainty principle, we introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically along the principal component of the training distribution. As squeezing enhances the signal-to-noise ratio in physics, we hypothesize that scaling noise in a data-dependent manner can better assist diffusion models in learning important data features. We study two configurations: (i) a Heisenberg diffusion model that compensates the scaling on the principal axis with inverse scaling on orthogonal directions and (ii) a standard SDM variant that scales only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64, mild antisqueezing - i.e. increasing variance on the principal axis - consistently improves FID by up to 15% and shifts the precision-recall frontier toward higher recall. Our results demonstrate that simple, data-aware noise shaping can deliver robust generative gains without architectural changes. |
| 2025-08-20 | [Calibration offset estimation in mobile hearing tests via categorical loudness scaling](http://arxiv.org/abs/2508.14824v1) | Chen Xu, Birger Kollmeier | Objective: To enable reliable smartphone-based hearing assessments by developing methods to estimate device calibration offsets using categorical loudness scaling (CLS). Design: Calibration offsets were simulated from a Gaussian distribution. Two prediction models - a Bayesian regression model and a nearest neighbor model - were trained on CLS-derived parameters and data from the Oldenburg Hearing Health Repository (OHHR). CLS was chosen because it provides level-independent measures (e.g., dynamic range) that remain robust despite calibration errors. Study Sample: The dataset comprised CLS results from N = 847 participants with a mean age of 70.0 years (SD = 8.7), including 556 male and 291 female listeners with diverse hearing profiles. Results: The Bayesian regression model achieved correlations of up to 0.81 between estimated and true calibration offsets, enabling accurate individual-level correction. Compared to threshold-based approaches, calibration uncertainty was reduced by factors between 0.41 and 0.79, demonstrating greater robustness in uncontrolled environments. Conclusions: CLS-based models can effectively compensate for missing calibration in mobile hearing assessments. This approach provides a practical alternative to threshold-based methods, supporting the use of smartphone-based tests outside laboratory settings and expanding access to reliable hearing healthcare in everyday and resource-limited contexts. |
| 2025-08-20 | [Equipartition and the temperature of maximum density of TIP4/2005 water](http://arxiv.org/abs/2508.14800v1) | Dilipkumar N. Asthagiri, Thomas L. Beck | We simulate TIP4P/2005 water in the temperature range of 257 K to 318 K with time steps of $\delta =$ 0.25, 0.50, and 2.0 fs. Within the computed statistical uncertainties, the density-temperature behavior obtained using 0.25 fs and 0.50 fs are in excellent agreement with each other but differ from those obtained using $\delta t = 2.0$ fs, a choice that leads to a breakdown of equipartition. The temperature of maximum density (TMD) is 277.15 K with $\delta t = $0.25 fs or 0.50 fs, but is shifted to 273.15 K for simulations with $\delta t = 2.0$ fs. This shift is comparable in magnitude to the shift in TMD due to nuclear quantum effects, emphasizing the care required in the parameterization and classical statistical mechanical simulation of a fluid that displays nontrivial nuclear quantum effects under ambient conditions. Enhancing the water-water dispersion interaction, as has been recommended for modeling disordered solvated proteins, degrades the description of the liquid-vapor phase envelope. |
| 2025-08-20 | [Safe and Transparent Robots for Human-in-the-Loop Meat Processing](http://arxiv.org/abs/2508.14763v1) | Sagar Parekh, Casey Grothoff et al. | Labor shortages have severely affected the meat processing sector. Automated technology has the potential to support the meat industry, assist workers, and enhance job quality. However, existing automation in meat processing is highly specialized, inflexible, and cost intensive. Instead of forcing manufacturers to buy a separate device for each step of the process, our objective is to develop general-purpose robotic systems that work alongside humans to perform multiple meat processing tasks. Through a recently conducted survey of industry experts, we identified two main challenges associated with integrating these collaborative robots alongside human workers. First, there must be measures to ensure the safety of human coworkers; second, the coworkers need to understand what the robot is doing. This paper addresses both challenges by introducing a safety and transparency framework for general-purpose meat processing robots. For safety, we implement a hand-detection system that continuously monitors nearby humans. This system can halt the robot in situations where the human comes into close proximity of the operating robot. We also develop an instrumented knife equipped with a force sensor that can differentiate contact between objects such as meat, bone, or fixtures. For transparency, we introduce a method that detects the robot's uncertainty about its performance and uses an LED interface to communicate that uncertainty to the human. Additionally, we design a graphical interface that displays the robot's plans and allows the human to provide feedback on the planned cut. Overall, our framework can ensure safe operation while keeping human workers in-the-loop about the robot's actions which we validate through a user study. |
| 2025-08-20 | [Distributional Adversarial Attacks and Training in Deep Hedging](http://arxiv.org/abs/2508.14757v1) | Guangyi He, Tobias Sutter et al. | In this paper, we study the robustness of classical deep hedging strategies under distributional shifts by leveraging the concept of adversarial attacks. We first demonstrate that standard deep hedging models are highly vulnerable to small perturbations in the input distribution, resulting in significant performance degradation. Motivated by this, we propose an adversarial training framework tailored to increase the robustness of deep hedging strategies. Our approach extends pointwise adversarial attacks to the distributional setting and introduces a computationally tractable reformulation of the adversarial optimization problem over a Wasserstein ball. This enables the efficient training of hedging strategies that are resilient to distributional perturbations. Through extensive numerical experiments, we show that adversarially trained deep hedging strategies consistently outperform their classical counterparts in terms of out-of-sample performance and resilience to model misspecification. Our findings establish a practical and effective framework for robust deep hedging under realistic market uncertainties. |
| 2025-08-20 | [Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds](http://arxiv.org/abs/2508.14661v1) | Alexander Raab, Stephan Weiss et al. | Aiming to enhance the consistency and thus long-term accuracy of Extended Kalman Filters for terrestrial vehicle localization, this paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the robot's pose in a space with reduced dimensionality, the approach ensures feasible estimates on generic smooth surfaces, without introducing artificial constraints or simplifications that may degrade a filter's performance. The accompanying measurement models are compatible with common loosely- and tightly-coupled sensor modalities and also implicitly account for the ground geometry. We extend the formulation by introducing a novel correction scheme that embeds additional domain knowledge into the sensor data, giving more accurate uncertainty approximations and further enhancing filter consistency. The proposed estimator is seamlessly integrated into a validated modular state estimation framework, demonstrating compatibility with existing implementations. Extensive Monte Carlo simulations across diverse scenarios and dynamic sensor configurations show that the M-ESEKF outperforms classical filter formulations in terms of consistency and stability. Moreover, it eliminates the need for scenario-specific parameter tuning, enabling its application in a variety of real-world settings. |
| 2025-08-20 | [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](http://arxiv.org/abs/2508.14654v1) | Peilin Ji, Xiao Xue et al. | In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response. |
| 2025-08-20 | [Experimental validation of universal filtering and smoothing for linear system identification using adaptive tuning](http://arxiv.org/abs/2508.14629v1) | Zihao Liu, Sima Abolghasemi et al. | In Kalman filtering, unknown inputs are often estimated by augmenting the state vector, which introduces reliance on fictitious input models. In contrast, minimum-variance unbiased methods estimate inputs and states separately, avoiding fictitious models but requiring strict sensor configurations, such as full-rank feedforward matrices or without direct feedthrough. To address these limitations, two universal approaches have been proposed to handle systems with or without direct feedthrough, including cases of rank-deficient feedforward matrices. Numerical studies have shown their robustness and applicability, however, they have so far relied on offline tuning, and performance under physical sensor noise and structural uncertainties has not yet been experimentally validated. Contributing to this gap, this paper experimentally validates the universal methods on a five-storey shear frame subjected to shake table tests and multi-impact events. Both typical and rank-deficient conditions are considered. Furthermore, a self-tuning mechanism is introduced to replace impractical offline tuning and enable real-time adaptability. The findings of this paper provide strong evidence of the robustness and adaptability of the methods for structural health monitoring applications, particularly when sensor networks deviate from ideal configurations. |
| 2025-08-20 | [A Simple and Scalable Kernel Density Approach for Reliable Uncertainty Quantification in Atomistic Machine Learning](http://arxiv.org/abs/2508.14613v1) | Daniel Willimetz, LukÃ¡Å¡ Grajciar | Machine learning models are increasingly used to predict material properties and accelerate atomistic simulations, but the reliability of their predictions depends on the representativeness of the training data. We present a scalable, GPU-accelerated uncertainty quantification framework based on $k$-nearest-neighbor kernel density estimation (KDE) in a PCA-reduced descriptor space. This method efficiently detects sparsely sampled regions in large, high-dimensional datasets and provides a transferable, model-agnostic uncertainty metric without requiring retraining costly model ensembles. The framework is validated across diverse case studies varying in: i) chemistry, ii) prediction models (including foundational neural network), iii) descriptors used for KDE estimation, and iv) properties whose uncertainty is sought. In all cases, the KDE-based score reliably flags extrapolative configurations, correlates well with conventional ensemble-based uncertainties, and highlights regions of reduced prediction trustworthiness. The approach offers a practical route for improving the interpretability, robustness, and deployment readiness of ML models in materials science. |
| 2025-08-20 | [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](http://arxiv.org/abs/2508.14597v1) | Nitish Kumar Mahala, Muzammil Khan et al. | Fire outbreaks pose critical threats to human life and infrastructure, necessitating high-fidelity early-warning systems that detect combustion precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal dynamics influenced by illumination variability, flow kinematics, and environmental noise, undermining the reliability of traditional detectors. To address these challenges without the logistical complexity of multi-sensor arrays, we propose an information-fusion framework by integrating smoke feature representations extracted from monocular imagery. Specifically, a Two-Phase Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke detection, leveraging a novel smoke segmentation dataset, constructed via optical flow-based motion encoding, is proposed. The optical flow estimation is performed with a four-color-theorem-inspired dual-phase level-set fractional-order variational model, which preserves motion discontinuities. The resulting color-encoded optical flow maps are fused with appearance cues via a Gaussian Mixture Model to generate binary segmentation masks of the smoke regions. These fused representations are fed into the novel Shifted-Windows Transformer, which is augmented with a multi-scale uncertainty estimation head and trained under a two-phase learning regimen. First learning phase optimizes smoke detection accuracy, while during the second phase, the model learns to estimate plausibility confidence in its predictions by jointly modeling aleatoric and epistemic uncertainties. Extensive experiments using multiple evaluation metrics and comparative analysis with state-of-the-art approaches demonstrate superior generalization and robustness, offering a reliable solution for early fire detection in surveillance, industrial safety, and autonomous monitoring applications. |
| 2025-08-19 | [BLIPs: Bayesian Learned Interatomic Potentials](http://arxiv.org/abs/2508.14022v1) | Dario Coscia, Pim de Haan et al. | Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool in simulation-based chemistry. However, like most deep learning models, MLIPs struggle to make accurate predictions on out-of-distribution data or when trained in a data-scarce regime, both common scenarios in simulation-based chemistry. Moreover, MLIPs do not provide uncertainty estimates by construction, which are fundamental to guide active learning pipelines and to ensure the accuracy of simulation results compared to quantum calculations. To address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian framework for training or fine-tuning MLIPs, built on an adaptive version of Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and minimal computational overhead for energy and forces prediction at inference time, while integrating seamlessly with (equivariant) message-passing architectures. Empirical results on simulation-based computational chemistry tasks demonstrate improved predictive accuracy with respect to standard MLIPs, and trustworthy uncertainty estimates, especially in data-scarse or heavy out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP yields consistent performance gains and calibrated uncertainties. |
| 2025-08-19 | [Data Compression with Noise Suppression for Inference under Noisy Covariance](http://arxiv.org/abs/2508.14021v1) | Sunao Sugiyama, Minsu Park | In many fields including cosmology, statistical inference often relies on Gaussian likelihoods whose covariance matrices are estimated from a finite number of simulations. This finite-sample estimation introduces noise into the covariance, which propagates to parameter estimates, a phenomenon known as the Dodelson-Schneider (DS) effect, leading to inflated uncertainties. While the Massively Optimized Parameter Estimation and Data compression (MOPED) algorithm offers lossless Fisher information-preserving compression, it does not mitigate the DS effect when the compression matrix itself is derived from noisy covariances. In this paper, we propose a modified compression scheme, powered MOPED ($p$-MOPED), which suppresses noise propagation by balancing information retention and covariance estimate noise reduction through a tunable power-law transformation of the sample correlation matrix. We test $p$-MOPED against standard and diagonal MOPED on toy models and on cosmological data from the Subaru Hyper Suprime-Cam Year 3 weak lensing survey. Our results demonstrate that $p$-MOPED consistently outperforms other approaches, especially in regimes with limited simulations, offering a robust compression strategy for high-dimensional data analyses under practical constraints. |
| 2025-08-19 | [Dark Energy Survey Year 3 Results: Cosmological constraints from second and third-order shear statistics](http://arxiv.org/abs/2508.14018v1) | R. C. H. Gomes, S. Sugiyama et al. | We present a cosmological analysis of the third-order aperture mass statistic using Dark Energy Survey Year 3 (DES Y3) data. We perform a complete tomographic measurement of the three-point correlation function of the Y3 weak lensing shape catalog with the four fiducial source redshift bins. Building upon our companion methodology paper, we apply a pipeline that combines the two-point function $\xi_{\pm}$ with the mass aperture skewness statistic $\langle M_{\rm ap}^3\rangle$, which is an efficient compression of the full shear three-point function. We use a suite of simulated shear maps to obtain a joint covariance matrix. By jointly analyzing $\xi_\pm$ and $\langle M_{\rm ap}^3\rangle$ measured from DES Y3 data with a $\Lambda$CDM model, we find $S_8=0.780\pm0.015$ and $\Omega_{\rm m}=0.266^{+0.039}_{-0.040}$, yielding 111% of figure-of-merit improvement in $\Omega_m$-$S_8$ plane relative to $\xi_{\pm}$ alone, consistent with expectations from simulated likelihood analyses. With a $w$CDM model, we find $S_8=0.749^{+0.027}_{-0.026}$ and $w_0=-1.39\pm 0.31$, which gives an improvement of $22\%$ on the joint $S_8$-$w_0$ constraint. Our results are consistent with $w_0=-1$. Our new constraints are compared to CMB data from the Planck satellite, and we find that with the inclusion of $\langle M_{\rm ap}^3\rangle$ the existing tension between the data sets is at the level of $2.3\sigma$. We show that the third-order statistic enables us to self-calibrate the mean photometric redshift uncertainty parameter of the highest redshift bin with little degradation in the figure of merit. Our results demonstrate the constraining power of higher-order lensing statistics and establish $\langle M_{\rm ap}^3\rangle$ as a practical observable for joint analyses in current and future surveys. |
| 2025-08-19 | [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](http://arxiv.org/abs/2508.13990v1) | Daniel KlÃ¶tzl, Ozan Tastekin et al. | Multidimensional data is often associated with uncertainties that are not well-described by normal distributions. In this work, we describe how such distributions can be projected to a low-dimensional space using uncertainty-aware principal component analysis (UAPCA). We propose to model multidimensional distributions using Gaussian mixture models (GMMs) and derive the projection from a general formulation that allows projecting arbitrary probability density functions. The low-dimensional projections of the densities exhibit more details about the distributions and represent them more faithfully compared to UAPCA mappings. Further, we support including user-defined weights between the different distributions, which allows for varying the importance of the multidimensional distributions. We evaluate our approach by comparing the distributions in low-dimensional space obtained by our method and UAPCA to those obtained by sample-based projections. |
| 2025-08-19 | [GBEES-GPU: An efficient parallel GPU algorithm for high-dimensional nonlinear uncertainty propagation](http://arxiv.org/abs/2508.13986v1) | Benjamin L. Hanson, Carlos Rubio et al. | Eulerian nonlinear uncertainty propagation methods often suffer from finite domain limitations and computational inefficiencies. A recent approach to this class of algorithm, Grid-based Bayesian Estimation Exploiting Sparsity, addresses the first challenge by dynamically allocating a discretized grid in regions of phase space where probability is non-negligible. However, the design of the original algorithm causes the second challenge to persist in high-dimensional systems. This paper presents an architectural optimization of the algorithm for CPU implementation, followed by its adaptation to the CUDA framework for single GPU execution. The algorithm is validated for accuracy and convergence, with performance evaluated across distinct GPUs. Tests include propagating a three-dimensional probability distribution subject to the Lorenz '63 model and a six-dimensional probability distribution subject to the Lorenz '96 model. The results imply that the improvements made result in a speedup of over 1000 times compared to the original implementation. |
| 2025-08-19 | [Dynamics-independent bounds on state transformations and precision in open quantum systems](http://arxiv.org/abs/2508.13884v1) | Yoshihiko Hasegawa | We derive dynamics-independent upper bounds on achievable quantum state transformations. Modeling the evolution as a joint unitary on the system and its environment, we show that the R\'enyi divergence between the initial system state and any state reachable via the dynamics is bounded from above by a quantity determined solely by the eigenvalues of the initial system and environment density operators. As a consequence, we establish dynamics-independent lower bounds on the relative variance for arbitrary measurements, which parallel thermodynamic uncertainty relations. Moreover, we obtain dynamics- and measurement-independent lower bounds on the variance of parameter estimators. These results depend only on the initial eigenvalues of the system and environment and hold for any joint unitary, providing computable bounds for open quantum systems. |
| 2025-08-19 | [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](http://arxiv.org/abs/2508.13874v1) | Rouqaiah Al-Refai, Pankaja Priya Ramasamy et al. | The rapid advancement of authentication systems and their increasing reliance on biometrics for faster and more accurate user verification experience, highlight the critical need for a reliable framework to evaluate the suitability of biometric modalities for specific applications. Currently, the most widely known evaluation framework is a comparative table from 1998, which no longer adequately captures recent technological developments or emerging vulnerabilities in biometric systems. To address these challenges, this work revisits the evaluation of biometric modalities through an expert survey involving 24 biometric specialists. The findings indicate substantial shifts in property ratings across modalities. For example, face recognition, shows improved ratings due to technological progress, while fingerprint, shows decreased reliability because of emerging vulnerabilities and attacks. Further analysis of expert agreement levels across rated properties highlighted the consistency of the provided evaluations and ensured the reliability of the ratings. Finally, expert assessments are compared with dataset-level uncertainty across 55 biometric datasets, revealing strong alignment in most modalities and underscoring the importance of integrating empirical evidence with expert insight. Moreover, the identified expert disagreements reveal key open challenges and help guide future research toward resolving them. |
| 2025-08-19 | [OpenLB-UQ: An Uncertainty Quantification Framework for Incompressible Fluid Flow Simulations](http://arxiv.org/abs/2508.13867v1) | Mingliang Zhong, Adrian KummerlÃ¤nder et al. | Uncertainty quantification (UQ) is crucial in computational fluid dynamics to assess the reliability and robustness of simulations, given the uncertainties in input parameters. OpenLB is an open-source lattice Boltzmann method library designed for efficient and extensible simulations of complex fluid dynamics on high-performance computers. In this work, we leverage the efficiency of OpenLB for large-scale flow sampling with a dedicated and integrated UQ module. To this end, we focus on non-intrusive stochastic collocation methods based on generalized polynomial chaos and Monte Carlo sampling. The OpenLB-UQ framework is extensively validated in convergence tests with respect to statistical metrics and sample efficiency using selected benchmark cases, including two-dimensional Taylor--Green vortex flows with up to four-dimensional uncertainty and a flow past a cylinder. Our results confirm the expected convergence rates and show promising scalability, demonstrating robust statistical accuracy as well as computational efficiency. OpenLB-UQ enhances the capability of the OpenLB library, offering researchers a scalable framework for UQ in incompressible fluid flow simulations and beyond. |
| 2025-08-19 | [Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems](http://arxiv.org/abs/2508.13839v1) | Yue Xiu, Yang Zhao et al. | The cell-free integrated sensing and communication (CF-ISAC) architecture is a promising enabler for 6G, offering spectrum efficiency and ubiquitous coverage. However, real deployments suffer from hardware impairments, especially nonlinear distortion from power amplifiers (PAs), which degrades both communication and sensing. To address this, we propose a movable antenna (MA)-aided CF-ISAC system that mitigates distortion and enhances robustness. The PAs nonlinearities are modeled by a third-order memoryless polynomial, where the third-order distortion coefficients (3RDCs) vary across access points (APs) due to hardware differences, aging, and environmental conditions. We design a distributed distortion-aware worst-case robust optimization framework that explicitly incorporates uncertainty in 3RDCs. First, we analyze the worst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB) and communication rate. Then, to address the resulting non-convexity, we apply successive convex approximation (SCA) for estimating the 3RDCs. With these, we jointly optimize beamforming and MA positions under transmit power and sensing constraints. To efficiently solve this highly non-convex problem, we develop an MA-enabled self-attention convolutional graph neural network (SACGNN) algorithm. Simulations demonstrate that our method substantially enhances the communication-sensing trade-off under distortion and outperforms fixed-position antenna baselines in terms of robustness and capacity, thereby highlighting the advantages of MA-aided CF-ISAC systems. |
| 2025-08-19 | [Online Conformal Selection with Accept-to-Reject Changes](http://arxiv.org/abs/2508.13838v1) | Kangdao Liu, Huajun Xi et al. | Selecting a subset of promising candidates from a large pool is crucial across various scientific and real-world applications. Conformal selection offers a distribution-free and model-agnostic framework for candidate selection with uncertainty quantification. While effective in offline settings, its application to online scenarios, where data arrives sequentially, poses challenges. Notably, conformal selection permits the deselection of previously selected candidates, which is incompatible with applications requiring irreversible selection decisions. This limitation is particularly evident in resource-intensive sequential processes, such as drug discovery, where advancing a compound to subsequent stages renders reversal impractical. To address this issue, we extend conformal selection to an online Accept-to-Reject Changes (ARC) procedure: non-selected data points can be reconsidered for selection later, and once a candidate is selected, the decision is irreversible. Specifically, we propose a novel conformal selection method, Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC), which incorporates online Benjamini-Hochberg procedure into the candidate selection process. We provide theoretical guarantees that OCS-ARC controls the false discovery rate (FDR) at or below the nominal level at any timestep under both i.i.d. and exchangeable data assumptions. Additionally, we theoretically show that our approach naturally extends to multivariate response settings. Extensive experiments on synthetic and real-world datasets demonstrate that OCS-ARC significantly improves selection power over the baseline while maintaining valid FDR control across all examined timesteps. |
| 2025-08-18 | [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](http://arxiv.org/abs/2508.13144v1) | David Heineman, Valentin Hofmann et al. | Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances. |
| 2025-08-18 | [The ALPINE-CRISTAL-JWST survey: spatially resolved star formation relations at $z\sim5$](http://arxiv.org/abs/2508.13136v1) | C. Accard, M. BÃ©thermin et al. | Star formation governs galaxy evolution, shaping stellar mass assembly and gas consumption across cosmic time. The Kennicutt-Schmidt (KS) relation, linking star formation rate (SFR) and gas surface densities, is fundamental to understand star formation regulation, yet remains poorly constrained at $z > 2$ due to observational limitations and uncertainties in locally calibrated gas tracers. The [CII] $158 {\rm \mu m}$ line has recently emerged as a key probe of the cold ISM and star formation in the early Universe. We investigate whether the resolved [CII]-SFR and KS relations established at low redshift remain valid at $4 < z < 6$ by analysing 13 main-sequence galaxies from the ALPINE and CRISTAL surveys, using multi-wavelength data (HST, JWST, ALMA) at $\sim2$ kpc resolution. We perform pixel-by-pixel spectral energy distribution (SED) modelling with CIGALE on resolution-homogenised images. We develop a statistical framework to fit the [CII]-SFR relation that accounts for pixel covariance and compare our results to classical fitting methods. We test two [CII]-to-gas conversion prescriptions to assess their impact on inferred gas surface densities and depletion times. We find a resolved [CII]-SFR relation with a slope of $0.87 \pm 0.15$ and intrinsic scatter of $0.19 \pm 0.03$ dex, which is shallower and tighter than previous studies at $z\sim5$. The resolved KS relation is highly sensitive to the [CII]-to-gas conversion factor: using a fixed global $\alpha_{\rm [CII]}$ yields depletion times of $0.5$-$1$ Gyr, while a surface brightness-dependent $W_{\rm [CII]}$, places some galaxies with high gas density in the starburst regime ($<0.1$ Gyr). Future inputs from both simulations and observations are required to better understand how the [CII]-to-gas conversion factor depends on local ISM properties. We need to break this fundamental limit to properly study the KS relation at $z\gtrsim4$. |
| 2025-08-18 | [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](http://arxiv.org/abs/2508.13121v1) | Carlos Celemin | This work introduces an automated testing approach that employs agents controlling game characters to detect potential bugs within a game level. Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient search, the method determines the next sampling point by analyzing the data collected so far and calculates the data point that will maximize information acquisition. To support the BO process, we introduce a game testing-specific model built on top of a grid map, that features the smoothness and uncertainty estimation required by BO, however and most importantly, it does not suffer the scalability issues that traditional models carry. The experiments demonstrate that the approach significantly improves map coverage capabilities in both time efficiency and exploration distribution. |
| 2025-08-18 | [Reasonable uncertainty: Confidence intervals in empirical Bayes discrimination detection](http://arxiv.org/abs/2508.13110v1) | Jiaying Gu, Nikolaos Ignatiadis et al. | We revisit empirical Bayes discrimination detection, focusing on uncertainty arising from both partial identification and sampling variability. While prior work has mostly focused on partial identification, we find that some empirical findings are not robust to sampling uncertainty. To better connect statistical evidence to the magnitude of real-world discriminatory behavior, we propose a counterfactual odds-ratio estimand with a attractive properties and interpretation. Our analysis reveals the importance of careful attention to uncertainty quantification and downstream goals in empirical Bayes analyses. |
| 2025-08-18 | [Surrogate-based Bayesian calibration methods for climate models: a comparison of traditional and non-traditional approaches](http://arxiv.org/abs/2508.13071v1) | Maike F. Holthuijzen, Atlanta Chakraborty et al. | Parameter calibration is crucial for reducing uncertainty and improving simulation accuracy in physics-based models, yet computational constraints pose significant challenges. Bayesian calibration methods offer a principled framework for combining prior knowledge with data while rigorously quantifying uncertainty. In this work, we compare four emulator-based Bayesian calibration methods: Calibrate-Emulate-Sample (CES), History Matching (HM), Bayesian Optimal Experimental Design (BOED), and a novel Goal-Oriented BOED (GBOED) approach, using the Lorenz '96 multiscale system as a testbed. Our GBOED formulation explicitly targets calibration-relevant quantities and leverages information-theoretic criteria for data selection. We assess each method in terms of calibration accuracy, uncertainty quantification, computational cost, and convergence behavior. We evaluate each method's performance in balancing computational cost, implementation complexity, and uncertainty quantification (UQ), with additional insights into convergence behavior as model evaluations increase. We find CES offers excellent performance but at high computational expense, while GBOED achieves comparable accuracy using fewer model evaluations. Standard BOED underperforms with respect to calibration accuracy, and HM shows moderate effectiveness but can be useful as a precursor. Our results highlight trade-offs among Bayesian strategies and demonstrate the promise of goal-oriented design in calibration workflows. |
| 2025-08-18 | [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](http://arxiv.org/abs/2508.13057v1) | Adolfo GonzÃ¡lez, VÃ­ctor Parada | Demand forecasting is essential for strategic planning in competitive environments, enabling resource optimization and improved responsiveness to market dynamics. However, multivariate time series modeling faces challenges due to data complexity, uncertainty, and frequent regime shifts. Traditional evaluation metrics can introduce biases and limit generalization. This work compares two custom evaluation functions: FMAE (Focused Mean Absolute Error), focused on minimizing absolute errors, and HEF (Hierarchical Evaluation Function), designed to weight global metrics and penalize large deviations. Experiments were conducted under different data splits (91:9, 80:20, 70:30) using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative accuracy, robustness, and computational efficiency. Results show that HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE), enhancing model robustness and explanatory power. These findings were confirmed via visualizations and statistical tests. Conversely, FMAE offers advantages in local metrics (MAE, MASE) and execution time, making it suitable for short-term scenarios. The study highlights a methodological trade-off: HEF is ideal for strategic planning, while FMAE is better suited for operational efficiency. A replicable framework is proposed for optimizing predictive models in dynamic environments. |
| 2025-08-18 | [A test of Ca ii H & K photometry for isolating massive globular clusters below the metallicity floor](http://arxiv.org/abs/2508.13031v1) | Bas van Heumen, William E. Harris et al. | The serendipitous discovery of the M31 globular cluster (GC) EXT8 has presented a significant challenge to current theories for GC formation. By finding other GCs similar to EXT8, it should become clear if and/or how EXT8 can fit into our current understanding of GC formation. We aim to test the potential of integrated-light narrow-band Ca II H & K photometry as a proxy for the metallicity of GCs to be able to provide effective candidate selection for massive GCs below the GC metallicity floor ([Fe/H] $\leq$ -2.5), such as EXT8. We investigate the behaviour of two colours involving the CaHK filter employed by the Pristine survey, CaHK-u and CaHK-g, as a function of metallicity through CFHT MegaCam imaging of EXT8 and a wide set of M31 GCs covering the metallicity range of -2.9 $\leq$ [Fe/H] $\leq$ +0.4. Additionally, we investigate if the CaHK colours are strongly influenced by horizontal branch morphology through available morphology measurements. In both of the CaHK colours, EXT8 and two other potential GCs below the metallicity floor can be selected from other metal-poor GCs ([Fe/H] $\leq$ -1.5) with CaHK-g showing the larger metallicity sensitivity. The RMS of linear fits to the metal-poor GCs show an uncertainty of 0.3 dex on metallicity estimations for both colours. Comparisons with u-g and g-z/F450W-F850L colours reinforce the notion that CaHK photometry can be used for effective candidate selection as they reduce false positive selection rates by at least a factor of 2. We find no strong influence of the horizontal branch morphology on the CaHK colours that would interfere with candidate selection, although the assessment is limited by quantity and quality of available data. |
| 2025-08-18 | [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](http://arxiv.org/abs/2508.13021v1) | Pengcheng Huang, Shuhao Liu et al. | Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler. |
| 2025-08-18 | [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](http://arxiv.org/abs/2508.12997v1) | Haishun Chen, Cai Xu et al. | Multi-view evidential learning aims to integrate information from multiple views to improve prediction performance and provide trustworthy uncertainty esitimation. Most previous methods assume that view-specific evidence learning is naturally reliable. However, in practice, the evidence learning process tends to be biased. Through empirical analysis on real-world data, we reveal that samples tend to be assigned more evidence to support data-rich classes, thereby leading to unreliable uncertainty estimation in predictions. This motivates us to delve into a new Biased Evidential Multi-view Learning (BEML) problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning (FAML). FAML first introduces an adaptive prior based on training trajectory, which acts as a regularization strategy to flexibly calibrate the biased evidence learning process. Furthermore, we explicitly incorporate a fairness constraint based on class-wise evidence variance to promote balanced evidence allocation. In the multi-view fusion stage, we propose an opinion alignment mechanism to mitigate view-specific bias across views, thereby encouraging the integration of consistent and mutually supportive evidence. Extensive experiments on five real-world multi-view datasets demonstrate that FAML achieves more balanced evidence allocation and improves both prediction performance and the reliability of uncertainty estimation compared to state-of-the-art methods. |
| 2025-08-18 | [Likelihood-Based Heterogeneity Inference Reveals Non-Stationary Effects in Biohybrid Cell-Cargo Transport](http://arxiv.org/abs/2508.12976v1) | Jan Albrecht, Lara S. Dautzenberg et al. | Variability of motility behavior in populations of microbiological agents is an ubiquitous phenomenon even in the case of genetically identical cells. Accordingly, passive objects introduced into such biological systems and driven by them will also exhibit heterogeneous motion patterns. Here, we study a biohybrid system of passive beads driven by active ameboid cells and use a likelihood approach to estimate the heterogeneity of the bead dynamics from their discretely sampled trajectories. We showcase how this approach can deal with information-scarce situations and provides natural uncertainty bounds for heterogeneity estimates. Using these advantages we particularly uncover that the heterogeneity in the system is time-dependent. |
| 2025-08-15 | [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](http://arxiv.org/abs/2508.11573v1) | Mogens Plessen | Automatic Section Control (ASC) is a long-standing trend for spraying in agriculture. It promises to minimise spray overlap areas. The core idea is to (i) switch off spray nozzles on areas that have already been sprayed, and (ii) to dynamically adjust nozzle flow rates along the boom bar that holds the spray nozzles when velocities of boom sections vary during turn maneuvers. ASC is not possible without sensors, in particular for accurate positioning data. Spraying and the movement of modern wide boom bars are highly dynamic processes. In addition, many uncertainty factors have an effect such as cross wind drift, boom height, nozzle clogging in open-field conditions, and so forth. In view of this complexity, the natural question arises if a simpler alternative exist. Therefore, an Automatic Multi-Sections Control method is compared to a proposed simpler one- or two-sections alternative that uses predictive spray switching. The comparison is provided under nominal conditions. Agricultural spraying is intrinsically linked to area coverage path planning and spray switching logic. Combinations of two area coverage path planning and switching logics as well as three sections-setups are compared. The three sections-setups differ by controlling 48 sections, 2 sections or controlling all nozzles uniformly with the same control signal as one single section. Methods are evaluated on 10 diverse real-world field examples, including non-convex field contours, freeform mainfield lanes and multiple obstacle areas. A preferred method is suggested that (i) minimises area coverage pathlength, (ii) offers intermediate overlap, (iii) is suitable for manual driving by following a pre-planned predictive spray switching logic for an area coverage path plan, and (iv) and in contrast to ASC can be implemented sensor-free and therefore at low cost. |
| 2025-08-15 | [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](http://arxiv.org/abs/2508.11547v1) | Martin JirouÅ¡ek, TomÃ¡Å¡ BÃ¡Äa et al. | This paper addresses the problem of tracking the position of a cable-suspended payload carried by an unmanned aerial vehicle, with a focus on real-world deployment and minimal hardware requirements. In contrast to many existing approaches that rely on motion-capture systems, additional onboard cameras, or instrumented payloads, we propose a framework that uses only standard onboard sensors--specifically, real-time kinematic global navigation satellite system measurements and data from the onboard inertial measurement unit--to estimate and control the payload's position. The system models the full coupled dynamics of the aerial vehicle and payload, and integrates a linear Kalman filter for state estimation, a model predictive contouring control planner, and an incremental model predictive controller. The control architecture is designed to remain effective despite sensing limitations and estimation uncertainty. Extensive simulations demonstrate that the proposed system achieves performance comparable to control based on ground-truth measurements, with only minor degradation (< 6%). The system also shows strong robustness to variations in payload parameters. Field experiments further validate the framework, confirming its practical applicability and reliable performance in outdoor environments using only off-the-shelf aerial vehicle hardware. |
| 2025-08-15 | [Integrating Uncertainties for Koopman-Based Stabilization](http://arxiv.org/abs/2508.11533v1) | Yicheng Lin, Bingxian Wu et al. | Over the past decades, the Koopman operator has been widely applied in data-driven control, yet its theoretical foundations remain underexplored. This paper establishes a unified framework to address the robust stabilization problem in data-driven control via the Koopman operator, fully accounting for three uncertainties: projection error, estimation error, and process disturbance. It comprehensively investigates both direct and indirect data-driven control approaches, facilitating flexible methodology selection for analysis and control. For the direct approach, considering process disturbances, the lifted-state feedback controller, designed via a linear matrix inequality (LMI), robustly stabilizes all lifted bilinear systems consistent with noisy data. For the indirect approach requiring system identification, the feedback controller, designed using a nonlinear matrix inequality convertible to an LMI, ensures closed-loop stability under worst-case process disturbances. Numerical simulations via cross-validation validate the effectiveness of both approaches, highlighting their theoretical significance and practical utility. |
| 2025-08-15 | [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](http://arxiv.org/abs/2508.11460v1) | Aurora Grefsrud, Nello Blaser et al. | Rigorous statistical methods, including parameter estimation with accompanying uncertainties, underpin the validity of scientific discovery, especially in the natural sciences. With increasingly complex data models such as deep learning techniques, uncertainty quantification has become exceedingly difficult and a plethora of techniques have been proposed. In this case study, we use the unifying framework of approximate Bayesian inference combined with empirical tests on carefully created synthetic classification datasets to investigate qualitative properties of six different probabilistic machine learning algorithms for class probability and uncertainty estimation: (i) a neural network ensemble, (ii) neural network ensemble with conflictual loss, (iii) evidential deep learning, (iv) a single neural network with Monte Carlo Dropout, (v) Gaussian process classification and (vi) a Dirichlet process mixture model. We check if the algorithms produce uncertainty estimates which reflect commonly desired properties, such as being well calibrated and exhibiting an increase in uncertainty for out-of-distribution data points. Our results indicate that all algorithms are well calibrated, but none of the deep learning based algorithms provide uncertainties that consistently reflect lack of experimental evidence for out-of-distribution data points. We hope our study may serve as a clarifying example for researchers developing new methods of uncertainty estimation for scientific data-driven modeling. |
| 2025-08-15 | [Improving Air Shower Simulations by Tuning Pythia 8/Angantyr with Accelerator Data](http://arxiv.org/abs/2508.11458v1) | Michael Windau, ChloÃ© Gaudu et al. | We present a combined analysis of the Pythia 8 event generator using accelerator data and evaluate its impact on air shower observables. Reliable simulations with event generators are essential for particle physics analyses, achievable through advanced tuning to experimental data. Pythia 8 has emerged as a promising high-energy interaction model for cosmic ray air shower simulations, offering well-documented parameter settings and a user-friendly interface to enable automatic tuning efforts. Using data from collider and fixed-target experiments, we first derive tunes for each domain separately, before tuning both domains simultaneously. To achieve this, we define a core set of observables and quantify their dependence on selected parameters. The tuning efforts are based on gradient descent and Bayesian methods, the latter providing a full uncertainty propagation of the parameters to the observables. Results for the impact of a combined analysis for the Pythia 8/Angantyr event generator on air shower observables, such as particle densities at ground level and energy deposit profiles, are presented. |
| 2025-08-15 | [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](http://arxiv.org/abs/2508.11453v1) | Jiayue Jin, Lang Qian et al. | Recent years have witnessed remarkable progress in autonomous driving, with systems evolving from modular pipelines to end-to-end architectures. However, most existing methods are trained offline and lack mechanisms to adapt to new environments during deployment. As a result, their generalization ability diminishes when faced with unseen variations in real-world driving scenarios. In this paper, we break away from the conventional "train once, deploy forever" paradigm and propose EvoPSF, a novel online Evolution framework for autonomous driving based on Planning-State Feedback. We argue that planning failures are primarily caused by inaccurate object-level motion predictions, and such failures are often reflected in the form of increased planner uncertainty. To address this, we treat planner uncertainty as a trigger for online evolution, using it as a diagnostic signal to initiate targeted model updates. Rather than performing blind updates, we leverage the planner's agent-agent attention to identify the specific objects that the ego vehicle attends to most, which are primarily responsible for the planning failures. For these critical objects, we compute a targeted self-supervised loss by comparing their predicted waypoints from the prediction module with their actual future positions, selected from the perception module's outputs with high confidence scores. This loss is then backpropagated to adapt the model online. As a result, our method improves the model's robustness to environmental changes, leads to more precise motion predictions, and therefore enables more accurate and stable planning behaviors. Experiments on both cross-region and corrupted variants of the nuScenes dataset demonstrate that EvoPSF consistently improves planning performance under challenging conditions. |
| 2025-08-15 | [Conformal Prediction Meets Long-tail Classification](http://arxiv.org/abs/2508.11345v1) | Shuqi Liu, Jianguo Huang et al. | Conformal Prediction (CP) is a popular method for uncertainty quantification that converts a pretrained model's point prediction into a prediction set, with the set size reflecting the model's confidence. Although existing CP methods are guaranteed to achieve marginal coverage, they often exhibit imbalanced coverage across classes under long-tail label distributions, tending to over cover the head classes at the expense of under covering the remaining tail classes. This under coverage is particularly concerning, as it undermines the reliability of the prediction sets for minority classes, even with coverage ensured on average. In this paper, we propose the Tail-Aware Conformal Prediction (TACP) method to mitigate the under coverage of the tail classes by utilizing the long-tail structure and narrowing the head-tail coverage gap. Theoretical analysis shows that it consistently achieves a smaller head-tail coverage gap than standard methods. To further improve coverage balance across all classes, we introduce an extension of TACP: soft TACP (sTACP) via a reweighting mechanism. The proposed framework can be combined with various non-conformity scores, and experiments on multiple long-tail benchmark datasets demonstrate the effectiveness of our methods. |
| 2025-08-15 | [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](http://arxiv.org/abs/2508.11340v1) | Yuanlin Liu, Zhihan Zhou et al. | Information on the number and category of cervical cells is crucial for the diagnosis of cervical cancer. However, existing classification methods capable of automatically measuring this information require the training dataset to be representative, which consumes an expensive or even unaffordable human cost. We herein propose active labeling that enables us to construct a representative training dataset using a much smaller human cost for data-efficient cervical cell classification. This cost-effective method efficiently leverages the classifier's uncertainty on the unlabeled cervical cell images to accurately select images that are most beneficial to label. With a fast estimation of the uncertainty, this new algorithm exhibits its validity and effectiveness in enhancing the representative ability of the constructed training dataset. The extensive empirical results confirm its efficacy again in navigating the usage of human cost, opening the avenue for data-efficient cervical cell classification. |
| 2025-08-15 | [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](http://arxiv.org/abs/2508.11338v1) | Prathamesh Devadiga, Yashmitha Shailesh | We introduce RegimeNAS, a novel differentiable architecture search framework specifically designed to enhance cryptocurrency trading performance by explicitly integrating market regime awareness. Addressing the limitations of static deep learning models in highly dynamic financial environments, RegimeNAS features three core innovations: (1) a theoretically grounded Bayesian search space optimizing architectures with provable convergence properties; (2) specialized, dynamically activated neural modules (Volatility, Trend, and Range blocks) tailored for distinct market conditions; and (3) a multi-objective loss function incorporating market-specific penalties (e.g., volatility matching, transition smoothness) alongside mathematically enforced Lipschitz stability constraints. Regime identification leverages multi-head attention across multiple timeframes for improved accuracy and uncertainty estimation. Rigorous empirical evaluation on extensive real-world cryptocurrency data demonstrates that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving an 80.3% Mean Absolute Error reduction compared to the best traditional recurrent baseline and converging substantially faster (9 vs. 50+ epochs). Ablation studies and regime-specific analysis confirm the critical contribution of each component, particularly the regime-aware adaptation mechanism. This work underscores the imperative of embedding domain-specific knowledge, such as market regimes, directly within the NAS process to develop robust and adaptive models for challenging financial applications. |
| 2025-08-15 | [Demonstrating the velocity response of a table-top EPR Speedmeter](http://arxiv.org/abs/2508.11316v1) | S. L. Kranzhoff, S. L. Danilishin et al. | The sensitivity of gravitational-wave interferometers is fundamentally limited by quantum noise, as dictated by the Heisenberg uncertainty principle, due to their continuous position measurement of the end mirrors. Speedmeter configurations, which measure mirror velocity rather than position, have been proposed as a means to suppress quantum back-action noise, but practical implementations remain at an early stage. In this work, we present a table-top realisation of the Einstein- Podolsky-Rosen (EPR) Speedmeter concept, employing an optical readout scheme based on two orthogonal polarisation modes that probe the interferometer with different effective bandwidths. Using a triangular cavity, we demonstrate that the differential optical response between the linear p- and s-polarised modes exhibits a speed-like frequency dependence: vanishing at DC and increasing linearly with signal frequency, up to the bandwidth of the slower mode. With this we show that an optical system equivalent to the EPR Speedmeter indeed performs a velocity readout of the end mirror. |
| 2025-08-14 | [The Dark Energy Bedrock All-Sky Supernova Program: Cross Calibration, Simulations, and Cosmology Forecasts](http://arxiv.org/abs/2508.10877v1) | Maria Acevedo, Nora R. Sherman et al. | Type Ia supernovae (SNe Ia) have been essential for probing the nature of dark energy; however, most SN analyses rely on the same low-redshift sample, which may lead to shared systematics. In a companion paper (Sherman et al., submitted), we introduce the Dark Energy Bedrock All-Sky Supernova (DEBASS) program, which has already collected more than 500 low-redshift SNe Ia on the Dark Energy Camera (DECam), and present an initial release of 77 SNe Ia within the Dark Energy Survey (DES) footprint observed between 2021 and 2024. Here, we examine the systematics, including photometric calibration and selection effects. We find agreement at the 10 millimagnitude level among the tertiary standard stars of DEBASS, DES, and Pan-STARRS1. Our simulations reproduce the observed distributions of DEBASS SN light-curve properties, and we measure a bias-corrected Hubble residual scatter of $0.08$ mag, which, while small, is found in 10% of our simulations. We compare the DEBASS SN distances to the Foundation sample and find consistency with a median residual offset of $0.016 \pm 0.019$ mag. Selection effects have negligible impacts on distances, but a different photometric calibration solution shifts the median residual $-0.015 \pm 0.019$ mag, highlighting calibration sensitivity. Using conservative simulations, we forecast that replacing historical low-redshift samples with the full DEBASS sample (>400 SNe Ia) will improve the statistical uncertainties on dark energy parameters $w_0$ and $w_a$ by 30% and 24% respectively, enhance the dark energy Figure of Merit by up to 60%, and enable a measurement of $f\sigma_8$ at the 25% level. |
| 2025-08-14 | [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](http://arxiv.org/abs/2508.10867v1) | Yizhi Zhou, Ziwei Kang et al. | Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial odometry (VIO) systems. Consistency is crucial for ensuring the estimation accuracy of a UWBaided VIO system. An inconsistent estimator can degrade localization performance, where the inconsistency primarily arises from two main factors: (1) the estimator fails to preserve the correct system observability, and (2) UWB anchor positions are assumed to be known, leading to improper neglect of calibration uncertainty. In this paper, we propose a consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system based on the Lie group. Our method incorporates the UWB anchor state into the system state, explicitly accounting for UWB calibration uncertainty and enabling the joint and consistent estimation of both robot and anchor states. Furthermore, observability consistency is ensured by leveraging the invariant error properties of the Lie group. We analytically prove that the CVIRO algorithm naturally maintains the system's correct unobservable subspace, thereby preserving estimation consistency. Extensive simulations and experiments demonstrate that CVIRO achieves superior localization accuracy and consistency compared to existing methods. |
| 2025-08-14 | [Comparison of Data Reduction Criteria for Online Gaussian Processes](http://arxiv.org/abs/2508.10815v1) | Thore Wietzke, Knut Graichen | Gaussian Processes (GPs) are widely used for regression and system identification due to their flexibility and ability to quantify uncertainty. However, their computational complexity limits their applicability to small datasets. Moreover in a streaming scenario, more and more datapoints accumulate which is intractable even for Sparse GPs. Online GPs aim to alleviate this problem by e.g. defining a maximum budget of datapoints and removing redundant datapoints. This work provides a unified comparison of several reduction criteria, analyzing both their computational complexity and reduction behavior. The criteria are evaluated on benchmark functions and real-world datasets, including dynamic system identification tasks. Additionally, acceptance criteria are proposed to further filter out redundant datapoints. This work yields practical guidelines for choosing a suitable criterion for an online GP algorithm. |
| 2025-08-14 | [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](http://arxiv.org/abs/2508.10798v1) | Troi Williams | Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks. |
| 2025-08-14 | [When Experts Disagree: Characterizing Annotator Variability for Vessel Segmentation in DSA Images](http://arxiv.org/abs/2508.10797v1) | M. Geshvadi, G. So et al. | We analyze the variability among segmentations of cranial blood vessels in 2D DSA performed by multiple annotators in order to characterize and quantify segmentation uncertainty. We use this analysis to quantify segmentation uncertainty and discuss ways it can be used to guide additional annotations and to develop uncertainty-aware automatic segmentation methods. |
| 2025-08-14 | [Accelerating Stochastic Energy System Optimization Models: Temporally Split Benders Decomposition](http://arxiv.org/abs/2508.10789v1) | Shima Sasanpour, Manuel Wetzel et al. | Stochastic programming can be applied to consider uncertainties in energy system optimization models for capacity expansion planning. However, these models become increasingly large and time-consuming to solve, even without considering uncertainties. For two-stage stochastic capacity expansion planning problems, Benders decomposition is often applied to ensure that the problem remains solvable. Since stochastic scenarios can be optimized independently within subproblems, their optimization can be parallelized. However, hourly-resolved capacity expansion planning problems typically have a larger temporal than scenario cardinality. Therefore, we present a temporally split Benders decomposition that further exploits the parallelization potential of stochastic expansion planning problems. A compact reformulation of the storage level constraint into linking variables ensures that long-term storage operation can still be optimized despite the temporal decomposition. We demonstrate this novel approach with model instances of the German power system with up to 87 million rows and columns. Our results show a reduction in computing times of up to 60% and reduced memory requirements. Additional enhancement strategies and the use of distributed memory on high-performance computers further improve the computing time by over 80%. |
| 2025-08-14 | [Estimating Covariance for Global Minimum Variance Portfolio: A Decision-Focused Learning Approach](http://arxiv.org/abs/2508.10776v1) | Juchan Kim, Inwoo Tae et al. | Portfolio optimization constitutes a cornerstone of risk management by quantifying the risk-return trade-off. Since it inherently depends on accurate parameter estimation under conditions of future uncertainty, the selection of appropriate input parameters is critical for effective portfolio construction. However, most conventional statistical estimators and machine learning algorithms determine these parameters by minimizing mean-squared error (MSE), a criterion that can yield suboptimal investment decisions. In this paper, we adopt decision-focused learning (DFL) - an approach that directly optimizes decision quality rather than prediction error such as MSE - to derive the global minimum-variance portfolio (GMVP). Specifically, we theoretically derive the gradient of decision loss using the analytic solution of GMVP and its properties regarding the principal components of itself. Through extensive empirical evaluation, we show that prediction-focused estimation methods may fail to produce optimal allocations in practice, whereas DFL-based methods consistently deliver superior decision performance. Furthermore, we provide a comprehensive analysis of DFL's mechanism in GMVP construction, focusing on its volatility reduction capability, decision-driving features, and estimation characteristics. |
| 2025-08-14 | [On data-driven robust distortion risk measures for non-negative risks with partial information](http://arxiv.org/abs/2508.10682v1) | Xiangyu Han, Yijun Hu et al. | In this paper, by proposing two new kinds of distributional uncertainty sets, we explore robustness of distortion risk measures against distributional uncertainty. To be precise, we first consider a distributional uncertainty set which is characterized solely by a ball determined by general Wasserstein distance centered at certain empirical distribution function, and then further consider additional constraints of known first moment and any other higher moment of the underlying loss distribution function. Under the assumption that the distortion function is strictly concave and twice differentiable, and that the underlying loss random variable is non-negative and bounded, we derive closed-form expressions for the distribution functions which maximize a given distortion risk measure over the distributional uncertainty sets respectively. Moreover, we continue to study the general case of a concave distortion function and unbounded loss random variables. Comparisons with existing studies are also made. Finally, we provide a numerical study to illustrate the proposed models and results. Our work provides a novel generalization of several known achievements in the literature. |
| 2025-08-14 | [A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners](http://arxiv.org/abs/2508.10679v1) | Jinhua He, Tingzhe Pan et al. | With the continuous increase in the penetration of renewable energy in the emerging power systems, the pressure on system peak regulation has been significantly intensified. Against this backdrop, demand side resources particularly air conditioning loads have garnered considerable attention for their substantial regulation potential and fast response capabilities, making them promising candidates for providing auxiliary peak shaving services. This study focuses on fixed frequency air conditioners (FFACs) and proposes an optimization model and solution method for their participation in demand response (DR) programs. First, a probabilistic response model for FFACs is developed based on the Markov assumption. Second, by sampling this probabilistic model, the aggregate power consumption of an FFAC cluster under decentralized control is obtained. Subsequently, a robust optimization model is formulated to maximize the profit of an aggregator managing the FFAC cluster during DR events, taking into account the aggregated response power. The model explicitly considers temperature uncertainty to ensure user comfort in a robust sense. Finally, leveraging the structure of the proposed model, it is reformulated as a mixed-integer linear programming (MILP) problem and solved using a commercial optimization solver. Simulation results validate the effectiveness of the proposed model and solution approach. |
| 2025-08-14 | [On the Derivation of Equations of Motion from Symmetries in Quantum-Mechanical Systems via Heisenberg's Uncertainty](http://arxiv.org/abs/2508.10661v1) | Enrique Casanova, JosÃ© Rojas et al. | We propose the construction of equations of motion based on symmetries in quantum-mechanical systems, using Heisenberg's uncertainty principle as a minimal foundation. From canonical operators, two spaces of conjugate operators are constructed, along with a third space derived from the former, which includes the ``Symmetry-Dilation'' operator. When this operator commutes with the main equation of motion, it defines the set of observables compatible with a complete basis of operators (symmetry generators), organized into a Lie algebra dependent on Heisenberg's uncertainty principle within Minkowski spacetime. Furthermore, by requiring the dilation operator to commute with the central operator, the wavefunction is constrained, thereby constructing known structures. Specific cases are derived -- relativistic, non-relativistic, and a lesser-studied case: ``ultra-relativistic (Carroll-Schr\"odinger)''. Our work may open new avenues for understanding and classifying symmetries in quantum mechanics, as well as offer an alternative method for deriving equations of motion and applying them to complex scenarios involving exotic particles. |
| 2025-08-13 | [Data-driven analyses and model-independent fits for present $b\to s \ell \ell$ results](http://arxiv.org/abs/2508.09986v1) | T. Hurth, F. Mahmoudi et al. | We present a critical assessment of the present $B$ anomalies in the exclusive $b \to s \ell\ell$ mode based on the QCD factorisation approach (QCDf). In particular, we analyse the impact of different local form factor calculations and of the largest bin in the low-$q^2$ region.   We also present a model-independent analysis of the new results of the CMS experiment on the $B \to K^* \mu^+\mu^-$ angular observables and compare them with the corresponding LHCb data. In addition, we update the global fit by including all $b \to s$ observables incorporating the new data from CMS. In these analyses, we use 10% or higher guesstimates of the non-factorisable power corrections as additional uncertainties, serving as a placeholder for robust estimates of these contributions.   Updating earlier results, we also analyse the combined LHCb and CMS data on the $B \to K^* \mu^+\mu^-$ angular observables using data-driven approaches to find indications whether these tensions between the QCDf predictions and the present data are due to underestimated subleading hadronic contributions or due to new physics effects. |
| 2025-08-13 | [Laboratory Measurements of Ca XIX Dielectronic Recombination Satellites](http://arxiv.org/abs/2508.09975v1) | Filipe Grilo, Marc Botz et al. | We report measurements of the K$\alpha$ emission from the astrophysically very abundant Ca XIX (He-like ion) and its satellite lines resonantly excited by dielectronic recombination (DR). We achieve an electron-energy resolution of 8 eV in a cryogenic electron beam ion trap, and determine the energies of the exciting electrons and the emitted photons up to the KLn ($n\le 8$) manifold with $0.05\%$ and $0.1\%$ respective uncertainties. For the KLL satellites, energies agree very well with our predictions using the Flexible Atomic Code (FAC) and previous state-of-the-art calculations. Our calculations also agree with our experimental direct excitation cross-sections for K$\alpha$ within their $10\%$ uncertainty. We extract DR coefficient rates and find good agreement with values tabulated in the OPEN-ADAS database. As an application, we experimentally benchmark Ca XIX atomic data used to model high-temperature astrophysical plasmas by comparing FAC synthetic spectra with recent XRISM observations revealing the contributions of DR satellites to the Ca XIX lines. |
| 2025-08-13 | [Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems](http://arxiv.org/abs/2508.09908v1) | Haoshu Cheng, Martin Guay et al. | In this paper, we investigate the problem of tracking formations driven by bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty in the presence of multiple moving leaders. To estimate the leaders' velocities and accelerations, we first design a distributed observer for the leader system, utilizing a bearing-based localization condition in place of the conventional connectivity assumption. This observer, coupled with an adaptive mechanism, enables the synthesis of a novel distributed control law that guides the formation towards the target formation, without requiring prior knowledge of the system parameters. Furthermore, we establish a sufficient condition, dependent on the initial formation configuration, that ensures collision avoidance throughout the formation evolution. The effectiveness of the proposed approach is demonstrated through a numerical example. |
| 2025-08-13 | [Multi-head committees enable direct uncertainty prediction for atomistic foundation models](http://arxiv.org/abs/2508.09907v1) | Hubert Beck, Pavol Simko et al. | Machine learning potentials have become a standard tool for atomistic materials modelling. While models continue to become more generalisable, an open challenge relates to efficient uncertainty predictions for active learning and robust error analysis. In this work, we utilise MACE and its multi-head mechanism to implement a committee neural network potential for message-passing architectures, where the committee comprises multiple output modules attached to the same atomic environment descriptors. As with traditional committees of independent networks, the standard deviation of the predictions functions as an estimate of the model's uncertainty. We show for a range of datasets in custom-build models that the uncertainty of the force predictions correlates well with the true errors. We subsequently apply this concept to foundation models, specifically MACE-MP-0, where we train only the newly attached output heads while keeping the remaining part of the model fixed. We use this approach in an active learning workflow to condense the training set of the foundation model to just 5\% of its original size. The foundation model multi-head committee trained on the condensed training set enables reliable uncertainty estimation without any substantial decrease in prediction accuracy. |
| 2025-08-13 | [$gg \to ZH$ : updated predictions at NLO QCD](http://arxiv.org/abs/2508.09905v1) | Benjamin Campillo Aveleira, Long Chen et al. | We present state-of-the-art predictions for the inclusive cross section of gluon-initiated $ZH$ production, following the recommendations of the LHC Higgs Working Group. In particular, we include NLO QCD corrections, where the virtual corrections are obtained from the combination of a forward expansion and a high-energy expansion, and the real corrections are exact. The expanded results for the virtual corrections are compared in detail to full numerical results. The updated predictions show a reduction of the scale uncertainties to the level of 15%, and they include an estimate of the top-mass-scheme uncertainty. |
| 2025-08-13 | [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](http://arxiv.org/abs/2508.09732v1) | Romeo Valentin, Sydney M. Katz et al. | Recent advances in data-driven computer vision have enabled robust autonomous navigation capabilities for civil aviation, including automated landing and runway detection. However, ensuring that these systems meet the robustness and safety requirements for aviation applications remains a major challenge. In this work, we present a practical vision-based pipeline for aircraft pose estimation from runway images that represents a step toward the ability to certify these systems for use in safety-critical aviation applications. Our approach features three key innovations: (i) an efficient, flexible neural architecture based on a spatial Soft Argmax operator for probabilistic keypoint regression, supporting diverse vision backbones with real-time inference; (ii) a principled loss function producing calibrated predictive uncertainties, which are evaluated via sharpness and calibration metrics; and (iii) an adaptation of Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling runtime detection and rejection of faulty model outputs. We implement and evaluate our pose estimation pipeline on a dataset of runway images. We show that our model outperforms baseline architectures in terms of accuracy while also producing well-calibrated uncertainty estimates with sub-pixel precision that can be used downstream for fault detection. |
| 2025-08-13 | [CKFNet: Neural Network Aided Cubature Kalman filtering](http://arxiv.org/abs/2508.09727v1) | Jinhui Hu, Haiquan Zhao et al. | The cubature Kalman filter (CKF), while theoretically rigorous for nonlinear estimation, often suffers performance degradation due to model-environment mismatches in practice. To address this limitation, we propose CKFNet-a hybrid architecture that synergistically integrates recurrent neural networks (RNN) with the CKF framework while preserving its cubature principles. Unlike conventional model-driven approaches, CKFNet embeds RNN modules in the prediction phase to dynamically adapt to unmodeled uncertainties, effectively reducing cumulative error propagation through temporal noise correlation learning. Crucially, the architecture maintains CKF's analytical interpretability via constrained optimization of cubature point distributions. Numerical simulation experiments have confirmed that our proposed CKFNet exhibits superior accuracy and robustness compared to conventional model-based methods and existing KalmanNet algorithms. |
| 2025-08-13 | [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](http://arxiv.org/abs/2508.09639v1) | Akshat Dubey, Aleksandar AnÅ¾el et al. | Explainable Artificial Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP), have become essential tools for interpreting complex ensemble tree-based models, especially in high-stakes domains such as healthcare analytics. However, SHAP values are usually treated as point estimates, which disregards the inherent and ubiquitous uncertainty in predictive models and data. This uncertainty has two primary sources: aleatoric and epistemic. The aleatoric uncertainty, which reflects the irreducible noise in the data. The epistemic uncertainty, which arises from a lack of data. In this work, we propose an approach for decomposing uncertainty in SHAP values into aleatoric, epistemic, and entanglement components. This approach integrates Dempster-Shafer evidence theory and hypothesis sampling via Dirichlet processes over tree ensembles. We validate the method across three real-world use cases with descriptive statistical analyses that provide insight into the nature of epistemic uncertainty embedded in SHAP explanations. The experimentations enable to provide more comprehensive understanding of the reliability and interpretability of SHAP-based attributions. This understanding can guide the development of robust decision-making processes and the refinement of models in high-stakes applications. Through our experiments with multiple datasets, we concluded that features with the highest SHAP values are not necessarily the most stable. This epistemic uncertainty can be reduced through better, more representative data and following appropriate or case-desired model development techniques. Tree-based models, especially bagging, facilitate the effective quantification of epistemic uncertainty. |
| 2025-08-13 | [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](http://arxiv.org/abs/2508.09623v1) | Akshay Thakur, Sawan Kumar et al. | Solving partial differential equations (PDEs) within the framework of probabilistic numerics offers a principled approach to quantifying epistemic uncertainty arising from discretization. By leveraging Gaussian process regression and imposing the governing PDE as a constraint at a finite set of collocation points, probabilistic numerics delivers mesh-free solutions at arbitrary locations. However, the high computational cost, which scales cubically with the number of collocation points, remains a critical bottleneck, particularly for large-scale or high-dimensional problems. We propose a scalable enhancement to this paradigm through two key innovations. First, we develop a stochastic dual descent algorithm that reduces the per-iteration complexity from cubic to linear in the number of collocation points, enabling tractable inference. Second, we exploit a clustering-based active learning strategy that adaptively selects collocation points to maximize information gain while minimizing computational expense. Together, these contributions result in an $h$-adaptive probabilistic solver that can scale to a large number of collocation points. We demonstrate the efficacy of the proposed solver on benchmark PDEs, including two- and three-dimensional steady-state elliptic problems, as well as a time-dependent parabolic PDE formulated in a space-time setting. |
| 2025-08-13 | [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](http://arxiv.org/abs/2508.09561v1) | Changyuan Zhao, Guangyuan Liu et al. | Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems. |
| 2025-08-12 | [Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs](http://arxiv.org/abs/2508.09135v1) | Wenxin Zhang, Mark van der Laan | Adaptive experimental designs have gained popularity in clinical trials and online experiments. Unlike traditional, fixed experimental designs, adaptive designs can dynamically adjust treatment randomization probabilities and other design features in response to data accumulated sequentially during the experiment. These adaptations are useful to achieve diverse objectives, including reducing uncertainty in the estimation of causal estimands or increasing participants' chances of receiving better treatments during the experiment. At the end of the experiment, it is often desirable to answer causal questions from the observed data. However, the adaptive nature of such experiments and the resulting dependence among observations pose significant challenges to providing valid statistical inference and efficient estimation of causal estimands. Building upon the Targeted Maximum Likelihood Estimator (TMLE) framework tailored for adaptive designs (van der Laan, 2008), we introduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a variety of causal estimands from adaptive experiment data. We establish asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed positivity and design stabilization assumptions for adaptive experiments. Motivated by efficiency results, we further propose a novel adaptive design aimed at minimizing the variance of estimators based on data generated under that design. Using the average treatment effect as a representative example, simulation studies show that ADL-TMLE demonstrates superior variance-reduction performance across different types of adaptive experiments, and that the proposed adaptive design attains lower variance than the standard efficiency-oriented adaptive design. Finally, we generalize this estimation and design framework to broader settings with longitudinal structures. |
| 2025-08-12 | [A New Method of Deriving Doppler Velocities for Solar Orbiter SPICE](http://arxiv.org/abs/2508.09121v1) | J. E. Plowman, D. M. Hassler et al. | This paper presents a follow-up to previous work on correcting PSF-induced Doppler artifacts in observations by the SPICE spectrograph on Solar Orbiter. In a previous paper, we demonstrated correction of these artifacts in the $y-\lambda$ plane with PSF Regularization, treating the forward problem with a method based on large sparse matrix inversion. It has since been found that similar apparent artifacts are also present in the $x-\lambda$ direction, i.e., across adjacent slit positions. This is difficult (although not impossible) to correct with the previous matrix inversion method due to the time variation between slit positions. We have therefore devised a new method which addresses both $x-\lambda$ and $y-\lambda$ artifacts simultaneously by applying wavelength dependent shifts at each $x-y$ plane of the spectral cube. This paper demonstrates the SPICE data issue, describes the new method, and shows a comparison with the previous one. We explore the time variation of the correction parameters for the SPICE data and show a clear orbit dependence. The results of the method are significantly higher quality derived Doppler signals, which we estimate at less than $\sim$ 5 km/s uncertainty for brighter lines in the absence of other systematics. Furthermore, we show the new SPICE polar observation results as a demonstration. The correction codes are written in Python, publicly available on GitHub, and can be directly applied to SPICE level 2 datasets. |
| 2025-08-12 | [Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring](http://arxiv.org/abs/2508.09085v1) | Zihan Fang, Zheng Lin et al. | Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness. |
| 2025-08-12 | [CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks](http://arxiv.org/abs/2508.09054v1) | Debdeep Mukherjee, Eduardo Di Santi et al. | Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability. |
| 2025-08-12 | [Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2508.09014v1) | Kaiwen Huang, Tao Zhou et al. | Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at https://github.com/taozh2017/UCSeg. |
| 2025-08-12 | [Current-Enhanced Excited States in Lattice QCD Three-Point Functions](http://arxiv.org/abs/2508.09006v1) | Lorenzo Barca | Excited-state contamination remains one of the leading sources of systematic uncertainty in the precise determination of hadron structure observables from lattice QCD. In this letter, we present a general argument, inspired by current-meson dominance and implemented through the variational method, to identify which excited states are enhanced by the choice of the inserted current and kinematics. The argument is supported by numerical evidence across multiple hadronic channels and provides both a conceptual understanding and practical guidance to account for excited-state effects in hadron three-point function analyses. |
| 2025-08-12 | [Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty](http://arxiv.org/abs/2508.08992v1) | Rui Wang, Qihan Lin et al. | Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT. |
| 2025-08-12 | [Mutually equi-biased bases](http://arxiv.org/abs/2508.08969v1) | Seyed Javad Akhtarshenas, Saman Karimi et al. | In the framework of mutually unbiased bases (MUBs), a measurement in one basis gives \emph{no information} about the outcomes of measurements in another basis. Here, we relax the no-information condition by allowing the $d$ outcomes to be predicted according to a predefined probability distribution $q=(q_0,\cdots,q_{d-1})$. The notion of mutual unbiasedness, however, is preserved by requiring that the extracted information is the same for any preparation and any measurement; regardless of which state from which basis is chosen to prepare the system, the outcomes of measuring the system with respect to the other basis generate the same probability distribution. In the light of this, we define the notion of \emph{mutually equi-biased bases} (MEBs) such that within each basis the states are equi-biased with respect to the states of the other basis and that the bases are mutually equi-biased with respect to each other. For $d=2,3$, we derive a complete set of $d+1$ MEBs. The mutual equi-biasedness imposes nontrivial constraints on the distribution $q$, leading for $d=3$ to the restriction $1/3\le\mu \le 1/2$ where $\mu=\sum_{k=0}^{2}q_k^2$. To capture the incompatibility of the measurements in MEBs, we derive an inequality for the probabilities of projective measurements in a qudit system, which yields an associated entropic uncertainty inequality. Finally, we construct a class of positive maps and their associated entanglement witnesses based on MEBs. While an entanglement witness constructed from MUBs is generally finer than one based on MEBs when both use the same number of bases, for certain values of the index $\mu$, employing a larger set of MEBs can yield a finer witness. We illustrate this behavior using isotropic states of a $3\times 3$ system. |
| 2025-08-12 | [A comprehensive analysis of Drell-Yan production uncertainties and mass effects at moderate and low dilepton masses](http://arxiv.org/abs/2508.08956v1) | Ekta Chaubey, Claude Duhr et al. | We present a thorough investigation of the sources of uncertainties to the Drell-Yan production using state-of-the-art predictions for both neutral and charged current channels, focusing on the low invariant mass region. Differential predictions for the invariant mass spectrum are provided at N$^3$LO supplemented with exact charm and bottom quark mass effects calculated at $\mathcal{O}(\alpha_s^2)$. The impact of PDF choices (including approximate N$^3$LO), scale variations, the variation of the strong coupling constant, and impact heavy quark mass effects on the distributions is studied in detail. We also comment on the correlation of high-energy astrophysical processes with the low-mass DY region. |
| 2025-08-12 | [Hi-fi functional priors by learning activations](http://arxiv.org/abs/2508.08880v1) | Marcin Sendera, Amin Sorkhei et al. | Function-space priors in Bayesian Neural Networks (BNNs) provide a more intuitive approach to embedding beliefs directly into the model's output, thereby enhancing regularization, uncertainty quantification, and risk-aware decision-making. However, imposing function-space priors on BNNs is challenging. We address this task through optimization techniques that explore how trainable activations can accommodate higher-complexity priors and match intricate target function distributions. We investigate flexible activation models, including Pade functions and piecewise linear functions, and discuss the learning challenges related to identifiability, loss construction, and symmetries. Our empirical findings indicate that even BNNs with a single wide hidden layer when equipped with flexible trainable activation, can effectively achieve desired function-space priors. |
| 2025-08-11 | [Autonomous Air-Ground Vehicle Operations Optimization in Hazardous Environments: A Multi-Armed Bandit Approach](http://arxiv.org/abs/2508.08217v1) | Jimin Choi, Max Z. Li | Hazardous environments such as chemical spills, radiological zones, and bio-contaminated sites pose significant threats to human safety and public infrastructure. Rapid and reliable hazard mitigation in these settings often unsafe for humans, calling for autonomous systems that can adaptively sense and respond to evolving risks. This paper presents a decision-making framework for autonomous vehicle dispatch in hazardous environments with uncertain and evolving risk levels. The system integrates a Bayesian Upper Confidence Bound (BUCB) sensing strategy with task-specific vehicle routing problems with profits (VRPP), enabling adaptive coordination of unmanned aerial vehicles (UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning. Using VRPP allows selective site visits under resource constraints by assigning each site a visit value that reflects sensing or cleaning priorities. Site-level hazard beliefs are maintained through a time-weighted Bayesian update. BUCB scores guide UAV routing to balance exploration and exploitation under uncertainty, while UGV routes are optimized to maximize expected hazard reduction under resource constraints. Simulation results demonstrate that our framework reduces the number of dispatch cycles to resolve hazards by around 30% on average compared to baseline dispatch strategies, underscoring the value of uncertainty-aware vehicle dispatch for reliable hazard mitigation. |
| 2025-08-11 | [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](http://arxiv.org/abs/2508.08206v1) | Amirhossein Taherpour, Abbas Taherpour et al. | We propose a joint learning framework for Byzantine-resilient spectrum sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic access under channel state information (CSI) uncertainty. The sensing stage performs logit-domain Bayesian updates with trimmed aggregation and attention-weighted consensus, and the base station (BS) fuses network beliefs with a conservative minimum rule, preserving detection accuracy under a bounded number of Byzantine users. Conditioned on the sensing outcome, we pose downlink design as sum mean-squared error (MSE) minimization under transmit-power and signal-leakage constraints and jointly optimize the BS precoder, IRS phase shifts, and user equalizers. With partial (or known) CSI, we develop an augmented-Lagrangian alternating algorithm with projected updates and provide provable sublinear convergence, with accelerated rates under mild local curvature. With unknown CSI, we perform constrained Bayesian optimization (BO) in a geometry-aware low-dimensional latent space using Gaussian process (GP) surrogates; we prove regret bounds for a constrained upper confidence bound (UCB) variant of the BO module, and demonstrate strong empirical performance of the implemented procedure. Simulations across diverse network conditions show higher detection probability at fixed false-alarm rate under adversarial attacks, large reductions in sum MSE for honest users, strong suppression of eavesdropper signal power, and fast convergence. The framework offers a practical path to secure opportunistic communication that adapts to CSI availability while coherently coordinating sensing and transmission through joint learning. |
| 2025-08-11 | [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](http://arxiv.org/abs/2508.08204v1) | Kyle Moore, Jesse Roberts et al. | There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis. |
| 2025-08-11 | [Atomic thermometry in optical lattice clocks](http://arxiv.org/abs/2508.08164v1) | Irene Goti, Tommaso Petrucciani et al. | Accurate measurement of atomic temperature is essential for a wide range of applications, from quantum sensing to precision metrology. In optical lattice clocks, precise characterization of atomic temperature is required to minimize systematic uncertainties at the $10^{-18}$ level. In this work, we investigate atomic temperature measurements in the ytterbium optical lattice clock developed at INRIM, IT-Yb1, employing sideband and Doppler spectroscopy across a wide range of trapping conditions. By implementing clock-line-mediated Sisyphus cooling, we reduce the atomic temperature and enable operation at shallower lattice depths down to $D = 50E_{R}$. We compare temperature estimates obtained from the harmonic oscillator model with those derived using a Born-Oppenheimer-based approach, which is expected to provide a more accurate description of atomic motion in both longitudinal and radial directions, especially for hotter atoms whose motion deviates from the harmonic regime. Discrepancies up to a factor of two in extracted temperatures are observed depending on the chosen model. We assess the impact of these modeling differences on the evaluation of lattice frequency shifts and find deviations up to $8\times10^{-17}$. Even though extended Sisyphus cooling reduces these inconsistencies to the $1\times10^{-18}$ level or below, residual biases may still limit the accuracy of optical lattice clocks. |
| 2025-08-11 | [Precision Test of Bound-State QED at Intermediate-Z with Kaonic Neon](http://arxiv.org/abs/2508.08161v1) | Manti S., Sgaramella F. et al. | We report Dirac-Fock calculations of transition energies for kaonic neon (KNe). For the most intense line, the 7-6 transition, the calculated energy is 9450.28 eV, which includes a bound-state QED (BSQED) contribution of 12.66 eV. This is in excellent agreement with the recent SIDDHARTHA-2 measurement at DA$\Phi$NE of 9450.23 $\pm$ 0.37 (stat.) $\pm$ 1.50 (syst.) eV. With the QED shift far exceeding experimental uncertainty, these results establish kaonic atoms as powerful platforms for precision tests of BSQED in intermediate-Z systems. |
| 2025-08-11 | [Robust Adaptive Discrete-Time Control Barrier Certificate](http://arxiv.org/abs/2508.08153v1) | Changrui Liu, Anil Alan et al. | This work develops a robust adaptive control strategy for discrete-time systems using Control Barrier Functions (CBFs) to ensure safety under parametric model uncertainty and disturbances. A key contribution of this work is establishing a barrier function certificate in discrete time for general online parameter estimation algorithms. This barrier function certificate guarantees positive invariance of the safe set despite disturbances and parametric uncertainty without access to the true system parameters. In addition, real-time implementation and inherent robustness guarantees are provided. Our approach demonstrates that, using the proposed robust adaptive CBF framework, the parameter estimation module can be designed separately from the CBF-based safety filter, simplifying the development of safe adaptive controllers for discrete-time systems. The resulting safety filter guarantees that the system remains within the safe set while adapting to model uncertainties, making it a promising strategy for real-world applications involving discrete-time safety-critical systems. |
| 2025-08-11 | [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](http://arxiv.org/abs/2508.08139v1) | Tianyi Zhou, Johanne Medina et al. | Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation. |
| 2025-08-11 | [OFAL: An Oracle-Free Active Learning Framework](http://arxiv.org/abs/2508.08126v1) | Hadi Khorsand, Vahid Pourahmadi | In the active learning paradigm, using an oracle to label data has always been a complex and expensive task, and with the emersion of large unlabeled data pools, it would be highly beneficial If we could achieve better results without relying on an oracle. This research introduces OFAL, an oracle-free active learning scheme that utilizes neural network uncertainty. OFAL uses the model's own uncertainty to transform highly confident unlabeled samples into informative uncertain samples. First, we start with separating and quantifying different parts of uncertainty and introduce Monte Carlo Dropouts as an approximation of the Bayesian Neural Network model. Secondly, by adding a variational autoencoder, we go on to generate new uncertain samples by stepping toward the uncertain part of latent space starting from a confidence seed sample. By generating these new informative samples, we can perform active learning and enhance the model's accuracy. Lastly, we try to compare and integrate our method with other widely used active learning sampling methods. |
| 2025-08-11 | [Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors](http://arxiv.org/abs/2508.08097v1) | Muhammad Asif, Zain Ali et al. | This work explores the integration of rate-splitting multiple access (RSMA), simultaneous wireless information and power transfer (SWIPT), and beyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the spectral-efficiency, energy-efficiency, coverage, and connectivity of future sixth-generation (6G) communication networks. Specifically, with a multiuser BD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding vectors, the common rate proportion of users, the power-splitting ratios, and scattering matrix of BD-RIS node, under the assumption of imperfect channel state information (CSI). Additionally, to better capture practical hardware behavior, we incorporate a nonlinear energy harvesting model under energy harvesting constraints. We design a robust optimization framework to maximize the system sum-rate, while explicitly accounting for the worst-case impact of CSI uncertainties. Further, we introduce an alternating optimization framework that partitions the problem into several blocks, which are optimized iteratively. More specifically, the transmit precoding vectors are optimized by reformulating the problem as a convex semidefinite programming through successive-convex approximation (SCA), whereas the power-splitting problem is solved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the scattering matrix of the BD-RIS, we first employ SCA to reformulate the problem into a convex form, and then design a manifold optimization strategy based on the Conjugate-Gradient method. Finally, numerical simulation results reveal that the proposed scheme provides significant performance improvements over existing benchmarks and demonstrates rapid convergence within a reasonable number of iterations. |
| 2025-08-11 | [FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence](http://arxiv.org/abs/2508.08075v1) | Meishen He, Wenjun Ma et al. | The Dempster-Shafer theory of evidence has been widely applied in the field of information fusion under uncertainty. Most existing research focuses on combining evidence within the same frame of discernment. However, in real-world scenarios, trained algorithms or data often originate from different regions or organizations, where data silos are prevalent. As a result, using different data sources or models to generate basic probability assignments may lead to heterogeneous frames, for which traditional fusion methods often yield unsatisfactory results. To address this challenge, this study proposes an open-world information fusion method, termed Full Negation Belief Transformation (FNBT), based on the Dempster-Shafer theory. More specially, a criterion is introduced to determine whether a given fusion task belongs to the open-world setting. Then, by extending the frames, the method can accommodate elements from heterogeneous frames. Finally, a full negation mechanism is employed to transform the mass functions, so that existing combination rules can be applied to the transformed mass functions for such information fusion. Theoretically, the proposed method satisfies three desirable properties, which are formally proven: mass function invariance, heritability, and essential conflict elimination. Empirically, FNBT demonstrates superior performance in pattern classification tasks on real-world datasets and successfully resolves Zadeh's counterexample, thereby validating its practical effectiveness. |
| 2025-08-08 | [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](http://arxiv.org/abs/2508.06468v1) | Jesutofunmi Ajayi, Antonio Di Maio et al. | Network Slicing has emerged as a powerful technique to enable cost-effective, multi-tenant communications and services over a shared physical mobile network infrastructure. One major challenge of service provisioning in slice-enabled networks is the uncertainty in the demand for the limited network resources that must be shared among existing slices and potentially new Network Slice Requests. In this paper, we consider admission control of Network Slice Requests in an online setting, with the goal of maximizing the long-term revenue received from admitted requests. We model the Slice Admission Control problem as an Online Multidimensional Knapsack Problem and present two reservation-based policies and their algorithms, which have a competitive performance for Online Multidimensional Knapsack Problems. Through Monte Carlo simulations, we evaluate the performance of our online admission control method in terms of average revenue gained by the Infrastructure Provider, system resource utilization, and the ratio of accepted slice requests. We compare our approach with those of the online First Come First Serve greedy policy. The simulation's results prove that our proposed online policies increase revenues for Infrastructure Providers by up to 12.9 % while reducing the average resource consumption by up to 1.7% In particular, when the tenants' economic inequality increases, an Infrastructure Provider who adopts our proposed online admission policies gains higher revenues compared to an Infrastructure Provider who adopts First Come First Serve. |
| 2025-08-08 | [Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials](http://arxiv.org/abs/2508.06456v1) | Yonatan Kurniawan, Mingjian Wen et al. | Machine learning interatomic potentials (MLIPs) enable atomistic simulations with near first-principles accuracy at substantially reduced computational cost, making them powerful tools for large-scale materials modeling. The accuracy of MLIPs is typically validated on a held-out dataset of \emph{ab initio} energies and atomic forces. However, accuracy on these small-scale properties does not guarantee reliability for emergent, system-level behavior -- precisely the regime where atomistic simulations are most needed, but for which direct validation is often computationally prohibitive. As a practical heuristic, predictive precision -- quantified as inverse uncertainty -- is commonly used as a proxy for accuracy, but its reliability remains poorly understood, particularly for system-level predictions. In this work, we systematically assess the relationship between predictive precision and accuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes, focusing on ensemble-based uncertainty quantification methods for neural network potentials, including bootstrap, dropout, random initialization, and snapshot ensembles. We use held-out cross-validation for ID assessment and calculate cold curve energies and phonon dispersion relations for OOD testing. These evaluations are performed across various carbon allotropes as representative test systems. We find that uncertainty estimates can behave counterintuitively in OOD settings, often plateauing or even decreasing as predictive errors grow. These results highlight fundamental limitations of current uncertainty quantification approaches and underscore the need for caution when using predictive precision as a stand-in for accuracy in large-scale, extrapolative applications. |
| 2025-08-08 | [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](http://arxiv.org/abs/2508.06452v1) | Mattia Litrico, Mario Valerio Giuffrida et al. | Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance. |
| 2025-08-08 | [$b \to c$ semileptonic sum rule: Current status and prospects](http://arxiv.org/abs/2508.06322v1) | Motoi Endo, Syuhei Iguro et al. | The $b \to c$ semileptonic sum rules provide relations between the decay rates of $B \to D^{(*)} \tau\bar\nu$ and $\Lambda_b \to \Lambda_c \tau\bar\nu$. Starting from the heavy quark and zero-recoil limits, we revisit the derivation of the sum rule for total decay rates. We then examine deviations from the limits and investigate corrections arising from realistic hadron masses and higher-order contributions to form factors, taking account of uncertainties. We show that these corrections are negligible compared to current experimental uncertainties, indicating that the sum rule is useful for cross-checking experimental consistency and testing the validity of the Standard Model predictions. In future, precise determinations of the form factors particularly for the tensor operator will be necessary to compare the sum rule predictions with $\Lambda_b \to \Lambda_c \tau\bar\nu$ data from the LHCb experiment and the Tera-Z projects. |
| 2025-08-08 | [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](http://arxiv.org/abs/2508.06317v1) | Jian Hu, Zixu Cheng et al. | Video Temporal Grounding (TG) aims to temporally locate video segments matching a natural language description (a query) in a long video. While Vision-Language Models (VLMs) are effective at holistic semantic matching, they often struggle with fine-grained temporal localisation. Recently, Group Relative Policy Optimisation (GRPO) reformulates the inference process as a reinforcement learning task, enabling fine-grained grounding and achieving strong in-domain performance. However, GRPO relies on labelled data, making it unsuitable in unlabelled domains. Moreover, because videos are large and expensive to store and process, performing full-scale adaptation introduces prohibitive latency and computational overhead, making it impractical for real-time deployment. To overcome both problems, we introduce a Data-Efficient Unlabelled Cross-domain Temporal Grounding method, from which a model is first trained on a labelled source domain, then adapted to a target domain using only a small number of unlabelled videos from the target domain. This approach eliminates the need for target annotation and keeps both computational and storage overhead low enough to run in real time. Specifically, we introduce. Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain knowledge transfer in learning video temporal grounding without target labels. URPA generates multiple candidate predictions using GRPO rollouts, averages them to form a pseudo label, and estimates confidence from the variance across these rollouts. This confidence then weights the training rewards, guiding the model to focus on reliable supervision. Experiments on three datasets across six cross-domain settings show that URPA generalises well using only a few unlabelled target videos. Codes will be released once published. |
| 2025-08-08 | [A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions](http://arxiv.org/abs/2508.06303v1) | Gerhard Kirsten, Bilgesu Bilgin et al. | Computing with discrete representations of high-dimensional probability distributions is fundamental to uncertainty quantification, Bayesian inference, and stochastic modeling. However, storing and manipulating such distributions suffers from the curse of dimensionality, as memory and computational costs grow exponentially with dimension. Monte Carlo methods require thousands to billions of samples, incurring high computational costs and producing inconsistent results due to stochasticity. We present an efficient tensor train method for performing exact arithmetic operations on discretizations of continuous probability distributions while avoiding exponential growth. Our approach leverages low-rank tensor train decomposition to represent latent random variables compactly using Dirac deltas, enabling deterministic addition, subtraction and multiplication operations directly in the compressed format. We develop an efficient implementation using sparse matrices and specialized data structures that further enhances performance. Theoretical analysis demonstrates polynomial scaling of memory and computational complexity under rank assumptions, and shows how statistics of latent variables can be computed with polynomial complexity. Numerical experiments spanning randomized linear algebra to stochastic differential equations demonstrate orders-of-magnitude improvements in memory usage and computational time compared to conventional approaches, enabling tractable deterministic computations on discretized random variables in previously intractable dimensions. |
| 2025-08-08 | [Situationally-aware Path Planning Exploiting 3D Scene Graphs](http://arxiv.org/abs/2508.06283v1) | Saad Ejaz, Marco Giberna et al. | 3D Scene Graphs integrate both metric and semantic information, yet their structure remains underutilized for improving path planning efficiency and interpretability. In this work, we present S-Path, a situationally-aware path planner that leverages the metric-semantic structure of indoor 3D Scene Graphs to significantly enhance planning efficiency. S-Path follows a two-stage process: it first performs a search over a semantic graph derived from the scene graph to yield a human-understandable high-level path. This also identifies relevant regions for planning, which later allows the decomposition of the problem into smaller, independent subproblems that can be solved in parallel. We also introduce a replanning mechanism that, in the event of an infeasible path, reuses information from previously solved subproblems to update semantic heuristics and prioritize reuse to further improve the efficiency of future planning attempts. Extensive experiments on both real-world and simulated environments show that S-Path achieves average reductions of 5.7x in planning time while maintaining comparable path optimality to classical sampling-based planners and surpassing them in complex scenarios, making it an efficient and interpretable path planner for environments represented by indoor 3D Scene Graphs. |
| 2025-08-08 | [Thermodynamic uncertainty relation for feedback cooling](http://arxiv.org/abs/2508.06174v1) | Kousuke Kumasaki, Kaito Tojo et al. | Feedback cooling enables a system to achieve low temperatures through measurement-based control. Determining the thermodynamic cost required to achieve the ideal cooling efficiency within a finite time remains an important problem. In this work, we establish a thermodynamic uncertainty relation (TUR) for feedback cooling in classical underdamped Langevin systems, thereby deriving a trade-off between the cooling efficiency and the entropy reduction rate. The obtained TUR implies that simultaneous achievement of the ideal cooling efficiency and finite entropy reduction rate is asymptotically possible by letting the fluctuation of the reversible local mean velocity diverge. This is shown to be feasible by using a feedback control based on the Kalman filter. Our results clarify the thermodynamic costs of achieving the fundamental cooling limit of feedback control from the perspective of the TUR. |
| 2025-08-08 | [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](http://arxiv.org/abs/2508.06169v1) | Wenpeng Xing, Jie Chen et al. | Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts. |
| 2025-08-08 | [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](http://arxiv.org/abs/2508.06145v1) | Byeonghun Bang, Jongsuk Yoon et al. | The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information. |
| 2025-08-07 | [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](http://arxiv.org/abs/2508.05634v1) | Jianpeng Yao, Xiaopan Zhang et al. | Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/. |
| 2025-08-07 | [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](http://arxiv.org/abs/2508.05544v1) | Guang Yang, Xinyang Liu | Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications. |
| 2025-08-07 | [Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy](http://arxiv.org/abs/2508.05466v1) | Yun Li, Jicheng Shi et al. | This paper studies the finite-horizon robust optimal control of linear systems subject to model mismatch and additive stochastic disturbances. Utilizing the system level synthesis (SLS) parameterization, we propose a novel SLS design using output-feedback affine control policy and extend it to a distributionally robust setting to improve system resilience by minimizing the cost function while ensuring constraint satisfaction against the worst-case uncertainty distribution. The scopes of model mismatch and stochastic disturbances are quantified using the 1-norm and a Wasserstein metric-based ambiguity set, respectively. For the closed-loop dynamics, we analyze the distributional shift between the predicted output-input response -- computed using nominal parameters and empirical disturbance samples -- and the actual closed-loop distribution, highlighting its dependence on model mismatch and SLS parameterization. Assuming convex and Lipschitz continuous cost functions and constraints, we derive a tractable reformulation of the distributionally robust SLS (DR-SLS) problem by leveraging tools from robust control and distributionally robust optimization (DRO). Numerical experiments validate the performance and robustness of the proposed approach. |
| 2025-08-07 | [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](http://arxiv.org/abs/2508.05454v1) | Wei Li, Zixin Wang et al. | Accurate and reliable energy time series prediction is of great significance for power generation planning and allocation. At present, deep learning time series prediction has become the mainstream method. However, the multi-scale time dynamics and the irregularity of real data lead to the limitations of the existing methods. Therefore, we propose EnergyPatchTST, which is an extension of the Patch Time Series Transformer specially designed for energy forecasting. The main innovations of our method are as follows: (1) multi-scale feature extraction mechanism to capture patterns with different time resolutions; (2) probability prediction framework to estimate uncertainty through Monte Carlo elimination; (3) integration path of future known variables (such as temperature and wind conditions); And (4) Pre-training and Fine-tuning examples to enhance the performance of limited energy data sets. A series of experiments on common energy data sets show that EnergyPatchTST is superior to other commonly used methods, the prediction error is reduced by 7-12%, and reliable uncertainty estimation is provided, which provides an important reference for time series prediction in the energy field. |
| 2025-08-07 | [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](http://arxiv.org/abs/2508.05441v1) | Zuyuan Zhang, Arnob Ghosh et al. | Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\alpha$ achieves explicit tail-risk control over the expected loss in the "worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability. |
| 2025-08-07 | [LLM-based Multi-Agent Copilot for Quantum Sensor](http://arxiv.org/abs/2508.05421v1) | Rong Sha, Binglin Wang et al. | Large language models (LLM) exhibit broad utility but face limitations in quantum sensor development, stemming from interdisciplinary knowledge barriers and involving complex optimization processes. Here we present QCopilot, an LLM-based multi-agent framework integrating external knowledge access, active learning, and uncertainty quantification for quantum sensor design and diagnosis. Comprising commercial LLMs with few-shot prompt engineering and vector knowledge base, QCopilot employs specialized agents to adaptively select optimization methods, automate modeling analysis, and independently perform problem diagnosis. Applying QCopilot to atom cooling experiments, we generated 10${}^{\rm{8}}$ sub-$\rm{\mu}$K atoms without any human intervention within a few hours, representing $\sim$100$\times$ speedup over manual experimentation. Notably, by continuously accumulating prior knowledge and enabling dynamic modeling, QCopilot can autonomously identify anomalous parameters in multi-parameter experimental settings. Our work reduces barriers to large-scale quantum sensor deployment and readily extends to other quantum information systems. |
| 2025-08-07 | [Metallicity of Active Galactic Nuclei from ultraviolet and optical emission lines-II. Revisiting the $C43$ metallicity calibration and its implications](http://arxiv.org/abs/2508.05397v1) | O. L. Dors, C. B. Oliveira et al. | In this study, a new semi-empirical calibration is proposed between ultraviolet emission lines (\ion{C}{iii}]$\lambda1909$, \ion{C}{iv}$\lambda1549$, \ion{He}{ii}]$\lambda1640$) of type~2 AGNs and their metallicity ($Z$). This calibration is derived by comparing a large sample of 106 objects (data taken from the literature) located over a wide range of redshifts ($0 \: \lesssim \: z \: \lesssim \: 4.0$) with predictions from photoionization models that adopt a recent C/O-O/H relation derived via estimates using the $T_{\rm e}$ method, which is considered the most reliable method. We found that the new calibration produces $Z$ values in agreement (within an uncertainty of $\pm 0.1$ dex) with those from other calibrations and from estimates via the $T_{\rm e}$-method. We find also that AGN metallicities are already high at early epochs, with no evidence for monotonic evolution across the redshift range $0 \: \lesssim \: z \: \lesssim \: 12$. Notably, the highest metallicities in our sample, reaching up to $\rm 4\: Z_{\odot}$, are found in objects at $2 \lesssim z \lesssim 3$. This redshift range coincides with the peak of the cosmic star formation rate history, suggesting a strong connection between the major epoch of star formation, black hole growth, and rapid metal enrichment in the host galaxies of AGNs. Furthermore, our analysis reveals no significant correlation between AGN metallicity and radio properties (radio spectral index or radio luminosity) or host galaxy stellar mass. The lack of a clear mass-metallicity relation, consistent with findings for local AGNs, suggests that the chemical evolution of the nuclear gas is decoupled from the global properties of the host galaxy. |
| 2025-08-07 | [Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games](http://arxiv.org/abs/2508.05378v1) | Zhisen Jiang, Saverio Bolognani et al. | The integration of distributed energy resources into transmission grid operations presents a complex challenge, particularly in the context of reactive power procurement for voltage support. This paper addresses this challenge by formulating the voltage regulation problem as a Stackelberg game, where the Transmission System Operator (TSO) designs incentives to guide the reactive power responses of Distribution System Operators (DSOs). We utilize a gradient-based iterative algorithm that updates the incentives to ensure that DSOs adjust their reactive power injections to maintain voltage stability. We incorporate principles from online feedback optimization to enable real-time implementation, utilizing voltage measurements in both TSO's and DSOs' policies. This approach not only enhances the robustness against model uncertainties and changing operating conditions but also facilitates the co-design of incentives and automation. Numerical experiments on a 5-bus transmission grid demonstrate the effectiveness of our approach in achieving voltage regulation while accommodating the strategic interactions of self-interested DSOs. |
| 2025-08-07 | [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](http://arxiv.org/abs/2508.05342v1) | Shunlei Li, Longsen Gao et al. | Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. |
| 2025-08-07 | [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](http://arxiv.org/abs/2508.05310v1) | Jelle Luijkx, Zlatan AjanoviÄ‡ et al. | Human teaching effort is a significant bottleneck for the broader applicability of interactive imitation learning. To reduce the number of required queries, existing methods employ active learning to query the human teacher only in uncertain, risky, or novel situations. However, during these queries, the novice's planned actions are not utilized despite containing valuable information, such as the novice's capabilities, as well as corresponding uncertainty levels. To this end, we allow the novice to say: "I plan to do this, but I am uncertain." We introduce the Active Skill-level Data Aggregation (ASkDAgger) framework, which leverages teacher feedback on the novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating threshold to track sensitivity, specificity, or a minimum success rate; (2) Foresight Interactive Experience Replay (FIER), which recasts valid and relabeled novice action plans into demonstrations; and (3) Prioritized Interactive Experience Replay (PIER), which prioritizes replay based on uncertainty, novice success, and demonstration age. Together, these components balance query frequency with failure incidence, reduce the number of required demonstration annotations, improve generalization, and speed up adaptation to changing domains. We validate the effectiveness of ASkDAgger through language-conditioned manipulation tasks in both simulation and real-world environments. Code, data, and videos are available at https://askdagger.github.io. |
| 2025-08-06 | [Occupancy Learning with Spatiotemporal Memory](http://arxiv.org/abs/2508.04705v1) | Ziyang Leng, Jiawei Yang et al. | 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%. |
| 2025-08-06 | [Open Scene Graphs for Open-World Object-Goal Navigation](http://arxiv.org/abs/2508.04678v1) | Joel Loo, Zhanxin Wu et al. | How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments. |
| 2025-08-06 | [Stochastic Calculus for Pathwise Observables of Markov-Jump Processes: Unification of Diffusion and Jump Dynamics](http://arxiv.org/abs/2508.04647v1) | Lars TorbjÃ¸rn Stutzer, Cai Dieball et al. | Path-wise observables--functionals of stochastic trajectories--are at the heart of time-average statistical mechanics and are central to thermodynamic inequalities such as uncertainty relations, speed limits, and correlation-bounds. They provide a means of thermodynamic inference in the typical situation, when not all dissipative degrees of freedom in a system are experimentally accessible. So far, theories focusing on path-wise observables have been developing in two major directions, diffusion processes and Markov-jump dynamics, in a virtually disjoint manner. Moreover, even the respective results for diffusion and jump dynamics were derived with a patchwork of different approaches that are predominantly indirect. Stochastic calculus was recently shown to provide a direct approach to path-wise observables of diffusion processes, while a corresponding framework for jump dynamics remained elusive. In our work we develop, in an exact parallelism with continuous-space diffusion, a complete stochastic calculus for path-wise observables of Markov-jump processes. We formulate a "Langevin equation" for jump processes, define general path-wise observables, and establish their covariation structure, whereby we fully account for transients and time-inhomogeneous dynamics. We prove the known kinds of thermodynamic inequalities in their most general form and discus saturation conditions. We determine the response of path-wise observables to general (incl. thermal) perturbations and carry out the continuum limit to achieve the complete unification of diffusion and jump dynamics. Our results open new avenues in the direction of discrete-state analogs of generative diffusion models and the learning of stochastic thermodynamics from fluctuating trajectories. |
| 2025-08-06 | [Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials](http://arxiv.org/abs/2508.04633v1) | James P. Long, Abhishikta Roy et al. | In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model. |
| 2025-08-06 | [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](http://arxiv.org/abs/2508.04588v1) | Nicola Casali, Alessandro Brusaferri et al. | Accurate estimation of intravoxel incoherent motion (IVIM) parameters from diffusion-weighted MRI remains challenging due to the ill-posed nature of the inverse problem and high sensitivity to noise, particularly in the perfusion compartment. In this work, we propose a probabilistic deep learning framework based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling estimation of total predictive uncertainty and decomposition into aleatoric (AU) and epistemic (EU) components. The method was benchmarked against non probabilistic neural networks, a Bayesian fitting approach and a probabilistic network with single Gaussian parametrization. Supervised training was performed on synthetic data, and evaluation was conducted on both simulated and two in vivo datasets. The reliability of the quantified uncertainties was assessed using calibration curves, output distribution sharpness, and the Continuous Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper predictive distributions for the D and f parameters, although slight overconfidence was observed in D*. The Robust Coefficient of Variation (RCV) indicated smoother in vivo estimates for D* with MDNs compared to Gaussian model. Despite the training data covering the expected physiological range, elevated EU in vivo suggests a mismatch with real acquisition conditions, highlighting the importance of incorporating EU, which was allowed by DE. Overall, we present a comprehensive framework for IVIM fitting with uncertainty quantification, which enables the identification and interpretation of unreliable estimates. The proposed approach can also be adopted for fitting other physical models through appropriate architectural and simulation adjustments. |
| 2025-08-06 | [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](http://arxiv.org/abs/2508.04570v1) | A. Tarik Leblebici, Sumeyra Hassan et al. | This paper proposes a joint communication and indoor positioning (JCP) system based on visible light communication (VLC) designed for high-precision indoor environments. The framework supports 2D and 3D positioning using received signal strength (RSS) from pilot transmissions, enhanced by the radical axis theorem to improve accuracy under measurement uncertainties. Communication is achieved using spatial modulation (SM) with M-ary pulse amplitude modulation (PAM), where data is conveyed through the modulation symbol and the active light-emitting diode (LED) index, improving spectral efficiency while maintaining low complexity. A pilot-aided least squares (LS) estimator is employed for joint channel and dimming coefficient estimation, enabling robust symbol detection in multipath environments characterized by both line-of-sight (LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician fading. The proposed system incorporates a dimming control mechanism to meet lighting requirements while maintaining reliable communication and positioning performance. Simulation results demonstrate sub-centimeter localization accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below 10^{-6} for low-order PAM schemes. Additionally, comparative analysis across user locations reveals that positioning and communication performance improve significantly near the geometric center of the LED layout. These findings validate the effectiveness of the proposed system for future 6G indoor networks requiring integrated localization and communication under practical channel conditions. |
| 2025-08-06 | [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](http://arxiv.org/abs/2508.04537v1) | Alkesh K. Srivastava, Aamodh Suresh et al. | We address the challenge of multi-robot autonomous hazard mapping in high-risk, failure-prone, communication-denied environments such as post-disaster zones, underground mines, caves, and planetary surfaces. In these missions, robots must explore and map hazards while minimizing the risk of failure due to environmental threats or hardware limitations. We introduce a behavior-adaptive, information-theoretic planning framework for multi-robot teams grounded in the concept of Behavioral Entropy (BE), that generalizes Shannon entropy (SE) to capture diverse human-like uncertainty evaluations. Building on this formulation, we propose the Behavior-Adaptive Path Planning (BAPP) framework, which modulates information gathering strategies via a tunable risk-sensitivity parameter, and present two planning algorithms: BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for safe deployment under high risk. We provide theoretical insights on the informativeness of the proposed BAPP framework and validate its effectiveness through both single-robot and multi-robot simulations. Our results show that the BAPP stack consistently outperforms Shannon-based and random strategies: BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot survivability with minimal loss in information gain. In multi-agent deployments, BAPP scales effectively through spatial partitioning, mobile base relocation, and role-aware heterogeneity. These findings underscore the value of behavior-adaptive planning for robust, risk-sensitive exploration in complex, failure-prone environments. |
| 2025-08-06 | [Metric Learning in an RKHS](http://arxiv.org/abs/2508.04476v1) | Gokcan Tatli, Yi Chen et al. | Metric learning from a set of triplet comparisons in the form of "Do you think item h is more similar to item i or item j?", indicating similarity and differences between items, plays a key role in various applications including image retrieval, recommendation systems, and cognitive psychology. The goal is to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric learning using kernel methods and neural networks have shown great empirical promise. While previous works have addressed certain aspects of this problem, there is little or no theoretical understanding of such methods. The exception is the special (linear) case in which the RKHS is the standard Euclidean space $\mathbb{R}^d$; there is a comprehensive theory for metric learning in $\mathbb{R}^d$. This paper develops a general RKHS framework for metric learning and provides novel generalization guarantees and sample complexity bounds. We validate our findings through a set of simulations and experiments on real datasets. Our code is publicly available at https://github.com/RamyaLab/metric-learning-RKHS. |
| 2025-08-06 | [Case Studies of Generative Machine Learning Models for Dynamical Systems](http://arxiv.org/abs/2508.04459v1) | Nachiket U. Bapat, Randy C. Paffenroth et al. | Systems like aircraft and spacecraft are expensive to operate in the real world. The design, validation, and testing for such systems therefore relies on a combination of mathematical modeling, abundant numerical simulations, and a relatively small set of real-world experiments. Due to modeling errors, simplifications, and uncertainties, the data synthesized by simulation models often does not match data from the system's real-world operation. We consider the broad research question of whether this model mismatch can be significantly reduced by generative artificial intelligence models (GAIMs). Unlike text- or image-processing, where generative models have attained recent successes, GAIM development for aerospace engineering applications must not only train with scarce operational data, but their outputs must also satisfy governing equations based on natural laws, e.g., conservation laws. The scope of this paper primarily focuses on two case studies of optimally controlled systems that are commonly understood and employed in aircraft guidance, namely: minimum-time navigation in a wind field and minimum-exposure navigation in a threat field. We report GAIMs that are trained with a relatively small set, of the order of a few hundred, of examples and with underlying governing equations. By focusing on optimally controlled systems, we formulate training loss functions based on invariance of the Hamiltonian function along system trajectories. We investigate three GAIM architectures, namely: the generative adversarial network (GAN) and two variants of the variational autoencoder (VAE). We provide architectural details and thorough performance analyses of these models. The main finding is that our new models, especially the VAE-based models, are able to synthesize data that satisfy the governing equations and are statistically similar to the training data despite small volumes of training data. |
| 2025-08-06 | [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](http://arxiv.org/abs/2508.04457v1) | Simon Baur, Wojciech Samek et al. | Reliable uncertainty quantification is crucial for trustworthy decision-making and the deployment of AI models in medical imaging. While prior work has explored the ability of neural networks to quantify predictive, epistemic, and aleatoric uncertainties using an information-theoretical approach in synthetic or well defined data settings like natural image classification, its applicability to real life medical diagnosis tasks remains underexplored. In this study, we provide an extensive uncertainty quantification benchmark for multi-label chest X-ray classification using the MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for convolutional (ResNet) and transformer-based (Vision Transformer) architectures across a wide range of tasks. Additionally, we extend Evidential Deep Learning, HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting. Our analysis provides insights into uncertainty estimation effectiveness and the ability to disentangle epistemic and aleatoric uncertainties, revealing method- and architecture-specific strengths and limitations. |
| 2025-08-05 | [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2508.03692v1) | Ao Liang, Youquan Liu et al. | Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community. |
| 2025-08-05 | [La La LiDAR: Large-Scale Layout Generation from LiDAR Data](http://arxiv.org/abs/2508.03691v1) | Youquan Liu, Lingdong Kong et al. | Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation. |
| 2025-08-05 | [Streaming Generated Gaussian Process Experts for Online Learning and Control](http://arxiv.org/abs/2508.03679v1) | Zewen Yang, Dongfa Zhang et al. | Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \underline{s}treaming \underline{k}ernel-induced progressivel\underline{y} generated expert framework of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches. |
| 2025-08-05 | [Theoretical framework for lattice QCD computations of $B\to K \ell^+ \ell^-$ and $\bar{B}_s\to \ell^+\ell^- Î³$ decays rates, including contributions from "Charming Penguins"](http://arxiv.org/abs/2508.03655v1) | R. Frezzotti, G. Gagliardi et al. | We develop a strategy for computing the $B\to K\ell^+\ell^-$ and $\bar{B}_s\to\gamma\ell^+\ell^-$ decay amplitudes using lattice QCD (where $\ell^\pm$ are charged leptons). We focus on those terms which contain complex contributions to the amplitude, due to on-shell intermediate states propagating between the weak operator and electromagnetic current(s). Such terms, which are generally estimated using model calculations and represent significant uncertainties in the phenomenological predictions for these decays, cannot be computed using standard lattice QCD techniques. It has recently been shown that such contributions can be computed using spectral-density methods and our proposed strategy, which we discuss in detail, is built on this approach. The complex contributions include the ``charming penguins" (matrix elements of the current-current operators $O_1^{(c)}$ and $O_2^{(c)}$ defined in Eq. (6) below), in which the charm-quark loop can propagate long distances, particularly close to the region of charmonium resonances. They also include the contributions from the chromomagnetic operator ($O_8$ in standard notation, defined in Eq. (8) below). We discuss the renormalization of the ultra-violet divergences, and in particular those which arise due to ``contact" terms, and explain how those which appear as inverse powers of the lattice spacing can be subtracted non-perturbatively. We apply the spectral density methods in an instructive exploratory computation of the charming penguin diagram in $B\to K\ell^+\ell^-$ decays in which the virtual photon is emitted from the charm-quark loop (the diagram in Fig. 1(a) below) and discuss the prospects and strategies for the reliable determination of the amplitudes in future dedicated computations. |
| 2025-08-05 | [RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data](http://arxiv.org/abs/2508.03578v1) | Jonas Leo Mueller, Lukas Engel et al. | Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications. |
| 2025-08-05 | [Machine learning the single-$Î›$ hypernuclei with neural-network quantum states](http://arxiv.org/abs/2508.03575v1) | Zi-Xiao Zhang, Yi-Long Yang et al. | Single-$\Lambda$ hypernuclei are the most straightforward extension of atomic nuclei. A thorough description of baryonic system beyond first-generation quark sector is indispensable for the maturation of nuclear $ab$ $initio$ methods. This study pioneers the application of neural-network quantum states to hypernuclei, with trainable parameters determined by variational Monte Carlo approach (VMC-NQS). In order to reduce the numerical uncertainty and treat the nucleons and hyperons in a unified manner, spinor grouping (SG) method is proposed to analytically integrate out isospin degrees of freedom. A novel spin purification scheme is developed to address the severe spin contamination occurring in standard energy minimization due to the weakly bound characteristic of light single-$\Lambda$ hypernuclei. The energy spectrum of $s$-shell hypernuclei is computed with one-thousandth level accuracy and benchmarked against existing stochastic variational results, showing superior performance. By comparing two different sets of Hamiltonian based on pionless effective field theory (pionless EFT), we choose an optimal model and further carry out calculations of selected $p$-shell charge-symmetric hypernuclei with mass number up to 13, exhibiting satisfactory consistency with experimental results. Our findings underscore the potential of VMC-NQS family in approaching exact solution of few-body systems and the accuracy of pionless EFT in modeling hypernuclei. This is crucial for understanding hyperon-nucleon-nucleon and hyperon-hyperon-nucleon interactions, providing a powerful tool for precisely predicting the properties of multi-strangeness hypernuclei. |
| 2025-08-05 | [Regulator and gauge dependence of the Abelian gauge coupling in asymptotically safe quantum gravity](http://arxiv.org/abs/2508.03563v1) | Maksym Riabokon, Marc Schiffer et al. | Both General Relativity and the Standard Model of particle physics are not UV complete. General Relativity is perturbatively non-renormalizable, while the Standard Model features Landau poles, where couplings are predicted to diverge at finite energies, e.g., in the Abelian gauge sector. Asymptotically safe quantum gravity may resolve both of these issues at the same time. In this paper, we assess the systematic uncertainties associated with this scenario, in particular with the gravitationally induced UV-completion of the Abelian gauge sector. Specifically, we study the dependence of this qualitative feature, namely the existence of a UV-complete gauge sector, on unphysical choices like the gauge, and the regulator function. Intriguingly, in some scenarios, we find simultaneous points of minimal sensitivity relative to both the regulator and gauge parameters, which allow for a UV completion. This provides further indications that the simultaneous UV-completion of quantum gravity and matter via an asymptotically safe fixed point is a robust physical feature, and that physical quantities, like scaling exponents, can become independent of unphysical choices. |
| 2025-08-05 | [An Evolutionary Analysis of Narrative Selection](http://arxiv.org/abs/2508.03540v1) | Federico Innocenti, Roberto Rozzi | We study the performance of different methods for processing information, incorporating narrative selection within an evolutionary model. All agents update their beliefs according to Bayes' Rule, but some strategically choose the narrative they use in updating according to heterogeneous criteria. We simulate the endogenous composition of the population, considering different laws of motion for the underlying state of the world. We find that conformists -- that is, agents that choose the narrative to conform to the average belief in the population -- have an evolutionary advantage over other agents across all specifications. The survival chances of the remaining types depend on the uncertainty regarding the state of the world. Agents who tend to develop mild beliefs perform better when the uncertainty is high, whereas agents who tend to develop extreme beliefs perform better when the uncertainty is low. |
| 2025-08-05 | [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](http://arxiv.org/abs/2508.03520v1) | Md Rakibul Hasan, Md Zakir Hossain et al. | Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems. |
| 2025-08-05 | [MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation](http://arxiv.org/abs/2508.03511v1) | Yazhou Zhu, Haofeng Zhang | Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: https://github.com/YazhouZhu19/MAUP. |
| 2025-08-04 | [DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting](http://arxiv.org/abs/2508.02616v1) | Ali Forootani, Mohammad Khosravi et al. | Time series forecasting plays a vital role across scientific, industrial, and environmental domains, especially when dealing with high-dimensional and nonlinear systems. While Transformer-based models have recently achieved state-of-the-art performance in long-range forecasting, they often suffer from interpretability issues and instability in the presence of noise or dynamical uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting framework that combines the representational power of Transformers with the theoretical rigor of Koopman operator theory. Our model features a modular encoder-propagator-decoder structure, where temporal dynamics are learned via a spectrally constrained, linear Koopman operator in a latent space. We impose structural guarantees-such as bounded spectral radius, Lyapunov based energy regularization, and orthogonal parameterization to ensure stability and interpretability. Comprehensive evaluations are conducted on both synthetic dynamical systems, real-world climate dataset (wind speed and surface pressure), financial time series (cryptocurrency), and electricity generation dataset using the Python package that is prepared for this purpose. Across all experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline Transformer models in terms of accuracy, robustness to noise, and long-term forecasting stability. These results establish DeepKoopFormer as a flexible, interpretable, and robust framework for forecasting in high dimensional and dynamical settings. |
| 2025-08-04 | [Quark mass corrections in di-Higgs production amplitude at high-energy](http://arxiv.org/abs/2508.02589v1) | Sebastian Jaskiewicz | A large theoretical uncertainty due to the choice of the top-quark mass renormalisation scheme is present in QCD predictions for Higgs boson pair production. In these proceedings, we report on the recent progress in tackling these uncertainties for the $gg\to HH$ amplitude in the high-energy limit $s,|t|,|u| \gg m_t^2 \gg m_H^2$. Using the Method of Regions and Soft-Collinear Effective Theory, the leading power in $m_t$ behaviour of the amplitude is understood to all orders in the strong coupling expansion, and leading logarithmic resummation leads to a significant reduction in the scheme choice uncertainty in the virtual amplitude for di-Higgs production at high energies. |
| 2025-08-04 | [Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification](http://arxiv.org/abs/2508.02566v1) | Javier Fumanal-Idocin, Raquel Fernandez-Peralta et al. | Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. Unlike classical methods that apply a uniform feature set, DFS customizes feature selection per sample, providing insight into the decision-making process for each case. DFS is especially significant in settings where decision transparency is key, i.e., clinical decisions; however, existing methods use opaque models, which hinder their applicability in real-life scenarios. This paper introduces a novel approach leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. We also show how this method provides a quantitative measure of uncertainty for each feature query and can make the feature selection process computationally lighter by constraining the feature search space. We also discuss when greedy selection of conditional mutual information is equivalent to selecting features that minimize the difference with respect to the global model predictions. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and RL methods, which are mostly considered opaque, compared to our explainable rule-based system. |
| 2025-08-04 | [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](http://arxiv.org/abs/2508.02528v1) | Jingsong Liu, Xiaofeng Deng et al. | Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis. |
| 2025-08-04 | [Quantitative and Predictive Folding Models from Limited Single-Molecule Data Using Simulation-Based Inference](http://arxiv.org/abs/2508.02509v1) | Lars Dingeldein, Aaron Lyons et al. | The study of biomolecular folding has been greatly advanced by single-molecule force spectroscopy (SMFS), which enables the observation of the dynamics of individual molecules. However, extracting quantitative models of fundamental properties such as folding landscapes from SNFS data is very challenging due to instrumental noise, linker artifacts, and the inherent stochasticity of the process, often requiring extensive datasets and complex calibration experiments. Here, we introduce a framework based on simulation-based inference (SBI) that overcomes these limitations by integrating physics-based modeling with deep learning. We apply this framework to analyze constant-force measurements of a DNA hairpin. From a single, short experimental trajectory of only two seconds, we successfully reconstruct the hairpin's free energy landscape and folding dynamics, obtaining results that are in close agreement with established deconvolution methods that require approximately 100 times more data. Furthermore, the Bayesian nature of this approach robustly quantifies uncertainties for inferred parameter values, including the free-energy profile, diffusion coefficients, and linker stiffness, without needing independent measurements of instrumental properties. The inferred model is predictive, generating simulated trajectories that quantitatively reproduce the thermodynamic and kinetic properties of the experimental data. This work establishes SBI as a highly efficient and powerful tool for analyzing single-molecule experiments. The ability to derive statistically robust models from minimal datasets is crucial for investigating complex biomolecular systems where extensive data collection is impractical or impossible. Consequently, our SBI framework enables the rigorous quantitative analysis of previously intractable biomolecular systems, paving the way for novel applications of SMFS. |
| 2025-08-04 | [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](http://arxiv.org/abs/2508.02503v1) | Maxime Bouscary, Saurabh Amin | LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\% to 92\% on the most complex problems. |
| 2025-08-04 | [Pre-discovery TESS Observations of Interstellar Object 3I/ATLAS](http://arxiv.org/abs/2508.02499v1) | Jorge Martinez-Palomera, Amy Tuson et al. | 3I/ATLAS, also known as C/2025 N1 (ATLAS), is the third known interstellar object to pass through our Solar System. We report serendipitous Transiting Exoplanet Survey Satellite (TESS) observations of 3I/ATLAS taken between 2025-05-07 and 2025-06-02,, 55 days prior to the discovery date (2025-07-01) and 14 days prior to the current earliest observation (2025-05-21). We retrieve the TESS pixel data, perform a robust background correction and use a data-driven approach to refine the object's ephemeris. We find a statistically significant offset between the target's observed and predicted positions and we show that this is dominated by uncertainty in the TESS World Coordinate System (WCS) rather than the ephemeris. 3I/ATLAS is too faint to be detected in the individual 200\,second TESS integrations, so we perform image stacking to improve detectability. After co-adding the TESS image data, we performed aperture and Pixel Response Function (PRF) photometry to create two light curves for 3I/ATLAS. Each light curve consists of 15 measurements with $\text{SNR}>3$, collected across two different TESS cameras during the 26\,days that the object was observed, but the PRF light curve is more robust against image noise. The PRF light curve in the TESS bandpass shows a gradual increase in brightness from $T_{\text{mag}} = 20.9 \pm 0.29$ to $T_{\text{mag}} = 19.57 \pm 0.15$. This is expected as 3I/ATLAS approaches the inner Solar System. This paper highlights the power of using TESS for Solar System science; by increasing the photometric observing baseline, future studies will be able to investigate the long-term behavior of 3I/ATLAS |
| 2025-08-04 | [Clinical Expert Uncertainty Guided Generalized Label Smoothing for Medical Noisy Label Learning](http://arxiv.org/abs/2508.02495v1) | Kunyu Zhang, Lin Gu et al. | Many previous studies have proposed extracting image labels from clinical notes to create large-scale medical image datasets at a low cost. However, these approaches inherently suffer from label noise due to uncertainty from the clinical experts. When radiologists and physicians analyze medical images to make diagnoses, they often include uncertainty-aware notes such as ``maybe'' or ``not excluded''. Unfortunately, current text-mining methods overlook these nuances, resulting in the creation of noisy labels. Existing methods for handling noisy labels in medical image analysis, which typically address the problem through post-processing techniques, have largely ignored the important issue of expert-driven uncertainty contributing to label noise. To better incorporate the expert-written uncertainty in clinical notes into medical image analysis and address the label noise issue, we first examine the impact of clinical expert uncertainty on label noise. We then propose a clinical expert uncertainty-aware benchmark, along with a label smoothing method, which significantly improves performance compared to current state-of-the-art approaches. |
| 2025-08-04 | [Uncertainty-Aware Perception-Based Control for Autonomous Racing](http://arxiv.org/abs/2508.02494v1) | Jelena Trisovic, Andrea Carron et al. | Autonomous systems operating in unknown environments often rely heavily on visual sensor data, yet making safe and informed control decisions based on these measurements remains a significant challenge. To facilitate the integration of perception and control in autonomous vehicles, we propose a novel perception-based control approach that incorporates road estimation, quantification of its uncertainty, and uncertainty-aware control based on this estimate. At the core of our method is a parametric road curvature model, optimized using visual measurements of the road through a constrained nonlinear optimization problem. This process ensures adherence to constraints on both model parameters and curvature. By leveraging the Frenet frame formulation, we embed the estimated track curvature into the system dynamics, allowing the controller to explicitly account for perception uncertainty and enhancing robustness to estimation errors based on visual input. We validate our approach in a simulated environment, using a high-fidelity 3D rendering engine, and demonstrate its effectiveness in achieving reliable and uncertainty-aware control for autonomous racing. |
| 2025-08-04 | [Gauge theory approach to describe ice crystals habit evolution in ice clouds](http://arxiv.org/abs/2508.02472v1) | Gianluca Di Natale, Francesco Pio De Cosmo et al. | Ice clouds, particularly cirrus clouds, significantly influence Earth's radiative balance but remain poorly characterized in current climate models. A major uncertainty arises from the variability of their microphysical properties, especially the evolution of ice crystal habits under depositional growth. We propose a heuristic method to describe habit evolution based on four fundamental shapes identified in the literature and from in situ observations: droxtals, plates, columns, and rosettes. These represent the primary forms that are relevant under depositional growth, excluding aggregation. In this study, we employ a non-Abelian gauge theory within a field-theoretical framework, imposing an SU(2) $\otimes$ U(1) symmetry on the fields associated with each habit probability growth. This symmetry enables the derivation of a modified system of coupled Fokker-Planck equations, capturing the stochastic growth dynamics of ice crystals while incorporating phenomenological mutual influences among habits. This framework outlines a novel theoretical direction for integrating symmetry principles and field-theoretical tools into the modelling of habit dynamics in ice clouds. |
| 2025-08-01 | [Numerical Uncertainty in Linear Registration: An Experimental Study](http://arxiv.org/abs/2508.00781v1) | Niusha Mirhakimi, Yohan Chatelain et al. | While linear registration is a critical step in MRI preprocessing pipelines, its numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA) simulations, we assessed the most commonly used linear registration tools within major software packages (SPM, FSL, and ANTs) across multiple image similarity measures, two brain templates, and both healthy control (HC, n=50) and Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the influence of linear registration tools and similarity measures on numerical stability. Among the evaluated tools and with default similarity measures, SPM exhibited the highest stability. FSL and ANTs showed greater and similar ranges of variability, with ANTs demonstrating particular sensitivity to numerical perturbations that occasionally led to registration failure. Furthermore, no significant differences were observed between healthy and PD cohorts, suggesting that numerical stability analyses obtained with healthy subjects may generalise to clinical populations. Finally, we also demonstrated how numerical uncertainty measures may support automated quality control (QC) of linear registration results. Overall, our experimental results characterize the numerical stability of linear registration experimentally and can serve as a basis for future uncertainty analyses. |
| 2025-08-01 | [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](http://arxiv.org/abs/2508.00754v1) | Yaxin Ma, Benjamin Colburn et al. | Bayesian neural networks and deep ensemble methods have been proposed for uncertainty quantification; however, they are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue. We propose an effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection. Specifically, we leverage the information potential field derived from kernel density estimation to approximate the feature space density of the training set. By comparing this density with the feature space representation of test samples, we can effectively determine whether a distributional shift has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The results demonstrate that our method outperforms baseline models. |
| 2025-08-01 | [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](http://arxiv.org/abs/2508.00750v1) | Prerana Ramkumar | Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications. |
| 2025-08-01 | [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](http://arxiv.org/abs/2508.00734v1) | Liuyun Xu, Seymour M. J. Spence | Existing variance reduction techniques used in stochastic simulations for rare event analysis still require a substantial number of model evaluations to estimate small failure probabilities. In the context of complex, nonlinear finite element modeling environments, this can become computationally challenging-particularly for systems subjected to stochastic excitation. To address this challenge, a multi-fidelity stratified sampling scheme with adaptive machine learning metamodels is introduced for efficiently propagating uncertainties and estimating small failure probabilities. In this approach, a high-fidelity dataset generated through stratified sampling is used to train a deep learning-based metamodel, which then serves as a cost-effective and highly correlated low-fidelity model. An adaptive training scheme is proposed to balance the trade-off between approximation quality and computational demand associated with the development of the low-fidelity model. By integrating the low-fidelity outputs with additional high-fidelity results, an unbiased estimate of the strata-wise failure probabilities is obtained using a multi-fidelity Monte Carlo framework. The overall probability of failure is then computed using the total probability theorem. Application to a full-scale high-rise steel building subjected to stochastic wind excitation demonstrates that the proposed scheme can accurately estimate exceedance probability curves for nonlinear responses of interest, while achieving significant computational savings compared to single-fidelity variance reduction approaches. |
| 2025-08-01 | [Efficient Solution and Learning of Robust Factored MDPs](http://arxiv.org/abs/2508.00707v1) | Yannik Schnitzer, Alessandro Abate et al. | Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling epistemic uncertainty about transition dynamics. Learning r-MDPs from interactions with an unknown environment enables the synthesis of robust policies with provable (PAC) guarantees on performance, but this can require a large number of sample interactions. We propose novel methods for solving and learning r-MDPs based on factored state-space representations that leverage the independence between model uncertainty across system components. Although policy synthesis for factored r-MDPs leads to hard, non-convex optimisation problems, we show how to reformulate these into tractable linear programs. Building on these, we also propose methods to learn factored model representations directly. Our experimental results show that exploiting factored structure can yield dimensional gains in sample efficiency, producing more effective robust policies with tighter performance guarantees than state-of-the-art methods. |
| 2025-08-01 | [Chemical abundances of seven stars in the GD-1 stream](http://arxiv.org/abs/2508.00671v1) | Jing-Kun Zhao, Guang-Wei Li et al. | We present the first detailed chemical abundances for seven GD-1 stream stars from Subaru/HDS spectroscopy. Atmospheric parameters were derived via color calibrations ($T\rm_{eff}$) and iterative spectroscopic analysis. LTE abundances for 14 elements ($\alpha$, odd-Z, iron-peak, n-capture) were measured. Six stars trace the main orbit, one resides in a `blob'. All exhibit tightly clustered metallicities ([Fe/H] = -2.38, {\bf intrinsic dispersion smaller than 0.05 dex, average uncertainty is about 0.13 dex}). While one star shows binary mass transfer signatures, the other six display consistent abundance patterns (dispersions $<$ uncertainties). Their iron-peak elements (Sc, Cr, Mn, Ni) match Milky Way halo stars. In contrast, Y and Sr are systematically lower than halo stars of similar [Fe/H]. Significantly, six stars show consistently enhanced [Eu/Fe] $\sim$ 0.60 ($\sigma$ = 0.08). A tight Ba-Eu correlation (r = 0.83, p=0.04) exists, with [Ba/Fe] = -0.03 $\pm$ 0.05, indicating a common r-process origin. This extreme chemical homogeneity strongly supports an origin from a single disrupted globular cluster. The lack of light-element anti-correlations may stem from our sample size or the progenitor's low mass. |
| 2025-08-01 | [Uncertainty Relation for Pseudo-Hermitian Quantum Systems](http://arxiv.org/abs/2508.00648v1) | Boubakeur Khantoul, Bilel Hamil et al. | This study investigates pseudo-Hermitian quantum mechanics, where the Hamiltonian satisfies a modified Hermiticity condition. We extend the uncertainty relation for such systems, demonstrating its equivalence to the standard Hermitian case within a pseudo-Hermitian inner product. Analytical solutions to the time-dependent Schr\"odinger equation with a linearly evolving potential are derived. Furthermore, we show that the uncertainty relation for position and momentum remains real and greater than 1/2, highlighting the significance of non-Hermitian systems in quantum mechanics. |
| 2025-08-01 | [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](http://arxiv.org/abs/2508.00643v1) | Albert Matveev, Sanmitra Ghosh et al. | Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification. |
| 2025-08-01 | [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](http://arxiv.org/abs/2508.00600v1) | Mingruo Yuan, Shuyi Zhang et al. | Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines. |
| 2025-08-01 | [Output-recurrent gated state space model for multiphase flows modeling and uncertainty quantification of exhaust vehicles](http://arxiv.org/abs/2508.00588v1) | Ruilin Chen, Ming Fang et al. | This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for complex multiphase flows modeling and uncertainty quantification of exhaust vehicles during motion. By establishing the state-space formulation of the gas-liquid Navier-Stokes equations applying semigroup theory and Galerkin projection, explicitly characterizing the dynamic coupling evolution between the velocity, pressure, and volume fraction fields. A novel Gated State Space Transition (GSST) unit is designed to learn parameterized transition and input matrices with adaptive timescales, enhancing physical interpretability and computational efficiency. The output recursion mechanism aligns with the numerical solution characteristics of state-space equations, mitigating long-term error accumulation and addressing training-inference pattern mismatch issues inherent in teacher forcing and scheduled sampling. Validations on the underwater cone-head and water-exit hemisphere-head vehicles demonstrate that: OR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and computational efficiency through its physics-informed adaptive state-space unit design and parallel matrix operations; The output recursion mechanism ensures more stable training, better generalization, and higher prediction accuracy than teacher forcing and scheduled sampling; OR-GSSM accurately captures the gas-phase expansion, gas-liquid mixing formation, backflow jet generation, bubble shedding, and entire water-exit process, etc, showcasing outstanding modeling capability; Its uncertainty quantification effectively characterizes flow features and uncertainty distributions, validating prediction reliability. The proposed method resolves the accuracy-real-time trade-off in traditional computational fluid dynamics, advancing machine learning for multiphase flow modeling and uncertainty quantification in exhaust vehicles. |
| 2025-07-31 | [Formal Bayesian Transfer Learning via the Total Risk Prior](http://arxiv.org/abs/2507.23768v1) | Nathan Wycoff, Ali Arab et al. | In analyses with severe data-limitations, augmenting the target dataset with information from ancillary datasets in the application domain, called source datasets, can lead to significantly improved statistical procedures. However, existing methods for this transfer learning struggle to deal with situations where the source datasets are also limited and not guaranteed to be well-aligned with the target dataset. A typical strategy is to use the empirical loss minimizer on the source data as a prior mean for the target parameters, which places the estimation of source parameters outside of the Bayesian formalism. Our key conceptual contribution is to use a risk minimizer conditional on source parameters instead. This allows us to construct a single joint prior distribution for all parameters from the source datasets as well as the target dataset. As a consequence, we benefit from full Bayesian uncertainty quantification and can perform model averaging via Gibbs sampling over indicator variables governing the inclusion of each source dataset. We show how a particular instantiation of our prior leads to a Bayesian Lasso in a transformed coordinate system and discuss computational techniques to scale our approach to moderately sized datasets. We also demonstrate that recently proposed minimax-frequentist transfer learning techniques may be viewed as an approximate Maximum a Posteriori approach to our model. Finally, we demonstrate superior predictive performance relative to the frequentist baseline on a genetics application, especially when the source data are limited. |
| 2025-07-31 | [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](http://arxiv.org/abs/2507.23756v1) | Diana Mortagua | This study centers on overcoming the challenge of selecting the best annotators for each query in Active Learning (AL), with the objective of minimizing misclassifications. AL recognizes the challenges related to cost and time when acquiring labeled data, and decreases the number of labeled data needed. Nevertheless, there is still the necessity to reduce annotation errors, aiming to be as efficient as possible, to achieve the expected accuracy faster. Most strategies for query-annotator pairs do not consider internal factors that affect productivity, such as mood, attention, motivation, and fatigue levels. This work addresses this gap in the existing literature, by not only considering how the internal factors influence annotators (mood and fatigue levels) but also presenting a new query-annotator pair strategy, using a Knowledge-Based Recommendation System (RS). The RS ranks the available annotators, allowing to choose one or more to label the queried instance using their past accuracy values, and their mood and fatigue levels, as well as information about the instance queried. This work bases itself on existing literature on mood and fatigue influence on human performance, simulating annotators in a realistic manner, and predicting their performance with the RS. The results show that considering past accuracy values, as well as mood and fatigue levels reduces the number of annotation errors made by the annotators, and the uncertainty of the model through its training, when compared to not using internal factors. Accuracy and F1-score values were also better in the proposed approach, despite not being as substantial as the aforementioned. The methodologies and findings presented in this study begin to explore the open challenge of human cognitive factors affecting AL. |
| 2025-07-31 | [DICOM De-Identification via Hybrid AI and Rule-Based Framework for Scalable, Uncertainty-Aware Redaction](http://arxiv.org/abs/2507.23736v1) | Kyle Naddeo, Nikolas Koutsoubis et al. | Access to medical imaging and associated text data has the potential to drive major advances in healthcare research and patient outcomes. However, the presence of Protected Health Information (PHI) and Personally Identifiable Information (PII) in Digital Imaging and Communications in Medicine (DICOM) files presents a significant barrier to the ethical and secure sharing of imaging datasets. This paper presents a hybrid de-identification framework developed by Impact Business Information Solutions (IBIS) that combines rule-based and AI-driven techniques, and rigorous uncertainty quantification for comprehensive PHI/PII removal from both metadata and pixel data.   Our approach begins with a two-tiered rule-based system targeting explicit and inferred metadata elements, further augmented by a large language model (LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of synthetic datasets simulating realistic clinical PHI/PII. For pixel data, we employ an uncertainty-aware Faster R-CNN model to localize embedded text, extract candidate PHI via Optical Character Recognition (OCR), and apply the NER pipeline for final redaction. Crucially, uncertainty quantification provides confidence measures for AI-based detections to enhance automation reliability and enable informed human-in-the-loop verification to manage residual risks.   This uncertainty-aware deidentification framework achieves robust performance across benchmark datasets and regulatory standards, including DICOM, HIPAA, and TCIA compliance metrics. By combining scalable automation, uncertainty quantification, and rigorous quality assurance, our solution addresses critical challenges in medical data de-identification and supports the secure, ethical, and trustworthy release of imaging data for research. |
| 2025-07-31 | [Distributed AI Agents for Cognitive Underwater Robot Autonomy](http://arxiv.org/abs/2507.23735v1) | Markus Buchholz, Ignacio Carlucho et al. | Achieving robust cognitive autonomy in robots navigating complex, unpredictable environments remains a fundamental challenge in robotics. This paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a groundbreaking architecture leveraging distributed Large Language Model AI agents integrated within the Robot Operating System 2 (ROS 2) framework to enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA decentralises cognition into specialised AI agents responsible for multimodal perception, adaptive reasoning, dynamic mission planning, and real-time decision-making. Central innovations include flexible agents dynamically adapting their roles, retrieval-augmented generation utilising vector databases for efficient knowledge management, reinforcement learning-driven behavioural optimisation, and autonomous on-the-fly ROS 2 node generation for runtime functional extensibility. Extensive empirical validation demonstrates UROSA's promising adaptability and reliability through realistic underwater missions in simulation and real-world deployments, showing significant advantages over traditional rule-based architectures in handling unforeseen scenarios, environmental uncertainties, and novel mission objectives. This work not only advances underwater autonomy but also establishes a scalable, safe, and versatile cognitive robotics framework capable of generalising to a diverse array of real-world applications. |
| 2025-07-31 | [High-resolution eikonal imaging and uncertainty quantification of the Kilauea caldera](http://arxiv.org/abs/2507.23692v1) | Angela F. Gao, John D. Wilding et al. | Images of the Earth's interior can provide us with insight into the underlying properties of the Earth, such as how seismic activity might emerge and the interplay between seismic and volcanic activity. Understanding these systems requires reliable high-resolution images to understand mechanisms and estimate physical quantities. However, reliable images are often difficult to obtain due to the non-linear nature of seismic wave propagation and the ill-posedness of the related inverse problem. Reconstructions rely on good initial estimates as well as hand-crafted priors, which can ultimately bias solutions. In our work, we present a 3D reconstruction of Kilauea's magmatic system at a previously unattained resolution. Our eikonal tomography procedure improves upon prior imaging results of Kilauea through increased resolution and per-pixel uncertainties estimated through variational inference. In particular, solving eikonal imaging using variational inference with stochastic gradient descent enables stable inversion and uncertainty quantification in the absence of strong prior knowledge of the velocity structure. Our work makes two key contributions: developing a stochastic eikonal tomography scheme with uncertainty quantification and illuminating the structure and melt quantity of the magmatic system that underlies Kilauea. |
| 2025-07-31 | [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](http://arxiv.org/abs/2507.23607v1) | Tien Huu Do, Antoine Masquelier et al. | Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models. |
| 2025-07-31 | [Branching ratios and CP asymmetries of $B^0 \to Î·_c f_0$ in the improved perturbative QCD formalism](http://arxiv.org/abs/2507.23578v1) | Min-Qi Li, Xin Liu et al. | Motivated by the idea of fragmented scalar glueball, we investigate the decays $B^0 \to \eta_c f_0$ within the improved perturbative QCD (iPQCD) framework by including the known next-to-leading order corrections. Here, $B^0$ and $f_0$ denote the neutral $B_{d,s}^0$ mesons and the light scalar mesons $f_0(500, 980, 1370, 1500)$ under the $q\bar q$ assignment. The {\it CP}-averaged branching ratios (BRs) and the {\it CP} asymmetries of $B^0 \to \eta_c f_0$ are evaluated with the $f_0(500)[f_0(1370)]-f_0(980)[f_0(1500)]$ mixing in quark-flavor basis. For effective comparisons with the near-future measurements, we further derive the $B^0 \to \eta_c f_0 (\to \pi^+ \pi^-/K^+ K^-)$ BRs under the narrow-width approximation. ${\rm BR}(B_s^0 \to \eta_c f_0(980) (\to \pi^+ \pi^-))= (2.87^{+1.38}_{-1.29}) \times 10^{-4}$ obtained in the iPQCD formalism agrees with the available measurements and predictions within uncertainties. Large BRs of $B_s^0 \to \eta_c f_0(1500) (\to \pi^+ \pi^-/K^+ K^-)$ and large direct {\it CP} asymmetries of $B^0 \to \eta_c f_0(1370, 1500)$ are accessible in the LHCb and Belle-II experiments. The experimental tests of these iPQCD predictions would help us to understand the nature of these light scalars more deeply and provide evidences to decipher $f_0(1500)$ as a primary or fragmented scalar glueball potentially. |
| 2025-07-31 | [Subthreshold parameters of $Ï€Ï€$ scattering revisited](http://arxiv.org/abs/2507.23566v1) | MariÃ¡n KolesÃ¡r, Jaroslav Å˜Ã­ha | Using the most recent experimental data and lattice QCD calculations of $\pi\pi$ scattering lengths, while employing dispersive representations of the amplitude based on Roy equations, we compute the subthreshold parameters of this process. We use Monte Carlo sampling to numerically model the probability distribution of the results based on all uncertainties in the inputs. We also investigate the dependence of the results on a theoretical correlation between the $\pi\pi$ scattering lengths $a^0_0$ and $a^2_0$, which was previously established in the framework of two-flavour chiral perturbation theory. |
| 2025-07-31 | [Latest neutrino results from the FASER experiment and their implications for forward hadron production](http://arxiv.org/abs/2507.23552v1) | FASER Collaboration, Roshan Mammen Abraham et al. | The muon puzzle -- an excess of muons relative to simulation predictions in ultra-high-energy cosmic-ray air showers -- has been reported by many experiments. This suggests that forward particle production in hadronic interactions is not fully understood. Some of the scenarios proposed to resolve this predict reduced production of forward neutral pions and enhanced production of forward kaons (or other particles). The FASER experiment at the LHC is located 480 m downstream of the ATLAS interaction point and is sensitive to neutrinos and muons, which are the decay products of forward charged pions and kaons. In this study, the latest measurements of electron and muon neutrino fluxes are presented using the data corresponding to 9.5 $\mathrm{fb^{-1}}$ and 65.6 $\mathrm{fb^{-1}}$ of proton-proton collisions with $\sqrt{s}=13.6~\mathrm{TeV}$ by the FASER$\nu$ and the FASER electronic detector, respectively. These fluxes are compared with predictions from recent hadronic interaction models, including EPOS-LHCr, SIBYLL 2.3e, and QGSJET 3. The predictions are generally consistent with the measured fluxes from FASER, although some discrepancies appear in certain energy bins. More precise flux measurements with additional data will follow soon, enabling validation of pion, kaon, and charm meson production with finer energy binning, reduced uncertainties, and multi-differential analyses. |
| 2025-07-31 | [A decomposition of Fisher's information to inform sample size for developing or updating fair and precise clinical prediction models -- Part 3: continuous outcomes](http://arxiv.org/abs/2507.23548v1) | Rebecca Whittle, Richard D Riley et al. | Clinical prediction models enable healthcare professionals to estimate individual outcomes using patient characteristics. Current sample size guidelines for developing or updating models with continuous outcomes aim to minimise overfitting and ensure accurate estimation of population-level parameters, but do not explicitly address the precision of predictions. This is a critical limitation, as wide confidence intervals around predictions can undermine clinical utility and fairness, particularly if precision varies across subgroups. We propose methodology for calculating the sample size required to ensure precise and fair predictions in models with continuous outcomes. Building on linear regression theory and the Fisher's unit information matrix, our approach calculates how sample size impacts the epistemic (model-based) uncertainty of predictions and allows researchers to either (i) evaluate whether an existing dataset is sufficiently large, or (ii) determine the sample size needed to target a particular confidence interval width around predictions. The method requires real or synthetic data representing the target population. To assess fairness,the approach can evaluate prediction precision across subgroups. Extensions to prediction intervals are included to additionally address aleatoric uncertainty. Our methodology provides a practical framework for examining required sample sizes when developing or updating prediction models with continuous outcomes, focusing on achieving precise and equitable predictions. It supports the development of more reliable and fair models, enhancing their clinical applicability and trustworthiness. |
| 2025-07-30 | [Solitons, chaos, and quantum phenomena: a deterministic approach to the SchrÃ¶dinger equation](http://arxiv.org/abs/2507.22868v1) | DamiÃ  Gomila | We show that the Schr\"odinger equation describes the ensemble mean dynamics of solitons in a Galilean invariant field theory where we interpret solitons as particles. On a zero background, solitons move classically, following Newton`s second law, however, on a non-zero amplitude chaotic background, their momentum and position fluctuate fulfilling an exact uncertainty relation, which give rise to the emergence of quantum phenomena. The Schrodinger equation for the ensemble of solitons is obtained from this exact uncertainty relation, and the amplitude of the background fluctuations is what corresponds to the value of $\hbar$. We confirm our analytical results running simulations of solitons moving against a potential barrier and comparing the ensemble probabilities with the predictions of the time dependent Schr\"odinger equation, providing a deterministic version of the quantum tunneling effect. We conclude with a discussion of how our theory does not present statistical independence between measurement and experiment outcome. |
| 2025-07-30 | [A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model](http://arxiv.org/abs/2507.22854v1) | Andris Ambainis, Joao F. Doriguello et al. | We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces. |
| 2025-07-30 | [Robust Contract with Career Concerns](http://arxiv.org/abs/2507.22852v1) | Tan Gan, Hongcheng Li | An employer contracts with a worker to incentivize efforts whose productivity depends on ability; the worker then enters a market that pays him contingent on ability evaluation. With non-additive monitoring technology, the interdependence between market expectations and worker efforts can lead to multiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy (1992)). We identify a sufficient and necessary criterion for the employer to face such strategic uncertainty--one linked to skill-effort complementarity, a pervasive feature of labor markets. To fully implement work, the employer optimally creates private wage discrimination to iteratively eliminate pessimistic market expectations and low worker efforts. Our result suggests that present contractual privacy, employers' coordination motives generate within-group pay inequality. The comparative statics further explain several stylized facts about residual wage dispersion. |
| 2025-07-30 | [An Uncertainty Principle for Probabilistic Computation in the Retina](http://arxiv.org/abs/2507.22785v1) | Jayanth R Taranath, Salim M'Jahad | We introduce a probabilistic model of early visual processing, beginning with the interaction between a light wavefront and the retina. We argue that perception originates not with deterministic transduction, but with probabilistic threshold crossings shaped by quantum photon arrival statistics and biological variability. We formalize this with an uncertainty relation, \( \Delta \alpha \cdot \Delta t \geq \eta \), through the transformation of light into symbolic neural code through the layered retinal architecture. Our model is supported by previous experimental results, which show intrinsic variability in retinal responses even under fixed stimuli. We contrast this with a classical null hypothesis of deterministic encoding and propose experiments to further test our uncertainty relation. By re-framing the retina as a probabilistic measurement device, we lay the foundation for future models of cortical dynamics rooted in quantum-like computation. We are not claiming that the brain could be working as a quantum-system, but rather putting forth the argument that the brain as a classical system could still implement quantum-inspired computations. We define quantum-inspired computation as a scheme that includes both probabilistic and time-sensitive computation, clearly separating it from classically implementable probabilistic systems. |
| 2025-07-30 | [A comparison of variable selection methods and predictive models for postoperative bowel surgery complications](http://arxiv.org/abs/2507.22771v1) | Ã–zge Åžahin, Annemiek Kwast et al. | Accurate prediction of postoperative complications can support personalized perioperative care. However, in surgical settings, data collection is often constrained, and identifying which variables to prioritize remains an open question. We analyzed 767 elective bowel surgeries performed under an Enhanced Recovery After Surgery protocol at Medisch Spectrum Twente (Netherlands) between March 2020 and December 2023. Although hundreds of variables were available, most had substantial missingness or near-constant values and were therefore excluded. After data preprocessing, 34 perioperative predictors were selected for further analysis. Surgeries from 2020 to 2022 ($n=580$) formed the development set, and 2023 cases ($n=187$) provided temporal validation. We modeled two binary endpoints: any and serious postoperative complications (Clavien Dindo $\ge$ IIIa). We compared weighted logistic regression, stratified random forests, and Naive Bayes under class imbalance (serious complication rate $\approx$11\%; any complication rate $\approx$35\%). Probabilistic performance was assessed using class-specific Brier scores. We advocate reporting probabilistic risk estimates to guide monitoring based on uncertainty. Random forests yielded better calibration across outcomes. Variable selection modestly improved weighted logistic regression and Naive Bayes but had minimal effect on random forests. Despite single-center data, our findings underscore the value of careful preprocessing and ensemble methods in perioperative risk modeling. |
| 2025-07-30 | [Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models](http://arxiv.org/abs/2507.22766v1) | Felix Kronenwett, Georg Maier et al. | Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters. |
| 2025-07-30 | [Pionic gluons from global QCD analysis of experimental and lattice data](http://arxiv.org/abs/2507.22730v1) | William Good, Patrick C. Barry et al. | We perform the first global QCD analysis of parton distribution functions (PDFs) in the pion, with lattice-QCD data on gluonic pseudo--Ioffe-time distributions fitted simultaneously with experimental Drell-Yan and leading neutron electroproduction data. Inclusion of the lattice results with parametrized systematic corrections significantly reduces the uncertainties on the gluon PDF at parton momentum fractions $x \gtrsim 0.2$, revealing a higher gluon density in the pion at large $x$ than in the proton. The similar gluon momentum fractions in the pion and proton further suggests a relative suppression of the pion gluon density at small $x$. |
| 2025-07-30 | [Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection](http://arxiv.org/abs/2507.22693v1) | Naoki Aizawa, Keita Emura et al. | This study proposes an encrypted PID control system with a disturbance observer (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to achieve control performance while providing resistance to malleability-based attacks. The controller integrates a DOB with a PID structure to compensate for modeling uncertainties by estimating and canceling external disturbances. To enhance security, the system is designed to output error symbols when ciphertexts are falsified during decryption or evaluation, enabling real-time detection of malleability-based signal or parameter falsification. To validate the proposed method, we conduct stage positioning control experiments and attack detection tests using an industrial linear stage. The results show that the encrypted DOB-based PID controller outperforms a conventional encrypted PID controller in terms of tracking accuracy. Furthermore, the system successfully detects two types of malleability-based attacks: one that destabilizes the control system, and another that degrades its performance. The primary contributions of this study are: (i) the implementation of a KHE-based encrypted DOB-PID controller, (ii) the improvement of control performance under uncertainties, and (iii) the experimental demonstration of attack detection capabilities in encrypted control systems. |
| 2025-07-30 | [Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach](http://arxiv.org/abs/2507.22671v1) | Sami Saeed Alghamdi, Christopher Bull et al. | Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools. |
| 2025-07-30 | [Robust Voting under Uncertainty](http://arxiv.org/abs/2507.22655v1) | Satoshi Nakada, Shmuel Nitzan et al. | This paper proposes normative criteria for voting rules under uncertainty about individual preferences. The criteria emphasize the importance of responsiveness, i.e., the probability that the social outcome coincides with the realized individual preferences. Given a convex set of probability distributions of preferences, denoted by $P$, a voting rule is said to be $P$-robust if, for each probability distribution in $P$, at least one individual's responsiveness exceeds one-half. Our main result establishes that a voting rule is $P$-robust if and only if there exists a nonnegative weight vector such that the weighted average of individual responsiveness is strictly greater than one-half under every extreme point of $P$. In particular, if the set $P$ includes all degenerate distributions, a $P$-robust rule is a weighted majority rule without ties. |
| 2025-07-29 | [Planning Persuasive Trajectories Based on a Leader-Follower Game Model](http://arxiv.org/abs/2507.22022v1) | Chaozhe R. He, Yichen Dong et al. | We propose a framework that enables autonomous vehicles (AVs) to proactively shape the intentions and behaviors of interacting human drivers. The framework employs a leader-follower game model with an adaptive role mechanism to predict human interaction intentions and behaviors. It then utilizes a branch model predictive control (MPC) algorithm to plan the AV trajectory, persuading the human to adopt the desired intention. The proposed framework is demonstrated in an intersection scenario. Simulation results illustrate the effectiveness of the framework for generating persuasive AV trajectories despite uncertainties. |
| 2025-07-29 | [Uncertainty Estimation of the Optimal Decision with Application to Cure Process Optimization](http://arxiv.org/abs/2507.21995v1) | Yezhuo Li, Qiong Zhang et al. | Decision-making in manufacturing often involves optimizing key process parameters using data collected from simulation experiments. Gaussian processes are widely used to surrogate the underlying system and guide optimization. Uncertainty often inherent in the decisions given by the surrogate model due to limited data and model assumptions. This paper proposes a surrogate model-based framework for estimating the uncertainty of optimal decisions and analyzing its sensitivity with respect to the objective function. The proposed approach is applied to the composite cure process simulation in manufacturing. |
| 2025-07-29 | [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](http://arxiv.org/abs/2507.21931v1) | Carel van Niekerk, Renato Vukovic et al. | Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.   RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.   By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training. |
| 2025-07-29 | [Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs](http://arxiv.org/abs/2507.21893v1) | Saeed Ghorbani | We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences. |
| 2025-07-29 | [Multi-Gap superconductivity in HgS under pressure](http://arxiv.org/abs/2507.21869v1) | Pietro Maria Forcella, Cesare Tresca et al. | Mercury chalcogenides are a class of materials that exhibit diverse structural phases under pressure, leading to a range of exotic physical properties, including topological phases and chiral phonons. In particular, the phase diagram of mercury sulfide (HgS) remains difficult to characterize, with significant uncertainty surrounding the transition pressure between phases. Based on recent experimental results, we employ Density Functional Theory and Superconducting Density Functional Theory to investigate the pressure-induced structural phase transition in HgS and its interplay with the emergence of superconductivity as the crystal transitions from the cinnabar phase (space group P3$_1$21) to the rock salt phase (space group Fm$\bar{3}$m). Remarkably, the rocksalt phase hosts a multigap superconducting state driven by distinct Fermi surface sheets, with two dominant gaps; the unusually high critical temperature of $\sim$11 K emerges naturally within this multiband scenario, highlighting the role of interband coupling beyond isotropic models. These results place HgS among the few systems where multiband superconducting gap structures emerge under pressure. |
| 2025-07-29 | [Two-neutrino $Î²Î²$ decay to excited states at next-to-leading order](http://arxiv.org/abs/2507.21868v1) | Daniel Castillo, Dorian Frycz et al. | We study two-neutrino double-beta decay ($2\nu\beta\beta$) into first-excited $0^+_2$ states of nuclei used in $\beta\beta$ decay experiments, including $^{76}$Ge, $^{82}$Se, $^{130}$Te, and $^{136}$Xe. We calculate the corresponding nuclear matrix elements (NMEs) within the nuclear shell model, using various Hamiltonians that describe well the spectroscopy of the initial and final nuclei. We evaluate the next-to-leading order (NLO) long-range NMEs recently introduced within chiral effective field theory, keeping three terms in the expansion of the energy denominator. In most cases, NLO contributions to the half-life are below 5%, but they can increase to 25% due to cancellations in the leading-order Gamow-Teller NME. A detailed analysis in terms of nuclear deformation, including triaxiality, indicates that larger deformation differences between the initial and final states generally lead to smaller NMEs, but the seniority structure of the states also plays a relevant role. The lower range of our predicted half-lives, with uncertainties dominated by the nuclear Hamiltonian used, are slightly longer than the current experimental limit in $^{76}$Ge and the very recent half-life indication in $^{82}$Se. |
| 2025-07-29 | [The Lyman-$Î±$ Forest from LBGs: First 3D Correlation Measurement with DESI and Prospects for Cosmology](http://arxiv.org/abs/2507.21852v1) | Hiram K. Herrera-Alcantar, Eric Armengaud et al. | The Lyman-$\alpha$ (Ly$\alpha$) forest is a key tracer of large-scale structure at redshifts z > 2, traditionally studied using spectra of quasars. Here, we explore the viability Lyman Break Galaxies (LBGs) as alternative background sources for Ly$\alpha$ forest studies. We analyze 4,151 Ly$\alpha$ forest skewers extracted from LBG spectra obtained in the DESI pilot surveys in the COSMOS and XMM-LSS fields. We present the first measurement of the Ly$\alpha$ forest auto-correlation function derived exclusively from LBG spectra, probing comoving separations up to 48 $h^{-1}$Mpc at an effective redshift of $z_\mathrm{eff}$ = 2.70. The measured signal is consistent with that from DESI DR2 quasar Ly$\alpha$ forest spectra at a comparable redshift, validating LBGs as reliable background sources. We also measure the cross-correlation between the LBG Ly$\alpha$ forest and 13,362 galaxy positions, showing that this observable serves as a sensitive diagnostic for galaxy redshift uncertainties and systematic offsets. Finally, using synthetic LBG spectra and Fisher forecasts, we show that a future wide-area survey over 5000 deg$^2$, targeting 1000 LBGs per deg$^2$ at similar signal-to-noise than our dataset, could enable Ly$\alpha$ forest baryon acoustic oscillation (BAO) measurements with 0.4% precision on the isotropic BAO scale and 1.3% on the anisotropic (Alcock-Paczynski) scale. Combining BAO with a Ly$\alpha$ forest full-shape analysis improves the AP constraint to 0.6%. These results open a new path for precision cosmology at high redshift using dense LBG samples. |
| 2025-07-29 | [Probabilistic Active Goal Recognition](http://arxiv.org/abs/2507.21846v1) | Chenyuan Zhang, Cristian Rojas Cardenas et al. | In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems. |
| 2025-07-29 | [Distribution-Based Masked Medical Vision-Language Model Using Structured Reports](http://arxiv.org/abs/2507.21794v1) | Shreyank N Gowda, Ruichi Zhang et al. | Medical image-language pre-training aims to align medical images with clinically relevant text to improve model performance on various downstream tasks. However, existing models often struggle with the variability and ambiguity inherent in medical data, limiting their ability to capture nuanced clinical information and uncertainty. This work introduces an uncertainty-aware medical image-text pre-training model that enhances generalization capabilities in medical image analysis. Building on previous methods and focusing on Chest X-Rays, our approach utilizes structured text reports generated by a large language model (LLM) to augment image data with clinically relevant context. These reports begin with a definition of the disease, followed by the `appearance' section to highlight critical regions of interest, and finally `observations' and `verdicts' that ground model predictions in clinical semantics. By modeling both inter- and intra-modal uncertainty, our framework captures the inherent ambiguity in medical images and text, yielding improved representations and performance on downstream tasks. Our model demonstrates significant advances in medical image-text pre-training, obtaining state-of-the-art performance on multiple downstream tasks. |
| 2025-07-29 | [The impact of large-scale EV charging on the real-time operation of distribution systems: A comprehensive review](http://arxiv.org/abs/2507.21759v1) | Zhe Yu, Chuang Yang et al. | With the large-scale integration of electric vehicles (EVs) in the distribution grid, the unpredictable nature of EV charging introduces considerable uncertainties to the grid's real-time operations. This can exacerbate load fluctuations, compromise power quality, and pose risks to the grid's stability and security. However, due to their dual role as controllable loads and energy storage devices, EVs have the potential to mitigate these fluctuations, balance the variability of renewable energy sources, and provide ancillary services that support grid stability. By leveraging the bidirectional flow of information and energy in smart grids, the adverse effects of EV charging can be minimized and even converted into beneficial outcomes through effective real-time management strategies. This paper explores the negative impacts of EV charging on the distribution system's real-time operations and outlines methods to transform these challenges into positive contributions. Additionally, it provides an in-depth analysis of the real-time management system for EV charging, focusing on state estimation and management strategies. |
| 2025-07-28 | [Locally Adaptive Conformal Inference for Operator Models](http://arxiv.org/abs/2507.20975v1) | Trevor Harris, Yan Liu | Operator models are regression algorithms for functional data and have become a key tool for emulating large-scale dynamical systems. Recent advances in deep neural operators have dramatically improved the accuracy and scalability of operator modeling, but lack an inherent notion of predictive uncertainty. We introduce Local Spectral Conformal Inference (LSCI), a new framework for locally adaptive, distribution-free uncertainty quantification for neural operator models. LSCI uses projection-based depth scoring and localized conformal inference to generate function-valued prediction sets with statistical guarantees. We prove approximate finite-sample marginal coverage under local exchangeability, and demonstrate significant gains in adaptivity and coverage across synthetic and real-world operator learning tasks. |
| 2025-07-28 | [The Concordance of Weak Lensing and Escape Velocity Mass Estimates for Galaxy Clusters](http://arxiv.org/abs/2507.20938v1) | Alexander Rodriguez, Christopher J. Miller | In the $\Lambda$CDM paradigm, the masses of the galaxy clusters inferred using background galaxies via weak-lensing shear should agree with the masses measured using the galaxy projected radius-velocity phase-space data via the escape velocity profile. However, prior work indicates that the correlation between caustic-inferred escape masses and weak lensing masses is statistically consistent with zero. Based on recent advancements in the measurement of the escape edge and its physical interpretation, we conduct a revised comparison between these two independent mass inference techniques for 46 galaxy clusters between $0.05 \le z \le 0.3$ and over an order of magnitude in mass, $14.4 \le {\rm log}_{10} M/M_{\odot} \le 15.4$. We find excellent agreement, with a correlation ($0.679^{+0.046}_{-0.049}$), and a mean relative difference between the two mass measurements consistent with zero (0.02 $\pm$ 0.02 dex). The observed scatter between these direct mass estimates is 0.17 dex and is consistent with the reported individual mass errors, suggesting that there is no need for an additional intrinsic component. We discuss the important practical consequences of these results, focusing on the systematic uncertainties inherent to each technique, and their implications for cosmology. |
| 2025-07-28 | [Target-density formation in swarms with stochastic sensing and dynamics](http://arxiv.org/abs/2507.20911v1) | Jason Hindes, George Stantchev et al. | An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target. |
| 2025-07-28 | [DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](http://arxiv.org/abs/2507.20879v1) | Weicheng Zheng, Xiaofei Mao et al. | Vision-Language Models (VLMs) are advancing autonomous driving, yet their potential is constrained by myopic decision-making and passive perception, limiting reliability in complex environments. We introduce DriveAgent-R1 to tackle these challenges in long-horizon, high-level behavioral decision-making. DriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that adaptively switches between efficient text-based and in-depth tool-based reasoning, and an Active Perception mechanism with a vision toolkit to proactively resolve uncertainties, thereby balancing decision-making efficiency and reliability. The agent is trained using a novel, three-stage progressive reinforcement learning strategy designed to master these hybrid capabilities. Extensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art performance, outperforming even leading proprietary large multimodal models, such as Claude Sonnet 4. Ablation studies validate our approach and confirm that the agent's decisions are robustly grounded in actively perceived visual evidence, paving a path toward safer and more intelligent autonomous systems. |
| 2025-07-28 | [Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling](http://arxiv.org/abs/2507.20861v1) | Marco Faroni, Carlo Odesco et al. | Physics-based simulations and learning-based models are vital for complex robotics tasks like deformable object manipulation and liquid handling. However, these models often struggle with accuracy due to epistemic uncertainty or the sim-to-real gap. For instance, accurately pouring liquid from one container to another poses challenges, particularly when models are trained on limited demonstrations and may perform poorly in novel situations. This paper proposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed to mitigate these inaccuracies. By incorporating estimates of model uncertainty, the proposed MCTS strategy biases the search towards actions with lower predicted uncertainty. This approach enhances the reliability of planning under uncertain conditions. Applied to a liquid pouring task, our method demonstrates improved success rates even with models trained on minimal data, outperforming traditional methods and showcasing its potential for robust decision-making in robotics. |
| 2025-07-28 | [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](http://arxiv.org/abs/2507.20850v1) | Meiting Dang, Yanping Wu et al. | Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method. |
| 2025-07-28 | [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](http://arxiv.org/abs/2507.20804v1) | Xueyao Wan, Hang Yu | Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths. |
| 2025-07-28 | [Physical Constraints on the Rhythmicity of the Biological Clock](http://arxiv.org/abs/2507.20750v1) | YeongKyu Lee, Changbong Hyeon | Circadian rhythms in living organisms are temporal orders emerging from biochemical circuits driven out of equilibrium. Here, we study how the rhythmicity of a biochemical clock is shaped using the KaiABC system. A phase diagram constructed as a function of KaiC and KaiA concentrations reveals a sharply bounded limit-cycle region, which naturally explains arrhythmia upon protein over-expression. Beyond the Hopf bifurcation, intrinsic noise enables regular oscillation via coherence resonance. Within the limit-cycle region, greater rhythmic precision incurs a higher energetic cost, following the thermodynamic uncertainty relation. The cost-minimizing period of the KaiABC clock ($\sim$21-hr) is close enough to entrain to 24-hr cycle of environment. Our study substantiates universal physical constraints on the robustness, precision, and efficiency of noisy biological clocks. |
| 2025-07-28 | [Generalized Uncertainty Principle as a Mechanism for CP Violation](http://arxiv.org/abs/2507.20727v1) | Hector Gisbert, Victor Ilisie et al. | Within quantum electrodynamics we show that the Generalized Uncertainty Principle induces higher-derivative corrections that promote the topological invariant $F_{\mu\nu}\,\widetilde F^{\mu\nu}$ to the dynamical, non-topological operator $\partial^\lambda F_{\mu\nu}\,\partial_\lambda \widetilde F^{\mu\nu}$. We explore the resulting phenomenology, focusing on the generation of electric dipole moments. Our findings open a new low-energy window for testing quantum-gravity scenarios through precision measurements of charge-parity violation. |
| 2025-07-28 | [Uncertainty-driven Embedding Convolution](http://arxiv.org/abs/2507.20718v1) | Sungjun Lim, Kangjun Noh et al. | Text embeddings are essential components in modern NLP pipelines. While numerous embedding models have been proposed, their performance varies across domains, and no single model consistently excels across all tasks. This variability motivates the use of ensemble techniques to combine complementary strengths. However, most existing ensemble methods operate on deterministic embeddings and fail to account for model-specific uncertainty, limiting their robustness and reliability in downstream applications. To address these limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC first transforms deterministic embeddings into probabilistic ones in a post-hoc manner. It then computes adaptive ensemble weights based on embedding uncertainty, grounded in a Bayes-optimal solution under a surrogate loss. Additionally, UEC introduces an uncertainty-aware similarity function that directly incorporates uncertainty into similarity scoring. Extensive experiments on retrieval, classification, and semantic similarity benchmarks demonstrate that UEC consistently improves both performance and robustness by leveraging principled uncertainty modeling. |
| 2025-07-25 | [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](http://arxiv.org/abs/2507.19458v1) | Amir Fard, Arnold X. -X. Yuan | Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows. |
| 2025-07-25 | [DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment](http://arxiv.org/abs/2507.19418v1) | Yiwei Lou, Yuanpeng He et al. | Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios. |
| 2025-07-25 | [Measurement of the Inelastic Proton-Proton Cross-Section at $\sqrt{s} \geq 40$ TeV Using the Hybrid Data of the Pierre Auger Observatory](http://arxiv.org/abs/2507.19326v1) | Olena Tkachenko | Measuring proton-proton interaction cross-sections at center-of-mass energies above 40 TeV remains a significant challenge in particle physics. The Pierre Auger Observatory provides a unique opportunity to study the interactions at the highest energies through the distribution of the depth of maximum shower development ($X_\mathrm{max}$) observed by its Fluorescence Detector. In previous studies, the determination of the interaction cross-section at ultrahigh energies has relied on the assumption that the tail of the $X_\mathrm{max}$ distribution is proton-dominated, which restricts the analysis to a limited energy range below the ankle and introduces related systematic uncertainties. In this contribution, we adopt a novel method for the simultaneous estimation of the proton-proton interaction cross-section and the primary cosmic-ray mass composition using data from the Pierre Auger Observatory, avoiding assumptions about one quantity to infer the other and thus improving the accuracy and robustness of our analysis. In addition, a systematic shift in the $X_\mathrm{max}$ scale is fitted to account for both experimental uncertainties and theoretical constraints on the modeling of particle interactions. The obtained results are consistent with previous analyses and provide additional constraints on hadronic interaction models. The measured proton-proton inelastic cross-section at ultra-high energies agrees well with extrapolations of accelerator data. The inferred cosmic-ray composition and the $X_\mathrm{max}$-scale shift are also compatible with previous estimates. |
| 2025-07-25 | [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](http://arxiv.org/abs/2507.19263v1) | Achille Morenville, Ã‰ric Piette | In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings. |
| 2025-07-25 | [Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation](http://arxiv.org/abs/2507.19246v1) | Robert Hahn, Sebastian SchÃ¶ps | While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort. |
| 2025-07-25 | [Constraining the origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory: a three-dimensional approach](http://arxiv.org/abs/2507.19216v1) | Marta Bianciotto | Unveiling the sources of ultra-high-energy cosmic rays remains one of the main challenges of high-energy astrophysics. Measurements of anisotropies in their arrival directions are key to identifying their sources, yet magnetic deflections obscure direct associations. In this work, we reconstruct the sky regions of possible origin of the highest-energy cosmic-ray events detected by the Pierre Auger Observatory by tracing their trajectories through Galactic magnetic fields using up-to-date models, while fully accounting for energy and directional uncertainties. A mixed composition at injection is assumed to model the detected charge distributions of such events. Different classes of astrophysical sources are investigated and tested for a correlation with the inferred regions of origin of the events. By incorporating constraints on the maximum propagation distances, we also allow for a three-dimensional localization of the possible source regions. Our findings provide new constraints on the sources of the highest-energy cosmic particles and offer fresh insights into the role of Galactic magnetic fields in shaping the observed ultra-high-energy cosmic-ray sky. |
| 2025-07-25 | [Emulating redshift-mixing due to blending in weak gravitational lensing](http://arxiv.org/abs/2507.19130v1) | Zekang Zhang, Daniel Gruen et al. | Galaxies whose images overlap in the focal plane of a telescope, commonly referred to as blends, are often located at different redshifts. Blending introduces a challenge to weak lensing cosmology probes, as such blends are subject to shear signals from multiple redshifts. This effect can be described by joining shear bias and redshift characterisation in the effective redshift distribution, $n_{\gamma}(z)$, which includes the response of apparent shapes of detected objects to shear of galaxies at redshift $z$. In this work, we propose a novel method to correct $n_{\gamma}(z)$ for redshift-mixed blending by emulating the shear response to neighbouring galaxies. Specifically, we design a ``half-sky-shearing'' simulation with HSC-Wide-like specifications, in which we extract the response of a detected object's measured ellipticity to shear of neighbouring galaxies among numerous galaxy pairs. We demonstrate the feasibility of accurately emulating these pairwise responses and validate the robustness of our approach under varying observing conditions and galaxy population uncertainties. We find that the effective redshift of sources at the high-redshift tail of the distribution is about 0.05 lower than expected when not modelling the effect. Given appropriately processed image simulations, our correction method can be readily incorporated into future cosmological analyses to mitigate this source of systematic error. |
| 2025-07-25 | [Boundary-layer transition in the age of data: from a comprehensive dataset to fine-grained prediction](http://arxiv.org/abs/2507.19120v1) | Wenhui Chang, Hongyuan Hu et al. | The laminar-to-turbulent transition remains a fundamental and enduring challenge in fluid mechanics. Its complexity arises from the intrinsic nonlinearity and extreme sensitivity to external disturbances. This transition is critical in a wide range of applications, including aerospace, marine engineering, geophysical flows, and energy systems. While the governing physics can be well described by the Navier-Stokes equations, practical prediction efforts often fall short due to the lack of comprehensive models for perturbation initialization and turbulence generation in numerical simulations. To address the uncertainty introduced by unforeseeable environmental perturbations, we propose a fine-grained predictive framework that accurately predicts the transition location. The framework generates an extensive dataset using nonlinear parabolized stability equations (NPSE). NPSE simulations are performed over a wide range of randomly prescribed initial conditions for the generic zero-pressure-gradient flat-plate boundary-layer flow, resulting in a large dataset that captures the nonlinear evolution of instability waves across three canonical transition pathways (Type-K, -H, and -O). From a database of 3000 simulation cases, we extract diagnostic quantities (e.g., wall pressure signals and skin-friction coefficients) from each simulation to construct a feature set that links pre-transition flow characteristics to transition onset locations. Machine learning models are systematically evaluated, with ensemble methods-particularly XGBoost-demonstrating exceptional predictive accuracy (mean relative error of approximately 0.001). Compared to methods currently available (e.g., N-factor, transitional turbulence model), this approach accounts for the physical process and achieves transition prediction without relying on any empirical parameters. |
| 2025-07-25 | [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](http://arxiv.org/abs/2507.19117v1) | Swapnil Hingmire, Ze Shi Li et al. | Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed. |
| 2025-07-25 | [Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks](http://arxiv.org/abs/2507.19111v1) | Bowen Li, Junting Chen | Dynamic low altitude networks offer significant potential for efficient and reliable data transport via unmanned aerial vehicles (UAVs) relays which usually operate with predetermined trajectories. However, it is challenging to optimize the data routing and resource allocation due to the time-varying topology and the need to control interference with terrestrial systems. Traditional schemes rely on time-expanded graphs with uniform and fine time subdivisions, making them impractical for interference-aware applications. This paper develops a dynamic space-time graph model with a cross-layer optimization framework that converts a joint routing and predictive resource allocation problem into a joint bottleneck path planning and resource allocation problem. We develop explicit deterministic bounds to handle the channel uncertainty and prove a monotonicity property in the problem structure that enables us to efficiently reach the globally optimal solution to the predictive resource allocation subproblem. Then, this approach is extended to multi-commodity transmission tasks through time-frequency allocation, and a bisection search algorithm is developed to find the optimum solution by leveraging the monotonicity of the feasible set family. Simulations verify that the single-commodity algorithm approaches global optimality with more than 30 dB performance gain over the classical graph-based methods for delay-sensitive and large data transportation. At the same time, the multi-commodity method achieves 100X improvements in dense service scenarios and enables an additional 20 dB performance gain by data segmenting. |
| 2025-07-24 | [Partial-State DADS Control for Matched Unmodeled Dynamics](http://arxiv.org/abs/2507.18609v1) | Iasson Karafyllis, Miroslav Krstic | We extend the Deadzone-Adapted Disturbance Suppression (DADS) control to time-invariant systems with dynamic uncertainties that satisfy the matching condition and for which no bounds for the disturbance and the unknown parameters are known. This problem is equivalent to partial-state adaptive feedback, where the states modeling the dynamic uncertainty are unmeasured. We show that the DADS controller can bypass small-gain conditions and achieve robust regulation for systems in spite of the fact that the strength of the interconnections has no known bound. Moreover, no gain and state drift arise, regardless of the size of the disturbances and unknown parameters. Finally, the paper provides the detailed analysis of a control system where the unmeasured state (or the dynamic uncertainty) is infinite-dimensional and described by a reaction-diffusion Partial Differential Equation, where the diffusion coefficient and the reaction term are unknown. It is shown that even in the infinite-dimensional case, a DADS controller can be designed and guarantees robust regulation of the plant state. |
| 2025-07-24 | [Searching for Gravitational Waves with Gaia and its Cross-Correlation with PTA: Absolute vs Relative Astrometry](http://arxiv.org/abs/2507.18593v1) | Massimo Vaglio, Mikel Falxa et al. | Astrometric missions like Gaia provide exceptionally precise measurements of stellar positions and proper motions. Gravitational waves traveling between the observer and distant stars can induce small, correlated shifts in these apparent positions, a phenomenon known as astrometric deflection. The precision and scale of astrometric datasets make them well-suited for searching for a stochastic gravitational wave background, whose signature appears in the two-point correlation function of the deflection field across the sky. Although Gaia achieves high accuracy in measuring angular separations in its focal plane, systematic uncertainties in the satellite's absolute orientation limit the precision of absolute position measurements. These orientation errors can be mitigated by focusing on relative angles between star pairs, which effectively cancel out common-mode orientation noise. In this work, we compute the astrometric response and the overlap reduction functions for this relative astrometry approach, correcting previous expressions presented in the literature. We use a Fisher matrix analysis to compare the sensitivity of relative astrometry to that of conventional absolute astrometry. Our analysis shows that while the relative method is theoretically sound, its sensitivity is limited for closely spaced star pairs within a single Gaia field of view. Pairs with large angular separations could provide competitive sensitivity, but are practically inaccessible due to Gaia's scanning law. Finally, we demonstrate that combining astrometric data with observations from pulsar timing arrays leads to slight improvements in sensitivity at frequencies greater than approximately 10^-7 Hz. |
| 2025-07-24 | [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](http://arxiv.org/abs/2507.18562v1) | Jiafeng Xiong, Yuting Zhao | Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. |
| 2025-07-24 | [Large deviations of ionic currents in dilute electrolytes](http://arxiv.org/abs/2507.18556v1) | Jafar Farhadi, David T. Limmer | We evaluate the exponentially rare fluctuations of the ionic current for a dilute electrolyte by means of macroscopic fluctuation theory. We consider the fluctuating hydrodynamics of a fluid electrolyte described by a stochastic Poisson-Nernst-Planck equation. We derive the Euler-Lagrange equations that dictate the optimal concentration profiles of ions conditioned on exhibiting a given current, whose form determines the likelihood of that current in the long-time limit. For a symmetric electrolyte under small applied voltages, number density fluctuations are small, and ionic current fluctuations are Gaussian with a variance determined by the Nernst-Einstein conductivity. Under large applied potentials, where number densities vary, the ionic current distribution is generically non-Gaussian. Its structure is constrained thermodynamically by Gallavotti-Cohen symmetry and the thermodynamic uncertainty principle. |
| 2025-07-24 | [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](http://arxiv.org/abs/2507.18498v1) | Zongzheng Zhang, Xuchong Qiu et al. | Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction. |
| 2025-07-24 | [A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks](http://arxiv.org/abs/2507.18492v1) | Mirhan Ãœrkmez, Carsten KallesÃ¸e et al. | Water utilities aim to reduce the high electrical costs of Water Distribution Networks (WDNs), primarily driven by pumping. However, pump scheduling is challenging due to model uncertainties and water demand forecast errors. This paper presents a Robust Model Predictive Control (RMPC) method for optimal and reliable pump scheduling, extending a previous efficient robust control method tailored to our model. A linear model with bounded additive disturbances is used to represent tank water level evolution, with uncertainty bounds derived from WDN simulation and demand data. At each time step, a pump scheduling policy, affine in past disturbances, is optimized to satisfy system constraints over a prediction horizon. The resulting policies are then applied in a receding horizon fashion. The optimization problem is formulated to require $\mathcal{O}(N^6)$ computations per iteration with an interior-point method, which is reduced to $\mathcal{O}(N^3)$ by reformulating it into a sparse form. When evaluated on a model representing the water distribution network of Randers, a medium-sized town in Denmark, the method surpasses nominal and constraint-tightening model predictive control (MPC) approaches in terms of meeting constraints and provides comparable economic outcomes. |
| 2025-07-24 | [Relativistic Calculations of Energy Levels, Field Shift Factors, and Polarizabilities of Mercury and Copernicium](http://arxiv.org/abs/2507.18490v1) | Hongxu Liu, Jize Han et al. | Mercury (Hg) and superheavy element copernicium (Cn) are investigated using equation-of-motion relativistic coupled-cluster (EOM-RCC) and configuration interaction plus many-body perturbation theory (CI+MBPT) methods. Key atomic properties including ionization potentials (IP), excitation energies (EEs), isotope field shift factors (F), and static electric dipole polarizabilities ({\alpha}) are calculated for ground and low-lying excited states. To evaluate the theoretical accuracy, calculations for both Hg and Cn are performed, with experimental data of Hg serving as benchmarks. Furthermore, basis set dependence has been systematically evaluated in the EOM-RCC calculations, with corresponding uncertainty estimates having been provided. The calculated atomic properties could provide valuable insights into the electronic structure and chemical behavior of superheavy elements. |
| 2025-07-24 | [INLA-RF: A Hybrid Modeling Strategy for Spatio-Temporal Environmental Data](http://arxiv.org/abs/2507.18488v1) | Mario Figueira, Michela Cameletti et al. | Environmental processes often exhibit complex, non-linear patterns and discontinuities across space and time, posing significant challenges for traditional geostatistical modeling approaches. In this paper, we propose a hybrid spatio-temporal modeling framework that combines the interpretability and uncertainty quantification of Bayesian models -- estimated using the INLA-SPDE approach -- with the predictive power and flexibility of Random Forest (RF). Specifically, we introduce two novel algorithms, collectively named INLA-RF, which integrate a statistical spatio-temporal model with RF in an iterative two-stage framework. The first algorithm (INLA-RF1) incorporates RF predictions as an offset in the INLA-SPDE model, while the second (INLA-RF2) uses RF to directly correct selected latent field nodes. Both hybrid strategies enable uncertainty propagation between modeling stages, an aspect often overlooked in existing hybrid approaches. In addition, we propose a Kullback-Leibler divergence-based stopping criterion. We evaluate the predictive performance and uncertainty quantification capabilities of the proposed algorithms through two simulation studies. Results suggest that our hybrid approach enhances spatio-temporal prediction while maintaining interpretability and coherence in uncertainty estimates. |
| 2025-07-24 | [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](http://arxiv.org/abs/2507.18484v1) | Xiao Yang, Lingxuan Wu et al. | Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving. |
| 2025-07-24 | [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](http://arxiv.org/abs/2507.18423v1) | Mizuki Funato, Yohei Sawada | Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions. |
| 2025-07-23 | [A narrowband burst from FRB 20190520B simultaneously observed by FAST and Parkes](http://arxiv.org/abs/2507.17696v1) | Yuhao Zhu, Chenhui Niu et al. | Fast Radio Bursts (FRBs) are short-duration radio transients with mysterious origins. Since its uncertainty, there are very few FRBs that are observed by different instruments, simultaneously. This study presents a detailed analysis of a burst from FRB 20190520B observed by FAST and Parkes at the same time. The spectrum of this individual burst ended at the upper limit of the FAST frequency band and was simultaneously detected by the Parkes telescope in the 1.5-1.8 GHz range. By employing spectral energy distribution (SED) and spectral sharpness methods, we confirmed the presence of narrowband radiation in FRB 20190520B, which is crucial for understanding its radiation mechanisms. Our findings support the narrowband characteristics that most repeaters exhibit. This work also highlights the necessity of continued multiband observations to explore its periodicity and frequency-dependent properties, contributing to an in-depth understanding of FRB phenomena. |
| 2025-07-23 | [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](http://arxiv.org/abs/2507.17695v1) | Ilias Chatzistefanidis, Navid Nikaein | Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance. |
| 2025-07-23 | [The Joint Asymptotic Distribution of Entropy and Complexity](http://arxiv.org/abs/2507.17625v1) | Angelika Silbernagel, Christian WeiÃŸ | We derive the asymptotic distribution of ordinal-pattern frequencies under weak dependence conditions and investigate the long-run covariance matrix not only analytically for moving-average, Gaussian, and the novel generalized coin-tossing processes, but also approximately by a simulation-based approach. Then, we deduce the asymptotic distribution of the entropy-complexity pair, which emerged as a popular tool for summarizing the time-series dynamics. Here, we make the necessary distinction between a uniform and a non-uniform ordinal pattern distribution and, thus, obtain two different limit theorems. On this basis, we consider a test for serial dependence and check its finite-sample performance. Moreover, we use our asymptotic results to approximate the estimation uncertainty of entropy-complexity pairs. |
| 2025-07-23 | [Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling](http://arxiv.org/abs/2507.17526v1) | Leandro Von Krannichfeldt, Kristina Orehounig et al. | Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling. |
| 2025-07-23 | [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](http://arxiv.org/abs/2507.17477v1) | Haoran Sun, Zekun Zhang et al. | Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance. |
| 2025-07-23 | [Analysing Models for Volatility Clustering with Subordinated Processes: VGSA and Beyond](http://arxiv.org/abs/2507.17431v1) | Sourojyoti Barick, Sudip Ratan Chandra | This paper explores a comprehensive class of time-changed stochastic processes constructed by subordinating Brownian motion with Levy processes, where the subordination is further governed by stochastic arrival mechanisms such as the Cox Ingersoll Ross (CIR) and Chan Karolyi Longstaff Sanders (CKLS) processes. These models extend classical jump frameworks like the Variance Gamma (VG) and CGMY processes, allowing for more flexible modeling of market features such as jump clustering, heavy tails, and volatility persistence. We first revisit the theory of Levy subordinators and establish strong consistency results for the VG process under Gamma subordination. Building on this, we prove asymptotic normality for both the VG and VGSA (VG with stochastic arrival) processes when the arrival process follows CIR or CKLS dynamics. The analysis is then extended to the more general CGMY process under stochastic arrival, for which we derive analogous consistency and limit theorems under positivity and regularity conditions on the arrival process. A simulation study accompanies the theoretical work, confirming our results through Monte Carlo experiments, with visualizations and normality testing (via Shapiro-Wilk statistics) that show approximate Gaussian behavior even for processes driven by heavy-tailed jumps. This work provides a rigorous and unified probabilistic framework for analyzing subordinated models with stochastic time changes, with applications to financial modeling and inference under uncertainty. |
| 2025-07-23 | [Confidence Calibration in Vision-Language-Action Models](http://arxiv.org/abs/2507.17383v1) | Thomas P Zollo, Richard Zemel | Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification. |
| 2025-07-23 | [Exploring Spatial Diversity for Region-based Active Learning](http://arxiv.org/abs/2507.17367v1) | Lile Cai, Xun Xu et al. | State-of-the-art methods for semantic segmentation are based on deep neural networks trained on large-scale labeled datasets. Acquiring such datasets would incur large annotation costs, especially for dense pixel-level prediction tasks like semantic segmentation. We consider region-based active learning as a strategy to reduce annotation costs while maintaining high performance. In this setting, batches of informative image regions instead of entire images are selected for labeling. Importantly, we propose that enforcing local spatial diversity is beneficial for active learning in this case, and to incorporate spatial diversity along with the traditional active selection criterion, e.g., data sample uncertainty, in a unified optimization framework for region-based active learning. We apply this framework to the Cityscapes and PASCAL VOC datasets and demonstrate that the inclusion of spatial diversity effectively improves the performance of uncertainty-based and feature diversity-based active learning methods. Our framework achieves $95\%$ performance of fully supervised methods with only $5-9\%$ of the labeled pixels, outperforming all state-of-the-art region-based active learning methods for semantic segmentation. |
| 2025-07-23 | [On Distributionally Robust Lossy Source Coding](http://arxiv.org/abs/2507.17366v1) | Giuseppe Serra, Photios A. Stavrou et al. | In this paper, we investigate the problem of distributionally robust source coding, i.e., source coding under uncertainty in the source distribution, discussing both the coding and computational aspects of the problem. We propose two extensions of the so-called Strong Functional Representation Lemma (SFRL), considering the cases where, for a fixed conditional distribution, the marginal inducing the joint coupling belongs to either a finite set of distributions or a Kullback-Leibler divergence sphere (KL-Sphere) centered at a fixed nominal distribution. Using these extensions, we derive distributionally robust coding schemes for both the one-shot and asymptotic regimes, generalizing previous results in the literature. Focusing on the case where the source distribution belongs to a given KL-Sphere, we derive an implicit characterization of the points attaining the robust rate-distortion function (R-RDF), which we later exploit to implement a novel algorithm for computing the R-RDF. Finally, we characterize the analytical expression of the R-RDF for Bernoulli sources, providing a theoretical benchmark to evaluate the estimation performance of the proposed algorithm. |
| 2025-07-23 | [Integrating Belief Domains into Probabilistic Logic Programs](http://arxiv.org/abs/2507.17291v1) | Damiano Azzolini, Fabrizio Riguzzi et al. | Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications. |
| 2025-07-22 | [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](http://arxiv.org/abs/2507.16806v1) | Mehul Damani, Isha Puri et al. | When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models. |
| 2025-07-22 | [Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2507.16796v1) | Mian Ibad Ali Shah, Enda Barrett et al. | This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities. |
| 2025-07-22 | [Quantum teleportation of an elemental silicon nanophotonic CNOT gate](http://arxiv.org/abs/2507.16783v1) | Kai-Chi Chang, Xiang Cheng et al. | Large-scale quantum computers possess the capacity to effectively tackle practical problems that can be insurmountable for classical computers. The main challenge in building these quantum computers is to realize scalable modules for remote qubits and entanglement. By assembling small, specialized parts into a larger architecture, the modular approach mitigates complexity and uncertainty. Such a distributed architecture requires non-local quantum gate operations between remote qubits. An essential method for implementing such operations, known as quantum gate teleportation, requires only local operations, classical communication, and shared entanglement. Till today, the quantum gate teleportation using a photonic chip has remained elusive. Here we experimentally demonstrate the quantum teleportation of an on-chip controlled-NOT (CNOT) gate, assisted with the scalable silicon chip platform, high-fidelity local quantum logic gates, linear optical components, post-selected entanglement, and coincidence measurements from photonic qubits. First, we measure and characterize our teleported chip-scale CNOT gate with an average truth table fidelity of 93.1 +- 0.3%. Second, for different input polarization states, we obtain an average quantum state fidelity of 87.0 +- 2.2% with our teleported on-chip CNOT gate. Third, we use our non-local CNOT gate for remote entanglement creation of four Bell states, with an average quantum state fidelity of 86.2 +- 0.8%. Fourthly, we fully characterize our teleported on-chip CNOT gate with a quantum process fidelity 83.1 +- 2.0%, and an average non-local CNOT gate fidelity of 86.5 +- 2.2%. Our teleported photonic on-chip quantum logic gate could be extended both to multiple qubits and chip-scale modules towards fault-tolerant and large-scale distributed quantum computation. |
| 2025-07-22 | [A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling](http://arxiv.org/abs/2507.16771v1) | Michael Grosskopf, Kellin Rumsey et al. | The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case. |
| 2025-07-22 | [Planck constraints on the scale dependence of isotropic cosmic birefringence](http://arxiv.org/abs/2507.16714v1) | M. Ballardini, A. Gruppuso et al. | The rotation of the linear polarisation plane of photons during propagation, also known as cosmic birefringence, is a powerful probe of parity-violating extensions of standard electromagnetism. Using Planck legacy data, we confirm previous estimates of the isotropic birefringence angle, finding $\beta \simeq 0.30 \pm 0.05$ [deg] at 68% CL, not including the systematic error from the instrumental polarisation angle. If this is a genuine signal, it could be explained by theories of Chern--Simons-type coupled to electromagnetism, which could lead to a harmonic scale-dependent birefringence signal, if the hypothesis of an ultra-light (pseudo) scalar field does not hold. To investigate these models, we pursue two complementary approaches: first, we fit the birefringence angle estimated at different multipoles, $\beta_{\ell}$, with a power-law model and second, we perform a non-parametric Bayesian reconstruction of it. Both methods yield results consistent with a non-vanishing constant birefringence angle. The first method shows no significant dependence on the harmonic scale (up to $1.8\sigma$ CL), while the second method demonstrates that a constant model is favored by Bayesian evidence. This conclusion is robust across all four published Planck CMB solutions. Finally, we forecast that upcoming CMB observations by Simons Observatory, LiteBIRD and a wishful CMB-Stage 4 experiment could reduce current uncertainties by a factor of approximately 7. |
| 2025-07-22 | [Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains](http://arxiv.org/abs/2507.16670v1) | Amandeep Kaur, Gyan Prakash | Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty. |
| 2025-07-22 | [FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture](http://arxiv.org/abs/2507.16668v1) | Somayeh Sobati-M | Modern smart grids demand fast, intelligent, and energy-aware computing at the edge to manage real time fluctuations and ensure reliable operation. This paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration and Twin based Execution a next-generation fog cloud framework designed to enhance autonomy, resilience, and efficiency in distributed energy systems. FOGNITE combines three core components: federated learning, reinforcement learning, and digital twin validation. Each fog node trains a local CNN LSTM model on private energy consumption data, enabling predictive intelligence while preserving data privacy through federated aggregation. A reinforcement learning agent dynamically schedules tasks based on current system load and energy conditions, optimizing for performance under uncertainty.   To prevent unsafe or inefficient decisions, a hierarchical digital twin layer simulates potential actions before deployment, significantly reducing execution errors and energy waste. We evaluate FOGNITE on a real world testbed of Raspberry Pi devices, showing up to a 93.7% improvement in load balancing accuracy and a 63.2% reduction in energy waste compared to conventional architectures. By shifting smart grid control from reactive correction to proactive optimization, FOGNITE represents a step toward more intelligent, adaptive, and sustainable energy infrastructures |
| 2025-07-22 | [A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market](http://arxiv.org/abs/2507.16655v1) | Mahdi Goldani | In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis. |
| 2025-07-22 | [Bayesian Variational Inference for Mixed Data Mixture Models](http://arxiv.org/abs/2507.16545v1) | Junyang Wang, James Bennett et al. | Heterogeneous, mixed type datasets including both continuous and categorical variables are ubiquitous, and enriches data analysis by allowing for more complex relationships and interactions to be modelled. Mixture models offer a flexible framework for capturing the underlying heterogeneity and relationships in mixed type datasets. Most current approaches for modelling mixed data either forgo uncertainty quantification and only conduct point estimation, and some use MCMC which incurs a very high computational cost that is not scalable to large datasets. This paper develops a coordinate ascent variational inference algorithm (CAVI) for mixture models on mixed (continuous and categorical) data, which circumvents the high computational cost of MCMC while retaining uncertainty quantification. We demonstrate our approach through simulation studies as well as an applied case study of the NHANES risk factor dataset. In addition, we show that the posterior means from CAVI for this model converge to the true parameter value as the sample size n tends to infinity, providing theoretical justification for our method. |
| 2025-07-22 | [A Distributed Actor-Critic Algorithm for Fixed-Time Consensus in Nonlinear Multi-Agent Systems](http://arxiv.org/abs/2507.16520v1) | Aria Delshad, Maryam Babazadeh | This paper proposes a reinforcement learning (RL)-based backstepping control strategy to achieve fixed time consensus in nonlinear multi-agent systems with strict feedback dynamics. Agents exchange only output information with their neighbors over a directed communication graph, without requiring full state measurements or symmetric communication. Achieving fixed time consensus, where convergence occurs within a pre-specified time bound that is independent of initial conditions is faced with significant challenges due to the presence of unknown nonlinearities, inter-agent couplings, and external disturbances. This work addresses these challenges by integrating actor critic reinforcement learning with a novel fixed time adaptation mechanism. Each agent employs an actor critic architecture supported by two estimator networks designed to handle system uncertainties and unknown perturbations. The adaptation laws are developed to ensure that all agents track the leader within a fixed time regardless of their initial conditions. The consensus and tracking errors are guaranteed to converge to a small neighborhood of the origin, with the convergence radius adjustable through control parameters. Simulation results demonstrate the effectiveness of the proposed approach and highlight its advantages over state-of-the-art methods in terms of convergence speed and robustness. |
| 2025-07-21 | [Euclid preparation: Expected constraints on initial conditions](http://arxiv.org/abs/2507.15819v1) | Euclid Collaboration, F. Finelli et al. | The Euclid mission of the European Space Agency will deliver galaxy and cosmic shear surveys, which will be used to constrain initial conditions and statistics of primordial fluctuations. We present highlights for the Euclid scientific capability to test initial conditions beyond LCDM with the main probes, i.e. 3D galaxy clustering from the spectroscopic survey, the tomographic approach to 3x2pt statistics from photometric galaxy survey, and their combination. We provide Fisher forecasts from the combination of Euclid spectroscopic and photometric surveys for spatial curvature, running of the spectral index of the power spectrum of curvature perturbations, isocurvature perturbations, and primordial features. For the parameters of these models we also provide the combination of Euclid forecasts (pessimistic and optimistic) with current and future measurements of the cosmic microwave background (CMB) anisotropies., i.e. Planck, the Simons Observatory (SO), and CMB-S4. We provide Fisher forecasts for how the power spectrum and bispectrum from the Euclid spectroscopic survey will constrain the local, equilateral, and orthogonal shapes of primordial non-Gaussianity. We also review how Bayesian field-level inference of primordial non-Gaussianity can constrain local primordial non-Gaussianity. We show how Euclid, with its unique combination of the main probes, will provide the tightest constraints on low redshift to date. By targeting a markedly different range in redshift and scale, Euclid's expected uncertainties are complementary to those obtained by CMB primary anisotropy, returning the tightest combined constraints on the physics of the early Universe. |
| 2025-07-21 | [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](http://arxiv.org/abs/2507.15803v1) | Danhui Chen, Ziquan Liu et al. | Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in. |
| 2025-07-21 | [Deterministic Quantum Search via Recursive Oracle Expansion](http://arxiv.org/abs/2507.15797v1) | John Burke, Ciaran McGoldrick | We introduce a novel deterministic quantum search algorithm that provides a practical alternative to conventional probabilistic search approaches. Our scheme eliminates the inherent uncertainty of quantum search without relying on arbitrary phase rotations, a key limitation of other deterministic methods. The algorithm achieves certainty by recursively expanding the base oracle so that it marks all states prefixed by the same two bits as the target, encompassing exactly one-quarter of the search space. This enables a step-by-step reduction of the superposition until the target state can be measured with certainty. The algorithm achieves deterministic success with a query complexity of $O(N^{\log_2(3)/2}) \approx O(N^{0.7925})$, falling between Grover's $O(\sqrt{N})$ scaling and the classical $O(N)$. Our approach relies exclusively on two-qubit nearest-neighbour diffusion operators, avoiding global diffusion entirely. We show that, despite the increased query complexity, this design reduces the total number of two-qubit gates required for diffusion by more than an order of magnitude for search spaces up to at least 18 qubits, with even greater advantages on hardware with limited qubit connectivity. The scheme's inherent determinism, reliance on simple nearest-neighbour, low-depth operations, and scalable recursive structure make it well-suited for hardware implementation. Additionally, we show that the algorithm naturally supports partial database search, enabling deterministic identification of selected target bits without requiring a full search, further broadening its applicability. |
| 2025-07-21 | [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](http://arxiv.org/abs/2507.15782v1) | Ruochu Yang, Yu Zhou et al. | Household robots have been a longstanding research topic, but they still lack human-like intelligence, particularly in manipulating open-set objects and navigating large environments efficiently and accurately. To push this boundary, we consider a generalized multi-object collection problem in large scene graphs, where the robot needs to pick up and place multiple objects across multiple locations in a long mission of multiple human commands. This problem is extremely challenging since it requires long-horizon planning in a vast action-state space under high uncertainties. To this end, we propose a novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a multimodal action cost similarity function, our algorithm can both reflect the history and look into the future to optimize plans, striking a good balance of quality and efficiency. Simulation experiments demonstrate that compared with latest works, our algorithm improves the overall mission performance by 30% in terms of fulfilling human commands, maximizing mission success rates, and minimizing mission costs. |
| 2025-07-21 | [Learning Climate Sensitivity from Future Observations, Fast and Slow](http://arxiv.org/abs/2507.15767v1) | Adam Michael Bauer, Cristian Proistosescu et al. | Climate sensitivity has remained stubbornly uncertain since the Charney Report was published some 45 years ago. Two factors in future climate projections could alter this dilemma: (i) an increased ratio of CO$_2$ forcing relative to aerosol cooling, owing to both continued accumulation of CO$_2$ and declining aerosol emissions, and (ii) a warming world, whereby CO$_2$-induced warming becomes more pronounced relative to climate variability. Here, we develop a novel modeling approach to explore the rates of learning about equilibrium climate sensitivity and the transient climate response (TCR) and identify the physical drivers underpinning these learning rates. Our approach has the advantage over past work by accounting for the full spectrum of parameter uncertainties and covariances, while also taking into account serially correlated internal climate variability. Moreover, we provide a physical explanation of how quickly we may hope to learn about climate sensitivity. We find that, although we are able to constrain future TCR regardless of the true underlying value, constraining ECS is more difficult, with low values of ECS being more easily ascertained than high values. This asymmetry can be explained by most of the warming this century being attributable to the fast climate mode, which is more useful for constraining TCR than it is for ECS. We further show that our inability to constrain the deep ocean response is what limits our ability to learn high values of ECS. |
| 2025-07-21 | [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](http://arxiv.org/abs/2507.15741v1) | GÃ¡bor Lugosi, Marcos Matabuena | This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces. Leveraging a newly defined notion of homoscedasticity, we develop a conformal prediction algorithm that offers finite-sample coverage guarantees and fast convergence rates of the oracle estimator. In heteroscedastic settings, we forgo these non-asymptotic guarantees to gain statistical efficiency, proposing a local $k$--nearest--neighbor method without conformal calibration that is adaptive to the geometry of each particular nonlinear space. Both procedures work with any regression algorithm and are scalable to large data sets, allowing practitioners to plug in their preferred models and incorporate domain expertise. We prove consistency for the proposed estimators under minimal conditions. Finally, we demonstrate the practical utility of our approach in personalized--medicine applications involving random response objects such as probability distributions and graph Laplacians. |
| 2025-07-21 | [Symplectic coherence: a measure of position-momentum correlations in quantum states](http://arxiv.org/abs/2507.15738v1) | Varun Upreti, Ulysse Chabaud | The interdependence of position and momentum, as highlighted by the Heisenberg uncertainty principle, is a cornerstone of quantum physics. Yet, position-momentum correlations have received little systematic attention. Motivated by recent developments in bosonic quantum physics that underscore their relevance in quantum thermodynamics, metrology, and computing, we establish a general framework to study and quantify position-momentum correlations in quantum states. We introduce symplectic coherence, a faithful and easily computable measure defined as the Frobenius norm of the block of the covariance matrix encoding position-momentum correlations, and demonstrate that symplectic coherence is monotone under relevant operations and robust under small perturbations. Furthermore, using a recent mapping by Barthe et al. (Phys. Rev. Lett. 134, 070604) which relates the covariance matrix of a bosonic state to the density matrix of a finite-dimensional system, we show that position-momentum correlations correspond to beyond-classical correlations in a virtual finite-dimensional quantum state, with symplectic coherence mapping naturally to geometric quantum discord. Taking energy constraints into account, we determine the maximal position-momentum correlations achievable at fixed energy, revealing structural insights about the corresponding optimal states. Finally, we illustrate the operational relevance of symplectic coherence through several examples in quantum information tasks and quantum thermodynamics. In the process, we establish new technical results on matrix norms and quantum covariance matrices, and demonstrate the conceptual significance of viewing covariance matrices as density matrices of virtual quantum states. |
| 2025-07-21 | [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](http://arxiv.org/abs/2507.15727v1) | Xuchuang Wang, Bo Sun et al. | This paper introduces a novel multi-agent ski-rental problem that generalizes the classical ski-rental dilemma to a group setting where agents incur individual and shared costs. In our model, each agent can either rent at a fixed daily cost, or purchase a pass at an individual cost, with an additional third option of a discounted group pass available to all. We consider scenarios in which agents' active days differ, leading to dynamic states as agents drop out of the decision process. To address this problem from different perspectives, we define three distinct competitive ratios: overall, state-dependent, and individual rational. For each objective, we design and analyze optimal deterministic and randomized policies. Our deterministic policies employ state-aware threshold functions that adapt to the dynamic states, while our randomized policies sample and resample thresholds from tailored state-aware distributions. The analysis reveals that symmetric policies, in which all agents use the same threshold, outperform asymmetric ones. Our results provide competitive ratio upper and lower bounds and extend classical ski-rental insights to multi-agent settings, highlighting both theoretical and practical implications for group decision-making under uncertainty. |
| 2025-07-21 | [Evaluation of hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles using electrochemical permeation and thermal desorption spectroscopy](http://arxiv.org/abs/2507.15711v1) | Nicholas Winzer | Hydrogen diffusion and trapping in ferritic steels containing (Ti,Cr)C particles was investigated using electrochemical permeation (EP) and thermal desorption spectroscopy (TDS). The trapping parameters for the test materials were evaluated by fitting the measurements with a finite element model based on the McNabb-Foster equations using least-squares optimisation. The measurements showed that hydrogen diffusion in ferrite is slowed significantly by the presence of fine (<5 nm) (Ti,Cr)C particles; coarser particles had little or no effect. The TDS measurements were consistent with hydrogen traps with a high energy barrier. The uniqueness of the hydrogen trapping parameters obtained using the fitting procedure was evaluated. It was found that the system was overdetermined; the measurements could be fitted with multiple combinations of trapping parameters. Consequently, it was not possible to determine the individual trapping parameters using this procedure. Trapping parameters were also evaluated from TDS measurements by applying Kissinger's equation. Using this procedure a trap binding energy of 0.24 eV was calculated for all materials, albeit with a high degree of uncertainty. |
| 2025-07-21 | [Ubiquity of Uncertainty in Neuron Systems](http://arxiv.org/abs/2507.15702v1) | Brandon B. Le, Bennett Lamb et al. | We demonstrate that final-state uncertainty is ubiquitous in multistable systems of coupled neuronal maps, meaning that predicting whether one such system will eventually be chaotic or nonchaotic is often nearly impossible. We propose a "chance synchronization" mechanism that governs the emergence of unpredictability in neuron systems and support it by using basin classification, uncertainty exponent, and basin entropy techniques to analyze five simple discrete-time systems, each consisting of a different neuron model. Our results illustrate that uncertainty in neuron systems is not just a product of noise or high-dimensional complexity; it is also a fundamental property of low-dimensional, deterministic models, which has profound implications for understanding brain function, modeling cognition, and interpreting unpredictability in general multistable systems. |
| 2025-07-18 | [Missing baryons recovered: a measurement of the gas fraction in galaxies and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing](http://arxiv.org/abs/2507.14136v1) | Boryana Hadzhiyska, Simone Ferraro et al. | We present new constraints on the halo masses and matter density profiles of DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs) and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing convergence map from ACT DR6. This provides an independent, lensing-based calibration of halo masses, complementary to methods relying on clustering or dynamics. We derive constraints on the mean halo mass for three DESI-selected samples, finding $\log(M_{\rm halo}/(M_\odot/h)) \approx 13.18$, 13.03 and 13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a halo model approach, we also compare the projected galaxy-matter density profiles with previously reported gas profiles inferred from measurements of the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the key uncertainties in interpreting kSZ signals -- the unknown host halo mass distribution -- by providing an independent and consistent mass calibration. The agreement between the gas and total mass profiles at large aperture suggests that sufficiently far from the group center (2--3 virial radii), we recover all the baryons, offering a resolution to the 'missing baryon' problem. We further study the cumulative gas fractions for all galaxies as well as for the most massive galaxy groups in the sample ($\log(M_{\rm halo}/(M_\odot/h)) \approx 13.5$), finding values that are physically sensible and in agreement with previous findings using kSZ and X-ray data: compared to the TNG300 simulation, the observed gas fractions are systematically lower at fixed radius by $\gtrsim$4$\sigma$, providing compelling, independent evidence for stronger baryonic feedback in the real Universe. These findings highlight the power of combining CMB lensing with galaxy surveys to probe the interplay between baryons and dark matter in group-sized halos. |
| 2025-07-18 | [On the relation between perspective-neutral, algebraic, and effective quantum reference frames](http://arxiv.org/abs/2507.14131v1) | Philipp A. Hoehn, Julian De Vuyst et al. | The framework of internal quantum reference frames (QRFs) constitutes a universal toolset for dealing with symmetries in quantum theory and has led to new revelations in quantum gravity, gauge theories and foundational physics. Multiple approaches have emerged, sometimes differing in scope and the way symmetries are implemented, raising the question as to their relation. Here, we investigate the relation between three approaches to QRFs for gauge symmetries, namely the effective semiclassical, algebraic, and perspective-neutral (PN) approaches. Rather than constructing Hilbert spaces, as the PN approach, the effective approach is based on a quantum phase space parametrized by expectation values and fluctuations, while the emphasis of the algebraic approach is on the state space of complex linear functionals on a kinematical algebra. Nevertheless, external frame information is treated as gauge in all three formalisms, manifested in constraints on states and algebra. We show that these three approaches are, in fact, equivalent for ideal QRFs, distinguished by sharp orientations, which is the previous setting of the first two approaches. Our demonstration pertains to single constraints, including relativistic ones, and encompasses QRF changes. In particular, the QRF transformations of the PN framework agree semiclassically with those of the older effective approach, by which it was inspired. As a physical application, we explore the QRF covariance of uncertainties and fluctuations, which turn out to be frame-dependent. This is particularly well-suited for the effective and algebraic approaches, for which these quantities form a natural basis. Finally, we pave the way towards extending these two approaches to non-ideal QRFs by studying the projection and gauge-fixing operations of the Page-Wootters formalism, built into the PN framework, on algebraic states. |
| 2025-07-18 | [Integrating Forecasting Models Within Steady-State Analysis and Optimization](http://arxiv.org/abs/2507.14117v1) | Aayushya Agarwal, Larry Pileggi | Extreme weather variations and the increasing unpredictability of load behavior make it difficult to determine power grid dispatches that are robust to uncertainties. While machine learning (ML) methods have improved the ability to model uncertainty caused by loads and renewables, accurately integrating these forecasts and their sensitivities into steady-state analyses and decision-making strategies remains an open challenge. Toward this goal, we present a generalized methodology that seamlessly embeds ML-based forecasting engines within physics-based power flow and grid optimization tools. By coupling physics-based grid modeling with black-box ML methods, we accurately capture the behavior and sensitivity of loads and weather events by directly integrating the inputs and outputs of trained ML forecasting models into the numerical methods of power flow and grid optimization. Without fitting surrogate load models, our approach obtains the sensitivities directly from data to accurately predict the response of forecasted devices to changes in the grid. Our approach combines the sensitivities of forecasted devices attained via backpropagation and the sensitivities of physics-defined grid devices. We demonstrate the efficacy of our method by showcasing improvements in sensitivity calculations and leveraging them to design a robust power dispatch that improves grid reliability under stochastic weather events. Our approach enables the computation of system sensitivities to exogenous factors which supports broader analyses that improve grid reliability in the presence of load variability and extreme weather conditions. |
| 2025-07-18 | [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](http://arxiv.org/abs/2507.14102v1) | Shravan Venkatraman, Pavan Kumar S et al. | Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL |
| 2025-07-18 | [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](http://arxiv.org/abs/2507.14099v1) | Markus Buchholz, Ignacio Carlucho et al. | Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges. |
| 2025-07-18 | [Global Bayesian Analysis of $\mathrm{J}/Ïˆ$ Photoproduction on Proton and Lead Targets](http://arxiv.org/abs/2507.14087v1) | Heikki MÃ¤ntysaari, Hendrik Roch et al. | We perform a global Bayesian analysis of diffractive $\mathrm{J}/\psi$ production in $\gamma+p$ and $\gamma+\mathrm{Pb}$ collisions using a color glass condensate (CGC) based calculation framework. As past calculations have shown that CGC-based models typically overpredict the $\mathrm{J}/\psi$ production in $\gamma+\mathrm{Pb}$ collisions at high center of mass energy, we address the question of whether it is possible to describe coherent and incoherent diffractive $\mathrm{J}/\psi$ data from $\gamma+p$ collisions at HERA and the LHC, and from $\gamma+\mathrm{Pb}$ collisions at the LHC simultaneously. Our results indicate that a simultaneous description of $\gamma+p$ and $\gamma+\mathrm{Pb}$ data is challenging, with results improving when an overall $K$-factor -- scaling $\gamma+p$ and $\gamma+\mathrm{Pb}$ cross sections to absorb model uncertainties -- is introduced. |
| 2025-07-18 | [Direct Measurement of the Accretion Disk Formed in Prompt Collapse Mergers with Future Gravitational-Wave Observatories](http://arxiv.org/abs/2507.14071v1) | Arnab Dhani, Alessandro Camilletti et al. | The production site of heavy r-process elements, such as Gold and Uranium, is uncertain. Neutron star mergers are the only astrophysical phenomenon in which we have witnessed their formation. However, the amount of heavy elements resulting from the merger remains poorly constrained, mainly due to uncertainties on the mass and angular momentum of the disk formed in the merger remnant. Matter accretion from the disk is also thought to power gamma ray-bursts. We discover from numerical relativity simulations that the accretion disk influences the ringdown gravitational-wave signal produced by binaries that promptly collapse to black-hole at merger. We propose a method to \emph{directly} measure the mass of the accretion disk left during black hole formation in binary mergers using observatories such as the Einstein Telescope or Cosmic Explorer with a relative error of 10\% for binaries at a distance of up to 30~Mpc, corresponding to an event rate of 0.001 to 0.25 events per year. |
| 2025-07-18 | [VLA-Mark: A cross modal watermark for large vision-language alignment model](http://arxiv.org/abs/2507.14067v1) | Shuliang Liu, Qi Zheng et al. | Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking |
| 2025-07-18 | [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](http://arxiv.org/abs/2507.14056v1) | Alejandro Rodriguez-Garcia, Anindya Ghosh et al. | Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks. |
| 2025-07-18 | [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](http://arxiv.org/abs/2507.14034v1) | Jochen Wulf, Jurg Meierhofer et al. | Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems. |
| 2025-07-17 | [$\texttt{raccoon}$: A Python package for removing wiggle artifacts in the JWST NIRSpec integral field spectroscopy](http://arxiv.org/abs/2507.13341v1) | Anowar J. Shajib | $\texttt{raccoon}$ is a Python package for removing resampling noise - commonly referred to as "wiggles'' - from spaxel-level spectra in datacubes obtained from the JWST Near Infrared Spectrograph's (NIRSpec) integral field spectroscopy (IFS) mode. These wiggles arise as artifacts during resampling of the 2D raw data into 3D datacubes, due to the point spread function (PSF) being undersampled. The standard JWST data reduction pipeline does not correct for this noise. The wiggle artifacts can significantly degrade the scientific usability of the data, particularly at the spaxel level, undermining the exquisite spatial resolution of NIRSpec. $\texttt{raccoon}$ provides an empirical correction by modeling and removing these artifacts, thereby restoring the fidelity of the extracted spectra. $\texttt{raccoon}$ forward-models the wiggles as a chirp function impacting one or more template spectra that are directly fit to the original data across the entire wavelength range. The best-fit wiggle model is then used to clean the data while propagating the associated uncertainties. |
| 2025-07-17 | [A Framework for Waterfall Pricing Using Simulation-Based Uncertainty Modeling](http://arxiv.org/abs/2507.13324v1) | Nicola Jean, Giacomo Le Pera et al. | We present a novel framework for pricing waterfall structures by simulating the uncertainty of the cashflow generated by the underlying assets in terms of value, time, and confidence levels. Our approach incorporates various probability distributions calibrated on the market price of the tranches at inception. The framework is fully implemented in PyTorch, leveraging its computational efficiency and automatic differentiation capabilities through Adjoint Algorithmic Differentiation (AAD). This enables efficient gradient computation for risk sensitivity analysis and optimization. The proposed methodology provides a flexible and scalable solution for pricing complex structured finance instruments under uncertainty |
| 2025-07-17 | [Testing halo models for constraining astrophysical feedback with multi-probe modeling: I. 3D Power spectra and mass fractions](http://arxiv.org/abs/2507.13317v1) | Pranjal R. S., Shivam Pandey et al. | Upcoming Stage-IV surveys will deliver measurements of distribution of matter with unprecedented precision, demanding highly accurate theoretical models for cosmological parameter inference. A major source of modeling uncertainty lies in astrophysical processes associated with galaxy formation and evolution, which remain poorly understood. Probes such as the thermal and kinematic Sunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio bursts offer a promising avenue for mapping the distribution and thermal properties of cosmic baryons. A unified analytical framework capable of jointly modeling these observables is essential for fully harnessing the complementary information while mitigating probe-specific systematics. In this work, we present a detailed assessment of existing analytical models, which differ in their assumptions and prescriptions for simultaneously describing the distribution of matter and baryons in the universe. Using the Magneticum hydrodynamical simulation, we test these models by jointly analyzing the 3D auto- and cross-power spectra of the matter and baryonic fields that underpin the above probes. We find that all models can reproduce the power spectra at sub-percent to few-percent accuracy, depending on the tracer combination and number of free parameters. Their ability to recover underlying halo properties, such as the evolution of gas abundance and thermodynamic profiles with halo mass, varies considerably. Our results suggest that these models require further refinement and testing for reliable interpretation of multi-wavelength datasets. |
| 2025-07-17 | [Systematic study of the validity of the eikonal model including uncertainties](http://arxiv.org/abs/2507.13291v1) | Daniel Shiu, ChloÃ« Hebborn et al. | Nuclear reactions at intermediate beam energies are often interpreted using the eikonal model. In the analysis of complex reaction probes, where few-body reaction methods are needed, the eikonal method may be used as an efficient way for describing the fragment-target reaction process. In this work, we perform a systematic study to test the validity of the eikonal approximation for nucleon-nucleus reactions. We also quantify uncertainties due to the nucleon optical potential on reaction observables. We inspect the validity of the eikonal model and its semiclassical correction by comparing it to exact solutions (obtained from solving the optical model equation with a finite differences method) for a wide range of reactions. We also study the effect of relativistic corrections, both kinematic and dynamic, by effectively incorporating the relativistic effects at intermediate energies. The uncertainties from a Bayesian global optical potential (KDUQ) are propagated to the observables of interest. Our study includes neutron and proton reactions on $^{27}$Al, $^{40}$Ca, $^{90}$Zr and $^{208}$Pb, for a wide range of energies $E_{lab}=0-400$ MeV. Our results show that for the proton absorption cross section, the eikonal model can be used down to around $60$ MeV and the semiclassical correction extends its use to $30$ MeV. However, the validity of the eikonal model for the neutron total cross section only goes down to $\approx120$ MeV, a range extended to $\approx 50$ MeV when using the semiclassical correction. We find the semi-classical correction to the eikonal model to be less effective in describing the angular distributions. The $1\sigma$ uncertainty intervals on the observables we studied is less than $5$% for most of the energies considered, but increases rapidly for higher energies, namely energies outside the range of KDUQ ($E_{lab}>200$ MeV). |
| 2025-07-17 | [Multi-wavelength study of the high Galactic latitude supernova remnant candidate \snr\ associated with the Calvera pulsar](http://arxiv.org/abs/2507.13210v1) | Emanuele Greco, Michela Rigoselli et al. | The candidate supernova remnant (SNR) G118.4+37.0 (Calvera's SNR), discovered as a faint radio ring at high Galactic latitude and coincident with extended Fermi/LAT gamma-ray emission, is likely associated to the X-ray pulsar 1RXS J141256.0+792204 (Calvera). Previous XMM-Newton data hinted at soft diffuse X-ray emission inside the ring but lacked sufficient exposure for detailed characterisation. We obtained new XMM-Newton observations, and produced count-rate images, equivalent width and median photon energy maps to identify optimal regions for spectral analysis. We complemented these observations with a reanalysis of Fermi/LAT gamma-ray data and new Telescopio Nazionale Galileo observations aimed to search for Halpha emission. The X-ray diffuse emission is well described by a model of shock-heated plasma with temperature kT \sim 0.15 keV, mildly under-solar N and o abundances and densities ne=0.1-0.7 cm-3. According to our estimates, Calvera's SNR is 10-20 kya old and lies at a distance of 4-5 kpc. A distinti "Clump" region shows hared emission equally well described by a thermal (kT\sim 1.7 keV) or a non thermal model (Gamma \sim 2.7). The brightest X-ray area is close to the gamma-ray peak and to an isolated Alpha filament. G118.4+37.0 is a middle-aged remnant which expands in a tenuous medium and encountered a denser phase, likely the relic of the wind activity of the massive progenitor star. The estimated SNR distance is consistent within the uncertainties with that estimated for Calvera, confirming that this peculiar pulsar was born in the explosion of a massive star high above the Galactic disk. Our measured ambient density, together with the patchy morphology of the gamma-ray emission and the detection of Halpha filaments indicates that a hadronic origin is compatible with the gamma-ray flux, though a mixed leptonic-hadronic cannot be excluded |
| 2025-07-17 | [Performance Portable Gradient Computations Using Source Transformation](http://arxiv.org/abs/2507.13204v1) | Kim Liegeois, Brian Kelley et al. | Derivative computation is a key component of optimization, sensitivity analysis, uncertainty quantification, and nonlinear solvers. Automatic differentiation (AD) is a powerful technique for evaluating such derivatives, and in recent years, has been integrated into programming environments such as Jax, PyTorch, and TensorFlow to support derivative computations needed for training of machine learning models, resulting in widespread use of these technologies. The C++ language has become the de facto standard for scientific computing due to numerous factors, yet language complexity has made the adoption of AD technologies for C++ difficult, hampering the incorporation of powerful differentiable programming approaches into C++ scientific simulations. This is exacerbated by the increasing emergence of architectures such as GPUs, which have limited memory capabilities and require massive thread-level concurrency. Portable scientific codes rely on domain specific programming models such as Kokkos making AD for such codes even more complex. In this paper, we will investigate source transformation-based automatic differentiation using Clad to automatically generate portable and efficient gradient computations of Kokkos-based code. We discuss the modifications of Clad required to differentiate Kokkos abstractions. We will illustrate the feasibility of our proposed strategy by comparing the wall-clock time of the generated gradient code with the wall-clock time of the input function on different cutting edge GPU architectures such as NVIDIA H100, AMD MI250x, and Intel Ponte Vecchio GPU. For these three architectures and for the considered example, evaluating up to 10 000 entries of the gradient only took up to 2.17x the wall-clock time of evaluating the input function. |
| 2025-07-17 | [Medium-induced modification of azimuthal correlations of electrons from heavy-flavor hadron decays with charged particles in Pb-Pb collisions at $\sqrt{s_{\rm{NN}} = 5.02}$ TeV](http://arxiv.org/abs/2507.13197v1) | ALICE Collaboration | The azimuthal-correlation distributions between electrons from the decays of heavy-flavor hadrons and associated charged particles in Pb-Pb collisions at $\sqrt{s_{\rm NN}} = 5.02$ TeV are reported for the 0-10% and 30-50% centrality classes. This is the first measurement to provide access to the azimuthal-correlation observables in the heavy-flavor sector in Pb-Pb collisions. The analysis is performed for trigger electrons from heavy-flavor hadron decays with transverse momentum $4 < p_{\rm T}^{\rm e} < 16$ GeV/$c$, considering associated particles within the transverse-momentum range $1 < p_{\rm T}^{\rm assoc} < 7$ GeV/$c$, and a pseudorapidity difference of $|\Delta\eta| < 1$ between the trigger electron and associated particles. The per-trigger nuclear modification factor ($I_{\rm AA}$) is calculated to compare the near- and away-side peak yields to those in pp collisions at $\sqrt{s} = 5.02$ TeV. In 0-10% central collisions, the $I_{\rm AA}$ indicates a hint of enhancement of associated-particle yields with $p_{\rm T} < 3$ GeV/$c$ on the near side, and a suppression of yields with $p_{\rm T} > 4$ GeV/$c$ on the away side. The $I_{\rm AA}$ for electron triggers from heavy-flavor hadron decays is compared with that for light-flavor and strange-particle triggers to investigate the dependence on different fragmentation processes and parton-medium dynamics, and is found to be the same within uncertainties. |
| 2025-07-17 | [$\overlineÎ£^{\pm}$ production in pp and p-Pb collisions at $\sqrt{s_{\rm NN}}$ = 5.02 TeV with ALICE](http://arxiv.org/abs/2507.13183v1) | ALICE Collaboration | The transverse momentum spectra and integrated yields of $\overline{\Sigma}^{\pm}$ have been measured in pp and p-Pb collisions at $\sqrt{s_{\mathrm{NN}}} = 5.02$ TeV with the ALICE experiment. Measurements are performed via the newly accessed decay channel $\overline{\Sigma}^{\pm} \rightarrow {\rm\overline{n}}\pi^{\pm}$. A new method of antineutron reconstruction with the PHOS electromagnetic spectrometer is developed and applied to this analysis. The $p_{\rm T}$ spectra of $\overline{\Sigma}^{\pm}$ are measured in the range $0.5 < p_{\rm T} < 3$ GeV/$c$ and compared to predictions of the PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4 models. The EPOS LHC and EPOS4 models provide the best descriptions of the measured spectra both in pp and p-Pb collisions, while models which do not account for multiparton interactions provide a considerably worse description at high $p_{\rm T}$. The total yields of $\overline{\Sigma}^{\pm}$ in both pp and p-Pb collisions are compared to predictions of the Thermal-FIST model and dynamical models PYTHIA 8, DPMJET, PHOJET, EPOS LHC and EPOS4. All models reproduce the total yields in both colliding systems within uncertainties. The nuclear modification factors $R_{\rm pPb}$ for both $\overline{\Sigma}^{+}$ and $\overline{\Sigma}^{-}$ are evaluated and compared to those of protons, $\Lambda$ and $\Xi$ hyperons, and predictions of EPOS LHC and EPOS4 models. No deviations of $R_{\rm pPb}$ for $\overline{\Sigma}^{\pm}$ from the model predictions or measurements for other hadrons are found within uncertainties. |
| 2025-07-17 | [The Time-Energy Principle in Algebraic Geometry](http://arxiv.org/abs/2507.13134v1) | Renaud Gauthier | We consider the time-energy uncertainty principle from Quantum Mechanics and provide its Algebro-Geometric interpretation within the context of stacks. |
| 2025-07-17 | [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](http://arxiv.org/abs/2507.13092v1) | Hyo-Jeong Jang, Hye-Bin Shin et al. | Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications. |
| 2025-07-16 | [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](http://arxiv.org/abs/2507.12461v1) | Trong-Thang Pham, Anh Nguyen et al. | Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets. |
| 2025-07-16 | [Precision measurement of the ${\itÎž}_b^0$ baryon lifetime](http://arxiv.org/abs/2507.12402v1) | LHCb collaboration, R. Aaij et al. | A sample of $pp$ collision data, corresponding to an integrated luminosity of 5.4 fb$^{-1}$ and collected by the LHCb experiment during LHC Run 2, is used to measure the ratio of the lifetime of the ${\it{\Xi}}_b^0$ baryon to that of the ${\it{\Lambda}}_b^0$ baryon, $r_{\tau}\equiv\tau_{{\it{\Xi}}_b^0}/\tau_{{\it{\Lambda}}_b^0}$. The value ${r_{\tau}^{\rm Run\,2}=1.004\pm0.009\pm0.006}$ is obtained, where the first uncertainty is statistical and the second systematic. This value is averaged with the corresponding value from Run 1 to obtain ${r_{\tau} = 1.004\pm0.008\pm0.005}$. Multiplying by the known value of the ${\it{\Lambda}}_b^0$ lifetime yields ${{\tau_{{\it{\Xi}}_b^0}} = 1.475\pm0.012\pm0.008\pm0.009~{\rm ps}}$, where the last uncertainty is due to the limited knowledge of the ${\it{\Lambda}}_b^0$ lifetime. This measurement improves the precision of the current world average of the ${\it{\Xi}}_b^0$ lifetime by about a factor of two, and is in good agreement with the most recent theoretical predictions. |
| 2025-07-16 | [Surrogate modeling for uncertainty quantification in nonlinear dynamics](http://arxiv.org/abs/2507.12358v1) | S. Marelli, S. SchÃ¤r et al. | Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use. |
| 2025-07-16 | [The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models](http://arxiv.org/abs/2507.12335v1) | Morgan Deal, GaÃ«l Buldgen et al. | The transport of chemical elements in stellar interiors is one of the greatest sources of uncertainties of solar and stellar modelling. The Sun, with its exquisite spectroscopic, helioseismic and neutrino observations, offers a prime environment to test the prescriptions used for both microscopic and macroscopic transport processes. We study in detail the impact of various formalisms for atomic diffusion on helioseismic constraints in both CLES (Scuflaire et al., 2008a) and Cesam2k2 (Morel and Lebreton 2008; Marques et al. 2013; Deal et al. 2018) models and compare both codes in detail. Moreover, due to the inability of standard models using microscopic diffusion to reproduce light element depletion in the Sun (Li, Be), another efficient process must be included to reproduce these constraints (rotation-induced: Eggenberger et al. 2022, overshooting -- or penetrative convection -- below the convective envelope: Th\'evenin et al. 2017, or ad hoc turbulence: Lebreton and Maeder 1987; Richer, Michaud, and Turcotte 2000). However, introducing such an extra mixing leads to issues with the CNO neutrino fluxes (see Buldgen et al. 2023), which seem to be systematically lower than the Borexino observations (Appel et al., 2022. Another key aspect to consider when reconciling models with neutrino fluxes is the impact of electronic screening (Mussack and D\"appen, 2011). |
| 2025-07-16 | [Uncertainty and entropies of classical channels](http://arxiv.org/abs/2507.12310v1) | Takla Nateeboon | In this thesis, I studied a mathematical development to define and quantify the uncertainty inherent in classical channels. This thesis starts with the introduction and background on how to formally think about uncertainty in the domain of classical states. The concept of probability vector majorization and its variants, relative majorization and conditional majorization, are reviewed. This thesis introduces three conceptually distinct approaches to formalize the notion of uncertainty inherent in classical channels. These three approaches define the same preordering on the domain of classical channels, leading to characterizations from many perspectives. With the solid foundation of uncertainty comparison, classical channel entropy is then defined to be an additive monotone with respect to the majorization relation. The well-known entropies in the domain of classical states are uniquely extended to the domain of channels via the optimal extensions, providing not only a solid foundation but also the quantifiers of uncertainty inherent in classical channels. |
| 2025-07-16 | [Forecasting Climate Policy Uncertainty: Evidence from the United States](http://arxiv.org/abs/2507.12276v1) | Donia Besher, Anirban Sengupta et al. | Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers strive to balance economic growth with environmental goals. High levels of CPU can slow down investments in green technologies, make regulatory planning more difficult, and increase public resistance to climate reforms, especially during times of economic stress. This study addresses the challenge of forecasting the US CPU index by building the Bayesian Structural Time Series (BSTS) model with a large set of covariates, including economic indicators, financial cycle data, and public sentiments captured through Google Trends. The key strength of the BSTS model lies in its ability to efficiently manage a large number of covariates through its dynamic feature selection mechanism based on the spike-and-slab prior. To validate the effectiveness of the selected features of the BSTS model, an impulse response analysis is performed. The results show that macro-financial shocks impact CPU in different ways over time. Numerical experiments are performed to evaluate the performance of the BSTS model with exogenous variables on the US CPU dataset over different forecasting horizons. The empirical results confirm that BSTS consistently outperforms classical and deep learning frameworks, particularly for semi-long-term and long-term forecasts. |
| 2025-07-16 | [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](http://arxiv.org/abs/2507.12262v1) | Zachary James, Joseph Guinness | Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset. |
| 2025-07-16 | [What are we talking about when we discuss the Born-Oppenheimer approximation?](http://arxiv.org/abs/2507.12223v1) | Olimpia Lombardi, Sebastian Fortin et al. | Nick Huggett, James Ladyman, and Karim Thebault (HLT) have presented a comprehensive article examining the Born-Oppenheimer Approximation (BOA). Their central objective is to challenge our position on the matter-namely, that the BOA incorporates a classical assumption incompatible with the Heisenberg Uncertainty Principle. In contrast, HLT contend that the BOA involves no such classical assumption and, as a result, supports the view that chemistry can be reduced to physics. The purpose of this paper is to offer a critical analysis of the HLT article and to clarify why we consider their arguments unpersuasive. |
| 2025-07-16 | [Explainable Evidential Clustering](http://arxiv.org/abs/2507.12192v1) | Victor F. Lopes de Souza, Karima Bakhti et al. | Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of "tolerable" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time. |
| 2025-07-16 | [Learning, fast and slow: a two-fold algorithm for data-based model adaptation](http://arxiv.org/abs/2507.12187v1) | Laura Boca de Giuli, Alessio La Bella et al. | This article addresses the challenge of adapting data-based models over time. We propose a novel two-fold modelling architecture designed to correct plant-model mismatch caused by two types of uncertainty. Out-of-domain uncertainty arises when the system operates under conditions not represented in the initial training dataset, while in-domain uncertainty results from real-world variability and flaws in the model structure or training process. To handle out-of-domain uncertainty, a slow learning component, inspired by the human brain's slow thinking process, learns system dynamics under unexplored operating conditions, and it is activated only when a monitoring strategy deems it necessary. This component consists of an ensemble of models, featuring (i) a combination rule that weights individual models based on the statistical proximity between their training data and the current operating condition, and (ii) a monitoring algorithm based on statistical control charts that supervises the ensemble's reliability and triggers the offline training and integration of a new model when a new operating condition is detected. To address in-domain uncertainty, a fast learning component, inspired by the human brain's fast thinking process, continuously compensates in real time for the mismatch of the slow learning model. This component is implemented as a Gaussian process (GP) model, trained online at each iteration using recent data while discarding older samples. The proposed methodology is tested on a benchmark energy system referenced in the literature, demonstrating that the combined use of slow and fast learning components improves model accuracy compared to standard adaptation approaches. |
| 2025-07-15 | [Canonical Bayesian Linear System Identification](http://arxiv.org/abs/2507.11535v1) | Andrey Bryutkin, Matthew E. Levine et al. | Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data. |
| 2025-07-15 | [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](http://arxiv.org/abs/2507.11488v1) | Pakizar Shamoi, Nuray Toganas et al. | Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical. |
| 2025-07-15 | [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](http://arxiv.org/abs/2507.11420v1) | Mingcong Li | Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant challenge.Conventional robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a priori.This article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational data.Concurrently, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled dynamics.This dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional approaches.We formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high probability.Numerical experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies. |
| 2025-07-15 | [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](http://arxiv.org/abs/2507.11413v1) | Christos N. Efrem, Ioannis Krikidis | We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average. |
| 2025-07-15 | [Towards NNLO QCD predictions for off-shell top-quark pair production and decays](http://arxiv.org/abs/2507.11410v1) | Luca Buonocore, Massimiliano Grazzini et al. | We consider QCD radiative corrections to $W^+W^-b {\bar b}$ production with leptonic decays and massive bottom quarks at the LHC. We perform an exact next-to-leading order (NLO) calculation within the $q_T$-subtraction formalism and validate it against an independent computation in the dipole subtraction scheme. Non-resonant and off-shell effects related to the top quarks and the leptonic decays of the $W^\pm$ bosons are consistently included. We also consider the approximation in which the real-emission contribution is computed exactly while the virtual is evaluated in the double-pole approximation (DPA), which formally requires the inclusion of both factorisable and non-factorisable corrections. We evaluate such contributions and show that the DPA performs remarkably well at both the inclusive and differential levels. We then extend our calculation to the next-to-next-to-leading order (NNLO). All tree-level and one-loop amplitudes are evaluated exactly, while the missing two-loop virtual contribution is estimated using the DPA. The factorisable two-loop corrections are explicitly computed by relying on available results for the polarised two-loop on-shell top-quark pair production amplitudes and the corresponding top-quark decays. The non-factorisable contributions are inferred by exploiting the cancellation of logarithmic singularities in the $\Gamma_t\to 0$ limit through an on-shell matching procedure. The NNLO corrections for the inclusive cross section are found to increase the NLO prediction by approximately $11\%$, with a numerical uncertainty that is conservatively estimated to be below the $2\%$ level $\unicode{x2013}$ significantly smaller than the $5\%$ residual perturbative uncertainties. |
| 2025-07-15 | [Bayesian Model Selection and Uncertainty Propagation for Beam Energy Scan Heavy-Ion Collisions](http://arxiv.org/abs/2507.11394v1) | Syed Afrid Jahan, Hendrik Roch et al. | We apply the Bayesian model selection method (based on the Bayes factor) to optimize $\sqrt{s_\mathrm{NN}}$-dependence in the phenomenological parameters of the (3+1)-dimensional hybrid framework for describing relativistic heavy-ion collisions within the Beam Energy Scan program at the Relativistic Heavy-Ion Collider. The effects of various experimental measurements on the posterior distribution are investigated. We also make model predictions for longitudinal flow decorrelation, rapidity-dependent anisotropic flow and identified particle $v_0(p_\mathrm{T})$ in Au+Au collisions, as well as anisotropic flow coefficients in small systems. Systematic uncertainties in the model predictions are estimated using the variance of the simulation results with a few parameter sets sampled from the posterior distributions. |
| 2025-07-15 | [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](http://arxiv.org/abs/2507.11385v1) | George D. Pasparakis, Ioannis A. Kougioumtzoglou et al. | A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics. |
| 2025-07-15 | [The miniJPAS survey quasar selection V: combined algorithm](http://arxiv.org/abs/2507.11380v1) | Ignasi PÃ©rez-RÃ fols, L. Raul Abramo et al. | Aims. Quasar catalogues from narrow-band photometric data are used in a variety of applications, including targeting for spectroscopic follow-up, measurements of supermassive black hole masses, or Baryon Acoustic Oscillations. Here, we present the final quasar catalogue, including redshift estimates, from the miniJPAS Data Release constructed using several flavours of machine-learning algorithms. Methods. In this work, we use a machine learning algorithm to classify quasars, optimally combining the output of 8 individual algorithms. We assess the relative importance of the different classifiers. We include results from 3 different redshift estimators to also provide improved photometric redshifts. We compare our final catalogue against both simulated data and real spectroscopic data. Our main comparison metric is the $f_1$ score, which balances the catalogue purity and completeness. Results. We evaluate the performance of the combined algorithm using synthetic data. In this scenario, the combined algorithm outperforms the rest of the codes, reaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\geq2.1$ and $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its performance against real spectroscopic data, finding different performances. We conclude that our simulated data is not realistic enough and that a new version of the mocks would improve the performance. Our redshift estimates on mocks suggest a typical uncertainty of $\sigma_{\rm NMAD} =0.11$, which, according to our results with real data, could be significantly smaller (as low as $\sigma_{\rm NMAD}=0.02$). We note that the data sample is still not large enough for a full statistical consideration. |
| 2025-07-15 | [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](http://arxiv.org/abs/2507.11357v1) | Emile van Krieken, Pasquale Minervini et al. | The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons. |
| 2025-07-15 | [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](http://arxiv.org/abs/2507.11352v1) | Yunhao Yang, Neel P. Bhatt et al. | Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning. While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty. On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation. It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold. A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%. These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics. |
| 2025-07-14 | [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](http://arxiv.org/abs/2507.10510v1) | Jiangkai Wu, Zhiyuan Ren et al. | AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from "humans watching video" to "AI understanding video". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat. |
| 2025-07-14 | [Referential ambiguity and clarification requests: comparing human and LLM behaviour](http://arxiv.org/abs/2507.10445v1) | Chris Madge, Matthew Purver et al. | In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy. |
| 2025-07-14 | [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](http://arxiv.org/abs/2507.10425v1) | Alvaro H. C. Correia, Christos Louizos | Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift. |
| 2025-07-14 | [Massive stars advanced evolution: I -- New reaction rates for carbon and oxygen nuclear reactions](http://arxiv.org/abs/2507.10377v1) | T. Dumont, A. Bonhomme et al. | The nuclear rates for reactions involving 12C and 16O are key to compute the energy release and nucleosynthesis of massive stars during their evolution. These rates shape the stellar structure and evolution, and impact the nature of the final compact remnant. We explore the impact of new nuclear reaction rates for 12C({\alpha},{\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for massive stars. We aim to investigate how the structure and nucleosynthesis evolve and how these processes influence the stellar fate. We computed stellar models using the GENEC code, including updated rates for 12C({\alpha},{\gamma})16O and, for the three fusion reactions, new rates following a fusion suppression scenario and new theoretical rates obtained with TDHF calculations. The updated 12C({\alpha},{\gamma})16O rates mainly impact the chemical structure evolution changing the 12C/16O ratio with little effect on the CO core mass. This variation in the 12C/16O ratio is critical for predicting the stellar fate, which is very sensitive to 12C abundance. The combined new rates for 12C+12C and 16O+16O fusion reactions according to the HIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the ignition conditions to higher temperatures and densities. Theoretical TDHF rates primarily affect C-burning, increasing its duration and lowering the ignition temperature. These changes alter the core chemical structure, the carbon shell size and duration, and hence the compactness. They also affect nucleosynthesis. This work shows that accurate reaction rates for key processes in massive star evolution drive significant changes in stellar burning lifetimes, chemical evolution, and stellar fate. In addition, discrepancies between experimental and theoretical rates introduce uncertainties in model predictions, influencing both the internal structure and the supernova ejecta composition. |
| 2025-07-14 | [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](http://arxiv.org/abs/2507.10355v1) | Bo Jiang, Xueyang Ze et al. | Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach. |
| 2025-07-14 | [Gaussian Process Methods for Very Large Astrometric Data Sets](http://arxiv.org/abs/2507.10317v1) | Timothy Hapitas, Lawrence M. Widrow et al. | We present a novel non-parametric method for inferring smooth models of the mean velocity field and velocity dispersion tensor of the Milky Way from astrometric data. Our approach is based on Stochastic Variational Gaussian Process Regression (SVGPR) and provides an attractive alternative to binning procedures. SVGPR is an approximation to standard GPR, the latter of which suffers severe computational scaling with N and assumes independently distributed Gaussian Noise. In the Galaxy however, velocity measurements exhibit scatter from both observational uncertainty and the intrinsic velocity dispersion of the distribution function. We exploit the factorization property of the objective function in SVGPR to simultaneously model both the mean velocity field and velocity dispersion tensor as separate Gaussian Processes. This achieves a computational complexity of O(M^3) versus GPR's O(N^3), where M << N is a subset of points chosen in a principled way to summarize the data. Applied to a sample of ~8 x 10^5 stars from the Gaia DR3 Radial Velocity Survey, we construct differentiable profiles of the mean velocity and velocity dispersion as functions of height above the Galactic midplane. We find asymmetric features in all three diagonal components of the velocity dispersion tensor, providing evidence that the vertical dynamics of the Milky Way are in a state of disequilibrium. Furthermore, our dispersion profiles exhibit correlated structures at several locations in |z|, which we interpret as signatures of the Gaia phase spiral. These results demonstrate that our method provides a promising direction for data-driven analyses of Galactic dynamics. |
| 2025-07-14 | [How an overweight and rapidly rotating PG 1159 star in the Galactic halo challenges evolutionary models](http://arxiv.org/abs/2507.10314v1) | Nina Mackensen, Nicole Reindl et al. | PG 1159 stars are thought to be progenitors of the majority of H-deficient white dwarfs. Their unusual He-, C-, and O-dominated surface composition is typically believed to result from a late thermal pulse experienced by a single (pre-)white dwarf. Yet, other formation channels - involving close binary evolution - have recently been proposed and could lead to similar surface compositions. Here we present a non-local thermodynamic equilibrium spectral analysis based on new UV and archival optical spectra of one of the hottest PG 1159 stars, $\text{RX J}0122.9\text{ -}7521$. We find $T_\text{eff} = 175$ kK and a surface gravity of log $g = 7.7$, and an astonishingly low O/C ratio of $7.3 \times 10^{-3}$ by mass. By combining the spectroscopic surface gravity and Gaia parallax with a spectral energy distribution fit, we derive a mass of $M_\text{spec} = 1.8^{+1.1}_{-0.7}$ $M_\odot$. Although this spectroscopic mass is higher than predicted by evolutionary models, it is subject to substantial uncertainty. Furthermore, we find that $\text{RX J}0122.9\text{ -}7521$ shows strongly rotationally broadened lines, suggesting that the previously reported photometric period of $41$ min indeed corresponds to the rotational period of this star. Our kinematic analysis shows that $\text{RX J}0122.9\text{ -}7521$ belongs to the Galactic halo, which - assuming single-star evolution - is in stark contrast to its relatively high mass. The rapid rotation, high mass, and halo kinematics, as well as the lack of evidence for a close companion, lead us to believe that $\text{RX J}0122.9\text{ -}7521$ formed through the merger of two white dwarfs. Yet, none of the current models can explain the surface abundances of $\text{RX J}0122.9\text{ -}7521$. |
| 2025-07-14 | [High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy](http://arxiv.org/abs/2507.10291v1) | Kristine V. Ung, Connor A. Roncaioli et al. | The negatively charged diamond nitrogen-vacancy ($\mathrm{{NV}^-}$) center plays a central role in many cutting edge quantum sensing applications; despite this, much is still unknown about the energy levels in this system. The ionization energy of the $\mathrm{^{1}E}$ singlet state in the $\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33 eV. In this work, we further refine this energy by measuring the $\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond temperature via magnetically mediated spin-selective photoluminescence (PL) quenching; this PL quenching indicating at what wavelength ionization induces population transfer from the $\mathrm{^{1}E}$ into the neutral $\mathrm{{NV}^0}$ charge configuration. Measurements are performed for excitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm in increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in 5 K increments. We determine the $\mathrm{^{1}E}$ ionization energy to be between 2.29 and 2.33 eV, which provides about a two-fold reduction in uncertainty of this quantity. Distribution level: A. Approved for public release; distribution unlimited. |
| 2025-07-14 | [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](http://arxiv.org/abs/2507.10201v1) | Gleb Shishaev, Vasily Demyanov et al. | The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure. |
| 2025-07-14 | [Recursive Feasibility without Terminal Constraints via Parent-Child MPC Architecture](http://arxiv.org/abs/2507.10166v1) | Filip Surmaa, Anahita Jamshidnejad | This paper proposes a novel hierarchical model predictive control (MPC) framework, called the Parent-Child MPC architecture, to steer nonlinear systems under uncertainty towards a target set, balancing computational complexity and guaranteeing recursive feasibility and stability without relying on conservative terminal constraints in online decision-making. By coupling a small-horizon Child MPC layer with one or more large-horizon Parent MPC layers, the architecture ensures recursive feasibility and stability through adjustable stage-wise constraints derived from tube-based control. As is demonstrated in our case studies, compared to traditional MPC methods, the proposed Parent-Child MPC architecture enhances performance and computational efficiency, reduces conservativeness, and enables scalable planning for certain nonlinear systems. |

</details>

<!-- HISTORICAL_PAPERS_END -->
---


